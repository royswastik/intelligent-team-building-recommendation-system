A Semantic Feature for Relation Recognition Using A Web-based Corpus
Chen-Ming Hung
Institute of Information Science
Academia Sinica, Taipei, Taiwan
rglly@iis.sinica.edu.tw
Abstract
Selecting appropriate features to represent
an entity pair plays a key role in the task
of relation recognition. However, existing
syntactic features or lexical features cannot
capture the interaction between two enti-
ties because of the dearth of annotated rela-
tional corpus specialized for relation recog-
nition. In this paper, we propose a seman-
tic feature, called the latent topic feature,
which is topic-based and represents an en-
tity pair at the semantic level instead of the
word level. Moreover, to address the prob-
lem of insufficiently annotated corpora, we
propose an algorithm for compiling a train-
ing corpus from the Web. Experiment results
demonstrate that latent topic features are
as effective as syntactic or lexical features.
Moreover, the Web-based corpus can resolve
the problems caused by insufficiently anno-
tated relational corpora.
1 INTRODUCTION
Relation recognition is a challenging task because
finding appropriate features to represent the rela-
tionship between two entities is difficult and limited
by the scarcity of annotated corpora. Prior works
on relation recognition have focused on syntactic
features, e.g., parsing trees (Culotta and Sorensen,
2004; Zelenko et al, 2003), and on lexical fea-
tures, e.g., Part-Of-Speech (POS) features. These
approaches show that syntactic features and lexical
features outperform bag-of-words (BOW) on exist-
ing annotated corpora such as the RDC corpus of the
ACE project. The superior performance achieved by
syntactic and lexical features is due to their ability
to capture the grammatical relations between two
entities and the characteristics of the entities. For
example, (Culotta and Sorensen, 2004) add hyper-
nyms of entities to features derived from WordNet.
However, neither syntactic nor lexical features can
capture the interaction between two entities at the
semantic level.
Another issue in the task of relation recognition
is insufficiently annotated corpora. For example,
given a pair {the U.N. body, Kosovo}, we can only
find three sentences containing both entities in the
RDC corpus, which is commonly used corpus in the
relation recognition task. The problem of an in-
sufficiently annotated corpus biases feature vectors
and distorts the prediction of entity pairs. How-
ever, (Huang et al, 2004; Hung and Chien, 2007)
have shown that the Web can be used as an alterna-
tive source of documents related to a given query.
That is possibly because of the increasing size of
the Web and the efficiency in commercial search en-
gines, e.g., Google and Yahoo!.
To resolve the above problems, we propose a se-
mantic feature called the latent topic feature, which
is extracted by exploiting the Latent Dirichlet Al-
location (LDA) algorithm. Unlike syntactic fea-
tures or lexical features, latent topic features repre-
sent entity pairs as random mixtures of latent topics,
where each topic is characterized by a distribution
of words. We prove experimentally that latent topic
features are as effective as syntactic features or lexi-
cal features in capturing the interaction between two
entities. The experiment results are predictable. In
434
the above {the U.N. body, Kosovo} example, it may
be difficult to determine the relationship between
U.N. body and Kosovo straightforwardly. However,
making the right guess about the relationship is eas-
ier if the U.N. body is grouped with army and gov-
ernment. Therefore, the right guess in this example
is management.
To overcome the problems caused by an insuffi-
ciently annotated corpus, we exploit the Web as a
source of training data for the relation recognition
task. Given an entity pair, documents describing the
entity pair are extracted from the Web via commer-
cial search engines using both entities as the query.
In other words, snippets returned from the Web are
treated as documents related to the query. Our as-
sumption, which has been proved in previously pub-
lished works, is that returned snippets can capture
the interaction between two entities. After the latent
topic features extracted from returned snippets using
the Web as the corpus, an SVM classifier is trained
as the relation recognition classifier for use in the
later experiments.
The remainder of the paper is organized as fol-
lows. in Section 2, we discuss works related to fea-
ture selection in the relation recognition task as well
as using the Web as a corpus. The concept of la-
tent topic features is presented in Section 3. We also
explain how we represent a document in the vector
space of a latent topic feature. Section 4contains
an evaluation of the latent topic feature. We then
present our conclusions in Section 5.
2 RELATED WORK
In the field of information extraction (IE), the goal
of relation recognition is to find the relationship
between two entities. Without considering en-
tity detection, relation recognition depends heav-
ily on the representation of entity pairs. (Zelenko
et al, 2003) showed how to extract relations by
computing the kernel functions between the ker-
nels of shallow parse trees. The kernels are de-
fined over a shallow parse representation of the text
and used in conjunction with a Support Vector Ma-
chine (SVM) learning algorithm to extract person-
affiliation and organization-location relations. (Cu-
lotta and Sorensen, 2004) extended this work to es-
timate kernel functions between augmented depen-
dency trees, while (Kambhatla, 2004) combined lex-
ical features, syntactic features, and semantic fea-
tures in a maximum entropy model. However, the
semantic features discussed in (Kambhatla, 2004)
still focus on the word level instead of the concep-
tual level.
LDA is an aspect model that represents docu-
ments as a set of topics instead of a bag-of-words.
Latent semantic indexing (LSI) (Deerwester et al,
1990) and probabilistic latent semantic indexing
(PLSI) (Hofmann, 1999) are also aspect models and
have been widely used in the field of information
retrieval. LSI simply assumes that each document
is generated from single latent topic, while PLSI
attempts to relax the assumption by using a mix-
ture of latent topics for each document. However,
PLSI is highly dependent on training documents; in
other words, it cannot handle the probability of la-
tent topics in a previously unseen document. In ad-
dition, the number of parameters that must be esti-
mated in PLSI grows linearly with the number of
training documents. (Blei et al, 2003; Blei and
Jordan, 2003) proposed LDA to resolve the above-
mentioned limitations. It can easily generate an un-
seen document under controllable parameters.
A number of works, e.g., (Huang et al, 2004),
have investigated using the Web to acquire a train-
ing corpus or acquire additional information not pro-
vided by existing annotated corpora. (Huang et al,
2004) exploited the Web as a training corpus to train
a classifier with user-defined categories. However,
it is widely recognized that when using documents
on the Web users must spend a great deal of time
filtering out unrelated contents. (Hung and Chien,
2007) designed a bootstrapping method that adapts
an existing corpus with an automatic verification al-
gorithm in order to control the quality of returned
snippets in each iteration. (Matsuo et al, 2006) used
the Web to construct a social network system, called
POLYPHONET, which visualizes the relationship
between two personal names.
3 LATENT TOPIC FEATURE
In this section, we introduce the concept of using the
Web to augment an insufficiently annotated corpus
for relation recognition. Then we apply the LDA
algorithm to the corpus to extract the latent topic
435
features to represent entity pairs in the corpus for
relation recognition.
Figure 1: The framework of the proposed approach.
3.1 Compiling a Web-based Relational Corpus
For an entity pair, E = {e1, e2}, where e1 and e2
are named entities, it is difficult to find sufficient
sentences to describe their relationship from exist-
ing annotated corpora. In other words, given an en-
tity pair without a relation label, users cannot rec-
ognize the pair. Even with a widely used thesaurus,
like WordNet, we can only obtain hypernyms or syn-
onyms of given entities. It is not possible to obtain
knowledge about the interaction between two enti-
ties.
To capture the interaction between two entities,
we send both entities, e1 AND e2, to commer-
cial search engines and collect returned snippets as
training documents for an entity pair, E. Snippets
of returned search results are defined as the sur-
rounding contexts of queries highlighted by com-
mercial search engines. In other words, the full
texts of search results are not considered in the
collected corpus when filtering noisy information
in full documents. Let R be the relation label of
entity pairs {E1, . . . , EM}; then, the training cor-
pus for R is the collection of all returned snippets
for {E1, . . . , EM}. Through effective commercial
search engines such as Google and Yahoo!, sen-
tences describing the interaction between two enti-
ties can easily be retrieved. Returning to the ex-
ample {the U.N. body, Kosovo}, almost two mil-
lion sentences with co-occurrences of the two en-
tities are retrieved by Google. Another advantage of
using the Web to retrieve relevant documents is the
auto? correction ability of commercial search en-
gines. The feature can correct a misspelled query
or replace an uncommon word with a synonym or a
common word which is correct, so that more related
information about entity pairs can be retrieved from
the Web as returned snippets. For example, Google
can automatically link the U.N. body to United
Nations, which is used more frequently in search-
ing. Clearly, the number of returned snippets must
be considered. Actually, based on experiment re-
sults in Section 4, we set the number as five, which
achieves the best performance.
3.2 Modified LDA for the Relational Corpus
LDA is an aspect model with three levels, namely,
the corpus level, the document level, and the word
level. Given a document, variables of the corpus it
belongs to are sampled first, after which the variable
of the document is sampled once. Finally, variables
for words in the document are sampled.
For a document d in a corpus D, the modeling
process is as follows:
1. Sample ? ? Dir(?|?).
2. For each word wn in d, n ? {1, . . . , N}:
(a) sample zn ? Mult(?),
(b) sample a word wn ? p(wn|zn, ?) from a
multivariate Gaussian distribution condi-
tioned on the topic zn.
Note that ? is a vector of corpus-level variables
whose dimensionality is equal to the number of la-
tent topics; ? is a variable of the document and is as-
sumed to follow Dirichlet distribution for the given
corpus; and ? is a word-level variable. In addition,
Z = {z1, z2, . . . , zN} are latent factors that gener-
ate the document, and zn is the latent topic that wn
is generated from. Finally, N is the length of the
document d.
An entity pair E in the relational corpus is similar
to a document d in the text corpus. In other words,
the corpus DR is comprised of returned snippets for
all entity pairs ER with the same relation label R.
Therefore, given the parameters ? and ?, we obtain
the distribution of entity pair E as follows:
p(E|?, ?) =
? ?
zn
p(?, zn, SE |?, ?)d?,
where
p(?, zn, SE |?, ?) = p(?|?)
NE?
n=1
p(zn|?)p(wn|zn, ?),
436
NE is the number of words in the returned snippets
for E; and wn is the nth word in SE , the returned
snippets of E. Table 1 summarizes notations used in
the paper.
Table 1: Notations used in this paper.
SYMBOL DESCRIPTION
R relation label
DR corpus for R
ERj jth entity pair in the relation label R
SE returned snippets for an entity pair E
|ER| number of entity pairs in the relation R
NE number of words in SE
wn nth word in SE
zn latent topic that wn is generated from
In Section 3.1, we discussed the advantages of us-
ing the Web as a corpus to model entity pairs. In
the modeling process, we estimate the probability of
wn conditioned on zn, p(wn|zn, ?), to maximize the
probability of the entire corpus of R. The probabil-
ity that we try to maximize is
p(DR|?, ?) =
|ER|?
j=1
p(ERj |?, ?).
3.3 Latent Topic Feature
In different corpora, z obtains a different distribu-
tion to maximize the likelihood of the given corpus.
In this section, we describe how to exploit z as fea-
tures to represent a snippet for an entity pair E, i.e.,
SE . In Section 3.2, we noted that the parameters to
be estimated in the aspect model are all probabilities
of words in each latent topic z. Thus, we let the ex-
pected number of words generated from latent topics
be features of each entity pair. In other words, an en-
tity pair E is represented as a feature vector whose
length is equal to the number of latent topics and
whose ith attribute is equal to
?i +
NE?
n=1
|wn| ? p(wn|zn, ?),
where ?i is the ith prior Dirichlet parameter.
In addition, because there is no solution good
enough to determine the dimensionality of the fea-
ture vector or the number of latent topics, we set the
number of topics at thirty because it probably min-
imize the computation cost without significantly af-
fecting the performance.
4 EXPERIMENTS
In this section, we evaluate the performance of the
latent topic feature in representing entity pairs
extracted from the Relation Detection and Charac-
terization (RDC) corpus of the Automatic Content
Extraction 2003 model (ACE 2003)1.
4.1 The RDC Corpus
In the RDC corpus, five relation types, AT , NEAR,
PART , ROLE, and SOC, are defined; each rela-
tion type has extended sub-relations. Table 2 sum-
marizes the relations in the RDC corpus for ACE
2003. Based on Table 2, we find that the distribution
of the number relations is very unbalanced, ranging
from 2 to 773. In the following experiments, we only
consider the Role relation because it has the largest
numbers of sub-relations and it is easier to verify
the recognition results manually. Note that a rela-
tion is dropped if it has less than ten sub-relations in
order to avoid the bias of learned classifiers. There-
fore, the sub-relation founder in Role is dropped
in the following experiments because it occurs less
than ten times. Other is also dropped because its
definition is unclear.
Table 2: Distribution over relation types in the RDC
corpus (ACE 2003).
Relations Sub-Relations(Size)
AT Based-in(78) Located(773)
Residence(186)
NEAR Relative-location(73)
PART Part-of(242) Subsidiary(172)
Other(2)
ROLE Affiliate-partner(34)
Citizen-of(93) Client(33)
Founder(6) General-Staff(460)
Management(294) Member(398)
Owner(41) Other(98)
SOC Associate(25) Grandparent(3)
Parent(23) Sibling(5)
Spouse(22) Other-relative(24)
Other-Personal(10)
Other-Professional(88)
1http://projects.ldc.upenn.edu/ace/
437
4.2 Setting and Measurement
We used the package of (Chang and Lin, 2001) to
design the following experiments. In addition, ?-
SVM with a radial kernel function was used to learn
the relation classifier. To determine the parameters
in ?-SVM, i.e., ? and ?, we observed the perfor-
mance of the ?-SVM classifier by randomly select-
ing 80% of the sentences in the RDC corpus as train-
ing data and the remaining 20% as test data. In
other words, we applied five-fold cross validation
to build a temporary model for parameter estima-
tion. Furthermore, it is well known that parame-
ters in the SVM model must be optimized manually;
therefore, we estimate ? first and then estimate ?. ?
is fixed while ? is being estimated and vice versa.
After estimation, the best result is achieved at the
point that ? is equal to 2.5? 10?4 and ? is equal to
0.05. We summarize the results in Figure 2. The top
graph in Figure 2 is the accuracy curve, where fixed
? = 2.5 ? 10?4 and flexible ?; the bottom graph is
the accuracy curve with fixed ? = 0.05 and flexible
?.
Figure 2: Accuracy of five-fold cross validation us-
ing bigram features. Top: ? with ? = 2.5 ? 10?4.
Bottom: ? with ? = 0.05.
For each sub-relation in Role, binary classifica-
tion is used in the experiments and the F-measure of
each sub-relation is used as the metric for assessing
the performance of latent topic features.
F ? value = 2? Precision?RecallPrecision+Recall
Recall = ] of correct positive predictions] of positive examples
Precision = ] of correct positive predictions] of positive predictions
4.3 Web-based Corpus vs. Annotated Corpus
We now evaluate the performance of relational clas-
sifier on a Web-based corpus and on an annotated
corpus. To assess the performance of on the anno-
tated corpus, sentences in the RDC corpus contain-
ing a co-occurrence of both given entities were ex-
tracted as training data to learn a benchmark relation
classifier. On the other hand, the Web-based corpus
is compiled from snippets retrieved by using both
entities as a query. The latent topic feature is ap-
plied on both the Web-based corpus and the RDC
corpus using the procedure described in Section 4.2.
In addition, to analyze the effect of the number of
returned snippets, we increased the number of snip-
pets from 3 to 45 in increments of three and then
summarized the relationship between the number of
returned snippets and the achieved accuracy curve
shown in Figure 3. In the figure, the training data
is comprised of snippets of information returned by
querying 80% of the entity pairs selected at random
in the RDC corpus. The test data comprises snippets
returned by querying the remaining 20% of entity
pairs in the corpus.
From Figure 3, we observe that using five re-
turned snippets for each entity pair achieves the best
accuracy (0.85), which is substantially higher than
the accuracy achieved by using annotated corpus
(0.69). Note that using more returned snippets does
not guarantee higher accuracy. For example, when
39 returned snippets are used for each entity pair,
the accuracy (0.56) is almost the same as that (0.55)
achieved by using only 3 returned snippets. More-
over, it is significantly less than the accuracy (0.69)
achieved by using the RDC corpus. This is rea-
sonable because the greater the number of returned
snippets, the larger the amount of noisy information
introduced to the classifier, which degrades its per-
formance.
438
Figure 3: Accuracy of five-fold cross validation us-
ing the Web-based corpus and the annotated corpus.
4.4 Latent Topic Feature vs. Other Features
In this section, we compare the performance of
latent topic features with that of syntactic fea-
tures and lexical features, i.e., bag ? of ? words
or parts ? of ? speech. Because of the superior
performance achieved by using the Web-based cor-
pus described in Section 4.3, we extracted features
from the training corpus compiled from that corpus
rather than the annotated corpus.
Based on the results reported in Section 4.3, five
snippets were returned by the Web-based corpus for
each entity pair. For each sub-relation, a one-class
SVM was trained to perform binary classification.
Each sub-relation of Role in Table 3 is applied
with binary classification using a one-class SVM.
Table 5 summarizes the results of a comparison be-
tween the latent topic feature and the features
used by (Culotta and Sorensen, 2004). The lat-
ter depends on dependency tree kernels, which rep-
resent the grammatical dependencies in a sentence
and are considered as syntactical features. In Table
5, BOW denotes bag-of-word, sparse represents a
sparse kernel, and contiguous represents a contigu-
ous kernel.
Surprisingly, for every sub sub-relation in Table
3, the latent topic feature consistently achieves a
significantly higher average recall rate, but a lower
average precision rate. This may be due to the
latent topic feature?s ability to capture informa-
tion at the semantic level precisely, but it cannot
distinguish the information at the word level eas-
ily. In other words, the latent topic feature can
capture the common semantic information, proba-
bly the Role, of all sub-relations, but it cannot tell
the difference between citizen ? of and founder.
Table 4 shows the results of applying binary classifi-
cation to five relations in the RDC corpus. Although
the precision rate for each relation is still low, the
recall rate has been increased significantly. This de-
mostrates the ability of the latent topic feature to
capture semantic information.
Table 3: Binary classification results for each sub-
relation of Role.
Latent Topic Feature
F Prec. Rec.
Aff.-Part. 0.30 0.18 1.00
Client 0.40 0.28 0.71
Citizen-Of 0.62 0.47 0.91
Gen.-Staff 0.78 0.64 0.99
Manage. 0.56 0.39 1.00
Member 0.62 0.46 0.93
Owner 0.45 0.29 0.98
Table 4: Binary classification results for each rela-
tion in the RDC corpus.
Latent Topic Feature
F Prec. Rec.
At 0.61 0.48 0.84
NEAR 0.36 0.23 0.88
PART 0.58 0.46 0.80
ROLE 0.71 0.64 0.79
SOC 0.59 0.45 0.87
In Table 5, although the recall rate using the
latent topic feature is much higher than that
achieved by the other features, unfortunately, the
F ? score of the latent topic feature cannot be
redeemed because of the much lower precision rate.
Moreover, the latent topic feature is comparable
to the sparse kernel method in a different way be-
cause it has a low precision rate but a high recall
rate. Finally, the latent topic feature achieves a
higher average F-score than the bag-of-words fea-
ture, which proves the assumption that the latent
topic feature can better capture the interaction be-
tween two entities than features at the word level.
5 CONCLUSION
We have proposed a concept called the latent topic
feature for the task of relation recognition and
evaluated it on the RDC of the ACE project. The
feature captures the interaction between two entities
439
Table 5: Comparison between Latent topic
feature and other features.
Average
F Prec. Rec.
Latent Topic 0.58 0.45 0.84
Sparse 0.59 0.83 0.46
Contiguous 0.62 0.85 0.49
BOW 0.52 0.73 0.40
Sparse+BOW 0.62 0.80 0.50
Cont.+BOW 0.63 0.81 0.52
at the semantic level rather than at the word level.
Therefore, combining the latent topic feature
with syntactic features and lexical features should
achieve a better performance than using the features
separately. In our future work, we will devise an ap-
propriate way of combining latent topic features
with syntactical and lexical features.
Because of the lack of a sufficiently annotated
corpus for relation corpus for relation recognition,
we have also proposed using a Web-based corpus
to train classifiers for the purpose. Our experiment
results demonstrates that Web documents can accu-
rately capture information about the interaction be-
tween two named entities in the absence of an an-
notated corpus. By using a Web-based corpus, the
time cost to manually annotating a corpus for rela-
tion recognition is expected to be significantly re-
duced if the quality of returned snippetscan be con-
trolled.
References
David M. Blei and Michael I. Jordan. 2003. Model-
ing annotated data. In Proceedings of the 26th SIGIR,
pages 127?134. ACM Press.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc. of Uncertainty in Artificial Intelli-
gence, UAI?99, Stockholm.
Chien-Chung Huang, Shui-Lung Chuang, and Lee-Feng
Chien. 2004. Liveclassifier: creating hierarchical text
classifiers through web corpora. In WWW ?04: Pro-
ceedings of the 13th international conference on World
Wide Web, pages 184?192, New York, NY, USA.
Chen-Ming Hung and Leeq-Feng Chien. 2007. Web-
based text classification in the absence of manually la-
beled training documents. J. Am. Soc. Inf. Sci. Tech-
nol., 58(1):88?96.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy models
for information extraction. In The Companion Volume
to the Proceedings of 42st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 178?181,
Barcelona, Spain, July. Association for Computational
Linguistics.
Yutaka Matsuo, Junichiro Mori, Masahiro Hamasaki,
Keisuke Ishida, Takuichi Nishimura, Hideaki Takeda,
Koiti Hasida, and Mitsuru Ishizuka. 2006. Poly-
phonet: an advanced social network extraction system
from the web. In WWW ?06: Proceedings of the 15th
international conference on World Wide Web, pages
397?406, New York, NY, USA.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. J. Mach. Learn. Res., 3:1083?1106.
440
