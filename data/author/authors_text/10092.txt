Exploiting Unlabeled Text to Extract New Words of Different
Semantic Transparency for Chinese Word Segmentation
Richard Tzong-Han Tsai?? and Hsi-Chuan Hung?
?Department of Computer Science and Engineering,
Yuan Ze University, Chung-Li, Taoyuan, Taiwan
?Department of Computer Science and Information Engineering,
National Taiwan University, Taipei, Taiwan
thtsai@saturn.yzu.edu.tw yabthung@gmail.com
?corresponding author
Abstract
This paper exploits unlabeled text data
to improve new word identification and
Chinese word segmentation performance.
Our contributions are twofold. First,
for new words that lack semantic trans-
parency, such as person, location, or
transliteration names, we calculate as-
sociation metrics of adjacent character
segments on unlabeled data and encode
this information as features. Second, we
construct an internal dictionary by using
an initial model to extract words from
both the unlabeled training and test set
to maintain balanced coverage on the
training and test set. In comparison
to the baseline model which only uses
n-gram features, our approach increases
new word recall up to 6.0%. Addition-
ally, our approaches reduce segmenta-
tion errors up to 32.3%. Our system
achieves state-of-the-art performance for
both the closed and open tasks of the
2006 SIGHAN bakeoff.
1 Introduction
Many Asian languages do not delimit words by
spaces. Word segmentation is therefore a key
step for language processing tasks in these lan-
guages. Chinese word segmentation (CWS) sys-
tems can be built by supervised learning from a
labeled data set. However, labeled data sets are
expensive to prepare as it involves manual an-
notation efforts. Therefore, exploiting unlabeled
data to improve CWS performance becomes an
important research goal. In addition, new word
identification (NWI) is also very important be-
cause they represent the latest information, such
as new product names.
This paper explores methods of extracting
information from both internal and external
unlabeled data to augment NWI and CWS.
According to (Tseng and Chen, 2002), new
words can be divided into two major cate-
gories: Words with high or low semantic trans-
parency (ST), which describes the correlation of
semantic meanings between a word and its mor-
phemes. We designed effective strategies toward
the identification of these two new word types.
One is based on transductive learning and the
other is based on association metrics.
2 The Model
2.1 Formulation
We convert the manually segmented words into
tagged character sequences. We tag each char-
acter with either B, if it begins a word, or I, if
it is inside or at the end of a word.
2.2 Conditional Random Fields
CRFs are undirected graphical models trained
to maximize a conditional probability (Lafferty
et al, 2001). A linear-chain CRF with parame-
ters ? = ?1, ?2, . . . defines a conditional proba-
bility for a state sequence y = y1 . . . yT given an
input sequence x = x1. . . xT to be
P?(y|x) =
1
fx
exp
( T
?
t=1
?
k
?kfk(yt?1, yt, x, t)
)
where Zx is the normalization that makes the
probability of all state sequences sum to one;
fk(yt?1, yt, x, t) is often a binary-valued feature
function and ?k is its weight. The feature func-
tions can measure any aspect of a state transi-
tion, yt?1 ? yt, and the entire observation se-
quence, x, centered at the current position, t.
For example, one feature function might have
value 1 when yt?1 is the state B, yt is the state
I, and is the character ??. Large positive val-
ues for ?k indicate a preference for such an event;
large negative values make the event unlikely.
In our CRF model, each binary feature is mul-
tiplied with all states (yt) or all state transitions
(yt?1yt). For simplicity, we omit them in the fol-
lowing discussion. In addition, we use C0 rather
than xt to denote the current character.
931
3 Baseline n-gram Features
Character n-gram features have proven their ef-
fectiveness in ML-based CWS (Xue and Shen,
2003). We use 4 types of unigram feature func-
tions: C0, C1 (next character), C?1 (previous
character), C?2 (character preceding C?1). Fur-
thermore, 6 types of bigram features are used,
and are designated here as conjunctions of the
previously specified unigram features, C?2C?1,
C?1C0, C0C1, C?3C?1, C?2C0, and C?1C1.
4 New Word Identification
We mainly focus on improving new word iden-
tification (NWI) using unlabeled text. Words
with high and low ST are discussed separately
due to the disparity in their morphological char-
acteristics. However, it is unnecessary for our
system to classify words as high- or low-ST be-
cause our strategies for dealing with these two
classes are employed synchronously.
4.1 High-ST words
For a high-ST word, its meaning can be easily
derived from those of its morphemes. A word?s
semantic meaning correlates to its tendency of
being affixed to longer words. This behavior
can be recorded by the baseline n-gram model.
When the baseline model is used to segment a
sentence containing a high-ST word, since this
tendency is consistent with that is recorded in
the baseline model, this word tends to be suc-
cessfully segmented. For example, supposeW
? zhi-nan-che (compass chariot) is in the train-
ing set. The baseline n-gram model will record
the tendency of W zhi-nan (guide) that it
tends to be a prefix of a longer word. When
tagging a sentence that contains another high
ST word also containing W, such as W?
zhi-nan-zhen (compass), this word can be cor-
rectly identified.
Using only n-gram features may prevent some
occurrences of high-ST words from being iden-
tified due to the ambiguity of neighboring n-
grams. To rectify this problem, we introduce
the transductive dictionary (TD) feature, which
is similar to the traditional dictionary feature
that indicates if a sequence of characters in a
sentence matches a word w in an existing dic-
tionary. The difference is that the TD not only
comprises words in the training set, but con-
tains words extracted from the unlabeled test
set. The transductive dictionary is so named be-
cause it is generated following general concepts
of transductive learning. We believe adding TD
features can boost recall of high-ST words. More
details on the TD are found in Section 5. The
TD features that identify high-ST words are de-
tailed in Section 6.1.
4.2 Low-ST words
On the contrary, new words lack of ST, such
as transliteration names, are more likely to
be missed by the baseline n-gram model, be-
cause their morphemes? morphological tenden-
cies are not guaranteed to be consistent with
those recorded by n-gram features. For instance,
suppose )s tian-ping (libra) only appears as
individual words in the training set. The base-
line model cannot identify ?)s xiong-tian-
ping (a singer?s name) because ?)s is a low-
ST word and the morphological tendency of )
s is not consistent with the recorded one.
In English, there is a similar phenomenon
called multi-word expressions (MWEs).
(Choueka, 1988) regarded MWE as connected
collocations: a sequence of neighboring words
?whose exact meaning cannot be derived from
the meaning or connotation of its components?,
which means that MWEs also have low ST.
As some pioneers provide MWE identification
methods which are based on association metrics
(AM), such as likelihood ratio (Dunning, 1993).
The methods of identifying low-ST words can
be divided into two: filtering and merging. The
former uses AM to measure the likelihood that a
candidate is actually a whole word that cannot
be divided. Candidates with AMs lower than
the threshold are filtered out. The latter strat-
egy merges character segments in a bottom-up
fashion. AMs are employed to suggest the next
candidates for merging. Both methods suffer
from two main drawbacks of AM: dependency
on segment length and inability to use relational
information between context and tags. In the
first case, applying AMs to ranking character
segment pairs, it is difficult to normalize the val-
ues calculated from pairs of character segments
of various lengths. Secondly, AMs ignore the re-
lationships among n-grams (or other contextual
information) and labels, which are abundant in
annotated corpora, and they only use annota-
tion data to determine thresholds. In Section
6.2, we illustrate how encoding AMs as features
can avoid the above weaknesses.
5 Balanced Transductive Dictionary
The simplest TD is composed of words in the
training set and words extracted from the unla-
beled test set. The main problem of such TD
is the disparity in training and test set cov-
erage. During training, since its coverage is
100%, the enabled dictionary features will be as-
signed very high weights while n-gram features
932
will be assigned low weights. During testing,
when coverage is approximately 80-90%, most
tags are decided by dictionary features enabled
by IV words, while n-gram features have lit-
tle influence. As a result, it is likely that only
IV words are correctly segmented, while OOV
words are over-segmented. Loosely speaking, a
dictionary?s coverage of the training set is linked
to the degree of reliance placed by the CRF
model on the corresponding dictionary features.
Therefore, the dictionary should be made more
balanced in order to avoid the potential problem
of overfitting. Here a dictionary is said to be
more balanced if its coverage of the training set
approximates its coverage of the test set while
maximizing the latter. Afterward we name the
TD composed of words from gold training set
and tagged test set and as Na??ve TD (NTD) for
its unbalanced coverage in training and test set.
Our TD is constructed as follows. Given ini-
tial features, we use the model trained on the
whole training set with these features to label
the test set and add all words into our TD.
The next step is to balance our TD?s cover-
age of the training and test sets. Since coverage
of the test set cannot reach 100%, the only way
to achieve this goal is by slightly lowering the
dictionary?s coverage on the training set. We
apply n-fold cross-tagging to label the training
set data: Each fold that has 1/n of the train-
ing set is tagged by the model trained on the
other n ? 1 folds with initial features. All the
words identified by this cross-tagging process are
then added to our TD. The difference between
the NTD and our TD is that the NTD extracts
words from the gold training set, but our TD ex-
tracts words from the cross-tagged training set.
Finally, our TD is used to generate dictionary
features to train the final model. Since the TD
constructed from cross-tagging training set and
tagged test set exists more balanced coverage
of the training and test set, we call such a TD
?balanced TD?, shorted as BTD.
6 Our NWI Features
6.1 Transductive Dictionary Features
If a sequence of characters in a sentence matches
a word w in an existing dictionary, it may indi-
cate that the sequence of characters should be
segmented as one word. The traditional way
is to encode this information as binary word
match features. To distinguish the matches with
the same position and length, we propose a new
word match feature that contains frequency in-
formation to replace the original binary word
match feature. Since over 90% of words are four
or fewer characters in length, we only consider
words of one to four characters. In the following
sections, we use D to denote the dictionary.
6.1.1 Word Match Features (WM)
This feature indicates if there is a sequence of
neighboring characters around C0 that match a
word inD. Features of this type are identified by
their positions relative to C0 and their lengths.
Word match features are defined as:
WM(w = C?pos . . . C?pos+len?1)
=
{
1 if w ? D
0 otherwise
where len ? [1..4] is w?s length and pos ? [0..len]
is C0?s zero-based relative position in w (when
pos = len, the previous len characters form a
word found in D). If C0 is ?? and ???
is found in D, WM(C?2 . . . C0) is enabled.
6.1.2 Word Match with Word
Frequency (WMWF)
Given two different words that have the same
position and length, WM features cannot dif-
ferentiate which should have the greater weight.
This could cause problems when two matched
words of same length overlap. (Chen and Bai,
1998) solved this conflict by selecting the word
with higher (frequency ? length). We utilize
this idea to reform the WM features into our
WMWF features:
WMWFq(w = C?pos . . . C?pos+len?1)
=
{
1 if w ? D and log feq(w) = q
0 otherwise
where the word frequency is discretized into
10 bins in a logarithmic scale:log feq(w) =
min(dlog2 w?s frequency + 1e, 10)
thus q[0..10] is the discretized log frequency
of w. In this formulation, matching words with
higher log frequencies are more likely to be the
correct segmentation. Following the above ex-
ample, if the frequency of ??? is 15, then
the feature WMWF4(C?2 . . . C0) is enabled.
6.1.3 Discretization v.s. Zipf?s Law
Since current implementations of CRF models
only allow discrete features, the word frequency
must be discretized. There are two commonly
used discretization methods: equal-width inter-
val and equal-frequency interval, where the lat-
ter is shown to be more suitable for data fol-
lowing highly skewed distribution (Ismail and
Ciesielski, 2003). The word frequency distribu-
tion is the case: Zipf?s law (Zipf, 1949) states
that the word frequency is inversely proportional
to its rank (Adamic and Huberman, 2002):
f(x) ? z??
933
where f(x) is x?s frequency, z is its rank in the
frequency table, and ? is empirically found to be
close to unity. Obviously this distribution is far
from flat uniform. Hence the equal-frequency
binning turns out to be our choice.
Ideally, we would like each bin to have equal
expected number of values rather than following
empirical distribution. Therefore, we attempt
to discretize according to their underlying Zip-
fian distribution.
Adamic & Huberman (2002) shows that Zipf?s
law is equivalent to the power law, which de-
scribes Zipf?s law in a unranked form:
fX(x) ? x?(1+(1/?)),
where X is the random variable denoting the
word frequency and fX(x) is its probability den-
sity function. Approximated by integration, the
expected number of values in the bin [a, b] can
be calculated as
?
a?x?b
x ? Pr [X = x] ?
? b
a
x ? fX(d)dx
?
? b
a
x ? x?(1+(1/?))dx ? lnx|ba = ln(b/a)
(? ? ? 1)
Thus each bin has equal number of values within
it if and only if b/a is a constant, which is in a log
scale. This shows that our strategy to discretize
the WMWF and WMNF features in a log scale
is not only a conventional heuristic but also has
theoretical support.
6.2 Association Metric Features (AM)
In this section, we describe how to formulate
the association metrics as features to avoid the
weakness stated in Section 4.2. Our idea is to
enumerate all possible character segment pairs
before and after the segmentation point and
treat their association metrics as feature values.
Each possible pair corresponds to an individual
feature. For computational feasibility, only pairs
with total length shorter than five characters are
selected. All the enumerated segment pairs are
listed in the following table:
Feature x,y Feature x,y
AM1+1 c?1, c0 AM2+1 c?2c?1, c0
AM1+2 c?1, c0c1 AM2+2 c?2c?1, c0c1
AM1+3 c?1, c0c1c2 AM3+1 c?3c?2c?1, c0
We use Dunning?s method (Dunning, 1993)
because it does not depend on the assumption of
normality and it allows comparisons to be made
between the significance of the occurrences of
both rare and common phenomenon. The like-
lihood ratio test is applied as follows:
LR(x, y) =2? (logl(p1, k1, n1) + logl(p2, k2, n2)
? logl(p, k1, n1)? logl(p, k2, n2))
where logl(P,K,M) = K? lnP +(M ?K)? ln(1?
P ), k1 = freq(x, y); k2 = f(x,?y) = freq(x)?k1;
n1 = freq(y); n2 = N ? n1; p1 = p(x|y) = k1/n1;
p2 = p(x|y) = k2/n2; p = p(x) = (k1 + k2)/N ;
N is the number of words in corpus.
An important property of likelihood ratio is
that ?2LR is asymptotically x21 distributed.
Hence we can directly compute its p-value. We
then discretize the p-value into several bins, each
bin is defined by two significance levels 2?(q+1)
and 2?q. Thus, our AM feature is defined as:
AMq(x, y) =
?
?
?
1 if the p-value of
LR(x, y) ? [2?(q+1), 2?q]
0 otherwise
Since we have a constraint 0 ? q ? 10, thus,
the last interval is [0, 2?10]. We can think that
larger q implies higher tendency of current char-
acter to be labeled as ?I ?.
7 External Dictionary Features
7.1 Word Match with Ngram
Frequency (WMNF)
In addition to internal dictionaries extracted
from the training and test data, external dic-
tionaries can also be used. Unlike with internal
dictionaries, the true frequency of words in ex-
ternal dictionaries cannot be acquired. We must
treat each external dictionary word as an n-gram
and calculate its frequency in the entire unseg-
mented (training plus test) set as follows:
WMNFq(w = C?pos . . . C?pos+len?1)
=
{
1 if w ? D log ngram freq(w) = q
0 otherwise
where the frequencies are discretized into 10 bins
by the same way describing in previous section.
In this formulation, matching n-grams with
higher log frequencies are more likely to repre-
sent correct segmentations.
8 Experiments and Results
8.1 Data and Evaluation Metrics
We use two datasets in SIGHAN Bakeoff 2006:
one Simplified Chinese provided by Univ. of
Pennsylvania (UPUC) and one Traditional Chi-
nese provided by the City Univ. of HK
(CITYU), as shown in Table 1.
Two unlabeled text data used in our exper-
iments. For the CITYU dataset, we use part
of the CIRB40 corpus1 (134M). For the UPUC
dataset, we use the Contemporary Chinese Cor-
pus at PKU2 (73M).
1http://clqa.jpn.org/2006/04/corpus.html
2http://icl.pku.edu.cn/icl_res/
934
UPUC CITYU
F +/- Roov +/- Riv NC NCRR F +/- Roov +/- Riv NC NCRR
cl
os
ed
1 N-grams 93.0 n/a 71.1 n/a 95.7 14094 n/a 96.6 n/a 78.8 n/a 97.3 9642 n/a
2 (1) + AM (int raw) 94.3 +1.3 76.4 +5.3 96.5 11655 +17.3 97.3 +0.7 80.3 +1.5 97.9 7890 +18.2
3 (1) + WM, NTD(1) 93.4 +0.4 74.8 +3.7 95.4 13182 +6.5 97.0 +0.4 81.6 +2.8 97.3 8597 +10.8
4 (1) + WMWF, NTD(1) 93.7 +0.7 75.0 +3.9 95.8 12719 +9.7 97.2 +0.6 82.0 +3.2 97.6 8029 +16.7
5 (1) + WMWF, BTD(1) 94.0 +1.0 73.4 +2.3 96.7 12218 +13.3 97.4 +0.8 79.2 +0.4 98.3 7429 +23.0
6 (1) + WMWF, BTD(2)
+ AM (int raw)
94.5 +1.5 76.6 +5.5 96.7 11173 +20.7 97.5 +0.9 80.3 +1.5 98.2 7377 +23.5
7 Rank 1 in Closed 93.3 n/a 70.7 n/a 96.3 n/a n/a 97.2 n/a 78.7 n/a 98.1 n/a n/a
op
en
8 (1) + AM (ext raw) 94.3 +1.3 75.9 +4.8 96.6 11695 +17.0 97.3 +0.7 81.9 +3.1 97.9 7747 +19.7
9 (1) + WMWF, BTD(8)
+ AM (ext raw)
94.7 +1.7 77.1 +6.0 96.9 10844 +23.1 97.8 +1.2 82.2 +3.4 98.5 6531 +32.3
10 (9) + WMNF 95.0 +2.0 78.7 +7.6 97.1 10326 +26.7 97.9 +1.3 84.0 +5.2 98.5 6117 +36.6
11 Rank 1 in Open 94.4 n/a 76.8 n/a 96.6 n/a n/a 97.7 n/a 84.0 n/a 98.4 n/a n/a
Table 2: Comparison scores for UPUC and CITYU
Source Training
(Wds/Types)
Test
(Wds/Types)
UPUC 509K/37K 155K/17K
CITYU 1.6M/76K 220K/23K
Table 1: An overview of corpus statistics
We use SIGHAN?s evaluation script to score
all segmentation results. This script pro-
vides three basic metrics: Precision (P), Re-
call (R), and F-Measure (F). In addition, it
also provides three detailed metrics: ROOV
stands for the recall rate of the OOV words.
RIV stands for the recall rate of the IV
words, and NC stands for NChanges (inser-
tion+deletion+substitution) (Sproat and Emer-
son, 2003). In addition, we also compare the
NChange reduction rate (NCRR) because the
CWS?s state-of-the art F-measure is over 90%.
Here, the NCRR of any system s is calculated:
NCRR(s) = NChangebaseline ?NChanges
NChangebaseline
8.2 Results
Our system uses the n-gram features described
in Section 3 as our baseline features, denoted
as n-grams. We then sequentially add other fea-
tures and show the results in Table 2. Each con-
figuration is labeled with the features and the
resources used in it. For instance, AM(int raw)
means AM features computed from the inter-
nal raw data, including the unlabeled training
and test set, and WM, NTD(1) stands for WM
features based on the NTD employing config.1?s
feature as its initial features.
Our experiments are conducted in the follow-
ing order: starting from baseline model, we then
gradually add AM features (config.2) and TD
features (config.4 & 5) and combined them as
our final setting (config.6) for the closed task. In
the open task, we sequentially add AM features
(config.8), TD features (config.9), which only ex-
ploit internal and unlabeled data. Finally, the
last setting (config.10) employs external dictio-
naries besides all above features.
Association Metric At first, we compare
the effects after adding AM which is computed
based on the internal raw data (config.2). We
can see that adding AM can significantly im-
prove the performance on both datasets. Also,
the OOV-recall is improved 5.3% and 1.5% on
UPUC and CITYU respectively.
Transductive Dictionary Without lost of
generality, we firstly use the WM features in-
troduced in Section 6.1.1 to represent dictio-
nary features which is denoted as config. 3 in
Tables 3. We can see that the configuration
with WM features outperforms that with N-
grams (config.1). It is worth mentioning that
even though N-grams achieve satisfactory OOV
recall (0.788 and 0.711) in CITYU and UPUC,
config. 3 achieves higher OOV recall.
Frequency Information and BTD To show
the effectiveness of frequency, we compare WM
with WMWF features. In Table 2, we can
see that WMWF features (config.4) outperform
WM features (config.3) on both datasets in
terms of F-Measure and RIV. In addition,
switching the NTD (config.4) with BTD (con-
fig.5) can further improve RIV and F-score while
ROOV slightly decreases. This is not surpris-
ing. In a BTD, most incorrectly segmented
words appear infrequently. Unfortunately, the
new words detected by the baseline model also
have comparatively low frequencies. Therefore,
these words will be assigned into the same sev-
eral bins corresponding to infrequent words as
the incorrectly segmented words and share low
weights with them.
Combined Effects In config.6, we use the
model with N-gram plus AM features as initial
features to construct the BTD. In Table 2, we
can see that the increase of ROOV?s can recover
the loss brought by using BTD and further raise
935
the F-measure to the level of the state-of-the-art
open task performance.
In comparison of the baseline n-gram model,
our approach reduces the errors by an significant
number of 20.7% and 23.5% in the UPUC and
CITYU datasets, respectively. The OOV recall
of our approach increases 5.5% and 1.5% on the
UPUC and CITYU datasets, respectively. As-
tonishingly, in the UPUC dataset, with limited
information provided by training corpus and un-
labeled test data, our system still outperforms
the best SIGHAN open CWS system that are
allowed to use unlimited external resources.
8.2.1 Using External Unlabeled Data
In config.9, we also use the ngrams plus AM as
initial features to generate the BTD, but exter-
nal unlabeled data are used along with internal
data to calculate values of AM features. Com-
paring with config.6, we can see that ROOV,
RIV, and F-score are further improved, espe-
cially ROOV. Notably, this configuration can
reduce NChanges by 2.4% in comparison of the
best closed configuration.
8.2.2 Using External Dictionaries
To demonstrate that our approach can be
expandable by installing external dictionaries,
we add WMNF features based on the external
dictionaries into the config.9, and denote this
to be our config.10. We use the Grammati-
cal Knowledge-Base of Contemporary Chinese
(GKBCC) (Yu et al, 2003) and Chinese Elec-
tronic Dictionary for the UPUC and CITYU
dataset, respectively.
As shown in Table 2, all metrics of config.10
are better than config.9, especially ROOV. This
is because most of the new words do not exist in
external dictionaries; therefore, using external
dictionaries can complement our results.
9 Conclusion
This paper presents how to exploit unlabeled
data to improve both NWI and CWS perfor-
mance. For new high-ST words, since they
can be decomposed into semantically relevant
atomic parts, they could be identified by the n-
gram models. Using the property, we construct
an internal dictionary by using this model to
extract words from both the unlabeled training
and test set to maintain balanced coverage on
them, which makes the weights of the internal
dictionary features more accurate. Also, fre-
quency is initiatively considered in dictionary
features and shows its effectiveness.
For low-ST words, we employ AMs, which
is frequently used in English MWE extraction
to enhance the baseline n-gram model. We
show that this idea effectively extract much
more unknown person, location, and transliter-
ation names which are not found by the baseline
model.
The experiment results demonstrate that
adopting our two strategies generally benefi-
cial to NWI and CWS on both traditional
and simplified Chinese datasets. Our sys-
tem achieves state-of-the-art closed task perfor-
mance on SIGHAN bakeoff 2006 datasets. Un-
der such most stringent constraints defined in
the closed task, our performances are even com-
parable to open task performance. Moreover,
with only external unlabeled data, our system
also achieves state-of-the-art open task perfor-
mance on SIGHAN bakeoff 2006 datasets.
References
L.A. Adamic and B.A. Huberman. 2002. Zipf?s law
and the internet. Glottometrics, 3:143?150.
K. J. Chen and M. H. Bai. 1998. Unknown word
detection for chinese by a corpus-based learning
method. Computational Linguistics and Chinese
Language Processing, 3(1):27?44.
Y. Choueka. 1988. Looking for needles in a haystack
or locating interesting collocation expressions in
large textual databases. In RIAO.
T. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):65?74.
Michael K. Ismail and Vic Ciesielski. 2003. An em-
pirical investigation of the impact of discretization
on common data distributions. In Design and Ap-
plication of Hybrid Intelligent Systems. IOS Press,
Amsterdam, Netherlands.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for
segmenting and labeling sequence data. In ICML-
01, pages 282?289.
Richard Sproat and Thomas Emerson. 2003.
The first international chinese word segmentation
bakeoff. In SIGHAN-03.
Huihsin Tseng and Keh-Jiann Chen. 2002. Design of
chinese morphological analyzer. In SIGHAN-02.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In SIGHAN-03.
G.K. Zipf. 1949. Human Behavior and the Principle
of Least Effort. Addison-Wesley, Cambridge, MA.
936
