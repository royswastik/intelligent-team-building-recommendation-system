A Method for Forming Mutual Beliefs for Communication through
Human-robot Multi-modal Interaction
Naoto Iwahashi
Sony Computer Science Labs.
Tokyo, Japan
iwahashi@csl.sony.co.jp
Abstract
This paper describes a method of multi-
modal language processing that reflects
experiences shared by people and robots.
Through incremental online optimization
in the process of interaction, the user and
the robot form mutual beliefs represented
by a stochastic model. Based on these mu-
tual beliefs, the robot can interpret even
fragmental and ambiguous utterances, and
can act and generate utterances appropri-
ate for a given situation.
1 Introduction
The process of human communication is based on
certain beliefs shared by those who are communi-
cating. Language is one such mutual belief, and it
is used to convey meaning based on its relevance
to other mutual beliefs (Sperber and Wilson, 1995).
These mutual beliefs are formed through interac-
tion with the environment and other people, and the
meaning of utterances is embedded in such shared
experiences.
If those communicating want to logically con-
vince each other that proposition p is a mutual belief,
they must prove that the infinitely nested proposition
?They have information that they have information
that . . . that they have information that p? also holds.
However, in reality, all we can do is assume, based
on a few clues, that our beliefs are identical to those
of other people we are talking to. That is, it can
never be guaranteed that our beliefs are identical to
those of other people.
The processes of utterance generation and un-
derstanding rely on a system of mutual beliefs as-
sumed by each person, and this system changes au-
tonomously and recursively through these processes.
The listener interprets utterances based on their rel-
evance to their system of assumed mutual beliefs.
The listener also receives information for updating
their system of assumed mutual beliefs through this
process. In addition, the speaker can receive simi-
lar information through the response of the listener.
Through utterances, people simultaneously send and
receive information about one another?s system of
assumed mutual beliefs. In this sense, we can say
that a mutual belief system assumed by one per-
son couples with mutual belief systems assumed by
other people they are communicating with.
To enable humans and robots to communicate
with one another in a physical environment the
way people do, spoken-language processing meth-
ods must emphasize mutual understanding, and they
must have a mechanism that would enable the mu-
tual belief systems couple with one another. More-
over, language, perception, and behavior have to
be processed integratively in order for humans and
robots to physically share their environment as the
basis for the formation of common experiences and
in order for linguistic and nonlinguistic beliefs to
combine in the process of human-robot interaction.
Previous language processing methods, which are
characterized by fixed language knowledge, do not
satisfy these requirements because they cannot dy-
namically reflect experiences in the communication
process in a real environment.
I have been working on methods for forming lin-
guistic beliefs, such as beliefs about phonemes, lexi-
con, and grammar, based on common perceptual ex-
Figure 1: Interaction between a user and a robot
periences between people and robots (for further de-
tail, see (Iwahashi, 2001; Iwahashi, 2003)). This pa-
per describes a method that enables robots to learn
a system of mutual beliefs including nonlinguistic
ones through multi-modal language interaction with
people. The learning is based on incremental on-
line optimization, and it uses information from raw
speech and visual observations as well as behavioral
reinforcement, which is integrated in a probabilistic
framework.
Theoretical research (Clark, 1996) and its com-
putational modelling (Traum, 1994) focused on the
formation of mutual beliefs that is a direct target of
communication, and aimed at representing the for-
mation of mutual beliefs as a procedure- and rule-
driven process. In contrast, this study focuses on a
system of mutual beliefs that is used in the process of
utterance generation and understanding in a physical
environment, and aims at representing the formation
of this system by a mathematical model of coupling
systems.
2 Task for Forming Mutual Beliefs
The task for forming mutual beliefs was set up as
follows. A robot was sat at a table so that the robot
and the user sitting at the table could see and move
the objects on the table (Fig. 1) The user and the
robot initially shared certain basic linguistic beliefs,
including a lexicon with a small number of items and
a simple grammar, and the robot could understand
some utterances 1. The user asked the robot to move
an object by making an utterance and a gesture, and
the robot acted in response. If the robot responded
incorrectly, the user slapped the robot?s hand. The
1No function words were included in the lexicon.
Figure 2: A scene during which utterances were
made and understood
robot also asked the user to move an object, and the
user acted in response. Mutual beliefs were formed
incrementally, online, through such interaction.
Figure 2 shows an example of utterance genera-
tion and understanding using mutual beliefs. In the
scene shown in Fig. 2, the object on the left, Kermit,
has just been put on the table.
If the user in the figure wants to ask the robot to
move Kermit onto the box, he may say ?Kermit box
move-onto?. In this situation, if the user assumes
that the robot shares the belief that the object moved
in the previous action is likely to be the next target
for movement and the belief that the box is likely
to be something for the object to be moved onto,
he might just say ?move-onto?. To understand this
fragmental utterance, the robot has to have similar
beliefs. Inversely, when the robot wants to ask the
user to do something, mutual beliefs are used in the
same way.
3 Algorithm for Learning a System of
Mutual Beliefs
3.1 Setting
The robot has an arm with a hand, and a stereo
camera unit. A close-talk microphone is used for
speech input, and the speech is represented by a
time-sequence of Mel-scale cepstrum coefficients.
Visual observation oi of object i is represented
by using such features as color (three-dimensional:
L*a*b* parameters), size (one-dimensional), and
shape (two-dimensional). Trajectory u of the ob-
ject?s motion is represented by a time-sequence of its
positions. A touch sensor is attached to the robot?s
hand.
3.2 Representation of a system of mutual
beliefs
In the algorithm I developed, the system of mutual
beliefs consists of two parts: 1) a decision func-
tion, composed of a set of beliefs with values rep-
resenting the degree of confidence that each belief
is shared by the robot and the user, and 2) a global
confidence function, which represents the degree of
confidence for the decision function. The beliefs I
used are those concerning lexicon, grammar, behav-
ioral context, and motion-object relationship. The
degree of confidence for each belief is represented
by a scalar value. The beliefs are represented by
stochastic models as follows:
Lexicon L
The lexicon is represented by a set of pairs, each
with probability density function (pdf) p(s|ci) of
feature s of a spoken word and a pdf for representing
the image concept of lexical item ci, i = 1, . . . , N .
Two types of image concepts are used. One is a con-
cept for the static observation of an object. Con-
ditional pdf p(o|c) of feature o of an object given
lexical item c is represented by a Gaussian pdf. The
other is a concept for the movement of an object.
The concept is viewed as the process of change in
the relationship between the trajector and the land-
mark. Here, given lexical item c, position ot,p of
trajector object t, and position ol,p of landmark ob-
ject l, conditional pdf p(u|ot,p, ol,p, c) of trajectory u
is represented by a hidden Markov Model (HMM).
Pdf p(s|c) of the features of a spoken word is also
represented by an HMM.
Grammar G
I assume that an utterance can be understood based
on its conceptual structure z, which has three at-
tributes: landmark, trajector, and motion, each of
which contains certain elements of the utterance.
Grammar G is represented by a set of occurrence
probabilities for the orders of these attributes in an
utterance and a statistical bigram model of the lexi-
cal items for each of the three attributes.
Effect of behavioral context B
1
(i, q;H)
Effect of behavioral context represents the belief that
the current utterance refers to object i, given behav-
ioral context q. q includes information on whether
object i was a trajector or a landmark in the previ-
ous action and whether the user?s current gesture is
referring to object i. This belief is represented by a
parameter set H = {hc, hg, hp}, and it takes hc as
its value if object i was involved in the previous ac-
tion, hg if the object is being held, hp if it is being
pointed to, and 0 in all other cases.
Motion-object relationship B
2
(ot,f , ol,f , WM ;R)
Motion-object relationship represents the belief that
in the motion corresponing to lexical item WM , fea-
ture ot,f of object t and feature ol,f of object l are
typical for a trajector and a landmark, respectively.
This belief is represented by a conditional multivari-
ate Gaussian pdf, p(ot,f , ol,f |WM ;R), where R is
its parameter set.
3.3 Decision function
The beliefs described above are organized and as-
signed confidence values to obtain the decision func-
tion used in the process of utterance generation and
understanding. This decision function is written as
?(s, a, O, q, L,G,R,H,?)
= max
l,z
(
?
1
log p(s|z; L,G) [Speech]
+?
2
log p(u|ot,p, ol,p, WM ; L) [Motion]
+?
2
(
log p(ot,f |WT ; L) + log p(ol,f |WL; L)
)
[Object]
+?
3
log p(ot,f , ol,f , |WM ; R)
[Motion-Object Relationship]
+?
4
(
B
1
(t, q; H) +B
1
(l, q; H)
)
)
[Behavioral Context]
where ? = {?
1
, . . . , ?
4
} is a set of confidence values
for beliefs corresponding to the speech, motion, ob-
ject, motion-object relationship, and behavioral con-
text; a denotes the action, and it is represented by a
pair (t, u) of trajector object t and trajectory u of its
movement; O denotes the scene, which includes the
positions and features of all the objects in the scene;
and WT and WL denotes the sequences of the lexi-
cal items in the utterances for the trajector and land-
mark, respectively. Given O, q, L, G, R, H , and
?, the corresponding action, a? = (u?, t?), understood
to be the meaning of utterance s is determined by
maximizing the decision function as
a? = argmax
a
?(s, a, O, q, L,G,R,H,?).
3.4 Global confidence function
Global confidence function f outputs an estimate of
the probability that the robot?s utterance s will be
correctly understood by the user, and it is written as
f(x) =
1
?
arctan
(
x? ?
1
?
2
)
+ 0.5,
where ?
1
and ?
2
are the parameters of this function.
A margin in the value of the output of the decision
function in the process of generating an utterance is
used for input x of this function. Margin d obtained
in the process of generating utterance s that means
action a in scene O under behavioral context q is
defined as
d(s, a, O, q, L,G,R, H,?)
= min
A=a
(
?(s, a, O, q, L, G, R, H,?)
??(s, A, O, q, L, G,R, H,?)
)
.
We can easily see that a large margin increases the
probability of the robot being understood correctly
by the user. If there is a high probability of the
robot?s utterances being understood correctly even
when the margin is small, we can say that the robot?s
beliefs are consistent with those of the user. When
the robot asks for action a in scene O under behav-
ioral context q, the robot generates utterance s? so
as to make the value of the output of f as close as
possible to value of parameter ?, which represents
the taget probability of the robot?s utterance being
understood correctly. This utterance can be repre-
sented as
s? = argmin
s
(
f(d(s, a, O, q, L,G,R,H,?))? ?
)
.
The robot can increase the chance of being under-
stood correctly by using more words. On the other
hand, if the robot can predict correct understanding
with a sufficiently high probability, the robot can
manage with a fragmental utterance using a small
number of words.
3.5 Learning
The decision function and the global confidence
function are learned separately in the utterance un-
derstanding and utterance generation processes, re-
spectively.
The decision function is learned incrementally,
online, through a sequence of episodes each of
which consists of the following steps. 1) Through
an utterance and a gesture, the user asks the robot to
move an object. 2) The robot acts on its understand-
ing of the utterance. 3) If the robot acts correctly, the
process is terminated. Otherwise, the user slaps the
robot?s hand. 4) The robot acts in a different way. 5)
If the robot acts incorrectly, the user slaps the robot?s
hand.
When the robot acts correctly in the first or sec-
ond trial in an episode, the robot associates utterance
s, action a, scene O, and behavioral context q with
each other, and makes these associations a learning
sample. Then the robot adapts the values of parame-
ter set R for the belief about the motion-object rela-
tionship, parameter set H for the belief about the ef-
fect of the behavioral context, and a set of weighting
parameters, ?. R is learned by using the Bayesian
learning method. H and ? are learned based on the
minimum error criterion (Juang and Katagiri, 1992).
Lexicon L and grammar G are given beforehand and
are fixed. When the ith sample (si, ai, Oi, qi) is ob-
tained based on this process of association, Hi and
?i are adapted to minimize the probability of mis-
understanding based on the minimum error criterion
as
i
?
j=i?K
wi?j g(d (sj , aj, Oj, qj, L, G, Ri, Hi,?i))
? min,
where g(x) is ?x if x < 0 and 0 otherwise, and
K and wi?j represent the number of latest samples
used in the learning process and the weights for each
sample, respectively.
Global confidence function f is learned incremen-
tally, online, through a sequence of episodes which
consist of the following steps. 1) The robot gener-
ates an utterance to ask the user to move an object.
2) The user acts according to their understanding
of the robot?s utterance. 3) The robot determines
whether the user?s action is correct or not.
In each episode, the robot generates an utterance
that makes the value of the output of global confi-
dence function f as close to ? as possible. After each
episode, the value of margin d in the utterance gen-
eration process is associated with information about
whether the utterance was understood correctly or
not, and this sample of associations is used for learn-
ing. The learning is done so as to approximate the
probability that an utterance will be understood cor-
rectly by minimizing the weighted sum of squared
errors in the latest episodes. After the ith episode,
parameters ?
1
and ?
2
are adapted as
[?
1,i, ?2,i] ? (1? ?)[?1,i?1, ?2,i?1] + ?[??1,i, ??2,i],
where
(??
1,i, ??2,i)
= arg min
?
1
,?
2
i
?
j=i?K
wi?j(f(dj; ?1, ?2)? ej)
2,
where ei is 1 if the user?s understanding is correct
and 0 if it is not, and ? is the value that determines
the learning speed.
4 Experiments
4.1 Conditions
The lexicon used in the experiments included eleven
items for the static image and six items to describe
the motions. Eleven stuffed toys and four boxes
were used as objects in the interaction between a
user and a robot. In the learning process, the inter-
action was simulated by using software.
4.2 Learning of decision function
Sequence X of quadruplets (si, ai, Oi, qi) consisting
of the user?s utterance si, scene Oi, behavioral con-
text qi, and action ai that the user wanted to ask the
robot to perform (i = 1, . . . , nd), was used for the
interaction. At the beginning of the sequence, the
sentences were relatively complete (e.g., ?green ker-
mit red box move-onto?). Then the length of the sen-
tences was gradually reduced (e.g., ?move-onto?).
R could be estimated with high accuracy during
the episodes in which relatively complete utterances
were understood correctly. H and ? could be effec-
tively estimated based on the estimation of R when
fragmental utterances were given. Figure 3 shows
32 64 128
0
0.2
0.4
0.6
0.8
Episode
?1
?2
?3
C
on
fid
en
ce
 w
ei
gh
ts
1.0
1.2
1.4
1.6
?  h4   c 
Figure 3: Changes in the confidence values
32 64 1280
20
40
60
80
Episode
Er
ro
r 
ra
te
[%
]
w/o learning
with learning
Figure 4: The change in decision error rate
changes in the values of ?
1
, ?
2
, ?
3
, and ?
4
hc. We
can see that each value was adapted according to
the ambiguity of a given sentence. Figure 4 shows
the decision error (misunderstanding) rates obtained
during the course of the interaction, along with the
error rates obtained in the same data, X , by keeping
the values of the parameters of the decision function
fixed to their initial values.
Examples of actions generated as a result of cor-
rect understanding are shown together with the out-
put log probabilities from the weighted beliefs in
Figs. 5 (a) and (b), along with the second and third
action candidates, which led to incorrect actions. We
can see that each nonlinguistic belief was used ap-
propriately in understanding the utterances. Beliefs
(a)
" Move-onto "
previous
correct incorrect
Speech Motion BehavioralContext
Motion-Object 
Relationship
1st
(Correct)
2nd 
(Incorrect)
(b)
" Grover Small Kermit Jump-over "
correct
previous
incorrect
Speech Motion Behavioralcontext
Motion-object
relationshipObject
1st
(Correct)
3rd
(Incorrect)
Figure 5: Examples of fragmental utterances under-
stood correctly by the robot
about the behavioral context were more effective in
Fig. 5 (a), while in Fig. 5 (b), beliefs about the object
concepts were more effective than other nonlinguis-
tic beliefs in leading to the correct understanding.
This learning process is described in greater detail
in (Miyata et al, 2001)
4.3 Learning of the global confidence function
In the experiments with the learning of the global
confidence function, The robot?s utterances were ex-
pressed through text on a display instead of oral
speech, and they included one word describing the
motion and either no words or one to several words
describing the trajector and landmark objects or just
the trajector object.
A sequence of triplets (a, O, q) consisting of
scene O, behavioral context q, and action a that the
robot needed to ask the user to perform, was used for
the interaction. In each episode, the robot generated
an utterance to bring the global confidence function
as close to 0.75 as possible.
The changes in f(d) are shown in Fig. 5 (a),
where three lines are drawn for d
0.5 = f?1(0.5),
d
0.75 = f?1(0.75), and d0.9 = f?1(0.9) in order
to make the shape of f(d) easily recognizable. The
changes in the number of words used to describe the
objects in each utterance are shown in Fig. 5 (b),
along with the changes obtained in the case when
f(d) was not learned, which are shown for com-
parison. The initial values were set at d
0.9 = 161,
d
0.75 = 120, and d0.5 = 100, which means that a
large margin was necessary for an utterance to be
understood correctly. Note that when all the values
are close to 0, the slope in the middle of f is steep,
and the robot makes a decision that a small margin
is enough for its utterances to be understood cor-
rectly. After the learning began, these values rapidly
approached 0, and the number of words decreased.
The slope became temporarily smooth at around the
15th episode. Then, the number of words became
too small, which sometimes lead to misunderstand-
ing. Finally, the slope became steep again at around
the 35th episode.
5 Discussion
The above experiments illustrate the importance of
misunderstanding and clarification, i.e. error and re-
pair, in the formation of mutual beliefs between peo-
ple and machines. In the learning period for utter-
ance understanding by the robot, the values of the
parameters of the decision function changed signif-
icantly when the robot acted incorrectly in the first
trial, and correctly in the second trial. In the learn-
ing period for utterance generation by the robot,
in the experiment in which the target value of the
global confidence function was set to 0.95, which
was larger than 0.75 and closer to 1, the global con-
fidence function was not properly estimated because
almost all utterances were understood correctly (The
results of this experiment are not presented in de-
(a)
-40
-20
0
20
40
60
80
0 10 20 30 40 50 60 70
Episode
M
ar
gi
n 
d 90%75%
50%
(b)
1
2
3
4
0 10 20 30 40 50 60 70
Episode
N
um
be
r o
f w
o
rd
s
w/o learning
with learning
Figure 6: Changes in the global confidence function
(a) and the number of words needed to describe the
objects in each utterance (b)
tail in this paper). These results show that occa-
sional errors enhance the formation of mutual be-
liefs in both the utterance generation and utterance
understanding processes. This implies that in or-
der to obtain information about mutual beliefs, both
the robot and the user must face the risk of not be-
ing understood correctly. The importance of error
and repair to learning in general has been seen as an
exploration-exploitation trade-off in the area of re-
inforcement learning by machines (e.g. (Dayan and
Sejnowski, 1996)).
The experimental results showed that the robot
could learn its system of the beliefs the robot as-
sumed the user had. Because the user came to un-
derstand the robot?s fragmental and ambiguous ut-
terances, the user and the robot must have shared
similar beliefs, and must have been aware of that.
It would be interesting to investigate by experiment
the dynamics of sharing beliefs between a user and
a robot.
6 Related Works
(Winograd, 1972) and (Shapiro et al, 2000) ex-
plored the grounding of the meanings of utterances
in conversation onto the physical world by using
logic, but the researchers did not investigate the pro-
cessing of information from the real physical world.
(Matsui et al, 2000) focused on enabling robots to
work in the real world, and integrated language with
information from robot?s sensors by using pattern
recognition. (Inamura et al, 2000) investigated an
autonomous mobile robot that controlled its actions
and conversations with a user based on a Bayesian
network. The use of Bayesian networks in the in-
terpretation and generation of dialogue was also in-
vestigated by (Lemon et al, 2002). In (Singh et
al., 2000), the learning of dialogue strategies us-
ing reinforcement learning was investigated. Some
of these works looked at beliefs ?held by? the ma-
chines themselves, but none focused on the for-
mation of mutual beliefs between humans and ma-
chines through interaction, based on common expe-
riences.
7 Conclusion
The presented method enables the formation of mu-
tual beliefs between people and robots through in-
teraction in physical environments, and it facilitates
the process of human-machine communication. In
the future, I want to focus on the generalization of
learning of mutual beliefs and the learning of dia-
logue control.
References
H. Clark. 1996. Using Language. Cambridge University
Press.
P. Dayan and T. J. Sejnowski. 1996. Exploration
Bonuses and Dual Control. Machine Learning, 25:5?
22.
T. Inamura, M. Inaba and H. Inoue. 2000. Integration
Model of Learning Mechanism and Dialogue Strategy
based on Stochastic Experience Representation us-
ing Bayesian Network. Proceedings of International
Workshop on Robot and Human Interactive Communi-
cation, 27?29.
N. Iwahashi. 2001. Language Acquisition by Robots.
The Institute of Electronics, Information, and Commu-
nication Engineers Technical Report SP2001-96.
N. Iwahashi. 2003. Language Acquisition by Robots:
Towards New Paradigm of Language Processing.
Journal of Japanese Society for Artificial Intelligence,
18(1):49-58.
B.-H. Juang and S. Katagiri. 1992. Discriminative
Learning for Minimum Error Classification. IEEE
Transactions on Signal Processing, 40(12):3043?
3054.
O. Lemon, P. Parikh and S. Peters. 2002. Probabilistic
Dialogue Management. Proceedings of Third SIGdial
Workshop on Discourse and Dialogue, 125?128.
T. Matsui, H. Asoh, J. Fry, Y. Motomura, F. Asano, T
Kurita and N. Otsu. 1999. Integrated Natural Spo-
ken Dialogue System of Jijo-2 Mobile Robot for Office
Services. Proceedings of 15th National Conference on
Artificial Intelligence.
A. Miyata, N. Iwahashi and A. Kurematsu. 2001. Mutual
belief forming by robots based on the process of utter-
ance comprehension. Technical Report of The Insti-
tute of Electronics, Information, and Communication
Engineers, SP2001-98.
C. S. Shapiro, H. O. Ismail, and J. F. Santore. 2000. Our
Dinner with Cassie. Working Notes for AAAI 2000
Spring Symposium on Natural Dialogues with Prac-
tical Robotic Devices, 57-61.
S. Singh, M. Kearns, D. J. Litman and M. A. Malker.
2000. Empirical Evaluation of a Reinforce Learning
Spoken Dialogue System. Proc. 16th National Con-
ference on Artificial Intelligence, 645?651.
D. Sperber and D. Wilson. 1995. Relevance (2nd Edi-
tion). Blackwell.
D. R. Traum. 1994. A computationaltheory of grounding
in natural language conversation. Unpublished doc-
toral dissertation, University of Rochester.
T. Winograd. 1972. Understanding Natural Language.
Academic Press New York.
