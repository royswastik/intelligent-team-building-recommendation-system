Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 5?8,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Surprising parser actions and reading difficulty
Marisa Ferrara Boston, John Hale
Michigan State University
USA
{mferrara,jthale}@msu.edu
Reinhold Kliegl, Shravan Vasishth
Potsdam University
Germany
{kliegl,vasishth}@uni-potsdam.de
Abstract
An incremental dependency parser?s proba-
bility model is entered as a predictor in a
linear mixed-effects model of German read-
ers? eye-fixation durations. This dependency-
based predictor improves a baseline that takes
into account word length, n-gram probabil-
ity, and Cloze predictability that are typically
applied in models of human reading. This
improvement obtains even when the depen-
dency parser explores a tiny fraction of its
search space, as suggested by narrow-beam
accounts of human sentence processing such
as Garden Path theory.
1 Introduction
A growing body of work in cognitive science char-
acterizes human readers as some kind of probabilis-
tic parser (Jurafsky, 1996; Crocker and Brants, 2000;
Chater and Manning, 2006). This view gains sup-
port when specific aspects of these programs match
up well with measurable properties of humans en-
gaged in sentence comprehension.
One way to connect theory to data in this man-
ner uses a parser?s probability model to work out
the surprisal or log-probability of the next word.
Hale (2001) suggests this quantity as an index
of psycholinguistic difficulty. When the transi-
tion from previous word to current word is low-
probability, from the parser?s perspective, the sur-
prisal is high and the psycholinguistic claim is that
behavioral measures should register increased cog-
nitive difficulty. In other words, rare parser ac-
tions are cognitively costly. This basic notion has
proved remarkably applicable across sentence types
and languages (Park and Brew, 2006; Demberg and
Keller, 2007; Levy, 2008).
The present work uses the time spent looking at
a word during reading as an empirical measure of
sentence processing difficulty. From the theoretical
side, we calculate word-by-word surprisal pre-
dictions from a family of incremental depen-
dency parsers for German based on Nivre (2004);
these parsers differ only in the size k of the beam
used in the search for analyses of longer and longer
sentence-initial substrings. We find that predictions
derived even from very narrow-beamed parsers im-
prove a baseline eye-fixation duration model. The
fact that any member of this parser family derives
a useful predictor shows that at least some syn-
tactic properties are reflected in readers? eye fixa-
tion durations. From a cognitive perspective, the
utility of small k parsers for modeling comprehen-
sion difficulty lends credence to the view that the
human processor is a single-path analyzer (Frazier
and Fodor, 1978).
2 Parsing costs and theories of reading
difficulty
The length of time that a reader?s eyes spend fix-
ated on a particular word in a sentence is known
to be affected by a variety of word-level factors
such as length in characters, n-gram frequency and
empirical predictability (Ehrlich and Rayner, 1981;
Kliegl et al, 2004). This last factor is the one mea-
sured when human readers are asked to guess the
next word given a left-context string.
Any role for parser-derived syntactic factors
5
Figure 1: Dependency structure of a PSC sentence.
would have to go beyond these word-level influ-
ences. Our methodology imposes this requirement
by fitting a kind of regression known as a lin-
ear mixed-effects model to the total reading times
associated with each sentence-medial word in the
Potsdam Sentence Corpus (PSC) (Kliegl et al,
2006). The PSC records the eye-movements of 272
native speakers as they read 144 German sentences.
3 The Parsing Model
The parser?s outputs define a relation on
word pairs (Tesnie`re, 1959; Hays, 1964). The
structural description in Figure 1 is an example
output that depicts this dependency relation using
arcs. The word near the arrowhead is the dependent,
the other word its head (or governor).
These outputs are built up by monotonically
adding to an initially-empty set of dependency re-
lations as analysis proceeds from left to right. To
arrive at Figure 1 the Nivre parser passes through
a number of intermediate states that aggregate four
data structures, detailed below in Table 1.
? A stack of already-parsed unreduced words.
? An ordered input list of words.
h A function from dependent words to heads.
d A function from dependent words to arc types.
Table 1: Parser configuration.
The stack ? holds words that could eventually be
connected by new arcs, while ? lists unparsed words.
h and d are where the current set of dependency arcs
reside. There are only four possible transitions
from configuration to configuration. Left-Arc
and Right-Arc transitions create dependency re-
Error type Amount
Noun attachment 4.2%
Prepositional Phrase attachment 3.0%
Conjunction 1.9%
Adverb ambiguity 1.8%
Other 1.1%
Total error 12.1%
Table 2: Parser errors by category.
lations between the top elements in ? and ? , while
Shift and Reduce transitions manipulate ?.
When more than one transition is applicable, the
parser decides between them by consulting a proba-
bility model derived from the Negra and Tiger news-
paper corpora (Skut et al, 1997; Ko?nig and Lezius,
2003). This model is called Stack3 because it con-
siders only the parts-of-speech of the top three el-
ements of ? along with the top element of ? . On
the PSC this model achieves 87.9% precision and
79.5% recall for unlabeled dependencies. Most of
the attachments it gets wrong (Table 2) represent al-
ternative readings that would require semantic guid-
ance to rule out.
To compare ?serial? human sentence processing
models against ?parallel? models, our implemen-
tation does beam search in the space of Nivre-
configurations. The number of configurations main-
tained at any point is a changeable parameter k.
3.1 Surprisal
In Figure 1 the thermometer beneath the Ger-
man preposition ?in? graphically indicates a
high surprisal prediction derived from the depen-
dency parser. Greater cognitive effort, reflected in
reading time, should be observed on ?in? as com-
6
pared to ?alte.? The difficulty prediction at ?in? ul-
timately follows from the frequency of verbs tak-
ing prepositional complements that follow nominal
complements in the training data. Equation 1 ex-
presses the general theory: the surprisal of a word,
on a language model, is the logarithm of the pre-
fix probability eliminated in the transition from one
word to the next.
surprisal(n) = log2
(?n?1
?n
)
(1)
The prefix-probability ?n of an initial substring is
the total probability of all grammatical analyses that
derive w = w1...wn as a left-prefix (Equation 2).
?n =
?
d?D(G,wv)
Prob(d) (2)
In a complete parser, every member of D is in cor-
respondence with a state transition sequence. In the
beam-search approximation, only the top k config-
urations are retained from prefix to prefix, which
amounts to choosing a subset of D.
4 Study
The study addresses whether surprisal is a signif-
icant predictor of reading difficulty and, if it is,
whether the beam-size parameter k affects the use-
fulness of the calculated surprisal values in account-
ing for reading difficulty.
Using total reading time as a dependent measure,
we fit a baseline linear mixed-effects model (Equa-
tion 3) that takes into account word-level predictors
log frequency (lf), log bigram frequency (bi), word
length (len), and human predictability given the left
context (pr).
log (TRT ) = (3)
5.4? 0.02lf ? 0.01bi ? 0.59len?1 ? 0.02pr
All of the word-level predictors were statistically
significant at the ? level 0.05.
Beyond this baseline, we fitted ten other lin-
ear mixed-effects models. To the inventory of word-
level predictors, each of the ten regressions uniquely
added the surprisal predictions calculated from a
parser that retains at most k=1...9,100 analyses at
each prefix. We evaluated the change in relative
quality of fit due to surprisal with the Deviance In-
formation Criterion (DIC) discussed in Spiegelhal-
ter et al (2002). Whereas the more commonly ap-
plied Akaike Information Criterion (1973) requires
the number of estimated parameters to be deter-
mined exactly, the DIC facilitates the evaluation of
mixed-effects models by relaxing this requirement.
When comparing two models, if one of the models
has a lower DIC value, this means that the model fit
has improved.
4.1 Results and Discussion
Table 3 shows that the linear mixed-effects model of
German reading difficulty improves when surprisal
values from the dependency parser are used as pre-
dictors in addition to the word-level predictors. The
coefficients on the baseline predictors remained un-
changed (Equation 3) when any of the parser-based
predictors was added.
Table 3 also suggests the returns to be had in
accounting for reading time are greatest when the
beam is limited to a handful of parses. Indeed,
a parser that handles a few analyses at a time
(k=1,2,3) is just as valuable as one that spends far
greater memory resources (k=100). This observa-
tion is consistent with Brants and Crocker?s (2000)
observation that accuracy can be maintained even
when restricted to 1% of the memory required for
exhaustive parsing. The role of small k depen-
dency parsers in determining the quality of statisti-
cal fit challenges the assumption that cognitive func-
tions are global optima. Perhaps human parsing is
boundedly rational in the sense of the bound im-
posed by Stack3 (Simon, 1955).
5 Conclusion
This study demonstrates that surprisal calculated
with a dependency parser is a significant predictor of
reading times, an empirical measure of cognitive dif-
ficulty. Surprisal is a significant predictor even when
examined alongside the more commonly used pre-
dictors, word length, predictability, and n-gram fre-
quency. The viability of parsers that consider just a
small number of analyses at each increment is con-
sistent with conceptions of the human comprehender
that incorporate that restriction.
7
Model Coefficient Std. Error t value DIC
Baseline - - - 144511.1
k=1 0.033691 0.002285 15 143964.9
k=2 0.038573 0.002510 15 143946.2
k=3 0.037320 0.002693 14 143990.4
k=4 0.041035 0.002853 14 143975.7
k=5 0.048692 0.002953 16 143910.9
k=6 0.046580 0.003063 15 143951.6
k=7 0.045008 0.003118 14 143974.4
k=8 0.042039 0.003165 13 144006.4
k=9 0.040657 0.003225 13 144023.9
k=100 0.029467 0.003878 8 144125.4
Table 3: Coefficients and standard errors from the multiple regressions using different versions of surprisal (baseline
predictors? coefficients are not shown for space reasons). t values > 2 are statistically significant at ? = 0.05. The
table also shows DIC values for the baseline model (Equation 3) and the models with baseline predictors plus surprisal.
References
H. Akaike. 1973. Information theory and an extension
of the maximum likelihood principle. In B. N. Petrov
and F. Caski, editors, 2nd International Symposium on
Information Theory, pages 267?281, Budapest, Hun-
gary.
T. Brants and M. Crocker. 2000. Probabilistic pars-
ing and psychological plausibility. In Proceedings of
COLING 2000: The 18th International Conference on
Computational Linguistics.
N. Chater and C. Manning. 2006. Probabilistic mod-
els of language processing and acquisition. Trends in
Cognitive Sciences, 10:287?291.
M. W. Crocker and T. Brants. 2000. Wide-coverage
probabilistic sentence processing. Journal of Psy-
cholinguistic Research, 29(6):647?669.
V. Demberg and F. Keller. 2007. Data from eye-tracking
corpora as evidence for theories of syntactic process-
ing complexity. Manuscript, University of Edinburgh.
S. F. Ehrlich and K. Rayner. 1981. Contextual effects
on word perception and eye movements during read-
ing. Journal of Verbal Learning and Verbal Behavior,
20:641?655.
L. Frazier and J. D. Fodor. 1978. The sausage machine: a
new two-stage parsing model. Cognition, 6:291?325.
J. Hale. 2001. A probabilistic Earley parser as a psy-
cholinguistic model. In Proceedings of 2nd NAACL,
pages 1?8. Carnegie Mellon University.
D.G. Hays. 1964. Dependency theory: A formalism and
some observations. Language, 40:511?525.
D. Jurafsky. 1996. A probabilistic model of lexical and
syntactic access and disambiguation. Cognitive Sci-
ence, 20:137?194.
R. Kliegl, E. Grabner, M. Rolfs, and R. Engbert. 2004.
Length, frequency, and predictability effects of words
on eye movements in reading. European Journal of
Cognitive Psychology, 16:262?284.
R. Kliegl, A. Nuthmann, and R. Engbert. 2006. Track-
ing the mind during reading: The influence of past,
present, and future words on fixation durations. Jour-
nal of Experimental Psychology: General, 135:12?35.
E. Ko?nig and W. Lezius. 2003. The TIGER language -
a description language for syntax graphs, Formal def-
inition. Technical report, IMS, Universita?t Stuttgart,
Germany.
R. Levy. 2008. Expectation-based syntactic comprehen-
sion. Cognition, 106(3):1126?1177.
J. Nivre. 2004. Incrementality in deterministic depen-
dency parsing. In Incremental Parsing: Bringing En-
gineering and Cognition Together, Barcelona, Spain.
Association for Computational Linguistics.
J. Park and C. Brew. 2006. A finite-state model of hu-
man sentence processing. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the ACL, pages
49?56, Sydney, Australia.
H. Simon. 1955. A behavioral model of rational choice.
The Quarterly Journal of Economics, 69(1):99?118.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997.
An annotation scheme for free word order languages.
In Proceedings of the Fifth Conference on Applied
Natural Language Processing ANLP-97, Washington,
DC.
D. J. Spiegelhalter, N. G. Best, B. P. Carlin, and
A. van der Linde. 2002. Bayesian measures of model
complexity and fit (with discussion). Journal of the
Royal Statistical Society, 64(B):583?639.
L. Tesnie`re. 1959. Ele?ments de syntaxe structurale. Edi-
tions Klincksiek, Paris.
8
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1810?1815,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Model of Individual Differences in Gaze Control During Reading
Niels Landwehr
1
and Sebastian Arzt
1
and Tobias Scheffer
1
and Reinhold Kliegl
2
1
Department of Computer Science, Universit?at Potsdam
August-Bebel-Stra?e 89, 14482 Potsdam, Germany
{landwehr, sarzt, scheffer}@cs.uni-potsdam.de
2
Department of Psychology, Universit?at Potsdam
Karl-Liebknecht-Stra?e 24/25, 14476 Potsdam OT/Golm
kliegl@uni-potsdam.de
Abstract
We develop a statistical model of saccadic
eye movements during reading of isolated
sentences. The model is focused on rep-
resenting individual differences between
readers and supports the inference of the
most likely reader for a novel set of eye
movement patterns. We empirically study
the model for biometric reader identifica-
tion using eye-tracking data collected from
20 individuals and observe that the model
distinguishes between 20 readers with an
accuracy of up to 98%.
1 Introduction
During skilled reading, the eyes of a reader do
not move smoothly over a text. Instead, read-
ing proceeds by alternating between brief fixations
on individual words and short ballistic movements
called saccades that move the point of fixation to a
new location. Evidence in psychological research
indicates that patterns of fixations and saccades are
driven partly by low-level visual cues (e.g., word
length), and partly by linguistic and cognitive pro-
cessing of the text (Kliegl et al., 2006; Rayner,
1998).
Eye-movement patterns are frequently studied
in cognitive psychology as they provide a rich
and detailed record of the visual, oculomotor, and
linguistic processes involved in reading. Com-
putational models of eye-movement control de-
veloped in psychology, such as SWIFT (Engbert
et al., 2005; Schad and Engbert, 2012) or E-Z
Reader (Reichle et al., 1998; Reichle et al., 2012),
simulate the generation of eye movements based
on physiological and psychological constraints re-
lated to attention, visual perception, and the ocu-
lomotor system. Recently, the problem of mod-
eling eye movements has also been approached
from a machine-learning perspective. Matties and
S?gaard (2013) and Hara et al. (2012) study con-
ditional random field models to predict which
words in a text are fixated by a reader. Nilsson
and Nivre (2009) use a transition-based log-linear
model to predict a sequence of fixations for a text.
A central observation made by these studies, as
well as by earlier psychological work (Erdmann
and Dodge, 1898; Huey, 1908), is that eye move-
ment patterns vary significantly between individ-
uals. As one example of the strength of indi-
vidual differences in reading eye movements, we
cite Dixon (1951) who compared the reading pro-
cesses of university professors and graduate stu-
dents of physics, education, and history on read-
ing material in their own and the two other fields.
He did not find strong effects of his experimen-
tal variables (i.e., field of research, expertise in re-
search) but ?if there is one thing that this study
has shown, it is that individual differences in read-
ing skill existed among the subjects of all depart-
ments. Fast and slow readers were found in every
department, and the overlapping of distributions
from passage to passage was enormous? (p. 173).
Even though it is possible to predict across a large
base of readers with some accuracy whether spe-
cific words will be fixated (Matties and S?gaard,
2013), a strong variation between readers in at-
tributes such as the fraction of skipped words and
total number of saccades has been observed (Hara
et al., 2012).
Some recent work has studied eye movement
patterns as a biometric feature. Most studies are
based on an artificial visual stimulus, such as a
moving (Kasprowski and Ober, 2004; Komogort-
sev et al., 2010; Rigas et al., 2012b; Zhang and
Juhola, 2012) or fixed (Bednarik et al., 2005) dot
on a computer screen, or a specific image stimu-
lus (Rigas et al., 2012a). In the most common use
1810
case of biometric user identification, a decision on
whether access should be granted has to be made
after performing some test that requires the user?s
attention and therefore cannot take a long time. By
contrast, our work is motivated by a less intrusive
scenario in which the user is monitored continu-
ously during access to, for instance, a device or
document. When the accumulated evidence sup-
ports the conclusion that the user is not authorized,
access can be terminated or additional credentials
requested. In this use case, identification has to
be based on saccadic eye movements that occur
while a user is reading an arbitrary text?as op-
posed to movements that occur in response to a
fixed, controlled visual stimulus. Holland and Ko-
mogortsev (2012) study reader recognition based
on a set of aggregate features derived from eye
movements, irrespective of the text being read;
their work will serve as reference in our empiri-
cal study.
The paper is organized as follows. Section 2 de-
tails the problem setting. Section 3 introduces the
statistical model and discusses parameter estima-
tion and inference. Section 5 presents empirical
results, Section 6 concludes.
2 Problem Setting and Notation
Let R denote a set of readers, and
X = {X
1
, ...,X
n
} a set of texts. Each r ? R
generates a set of eye movement patterns
S
(r)
= {S
(r)
1
, . . . ,S
(r)
n
} on the set of texts X , by
S
(r)
i
? p(S|X
i
, r)
where p(S|X, r) is a reader-specific distribution
over eye movement patterns given a text X. A pat-
tern is a sequence S = ((s
1
, d
1
), . . . , (s
T
, d
T
)) of
fixations, consisting of a fixation position s
t
(posi-
tion in text that was fixated) and duration d
t
? R
(length of fixation in milliseconds). In our experi-
ments, individual sentences are presented in a sin-
gle line on a screen, thus we only model a hori-
zontal gaze position s
t
? R.
At test time, we observe novel eye move-
ment patterns
?
S = {
?
S
1
, . . . ,
?
S
m
} on a novel set
of texts
?
X = {
?
X
1
, ...,
?
X
m
} generated by an un-
known reader r ? R. The goal is to infer
r
?
= arg max
r?R
p(r|
?
S,
?
X ). (1)
?20 ?10 0 10 2000.1
0.20.3
0.4
Amplitude
Density
Refixation
 
 Empirical DistributionGamma Fit
?20 ?10 0 10 200
0.1
0.2
Amplitude
Density
Next Word Move
 
 Empirical DistributionGamma Fit
?20 ?10 0 10 2000.05
0.10.15
Amplitude
Density
Forward Skip
 
 Empirical DistributionGamma Fit
?20 ?10 0 10 2000.05
0.10.15
0.2
Amplitude
Density
Regression
 
 Empirical DistributionGamma Fit
Figure 1: Empirical distributions over amplitudes
and Gamma fits for different saccade types.
3 Statistical Model of Eye Movements
We solve Problem 1 by estimating reader-specific
models p(S|X;?
r
) for r ? R, and solving for
p(r|
?
S,
?
X ; ?) ?
(
m
?
i=1
p(
?
S
i
|
?
X
i
;?
r
)
)
p(r) (2)
where all ?
r
are aggregated into a global model ?.
Assuming a uniform prior p(r) over readers, this
reduces to predicting the reader r that maximizes
the likelihood p(
?
S|
?
X ;?
r
) =
?
m
i=1
p(
?
S
i
|
?
X
i
;?
r
).
We formulate a model p(S|X;?) of a sequence
S of fixations given a text X. The model defines
a dynamic probabilistic process that successively
generates the fixation positions s
t
and durations d
t
in S, reflecting how a reader generates a sequence
of saccades in response to a text stimulus X. The
joint distribution over all fixation positions and du-
rations is assumed to factorize as
p(s
1
, . . . , s
T
, d
1
, . . . , d
T
|X;?)
= p(s
1
, d
1
|X;?)
T?1
?
t=1
p(s
t+1
, d
t+1
|s
t
,X;?).
The conditional p(s
t+1
, d
t+1
|s
t
,X;?) models the
generation of the next fixation position and du-
ration given the current fixation position s
t
. In
the psychological literature, four different sac-
cadic types are distinguished: a reader can refix-
ate the current word (refixation), fixate the next
word in the text (next word movement), move the
1811
fixation to a word after the next word (forward
skip), or regress to fixate a word occurring ear-
lier than the currently fixated word in the text (re-
gression) (Heister et al., 2012). We observe em-
pirically, that modeling the amplitude as a mixture
of four Gamma distributions matches the empiri-
cal distribution of amplitudes in our data well?
see Figure 1. Modeling the amplitudes as a sin-
gle distribution, instead of a mixture of four dis-
tributions, results in a substantially lower out-
of-sampling likelihood of the model. Therefore,
at each time t, the model first draws a saccadic
type u
t+1
? {1, 2, 3, 4} from a multinomial dis-
tribution and then generates a saccade amplitude
a
t+1
and fixation duration d
t+1
from type-specific
Gamma distributions. Formally, the generative
process is given by
u
t+1
? p(u|pi) = Mult(u|pi) (3)
a
t+1
? p(a|u
t+1
, s
t
,X;?) (4)
d
t+1
? p(d|u
t+1
;?). (5)
Afterwards the model updates the fixation position
according to s
t+1
= s
t
+ a
t+1
. The joint param-
eter vector ? concatenates parameters of the indi-
vidual distributions in Equations 3 to 5. Figure 2
shows a slice in the dynamical model.
Given the current fixation position s
t
, the text
X, and the chosen saccadic type u
t+1
, the am-
plitude is constrained to fall within a specific
interval?for instance, within the characters of the
currently fixated word for refixations. Therefore,
we model the distribution over the saccade ampli-
tude given the saccadic type (Equation 4) as trun-
cated Gamma distributions, given by
1
G(x|[l, r];?, ?) =
{
G(x|?,?)?
r
l
G(x?|?,?)dx?
if x ? [l, r]
0 otherwise
where G(x|?, ?) =
1
?
?
?(?)
x
??1
e
?x
?
is the Gamma distribution with shape parameter
? and scale parameter ?, and ? is the Gamma
function. For x ? G(x|?, ?) it holds that
G(x|[l, r];?, ?) is the conditional distribution of
x given that x ? [l, r]. The distribution over a sac-
1
The definition is straightforwardly generalized to open
truncation intervals.
tu 1tu ?ta 1ta ?td 1td ?t 1t ?Xts 1ts ?
Figure 2: Graphical model notation of a slice in
the dynamic model. Parameters are omitted to
avoid notational clutter.
cade amplitude given the saccade type is given by
p(a|u
t+1
= 1, s
t
,X;?) =
{
?G(a|[0, r];?
1
, ?
1
) if a > 0
(1? ?)G(?a|[0, l]; ??
1
,
?
?
1
) otherwise
(6)
where the parameter ? reflects the probability for
a forward saccade within a refixation, and
p(a|u
t+1
= 2, s
t
,X;?) = G(a|[l
+
, r
+
];?
2
, ?
2
)
p(a|u
t+1
= 3, s
t
,X;?) = G(a|(r
+
,?);?
3
, ?
3
)
p(a|u
t+1
= 4, s
t
,X;?) = G(?a|(?l,?);?
4
, ?
4
).
(7)
Here, the truncation intervals reflect the con-
straints on the amplitude a
t+1
given u
t+1
, s
t
and
X. Let w
l
(w
r
) denote the position of the left-
most (right-most) character of the currently fix-
ated word, and let w
+
l
, w
+
r
denote these positions
for the word following the currently fixated word.
Then l = w
l
? s
t
, r = w
r
? s
t
, l
+
= w
+
l
? s
t
,
and r
+
= w
+
r
? s
t
. The parameter vector ?
contains the parameters ?, ??
1
,
?
?
1
and ?
i
, ?
i
for
i ? {2, 3, 4}.
The distribution over fixation durations given
saccade type is modeled by a Gamma distribution
p(d|u
t+1
;?) = G(d|?
u
t+1
, ?
u
t+1
)
with type-specific parameters ?
u
, ?
u
for
u ? {1, 2, 3, 4} that are concatenated into a
parameter vector ?.
It remains to specify the distribution over initial
fixation positions and durations p(s
1
, d
1
|X;?),
which is given by additional Gamma distributions
s
1
? G(d|?
0
, ?
0
) d
1
? G(d|?
0
, ?
0
)
1812
0 10 20 30 40 50 60 700
0.2
0.4
0.6
0.8
1
number of training sentences n
ident
ificat
ion a
ccura
cy
Accuracy Over Training Sentences n (m=72)
 
 
full modelsaccade type + amplitudesaccade type onlyHolland & K. (weighted)Holland & K. (unweighted)random guessing
0 10 20 30 40 50 60 700
0.2
0.4
0.6
0.8
1
number of test sentences m
ident
ificat
ion a
ccura
cy
Accuracy Over Test Sentences m (n=72)
 
 
full modelsaccade type + amplitudesaccade type onlyHolland & K. (weighted)Holland & K. (unweighted)random guessing
Figure 3: Reader identification accuracy as a function of the number of training sentences (left) and test
sentences (right) read for different model variants. Error bars indicate the standard error.
where the parameters ?
0
, ?
0
, ?
0
, ?
0
are aggregated
into the joint parameter vector ?.
4 Parameter Estimation and Inference
Given a set S
(r)
of eye movement observations for
reader r ? R on texts X , the MAP estimate of the
parameters is
?
r
= arg max
?
p(?|S
(r)
,X )
= arg max
?
(
n
?
i=1
p(S
(r)
i
|X
i
;?)
)
p(?). (8)
A Dirichlet distribution (add-one smoothing) is a
natural, conjugate prior for the multinomial distri-
bution; we use uninformative priors for all other
distributions. The structure of the model implies
that the posterior can be maximized by fitting the
parameters pi to the observed saccadic types un-
der the Dirichlet prior, and independently fitting
the distributions p(a
t
|u
t
,X, s
t
;?) and p(d
t
|u
t
;?)
by maximum likelihood to the saccade amplitudes
and durations observed for each saccade type.
The resulting maximum likelihood problems are
slightly non-standard in that we have to fit Gamma
distributions that are truncated differently for each
data point, depending on the textual content at
the position where the saccade occurs (see Equa-
tions 6 and 7). We solve the resulting optimization
problems using a Quasi-Newton method. To avoid
overfitting, we use a backoff-smoothing technique
for p(a
t
|u
t
,X, s
t
;?) and p(d
t
|u
t
;?): we replace
reader-specific parameter estimates by estimates
obtained from the corresponding data of all read-
ers if the number of data points from which the dis-
tributions are estimated falls below a cutoff value.
The cutoff value is tuned by cross-validation on
the training data.
At test time, we have to infer likelihoods
p(S
i
|X;?
r
) (Equation 2). This is done by evalu-
ating the multinomial and (truncated) Gamma dis-
tributions in the model for the corresponding ob-
servations and model parameters.
5 Empirical Study
We empirically study the proposed model and sev-
eral baselines using eye-movement records of 20
individuals (Heister et al., 2012). For each indi-
vidual, eye movements are recorded while read-
ing each of the 144 sentences in the Potsdam Sen-
tence Corpus (Kliegl et al., 2006). The data set
contains fixation positions and durations that have
been obtained from raw eye movement data by
appropriate preprocessing. Eye movements were
recorded with an EyeLink II system with a 500-
Hz sampling rate (SR Research, Osgoode, On-
tario, Canada). All recordings and calibration
were binocular. We randomly sample disjoint
sets of n training sentences and m test sentences
from the set of 144 sentences. Models are esti-
mated on the eye movement records of individu-
als on the training sentences (Equation 8). The
eye-movement records of one individual on all test
sentences constitute a test example; the model in-
fers the most likely individual to have generated
these test observations (Equation 2). Identifica-
tion accuracy is the fraction of times an individ-
ual is correctly inferred; random guessing yields
an accuracy of 0.05. Results are averaged over 20
training and test sets.
We study the model introduced in Section 3
1813
train
ing s
enten
ces n
test sentences m
Accuracy Over Training and Test Sentences
 
 
0 20 40 60
0
10
20
30
40
50
60
70
accuracy0
0.2
0.4
0.6
0.8
1
0 5 10 15 200.5
0.6
0.7
0.8
0.9
1
number of individuals
ident
ificat
ion a
ccura
cy
Accuracy Over Number of Individuals
 
 
full modelsaccade type + amplitudesaccade type onlyHolland & K. (weighted)Holland & K. (unweighted)
Figure 4: Identification accuracy as a function of the number of training and test sentences read for
full model (left). Identification accuracy as a function of the number of individuals that have to be
distinguished for different model variants (right). Error bars indicate the standard error.
(full model), a model variant in which the variable
d
t+1
and corresponding distribution is removed
(saccade type + amplitude), and a simple model
that only fits a multinomial distribution to saccade
types (saccade type only). Additionally, we com-
pare against the feature-based reader identification
approach by Holland & Komogortsev (2012). Six
of the 14 features used by Holland & Komogort-
sev depend on saccade velocities and vertical fix-
ation positions. As this information was not avail-
able in the preprocessed data set that we used, we
implemented the remaining features. There is ex-
tensive empirical evidence that saccade velocity
scales with saccade amplitude. Specifically, the
relationship between logarithmic peak saccade ve-
locity and logarithmic saccade amplitude is lin-
ear over a wide range of amplitudes and veloci-
ties; this is known as the main sequence relation-
ship (Bahill et al., 1975). Therefore, we do not ex-
pect that adding saccade velocities would dramat-
ically affect performance of this baseline. Holland
& Komogortsev employ a weighted combination
of features; we report results for the method with
and without feature weighting.
Figure 3 shows identification accuracy as a
function of the number n of training sentences
used to estimate model parameters (left) and as
a function of the number m of test sentences on
which inference of the most likely reader is based
(right, cf. Equation 2). The full model achieves
up to 98.25% accuracy, significantly outperform-
ing the Holland & Komogortsev (2012) baseline
(91.25%, without feature weighting) and simpler
model variants. All methods perform much better
than random guessing. Figure 4 (left) shows iden-
tification accuracy as a function of both training
size n and test size m for the full model.
We finally study how identification accuracy
changes with the number of individuals that have
to be distinguished. To this end, we perform the
same study as above, but with randomly sampled
subsets of the overall set of 20 individuals. In these
experiments, we average over 50 random train-test
splits. Figure 4 (right) shows identification ac-
curacy as a function of the number of individu-
als. We observe that identification accuracy drops
with the number of individuals for all methods;
our model consistently outperforms the baselines.
6 Conclusions
We have developed a model of individual differ-
ences in eye movements during reading, and stud-
ied its application in a biometric task. At test time,
individuals are identified based on eye movements
on novel text. Our approach thus provides poten-
tially unobtrusive biometric identification without
requiring users to react to a specific stimulus. Em-
pirical results show clear advantages over an exist-
ing approach for reader identification.
Acknowledgments
We would like to thank Christoph Sawade for in-
sightful discussions and help with the eye move-
ment data. We gratefully acknowledge support
from the German Research Foundation (DFG),
grant LA 3270/1-1.
1814
References
A. Terry Bahill, Michael R. Clark, and Lawrence Stark.
1975. The main sequence: a tool for studying hu-
man eye movements. Mathematical Biosciences,
24:191?204.
Roman Bednarik, Tomi Kinnunen, Andrei Mihaila, and
Pasi Fr?anti. 2005. Eye-movements as a biometric.
In Proceedings of the 14th Scandinavian Conference
on Image Analysis.
W. Robert Dixon. 1951. Studies in the psychology of
reading. In W. S. Morse, P. A. Ballantine, and W. R.
Dixon, editors, Univ. of Michigan Monographs in
Education No. 4. Univ. of Michigan Press.
Ralf Engbert, Antje Nuthmann, Eike M. Richter, and
Reinhold Kliegl. 2005. SWIFT: A dynamical
model of saccade generation during reading. Psy-
chological Review, 112(4):777?813.
Bruno Erdmann and Raymond Dodge. 1898. Psy-
chologische Untersuchungen ?uber das Lesen. Halle:
Max Niemeyer.
Tadayoshi Hara, Daichi Mochihashi, Yoshino Kano,
and Akiko Aizawa. 2012. Predicting word fixations
in text with a CRF model for capturing general read-
ing strategies among readers. In Proceedings of the
First Workshop on Eye-Tracking and Natural Lan-
guage Processing.
Julian Heister, Kay-Michael W?urzner, and Reinhold
Kliegl. 2012. Analysing large datasets of eye move-
ments during reading. In James S. Adelman, editor,
Visual word recognition. Vol. 2: Meaning and con-
text, individuals and development, pages 102?130.
Corey Holland and Oleg V. Komogortsev. 2012. Bio-
metric identification via eye movement scanpaths in
reading. In Proceedings of the 2011 International
Joint Conference on Biometrics.
Edmund B. Huey. 1908. The psychology and peda-
gogy of reading. Cambridge, Mass.: MIT Press.
Pawel Kasprowski and Jozef Ober. 2004. Eye move-
ments in biometrics. In Proceedings of the 2004 In-
ternational Biometric Authentication Workshop.
Reinhold Kliegl, Antje Nuthmann, and Ralf Engbert.
2006. Tracking the mind during reading: The in-
fluence of past, present, and future words on fix-
ation durations. Journal of Experimental Psychol-
ogy: General, 135(1):12?35.
Oleg V. Komogortsev, Sampath Jayarathna, Cecilia R.
Aragon, and Mechehoul Mahmoud. 2010. Biomet-
ric identification via an oculomotor plant mathemat-
ical model. In Proceedings of the 2010 Symposium
on Eye-Tracking Research & Applications.
Franz Matties and Anders S?gaard. 2013. With blink-
ers on: robust prediction of eye movements across
readers. In Proceedings of the 2013 Conference on
Empirical Natural Language Processing.
Mattias Nilsson and Joakim Nivre. 2009. Learning
where to look: Modeling eye movements in reading.
In Proceedings of the 13th Conference on Computa-
tional Natural Language Learning.
Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological Bulletin, 124(3):372?422.
Erik D. Reichle, Tessa Warren, and Kerry McConnell.
2012. Using e-z reader to model the effects of
higher-level language processing on eye movements
during reading. Psychonomic Bulletin & Review,
16(1):1?21.
Ioannis Rigas, George Economou, and Spiros Fotopou-
los. 2012a. Biometric identification based on
the eye movements and graph matching techniques.
Pattern Recognition Letters, 33(6).
Ioannis Rigas, George Economou, and Spiros Fotopou-
los. 2012b. Human eye movements as a trait for bio-
metrical identification. In Proceedings of the IEEE
5th International Conference on Biometrics: The-
ory, Applications and Systems.
Daniel Schad and Ralf Engbert. 2012. The zoom lens
of attention: Simulating shuffled versus normal text
reading using the swift model. Visual Cognition,
20(4-5):391?421.
Youming Zhang and Martti Juhola. 2012. On biomet-
ric verification of a user by means of eye movement
data mining. In Proceedings of the 2nd Interna-
tional Conference on Advances in Information Min-
ing and Management.
1815
