Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 937?946,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
CoCQA: Co-Training Over Questions and Answers 
with an Application to Predicting Question Subjectivity Orientation 
Baoli Li 
Emory University 
csblli@gmail.com 
Yandong Liu 
Emory University 
yliu49@emory.edu 
Eugene Agichtein 
Emory University 
eugene@mathcs.emory.edu
 
 
Abstract 
An increasingly popular method for 
finding information online is via the 
Community Question Answering 
(CQA) portals such as Yahoo! An-
swers, Naver, and Baidu Knows. 
Searching the CQA archives, and rank-
ing, filtering, and evaluating the sub-
mitted answers requires intelligent 
processing of the questions and an-
swers posed by the users. One impor-
tant task is automatically detecting the 
question?s subjectivity orientation: 
namely, whether a user is searching for 
subjective or objective information. 
Unfortunately, real user questions are 
often vague, ill-posed, poorly stated. 
Furthermore, there has been little la-
beled training data available for real 
user questions. To address these prob-
lems, we present CoCQA, a co-training 
system that exploits the association be-
tween the questions and contributed 
answers for question analysis tasks. 
The co-training approach allows 
CoCQA to use the effectively unlim-
ited amounts of unlabeled data readily 
available in CQA archives. In this pa-
per we study the effectiveness of 
CoCQA for the question subjectivity 
classification task by experimenting 
over thousands of real users? questions.
1 Introduction 
Automatic question answering (QA) has been 
one of the long-standing goals of natural lan-
guage processing, information retrieval, and 
artificial intelligence research. For a natural 
language question we would like to respond 
with a specific, accurate, and complete an-
swer that addresses the question. Although 
much progress has been made, answering 
complex, opinion, and even many factual 
questions automatically is still beyond the 
current state-of-the-art.  At the same time, the 
rise of popularity in social media and collabo-
rative content creation services provides a 
promising alternative to web search or com-
pletely automated QA. The explicit support 
for social interactions between participants, 
such as posting comments, rating content, and 
responding to questions and comments makes 
this medium particularly amenable to Ques-
tion Answering. Some very successful exam-
ples of Community Question Answering 
(CQA) sites are Yahoo! Answers 1  and 
Naver2, and Baidu Knows3. Yahoo! Answers 
alone has already amassed hundreds of mil-
lions of answers posted by millions of par-
ticipants on thousands of topics.  
The questions posted to such CQA portals 
are typically complex, subjective, and rely on 
human interpretation to understand the corre-
sponding information need. At the same time, 
the questions are also usually ill-phrased, 
vague, and often subjective in nature. Hence, 
analysis of the questions (and of the corre-
sponding user intent) in this setting is a par-
ticularly difficult task. At the same time, 
CQA content incorporates the relationships 
between questions and the corresponding an-
swers. Because of the various incentives pro-
vided by the CQA sites, answers posted by 
users tend to be, at least to some degree, re-
sponsive to the question. This observation 
suggests investigating whether the relation-
                                                 
1 http://answers.yahoo.com 
2 http://www.naver.com 
3 http://www.baidu.com 
937
ship between questions and answers can be 
exploited to improve automated analysis of the 
CQA content and the user intent behind the 
questions posted.  
     Figure 1: Example question (Yahoo! Answers) 
To this end, we exploit the ideas of co-
training, a general semi-supervised learning 
approach naturally applicable to cases of com-
plementary views on a domain, for example, 
web page links and content (Blum and 
Mitchell, 1998). In our setting, we focus on the 
complimentary views for a question, namely 
the text of the question and the text of the as-
sociated answers.  
As a concrete case-study of our approach 
we focus on one particularly important aspect 
of intent detection: the subjectivity orientation. 
We attempt to predict whether a question 
posted in a CQA site is subjective or objective. 
Objective questions are expected to be an-
swered with reliable or authoritative informa-
tion, typically published online and possibly 
referenced as part of the answer, whereas sub-
jective questions seek answers containing pri-
vate states, e.g. personal opinions, judgment, 
experiences. If we could automatically predict 
the orientation of a question, we would be able 
to better rank or filter the answers, improve 
search over the archives, and more accurately 
identify similar questions. For example, if a 
question is objective, we could try to find a 
few highly relevant articles as references, 
whereas if a question is subjective, useful an-
swers are not expected to be found in authori-
tative sources and tend to rank low with cur-
rent question answering and CQA search tech-
niques. Finally, learning how to identify ques-
tion orientation is a crucial component of in-
ferring user intent, a long-standing problem in 
web information access settings.  
In particular, we focus on the following re-
search questions: 
? Can we utilize the inherent structure of the 
CQA interactions and use the unlimited 
amounts of unlabeled data to improve classi-
fication performance, and/or reduce the 
amount of manual labeling required?  
? Can we automatically predict question sub-
jectivity in Community Question Answering 
? and which features are useful for this task 
in the real CQA setting? 
The rest of the paper is structured as fol-
lows. We first overview the community ques-
tion answering setting, and state the question 
orientation classification problem, which we 
use as the motivating application for our sys-
tem, more precisely. We then introduce our 
CoCQA system for semi-supervised classifi-
cation of questions and answers in CQA com-
munities (Section 3). We report the results of 
our experiments over thousands of real user 
questions in Section 4, showing the effective-
ness of our approach. Finally, we review re-
lated work in Section 5, and discuss our con-
clusions and future work in Section 6.
2 Question Orientation in CQA 
We first briefly describe the essential features 
of question answering communities such as 
Yahoo! Answers or Naver. Then, we formally 
state the problem addressed in this paper, and 
the features used for this setting. 
938
2.1 Community Question Answering  
Online social media content and associated 
services comprise one of the fastest growing 
segments on the Web. The explicit support for 
social interactions between participants, such 
as posting comments, rating content, and re-
sponding to questions and comments makes 
the social media unique. Question answering 
has been particularly amenable to social media 
by directly connecting information seekers 
with the community members willing to share 
the information. Yahoo! Answers, with mil-
lions of users and hundreds of millions of an-
swers for millions of questions is a very suc-
cessful implementation of CQA. 
For example, consider two example user-
contributed questions, objective and subjective 
respectively:  
Q1: What?s the difference between 
chemotherapy and radiation treat-
ments? 
Q2: Has anyone got one of those 
home blood pressure monitors? and 
if so what make is it and do you 
think they are worth getting? 
Figure 1 shows an example of community 
interactions in Yahoo! Answers around the 
question Q2 above. A user posted the question 
in the Health category of the site, and was able 
to obtain 10 responses from other users. Even-
tually, the asker chooses the best answer. Fail-
ing that, as shown in the example, the best an-
swer can also be chosen according to the votes 
from other users. Many of the interactions de-
pend on the perceived goals of the asker: if the 
participants interpret the question as subjec-
tive, they will tend to share their experiences 
and opinions, and if they interpret the question 
as objective, they may still share their experi-
ences but may also provide more factual in-
formation. 
2.2 Problem Definition 
We now state our problem of question orienta-
tion more precisely. We consider question ori-
entation from the perspective of user goals: 
authors of objective questions request authori-
tative, objective information (e.g., published 
literature or expert opinion), whereas authors 
of subjective questions seek opinions or judg-
ments of other users in the community.  We 
state our problem as follows. 
 
Question Subjectivity Problem: Given a 
question Q in a question answering com-
munity, predict whether Q has objective 
or subjective orientation, based on ques-
tion and answer text as well as the user 
and community feedback. 
3 CoCQA: A Co-Training Frame-
work over Questions and Answers 
In the CQA setting we could easily obtain 
thousands or millions of unlabeled examples 
from the online CQA archives. On the other 
hand, it is difficult to create a labeled dataset 
with a reasonable size, which could be used 
to train a perfect classifier to analyze ques-
tions from different domains and sub-
domains. Therefore, semi-supervised learning 
(Chapelle et al, 2006) is a natural approach 
for this setting. 
Intuitively, we can consider the text of the 
question itself or answers to it. In other 
words, we have multiple (at least two) natural 
views of the data, which satisfies the condi-
tions of the co-training approach (Blum and 
Mitchell, 1998). In co-training, two separate 
classifiers are trained on two sets of features, 
respectively. By automatically labeling the 
unlabeled examples, these two classifiers it-
eratively ?teach? each other by giving their 
partners a newly labeled data that they can 
predict with high confidence. Based on the 
original co-training algorithm in (Blum and 
Mitchell, 1998) and other implementations, 
we develop our algorithm CoCQA shown in 
Figure 2. 
At Steps 1 and 2, the K examples are com-
ing from different feature spaces, and each 
category (for example, Subjective and Objec-
tive) has top Kj most confident examples cho-
sen, where Kj corresponds to the distribution 
of class in the current set of labeled examples 
L. CoCQA will terminate when the incre-
ments of both classifiers are less than a speci-
fied threshold X or the maximum number of 
iterations are exceeded. Following the co-
training approach, we include the most confi-
dently predicted examples as additional ?la-
beled? data. The SVM output margin value 
was used to estimate confidence; alternative 
939
methods (including reliability of this confi-
dence prediction) could further improve per-
formance, and we will explore these issues in 
future work. Finally, the next question is how 
to estimate classification performance with 
training data. For each pass, we randomly split 
the original training data into N folds (N=10 in 
our experiments), and keep one part for valida-
tion and the rest, augmented with the newly 
added examples, as the expanded training set. 
After CoCQA terminates, we obtain two 
classifiers. When a new example arrives, we 
will classify it with these two classifiers based 
on both of the feature sets, and combine the 
predictions of these two classifiers. We ex-
plored two strategies to make the final deci-
sion based on the confidence values given by 
two classifiers: 
? Choose the class with higher confidence 
? Multiply the confidence values, and 
choose the class that has the highest 
product. 
We found the second heuristic to be more 
effective than the first in our experiments. As 
the base classifier we use SVM in the current 
implementation, but other classifiers could be 
incorporated as well. 
4 Experimental Evaluation  
We experiment with supervised and semi-
supervised methods on a relatively large data 
set from Yahoo! Answers. 
4.1 Datasets 
To our knowledge, there is no standard data-
set of real questions and answers posted by 
online users, labeled for subjectivity orienta-
tion. Hence, we had to create a dataset our-
selves. To create our dataset, we downloaded 
more than 30,000 resolved questions from 
each of the following top-level categories of 
Yahoo! Answers: Arts, Education, Health, 
Science, and Sports. We randomly chose 200 
questions from each category to create a raw 
dataset with 1,000 questions total. Then, we 
labeled the dataset with annotators from the 
Amazon?s Mechanical Turk service4.  
For annotation, each question was judged 
by 5 Mechanical Turk workers who passed a 
qualification test of 10 questions (labeled by 
ourselves) with at least 9 of them correctly 
marked. The qualification test was required to 
ensure that the raters were sufficiently com-
petent to make reasonable judgments. We 
grouped the tasks into 25 question batches, 
where the whole batch was submitted as the 
Mechanical Turk?s Human Intelligence Task 
(HIT). The batching of questions was done to 
easily detect the ?random? ratings produced 
by irresponsible workers. That is, each 
worker rated a batch of 25 questions.  
While precise definition of subjectivity is 
elusive, we decided to take the practical per-
spective, namely the "majority" interpreta-
tion. The annotators were instructed to guess 
orientation according to how the question 
would be answered by most people. We did 
not deal with multi-part questions: if any part 
of question was subjective, the whole ques-
tion was labeled as subjective. The gold stan-
dard was thus derived with the majority strat-
egy, followed by manual inspection as a ?san-
ity check?. At this stage we removed 22 ques-
tions with undeterminable meaning, including 
gems such as ?Upward Soccer 
                                                 
4 http://www.mturk.com 
Figure 2: Algorithm CoCQA: A co-training algo-
rithm for exploiting redundant feature sets in 
community question answering. 
Input: 
? FQ and FA are Question and Answer feature views 
? CQ and CA are classifiers trained on FQ and FA  respec-
tively 
? L is a set of labeled training examples 
? U is a set of unlabeled examples 
? K: Number of unlabeled examples to choose on  
each iteration 
? X:  the threshold for  increment 
? R:  the maximal number of iterations 
Algorithm CoCQA 
1. Train CQ ,0 on L: FQ , and record resulting   ACCQ,0 
2. Train CA ,0 on L: FA , and record resulting  ACCA ,0 
3. for j=1 to R do: 
        Use CQ,j-1 to predict labels for U and choose 
               top K items with highest confidence ? EQ, , j-1 
        Use CA,j-1 to predict labels for U and  choose  
                top K items with highest confidence ? EA, , j-1 
        Move examples EQ, , j-1 U EA, , j-1 ? L 
        Train CQ,j on L: FQ and record training  ACCQ,j 
        Train CA,j on L: FA and record training  ACCA,j 
             if Max(?ACCQ,j, ? ACCA,j) < X break 
  
4.     return final classifiers CQ,j ? CQ and CA,j ? CA
940
Shorts??5 and ?1+1=?fdgdgdfg??6. Fi-
nally, we create a labeled dataset consisting of 
978 resolved questions, available online7.  
 
 
Num. of 
SUB. Q 
Num. of 
OBJ. Q 
Total 
Num. 
Annotator
agreement
Arts 137 (70%) 58 (30%) 195 0.841 
Education 127 (64%) 70 (36%) 197 0.716 
Health 125 (64%) 69 (36%) 194 0.833 
Science 103 (52%) 94 (48%) 197 0.618 
Sports 154 (79%) 41 (21%) 195 0.877 
Total 646 (66%) 332 (34%) 978 0.777 
Table 1: Labeled dataset statistics. 
 
Table 1 reports the statistics of the annotated 
dataset. The overall inter-annotator percentage 
agreement between Mechanical Turk workers 
and final annotation is 0.777, indicating that 
the task is difficult, but feasible for humans to 
annotate manually.  
The statistics of our labeled sample show 
that the vast majority of the questions in all 
categories except for Science are subjective in 
nature. The relatively high ratio of subjective 
questions in the Science category is surprising. 
However, we find that users often post polem-
ics and statements instead of questions, using 
CQA as a forum to share their opinions on 
controversial topics. Overall, we were struck 
by the expressed need in Subjective informa-
tion, even for categories such as Health and 
Education, where objective information would 
intuitively seem more desirable.  
4.2 Features Used in Experiments 
For the subjectivied experiments to follow, 
we attempt to capture the linguistic 
characteristics identified in previous work 
(Section 5) in a lightweight and robust manner, 
due to the informal and noisy nature of CQA. 
In particular, we use the following feature 
classes, computed separately over question and 
answer content: 
? Character 3-grams  
? Words 
? Word with Character 3-grams 
? Word n-grams (n<=3, i.e. Wi, WiWi+1,  
WiWi+1Wi+2) 
                                                 
5http://answers.yahoo.com/question/?qid=20060829074901AA
DBRJ4  
6 http://answers.yahoo.com/question/?qid=1006012003651  
7 Available at http://ir.mathcs.emory.edu/datasets/. 
? Word and POS n-gram (n<=3, i.e. Wi, 
WiWi+1, Wi POSi+1, POSiWi+1 , 
POSiPOSi+1, etc.).  
We use the character 3-grams features to 
overcome spelling errors and problems of ill-
formatted or ungrammatical questions, and 
the POS information to capture common pat-
terns across domains, as words, especially the 
content words, are quite diverse in different 
topical domains. For word and character 3-
gram features, we consider two different ver-
sions: case-sensitive and case-insensitive. 
Case-insensitive features are assumed to be 
helpful for mitigating negative effects of ill-
formatted text. 
Moreover, we experimented with three 
term weighting schemes: Binary, TF, and 
TF*IDF. Term frequency (TF) exhibited bet-
ter performance in our development experi-
ments, so we use this weighting scheme for 
all the experiments in Section 4. Regarding 
features: both words and structure of the text 
(e.g., word order) can be used to infer subjec-
tivity. Therefore, the features we employ, 
such as words and word n-grams, are ex-
pected to be useful as a (coarse) proxy to 
grammatical and phrase features. Unlike tra-
ditional work on news-like text, the text of 
CQA and has poor spelling, grammar, and 
heavily uses non-standard abbreviations, 
hence our decision to use character n-grams.  
4.3 Experimental Setting 
Metrics: Since the prediction  on both sub-
jective questions and objective questions is 
equally important, we use the macro-
averaged F1 measure as the evaluation met-
ric. This is computed as the macro average of 
F1 measures computed for the Subjective and 
Objective classes individually. The F1 meas-
ure for either class is computed 
as
RecallPrecision 
Recall Precision 2
 
+
?? . 
 
Methods compared: We compare our ap-
proach with both the base supervised learning, 
as well as GE, a state-of-the-art semi-
supervised method:  
? Supervised: we use the LibSVM im-
plementation (Chang and Lin, 2001) 
with linear kernel.   
941
? GE: This is a state-of-the-art semi-
supervised learning algorithm, General-
ized Expectation (GE), introduced in 
(McCallum et al, 2007) that incorporates 
model expectations into the objective 
functions for parameter estimation. 
? CoCQA: Our method (Section 3).  
 
For semi-supervised learning experiments, 
we selected a random subset of 2,000 unla-
beled questions for each of the topical catego-
ries, for the total of 10,000 unlabeled questions. 
4.4 Experimental Results 
First we report the performance of our Super-
vised baseline system with a variety of fea-
tures, reporting the average results of 5-fold 
cross validation. Then we investigate the per-
formance to our new CoCQA framework under 
a variety of settings. 
4.4.1 Supervised Learning 
Table 2 reports the classification perform-
ance for varying units of representation (e.g., 
question text vs. answer text) and the varying 
feature sets. We used case-insensitive features 
and TF (term frequency within the text unit) as 
feature weights, as these two settings achieved 
the best results in our development experi-
ments. The rows show performance consider-
ing only the question text (question), the best 
answer (best_ans), text of all answers to a 
question (all_ans), the text of the question and 
the best answer (q_bestans), and the text of 
the question with all answers (q_allans), re-
spectively.  In particular, using the words in 
the question alone achieves F1 of 0.717, com-
pared to using words in the question and the 
best answers text (F1 of 0.695). For compari-
son, a na?ve baseline that always guesses the 
majority class (Subjective) obtains F1 of 0.398. 
With character 3-gram, our system achieves 
performance comparable with words as fea-
tures, but combining them together does not 
improve performance. We observe a slight 
gain with more complicated features, e.g. word 
n-gram, or word and POS n-grams, but the 
gain is not significant, and hence not worth the 
increased complexity of the feature generation. 
Finally, combining question text with answer 
text does not improve performance.  
Interestingly, the best answer itself is not as 
effective as the question for subjectivity 
analysis, nor is using all of the answers sub-
mitted. One possible reason is that approxi-
mately 40% of the best answers were chosen 
by the community and not the asker herself, 
are hence not necessarily representative of the 
asker intent.  
 
Feature
set
 
Unit 
Char 
3-
gram 
Word 
Word+ 
Char 
3-gram 
Word 
n-gram 
(n<=3) 
Word 
POS 
n-gram
(n<=3) 
question 0.700 0.717 0.694 0.716 0.720 
best_ans 0.587 0.597 0.578 0.580 0.565 
all_ans 0.603 0.628 0.607 0.648 0.630 
q_bestans 0.681 0.695 0.662 0.687 0.712 
q_allans 0.679 0.677 0.676 0.708 0.689 
Na?ve (majority class) baseline:  0.398 
Table 2. Performance of predicting question 
orientation on the mixed dataset with varying 
feature sets (Supervised). 
 
Table 3 reports the supervised subjectivity 
classification performance for each question 
category with word features. The overall clas-
sification results are significantly lower com-
pared to training and testing on the mixture of 
the questions drawn from all categories, 
likely caused by the small amount of labeled 
training data for each category. Another pos-
sibility is that the subjectivity clues are not 
topical and hence are not category dependent, 
with the possible exception of the questions 
in the Health domain.  
 
Category Arts Edu. Health Science Sports
F1 0.448 0.572 0.711 0.647 0.441 
Table 3. Experiment results on sub-categories 
with supervised SVM (q_bestans features).  
 
As words are simple and effective features 
in this experiment, we will use them in the 
subsequent experiments. Furthermore, the 
feature set using the words in the question 
with best answer together (q_bestans) exhibit 
higher performance than question with all 
answers (q_allans). Thus, we will only con-
sider questions and best answers in the fol-
lowing experiments with GE and CoCQA. 
4.4.2 Semi-Supervised Learning 
We now focus on investigating the effec-
tiveness of CoCQA, our co-training-based 
framework for community question answer-
ing analysis. Table 4 summarizes the main 
942
results of this section. The values for CoCQA 
are derived with the parameter settings: K=100, 
X=0.001. These optimal settings are chosen 
after comprehensive experiments with differ-
ent combinations, described later in this sec-
tion. GE does not exhibit a significant im-
provement over Supervised. In contrast, 
CoCQA performs significantly better than the 
purely supervised method, with F1 of 0.745 
compared to the F1 of 0.717 for Supervised. 
While it may seem surprising that a semi-
supervised method outperforms a supervised 
one, note that we use all of the available la-
beled data as provided to the Supervised 
method, as well as a large amount of unlabeled 
data, that is ultimately responsible for the per-
formance improvement. 
  
Features 
Method 
Question Question+ Best Answer 
Supervised 0.717 0.695 
GE 0.712 (-0.7%) 0.717 (+3.2%) 
CoCQA 0.731 (+1.9%) 0.745 (+7.2%) 
Table 4. Performance of CoCQA, GE, and Su-
pervised with the same feature and data settings.  
 
As an added advantage, CoCQA approach is 
also practical. In a realistic application, we 
have two different situations: offline and 
online. With online processing, we may not 
have best answers available to predict ques-
tion?s orientation, whereas we can employ in-
formation from best answers in offline setting. 
Co-training is a solution that is applicable to 
both situations. With CoCQA, we have two 
classifiers using the question text and the best 
answer text, respectively. We can use both of 
them to obtain better results in the offline set-
ting, while in online setting, we can use the 
text of the question alone. In contrast, GE may 
not have this flexibility.  
We now analyze the performance of 
CoCQA under a variety of settings to derive 
optimal parameters and to better understand 
the performance. Figure 3 reports the perform-
ance of CoCQA with varying the K parameter 
from 20 to 200. In this experiment, we fix X to 
be 0.001. The combination of question and 
best answer is superior to that of question and 
all answers. When K is 100, the system obtains 
the best result, 0.745.  
Figure 4 reports the number of co-training 
iterations needed to converge to optimal per-
formance. After 13 iterations (and 2500 unla-
beled examples added), CoCQA achieves op-
timal performance, and eventually terminates 
after an additional 3 iterations. While a vali-
dation set should have been used for CoCQA 
parameter tuning, Figures 3 and 4 indicate 
that CoCQA is not sensitive to the specific 
parameter settings. In particular, we observe 
that any K is greater than 100, and for any 
number of iterations R is greater than 10, 
CoCQA exhibits in effectively equivalent per-
formance. 
 
0.64
0.65
0.66
0.67
0.68
0.69
0.7
0.71
0.72
0.73
0.74
0.75
0.76
20 40 60 80 100 120 140 160 180 200K: # labeled examples added on each 
co-training iteration
F
1
CoCQA(Question and Best Answer)
Supervised Q_bestans
CoCQA(Question and All Answers)
Supervised Q_allans
 
Figure 3: Performance of CoCQA for varying 
the K (number of examples added on each it-
eration of co-training). 
 
Figure 5 reports the performance of 
CoCQA for varying the number of labeled 
examples from 50 to 400 (that is, up to 50% 
of the available labeled training data). Note 
that for this comparison we use the same fea-
ture sets  (words in question and best answer 
text), but using only the (varying) fractions of 
the manually labeled data. Surprisingly, 
CoCQA exhibits comparable performance of 
F1=0.685 with only 200 labeled examples are 
used, compared to the F1=0.695 for Super-
vised with all 800 labeled training examples 
on this feature set. In other words, CoCQA is 
able to achieve comparable performance to 
supervised SVM with only one quarter of the 
labeled training data. 
 
943
0.71
0.72
0.73
0.74
0.75
161377776666
# co-training iterations
F
1
0
500
1000
1500
2000
2500
3000
3500
T
ot
al
 #
 U
n
la
b
el
ed
 A
d
d
ed
CoCQA (Question + Best Answer)
Supervised
Total # Unlabeled
 
Figure 4: Performance and the total number of 
unlabeled examples added for varying number 
of co-training iterations (K=100, using q_bestans 
features) 
 
 
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
0.68
0.7
0.72
50 100 150 200 250 300 350 400
# of labeled data used
F1
CoCQA (Question + Best Answer)
Supervised Q_Best Ans
 
Figure 5: Performance of CoCQA with varying 
number of labeled examples used, compared to 
Supervised method, on same features. 
5 Related Work 
Question analysis, especially question classifi-
cation, has been long studied in the question 
answering research community. However, 
most of the previous research primarily con-
sidered factual questions, with the notable ex-
ception of the most recent TREC opinion QA 
track. Furthermore, the questions were specifi-
cally designed for benchmark evaluation. A 
related thread of research considered deep 
analysis of the questions (and corresponding 
sentences) by manually classifying questions 
along several orientation dimensions, notably 
(Stoyanov et al, 2005).  In contrast, our work 
focuses on analyzing real user questions 
posted in a question answering community. 
These questions are often complex or subjec-
tive, and are typically difficult to answer 
automatically as the question author probably 
was not able to find satisfactory answers with 
quick web search. 
Automatic complex question answering has 
been an active area of research, ranging from 
simple modification to factoid QA techniques 
(e.g., Soricut and Brill, 2003) to knowledge 
intensive approaches for specific domains 
(e.g., Harabagiu et al 2001, Fushman and Lin 
2007). However, the technology does not yet 
exist to automatically answer open-domain 
complex and subjective questions. While 
there has been some recent research (e.g., 
Agichtein et al 2008, Bian et al 2008) on 
retrieving high quality answers from CQA 
archives, the subjectivity orientation of the 
questions has not been considered as a feature 
for ranking.  
A related corresponding problem is com-
plex QA evaluation. Recent efforts at auto-
matic evaluation show that even for well-
defined, objective, complex questions, 
evaluation is extremely labor-intensive and 
introduces many challenges (Lin and 
Fushman 2006, Lin and Zhang 2007). As part 
of our contribution we showed that it is feasi-
ble to use the Amazon Mechanical Turk ser-
vice for evaluation by combining large degree 
of annotator redundancy (5 annotators per 
question) with more sparse but higher-quality 
expert annotation. 
The problem of automatic subjective ques-
tion answering has recently started to be ad-
dressed in the question answering commu-
nity, most recently as the first opinion QA 
track in (Dang et al, 2007). Unlike the con-
trolled TREC opinion track (introduced in 
2007), many of the questions in Yahoo! An-
swers community are inherently subjective, 
complex, ill-formed, or all of the above. To 
our knowledge, this paper is the first large-
scale study of subjective/objective orientation 
of information needs, and certainly the first in 
the CQA environment. 
A closely related research thread is subjec-
tivity analysis at document and sentence 
level. For example, reference (Yu, H., and 
Hatzivassiloglou, V. 2003; Somasundaran et 
944
al. 2007) attempted to classify sentences into 
those reporting facts or opinions. Also related 
is research on sentiment analysis (e.g., Pang et 
al., 2004) where the goal is to classify a sen-
tence or text fragment as being overall positive 
or negative. More generally, (Wiebe et al 
2004) and subsequent work focused on the 
analysis of subjective language in narrative 
text, primarily news. Our problem is quite dif-
ferent in the sense that we are trying to iden-
tify the orientation of a question. Nevertheless, 
our baseline method is similar to the methods 
and features used for sentiment analysis, and 
one of our contributions is evaluating the use-
fulness of the established features and tech-
niques to the new CQA setting. 
In order to predict question orientation, we 
build on co-training, one of the known semi-
supervised learning techniques. Many models 
and techniques have been proposed for classi-
fication, including support vector machines, 
decision tree based techniques, boosting-based 
techniques, and many others. We use LIBSVM 
(Chang and Lin, 2001) as a robust implemen-
tation of SVM algorithms. 
In summary, while we draw on many tech-
niques in question answering, natural language 
processing, and text classification, our work 
differs from previous research in that a) de-
velop a novel co-training based algorithm for 
question and answer classification; b) we ad-
dress a relatively new problem of automatic 
question subjectivity prediction; c) demon-
strate the effectiveness of our techniques in the 
new CQA setting and d) explore the character-
istics unique to CQA ? while showing good 
results for a quite difficult task. 
6 Conclusions 
We presented CoCQA, a co-training frame-
work for modeling the textual interactions in 
question answer communities. Unlike previous 
work, we have focused on real user questions 
(often noisy, ungrammatical, and vague) sub-
mitted in Yahoo! Answers, a popular commu-
nity question answering portal. We demon-
strated CoCQA for one particularly important 
task of automatically identifying question sub-
jectivity orientation, showing that CoCQA is 
able to exploit the structure of questions and 
corresponding answers. Despite the inherent 
difficulties of subjectivity analysis for real user 
questions, we have shown that by applying 
CoCQA to this task we can significantly im-
prove prediction performance, and substan-
tially reduce the size of the required training 
data, while outperforming a general state-of-
the-art semi-supervised algorithm that does 
not take advantage of the CQA characteris-
tics.  
In the future we plan to explore more so-
phisticated features such semantic concepts 
and relationships (e.g., derived from WordNet 
or Wikipedia), and richer syntactic and lin-
guistic information. We also plan to explore 
related variants of semi-supervised learning 
such as co-boosting methods to further im-
prove classification performance. We will 
also investigate other applications of our co-
training framework to tasks such as sentiment 
analysis in community question answering 
and similar social media content. 
Acknowledgments 
This research was partially supported by the 
Emory University Research Committee 
(URC) grant, and by the Emory College Seed 
grant. We thank the Yahoo! Answers team for 
providing access to the Answers API, and 
anonymous reviewers for their excellent sug-
gestions. 
References 
Agichtein, E., Castillo, C., Donato, D., Gionis, A., and 
Mishne, G. 2008. Finding High-Quality Content in 
Social Media with an Application to Community-
Based Question Answering. WSDM2008  
Bian, J., Liu, Y., Agichtein, E., and H. Zha. 2008, to 
appear. Finding the Right Facts in the Crowd: Fac-
toid Question Answering over Social Media, Pro-
ceedings of the Inter-national World Wide Web Con-
ference (WWW), 2008  
Blum, A., and Mitchell, T. 1998. Combining Labeled 
and Unlabeled Data with Co-Training. Proc. of the 
Annual Conference on Computational Learning 
Theory.  
Chang, C. C. and Lin, C. J. 2001. LIBSVM : a library 
for support vector machines. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm.  
Chapelle, O., Scholkopf, B., and Zien, A. 2006. Semi-
supervised Learning. The MIT Press, Cambridge, 
Mas-sachusetts.  
Dang, H. T., Kelly, D., and Lin, J. 2007. Overview of 
the TREC 2007 Question Answering track. In Pro-
ceedings of TREC-2007.  
945
Demner-Fushman, D. and Lin, J. 2007. Answering clini-
cal questions with knowledge-based and statistical 
techniques. Computational Linguistics, 33(1):63?103.  
Harabagiu, S., Moldovan, D., Pasca, M., Surdeanu, M. , 
Mihalcea, R., Girju, R., Rusa, V., Lacatusu, F., 
Morarescu, P., and Bunescu, R. 2001. Answering 
Complex, List and Context Questions with LCC's 
Question-Answering Server. In Proc. of TREC 2001.  
Lin, J. and Demner-Fushman, D. 2006. Methods for 
automatically evaluating answers to complex ques-
tions. In-formation Retrieval, 9(5):565?587  
Lin, J. and Zhang, P. 2007. Deconstructing nuggets: the 
stability and reliability of complex question answering 
evaluation. In Proceedings of the 30th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 327?334.  
Mann, G., and McCallum, A. 2007. Simple, Robust, 
Scalable Semi-supervised Learning via Expectation 
Regularization. Proceedings of ICML 2007.  
Pang, B., and Lee, L. 2004. A Sentimental Education: 
Sen-timent Analysis Using Subjective Summarization 
Based on Minimum Cuts. In Proc. of ACL.  
Prager, J. 2006. Open-Domain Question-Answering. 
Foundations and Trends in Information Retrieval.  
Sindhwani, V., Keerthi, S. 2006. Large Scale Semi-
supervised Linear SVMs. Proceedings of SIGIR 2006.  
Somasundaran, S., Wilson, T., Wiebe, J. and Stoyanov, 
V. 2007. QA with Attitude: Exploiting Opinion Type 
Analysis for Improving Question Answering in On-
line Discussions and the News. In proceedings of In-
ternational Conference on Weblogs and Social Media 
(ICWSM-2007).  
Soricut, R. and Brill, E. 2004. Automatic question an-
swering: Beyond the factoid. Proceedings of HLT-
NAACL.  
Stoyanov, V., Cardie, C., and Wiebe, J. 2005. Multi-
Perspective question answering using the OpQA cor-
pus. In Proceedings of EMNLP.  
Tri, N. T., Le, N. M., and Shimazu, A. 2006. Using 
Semi-supervised Learning for Question Classification. 
In Proceedings of ICCPOL-2006.  
Wiebe, J., Wilson, T., Bruce R., Bell M., and Martin M. 
2004. Learning subjective language. Computational 
Linguistics, 30 (3).  
Yu, H., and Hatzivassiloglou, V. 2003. Towards Answer-
ing Opinion Questions: Separating Facts from Opin-
ions and Identifying the Polarity of Opinion Sentences. 
In Proceedings of EMNLP-2003.  
Zhang, D., and Lee, W.S. 2003. Question Classification 
Using Support Vector Machines. Proceedings of the 
26th Annual International ACM SIGIR Conference on 
Re-search and Development in Information Retrieval.  
Zhu, X. 2005. Semi-supervised Learning Literature 
Survey. Technical Report 1530, Computer Sciences, 
University of Wisconsin-Madison. 
 
946
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 97?100,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
You?ve Got Answers: Towards Personalized Models for Predicting Success
in Community Question Answering
Yandong Liu and Eugene Agichtein
Emory University
{yliu49,eugene}@mathcs.emory.edu
Abstract
Question answering communities such as Ya-
hoo! Answers have emerged as a popular al-
ternative to general-purpose web search. By
directly interacting with other participants, in-
formation seekers can obtain specific answers
to their questions. However, user success in
obtaining satisfactory answers varies greatly.
We hypothesize that satisfaction with the con-
tributed answers is largely determined by the
asker?s prior experience, expectations, and
personal preferences. Hence, we begin to de-
velop personalized models of asker satisfac-
tion to predict whether a particular question
author will be satisfied with the answers con-
tributed by the community participants. We
formalize this problem, and explore a variety
of content, structure, and interaction features
for this task using standard machine learning
techniques. Our experimental evaluation over
thousands of real questions indicates that in-
deed it is beneficial to personalize satisfaction
predictions when sufficient prior user history
exists, significantly improving accuracy over
a ?one-size-fits-all? prediction model.
1 Introduction
Community Question Answering (CQA) has re-
cently become a viable method for seeking infor-
mation online. As an alternative to using general-
purpose web search engines, information seekers
now have an option to post their questions (often
complex, specific, and subjective) on Community
QA sites such as Yahoo! Answers, and have their
questions answered by other users. Hundreds of mil-
lions of answers have already been posted for tens of
millions of questions in Yahoo! Answers. However,
the success of obtaining satisfactory answers in the
available CQA portals varies greatly. In many cases,
the questions posted by askers go un-answered, or
are answered poorly, never obtaining a satisfactory
answer.
In our recent work (Liu et al, 2008) we have in-
troduced a general model for predicting asker sat-
isfaction in community question answering. We
found that previous asker history is a significant fac-
tor that correlates with satisfaction. We hypothesize
that asker?s satisfaction with contributed answers is
largely determined by the asker expectations, prior
knowledge and previous experience with using the
CQA site. Therefore, in this paper we begin to ex-
plore how to personalize satisfaction prediction -
that is, to attempt to predict whether a specific in-
formation seeker will be satisfied with any of the
contributed answers. Our aim is to provide a ?per-
sonalized? recommendation to the user that they?ve
got answers that satisfy their information need.
To the best of our knowledge, ours is the first ex-
ploration of personalizing prediction of user satis-
faction in complex and subjective information seek-
ing environments. While information seeker sat-
isfaction has been studied in ad-hoc IR context
(see (Kobayashi and Takeda, 2000) for an overview),
previous studies have been limited by the lack of re-
alistic user feedback. In contrast, we deal with com-
plex information needs and community-provided
answers, trying to predict subjective ratings pro-
vided by users themselves. Furthermore, while au-
tomatic complex QA has been an active area of re-
search, ranging from simple modification to factoid
QA technique (e.g., (Soricut and Brill, 2004)) to
knowledge intensive approaches for specialized do-
mains, the technology does not yet exist to automat-
ically answer open domain, complex, and subjective
questions. Hence, this paper contributes to both the
understanding of complex question answering, and
explores evaluation issues in a new setting.
The rest of the paper is organized as follows. We
describe the problem and our approach in Section
2, including our initial attempt at personalizing sat-
isfaction prediction. We report results of a large-
scale evaluation over thousands of real users and
97
tens of thousands of questions in Section 3. Our
results demonstrate that when sufficient prior asker
history exists, even simple personalized models re-
sult in significant improvement over a general pre-
diction model. We discuss our findings and future
work in Section 4.
2 Predicting Asker Satisfaction in CQA
We first briefly review the life of a question in a
QA community. A user (the asker) posts a question
by selecting a topical category (e.g., ?History?), and
then enters the question and, optionally, additional
details. After a short delay the question appears in
the respective category list of open questions. At
this point, other users can answer the question, vote
on other users? answers, or interact in other ways.
The asker may be notified of the answers as they are
submitted, or may check the contributed answers pe-
riodically. If the asker is satisfied with any of the
answers, she can choose it as best, and rate the an-
swer by assigning stars. At that point, the question
is considered as closed by asker. For more detailed
treatment of user interactions in CQA see (Liu et
al., 2008). If the asker rates the best answer with
at least three out of five ?stars?, we believe the asker
is satisfied with the response. But often the asker
never closes the answer personally, and instead, af-
ter a period of time, the question is closed automat-
ically. In this case, the ?best? answer may be cho-
sen by the votes, or alternatively by automatically
predicting answer quality (e.g., (Jeon et al, 2006)
or (Agichtein et al, 2008)). While the best answer
chosen automatically may be of high quality, it is un-
known if the asker?s information need was satisfied.
Based on our exploration we believe that the main
reasons for not ?closing? a question are a) the asker
loses interest in the information and b) none of the
answers are satisfactory. In both cases, the QA com-
munity has failed to provide satisfactory answers in
a timely manner and ?lost? the asker?s interest. We
consider this outcome to be ?unsatisfied?. We now
define asker satisfaction more precisely:
Definition 1 An asker in a QA community is consid-
ered satisfied iff: the asker personally has closed the
question and rated the best answer with at least 3
?stars?. Otherwise, the asker is unsatisfied.
This definition captures a key aspect of asker satis-
faction, namely that we can reliably identify when
the asker is satisfied but not the converse.
2.1 Asker Satisfaction Prediction Framework
We now briefly review our ASP (Asker Satisfac-
tion Prediction) framework that learns to classify
whether a question has been satisfactorily answered,
originally introduced in (Liu et al, 2008). ASP em-
ploys standard classification techniques to predict,
given a question thread, whether an asker would be
satisfied. A sample of features used to represent this
problem is listed in Table 1. Our features are or-
ganized around the basic entities in a question an-
swering community: questions, answers, question-
answer pairs, users, and categories. In total, we de-
veloped 51 features for this task. A sample of the
features used are listed in the Figure 1.
? Question Features: Traditional question answer-
ing features such as the wh-type of the question
(e.g., ?what? or ?where?), and whether the ques-
tion is similar to other questions in the category.
? Question-Answer Relationship Features: Over-
lap between question and answer, answer length,
and number of candidate answers. We also use
features such as the number of positive votes
(?thumbs up? in Yahoo! Answers), negative votes
(?thumbs down?), and derived statistics such as
the maximum of positive or negative votes re-
ceived for any answer (e.g., to detect cases of bril-
liant answers or, conversely, blatant abuse).
? Asker User History: Past asker activity history
such as the most recent rating, average past satis-
faction, and number of previous questions posted.
Note that only the information available about the
asker prior to posting the question was used.
? Category Features: We hypothesized that user
behavior (and asker satisfaction) varies by topi-
cal question category, as recently shown in refer-
ence (Agichtein et al, 2008). Therefore we model
the prior of asker satisfaction for the category,
such as the average asker rating (satisfaction).
? Text Features: We also include word unigrams and
bigrams to represent the text of the question sub-
ject, question detail, and the answer content. Sep-
arate feature spaces were used for each attribute to
keep answer text distinct from question text, with
frequency-based filtering.
Classification Algorithms: We experimented with
a variety of classifiers in the Weka framework (Wit-
ten and Frank, 2005). In particular, we com-
pared Support Vector Machines, Decision trees, and
Boosting-based classifiers. SVM performed the best
98
Feature Description
Question Features
Q: Q punctuation density Ratio of punctuation to words in the question
Q: Q KL div wikipedia KL divergence with Wikipedia corpus
Q: Q KL div category KL divergence with ?satisfied? questions in category
Q: Q KL div trec KL divergence with TREC questions corpus
Question-Answer Relationship Features
QA: QA sum pos vote Sum of positive votes for all the answers
QA: QA sum neg vote Sum of negative votes for all the answers
QA: QA KL div wikipedia KL Divergence of all answers with Wikipedia corpus
Asker User History Features
UH: UH questions resolved Number of questions resolved in the past
UH: UH num answers Number of all answers this user has received in the past
UH: UH more recent rating Rating for the last question before current question
UH: UH avg past rating Average rating given when closing questions in the past
Category Features
CA: CA avg time to close Average interval between opening and closing
CA: CA avg num answers Average number of answers for that category
CA: CA avg asker rating Average rating given by asker for category
CA: CA avg num votes Average number of ?best answer? votes in category
Table 1: Sample features: Question (Q), Question-
Answer Relationship (QA), Asker history (UH), and Cat-
egory (CA).
of the three during development, so we report results
using SVM for all the subsequent experiments.
2.2 Personalizing Asker Satisfaction Prediction
We now describe our initial attempt at personalizing
the ASP framework described above to each asker:
? ASP Pers+Text: We first consider the naive per-
sonalization approach where we train a separate
classifier for each user. That is, to predict a par-
ticular asker?s satisfaction with the provided an-
swers, we apply the individual classifier trained
solely on the questions (and satisfaction labels)
provided in the past by that user.
? ASP Group: A more robust approach is to train a
classifier on the questions from the group of users
similar to each other. Our current grouping was
done simply by the number of questions posted,
essentially grouping users with similar levels of
?activity?. As we will show below, text features
only help for users with at least 20 previous ques-
tions. So, we only include text features for groups
of users with at least 20 questions.
Certainly, more sophisticated personalization mod-
els and user clustering methods could be devised.
However, as we show next, even the simple models
described above prove surprisingly effective.
3 Experimental Evaluation
Wewant to predict, for a given user and their current
question whether the user will be satisfied, accord-
ing to our definition in Section 2. In other words, our
?truth? labels are based on the rating subsequently
given to the best answer by the asker herself. It is
usually more valuable to correctly predict whether
a user is satisfied (e.g., to notify a user of success).
#Questions per Asker # Questions # Answers # Users
1 132,279 1,197,089 132,279
2 31,692 287,681 15,846
3-4 23,296 213,507 7,048
5-9 15,811 143,483 2,568
10-14 5,554 54,781 481
15-19 2,304 21,835 137
20-29 2,226 23,729 93
30-49 1,866 16,982 49
50-100 842 4,528 14
Total: 216,170 1,963,615 158,515
Table 2: Distribution of questions, answers and askers
.
Hence, we focus on the Precision, Recall, and F1
values for the satisfied class.
Datasets: Our data was based on a snapshot of Ya-
hoo! Answers crawled in early 2008, containing
216,170 questions posted in 100 topical categories
by 158,515 askers, with associated 1,963,615 an-
swers in total. More detailed statistics, arranged by
the number of questions posted by each asker are
reported in (Table 2). The askers with only one
question (i.e., no prior history) dominate the dataset,
as many users try the service once and never come
back. However, for personalized satisfaction, at least
some prior history is needed. Therefore, in this early
version of our work, we focus on users who have
posted at least 2 questions - i.e., have the minimal
history of at least one prior question. In the future,
we plan to address the ?cold start? problem of pre-
dicting satisfaction of new users.
Methods compared:
? ASP: A ?one-size-fits-all? satisfaction predictor
that is trained on 10,000 randomly sampled ques-
tions with only non-textual features (Section 2.1).
? ASP+Text: The ASP classifier with text features.
? ASP Pers+Text and ASP Group: A personal-
ized classifiers described in Section 2.2.
3.1 Experimental Results
Figure 1 reports the satisfaction prediction accu-
racy for ASP, ASP Text, ASP Pers+Text, and
ASP Group for groups of askers with varying num-
ber of previous questions posted. Surprisingly,
for ASP Text, textual features only become help-
ful for users with more than 20 or 30 previous
questions posted and degrade performance other-
wise. Also note that baseline ASP classifier is
not able to achieve higher accuracy even for users
with large amount of past history. In contrast,
the ASP Pers+Text classifier, trained only on the
past question(s) of each user, achieves surprisingly
good accuracy ? often significantly outperforming
the ASP and ASP Text classifiers. The improve-
ment is especially dramatic for users with at least
99
Figure 1: Precision, Recall, and F1 of ASP, ASP Text, ASP Pers+Text, and ASP Group for predicting satisfaction of
askers with varying number of questions
20 previous questions. Interestingly, the simple
strategy of grouping users by number of previous
questions (ASP Group) is even more effective, re-
sulting in accuracy higher than both other meth-
ods for users with moderate amount of history. Fi-
nally, for users with only 2 questions total (that is,
only 1 previous question posted) the performance
of ASP Pers+Text is surprisingly high. We found
that the classifier simply ?memorizes? the outcome
of the only available previous question, and uses it
to predict the rating of the current question.
To better understand the improvement of person-
alized models, we report the most significant fea-
tures, sorted by Information Gain (IG), for three
sample ASP Pers+Text models (Table 3). Interest-
ingly, whereas for Pers 1 and Pers 2, textual features
such as ?good luck? in the answer are significant, for
Pers 3 non-textual features are most significant.
We also report the top 10 features with the high-
est information gain for the ASP and ASP Group
models (Table 4). Interestingly, while asker?s aver-
age previous rating is the top feature for ASP, the
length of membership of the asker is the most impor-
tant feature for ASP Group, perhaps allowing the
classifier to distinguish more expert users from the
active newbies. In summary, we have demonstrated
promising preliminary results on personalizing sat-
isfaction prediction even with relatively simple per-
sonalization models.
Pers 1 (97 questions) Pers 2 (49 questions) Pers 3 (25 questions)
UH total answers received Q avg pos votes Q content kl trec
UH questions resolved ?would? in answer Q content kl wikipedia
?good luck? in answer ?answer? in question UH total answers received
?is an? in answer ?just? in answer UH questions resolved
?want to? in answer ?me? in answer Q content kl asker all cate
?we? in answer ?be? in answer Q prev avg rating
?want in? answer ?in the? in question CA avg asker rating
?adenocarcinoma? in question CA History ?anybody? in question
?was? in question ?who is? in question Q content typo density
?live? in answer ?those? in answer Q detail len
Table 3: Top 10 features by Information Gain for three
sample ASP Pers+Text models
.
IG ASP IG ASP Group
0.104117 Q prev avg rating 0.30981 UH membersince in days
0.102117 Q most recent rating 0.25541 Q prev avg rating
0.047222 Q avg pos vote 0.22556 Q most recent rating
0.041773 Q sum pos vote 0.15237 CA avg num votes
0.041076 Q max pos vote 0.14466 CA avg time close
0.03535 A ques timediff in minutes 0.13489 CA avg asker rating
0.032261 UH membersince in days 0.13175 CA num ans per hour
0.031812 CA avg asker rating 0.12437 CA num ques per hour
0.03001 CA ratio ans ques 0.09314 Q avg pos vote
0.029858 CA num ans per hour 0.08572 CA ratio ans ques
Table 4: Top 10 features by information gain for ASP
(trained for all askers) and ASP Group (trained for the
group of askers with 20 to 29 questions)
4 Conclusions
We have presented preliminary results on personal-
izing satisfaction prediction, demonstrating signif-
icant accuracy improvements over a ?one-size-fits-
all? satisfaction prediction model. In the future we
plan to explore the personalization more deeply fol-
lowing the rich work in recommender systems and
collaborative filtering, with the key difference that
the asker satisfaction, and each question, are unique
(instead of shared items such as movies). In sum-
mary, our work opens a promising direction towards
modeling personalized user intent, expectations, and
satisfaction.
References
E. Agichtein, C. Castillo, D. Donato, A. Gionis, and
G. Mishne. 2008. Finding high-quality content in
social media with an application to community-based
question answering. In Proceedings of WSDM.
J. Jeon, W.B. Croft, J.H. Lee, and S. Park. 2006. A
framework to predict the quality of answers with non-
textual features. In Proceedings of SIGIR.
Mei Kobayashi and Koichi Takeda. 2000. Information
retrieval on the web. ACM Computing Surveys, 32(2).
Y. Liu, J. Bian, and E. Agichtein. 2008. Predicting in-
formation seeker satisfaction in community question
answering. In Proceedings of SIGIR.
R. Soricut and E. Brill. 2004. Automatic question an-
swering: Beyond the factoid. In HLT-NAACL.
I. Witten and E. Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kauf-
man, 2nd edition.
100
