Summarization by Analogy:
An Example-based Approach for News Articles
Megumi Makino and Kazuhide Yamamoto
Dept. of Electrical Engineering, Nagaoka University of Technology
1603-1 Kamitomioka, Nagaoka, Niigata 940-2188 Japan
{makino,ykaz}@nlp.nagaokaut.ac.jp
Abstract
Automatic summarization is an important
task as a form of human support technology.
We propose in this paper a new summariza-
tion method that is based on example-based
approach. Using example-based approach
for the summarization task has the following
three advantages: high modularity, absence
of the necessity to score importance for each
word, and high applicability to local con-
text. Experimental results have proven that
the summarization system attains approxi-
mately 60% accuracy by human judgment.
1 Introduction
The example-based approach generates language by
imitating instances, which originated in the machine
translation method based on the analogy (Nagao,
1984). The idea is derived from the observation that
a human being translates according to past transla-
tion experiences. In the machine translation task,
this approach has been implemented, and has so far
achieved efficient results (Sumita, 1998; Imamura,
2004).
In summarization, a human being also summa-
rizes with his own knowledge and experiences. For
this reason, we focus on a summarization method
which is based on analogy, example-based summa-
rization. The example-based method summarizes
the input text in three steps. First, it retrieves a simi-
lar instance to the input text. Second, it links equiv-
alent phrases between the input text and the similar
instance. Finally, a summary is acquired with com-
bination of some corresponding phrases. Here, we
employed a Japanese news article as the input text
and utilized news headlines as the instances. The
news headline consists of one brief sentence which
describes the main point.
We assert that the example-based summarization
has the following advantages:
(1)High modularity
Easy improvement and maintenance are required
to formulate a useful system in general. An
example-based framework makes it easy for us to
improve a system by only adding instances. Besides,
the addition of instances causes few side-effects.
(2)Use of similarity rather than importance
Almost all previous work on summarization has
focused on a sentence extraction. These works com-
pute importance for each word to extract a sentence.
However, it is difficult to compute the importance
which correlates with human sense. Example-based
summarization means there is no need to measure
the importance, and it computes the similarity in-
stead. We think it is easier to assess the similarity
between two expressions rather than the importance
of one expression.
(3)High applicability to local context
The statistical method, in general, attempts to
compute the probability of each word appearing in
the summary corpus (Knight and Marcu, 2002; Wit-
brock and Mittal, 1999). This may increase difficul-
ties in maintaining local context, since the statistical
approach focuses on the global probability. How-
ever, the example-based approach attempts to find
most locally similar instance out of the instance col-
lection, which may increase the fitness of input con-
texts.
For the three reasons given above, this paper
explains the system which summarizes a Japanese
news article to a one-sentence summary by imitat-
ing the similar instance.
739
As related work, Nguyen et al (2004) have pro-
posed an example-based sentence reduction model.
They deal with the compression of one sentence,
while we summarize some sentences into a one-
sentence summary. Thus, our summarization ratio
is inevitably lower than theirs, as it is considered to
be more difficult as a summarization task.
Many studies have summarized some sentences,
such as a news article, into a one-sentence summary.
Most of them extract the important sentence and
contract it. In contrast, our method generates a one-
sentence summary by combining phrases in some
sentences. Consequently, we can obtain high com-
pression summaries that include information from
many positions of the source.
2 Instance Collection
Our example-based summarization regards news
headlines as the instance collection. A news head-
line is a short sentence in which the primary point
is written. The following example is Japanese news
headlines:
Example (1) :
????????????????????
(Mitsubishi Motors Corp. produces passenger cars
in China.)
We use Japanese news headlines, like the above
examples, as instances. Besides, as we have noted,
only news headlines are used as instances; that is,
the pairs formed by an original sentence and its sum-
marized sentence are not used.
3 Example-based Summarization
3.1 Overview
Our example-based summarization system summa-
rizes a lengthy news article into a one-sentence sum-
mary by using instances. The overall process is il-
lustrated in figure 1. The system is composed of the
following three processes in this order:
1. Retrieve a similar instance to an input news ar-
ticle from the instance collection.
2. Align corresponding phrases between the input
news article and the similar instance.
3. Combine the corresponding phrases to form a
summary.
Detail of each process is described hereafter.
3.2 Retrieval of Similar Instance
The system measures a similarity between the input
and each instance in the instance collection when
it retrieves a similar instance. If many words are
shared between two expressions, we regard two ex-
pressions as similar. Hence, the similarity is calcu-
lated on basis of the overlaps of content words be-
tween the input news article I and the instance E ,
defined as follows:
Sim(E, I)=
n
?
i=1
Score(i)? {w ? ||T v1(E)?Tvi(I)||
+||To1(E)?Toi(I)||} (1)
where,
- n : the number of sentences in input,
- Tvi(?) : the verbs set in the last phrase of the i-th
sentence,
- Toi(?) : the set of content words in the i-th sen-
tence,
- ||Tv1(E)? Tvi(I)|| : the number of overlaps be-
tween Tv1(E) and Tvi(I).
In the equation, Score(i) and w are designed to give
a higher score if words indicating the main topic of
the input article are matched with words in the in-
stance. We have found that words have different
contributions, depending on the sentence position,
to the main topic. Therefore, we apply Score(i)
which depends on the sentence position i, and we
use the following experimentally-determined score
as Score(i).
Score(i) =
{
5.15 if i = 1
2.78/i0.28 otherwise (2)
The score indicates an agreement rate of content
words depending on the sentence position, which is
calculated by using 5000 pairs of newspaper?s body
and its title1 We have also found that the verbs in
the last phrase are appropriate for the main topic of
the input article. For that reason, we determine the
weight w=3 by our experiment.
Example 2 shows the similar instance obtained by
measuring the similarity.
Example (2) :
Input news article
?????????????????????
?????? 24??????????(skip the
1We used the same kind of newspaper as data set in section
4.1 for calculating Score(i).
740
Figure 1: Overview of example-based summarization
rest.)
(The Manufacturing Council held a meeting on the
24th, which discusses the hard-hitting strategy for
quality management. ...)
Obtained similar instance
????????? 18???????????
????
(The committee for the privatization of the Public
Roads Administration held the first meeting on the
18th at the prime minister?s office.)
3.3 Phrase Alignment
We compare the phrases in the input with those in
the similar instance, and the system aligns the corre-
sponding phrases. Here, the correspondence refers
to the link of the equivalent phrases between the in-
put and its similar instance. The detail of phrase
alignment procedures are shown in the following.
To begin with, sentences both in the input and in
the similar instance are analyzed using a Japanese
syntactic parser CaboCha1). The sentences are split
into phrases and named entities (NEs), such as PER-
SON, LOCATION, DATE, are recognized by the
tool.
Then the adnominal phrases in the similar in-
stance are deleted. This is because the adnomi-
nal phrases are of many types, depending on the
modified noun; accordingly, the adnominal phrase
should be used only if the modified nouns are ex-
actly matched between the input and the similar in-
stance.
Finally, the system links the corresponding
phrases. Here, phrase correspondence is one-to-
many, not one-to-one, and therefore a phrase in a
similar instance has some corresponding phrases in
the input. In order to compare phrases, the following
four measures are employed: (i) agreement of gram-
matical case, (ii) agreement of NE, (iii) similarity
with enhanced edit distance, and (iv) similarity by
means of mutual information. The measure of (i)
focuses on functional words, whereas the measures
of (ii)-(iv) note content words. Let us explain the
measures using example 2.
(i) Agreement of Grammatical Case
If there is a phrase which has the same grammati-
cal case2 in the input and in the similar instance, we
regard the phrase as the corresponding phrase. In
example 2, for example, the phrases ????? ?
(the hard-hitting strategy obj3), ??? (the meet-
ing obj)? in the input corresponds the phrase ???
??(the first meeting obj)? in the similar instance.
(ii) Agreement of Named Entity
Provided the input has the same NE tag as the sim-
ilar instance, the phrase involving its tag links the
corresponding phrase. For example, in example 2,
the phrase ?24? [DATE] (on the 24th.)? in the in-
put corresponds the phrase ?18? [DATE] (on the
18th.)? in the similar instance.
(iii) Similarity with Enhanced Edit Distance
We adopt the enhanced edit distance to link phrases
including the same characters, because Japanese ab-
breviation tends to include the same characters as
the original. For example, the abbreviation of ??
2Comma is also regarded as grammatical case (i.e., null
case) here.
3
?obj? is an object case marker.
741
??? (Bank of Japan)? is ????. The enhanced
edit distance is proposed by Yamamoto et al (2003).
The distance is a measure of similarity by counting
matching characters between two phrases. More-
over, the distance is assigned a different similarity
weight according to the type of matched characters.
We apply 1.0 to the weight only if Chinese-derived
characters (Kanji) are matched. We link phrases as
corresponding phrases, where the phrases are the top
three similar to a phrase in the similar instance.
(iv) Similarity with Mutual Information
We finally compute the similarity with mutual in-
formation to link syntactically similar phrases. For
example, given the following two expressions: ??
???? (to hold a meeting)? and ?????? (to
hold a convention)?, we regard?? (a meeting) and
?? (a convention) as similar. We use the similar-
ity proposed by Lin (1998). The method uses mu-
tual information and dependency relationships as the
phrase features. We extend the method to Japanese
by using a particle as the dependency relationships.
We link phrases as corresponding phrases, where the
phrases are the top three similar to a phrase in the
similar instance.
3.4 Combination of the Corresponding Phrases
Our system forms the one-sentence summary by
combining the corresponding phrases. Let us ex-
plain this process by using figure 2. We arrange the
phrase of the input on the node, where the phrases
is judged as the correspondence to the phrase in the
similar instance. For example, in figure 2, the sec-
ond nodes e and d denote the corresponding phrases
in the input, which correspond to the second phrase
had in the similar instance.
We assign the similarity between corresponding
phrases as the weight of node. In addition to this,
we employ phrase connection score to the weight of
edge. The score indicates the connectivity of con-
secutive two phrases, e.g. two nodes such as node
d and node e in figure 2. If you want to obtain a
fine summary, i.e., a summary that contains similar
phrases to the similar instance, and that is correct
grammatically, you have to search the best path ?Wp
for path sequence Wp = {w0,w1,w2, ? ? ? ,wm}, where
the best path maximizes the score.
?Wp =Wp s.t. argmax
p
Scorep(Wp) (3)
Figure 2: Optimal path problem that depends on
combination of the corresponding phrases4.
The best path ?Wp is a one-sentence summary which
is generated by our system. Take the case of the
thick line in figure 2, ?Wp is indicated as ?Wp =
{a,d,e,g,k,m,n}, namely, generated summary is
formed the phrases {a,d,e,g,k,m,n}. In eq.3,
Scorep(Wp) is given by
Scorep(Wp)=?
m
?
i=0
N(wi)+(1??)
m
?
i=1
E(wi?1,wi) (4)
where ? is the balancing factor among the
weights of node and edge. We score ? = 0.6 by
our experiment. m indicates the last number of the
phrase in the similar instance, N(wi) is given as fol-
lows:
N(wi)=max
{ 0.5 if (grammatical case or
NE tag is matched)
1/rank otherwise
(5)
where, rank indicates the rank order of the similarity
with the enhanced edit distance or mutual informa-
tion to the phrase wi. N(wi) illustrates the similar-
ity between corresponding two phrases. The node
score, shown above, is determined by the prelim-
inary experiment. The edge score E(wi?1,wi) is
given by
E(wi?1,wi) =
1
|loc(wi?1)? loc(wi)|+1
(6)
where, loc(wi) denotes where the location of the
sentence contains the phrase wi in the input. The
edge score means that if wi?1 and wi are located
closely to each other, a higher score is given, since a
good connection is expected in this case.
4The nodes, a, b, c,? ? ? , n, indicate the corresponding
phrases to the phrase in the similar sentence. For example, the
nodes, b, c, d correspond to ?The PRA Committee.? i is a phrase
number in the similar sentence.
742
4 Evaluation and Discussion
4.1 The Corpus
We used 26,784 news headlines as instances, which
were collected from the Nikkei-goo mail service2)
for 2001-2006. In order to adjust the weight w in the
eq.1 and the balancing parameter ? in eq.4, 150 in-
put news articles were used as the tuning set. A dif-
ferent group of 134 news articles were used for eval-
uation. We used Nihon Keizai Shimbun, a Japanese
newspaper 3) , from 1999 through 2000 as tuning and
test data.
4.2 Summarization Ratio
To calculate summarization ratio, we have compared
the number of characters in the input news articles
with that in the output summary. As the result,
we obtained a summarization ratio of 5%; namely,
95% characters in the input were reduced. From the
summarization ratio, our approach made it possible
to summarize sentences into one-sentence summary
with high compression.
4.3 Sectional Evaluation
We evaluated each part of our system by human
judgment5. We first evaluated the process by retriev-
ing similar instance. Next, we evaluated the pro-
cesses of phrase alignment and the combination by
assessing whether the output summaries were appro-
priate.
? Retrieving Process
An examinee evaluated the similar instances ob-
tained. Given an input news article and the similar
instance to the input, the examinee rates the follow-
ing scale from one to four, based on how similar the
similar instance obtained is to the summary which
the examinee generated from the input news article:
1) quite similar 2) slightly similar
3) not very similar 4) not similar
Out of 134 input articles, 77 inputs were ranked
either 1) quite similar or 2) slightly similar. As a
consequence, the accuracy of similar instance ob-
tained is approximately 57%, which indicates that
the similarity calculation for obtaining similar in-
stance is feasible.
5One examinee judged the parts of our system.
? Phrase Alignment and Combination
We also evaluated parts of phrase alignment and
the combination by human judgment. The exami-
nee compared 77 output summaries with their input.
Here, we limited 77 outputs judged as good similar
instances in evaluation of the process of retrieving
similar instance, because we evaluate specifically
the parts of phrase alignment and combination.
The examinee categorized them based on how
proper the output summary is to the input news arti-
cle:
1) quite proper 2) slightly proper
3) not very proper 4) not proper
As a result of judgment, 48 outputs out of 77 are
evaluated either 1) quite proper or 2) slightly proper.
Both a statistical method by Knight and
Marcu (2002) and an example-based method by
Nguyen et al (2004) contracted one-sentence with
a summarization ratio of approximately 60-70%.
Both papers indicated that a score of 7-8 on a scale
from one to ten was obtained. They deal with the
compression of one sentence, while we summarize
some sentences into a one-sentence summary. Thus,
our summarization ratio is lower than theirs, as it is
considered to be more difficult as a summarization
task. Despite this, we obtained the ratio that 62%
(48 out of 77 results) were judged proper. Although
direct comparison of the performance is impossible,
it is considered that our proposed method obtains a
competitive accuracy.
4.4 Discussions
? Examples of Output Summary
Figure 3 shows some examples of the output sum-
mary.
From figure 3, we can see that the similar in-
stances were effectively used, and the appropriate
summaries to the input are generated. For example,
the second summary in the figure is judged as a fine
summary contracting information of two sentences
according to the similar instance.
? Analysis of Summarization Errors
In the course of our summarization, we have ob-
served errors due to erroneous correspondences. In
Japanese, sometimes two or more phrases are con-
tracted into one phrase, as in the example below. We
now only attempt to correspond two phrases one by
743
Input news article?
??????????????????????????
??????????????????????????
??????????????????????????
??????????????????????????
???????????????????????????
?????????????????(skip the rest.)
(The prosecution made Kawano?s closing arguments on the 21st
in the trial at the Yokohama District Court. The ex-sergeant
Suguru Kawano is accused of gang-bashing by Atsugi Police
Station?s patrol group in a string of scandals of Kanagawa Pre-
fectural Police. The prosecutors demanded one and half year in
a prison. ...)
Obtained similar instance?
????? 22??8?????????????????
??????????????????????????
???
(The prosecution made Takuma?s closing arguments on the
22nd in the trial at the Osaka District Court, and asked for the
death penalty.)
Output summary?
??????????????????????????
????????????????
(The prosecution made Kawano?s closing arguments on the 21st
in the trial and demanded one and half years in prison.)
Figure 3: The examples of generated summary
one, and we thus can not deal with many-to-one cor-
respondences.
Example (3) :
?????/6????????/
(compare with the same month last year)
5??/???? 5???/
(in May)
We expect that this kind of phenomenon can
be solved by paraphrasing an input summary as
well as summary instance. Recently, several works
on paraphrasing techniques have been proposed in
Japanese, hence such pre-processing before align-
ment would be feasible.
5 Conclusion and Future Work
We have presented an example-based technique that
has been applied to the summarization task. The
essence of the proposed method is to generate a one-
sentence summary by combining instances each of
which imitates the given input.
As the result of human judgment, the retrieval
process of a similarity sentence attained 57% accu-
racy. And our method generated summary in which
62% were judged proper. We have confirmed by
our observation that the summaries were generated
by combining the phrases in many positions of the
input, while those summaries are not given just by
6
?/? indicates a phrase boundary.
common methods such as sentence extraction meth-
ods and sentence compression methods.
The sectional evaluation and the inspection of
example output show that this system works well.
However, larger scale evaluation and comparison of
its accuracy remain to be future work.
Tools and language resources
1) CaboCha, Ver.0.53, Matsumoto Lab., Nara Institute of
Science and Technology.
http://chasen.org/?taku/software/cabocha/
2) Nikkei News Mail, NIKKEI-goo,
http://nikkeimail.goo.ne.jp/
3) Nihon Keizai Shimbun Newspaper Corpus, years 1999?
2000, Nihon Keizai Shimbun, Inc.
References
Kenji Imamura. 2004. Automatic Construction of Trans-
lation Knowledge for Corpus-based Machine Transla-
tion. Ph.D. thesis, Nara Institute of Science and Tech-
nology.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion Beyond Sentence Extraction: A Probabilistic Ap-
proach to Sentence Compression. Artificial Intelli-
gence, 139(1):91?107.
Dekang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of COLING-ACL98,
pages 768?773.
Makoto Nagao. 1984. A Framework of a Mechanical
Translation Between Japanese and English By Anal-
ogy Principle. In Artificial and Human Intelligence,
pages 173?180.
Minh Le Nguyen, Susumu Horiguchi, Akira Shimazu,
and Bao Tu Ho. 2004. Example-Based Sentence
Reduction Using the Hidden Markov Model. ACM
Transactions on Asian Language Information Process-
ing, 3(2):146?158.
Eiichiro Sumita. 1998. An Example-Based Approach
to Transfer and Structural Disambiguation within Ma-
chine Translation. Ph.D. thesis, Kyoto University.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
Summarization: A Statistical Approach to Generat-
ing Highly Condensed Non-Extractive Summaries. In
Research and Development in Information Retrieval,
pages 315?316.
Eiko Yamamoto, Masahiro Kishida, Yoshinori Takenami,
Yoshiyuki Takeda, and Kyoji Umemura. 2003. Dy-
namic Programming Matching for Large Scale Infor-
mation Retrieval. In Proceedings of the 6th Interna-
tional Workshop on Information Retrieval with Asian
Languages, pages 100?108.
744
