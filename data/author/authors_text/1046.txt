c? 2002 Association for Computational Linguistics
Incremental Construction and
Maintenance of Minimal Finite-State
Automata
Rafael C. Carrasco? Mikel L. Forcada?
Universitat d?Alacant Universitat d?Alacant
Daciuk et al [Computational Linguistics 26(1):3?16 (2000)] describe a method for constructing
incrementally minimal, deterministic, acyclic finite-state automata (dictionaries) from sets of
strings. But acyclic finite-state automata have limitations: For instance, if one wants a linguistic
application to accept all possible integer numbers or Internet addresses, the corresponding finite-
state automaton has to be cyclic. In this article, we describe a simple and equally efficient method
for modifying any minimal finite-state automaton (be it acyclic or not) so that a string is added
to or removed from the language it accepts; both operations are very important when dictionary
maintenance is performed and solve the dictionary construction problem addressed by Daciuk
et al as a special case. The algorithms proposed here may be straightforwardly derived from
the customary textbook constructions for the intersection and the complementation of finite-
state automata; the algorithms exploit the special properties of the automata resulting from the
intersection operation when one of the finite-state automata accepts a single string.
1. Introduction
In a recent paper in this journal, Daciuk et al (2000) describe two methods for con-
structing incrementally minimal, deterministic, acyclic finite-state automata (dictio-
naries) from sets of strings: The first method adds strings in dictionary order, and the
second one is for unsorted data. Adding an entry is an important dictionary mainte-
nance operation, but so is removing an entry from the dictionary, for example, if it
is found to be incorrect. Since ordering cannot obviously be expected in the removal
case, we will focus on the second, more general problem (a solution for which has
already been sketched by Revuz [2000]).
But dictionaries or acyclic finite automata have limitations: For instance, if one
wants an application to accept all possible integer numbers or Internet addresses, the
corresponding finite-state automaton has to be cyclic. In this article, we show a simple
and equally efficient method for modifying any minimal finite-state automaton (be it
acyclic or not) so that a string is added to or removed from the language it accepts. The
algorithm may be straightforwardly derived from customary textbook constructions
for the intersection and the complementation of finite-state automata; the resulting
algorithm solves the dictionary construction problem addressed by Daciuk et al?s
(2000) second algorithm as a special case.
? Departament de Llenguatges i Sistemes Informa`tics, Universitat d?Alacant, E-03071 Alacant, Spain.
E-mail: carrasco@dlsi.ua.es
? Departament de Llenguatges i Sistemes Informa`tics, Universitat d?Alacant, E-03071 Alacant, Spain.
E-mail: mlf@dlsi.ua.es.
208
Computational Linguistics Volume 28, Number 2
This article has the following parts. In Section 2, we give some necessary math-
ematical preliminaries. The minimal automata resulting from adding or removing a
string are described in detail in Section 3; the algorithms are described in Section 4.
In Section 5, one addition and one removal example are explained in detail, and some
closing remarks are given in Section 6.
2. Mathematical Preliminaries
2.1 Finite-State Automata and Languages
As in Daciuk et al (2000), we will define a deterministic finite-state automaton as
M = (Q,?, ?, q0, F), where Q is a finite set of states, q0 ? Q is the start state, F ? Q is
a set of accepting states, ? is a finite set of symbols called the alphabet, and ?: Q ?
? ? Q is the next-state mapping. In this article, we will define ? as a total mapping;
the corresponding finite-state automaton will be called complete (Revuz 2000). This
involves no loss of generality, as any finite-state automaton may be made complete
by adding a new absorption state ? to Q, so that all undefined transitions point to it
and ?(?, a) =? for all a ? ?. Using complete finite-state automata is convenient for
the theoretical discussion presented in this article; real implementations of automata
and the corresponding algorithms need not contain an explicit representation of the
absorption state and its incoming and outgoing transitions.
For complete finite-state automata, the extended mapping ??: Q ? ?? ? Q (the
extension of ? for strings) is defined simply as
??(q, ) = q
??(q, ax ) = ??(?(q, a), x) (1)
for all a ? ? and x ? ??, with  the empty or null string. The language accepted by
automaton M
L(M) = {w ? ??: ??(q0, w) ? F} (2)
and the right language of state q
L(q) = {x ? ??: ??(q, x) ? F} (3)
are defined as in Daciuk et al (2000).
2.2 Single-String Automaton
We also find it convenient to define the (complete) single-string automaton for string
w, denoted Mw = (Qw,?, ?w, q0w, Fw), such that L(Mw) = {w}. This automaton has
Qw = Pr(w)?{?w}, where Pr(w) is the set of all prefixes of w and ?w is the absorption
state, Fw = {w}, and q0w =  (note that nonabsorption states in Qw will be named after
the corresponding prefix of w). The next-state function is defined as follows
?(x, a) =
{
xa if x , xa ? Pr(w)
?w otherwise
(4)
Note that the single-string automaton for a string w has |Qw| = |w|+ 2 states.
2.3 Operations with Finite-State Automata
2.3.1 Intersection Automaton. Given two finite-state automata M1 and M2, it is easy
to build an automaton M so that L(M) = L(M1) ? L(M2). This construction is found
209
Carrasco and Forcada Incremental Construction of Minimal FSA
in formal language theory textbooks (Hopcroft and Ullman 1979, page 59) and is
referred to as standard in papers (Karakostas, Viglas, and Lipton 2000). The (complete)
intersection automaton has Q = Q1?Q2, q0 = (q01, q02), F = F1?F2, and ?((q1, q2), a) =
(?1(q1, a), ?2(q2, a)) for all a ? ?, q1 ? Q1 and q2 ? Q2.
2.3.2 Complementary Automaton. Given a complete finite-state automaton M, it is
easy to build its complementary automaton M? so that L(M?) = ?? ? L(M); the only
change is the set of final states: F? = Q ? F (Hopcroft and Ullman 1979, page 59). In
particular, the complementary single-string automaton M?w accepting ?? ? {w} is
identical to Mw except that F?w = Q ? {w}.
2.3.3 Union Automaton. The above constructions may be combined easily to obtain a
construction to build, from two complete automata M1 and M2, the (complete) union
automaton M such that L(M) = L(M1) ? L(M2). It suffices to consider that, for any
two languages on ??, L1 ? L2 = ?? ? (?? ? L1) ? (?? ? L2). The resulting automaton
M is identical to the intersection automaton defined above except that F = (F1 ?Q2)?
(Q1 ? F2).
3. Adding and Removing a String
3.1 Adding a String
Given a (possibly cyclic) minimal complete finite-state automaton M, it is easy to
build a new complete automaton M? accepting L(M?) = L(M) ? {w} by applying the
union construct defined above to M and the single-string automaton Mw. The resulting
automaton M? = (Q?,?, ??, q?0, F
?), which may be minimized very easily (see below), has
four kinds of states in Q?:
? States of the form (q,?w) with q ? Q ? {?}, equivalent to those
nonabsorption states of M that are not reached by any prefix of w; they
will be called intact states, because they have the same transition
structure as their counterparts in M (that is, if ?(q, a) = q?, then
??((q,?w), a) = (q?,?w)) and belong to F? if q ? F. As a result, they have
exactly the same right languages, L((q,?w)) = L(q), because all of their
outgoing transitions go to other intact states. Furthermore, each state
(q,?w) has a different right language; therefore, no two intact states will
ever be merged into one by minimization (intact states may, however, be
eliminated, if they become unreachable, as we will describe below). For
large automata (dictionaries) M, these are the great majority of states (the
number of intact states ranges between |Q| ? |w| ? 1 and |Q|); therefore, it
will be convenient in practice to consider M? as a modified version of M,
and it will be treated as such in the algorithms found in this article.
? States of the form (q, x) with q ? Q ? {?} and x ? Pr(w), such that
??(q0, x) = q; they will be called cloned states, inspired by the
terminology in Daciuk et al (2000); the remaining states in
(Q ? {?})? Pr(w)?the great majority of states in Q ? Qw?may safely
be discarded because they are unreachable from the new start state
q?0 = (q0, ), which itself is a cloned state. Cloned states are modified
versions of the original states q ? Q ? {?}: All of their outgoing
transitions point to the corresponding intact states in Q?, that is,
(?(q, a),?w), except for the transition with symbol a : xa ? Pr(w), which
210
Computational Linguistics Volume 28, Number 2
now points to the corresponding cloned state (?(q, a), xa), that is,
??((q, x), a) =
{
(?(q, a), xa) if xa ? Pr(w)
(?(q, a),?w) otherwise
(5)
Cloned states are in F? if the corresponding original states are in F; in
addition, if there is a cloned state of the form (q, w), then it is in F?. There
are at most |w|+ 1 cloned states.
? States of the form (?, x), with x ? Pr(w). These states will be called
queue states; states of this form appear when the string w is not in L(M)
(the pertinent case, because we are adding it) and only if in the original
automaton ??(q0, x) =? for some x ? Pr(w). Only the final queue state
(?, w)?if it exists?is in F?. There are at most |w| queue states.
? The new absorption state ??= (?,?w) /? F.
This automaton has to be minimized; because of the nature of the construction algo-
rithm, however, minimization may be accomplished in a small number of operations.
It is not difficult to show that minimization may be performed by initializing a list R
called the register (Daciuk et al 2000) with all of the intact states and then testing,
one by one, queue and cloned states (starting with the last queue state (?, w) or, if it
does not exist, the last clone state (q, w), and descending in Pr(w)) against states in
the register and adding them to the register if they are not found to be equivalent to
a state in R. (Performing this check backwards avoids having to test the equivalence
of states by visiting their descendants recursively: see the end of Section 4.1.) Mini-
mization (including the elimination of unreachable states in M?) appears in Section 4
as part of the string addition and removal algorithms.
3.2 Removing a String
Again, given a (possibly cyclic) minimal complete finite-state automaton M, it is easy
to build a new complete automaton M? accepting L(M?) = L(M) ? {w} = L(M) ?
(?? ? {w}) by applying the intersection construct defined above to M and M?w. The
resulting automaton has the same sets of reachable states in Q? as in the case of adding
string w and therefore the same close-to-minimality properties; since w is supposed
to be in L(M), however, no queue states will be formed. (Note that, if w /? L(M),
a nonaccepting queue with all states eventually equivalent to ??= (?,?w) may be
formed.) The accepting states in F? are intact states (q,?w) and cloned states (q, x)
with q ? F, except for state (q, w). Minimization may be performed analogously to the
string addition case.
4. Algorithms
4.1 Adding a String
Figure 1 shows the algorithm that may be used to add a string to an existing automa-
ton, which follows the construction in Section 3.1. The resulting automaton is viewed
as a modification of the original one: Therefore, intact states are not created; instead,
unreachable intact states are eliminated later. The register R of states not needing
minimization is initialized with Q. The algorithm has three parts:
? First, the cloned and queue states are built and added to Q by using
function clone() for all prefixes of w. The function returns a cloned state
211
Carrasco and Forcada Incremental Construction of Minimal FSA
(with all transitions created), if the argument is a nonabsorption state in
Q ? {?}, or a queue state, if it operates on the absorption state ?? Q.
? Second, those intact states that have become unreachable as a result of
designating the cloned state q?0 as the new start state are removed from Q
and R, and the start state is replaced by its clone. Unreachable states are
simply those having no incoming transitions as constructed by the
algorithm or as a consequence of the removal of other unreachable
states; therefore, function unreachable() simply has to check for the
absence of incoming transitions. Note that only intact states (q,?w)
corresponding to q such that ??(q0, x) = q for some x ? Pr(w) may
become unreachable as a result of having been cloned.
? Third, the queue and cloned states are checked (starting with the last
state) against the register using function replace or register(), which is
essentially the same as the nonrecursive version in the second algorithm
in Daciuk et al (2000) and is shown in Figure 2. If argument state q is
found to be equivalent to a state p in the register R, function merge(p, q)
is called to redirect into p those transitions coming into q; if not,
argument state q is simply added to the register. Equivalence is checked
by function equiv(), shown in Figure 3, which checks for the equivalence
of states by comparing (1) whether both states are accepting or not, and
(2) whether the corresponding outgoing transitions lead to the same state
in R. Note that outgoing transitions cannot lead to equivalent states, as
there are no pairs of different equivalent states in the register
(?p, q ? R, equiv(p, q) ? p = q) and backwards minimization guarantees
that the state has no transitions to unregister states.
Finally, the new (minimal) automaton is returned. In real implementations, absorption
states are not explicitly stored; this results in small differences in the implementations
of the functions clone() and equiv().
4.2 Removing a String
The algorithm for removing a string from the language accepted by an automaton M?
differs from the previous algorithm only in that the line
F ? F ? {qlast}
has to be added after the first end for. Since the string removal algorithm will usually
be asked to remove a string that was in L(M), function clone() will usually generate
only cloned states and no queue states (see Section 3.2 for the special case w /? L(M)).
5. Examples
5.1 Adding a String
Assume that we want to add the string bra to the automaton in Figure 4, which accepts
the set of strings (ba)+?{bar} (for clarity, in all automata, the absorption state and all
transitions leading to it will not be drawn). The single-string automaton for string bra
is shown in Figure 5. Application of the first stages of the string addition algorithm
leads to the (unminimized) automaton in Figure 6. The automaton has, in addition to
the set of intact states {(0,?w), . . . , (5,?w)}, two cloned states ((0, ) and (1, b)) and two
queue states ((?, br) and (?, bra)). As a consequence of the designation of (0, ) as the
212
Computational Linguistics Volume 28, Number 2
algorithm addstring
Input: M = (Q,?, ?, q0, F) (minimal, complete), w ? ??
Output: M?=(Q?,?, ??, q?0, F
?) minimal, complete, and such that L(M?)=L(M)?{w}
R ? Q [initialize register]
q?0 ? clone(q0) [clone start state]
qlast ? q?0
for i = 1 to |w|
q ? clone(??(q0, w1 ? ? ?wi)) [create cloned and queue states;
add clones of accepting states to F]
?(qlast, wi) ? q
qlast ? q
end for
i ? 1
qcurrent ? q0
while(i ? |w| and unreachable(qcurrent))
qnext ? ?(qcurrent, wi)
Q ? Q ? {qcurrent} [remove unreachable state from Q
and update transitions in ?]
R ? R ? {qcurrent} [remove also from register]
qcurrent ? qnext
i ? i + 1
end while
if unreachable(qcurrent)
Q ? Q ? {qcurrent}
R ? R ? {qcurrent}
end if
q0 ? q?0 [replace start state]
for i = |w| downto 1
replace or register(??(q0, w1 ? ? ?wi)) [check queue and cloned states one by one]
end for
return M = (Q,?, ?, q0, F)
end algorithm
Figure 1
Algorithm to add a string w to the language accepted by a finite-state automaton while
keeping it minimal.
function replace or register(q)
if ?p ? R : equiv(p, q) then
merge(p, q)
else
R ? R ? {q}
end if
end function
Figure 2
The function replace or register().
213
Carrasco and Forcada Incremental Construction of Minimal FSA
function equiv(p, q)
if (p ? F ? q /? F) ? (p /? F ? q ? F) return false
for all symbols a ? ?
if ?(p, a) = ?(q, a) return false
end for
return true
end function
Figure 3
The function equiv(p, q).
Figure 4
Minimal automaton accepting the set of strings (ba)+ ? {bar}.
Figure 5
Single-string automaton accepting string bra.
0,?w 1,?w 2,?w
3,?w
4,?w 5,?w
0,? 1,b
?,br ?,bra
b a
r
b a
bb
a
r
a
Figure 6
Unminimized automaton accepting the set (ba)+ ? {bar} ? {bra}. Shadowed states (0,?w)
and (1,?w) have become unreachable (have no incoming transitions) and are eliminated in
precisely that order.
214
Computational Linguistics Volume 28, Number 2
Figure 7
Minimal automaton accepting the set (ba)+ ? {bar} ? {bra}.
? b ba bab babab a b a
Figure 8
Single-string automaton accepting the string baba.
0,?w 1,?w
2,?w
3,?w
4,?w
5,?w
6,?w
0,? 1,b 2,ba 4,bab 6,baba
b
a
r
b
r
a
a
b
b
r
a
r
b a
b
Figure 9
Unminimized automaton accepting the set (ba)+ ? {bar} ? {bra} ? {baba}. Shadowed states
(0,?w), (1,?w), and (2,?w) have become unreachable (have no incoming transitions) and are
eliminated in precisely that order.
new start state, shadowed states (0,?w) and (1,?w) become unreachable (have no in-
coming transitions) and are eliminated in precisely that order in the second stage of the
algorithm. The final stage of the algorithm puts intact states into the register and tests
queue and cloned states for equivalence with states in the register. The first state tested
is (?, bra), which is found to be equivalent to (3,?w); therefore, transitions coming
into (?, bra) are made to point to (3,?w). Then, states (?, br), (1, b) and (0, ) are tested
in order, found to have no equivalent in the register, and added to it. The resulting
minimal automaton, after a convenient renumbering of states, is shown in Figure 7.
5.2 Removing a String
Now let us consider the case in which we want to remove string baba from the
language accepted by the automaton in Figure 7 (the single-string automaton for baba
is shown in Figure 8). The automaton resulting from the application of the initial
(construction) stages of the automaton is shown in Figure 9. Note that state (6, baba) is
marked as nonaccepting, because we are removing a string. Again, as a consequence
of the designation of (0, ) as the new start state, shadowed states (0,?w), (1,?w),
215
Carrasco and Forcada Incremental Construction of Minimal FSA
0 1
2
3
4
5 6
7 8
b
r
a
a
r
b a
b
a
b
Figure 10
Minimal automaton accepting the set (ba)+ ? {bar} ? {bra} ? {baba}.
and (2,?w) become unreachable (have no incoming transitions) and are eliminated in
precisely that order in the second stage of the algorithm. The last stage of the algorithm
puts all intact states into the register, checks cloned states (6, baba), (4, bab), (2, ba),
(1, b) and (0, ) (no queue states, since baba is accepted by the automaton in Figure 7),
and finds none of them to be equivalent to those in the register, to which they are
added. The resulting minimal automaton is shown in Figure 10.
6. Concluding Remarks
We have derived, from basic results of language and automata theory, a simple method
for modifying a minimal (possibly cyclic) finite-state automaton so that it recognizes
one string more or one string less while keeping the finite-state automaton minimal.
These two operations may be applied to dictionary construction and maintenance
and generalize the result in Daciuk et al?s (2000) second algorithm (incremental con-
struction of acyclic finite-state automata from unsorted strings) in two respects, with
interesting practical implications:
? The method described here allows for the addition of strings to the
languages of cyclic automata (in practice, it may be convenient to have
cycles in dictionaries if we want them to accept, for example, all possible
integer numbers or Internet addresses). In this respect, the algorithm
presented also generalizes the string removal method sketched by Revuz
(2000) for acyclic automata.
? Removal of strings is as easy as addition. This means that, for example,
the detection of an erroneous entry in the dictionary does not imply
having to rebuild the dictionary completely.
The asymptotic time complexity of the algorithms is in the same class (O(|Q||w|)) as
that in Daciuk et al (2000), because the slowest part of the algorithm (the last one)
checks all queue and cloned states (O(|w|)) against all states of the register (O(|Q|)). As
suggested by one of the reviewers of this article, an improvement in efficiency may be
obtained by realizing that, in many cases, cloned states corresponding to the shortest
prefixes of string w are not affected by minimization, because their intact equivalents
have become unreachable and therefore have been removed from the register; the
solution lies in identifying these states and not cloning them (for example, Daciuk et
al.?s [2000] and Revuz?s [2000] algorithms do not clone them).
As for the future, we are working on an adaptation of this algorithm for the main-
tenance of morphological analyzers and generators using finite-state nondeterministic
letter transducers (Roche and Schabes 1997; Garrido et al 1999).
216
Computational Linguistics Volume 28, Number 2
Acknowledgments
The work reported in this article has been
funded by the Spanish Comisio?n
Interministerial de Ciencia y Tecnolog??a
through grant TIC2000-1599. We thank the
two reviewers for their suggestions and
Colin de la Higuera for his comments on
the manuscript.
References
Daciuk, Jan, Stoyan Mihov, Bruce W.
Watson, and Richard E. Watson. 2000.
Incremental construction of minimal
acyclic finite-state automata.
Computational Linguistics, 26(1):3?16.
Garrido, Alicia, Amaia Iturraspe, Sandra
Montserrat, Herm??nia Pastor, and
Mikel L. Forcada. 1999. A compiler for
morphological analysers and generators
based on finite-state transducers.
Procesamiento del Lenguaje Natural,
25:93?98.
Hopcroft, John E. and Jeffrey D. Ullman.
1979. Introduction to Automata Theory,
Languages, and Computation.
Addison-Wesley, Reading, MA.
Karakostas, George, Anastasios Viglas, and
Richard J. Lipton. 2000. On the
complexity of intersecting finite state
automata. In Proceedings of the 15th Annual
IEEE Conference on Computational
Complexity (CoCo?00), pages 229?234.
Revuz, Dominique. 2000. Dynamic acyclic
minimal automaton. In Preproceedings of
CIAA 2000: Fifth International Conference on
Implementation and Application of Automata,
pages 226?232, London, Ontario, July
24?25.
Roche, Emmanuel and Yves Schabes. 1997.
Introduction. In Emmanuel Roche and
Yves Schabes, editors, Finite-State Language
Processing. MIT Press, pages 1?65.
In: Proceedings of CoNLL-2000 and LLL -2000,  pages 123-125, Lisbon, Portugal, 2000. 
A Comparison of PCFG Models* 
J ose  Lu is  Verdd-Mas  and Jo rge  Ca lera -Rub io  and Rafae l  C.  Car rasco  
Departament  de Llenguatges i Sistemes Inform?tics 
Universitat d'Alacant, E-03071 Alacant (Spain) 
(verdu, calera, carrasco)@dlsi.ua.es 
Abst rac t  
In this paper, we compare three different ap- 
proaches to build a probabilistic ontext-free 
grammar for natural language parsing from a 
tree bank corpus: 1) a model that simply ex- 
tracts the rules contained in the corpus and 
counts the number of occurrences of each rule 
2) a model that also stores information about 
the parent node's category and, 3) a model that 
estimates the probabilities according to a gen- 
eralized k-gram scheme with k -- 3. The last 
one allows for a faster parsing and decreases the 
perplexity of test samples. 
1 In t roduct ion  
Recent work (Johnson, 1998) has explored the 
performance of parsers based on a probabilistic 
context-free grammar (PCFG) extracted from a 
training corpus. The results show that the type 
of tree representation used in the corpus can 
have a substantial effect in the estimated like- 
lihood of each sentence or parse tree. Accord- 
ing to (Johnson, 1998), weaker independence as- 
sumptions --such as decreasing the number of 
nodes or increasing the number of node labels-- 
improve the efficiency of the parser. The best 
results were obtained with parent-annotated la- 
bels where each node stores contextual informa- 
tion in the form of the category of the node's 
parent. This fact is in agreement with the 
observation put forward by Charniak (Char- 
niak, 1996) that simple PCFGs, directly ob -  
tained from a corpus, largely overgeneralize. 
This property suggests that, in these models, 
a large probability mass is assigned to incorrect 
* Work partially supported by the Spanish CICYT un- 
der grant TIC97-0941. 
parses and, therefore, any procedure that con- 
centrates the probability on the correct parses 
will increase the likelihood of the samples. 
In this spirit, we introduce a generalization of
the classic k-gram models, widely used for string 
processing (Brown et al, 1992; Ney et al, 1995), 
to the case of trees. The PCFG obtained in this 
way consists of rules that include information 
about the context where the rule is applied. 
The experiments were performed using the 
Wall Street Journal (WSJ) corpus of the Uni- 
versity of Pennsylvania (Marcus et al, 1993) 
modified as described in (Charniak, 1996) 
and (Johnson, 1998). 
2 A genera l i zed  k -gram mode l  
Recall that k-gram models are stochastic mod- 
els for the generation of sequences l,s2,... 
based on conditional probabilities, that is: 
1. the probability P(s l s2 . . .  s t lM)  of a se- 
quence in the model M is computed as a 
product 
pM(  Sl )pM(  S2\[Sl ) " " " pM(  St\[Sl S2 . . . St-l), 
and 
2. the dependence of the probabilities PM 
on previous history is assumed to be re- 
stricted to the immediate preceding con- 
text, in particular, the last k - 1 words: 
PM(St \ [S l  . . . S t -1 )  ---- pM(St \ [S t -k+l  . . . S t -1 ) .  
Note that in this kind of models, the probability 
that the observation st is generated at time t is 
computed as a function of the subsequence of
length k - 1 that immediately precedes t (this 
is called a state). However, in the case of trees, 
it is not obvious what context should be taken in 
to account. Indeed, there is a natural preference 
when processing strings (the usual left-to-right 
123 
VP 
label: VP s 
V NP PP NpvP 
~ ppVP 
Det N p NP Np PP 
Det N 
Figure 1: A sample parse tree of depth 3. 
order) but there are at least two standard ways 
of processing trees: ascending (or bottom-up) 
analysis and descending (or top-down) analysis. 
Ascending tree automata recognize a wider class 
of languages (Nivat and Podelski, 1!197; G~cseg 
and Steinby, 1984) and, therefore, they allow for 
richer descriptions. 
Thus, our model will compute the expansion 
probability for a given node as a function of the 
subtree of depth k - 2 that the node generates 1, 
i.e., every state stores a subtree of depth k - 2. 
In the particular case k = 2, only the label of the 
node is taken into account (this is analogous to 
the standard bigram model for strings) and the 
model coincides with the simple rule-counting 
approach. For instance, for the tree depicted in 
Fig. 1, the following rules are obtained: 
VP ~ V NP PP 
NP ~ Det N 
PP ~ P NP 
However, in case k = 3, the expansion proba- 
bilities depend on the states that are defined by 
the node label, the number of descendents the 
node and the sequence of labels in the descen- 
dents (if any). Therefore, for the same tree the 
following rules are obtained in this case: 
VP(V NP PP) ~ V NP(Det N) PP(P NP) 
NP(Det N) --+ Det N 
PP(P NP) ~ P NP(Det N) 
where each state has the form X(Z1...Zm). 
This is equivalent to a relabeling of the parse 
tree before extracting the rules. 
Finally, in the parent annotated model (PA) 
described in (Johnson, 1998) the states depend 
1Note that in our notation a single node tree has 
depth 0. This is in contrast to strings, where a single 
symbol has length 1. 
on both the node label and the node's parent 
--+ V NP VP ppVP 
---+ Det N 
P NP PP 
---+ Det N 
It is obvious that the k = 3 and PA models 
incorporate contextual information that is not 
present in the case k = 2 and, then, a higher 
number of rules for a fixed number of categories 
is possible. In practice, due to the finite size 
of the training corpus, the number of rules is 
always moderate. However, as higher values of 
k lead to an enormous number of possible rules, 
huge data sets would be necessary in order to 
have a reliable estimate of the probabilities for 
values above k = 3. A detailed mathematical 
description of these type of models can be found 
in (Rico-Juan et al, 2000) 
3 Exper imenta l  resu l t s  
The following table shows some data obtained 
with the three different models and the WSJ 
corpus. The second column contains the num- 
ber of rules in the grammar obtained from a 
training subset of the corpus (24500 sentences, 
about the first half in the corpus) and the last 
one contains the percentage of sentences in a 
test set (2000 sentences) that cannot be parsed 
by the grammar. 
Model 
k=2 
k=3 
PA 
number of rules %unparsed sent. 
11377 0 
64892 24 
18022 0.2 
As expected, the number of rules obtained in- 
creases as more information is conveyed by the 
node label, although this increase is not ex- 
treme. On the other hand, as the generaliza- 
tion power decreases, ome sentences in the test 
set become unparsable, that is, they cannot be 
generated by the grammar. The number of un- 
parsed sentences i  very small for the parent an- 
notated model but cannot be neglected for the 
k = 3 model. 
As we will use the perplexity of a test sample 
S = {wl, ..., w\]s\] } as an indication of the quality 
of the model, 
1 k~__S\]llog2p(WklM) PP = iS--- ~ 
124 
, unparsable sentences would produce an infinite 
perplexity. Therefore, we studied the perplex- 
ity of the test set for a linear combination of 
two models Mi and Mj with p(wklMi -- Mj) = 
)~p(wklMi) + (1 - ~)p(wklMj). The mixing pa- 
rameter ~ was chosen in order to minimize the 
perplexity. Figure 2 shows that there is always 
11o 
105 
lOO 
95 
90 
? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ..~.'?'"+'" 
i 
05  
Figure 2: Test set perplexity as a function of the 
mixture parameter A. Upper line: k -- 2 and PA. 
Lower line: k = 2 and k = 3. 
a minimum perplexity for an intermediate value 
of ),. The best results were obtained with a mix- 
ture of the k-gram models for k = 2 and k = 3 
with a heavier component (73%) of the last one. 
The minimum perplexity PPm and the corre- 
sponding value of ~ obtained are shown in the 
following table: 
Mixture model PPm Am 
k = 2 and PA 107.9 0.58 
k = 2 and k = 3 91.0 0.27 
It is also worth to remark that the model k = 3 
is the less ambiguous model and, then, parsing 
of sentences becomes much faster. 
4 Conc lus ion 
We have investigated the applicability of a 
PCFG model based on the extension of k-gram 
models described in (Rico-Juan et al, 2000). 
The perplexity of the test sample decreases 
when a combination of models with k = 2 and 
k = 3 is used to predict string probabilities. We 
are at present checking that the behavior also 
holds for other quality measures as the precision 
and recall of parses of sentences that express 
strong equivalence between the model and the 
data. 
Re ferences  
Peter F. Brown, Vincent J. Della Pietra, Peter V. 
deSouza, Jennifer C. Lai, and Robert L. Mercer. 
1992. Class-based n-gram models of natural lan- 
guage. Computational Linguistics, 18(4):467-479. 
Eugene Charniak. 1996. Tree-bank grammars. In 
Proceedings of the Thirteenth National Confer- 
ence on Artificial Intelligence and the Eighth 
Innovative Applications of Artificial Intelligence 
Conference, pages 1031-1036, Menlo Park. AAAI 
Press/MIT Press. 
Ferenc Gdcseg and Magnus Steinby. 1984. Tree Au- 
tomata. Akad@miai Kiad6, Budapest. 
Mark Johnson. 1998. PCFG models of linguistic 
tree representations. Computational Linguistics, 
24(4):613-632. 
Mitchell P. Marcus, Beatrice Santorini, and Mary 
Ann Marcinkiewicz. 1993. Building a large anno- 
tated corpus of english: the penn treebank. Com- 
putational Linguistics, 19:313-330. 
H. Ney, U. Essen, and R. Kneser. 1995. On the esti- 
mation of small probabilities by leaving-one-out. 
IEEE Trans. on Pattern Analysis and Machine 
Intelligence, 17(12):1202-1212. 
Maurice Nivat and Andreas Podelski. 1997. Min- 
imal ascending and descending tree automata. 
SIAM Journal on Computing, 26(1):39-58. 
Juan Ram6n Rico-Juan, Jorge Calera-Rubio, and 
Rafael C. Carrasco. 2000. Probabilistic k-testable 
tree language. In ICGI-2000, to appear. 
125 
