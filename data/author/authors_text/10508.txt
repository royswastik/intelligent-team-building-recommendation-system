How to Take Advantage of the Limitations with Markov Clustering? 
The Foundations of Branching Markov Clustering (BMCL) 
Hiroyuki Akama 
Tokyo Institute of Technology 
W9-10, 2-12-1, 
O-okayama, Meguroku, 
152-8552 Tokyo, Japan 
akama.h.aa@m.titech.ac.jp 
Maki Miyake 
University of Osaka 
Machikane-machi, Toyo-
naka-shi, 
560-0043 Osaka, Japan 
mmiyake@lang.osaka-
u.ac.jp 
Jaeyoung Jung 
Tokyo Institute of Technology 
W9-10, 2-12-1, 
O-okayama, Meguroku, 
152-8552 Tokyo, Japan 
jung.j.aa@m.titech.ac.jp 
 
Abstract 
In this paper, we propose a novel approach 
to optimally employing the MCL (Markov 
Cluster Algorithm) by ?neutralizing? the 
trivial disadvantages acknowledged by its 
original proposer.  Our BMCL (Branching 
Markov Clustering) algorithm makes it 
possible to subdivide a large core cluster 
into appropriately resized sub-graphs.  Util-
izing three corpora, we examine the effects 
of the BMCL which varies according to the 
curvature (clustering coefficient) of a hub 
in a network. 
1 MCL limitations? 
1.1 MCL and modularity Q 
The Markov Cluster Algorithm (MCL) (Van Don-
gen, 2000) is well-recognized as an effective 
method of graph clustering.  It involves changing 
the values of a transition matrix toward either 0 or 
1 at each step in a random walk until the stochastic 
condition is satisfied.  When the hadamard power 
for each transition probability value is divided by 
the sum of each column, the rescaling process 
yields a transition matrix for the next stage.  After 
repeatedly alternating for about 20 times between 
two steps?random walk (expansion) and probabil-
ity modification (inflation)?the process will fi-
nally reach a convergence stage in which the whole 
graph is subdivided into a set of ?hard? clusters that 
have no overlap.  Although this method has been 
generally applied in various domains with notable 
successes (such as Tribe-MCL clustering of pro-
teins (Enright et al, 2002); Synonymy Network, 
created by the addition of noise data (Gfeller, 
2005); and Lexical Acquisition (Dorow et al, 
2005)), Van Dongen et al (2001) frankly acknowl-
edge that there are limitations or weaknesses.  For 
instance, the readme file, which is included with 
the free MCL software available via the Internet 
from Van Dongen?s group, remarks that ?MCL is 
probably not suited for clustering tree graphs?. 
It should also be noted, however, that the group 
has provided no mathematical evidence for their 
claim of the MCL?s unsuitability for hierarchical 
applications.  What prompts this subtle caveat in 
the first place?  Is this a limitation on the type of 
graph clustering that can employ random walks for 
spectral analysis?  Or, is it difficult for this tech-
nique to (re-)form or adjust graph clusters that 
have already been clustered into a kind of multi-
layered organization?  Such questions are very im-
portant when comparing the MCL with other graph 
clustering methods that employ (greedy) algo-
rithms developed step by step in a tree form. 
A tree graph is essentially a kind of dendrogram, 
which means clustering results can be generated 
solely by making a cross cut at some height be-
tween the root and the leaves.  In other words, as 
there is no horizontal connection at the same level, 
it is not possible to create triangle circulation paths 
in a single stroke.  However, the graph coefficient 
known as ?curvature? (Dorow, 2005) is appropri-
ate for defining such structures.  The curvature, or 
the cluster coefficient, of a vertex is defined as a 
fraction of existing links among a node?s neighbors 
out of all possible links between neighbors.  Thus, 
a tree graph may be regarded as a chain of star 
graphs where all the vertices have a curvature val-
ue of 0. 
901
It is certainly true that when a hub has a low 
curvature value, the corresponding cluster will be 
less cohesive and more sparse than usual.  The 
modularity Q value is very low in such cases when 
we try to measure the accuracy of results from 
MCL clustering.  Modularity Q indicates differ-
ences in edge distributions between a graph with 
meaningful partitions and a random graph for iden-
tical vertices conditions.  According to Newman 
and Girvan, ? ?=
i
iii aeQ )(
2 , where i is the clus-
ter number of cluster ic , iie is the proportion of 
internal links in the whole graph and ia  is the ex-
pected proportion of ic ?s edges calculated as the 
total number of degrees in ic divided by the total of 
all degrees (2*the number of all edges) in the 
whole graph.  This value has been widely used as 
an index to evaluate the accuracy of clustering re-
sults. 
1.2 Karate club simulation 
However, it would be an exaggeration to regard 
Modularity Q is an almighty tool for accurately 
determining the attribution value of each vertex in 
a graph cluster.  That is only true for modularity-
based greedy algorithms that select vertices pair-
ings be merged into a cluster at each step of the 
tree-form integration process based on modularity 
optimization criterion.  However, such methods 
suffer from the problem that once a merger is exe-
cuted based on a discrimination error, there is no 
chance of subsequently splitting pairings that be-
long to different subgroups. 
This fatal error can be illustrated as follows.  
Zachary?s famous ?Karate Club? is often used as 
supervised data for graph clustering, because the 
complex relationships among the club members are 
presented as a graph composed of edges represent-
ing acquaintances and vertices coded indicating 
final attachments to factions.  If the results of 
graph clustering were to match with the actual 
composition of sects within the club, one could 
claim that the tested method was capable of simu-
lating the social relationships. 
However, the real difficulties lie at boundary 
positions.  It is worth pointing out that the degree 
of ambiguity is the same (0.5) for both vertices 3 
and 10 in Figure I, indicating that they occupy neu-
tral positions while in reality they belong to differ-
ent subgroups.  All modularity-based greedy algo-
rithms would inevitably bind the two nodes at an 
earlier step in the dendrogram construction (at the 
first merging step in experiments conducted by 
Newman and Danon and at the second in Pujol?s 
experiment).  In contrast, MCL is one of the rare 
clustering methods that avoids this type of mis-
judgment (accurate results for the karate club net-
work were also obtained with the Ward method), 
even though the modularity Q value for MCL is a 
little lower (0.371) than values for greedy algo-
rithms (for example, 0.3807 for Newman et al?s 
fast algorithm and 0.418 for Danon et al?s modi-
fied algorithm). 
 
Figure I Karate club  
 
The karate club case suggests the possibility of 
using both graph clustering and modularity Q from 
different perspectives.  MCL allows us to regard 
both clustering and discrimination on the same 
plan if we do not treat modularity Q as an optimi-
zation index but rather as an index of structuring 
dynamics balancing assembly and division.  To the 
extent that a graph clustering method is evaluated 
in terms of its effectiveness in a variety of dis-
crimination analyses with learning data extracted 
from real situations, it should be useful as a simu-
lation tool.  For example, it is possible to test with 
the karate club network the effects of supplement-
ing the network by adding to the original graph 
another hub with the highest degree value.  As the 
curvature value of this new hub varies according to 
the selection of vertices which become adjacent to 
it, we can re-execute MCL for the overall graph to 
see how curvature is closely related with how it 
influences clustering results.  In general cases, the 
hub of a whole graph also tends to be the represen-
tative node for the large-sized Markov cluster 
called the ?core cluster? (Jung, 2006). 
902
Let us imagine that a highly influential new-
comer joins the karate club and tries to contact 
with half (17) of all the members, functioning as a 
hub within the network.  Even though this is a 
purely hypothetical situation, it is possible to pre-
dict the impact on the network with MCL.   
 
 
 
 
Figures II, III Hub to high or low degree nodes 
 
For example, one could classify the 34 vertices 
into higher and lower degree subgroups, and set a 
hub that is adjacent to all vertices for one subgroup 
but is far from the other subgroup.  MCL results 
would indicate that even when adding a hub with 
the highest curvature value, it would be ineffectual 
in preventing a split (Figure II). However, if the 
newcomer were to be a friend with less sociable 
members, the club would be saved from being torn 
apart.  A hub connected with the lower degree sub-
group, and thus having the lowest curvature value, 
would become part of the largest core cluster, be-
cause the MCL would not subdivide the graph 
(Figure III).  In short, the results of MCL computa-
tion hinge on the curvature value of the hub with 
the highest degree value. 
2 The basic concept of BMCL 
This connection-sensitive feature of MCL brings 
us back to the limitations that Van Dogen et al 
inform their software users of.  Do these limita-
tions really render the MCL unsuitable for tree 
graphs?  Should we not regard a low modularity Q 
value for a graph as a positive attribute if it is due 
to the low curvature value for a hub?  In a very real 
sense, these questions are actually asking about the 
same thing.  The point can be clearer if conceived 
of in relation to a non-directed and cascading type 
of three-layer graph, as depicted in Figure IV. 
Figure IV Three-layer tree-form network 
 
The root node at the top (the hub) is linked to all 
the vertices in the intermediate layer but to none at 
the bottom layer, even though there are moderate 
levels of connectivity between the layers.  Connec-
tions within a layer are extremely rare or absent.  
Clearly, the curvature of the hub would be influ-
enced by the very low connectivity within layer 2. 
 
 0.01 0.02 0.03 
0.1 
1core 
cluster & 
singleton 
clusters 
1core cluster & 
singleton clus-
ters 
1cluster 
(not divided) 
0.15 
1cluster or 2 
core 
clusters 
1cluster 
(not divided) 
1cluster 
(not divided) 
0.2 2core clusters 
2core 
clusters 
1cluster 
(not divided) 
Table I. MCL results for the structured Random Graph 
903
We have executed computations at least 10 
times under the same condition in order to generate 
this type of structured random graph with 500 ver-
tices in the two layers respectively.  A random 
graph was produced by using a binominal distribu-
tion.  Although between connection rates were var-
ied from 0.1 to 0.2 and within connection rates for 
the intermediate layer from 0.01 to 0.03, no edges 
were inserted into the lower layer.  MCL results 
obtained for this architecture are almost constant, 
as shown in Table I. 
In this experiment, all singleton clusters con-
sisted of vertices belonging to layer 2.  In cases 
where the whole graph was split into 2 core clus-
ters, one cluster would correspond to the hub plus 
layer 3 while the other would correspond to layer 2.  
There was no exception when the between connec-
tion rate was 0.2.  This means that, quite curiously, 
the hub formed a core cluster around itself with 
vertices that were not all adjacent to it, so that ones 
that were connected with it in the raw data were all 
segregated into the other cluster.  In this case, the 
Modularity Q value for each core cluster was zero 
or extremely low. 
Nevertheless, in spite of this inaccuracy, this 
type of network can easily be by modified by the 
BMCL method that we discuss later.  It can be 
indirectly subdivided by graph clustering, if inside 
the same cluster, a latent shortcut is set between 
one vertex and another.  Such a latent connection 
can be counted in place of a path of length 2 that is 
traced in the original adjacency as a detour via a 
vertex of another cluster.  If all latent adjacency 
relationships are enumerated in this way, except 
for those for the hub, the core cluster will be re-
clustered by a second application of the MCL to 
realize a sort of hierarchical clustering (in this case 
for a quasi-tree graph), which has been regarded as 
being a limitation with the MCL. 
This principle can be called Branching Markov 
Clustering (BMCL) in the sense that it makes it 
possible to correct for unbalances in cluster-sizes 
by dividing large Markov clusters into appropriate 
branches.  In other words, BMCL is a way of re-
building adjacency relationships "inside" MCL 
clusters, by making reference to "outside" path in-
formation.  It then becomes natural to realize that 
the lower the curvature value of the hub is?
reflecting sparse connectivity inside the hub?s clus-
ter?the more effective BMCL will be in subdivid-
ing the core cluster, which will augment the modu-
larity Q value for the clustering results. 
3 Applying BMCL corpora data 
3.1 The BMCL algorithm 
In this section, we apply our BMCL method to a 
semantic network that is almost exhaustively ex-
tracted from typical documents of a specific struc-
ture.  It is supposed that if the MCL is applied to 
word association or co-occurrence data it will yield 
concept clusters where words are classified accord-
ing to similar topics or similar meanings as para-
digms.  However, because the word distribution of 
a corpus approximately follows Zipf?s law and 
produces a small-world scale-free network (Stey-
vers et al, 2005), the MCL will result in a biased 
distribution of cluster sizes, with a few extraordi-
narily large core clusters that lack any particular 
features. 
In order to overcome such difficulties in build-
ing appropriate lexical graphs for corpus data, we 
propose an original way of appropriately subdivid-
ing core clusters by taking into account graph coef-
ficients, especially the curvature of a hub word.  
As mentioned above, BMCL is most effective for 
clusters that, containing a high-degree and low-
curvature vertex, display a local part of a network 
with highly sparse connectivity when a hub is 
eliminated.  This feature increases the efficiency of 
the BMCL by making it possible to introduce 
moderate connection rates for latent adjacencies. 
In contrast to a ?real? adjacency between the ver-
tices ki, represented here by 1),( =kid , the ?latent? 
adjacency 1),( =jid v  will subsequently be defined 
to closely adapt to the connection state for the 
dataset, which we will utilize in testing the BMCL.  
The hub hM of each Markov cluster M is supposed 
to be the vertex with the largest degree for M .  
Here, we set a sufficiently large core cluster C , a 
set of hubs H and the hub of C as hC .Under such 
conditions, we can formulize the set of external 
hubs bypassing the intra-core connections jiK ,  as; 
}1),(),(,,|{ ,,, ==?? kjdkidKkHKK jijiji , 
where C
hh CjCiji
ji
???
?
,,
, , HCh ? . We also propose an 
additional function called 
n
ArgTopn , which identi-
fies the set of n nodes that have the highest connec-
904
tion values.  This is to produce a moderate connec-
tion rate which allows us to execute appropriate 
MCL operations by appropriately setting two prun-
ing thresholds, ?p and ?q.  These are applied in the 
row direction by fixing i  in the intra-core connec-
tion matrix to the number of the shortest paths be-
tween ji, -- || , jiK -- to make the following prun-
ing rule: 
1),(
|)|&&|(| ,,
=>?
??
=
jid
KArgTopnjKif
v
ji
n
pji
q?
?
 
This rule extracts from the intra-core connec-
tion matrix a latent adjacency matrix to which the 
MCL is applied once again in order to obtain ap-
propriately resized sub-clusters from a huge core 
cluster. 
3.2 A range of corpus data 
In this section, three documents were selected tak-
ing into consideration the curvature value of a hub 
with the highest degree and the density of connec-
tions with or without this hub among the vertices 
of a core cluster at the level of a raw data graph. 
I. Associative Concept Dictionary of Japanese 
Words (Ishizaki et al, 2001), hereafter abbreviated 
as ACDJ, which consists of 33,018 words and 
240,093 word pairing collected in an association 
task involving 10 participants.  Of these, 9,373 
critical words were selected to create well-arranged 
semantic network by removing the rarest 1-degree 
dangling words and rarer words with a degree of 2 
but curvature values of 0. 
II. Gakken?s Large Dictionary of Japanese (Kin-
daichi & Ikeda, 1988), hereafter abbreviated as 
GLDJ, which is an authoritative Japanese diction-
ary with some features of an encyclopedia in terms 
of its rich explanatory texts and copious examples.  
We selected 98,083 words after removing noise 
words, functional words, and 1,321 isolated words 
to extract word pairs by combining every head-
word with every other headword included within 
an entry text. 
III. WordNet. We used only the "data.noun" file 
where the lexical information for each noun is de-
fined by a set of index numbers corresponding not 
with words themselves but with their senses. The 
co-occurrence relationships for 98,794 meanings 
were extracted from every data block that contains 
a series of indexes, which also covers other parts-
of-speech. 
The principle for building a semantic network 
for each of these documents was to select relevant 
?word pairs? or ?index pairs? indicating the lexical 
relationships of adjacency, association or co-
occurrence, respectively.  Table II presents graph 
information for the three data sets and the results 
of applying both the MCL and the BMCL to them. 
 
 
Table II Data about the three corpora 
 
Although the first data (ACDJ) is much smaller, 
it is worthwhile executing because it represents a 
concrete example of the network type discussed 
earlier, namely, a three-layer architecture around a 
hub (quasi-tree graph).  The connection rate in the 
core cluster is very low (0.002 with and 0 without 
the hub), as is the modularity Q value for the MCL 
(0.094).  However, subdivision of the core cluster 
in the BMCL results yielded a high modularity Q 
value (0.606) when latent adjacencies derived from 
bypassing connections with a threshold of q? =3 
were used. 
The last two data (GLDJ and WordNet) are di-
rectly comparable because they are quite similar in 
size and provide sharp contrast, particularly in 
terms of curvature values (GLDJ: 8.51106E-05 << 
WordNet: 0.0405), and modularity Q values for the 
MCL (GLDJ: 0.176 << WordNet: 0.841).  For 
WordNet, the high connection rate in the core clus-
ter (0.03) makes it difficult for it to be subdivided 
by any clustering method, even if the hub is elimi-
nated.  In terms of the GLDJ, the core cluster was 
repeatedly divided by the BMCL and the modular-
ity for the subdivision turned out to be 0.2214 with 
a threshold of p?  = 1. 
However, there is another way to split the core 
cluster into sub graphs, which does not require the 
use of the latent adjacency information which is 
crucial for the BMCL.  That other method, which 
can be called the ?Simply-Repeated MCL (SR-
MCL)?, involves applying the MCL once again to 
ACDJ GLDJ WordNet
Num of Vertices 9373 98083 98794
Degree Mean 19.963 13.8939 63.7155
Hub Word House Archaic Words Individual
Degree of Hub 563 12959 2773
Curvature of Hub 0.0398 8.51106E-05 0.0405
Core Cluster Size 158 8962 2597
Connection Rate of Core Cluster 0.0022 0.000328782 0.030539
Ibid (Without Hub) 0 0.000153119 0.03005
Q for the First MCL 0.0946409 0.176 0.841275
Q for the BMCL 0.606284 0.221 -0.094
905
the part of the original adjacency matrix that corre-
sponds to the vertices apart from the hub, and 
which become members of the core cluster as a 
result of the first MCL.  In the case of ACDJ, it is 
impossible to execute the SR-MCL, because there 
is no edge that is not connected to the hub within 
the core cluster, and so all the vertices apart from 
the hub would be isolated if the hub were removed. 
A similar problem is also encountered with the 
core cluster of the GLDJ, even though the SR-
MCL increases the modularity Q value (0.769) 
much more than the BMCL.  Vertices that dangle 
from the hub?37% of the core members?would 
be dropped from the second MCL computation if 
the latent adjacency is not used, which, on the 
other hand, assures a high recall rate (0.88).  Thus, 
we have adopted an eclectic way to maintain both 
relatively high recall (the proportion of non-
isolated nodes) and relatively high precision (the 
modularity Q of the intra-core clustering).  This is 
what we may call a ?Mixed BMCL? which in-
volves combining the latent adjacency matrix ex-
clusively for the vertices dangling to the hub and 
the raw adjacency part matrix for the remaining 
ones that are connected among them.  As Figure V 
highlights, the F-measure 
RP
PR
?? +? )1(
(R: recall; P: 
precision) underscores the effectiveness of the 
Mixed BMCL for the GLDJ. 
 
Figure V Comparison of the methods (? =0.4) 
 
4 Conclusion 
This paper has examined MCL outputs obtained 
for some rather problematic conditions, such as the 
clustering of a tree graph and clustering for a net-
work that contains a hub that has a very low curva-
ture value.  In such cases, many of the vertices ad-
jacent to the hub are removed from the cluster that 
it represents.  However, compensating for that, the 
hub cluster will absorb many other vertices?some 
of which are not directly connected to the hub it-
self?to form a large-sized core cluster.  That is 
when our proposed method of Branching MCL 
(BMCL) is most effective in adjusting cluster sizes 
by utilizing latent adjacency.  Subdivision of the 
core cluster can facilitate the interpretation of the 
classified concepts. 
When the curvature of the hub is a little higher 
than in such extreme conditions, the combination 
of the ordinary MCL and the BMCL (a Mixed 
BMCL) can work well in increasing the F-Measure 
score.  However, it is not possible to reapply the 
MCL to a dense core cluster that is organized 
around a hub with a very high curvature value.  A 
direction for further research will be to automati-
cally select from between the BMCL and the 
Mixed-BMCL.  The SR-MCL or similar modifica-
tions may yield the optimal approach to dividing 
massive Markov clusters into appropriate subsets. 
References 
Clauset, A, Newman M.E.J., and Moore, C. Find-
ing Community Structure in Very Large Net-
works, Phys. Rev. E 70, 066111 (2004) 
Danon, L., Diaz-Guilera, A., and Arenas, A. Effect 
of Size Heterogeneity on Community Identifica-
tion in Complex Networks, J. Stat. Mech. 
P11010 (2006) 
Dorow, B. et al Using Curvature and Markov 
Clustering in Graphs for Lexical Acquisition and 
Word Sence Discrimination, MEANING-
2005,2nd Workshop organized by the MEAN-
ING Project, February,3rd-4th.(2005) 
Kindaichi, H., Ikeda, Y. Gakken?s Large Diction-
ary of Japanese, GAKKEN CO, LTD. (1988) 
Newman M. E. J. and Girvan M., Finding and 
evaluating community structure in networks, 
Physical Review E 69. 026113, (2004) 
Okamoto, J., Ishizaki, S. Associative Concept Dic-
tionary and its Comparison with Electronic Con-
cept Dictionaries, 
http://afnlp.org/pacling2001/pdf/okamoto.pdf, 
(2001). 
Pujol, J.M., B?jar, J. and Delgado, J. "Clustering 
Algorithm for Determining Community Struc-
ture in Large Networks".Physical Review E 74 
(2007):016107 
Van Dongen, S. Graph Clustering by Flow Simula-
tion. PhD thesis, University of Utrecht. (2000) 
 
0
0.2
0.4
0.6
0.8
1
1.2
Recall Precision F-Measure
SR-MCL
BMCL
Mixed BMCL 
906
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 57?60
Manchester, August 2008
Random Graph Model Simulations of Semantic Networks for Associative Concept Dictionaries 
Hiroyuki Akama Tokyo Institute of Technology 2-12-1 O-okayama Meguro-ku Tokyo 152-8550, Japan akama@dp.hum.titech.ac.jp Terry Joyce Tama University 802 Engyo Fujisawa-shi Kanagawa-ken 252-0805, Japan terry@tama.ac.jp 
Jaeyoung Jung Tokyo Institute of Technology 2-12-1 O-okayama Meguro-ku Tokyo 152-8550, Japan catherina@dp.hum.titech.ac.jp Maki Miyake Osaka University 1-8 Machikaneyama-cho Toyonaka-shi Osaka 560-0043, Japan mmiyake@lang.osaka-u.ac.jp 
 Abstract 
Word association data in dictionary form can be simulated through the combina-tion of three components: a bipartite graph with an imbalance in set sizes; a scale-free graph of the Barab?si-Albert model; and a normal distribution con-necting the two graphs.  Such a model makes it possible to simulate the complex features in degree distributions and the interesting graph clustering results that are typically observed for real data. 1 Modeling background Associative Concept Dictionaries (ACDs) consist of word pair data based on psychological ex-periments where the participants are typically asked to provide the semantically-related re-sponse word that comes to mind on presentation of a stimulus word. Two well-known ACDs for English are the University of South Florida word association, rhyme and word fragment norms (Nelson et al, 1998) and the Edinburgh Word Association Thesaurus of English (EAT; Kiss et al, 1973). Two ACDs for Japanese are Ishizaki?s Associative Concept Dictionary (IACD) (Oka-moto and Ishizaki, 2001) and the Japanese Word Association Database (JWAD) (Joyce, 2005, 2006, 2007). While there are a number of practical applica-tions for ACDs, three are singled out for mention                                                 ? 2008. Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.  
here. The first is in the area of artificial intelli-gence, where ACDs can contribute to the devel-opment of intelligent information retrieval sys-tems for societies requiring increasingly sophisti-cated navigation methods. A second application is in the field of medicine, where ACDs could be used in developing systems that seek to prevent dementia by checking higher brain functions with a brain dock. Finally, within educational settings, ACDs can greatly facilitate language learning through the manifestation of inherent cultural modes of thinking. The typical format of an ACD is to list the stimulus words (cue words) and their response words together with some statistics relating to the word pairing. The stimulus words are generally basic words determined in advance by the ex-perimenter, while the response words are seman-tically associated words provided by respondents on presentation of the stimulus word. The statis-tics for the word pairing include, for example, measured or calculated indices of distance or perhaps some classification of the semantic rela-tionship between the pair of words. In order to mathematically analyze the struc-ture of ACDs, the raw association data is often transformed into some form of graph or complex network representation, where the vertices stand for words and the edges indicate an associative relationship (Joyce and Miyake, 2007). However, to our knowledge, there have been no attempts at mathematically simulating an ACD as a way of determining in advance the architectural design of a dictionary. One reason is that it is a major challenge to compute maximum likelihood esti-mations (MLEs) or Monte-Carlo simulations for graph data (Snijder, 2005). Thus, it is extremely difficult to predict dependences for unknown 
57
factors such as the lexical distribution across a predetermined and controllable dictionary framework starting simply from a list of basic words. Accordingly, we propose an easier and more basic approach to constructing an ACD model by combining random graph models to simulate graph features in terms of degree distri-butions and clustering results.  2 Degree distributions for ACDs 2.1 Typical local skew It is widely known that Barab?si and Albert (1999) have suggested that the degree distributions of scale-free network structures correspond to a power law, expressed as 
r
ddxP
?
== )(  (where d stands for degree and 
r  is a small number, such as 2 or 3). This type of distribution is also known as Zipf's law describing the typical frequency distribution of words in a document and plots on a log scale as a falling diagonal stroke. However, in the degree distribution of ACDs, there is always a local skew, as a local peak or bump with a low hemline. Figure 1 presents two degree distributions; for the IACD (upper) ( r  = 1.8) and the JWAD (lower) ( r  = 2.3).                        Figure 1. Degree distributions for actual data  The plots indicate a combination of heteroge-neous distributions, consisting of a single degree 
distribution represented as a bell form with a steep slope on the right side. However, what is most interesting here is that throughout the dis-tribution range the curves remain regular and continuous, with an absence of any ruptures or fractures both before and after the local peaks. When actual ACD data is examined, one finds that as response words are not linked together, almost all the words located in the skewed part are stimulus words (which we refer to as peak words in this study), while the items before the local peak are less frequent response words that have a strong tendency to conform to a decaying distribution. It is therefore relatively natural to divide all word pairs into two types of graph: either a bipartite graph for new response words that are not already part of the stimulus list and a graph that conforms to Zipf's law for the fre-quencies of response words that are already pre-sent in the stimulus list. For the first type, new response words are represented as nodes only with incoming links, generating a bipartite graph with two sets of different sizes. This bipartite graph would exhibit the decaying distribution due to low-frequency response words prior to the local peak. In the second type of graph, response words are represented as nodes with both incom-ing and outgoing links. This second type is simi-lar to a scale-free graph, such as that incorpo-rated within the Barab?si-Albert (BA) model. 2.2 Bipartite Graph and BA Model A bipartite graph is a graph consisting of vertices that are divided into two independent sets, S and R, such that every edge connects to one S vertex and one R vertex. The graph can be represented by an adjacency matrix with diagonal zero sub-matrices, where the values of the lower right sub-matrices would all be zero were it not for the appearances of some stimulus words as response words. The lower right section is exactly where the extremely high degrees of hubs are produced, which far exceed the average numbers of response words. Thus, we adopt an approach to generating a scale-free graph that reflects Zipf's law for fre-quency distributions. According to the BA model, the probability that a node receives an additional link is proportional to its degree. Here, we im-plement the principle of preferential attachment formulated by Bollob?s (2003): 
?
=
+
=?
t
T
ttx
TdxmdNxP
1
1
)(/)()(    (1), 
0.00001
0.0001
0.001
0.01
0.1
1
1 10 100 1000
k
P
(
k
)
data
k^(-r)
0.00001
0.0001
0.001
0.01
0.1
1
1 10 100 1000
k
P
(
k
)
data
k^(-r)
58
with the addition of one condition that is specific to ACDs, which we explain below. The BA model starts with a small number, 
0
m  of vertices, and at each time step, T , a new vertex with m  edges is added and linked to m  different vertices that already exist in the graph. 
1+t
N  represents a random set of m  early vertices, )(id
t
 the degree of vertex 
i
 in the process at time t . The probability that a new vertex will be connected to a vertex 
i
 depends on the connectivity of that vertex, as expressed by Equation (1). However, we specifically assume that m  is a random natural number that is smaller than 
0
m , because in actual data the ratio of stimulus words among all responses words for each stimulus word is obviously far from constant. Moreover, the graph for the BA model here should be regarded as being a directed graph, because the very reason that hubs emerge within semantic network representations of ACDs is that the number of incoming edges is much larger than the expected number of nodes for each possible in-degree. In contrast, out-degree is limited by the number of responses for each stimulus word 
i
, which is represented as )(ic . Let )(ic  follow a normal distribution with a mean cm and a small variance value 2?  (which is not constant but nearly so) to smoothly combine the distribution of the bipartite graph and the power distribution. If a directed adjacency matrix for the network exclusively between stimulus words is expressed as 
)(
ij
BD
, then the sum of the non-zero values for each row in a random bipartite graph introducing new response words will be 
?
?
?
i
ij
BDiC )()(
 (The vertices of stimulus words with the subscript j are linked with the vertex of the stimulus word i). Thus, new response words?words that are not stimulus words?will be randomly allocated within a bipartite graph according to Equation (2): 
?
?
?
?==
i
ij
BDicrliP ))()(()1),((
1            (2), where r is the approximate number of such words. Equation (2) will yield the lower left and the upper right sections of the complete adja-cency matrix A  for the ACD model. The subse-quent sub-matrix tP  refers to the transposition of the prior sub-matrix P . The adjacency matrix in Equation (3) represents a pseudo bipartite graph structure where the upper left section is a zero sub-matrix (because there are no intercon-
nections among new response words), but the lower right section is not. Here, 
ij
B  (not )(
ij
BD , but the undirected counterpart to it), which corre-sponds to the BA model, is taken as a subsection of the adjacency matrix that must be non-directed for the whole composition. 
?
?
?
?
?
?
?
?
=
ij
t
BP
PO
A           (3) 
The key to understanding Equation (3) is to real-ize that P  is conditionally dependent on 
ij
B , because we assume a normal distribution for the number of non-zero values at each row in the lower section of A . 2.3 Simulation Results Taking into account the approximate numbers of possible new response words, in other words, the balance in sizes between the two sets in the bipartite graph, we built a composition of partial random graphs that could represent an adjacency matrix of the ACD model. Figure 2 presents one of the results obtained for the following conditions:
3000,1,5,3,10,90
0
====== rcmmt
m
? .  As the Figure shows, the local peak and the accompanying hemline in the degree distribution are clearly simulated by the complex combination of random graphs. 
10 20 30 40
-8
-6
-4
-2
  Figure 2. Degree distribution of an ACD model  The degree distribution for the artificial net-work is consistent with the features observed for actual ACD data, where more than 96% of the stimulus words in each data set are distributed across the peak section of the degree distribution, which is why we have referred to them as peak words. Moreover, it is easy to verify that without the assumption of a normal distribution for )(ic , distinct fractures emerge in the artificial curve where new response words in the bipartite struc-
Local peak 
59
ture would be distinguished from stimulus words located at initial points of the local peak. 3 Markov Clustering of ACDs 3.1 MCL This section introduces the graph clustering method that is applied to both the real and artifi-cial ACD data in order to compare them. Markov Clustering (MCL) proposed by Van Dongen (2001) is well known as a scalable unsupervised cluster algorithm for graphs that decomposes a whole graph into small coherent groups by simu-lating the probability movements of a random walker across the graph. It is believed that when MCL is applied to semantic networks, it yields clusters of words that share certain similarities in meaning or appear to be related to common con-cepts. 3.2 MCL Results The clustering results for the ACD model created by combining random graphs reveal that each of the resultant clusters contains only one stimulus word surrounded by several response words. This result is somewhat strange because there are dense connections between stimulus words, which would lead us to assume that clusters would have multiple stimulus word. However, the results of applying MCL clustering to the graph for the ACD model are in reality highly influenced by the sub-structure of the bipartite graph and less dependent on the scale-free structure. Nevertheless, the result is quite similar to results observed with real data. On examining MCL clustering results for different ACD semantic networks, we have observed that MCL clusters tend to consist of one word node with a relatively high degree and some other words with relatively low degrees. On closer inspection of the graph, it is possible to see several supporter nodes that gather around one leader node, forming a kind of small conceptual community. This suggests that the highest degree word for each cluster becomes a representative for that particular cluster consisting of some other low degree words. In short, MCL clustering is executed based on such high degree words that tend to have relatively low curvature values (Dorow, 2005) compared to their high average degree values. 
4 Conclusion In this paper, we have proposed a basic approach to simulating word association dictionary data through the application of graph methodologies. This modeling is expected not only to provide insights into the structures of real ACD data, but also to predict, by manipulating the model pa-rameters, possible forms for future ACDs. Future research will focus on constructing an exponen-tial random graph model for ACDs based on Markov Chain Monte Carlo (MCMC) methods. References Barab?si, Albert-L?szl? and R?ka Albert. 1999. Emergence of scaling in random networks, Science. 286:509-512. Bollob?s, B?la. 2003. Mathematical Results on Scale-free Random Graphs, http://www.stat.berk eley.edu/~aldous/Networks/boll1.pdf Dorow, Beate et al 2005. Using Curvature and Markov Clustering in Graphs for Lexical Acquisi-tion and Word Sense Discrimination, MEANING-2005,2nd Workshop organized by the MEANING Project, February,3rd-4th. Joyce, Terry and Maki Miyake. 2007. Capturing the Structures in Association Knowledge: Application of Network Analyses to Large-Scale Databases of Japanese Word Associations, Large-Scale Knowl-edge Resources. Construction and Application, Springer Verlag:116-131. Kiss, G.R., Armstrong, C., Milroy, R., and Piper, J. 1973. An associative thesaurus of English and its computer analysis, In Aitken, A.J., Bailey, R.W. and Hamilton-Smith, N. (Eds.), The Computer and Literary Studies, Edinburgh University Press. Nelson, Douglas L., Cathy L. McEvoy, & Thomas A. Schreiber. 1998. The University of South Florida word association, rhyme, and word fragment norms, Retrieved August 31, 2005, from http://www.usf.edu/FreeAssociation Okamato, Jun and Shun Ishizaki. 2001. Associative Concept Dictionary and its Comparison Electronic Concept Dictionaries. PACLING2001-4th Confer-ence of the Pacific Association for Computational Linguistics:214-220. Snijders, Tom A.B., Philippa E. Pattison, Garry L.Robins, Mark S. Handcock, 2005. New Specifi-cations for Exponential Random Graph Models, http://stat.gamma.rug.nl/SnijdersPattisonRobinsHandcock2006.pdf Steyvers, Mark and Josh Tenenbaum. 2005. The Large-Scale Structure of Semantic Networks, Sta-tistical Analyses and a Model of Semantic Growth, Cognitive Science. 29 (1):41-78. 
60
 61
