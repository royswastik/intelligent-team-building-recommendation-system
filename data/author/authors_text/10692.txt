BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 108?109,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
CBR-Tagger: a case-based reasoning approach to the 
gene/protein mention problem 
 
Mariana Neves Monica Chagoyen 
Biocomputing Unit Biocomputing Unit 
Centro Nacional de Biotecnolog?a - CSIC Centro Nacional de Biotecnolog?a - CSIC 
Madrid, 28049, Spain Madrid, 28049, Spain 
mlara@cnb.csic.es monica.chagoyen@cnb.csic.es 
 
Jos? M. Carazo Alberto Pascual-Montano 
Biocomputing Unit Departamento de Arquitectura de Computadores 
Centro Nacional de Biotecnolog?a - CSIC Facultad de Ciencias F?sicas, UCM 
Madrid, 28049, Spain Madrid, 28040, Spain 
carazo@cnb.csic.es pascual@fis.ucm.es 
 
Abstract 
This work proposes a case-based classifier to tackle 
the gene/protein mention problem in biomedical lit-
erature. The so called gene mention problem con-
sists of the recognition of gene and protein entities in 
scientific texts. A classification process aiming at 
deciding if a term is a gene mention or not is carried 
out for each word in the text. It is based on the selec-
tion of the best or most similar case in a base of 
known and unknown cases. The approach was 
evaluated on several datasets for different organisms 
and results show the suitability of this approach for 
the gene mention problem. 
1 Introduction 
This paper proposes a new method to the gene 
mention problem by using a case-based reasoning 
approach that performs a binary classification 
(gene mention or not) for each word in a text. In a 
first step cases are stored in two bases (known and 
unknown cases), followed by a search in these 
bases for the case most similar to the problem. The 
classification decision is given by the class of the 
case selected. The system was developed using 
Java and MySQL technologies and is available for 
download as part of the Moara project1.  
2 Proposed method 
The method here proposed identifies gene men-
tions in a text by means of classifying each token 
                                                          
1 http://biocomp.cnb.csic.es/~mlara/moara/index.html 
into two possible classes: gene mention or not. The 
system consists of two main steps: the construction 
of the case bases, and the testing phase, when the 
test dataset is presented to the system to identify 
the possible mentions. The words extracted from 
the training documents were the tokens used to 
construct the two case bases, one for known cases 
and the other for unknown cases, as proposed for 
the part-of-speech tagging problem in (Daelemans, 
Zavrel, Berck, & Gillis, 1996). 
The known cases are the ones used by the sys-
tem to classify those words that are not new, i.e. 
those that have were present in the training dataset. 
The attributes used to represent a known case are 
the word itself, the class of the word (if it is a gene 
mention or not), and the class of the preceding 
word (if it is a gene mention or not). 
The system uses a second case base to decide 
about words that are unknown to the system, i.e. 
those that are not present in the training set. The 
attributes of the unknown cases were the shape of 
the word, the class of the word (if it is a gene men-
tion or not), and the class of the preceding word (if 
it is a gene mention or not). Note that instead of 
saving the word itself, a shape of the word is kept 
in order to allow the system to be able to classify 
unknown words by means of looking for cases 
with similar shape. The shape of the word is given 
by its transformation in a set of symbols according 
to the type of character found.  
In the construction of cases, each word repre-
sents a single case, and in order to account for 
repetitions, the frequency of the case is incre-
mented to indicate the number of times that it ap-
pears in the training dataset. The training 
108
documents are read twice, one in the forward (from 
left to right), and one in the backward (from right 
to left) directions, in order to allow a more variety 
of cases.  This is important as the classification of 
a token may be influenced by its preceding and 
following words.  
CBR-Tagger has also been trained with addi-
tional corpora in order to better extract mentions 
from different organisms. These extra corpora are 
the datasets for gene normalization of the BioCrea-
tive task 1B (Hirschman, Colosimo, Morgan, & 
Yeh, 2005) for to yeast, mouse and fly and the 
BioCreative 2 Gene Normalization task (Morgan & 
Hirschman, 2007) for human.  
In the classification procedure, the text is token-
ized and a sliding window is applied first in the 
forward and then in the backward direction. In 
each case, the system keeps track of the class of 
the preceding token (false at the beginning), gets 
the shape of the token and tries to find in the bases 
a case most similar to it. The search procedure is 
divided in two parts, for the known and unknown 
cases. Priority is always given to the known cases 
since it saves the word exactly as they appeared in 
the training documents and the classification may 
be more precise than using the unknown cases.  
A token already classified as positive by the 
forward reading may be used for the backward 
reading as preceding class and might help recog-
nizing mentions composed by many tokens that 
would not have been totally recognized by one of 
the reading procedures only. After the identifica-
tion of the best case for each token, some post-
processing procedures are executed to check 
boundaries (for mentions composed of more than 
one token) as well as abbreviations and corre-
sponding full names. 
3 Results 
The results obtained with the BioCreative 2 gene 
mention task for the CBR-Tagger are shown in 
Table 1 along with the best result of the competi-
tion. Results are showed according to the datasets 
used for the training of the CBR-tagger: BioCrea-
tive 2 Gene Mention task (Wilbur, Smith, & Ta-
nabe, 2007) corpus only (CbrBC2), and the 
combination of it with the BioCreative task 1B 
gene normalization corpus (Hirschman et al, 2005) 
for the yeast (CbrBC2y), mouse (CbrBC2m), fly 
(CbrBC2f) and the three of them (CbrBC2ymf). 
 
Taggers P R FM 
CbrBC2 77.8 75.9 76.9 
CbrBC2y 82.7 52.6 64.7 
CbrBC2m 83.1 47.1 60.1 
CbrBC2f 82.0 65.9 73.0 
CbrBC2ymf 82.5 39.7 53.6 
Best BC2 result 88.5 86.0 87.2 
Table 1: Results for the BC2 gene mention task. 
 
CBR-Tagger has also been applied to the gene 
normalization problem in conjunction with two 
other available taggers: Abner2 and Banner3. Table 
2 summarizes the best mix of taggers configuration 
for each organism. Detailed results may be found 
at the author?s research page4. 
 
Organism Best configuration 
Yeast Abner+CbrBC2 
Mouse Abner+CbrBC2m 
Fly CbrBC2f 
Human Banner+CbrBC2ymf 
Table 2: Best taggers for each organism. 
Acknowledgments 
This work has been partially funded by the Spanish 
grants BIO2007-67150-C03-02, S-Gen-0166/2006, 
TIN2005-5619. APM acknowledges the support of 
the Spanish Ram?n y Cajal program. The authors 
acknowledge support from Integromics, S.L. 
References  
Daelemans, W., Zavrel, J., Berck, P., & Gillis, S. 
(1996). MBT: A Memory-Based Part of Speech Tag-
ger-Generator. Paper presented at the Fourth Work-
shop on Very Large Corpora, Copenhagen, Denmark. 
Hirschman, L., Colosimo, M., Morgan, A., & Yeh, A. 
(2005). Overview of BioCreAtIvE task 1B: normal-
ized gene lists. BMC Bioinformatics, 6 Suppl 1, S11. 
Morgan, A., & Hirschman, L. (2007). Overview of Bio-
Creative II Gene Normalization. Paper presented at 
the Second BioCreative Challenge Evaluation Work-
shop, Madrid-Spain. 
Wilbur, J., Smith, L., & Tanabe, L. (2007). BioCreative 
2. Gene Mention Task. Paper presented at the Second 
BioCreative Challenge Evaluation Workshop, Madrid, 
Spain. 
                                                          
2 http://pages.cs.wisc.edu/~bsettles/abner/ 
3 http://banner.sourceforge.net/ 
4 http://biocomp.cnb.csic.es/~mlara/mention.html 
109
Proceedings of the Workshop on BioNLP: Shared Task, pages 68?76,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Extraction of biomedical events using case-based reasoning 
 
 
Mariana L. Neves Jos? M. Carazo 
Biocomputing Unit Biocomputing Unit 
Centro Nacional de Biotecnolog?a - CSIC Centro Nacional de Biotecnolog?a - CSIC 
C/ Darwin 3, Campus de Cantoblanco,  
28049, Madrid, Spain 
C/ Darwin 3, Campus de Cantoblanco,  
28049, Madrid, Spain 
mlara@cnb.csic.es carazo@cnb.csic.es 
 
Alberto Pascual-Montano 
Departamento de Arquitectura de Computadores 
Universidad Complutense de Madrid, Facultad de 
Ciencias F?sicas 
28040, Madrid, Spain 
pascual@fis.ucm.es 
 
Abstract 
The BioNLP?09 Shared Task on Event Extrac-
tion presented an evaluation on the extraction 
of biological events related to genes/proteins 
from the literature. We propose a system that 
uses the case-based reasoning (CBR) machine 
learning approach for the extraction of the enti-
ties (events, sites and location). The mapping 
of the proteins in the texts to the previously ex-
tracted entities is carried out by some simple 
manually developed rules for each of the argu-
ments under consideration (cause, theme, site 
or location). We have achieved an f-measure of 
24.15 and 21.15 for Task 1 and 2, respectively. 
1 Introduction 
The increasing amount of biological data gener-
ated by the high throughput experiments has 
lead to a great demand of computational tools to 
process and interpret such amount of informa-
tion. The protein-protein interactions, as well as 
molecular events related to one entity only, are 
key issues as they take part in many biological 
processes, and many efforts have been dedicate 
to this matter. For example, databases are avail-
able for the storage of such interaction pairs, 
such as the Molecular INTeraction Database 
(Chatr-aryamontri et al, 2007) and IntAct 
(Kerrien et al, 2007). 
In the field of text mining solutions, many ef-
forts have been made. For example, the Bio-
Creative II protein-protein interaction (PPI) task 
(Krallinger, Leitner, Rodriguez-Penagos, & Va-
lencia, 2008) consists of four sub-tasks, includ-
ing the extraction of the protein interaction pairs 
in full-text documents, achieving an f-measure 
of up to 0.30. The initiative of annotation of 
both Genia corpus (J. D. Kim, Ohta, & Tsujii, 
2008) and BioInfer (Pyysalo et al, 2007) is an-
other good example. 
The BioNLP?09 Shared Task on Event Ex-
traction (J.-D. Kim, Ohta, Pyysalo, Kano, & 
Tsujii, 2009) proposes a comparative evaluation 
for the extraction of biological events related to 
one or more gene/protein and even other types 
of entities related to the localization of the re-
ferred event in the cell. The types of events that 
have been considered in the shared task were 
localization, binding, gene expression, transcrip-
tion, protein catabolism, phosphorylation, regu-
lation, positive regulation and negative 
regulation. A corpus that consisted of 800, 150 
and 260 PubMed documents (title and abstract 
text only) was made available for the training, 
development test and testing datasets, respec-
tively. For all documents, the proteins that took 
part in the events were provided. 
The shared task organization proposed three 
tasks. Task 1 (Event detection and characteriza-
tion) required the participants to extract the 
events from the text and map them to its respec-
68
tive theme(s), as an event may be associated to 
one or more themes, e.g. binding. Also, some 
events may have only a gene/protein as theme, 
e.g. protein catabolism, while some other may 
be also associated to another event, e.g. regula-
tion events. Task 2 (Event argument recognition) 
asked the participants to provide the many ar-
guments that may be related to the extracted 
event, such as its cause, that may be an anno-
tated or one of the previously extracted events. 
Other arguments include site and localization, 
which should be first extracted from the texts by 
the system, as they do not come annotated in the 
documents. Task 3 (Recognition of negation and 
speculations) evaluates the presence of negations 
and speculation related to the previously ex-
tracted events. 
Our group has participated in this shared task 
with a system implemented with the case-based 
reasoning (CBR) machine learning technique   
as well as some manual rules. We have pre-
sented results for tasks 1 and 2 exclusively. The 
system described here is part of the Moara pro-
ject1 and was developed in Java programming 
language and use MySQL database. 
2 Methods  
Case-based reasoning (CBR) (Aamodt & Plaza, 
1994) is the machine learning method that was 
used for extracting the terms and events here 
proposed and consists of first learning cases 
from the training documents, by means of saving 
them in a base of case, and further retrieving a 
case the most similar to a given problem during 
the testing step, from which will be given the 
final solution, hereafter called ?case-solution?. 
One of the advantages of the CBR algorithm is 
the possibility of getting an explanation of why 
to a given token has been attributed a certain 
category, by means of checking the features that 
compose the case-solution. Additionally, and 
due to the complexity of the tasks, a rule-based 
post-processing step was built in order to map 
the previously extracted terms and events among 
themselves.  
 
 
 
                                                          
1 http://moara.dacya.ucm.es 
2.1 Retaining the cases 
In this first step, documents of the training data-
set are tokenized according to spaces and punc-
tuations. The resulting tokens are represented in 
the CBR approach as cases composed of some 
predefined features that take into account the 
morphology and grammatical function of the 
tokens in the text as well as specific features 
related to the problem under consideration. The 
resulting cases are then stored in a base of case 
to be further retrieved (Figure 1). 
 
 
Figure 1: Training step in which cases are repre-
sented by some pre-defined features and further 
saved to a base. 
 
Regarding the features that compose a case, 
these were the ones that were considered during 
the training and development phases: the token 
itself (token); the token in lower case (lower-
case); the stem of the token (stem); the shape of 
the token (shape); the part-of-speech tag 
(posTag); the chunk tag (chunkTag); a biomedi-
cal entity tag (entityTag); the type of the term 
(termType); the type of the event (eventType); 
and the part of the term in the event (eventPart). 
The stem of a token was extracted using an 
available Java implementation2 of the Porter al-
gorithm (Porter, 1980), while the part-of-speech, 
chunk and bio-entity tags were taken from the 
GENIA Tagger (Tsuruoka et al, 2005). 
The shape of a token is given by a set of char-
acters that represent its morphology: ?a? for 
lower case letters, ?A? for upper case letters, ?1? 
for numbers, ?g? for Greek letters, ?p? for stop-
                                                          
2 http://www.tartarus.org/~martin/PorterStemmer 
69
words3, ?$? for identifying 3-letters prefixes or 
suffixes or any other symbol represented by it-
self. Here are some few example for the shape 
feature: ?Dorsal? would be represented by ?Aa?, 
?Bmp4? by ?Aa1?, ?the? by ?p?, ?cGKI(alpha)? 
by ?aAAA(g)?, ?patterning? by ?pat$a? (?$? 
symbol separating the 3-letters prefix) and ?ac-
tivity? by ?a$vity? (?$? symbol separating the 4-
letters suffix). No repetition is allowed in the 
case of the ?a? symbol for the lower case letters. 
 
 
Figure 2: Example of the termType, eventType and 
partEvent features. 
 
The last three features listed above are specific 
to the event detection task and were extracted 
from the annotation files (.a1 and .a2) that are 
part of the corpus. The termType feature is used 
to identify the type of the term in the event prob-
lem, and it is extracted from the term lines of 
both annotation files .a1 and .a2, i.e. the ones 
which the identifiers starts with a ?T?. The 
eventType features represent the event itself and 
it is extracted from the event lines of .a2 annota-
tion file, i.e. the ones that starts with an ?E?. Fi-
nally, eventPart represents the token according 
to its role, i.e. entity, theme, cause, site and loca-
tion. The termType, eventType and eventPart 
features are the hereafter called ?feature-
problem?, the features that are unknown to the 
system in the testing phase and which values are 
to be given by the case-solution. Figure 2 illus-
trate one example of these features for an extract 
of the annotation of the document ?1315834? 
from the training dataset. 
Usually, one case corresponds for each token 
of the documents in the training dataset. How-
ever, more than one case may be created from a 
token, as well as none at all, depending on the 
predefined features. For example, some tokens 
may derive in more than one case due to the 
shape feature, as for example, ?patterning? 
                                                          
3 
http://www.dcs.gla.ac.uk/idom/ir_resources/linguistic_utils/ 
(?pat$a?, ?a$ing?, ?a?). Also, according to the 
retaining strategy, some tokens may be associ-
ated to no case at all, for example, by restricting 
the value of a determined feature as the retaining 
strategy. In order to reduce the number of re-
tained cases, and consequently reduce the further 
retrieving time, only those tokens related to an 
event are retained, i.e., tokens with not null 
value for the termType feature. 
The text of a document may be read in the 
forward or backward direction during the train-
ing step, and even combining both of them 
(Neves, Chagoyen, Carazo, & Pascual-Montano, 
2008). Here, we have considered the forward 
direction exclusively. Also, another important 
point is the window of tokens under considera-
tion when setting the features of a case, if taking 
into account only the token itself or also the sur-
rounding tokens, the ones which come before or 
after it. Here we consider a window of (-1,0), 
i.e., for each token, we get the feature of the to-
ken itself and of the preceding one, exclusively.  
 
Training Testing Features / Tokens 
-1 0 -1 0 
stem 9 9 9 9 
shape  9  9 
posTag 9 9 9 9 
chunkTag     
entityTag 9 9 9 9 
termType 9 9 9 9 
eventType 9 9 9  
partEvent 9 9 9  
Table 1: Selected features in the training and testing 
steps for the tokens ?0? and ?-1?. The last three fea-
tures are the ones to be inferred.  
 
Many experiments have been carried out in or-
der to choose the best set of features (Table 1). 
The higher the number of features under consid-
eration, the greater is the number of cases to be 
retained and the higher is the time needed to 
search for the case-solution. He relies therefore 
the importance of choosing a small an efficient 
set of features. For this reason, the shape fea-
tures has not been considered for the preceding 
token (-1) in order to reduce the number of 
cases, as this shape usually result in more than 
one case per token. The termType feature is at 
the same time known and unknown in the testing 
step. It is know for the protein terms but is un-
70
known for the remaining entities (events, sites 
and locations).  
By considering these features for the 800 
documents in the training set, about 26,788 
unique cases were generated. It should be noted 
that no repetition of cases with the same values 
for the features are allowed, instead a field for 
the frequency of the case is incremented to keep 
track of the number of times that it has appeared 
during the training phase. The frequency range 
goes from 1 (more than 22,000 cases) to 238 
(one case only). 
2.2 Retrieving a case  
When a new document is presented to the sys-
tem, it is first read in the forward direction and 
tokenized according to space and punctuation 
and the resulting tokens are mapped to cases of 
features, exactly as discussed in the retaining 
step. The only difference here is the set of fea-
ture (cf. Table 1), as some of them are unknown 
to the system and are the ones to be inferred 
from the cases retained during the training step.  
 
 
Figure 3: Retrieval procedure to choose the most 
case-solution with higher frequency and based on 
MMF and MFC parameters.  
 
For each token, the system first creates a case 
(hereafter called ?case-problem?) based on the 
testing features and proceeds to search the base 
of cases for the case-solution the most similar to 
this case-problem (Figure 3). It should be noted 
that a token may have more than one case-
problem, depending of the values of the shape 
feature. The best case-solution among the ones 
found by the system will be the one with the 
higher frequency. The system always tries to 
find a case-solution with the higher number of 
features that have exactly the same value of the 
case-problem?s respective features. The stem is 
the only mandatory feature which value must be 
always matched between the case-problem and 
the case-solution. The value of the two features-
problem (eventType and partEvent) will be 
given by the values of the case-solution?s re-
spective features. If no case solution is found, 
the token is considered of not being related to 
the event domain in none of its parts (entity, 
theme, cause, etc.).  
Two parameters have been taken into consid-
eration in the retaining strategy: the minimum 
matching feature (MMF) and the minimum fre-
quency of the case (MFC). The first one set the 
minimum features that should be matched be-
tween the case-problem and the case-solution, as 
the higher the number of equal features between 
theses cases, the more precise is the decision 
inferred from the case-solution.  
On the other hand, the MFC parameter re-
stricts the cases that are to be considered by the 
search strategy, the ones with frequency higher 
than the value specified by this parameter. The 
higher the minimum frequency asked for a case, 
the lower is the number of cases under consid-
eration and the lower is the time for obtaining 
the case-solution. From the 26,788 cases we 
have retained during the training phase, about 
22,389 of them appeared just once and would 
not be considered by the searching procedure if 
the MFC parameter was set to 2, for example,  
therefore reducing the searching time.  
Experiments have been carried out in order to 
decide the values for both parameters and it re-
sulted that a better performance is achieved (cf. 
3) by setting the MFC to a value higher than 1. 
On the other hand, experiments have shown that 
the recall may decrease considerably when re-
stricting the MMF parameter. 
By repeating this procedure for all the tokens 
of the document, the latter may be then consid-
ered as being tagged with the event entities. 
However, in order to construct the output file 
required by the shared task organization, some 
manual rules have been created in order to map 
the events mapped to its respective arguments, 
as described in the next section. 
2.3 Post-processing rules 
For the tasks 1 and 2, the participants were 
asked to output the events present in the pro-
vided texts along with their respective argu-
ments. The events have been already extracted 
in the previous step; the tokens that were tagged 
as ?Entity? for the ?partEvent? feature (cf. Fig-
71
ure 2), hereafter called ?event-entity?. This en-
tity is the start point from which to search for the 
arguments which are incrementally extracted 
from the text in the following order: theme, 
theme 2, cause, site and location. Figure 4 re-
sumes the rules for each of the arguments. 
 
 
Figure 4: Resume of the post-processing rules for 
each type of argument.  
 
Themes: The theme-candidates for an event-
entity are the annotated proteins (.a1 file) as well 
as the events themselves, in the case of the regu-
lation, positive regulation and negative regula-
tion events. The first step is then to try to map 
each event to its theme and in case that no theme 
is found, the event is not considered anymore by 
the system and it is not printed to the output file. 
The theme searching strategy starts from the 
event-entity and consists of reading the text in 
both directions alternatively, one token in the 
forward direction followed by one token in the 
backward direction until a theme-candidate is 
found (Figure 5). The system halts if the end of 
the sentence is found or if the specified number 
of tokens in each direction is reached, 20 for the 
theme. By analyzing some of the false negatives 
returned from the experiments with the devel-
opment dataset, we have learned that few events 
are associated to themes present in a different 
sentence and although aware these cases, we 
have decided to restrict the searching to the sen-
tence boundaries in order to avoid a high num-
ber of false positives.  
In the case of a second theme, allowed for 
binding events only, a similar searching strategy 
is carried out, except that here the system reads 
up of 10 tokens in each direction, starting from 
the theme entity previously extracted. 
Cause: The cause-candidates are also the an-
notated proteins and, starting from the event-
entity, a similar search is carried out, restricted 
up to 30 tokens in each direction and to the 
boundaries of the same sentence. This procedure 
is carried out for the regulation, positive regula-
tion and negative regulation events only and the 
only extra restriction is that the candidate should 
not be the protein already assigned as theme. If 
no candidate is found, the system considers that 
there is no cause associated to the event under 
consideration. 
Site and Location: Here the candidates are 
the tokens tagged with the values of ?Entity? for 
the termType feature, and ?Site? and ?Location? 
for the partEvent feature, respectively. The 
search for the site is carried out for the binding 
and phosphorylation events and the location 
search for the localization event only. The pro-
cedure is restricted to the sentence boundaries 
and up to 20 and 30 tokens, respectively, starting 
from the event-entity. Once again, if not candi-
date is found, the system consider that there is 
no site or location associated to the event under 
consideration. 
 
 
Figure 5: Contribution of each class of error to the 
275 false positives analyzed here. 
3 Results  
This section presents the results of the experi-
ments carried out with the development and the 
blind test datasets as well as an analysis of the 
false negatives and false positives. Results here 
will be presented for tasks 1 and 2 in terms of 
precision, recall and f-measure. 
Experiments have been carried out with the 
development dataset in order to decide the best 
value of the MMF and MFC parameters (cf. 
2.2). Figure 6 shows the variation of the F-
measure according to both parameters for the 
values of 1, 3, 4, 5, 6, 7 and 8 for MMF; and 1, 
2, 5, 10, 15, 20 and 50 for MFC. 
Usually, recall is higher for a low value of 
MFC, as the searching for the case-solution is 
72
carried out over a greater number of cases and 
the possibility of finding a good case-problem is 
higher. On the other hand, precision increases 
when few cases are under considered by the 
search strategy, as fewer decisions are taken and 
the cases-solution have usually a high frequency, 
avoiding decision based on ?weak? cases of fre-
quency 1, for example.  
Figure 6 shows that the best value for MFC 
ranges from 2 to 20 and for MMF from 5 to 7 
and the best f-measure result is found for the 
values of 2 and 6 for these parameters (f2m6), 
respectively. As these experiments have been 
carried out after the deadline of the test dataset, 
the run that was submitted as the final solution 
was the one with the values of 2 and 1 for the 
MFC and MMF parameters (f2m1), respectively. 
Table 3 and 4 resumes the results obtained for 
the test dataset with the configuration that was 
submitted (f2m1), and the best one (f2m6) after 
accomplishing the experiments above described. 
Results have slightly improved by only trying to 
choose the best values for the parameters here 
considered.  
 
F-Measure
7
9
11
13
15
17
19
21
23
1 3 4 5 6 7 8
Minimum matching features
1 2 5 10 15 20 50
 
Figure 6: F-Measure for the development dataset in 
terms of the MFC (curves) and the MMF (x-axis). 
 
An automatic analysis of the false positives and 
false negatives has been performed for the de-
velopment dataset and for the results obtained 
with the final submission (f2m1), a total of 2502 
false positives and 1300 false negatives. We 
have found out that the mistakes are related 
mainly to the retrieving of the case-solution and 
to the mapping of an event to its arguments. The 
mistakes have been classified in seven groups 
described below and figures 7 and 8 show the 
percent contribution of each class for the false 
positives and false negatives, respectively. 
Events composed of more than one token 
(1): this mistake happens when the system is 
able to find the event with its correct type and 
arguments but with only part of its tokens, such 
as ?regulation? instead of ?up-regulation? and 
?reduced? or ?levels? instead of ?reduced lev-
els?, both in document 10411003. This is mainly 
due to our tokenization strategy of separating the 
tokens according to all punctuation and symbols 
(including hyphens) and also due to the evalua-
tion method that seems not consider alternatives 
to the text of an event. This mistake always re-
sults in one false positive and one false negative. 
 
tasks /  
results recall precision f-measure 
(f2m1) 28.63 20.88 24.15 task 1 
(f2m6) 27.18 23.92 25.45 
(f2m1) 25.02 18.32 21.15 task 2 
(f2m6) 24.49 21.63 22.97 
Table 3: Results for the test dataset (tasks 1 and 2). 
 
(f2m1) (f2m6) Results /  
Events p r fm p r fm 
prot. catab. 78.6 55.0 64.7 71.4 55.6 65.5 
phosphoryl. 49.6 56.1 52.7 46.0 55.2 50.2 
transcript. 48.9 19.8 28.1 38.7 29.6 33.5 
neg. reg. 9.8 7.9 8.8 7.9 7.7 7.8 
pos. reg. 10.0 6.6 7.9 10.2 8.0 9.0 
regulation 8.6 4.5 5.9 7.5 5.3 6.3 
localizat. 28.2 42.9 34.0 23.3 48.9 33.3 
gene expr. 51.8 55.1 53.4 52.6 61.2 56.6 
binding 19.5 12.1 14.9 22.4 14.4 17.5 
Table 4: Results by event for Task 2 on test dataset. 
 
Events and arguments in different sentences 
of the text (2):  as we already discussed in sec-
tion 2.3, our arguments searching strategy is re-
stricted to the boundaries of the sentence. Some 
examples of this mistake may be found in 
document 10395645 in which two events of the 
token ?activation [1354-1364]? is mapped to the 
themes ?caspase-6 [1190-1199]? and ?CPP32 
[1165-1170]?, both located in a different sen-
tence. This mistake usually affects only the false 
negatives but may cause also a false positive if 
the system happens to find a valid (wrong) ar-
73
gument in the same sentences for the event un-
der consideration. 
 
False Positives
case 
decision (3); 
74,3
composed 
tokens (1); 
5,2
site/location 
detection 
(7); 1,6
theme 
detection 
(5); 14,6
cause 
detection 
(6); 1,6
event type 
(4); 2,7
 
Figure 7: Percent contribution of each error to the 
false positives. 
 
False Negatives
theme 
detection 
(5); 56,2
site/location 
detection 
(7); 0,7
cause 
detection 
(6); 4,2
event type 
(4); 10,0
different 
sentences 
(2); 1,4
composed 
tokens (1); 
10,4
case 
decision (3); 
17,2
 
Figure 8: Percent contribution of each error to the 
false negatives. 
 
Decision for a case (3): this class of error is due 
to the selection of a wrong case-solution and we 
include in this class mistakes due to two situa-
tions: when the system fails to find any case-
solution for an event token (false negative) or 
when a case-solution is found for a non-event 
token (false positive). The first situation is only 
dependent of the searching strategy and its two 
parameters (MMF and MFC) while the second 
one is also related to the post-processing step, if 
the latter succeeds to find a theme for the incor-
rectly extracted event. An example of a false 
negative that falls in this group is ?dysregulation 
[727-740]? from document 10229231 that failed 
to be mapped to a case-solution. Regarding the 
false positives, this class of mistake is the major-
ity of them and it is due to the low precision of 
the system that frequently is able to find cases-
solution associated to tokens that are not events 
at all, such as the token ?transcript [392-402]? of 
document 10229231. It should be noted that the 
incorrect association of a token to a case-
solution does not result in a false positive a pri-
ori, but only if the post-processing step happen 
to find a valid theme to it, a mistake further de-
scribed in group 5. 
Wrong type of the event (4): this class of 
mistake is also due to the wrong selection of a 
case-solution, but the difference here is that the 
token is really an event, but the case-solution is 
of the wrong type, i.e. it has a wrong value for 
the eventType feature. The causes of this mis-
take are many, such as, the selection of features 
(cf. Table 1) or the value of the MFC parameter 
that may lead to the selection of a wrong but 
more frequent case. We also include in this 
group the few false negatives mistakes in which 
a token is associated to more than one type of 
event in the gold-standard, such as the token 
?Overexpression [475-489]? from document 
10229231 that is associated both to a Gene Ex-
pression and to a Positive Regulation event. One 
way of overcome it would be to allow the sys-
tem to associated more than one case to a token, 
taking the risk of decreasing the precision. 
Theme detection (5): in this group falls more 
than half of the false negatives and we include 
here only those mistakes in which the token was 
correctly associated to a case-solution of the cor-
rect type. These mistakes may be due to a vari-
ety of situations related to the theme detection, 
such as: the association of the event to another 
event when it should have been done to a protein 
or vice-versa (for the regulation events); the 
mapping of a binding event to one theme only 
when it should have been two theme or vice-
versa; the association of the event to the wrong 
protein theme, especially when there is more 
than one nearby; and even not being able to find 
any theme at all. Also, half of theses mistakes 
happen when an event is associated to more than 
one theme separately, not as a second theme. For 
example, the token ?associated [278-288]?, from 
document 10196286, is associated in the gold 
standard to three themes ? ?tumor necrosis fac-
tor receptor-associated factor (TRAF) 1 [294-
351]?, ?2 [353-354]? and ?3 [359-360]? ? and 
74
we were only able to extract the first of them. 
This is due to the fact that we restrict the system 
to search only one ?first? and one ?second? 
theme for each event. 
Cause detection (6): similar to the previous 
class, these mistakes happens when associating a 
cause to an event (regulation events only) when 
there is no cause related to it or vice-versa. For 
example, in document 10092805, the system has 
correctly mapped the token ?decreases [1230-
1239]? to the theme ?4E-BP1 [1240-1246]? but 
also associated to it an inexistent cause ?4E-BP2 
[1315-1321]?. The evaluation of Task 2 does not 
allow the partial evaluation of an event and 
therefore a false positive and a false negative 
would be returned for the example above. 
Site/Location detection (7): this error is 
similar to the previous one but related only to 
binding, phosphorylation and localization 
events, when the system fails to associate a site 
or a location to an event or vice-versa. For ex-
ample, in document 10395671, the token ?phos-
phorylation [1091-1106]? was correctly mapped 
to the theme ?Janus kinase 3 [1076-1090]? but 
was also associated to an inexistent site ?DNA 
[1200-1203]?. Once again, the evaluation of 
Task 2 does not allow the partial evaluation of 
the event and a false positive and a false nega-
tive would be returned. 
We have also carried out an evaluation of our 
own in order to check the performance of our 
system only on the extraction the entities (event, 
site and location), not taking into account the 
association to the arguments. Table 5 resumes 
the values of precision, recall and f-measure for 
each type of term. The high recall confirm that 
most of the entities were successful extracted 
although the precision is not always satisfactory, 
proving that the tagging of the entities is not as 
hard a task as it is the mapping of the arguments. 
Additional results and more a detailed analysis 
of the errors may be found at Moara page4. 
4 Conclusions 
Results show that our system has performed 
relatively well using a simple methodology of a 
machine learning based extraction of the entities 
and manual rules developed for the post-
                                                          
4 http://moara.dacya.ucm.es/results_shared_task.html 
processing step. The analysis of the mistakes 
presented here confirms the complexity of the 
tasks proposed but not the extraction of the 
event terms (cf. Table 5). 
We consider that the part of our system that 
requires most our attention is the retrieval of the 
case-solution and the theme detection of the 
post-processing step, in order to increase the 
precision and recall, respectively. The decision 
of searching for a second theme and of associat-
ing a single event separately to more than one 
theme is hard to be accomplished by manual 
rules and could better be learned automatically 
using a machine learning algorithm. 
 
(f2m1) (f2m6) Events 
p r fm p r fm 
prot. catab. 70.8 89.5 79.1 69.6 84.2 76.2 
phosphoryl. 75.0 94.7 83.7 79.1 89.5 84.0 
transcript. 22.7 75.9 34.9 36.4 74.6 48.9 
neg. reg. 26.4 56.5 36.0 25.3 43.5 32.0 
pos. reg. 24.3 63.7 35.2 26.5 59.1 36.6 
regulation 20.8 65.9 31.7 22.1 52.5 31.1 
localizat. 47.7 79.5 59.6 49.1 66.7 56.5 
gene expr. 46.5 83.4 59.7 50.8 80.2 62.2 
binding 29.7 71.1 41.9 29.7 64.4 40.7 
entity 12.5 55.3 20.4 16.8 50.0 25.1 
TOTAL 27.5 69.2 39.4 30.9 62.9 41.4 
Table 5: Evaluation of the extraction of the event and 
site/location entities for the development dataset. 
 
The automatic analysis of the false positive and 
false negative mistakes is a hard task since no 
hint is given for the reason of the mistake by the 
evaluation system, if due to the event type or to 
wrong theme, an incorrectly association to an 
event or even a missing cause or site.  
Acknowledgments 
This work has been partially funded by the 
Spanish grants BIO2007-67150-C03-02, S-Gen-
0166/2006, PS-010000-2008-1, TIN2005-5619. 
APM acknowledges the support of the Spanish 
Ram?n y Cajal program. The authors acknowl-
edge support from Integromics, S.L. 
References  
Aamodt, A., & Plaza, E. (1994). Case-Based Reason-
ing: Foundational Issues, Methodological Varia-
tions, and System Approaches. AI 
Communications, 7(1), 39-59. 
75
Chatr-aryamontri, A., Ceol, A., Palazzi, L. M., 
Nardelli, G., Schneider, M. V., Castagnoli, L., et 
al. (2007). MINT: the Molecular INTeraction da-
tabase. Nucleic Acids Res, 35(Database issue), 
D572-574. 
Kerrien, S., Alam-Faruque, Y., Aranda, B., Bancarz, 
I., Bridge, A., Derow, C., et al (2007). IntAct--
open source resource for molecular interaction 
data. Nucleic Acids Res, 35(Database issue), 
D561-565. 
Kim, J.-D., Ohta, T., Pyysalo, S., Kano, Y., & Tsujii, 
J. i. (2009). Overview of BioNLP'09 Shared Task 
on Event Extraction. Paper presented at the Pro-
ceedings of Natural Language Processing in Bio-
medicine (BioNLP) NAACL 2009 Workshop, 
Boulder, CO, USA. 
Kim, J. D., Ohta, T., & Tsujii, J. (2008). Corpus an-
notation for mining biomedical events from litera-
ture. BMC Bioinformatics, 9, 10. 
Krallinger, M., Leitner, F., Rodriguez-Penagos, C., & 
Valencia, A. (2008). Overview of the protein-
protein interaction annotation extraction task of 
BioCreative II. Genome Biol, 9 Suppl 2, S4. 
Neves, M., Chagoyen, M., Carazo, J. M., & Pascual-
Montano, A. (2008). CBR-Tagger: a case-based 
reasoning approach to the gene/protein mention 
problem. Paper presented at the Proceedings of 
the BioNLP 2008 Workshop at ACL 2008, Co-
lumbus, OH, USA. 
Porter, M. (1980). An algorithm for suffix stripping. 
Program, 14(3), 130-137. 
Pyysalo, S., Ginter, F., Heimonen, J., Bjorne, J., 
Boberg, J., Jarvinen, J., et al (2007). BioInfer: a 
corpus for information extraction in the biomedi-
cal domain. BMC Bioinformatics, 8, 50. 
Tsuruoka, Y., Tateishi, Y., Kim, J.-D., Ohta, T., 
McNaught, J., Ananiadou, S., et al (2005). De-
veloping a Robust Part-of-Speech Tagger for 
Biomedical Text. Paper presented at the Advances 
in Informatics - 10th Panhellenic Conference on 
Informatics. 
 
 
76
