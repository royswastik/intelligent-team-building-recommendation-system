SpokenDialogueforVirtualAdvisersinasemi-imme rsiveCommand
andControlenvironment
DominiqueEstival,MichaelBroughton,AndrewZschor n,ElizabethPronger
HumanSystemsIntegrationGroup,CommandandContro lDivision
DefenceScienceandTechnologyOrganisation
POBox1500,EdinburghSA5111
AUSTRALIA
{Dominique.Estival,Michael.Broughton,AndrewZscho rn}@dsto.defence.gov.au


Abstract
We present the spoken dialogue system
designed and implemented for Virtual
Advisers in the FOCAL environment. Its
architectureisbasedon:DialogueAgents
using propositional attitudes, a Natural
Language Understanding component
using typed unification grammar, and a
commercial speaker-independent speech
recognition system. The current
application aims to facilitate the multi-
media presentation of military planning
information in a semi-immersive
environment.
1 Introduction
In this paper, we present the spoken dialogue
system implemented for communicating with the
virtual advisers (VAs) in the Future Operations
Centre Analysis Laboratory (FOCAL) at the
Australian Defence Science and Technology
Organisation(DSTO).Weareexperimentingwith
the use of spoken dialogue with virtual
conversational characters to access multi-media
information during the conduct of military
operations and in particular to facilitate the
planningofsuchoperations.
Unlike telephone-based dialogue systems
(Estival, 2002),whicharemainlycreated fornew
commercial applications, dialogue systems for
Command and Control applications (Moore et al
1997) generally seek to simulate the military
domain and therefore require an understanding of
thatdomain.
2 UsingVirtualAdvisersinFOCAL
FOCAL was established to "pioneer a paradigm
shiftincommandenvironmentsthroughasuperior
useof capability andgreater situationawareness".
The facility was designed to experiment with
innovativetechnologiestosupportthisgoal,andi t
hasnowbeenrunningfortwoyears.
FOCAL contains a large-screen, semi-
immersive virtual reality environment as its
primary display, allowing vast quantities of
informationtobedisplayed. OurcurrentVAscan
be described as 3-dimensional "Talking Heads",
i.e. only the head and upper portions of the body
are represented. Theycandisplayexpression, lip-
synchronisation and head movement, along with
certain autonomous behaviours such as blinking
and gaze (Taplin et al, 2001). These factors all
combinetoaddlife-likenesstotheVAsandcreate
moreengaginginteractionwithusers.
Presenting information via aTalkingHeadhas
been commercially demonstrated by the virtual
newscaster ?Ananova? (Ananova, 2002).
Embodiedcharactersarealsobeingdevelopedand
include the PPP (Andre, Rist and Muller, 1998)
and Rea (Cassell, 2000).  PPP is a cartoon style
Personalized Plan-based Presenter that combines
pointing, head movements and facial expressions
to draw the viewer?s attention to the information
beingpresented. Rea isa virtual real-estateagen t
that takesanactive role inconversation, shenods
herheadtoindicateunderstandingofspokeninput,
orcanraiseherhandtoindicateadesiretospeak .
Several VAs have been implemented for
FOCAL, each having a particular role or
knowledge expertise.  For example, one adviser
may have specialist knowledge relating to legal
issues, another may have information relating to
the geography of a region.  Each VA has a
differentfacialappearance,voiceandmannerisms.
To demonstrate and evaluate the performance
of VAs (and of the other FOCAL projects), a
fictitious scenario has been developed that
incorporates key elements of military planning at
the operational level (see section 8).  The VAs
provide information rich briefs through the
combineduseofspokenoutputviaText-to-Speech
(TTS)andmultimedia.  Relevantquestionscanbe
asked at the end of the briefs through the use of
spokendialogue.
3 Previousimplementation:Franco
Asdescribedin(Taplinetal.,2001)thefirstVA in
FOCAL, named Franco, was also an animated 3-
dimensional "Talking Head" model, intended to
either deliver prepared information, such as a
briefing or slide show, or to interact
conversationally with users. To demonstrate the
conversational functionality (Broughton et al,
2002), it was implemented with a commercial
speaker-dependent automated speech recogniser
(ASR),DragonNaturallySpeaking?.TheNatural
Language understanding component was
implemented in NatLink (Gould, 2001) and a
simpleuser-drivendialoguemanagement,basedon
key-word recognition and nesting of dialogue
statestoprovidecontext,wasalsoimplementedin
Python.
Franco has been successful in demonstrating
the proof-of-concept of a VA in the FOCAL
environment.  Answering spoken questions about
specific military assets and platforms, it also
permits the display of other types of information
such as pictures, animated video clips, tabular
information from a database, and location details
ondigitalmaps.
4 Improvements
Although Francowas successful in demonstrating
the potential usefulness of a VA in a Command
andControlenvironment foroperationalplanning,
it suffers from certain limitations which we are
nowaddressinginafollow-upproject.
The first limitation, and the easiest to remedy,
was the unnaturalness of the synthetic voice we
had given Franco.  For greater effectiveness, we
had to provide ourVAwith amore natural voice
andwithanAustralianaccent. Wechosethenew
Australian TTS voice from Rhetorical, developed
byAppen (rVoice,2002).   This requiredmaking
somechanges, some of them relatively important,
to the interface with the talking head model to
achieve lip-synchronisation, but that aspect of the
workwillnotbeaddressedinthispaper.
The second limitation was the relative rigidity
of the dialogue management strategy we were
using.  The alternative approach we have
developed is to create Dialogue Agents
implemented in A TTITUDE.  This is described in
section6.
The third limitation was due to the speaker-
dependent nature of the ASR.  While a speaker-
dependent ASR allows greater flexibility in the
input towhich theVAcanrespond,wewantedto
develop a system which could not only be
demonstratedby the fewpeoplewhohave trained
the speech recogniser, but where visitors
themselvescouldbeparticipantsandcouldinteract
with theVA. Switching toa speaker-independent
ASR led us to radically modify our Spoken
Language Understanding component, and this is
describedinsection7.
The new implementationwe describe here has
allowed us not only to address those three
limitations, but also to alter fundamentally the
architectureofthesystem,openingupthedialogue
managementcomponentstocontrolandinteraction
by other tools and agents in the FOCAL
environment.  The resulting system is now fully
modular and provides scalability as well as
flexibility.
Thisnewimplementationallowsustofocusour
research into dialogue management issue, to
investigate the use of A TTITUDE  for dialogue
management and to experimentwithmore natural
languageinput.
5 Integration
Communication between the various components
ofthesystem(speechrecogniser,dialoguecontrol,
virtual adviser control andmultimedia display) is
nowachievedwith theCoABS (ControlofAgent
Based Systems) Grid infrastructure (Global
InfoTek,2002).TheCoABSGridwasdesignedto
allowalargenumberofheterogeneousprocedural,
object-oriented and agent-based systems to
communicate.  Using the CoABS Grid as our
infrastructure has allowed us to integrate all the
components of the dialogue system and it will
provideaneasywaytointegrateotheragentsanda
variety of input and output devices.
Communication between CoABS agents is
accomplishedviastringmessages.
6 DialogueManagementwithA TTITUDE
ATTITUDE  is a multi-agent architecture developed
at DSTO, capable of representing and reasoning
both with uncertainty and about multiple
alternative scenarios (Lambert, 1999).  It is a
multi-agent extension of the MetaCon reactive
planner developed for control of phased array
radars on the Swedish Airborne Early Warning
aircraft(LambertandRelbe,1998). A TTITUDE has
some similarities with Prolog and other logic
programming languages as well as with AI
research on blackboard and multi-agent
architectures.  Because A TTITUDE  was designed
specificallytosupporttheprogrammingofreactive
systems, it possesses powerful facilities for
handling interactions of the internal system
entities,bothwitheachotherandwiththeexterna l
world.
ATTITUDE isveryhigh-level,weakly-typed,and
thanks to the agent paradigm, it produces loosely
coupled and modularised systems. For these
reasons, and because A TTITUDE  implements
reasoningabout propositionalattitudes ,itprovides
a very attractive framework in which to develop
and express dialogue management control
strategies.  It is worth emphasizing here that
ATTITUDE is not merely a notation to represent
speech acts or  communicative acts between
agents, but that it is actually the programming
language and environment in which both the
agents themselves and the control structure for
interaction between the agents are implemented
andexecuted.
BecauseA TTITUDE hasneverbeenusedforthis
purpose before, this is an interesting area of
research in itself, and one of the goals of the
projecthasbeentoseehowA TTITUDE needstobe
extended to implement dialogue management.
Further, this allows us to investigate how far
attitude programming  (see section 6.2) can go
towardsexpressingspeechactsandcommunicative
acttype. However,wedonotclaimtoemploythe
full power of propositional attitudes in our
implementation yet. This is another area of
researchwhichwearenowexploring. Neitherare
we yet at the stage where we could perform
automatic detection of utterance type (Wright,
1998) or of dialogue act (Carberry and Lambert,
1999;PrasadandWalker,2002).
6.1 Propositionalattitudes
The A TTITUDE  programming environment is so
named because it utilises propositional attitude
instructions  as programming instructions (this has
beendubbed attitudeprogramming ).  Propositional
attitudesareallegedmentalstatescharacterisedb y
propositional attitude expressions, which are the
means by which individuals relate their own
mentalbehaviourtoothers'.
Propositional attitude instructions are of the form
shownin(1).

(1)[subject][attitude][propositionalexpression]

In(1):
-[subject]denotesthe individualwhosemental
stateisbeingcharacterised;
- [propositional expression] describes some
propositionalclaimabouttheworld;and
- [attitude]expresses the subject'sdispositional
attitudetowardthatclaimabouttheworld.
6.2 ATTITUDEprogramming
When software agent Mary  encounters the
propositionalattitudeinstruction" Fred  desire [the
door is closed]", Mary  will issue a message to
softwareagent Fred instructing Fred  todesirethat
the door be closed. Similarly, when encountering
thepropositionalattitudeinstruction" I believe [the
sky isblue]", Mary  herselfwillattempt to believe
thattheskyisblue.
An important characteristic of A TTITUDE
programming is that each propositional attitude
instruction either succeeds or fails, possibly with
sideeffects,dependinguponwhetherthe recipient
agentisabletosatisfytheinstructionalrequest. As
each propositional attitude instruction either
succeeds or fails, the execution path selected
through a network of propositional attitude
instructions (routine) is determined by the
successesandfailuresof thepropositionalattitud e
instructionsattemptedalong theway. Thecontrol
structure is therefore governed by a semantics of
success.
Computational routines for a software agent
arise by linking together particular choices of
propositional attitude instructions.Thesenetworks
ofpropositionalattitudeinstructionsthenprescri be
recipesdefiningthepossiblementalbehaviourofa
softwareagent.
6.3 The  ATTITUDEDialogueAgents
We have implemented a number of A TTITUDE
DialogueAgents. ThemainagentinourDialogue
Management architecture (shown in Figure 1) is
the Conductor.  It is the agent responsible for the
flowofinformationbetweentheotheragentsandit
manages multi-modal interactions. The other
agents, also described further in this section, are
the Speaker, the NLG (Natural Language
Generator), the MMP (Multimedia Presenter) and
several IS(InformationSource)agents.Inaddition
to theseagents,eachdialoguestate (seesection8 )
isalsoimplementedasanA TTITUDE agent,withits
ownsetofroutines.
As explained in section 6.2, each A TTITUDE
agent?s behaviour is programmed as a set of
routines

Figure1.DialoguewithA TTITUDE

The interaction between the A TTITUDE
DialogueagentsisshowninFigure1,inwhichthe
frame around the A TTITUDE  agents can be
interpretedasrepresentingtheCoABSgrid.
SpeakerAgent
Whenspeechfromtheuserhasbeendetectedand
recognised, the attribute-value pairs for that
utterance (see section 7) are sent to Speaker.
Speaker takes that information and produces a
correspondingA TTITUDE expression,whichisthen
forwardedto Conductor.
The linguistic coverage of the system is
determinedbythegrammarswhichareavailableat
each dialogue state.  For now, the coverage is
limited to a set of utterances appropriate for the
briefing scenario described in section 8.  These
were used to define the Regulus1 grammars from
whichtheNuancegrammarsarecompiled.Weare
nowplanningtomovefromRegulus1toRegulus2,
which will allow us to derive dialogue state
grammarsfromalargeEnglishgrammarusingthe
EBLstrategydescribedin(Rayneretal.,2002b)








Conductor
Thisagentisresponsiblefordialogueflowcontrol andallothe  rdialogue
agentsmustregisterwithit.

Conductor

receivescommunicativeactsfrom
 Speaker

.Forexample:

( whquestion  (property mig- 29flying -range?value?units))
Thisqueryisforwardedontoallregisteredagents .
 Conductor

choosesthe

mostappropriateresponsereceivedandsendsthist o
 MMP

topresentthe

answer.
Speaker
Speakerreceivesspeechrecognitionresultsinthe form

ofattribute

-

valuepairs,andtranslatestheseinto

Attitudeexpressionstosendto
 Conductor

.

InformationSource
(IS)Thiscategoryofagentseachregisterwith  Conductor  and interfacewithabackgrounddatasource,forexampl e,a
databaseofaircraftproperties.

Eachusestheirdatasourcetorespondtoqueriesf rom

Conductor

.

MultimediaPresenter
(MMP)Thisagentreceivesalistofexpressionsfrom  Conductor anddirectstheappropriateservicestopresent multimediadatatotheuser.Forexample:

((whanswer (property mig -29flying - range810nautical - miles))(image mig- 29))
Inthiscase,
 MMP

requestsanEnglishformofthe
 whanswer expressionandsendstheresulttotheTTS
application.Similarly,anappropriateapplication isdirectedt

odisplaytherequestedimage.

NaturalLanguageGenerator
(NLG)Receivesexpressionsfrom  MMP andusestemplatestoreturn correspondingEnglishsentences.

Englishquestionfromuser
Nuance/Regulus

Attitudeexpression

Query

Response

Presentationdirectives

NLGdirective
TTS
Englishstring

VirtualAdvisorspeaking

Englishstring

Attribute/Valuepairs
Multimediadisplayed
ConductorAgent
Conductor  takes an A TTITUDE  expression from
Speakerandforwardsitontoallthe IS agentsthat
have registered with it.  It then waits for all the
responses to come back from those agents, in the
formoflistsofexpressions.
Every response Conductor  receives is put into
its knowledge base, along with some extra
information:
-Sender:whichISagentsenttheresponse.
- In-Reply-To: which previous communicative
actthisisaresponseto.
- Strength: whether every expression of the
response is 'strong' (the sender believes it is
either absolute truthor absolutenegation)or if
oneormore is 'weak' (the senderbelieves it is
neitherabsolutetruthnorabsolutenegation).
-Bound-State: if thereareanyfreevariablesin
theresponse,orifitisfullyground.
- Unifiability: whether one or more of the
expressionsintheresponseisofthesameform
as Speaker?s initial expression.
The final expression in Conductor?s knowledge
baseisasshownin(2).

(2)(response?in-reply-to?sender?strength
   ?bound_state?unifiability?content)

Given the initial expression from Speaker and the
replies it receives from the IS agents, Conductor
chooses the 'best' response. For example, a
response that is strong, fully ground and unifies
with Speaker?s expression is deemed to be more
relevant and informative than a response that is
weak and contains free variables.  Conductor
forwardsthisresponseto MMP.
MultimediaPresenter(MMP)
MMP  iterates through the list of expressions sent
by Conductor  andpresents eachexpression to the
user.  MMP recognises classes of expressionsand
chooses topresent themusingcertainmedia.  For
example, some expressions are instructions to
changetheVAheadmodel,whileothersaretobe
translatedintoEnglishsentencesandspokenbythe
VA. For the latter function MMP  uses NLG (see
below).
Othermediathroughwhich MMP canchooseto
present the information contained in the
expressionsinclude:imageryfromadatabase(e.g.
pictures of military platforms, or of strategic
locations), video clips, images from weather or
radar information sources, virtual video, 3-
dimensional virtual battle space maps, textual
informationandaudio.
NaturalLanguageGenerator(NLG)
For now, NLG  uses templates to transform
ATTITUDE expressionsintoEnglish.  Forexample,
the instruction in (3) provides two possible
responsesfortheA TTITUDE expressionspecified: 1

(3)(property?assetoverview?valuetext)
  whanswerpriority10
 ((response1("The"?asset"isa"?value".") )
 ((response2("Iunderstandthatthe"?asset "isa
   "?value"."))))

When NLG  is first requested to generate the
Englishoutputfortheexpressionin(4.a),intende d
to be a communicative act of type whanswer, it
uses the template given in (4.b), corresponding to
"response1"in(3), toproducetheEnglishanswer
givenin(4.c).

(4.a)(propertymig-29overview"Russianmulti-role
   fighter"text)
b.("The"?asset"isa"?value".")
c.TheMig-29isaRussianmulti-rolefighter.

When NLG  isrequestedasecond time togenerate
the output for (3), it uses the template in (5.a),
corresponding to "response 2" in (3), to produce
theEnglishanswergivenin(5.b).

(5.a)("Iunderstandthatthe"?asset"isa"?valu e".")
b.IunderstandthattheMig-29isaRussianm ulti-
rolefighter.

Thus NLG  cycles through the list of templates for
appropriateresponses. Prioritiescanalsobegive n
to templates, enabling NLG  to use general
templates togetherwithmore specificand tailored
ones.
It is clear that template-based language
generation is too rigid for fully natural dialogues ,
andweintendtoexploremoreflexible techniques
after we implement a wider coverage English
grammar;however,ithassofarbeensufficientfor
                                                         
1Variablesaredenotedwith"?",whiletextstrings (tobesent
tospeechsynthesis,ordisplayedonaslide)areb etween
doublequotes,"".
our purposes, namely to demonstrate and
investigateagent-baseddialoguemanagement.
InformationSourceAgent(IS)
The ISagents,e.g.aWeatherAgentor aPlatform
Capabilities Agent, can answer users' questions,
eitherby using their own internalknowledgebase
orbyaccessingexternalInformationSources,such
as a weather information server, or a database of
military assets.  All IS agents register with
Conductor, and when an expression is sent by
Speaker,all IS agentstrytorespondtoit.

ByusingtheCoABSGridastheinfrastructureand
implementing the agentwithA TTITUDE, we leave
the architecture extremely flexible and scalable
(Kahn and Della Torre Cicalese, 2001). For
instance, it is possible to increase the amount of
information at the system?s disposal during run-
time by launching a new IS agent and by adding
sometemplatesto NLG.
6.4 Dialoguedesign
Fornow, thedialogue is specifiedasa finite stat e
machineandisstillverymuchsystemdirected. In
thebriefingapplication (seesection8.1), theVAs
first "push" the information that needs to be
presented, as briefing officers do in a normal
briefing.Someoftheinformationisalsopresente d
using visual aids, such as power point slides and
maps for specifying location  information.  The
information to be presented and the media to be
usedaredeterminedbytheagentforthatparticula r
dialoguestate.
The VA then allows users to ask questions to
repeat or clarify particular points, or to gain
additionalinformation.
7 SpokenLanguageProcessing
7.1 Speaker-independentspeechrecognition
Asstatedinsection4,oneofthemainmotivations
formovingfromaspeaker-dependenttoaspeaker-
independentASRwastoallowvisitorsinFOCAL
the possibility of using the system themselves,
rather than relying on a small set of trained
individuals to run demonstrations.  We chose to
usetheNuanceToolkit(Nuance,2002)forseveral
reasons:  besides its reliability as a speaker-
independent ASR for both telephone and
microphone speech, Nuance 8.0 provides
Australian-New Zealand English, as well as US
andUKEnglish,acousticlanguagemodels.  Even
more importantly for our purposes, Nuance
grammarscanbecompiledfromRegulus,ahigher-
level language processing component which has
already been used to develop several spoken
dialogue systems in different domains (Rayner et
al.,2001,RaynerandBouillon,2002).
7.2 SpokenLanguageUnderstanding
Following our decision to move from a speaker-
dependent to a speaker-independent ASR, we
decided to use Regulus to implement our Natural
Language Understanding component.  Regulus is
an Open Source environment which compiles
typed unification grammars into context-free
grammar language models compatible with the
Nuance Toolkit.  It is "written in a Prolog-based
feature-value notation and compiles into Nuance
GSLgrammars."(Rayneretal.,2002a).  Regulus
isalsodescribedindetailin(Rayneretal.,2001 ).
The main motivation for using Regulus is the
usual one of greater efficiency due to the more
compact nature of a unification grammar
representation compared with a context-free
grammar.  In addition, using Regulus to define a
higherlevelgrammar,weareabletoobtainasour
semantic representation a list of attribute-value
pairs, and this permits a more sophisticated
processingoftheinformationbytheotheragents.
Regulus also allows the development of bi-
directional grammars, and we intend tomake use
ofthisfunctionalityinlaterimplementationsoft he
NLG  agent. However, fornow, thegrammarswe
have developed have been limited to recognition
andunderstanding.
8 Currentapplicationimplementation
8.1 Dialoguescenario
The scenario for the current application was
developed by members of the Human Systems
Integration (HSI) group and is grounded on their
experience with, and observations of, military
operational planning.  It is based on a fictitious
scenario developed for training (the examples
givenherehaveallbeenmodified)andexemplifies
theJointMilitaryAppreciationProcess(JMAP)for
militaryplanningacross the three services (Army,
Navy and Air Force). A sub-scenario was chosen
for the development of the spoken dialogue with
theVAs. 2
8.2 Dialogueflow
The structured nature of a military planning task
such as this onemakes it very easy to partition it
intodifferentstages,whichcanthenbemappedto
different dialogue states. In our dialogue script,
each top-level dialogue state corresponds to a
sectionoftheplanningexercise,givenin(6).

(6)Commander'sInitialGuidance
-CDF(ChiefofDefenceForces)Intent
-PlanningGuidance
-Constraints
-Restrictions
-LegalIssues
-CommandandControl

These6topleveldialoguestatesarethenfollowed
byanOverallQuestionTime.
The mixed-initiative nature of the system can
bemodelledinafinitestatediagram,allowingfor
a) briefing-like system ?pushes?, b) confirmation
queriesfromthesystemandc)questionsfromthe
user.   However, because the system is primarily
agent-based, the dialogue can also evolve
dynamically. Forinstance,oncethesystemisina
?question? state,  the dialogue flow then allows
users to ask anumberofquestions,until they are
satisfied,and thedialoguecanmove toadifferent
state.
Each of the top level dialogue states also
corresponds to an IS agent with its own set of
ATTITUDE  routines.  These agents register with
Conductor  and act as experts in their particular
fields (e.g., the Legal Issues adviser).  Theagent s
contain knowledge which they use to answer
questionsposedtothemby Conductor. Allagents
have the ability to keep track of which state (or
topic)theyarein.Thisallowsnotonly Conductor,
but also the other dialogue agents, to distinguish
between providing the user with new information
orinformationthathasalreadybeenpresented.
                                                         
2
ThisistheCommander?sinitialguidancetotheTh eatre
PlanningGroup(TPG),whichispartoftheMission Analysis
sectionofJMAP.
8.3 KnowledgeRepresentation
Thecurrentontologydevelopedforthisapplication
is only a small part of the larger Knowledge
Representationontologytobeusedthroughout the
whole FOCAL system.  For now, we only
representtheconceptsneededinoursmalldomain,
and their relationships are translated into
ATTITUDE  statements, allowing agents to draw
inferences.    For example, if a user can ask the
question given  in (7.a), it will be translated int o
the listof  attributevaluepairsgiven in (7.b)a nd
sent to Speaker. Speaker then translates these
attributevaluepairsinto theA TTITUDE expression
in(7.c)andforwardsitonto Conductor.

(7.a) Whatdepartmentoverseesnegotiations
withunionsandindustry?
b. [questionwhatquestion,concept
negotiation,attributeoversee,obj1department]
c. conductordesire(comm_act(negotiation
oversee?department)fromspeakertype
whatquestionin-response-tonull)

As described in section 6, when Conductor  poses
thequestiontotheappropriateagents,theyrespon d
with the information in their knowledge base or
information they can extract from a database.
Agentsstoreknowledgeas believe statementssuch
astheoneshownin(8):

(8) Ibelieve(negotiationoversee?department
ofworkplacerelations?)

These believe  statementsarethenunifiedwith the
propositions translated by Speaker, and if
unification is successful, a reply is sent back to
Conductor.  Finally, Conductor  passes the answer
on to NLG  to match a template and produce an
Englishanswer,forinstance(9).

(9) TheDepartmentofWorkplaceRelations
overseesnegotiationswithunionsandindustry.

An agentwhich has access to a databasecan also
translate a user's question into the relevant
databasequerytoobtaintheanswer. Animportant
issue under research concerns the automatic
derivation of A TTITUDE  statements from a pre-
existingdatabase.
8.4 SeveraldifferentVAs
As explained above, each stage of the planning
processispresentedtotheuserbyaparticularVA
withitsassociated IS agentandtheVAthenallows
users to ask further questions. Besides their
specialised knowledge, theVAs are differentiated
through different head models, different TTS
voices (maleorfemale,differentregionalaccents)
anddifferentpersonalities.
Onceadialoguestateiscompletedandtheuser
has no further questions, the VA for that state
sendsamessage to Conductor  tomovetothenext
state.  Conductor can then initiate the change in
recognition grammar, voice for the next VA and
modelforthenextVAhead.
Having several VAs coming on at different
stagestopresentdifferent informationallowsfor a
VA tobe specialised in a particular domain,  just
as real briefing officers are during a realmilitar y
planningexercise.
Fornow,weonlydisplayoneVAatatime,but
weintendtoexperimentwithhavingmultipleVAs
at the same time. The final state of the dialogue
flowallowsuserstoaskquestionsaboutanyaspect
of the planning process, and questions can be
posedtoalltheVAs,soitwouldbenaturalforth e
userstoseealltheVAsatthatstage.
8.5 RapidPrototypingandEvaluation
The key word version developed previously (see
Broughton et al, 2002) has been maintained as a
rapid prototyping environment for evaluating new
scripts and dialogues.  It allows newdialogues to
be quickly tested by entering suitable key words,
sufficient to discriminate one question from
another. Thissystemprovesfasterfortestingtha n
the more precise method of grammar building.
Multiple response strings can be generated,
providing more naturalness for those interacting
with the VAs on a regular basis.  By rapidly
prototyping questions and responses, we can test
the intuitiveness of expected questions and the
smoothness and timeliness of responses,
particularly when presented combined with
multimedia.
The implemented system described here has
so faronlybeen testedwithothermembersof the
group,butdemonstrations tovisitorsandpotential
users will provide a more rigorous  form of
evaluation on an on-going basis.  An evaluation
phase for the project is scheduled for 2003-2004,
during which time we will have access to more
users andwill be able to conductmore structured
experiments.
9 NaturalInteractionwithVAs
In addition to the ASR and TTS systems
previously discussed, other technologies can be
combined into the overall system to increase
naturalnessofinteraction,andweareinvestigatin g
speaker recognitionaswellasa rangeofpointing
technologies.
Theneed for a speaker recognition systemhas
emerged with the move to a speaker independent
ASR.WithaspeakerdependentASR,userswould
load their individual profile before use, thus
enabling the system to know who was using it.
With a speaker-independent ASR, a speaker
recognition system would allow the VAs to
recognisewho is talking to themandenable them
to address known users by name.  We plan to
integrate within FOCAL the speaker recognition
system which has been developed at DSTO
(Roberts, 1998).  This system uses statistical
modelling techniques and is capable of both
speaker identification (recognising users from a
database of stored speech profiles) and speaker
verification (verifying the identity of a particula r
user).
We are also proposing to use pointing
techniques in combination with the speech and
language technologies to build a multimodal
system.  Multimodal systems were originally
demonstrated by Bolts (1980) and research is
continuing across varied applications (e.g., Oviatt
et al, 2000 and Gibbon et al, 2000).  However,
unlike systems such as MATCH (Johnston et al,
2002), where the issue is allowing multimodal
interaction on portable devices with very small
screens, in FOCAL we are concerned with
ensuring thatusersget the full benefit of the ver y
large screen and with allowing several users to
interact at a distance from the screen.  It is also
worth mentioning that, unlike the interactive
systemdescribedin (Rickeletal.,2002),whichi s
concernedwithtraining inamilitaryenvironment,
we are not trying to simulate a complete virtual
worldwithembodiedagents.
However, we propose to include traditional
pointingtechnologies,suchasthestandarddesktop
mouse, through to3-dimensional trackingsystems
for gaze, gesture and user tracking.  This will
involve integrating more complex language
understanding, as information will need to be
derived from both the user's utterance and from
whatisbeingpointedto.Forexample,tointerpre t
an utterance such as (10) uttered while the user
points toa locationonamap,weneed toperform
reference resolution on "this region", and match
thatreferenttotheitembeingpointedat.

(10)Whatdoweknowaboutthisregion?

10 Conclusion
We have now implemented in FOCAL the
infrastructure needed to perform spoken and
multimodaldialoguewithseveralVAs. This isof
interestinitself,asitwillallowustocontinue our
research on spoken language understanding and
spokendialoguesystemsandalsotoaddressissues
of language generation which have for now been
left aside.  Already we have been able to move
from a rigid dialogue control structure, with very
constrained input, to a more flexible and scalable
control structure allowing real connectivity
betweenagents.
Havingmoved to a speaker-independent ASR,
and takingadvantageof theopensourcenatureof
Regulus, we intend to pursue research issues
regarding robustprocessing of spoken input, such
asusinggrammarspecialisationfromacorpusand
devisingtechniquesforignoringpartsoftheinput .
We have implementeda dialoguemanagement
architecture based on A TTITUDE  agents which
communicate with each other using propositional
attitude expressions.  Other agents can now be
developed to  perform additional functions, in
particular to launch the display of other types of
informationandtointerpretothertypesofinput.
This will allow us to explore how spoken
dialogue with VAs can be combined with other
virtual interaction technologies (e.g., gesture,
pointing, gaze tracking). In this respect, the next
step in our project is the development of a full
fledge MMP  agent based on the framework
describedin(ColineauandParis,2003).

However,theworkwehavereportedheremust
also be seen as part of the larger research
programmeundertakenwithinFOCAL. Fromthis
perspective, this work is of interest because it
allows othermembers of theHSI group topursue
research in the usability of new technologies to
perform the paradigm shift in command
environments.  In particular, this project is
providing the support for further research into
whether this way of presenting information is
helpful in an operational command environment.
It allows us to devise experiments to explore the
crucial issue of trust in the information being
presented,andhowtheway theinformationbeing
presentedcanaffectthattrust.
Integratingspokendialoguewithplanningtools
willalsoallowustoexplorewhetherVAscanhelp
inmilitaryoperationplanning,andhowbesttouse
thesetools.

Acknowledgements
We wish to thank the Chief of C2D, and the
Director of Information Sciences Laboratory, for
sponsoring and funding this work. We wish to
acknowledge the work of Paul Taplin in
integrating speech synthesis and lip-
synchronisation, and the work of Benjamin Fry
from the University of South Australia in
developing the Regulus/Nuance grammars.
Finallywewishtothanktheothermembersofthe
HSIgroupinC2Dfortheirconstantandinvaluable
helpwiththeFOCALproject.

References
Ananova.2002. http://www.ananova.com.
E. Andre, T. Rist, and J. Muller. 1998. Integrating
Reactive and Scripted Behaviours in a Life-Like
Presentation Agent, Proceedings of the Second
International Conference on Autonomous Agents ,
261-268.
Appen.2002. http://www.appen.com.au.
R.A.Bolt.1980."Put-that-there":voiceandgestu reat
the graphics interface . Proceedings of the
SIGGRAPH, July,262-270.
Michael Broughton, Oliver Carr, Dominique Estival,
Paul Taplin, Steven Wark, Dale Lambert.  2002.
"Conversing with Franco, FOCAL?s Virtual
Adviser". Conversation Characters Workshop,
HumanFactors2002 ,Melbourne,Australia.
Sandra Carberry and Lynn Lambert. 1999. "A Process
Model for Recognizing Communicative Acts and
ModelingNegotiationSubdialogues". Computational
Linguistics.25,1,pp.1-53
Justine Cassell. 2000. Embodied Conversational
InterfaceAgents, Communicationsof theACM ,Vol.
43,No.4,70-78.
Nathalie Colineau and C?cile Paris. 2003. Framework
fortheDesignofIntelligentMultimediaPresentati on
Systems: An architecture proposal for FOCAL.
CMISTechnicalReport03/92,CSIRO,May2003.
Dominique Estival. 2002. "The Syrinx Spoken
Language System". International Journal of  Speech
Technology. vol.5.no.1.pp.85-96.
Michael Johnston, Srinivas Bangalore, Gunaranjan
Vasireddy, Amanda Stent, Patrick Ehlen, Marilyn
Walker, Steve Whittaker, Preetam Maloor. 2002.
"MATCH: anArchitecture forMultimodalDialogue
Systems". Proceedingsofthe40thAnnualMeetingof
the Association for Computational Linguistics
(ACL'02).pp.376-383.Philadelphia..
DafyddGibbon, IngeMertins,RogerK.Moore (Eds.).
2000.  Handbook of Multimodal and Spoken
Dialogue Systems: Resources, Terminology and
ProductEvaluation. KluwerAcademicPublishers.
Global InfoTek Inc. 2002. Control of Agent Based
Systems.  http://coabs.globalinfotek.com.
JoelGould. 2001. "Implementation and Acceptance of
NatLink, aPython-BasedMacroSystem forDragon
NaturallySpeaking", The Ninth International Python
Conference,March5-8,California
Martha L. Kahn and Cynthia Della Torre Cicalese.
2001. "CoABS Grid Scalability Experiments".
Proceedings of the Second International Workshop
on Infrastructure for Agents, MAS, and Scalable
MAS, AutonomousAgents2001Conference.
Dale A. Lambert and Mikael G. Relbe.  1998.
"Reasoning with Tolerance".  2nd  International
Conference on Knowledge-Based Intelligent
ElectronicSystems .IEEE.pp.418-427.
DaleA.Lambert.1999."AdvisersWithA TTITUDE for
Situation Awareness". Proceedings of the 1999
Workshop on Defence Applications of Signal
Processing. pp.113-118, Edited A. Lindsey, B.
Moran, J. Schroeder, M. Smith and L. White.
LaSalle,Illinois.
Dale A. Lambert. 2003. "Automating Cognitive
Routines", accepted for publication in the 6th
InternationalConferenceonInformationFusion.
R.Moore, J.Dowding,H.Bratt, J. Gawron,Y.Gorfu ,
A. Cheyer. 1997. "CommandTalk: A spoken-
language interface for battlefield simulations". In
Proceedings of the Fifth Conference on Applied
NaturalLanguageProcessing,pp1-7.
Nuance.2002.http://www.nuance.com/ .
Oviatt, S., Cohen, P., Wu, L., Vergo, J., Duncan, L .,
Suhm, B., Bers, J., Holzman, T., Winograd, T.,
Landay,J.,Larson,J.,Ferro,D.2000."Designing the
user interface formultimodal speech and pen-based
gesture applications: state-of-the-art systems and
future research directions". Human Computer
Interaction.
RashmiPrasad andMarilynWalker. 2002. "Training a
Dialogue Act Tagger for Human-Human and
Human-ComputerTravelDialogues". Proceedingsof
3rdSIGDIALWorkshop .Philadelphia.pp.162-173.
Manny Rayner, John Dowding, Beth Ann Hockey.
2001. "A Baseline method for compiling typed
unification grammars into context free language
models". In Proceedings of Eurospeech 2001, pp
729-732.Aalborg,Denmark.
Manny Rayner, John Dowding, Beth Ann Hockey.
2002a."RegulusDocumentation".
Manny Rayner, Beth Ann Hockey, John Dowding.
2002b.  "Grammar Specialisation meets Language
Modelling". ICSLP2002. Denver.
Manny Rayner and Pierrette Bouillon.  2002. "A
Flexible Speech to Speech Phrasebook Translator".
Proceedings of the ACL-02 Speech-Speech
TranslationWorkshop ,pp69-76.
Jeff Rickel, Stacy Marsella, Jonathan Gratch, Randa ll
Hill,DavidTraum,WilliamSwartout.2002.Toward
aNewGenerationofVirtualHumans for Interactive
Experiences. IEEE Intelligent Systems, 1094-7167,
pp.32-38.
William Roberts. 1998. "Automatic Speaker
Recognition Using Statistical Models". DSTO
ResearchReport,DSTO-RR-0131 ,DSTOElectronics
andSurveillanceResearchLaboratory.
rVoice. 2002. Rhetorical Systems,
http://www.rhetoricalsystems.com/rvoice.html.
Paul Taplin, Geoffrey Fox, Michael Coleman, Steven
Wark, Dale Lambert. 2001.  "Situation Awareness
Using a Virtual Adviser", Talking HeadWorkshop,
OzCHI2001 ,Fremantle,Australia.
Helen Wright. 1998. "Automatic utterance type
detection using suprasegmental features".
Proceedings of the 5th International Conference on
SpokenLanguageProcessing(ICSLP'98).Sydney.
