2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 386?395,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
A Comparative Investigation of Morphological Language Modeling for the
Languages of the European Union
Thomas Mu?ller, Hinrich Schu?tze and Helmut Schmid
Institute for Natural Language Processing
University of Stuttgart, Germany
{muellets,schmid}@ims.uni-stuttgart.de
Abstract
We investigate a language model that com-
bines morphological and shape features with
a Kneser-Ney model and test it in a large
crosslingual study of European languages.
Even though the model is generic and we use
the same architecture and features for all lan-
guages, the model achieves reductions in per-
plexity for all 21 languages represented in the
Europarl corpus, ranging from 3% to 11%. We
show that almost all of this perplexity reduc-
tion can be achieved by identifying suffixes by
frequency.
1 Introduction
Language models are fundamental to many natural
language processing applications. In the most com-
mon approach, language models estimate the proba-
bility of the next word based on one or more equiv-
alence classes that the history of preceding words is
a member of. The inherent productivity of natural
language poses a problem in this regard because the
history may be rare or unseen or have unusual prop-
erties that make assignment to a predictive equiva-
lence class difficult.
In many languages, morphology is a key source
of productivity that gives rise to rare and unseen
histories. For example, even if a model can learn
that words like ?large?, ?dangerous? and ?serious?
are likely to occur after the relatively frequent his-
tory ?potentially?, this knowledge cannot be trans-
ferred to the rare history ?hypothetically? without
some generalization mechanism like morphological
analysis.
Our primary goal in this paper is not to de-
velop optimized language models for individual lan-
guages. Instead, we investigate whether a simple
generic language model that uses shape and mor-
phological features can be made to work well across
a large number of languages. We find that this is
the case: we achieve considerable perplexity reduc-
tions for all 21 languages in the Europarl corpus.
We see this as evidence that morphological language
modeling should be considered as a standard part of
any language model, even for languages like English
that are often not viewed as a good application of
morphological modeling due to their morphological
simplicity.
To understand which factors are important for
good performance of the morphological compo-
nent of a language model, we perform an exten-
sive crosslingual analysis of our experimental re-
sults. We look at three parameters of the morpho-
logical model we propose: the frequency threshold
? that divides words subject to morphological clus-
tering from those that are not; the number of suffixes
used ?; and three different morphological segmen-
tation algorithms. We also investigate the differen-
tial effect of morphological language modeling on
different word shapes: alphabetical words, punctua-
tion, numbers and other shapes.
Some prior work has used morphological models
that require careful linguistic analysis and language-
dependent adaptation. In this paper we show that
simple frequency analysis performs only slightly
worse than more sophisticated morphological anal-
ysis. This potentially removes a hurdle to using
morphological models in cases where sufficient re-
sources to do the extra work required for sophisti-
cated morphological analysis are not available.
The motivation for using morphology in lan-
guage modeling is similar to distributional clustering
386
(Brown et al, 1992). In both cases, we form equiv-
alence classes of words with similar distributional
behavior. In a preliminary experiment, we find that
morphological equivalence classes reduce perplex-
ity as much as traditional distributional classes ? a
surprising result we intend to investigate in future
work.
The main contributions of this paper are as fol-
lows. We present a language model design and a
set of morphological and shape features that achieve
reductions in perplexity for all 21 languages rep-
resented in the Europarl corpus, ranging from 3%
to 11%, compared to a Kneser-Ney model. We
show that identifying suffixes by frequency is suf-
ficient for getting almost all of this perplexity reduc-
tion. More sophisticated morphological segmenta-
tion methods do not further increase perplexity or
just slightly. Finally, we show that there is one pa-
rameter that must be tuned for good performance for
most languages: the frequency threshold ? above
which a word is not subject to morphological gen-
eralization because it occurs frequently enough for
standard word n-gram language models to use it ef-
fectively for prediction.
The paper is organized as follows. In Section 2
we discuss related work. In Section 3 we describe
the morphological and shape features we use. Sec-
tion 4 introduces language model and experimental
setup. Section 5 discusses our results. Section 6
summarizes the contributions of this paper.
2 Related Work
Whittaker and Woodland (2000) apply language
modeling to morpheme sequences and investigate
data-driven segmentation methods. Creutz et al
(2007) propose a similar method that improves
speech recognition for highly inflecting languages.
They use Morfessor (Creutz and Lagus, 2007) to
split words into morphemes. Both approaches are
essentially a simple form of a factored language
model (FLM) (Bilmes and Kirchhoff, 2003). In a
general FLM a number of different back-off paths
are combined by a back-off function to improve the
prediction after rare or unseen histories. Vergyri et
al. (2004) apply FLMs and morphological features
to Arabic speech recognition.
These papers and other prior work on using mor-
phology in language modeling have been language-
specific and have paid less attention to the ques-
tion as to how morphology can be useful across
languages and what generic methods are appropri-
ate for this goal. Previous work also has concen-
trated on traditional linguistic morphology whereas
we compare linguistically motivated morphologi-
cal segmentation with frequency-based segmenta-
tion and include shape features in our study.
Our initial plan for this paper was to use com-
plex language modeling frameworks that allow ex-
perimenters to include arbitrary features (including
morphological and shape features) in the model. In
particular, we looked at publicly available imple-
mentations of maximum entropy models (Rosen-
feld, 1996; Berger et al, 1996) and random forests
(Xu and Jelinek, 2004). However, we found that
these methods do not currently scale to running a
large set of experiments on a multi-gigabyte parallel
corpus of 21 languages. Similar considerations ap-
ply to other sophisticated language modeling tech-
niques like Pitman-Yor processes (Teh, 2006), re-
current neural networks (Mikolov et al, 2010) and
FLMs in their general, more powerful form. In ad-
dition, perplexity reductions of these complex mod-
els compared to simpler state-of-the-art models are
generally not large.
We therefore decided to conduct our study in the
framework of smoothed n-gram models, which cur-
rently are an order of magnitude faster and more
scalable. More specifically, we adopt a class-based
approach, where words are clustered based on mor-
phological and shape features. This approach has the
nice property that the number of features used to es-
timate the classes does not influence the time needed
to train the class language model, once the classes
have been found. This is an important consideration
in the context of the questions asked in this paper as
it allows us to use large numbers of features in our
experiments.
3 Modeling of morphology and shape
Our basic approach is to define a number of morpho-
logical and shape features and then assign all words
with identical feature values to one class. For the
morphological features, we investigate three differ-
ent automatic suffix identification algorithms: Re-
387
s, e, d, ed, n, g, ng, ing, y, t, es, r, a, l, on, er, ion,
ted, ly, tion, rs, al, o, ts, ns, le, i, ation, an, ers, m, nt,
ting, h, c, te, sed, ated, en, ty, ic, k, ent, st, ss, ons, se,
ity, ble, ne, ce, ess, ions, us, ry, re, ies, ve, p, ate, in,
tions, ia, red, able, is, ive, ness, lly, ring, ment, led,
ned, tes, as, ls, ding, ling, sing, ds, ded, ian, nce, ar,
ating, sm, ally, nts, de, nd, ism, or, ge, ist, ses, ning,
u, king, na, el
Figure 1: The 100 most frequent English suffixes in Eu-
roparl, ordered by frequency
ports (Keshava and Pitler, 2006), Morfessor (Creutz
and Lagus, 2007) and Frequency, where Frequency
simply selects the most frequent word-final letter se-
quences as suffixes. The 100 most frequent suffixes
found by Frequency for English are given in Fig-
ure 1.
We use the ? most frequent suffixes for all three
algorithms, where ? is a parameter. The focus of our
work is to evaluate the utility of these algorithms for
language modeling; we do not directly evaluate the
quality of the suffixes.
A word is segmented by identifying the longest of
the ? suffixes that it ends with. Thus, each word has
one suffix feature if it ends with one of the ? suffixes
and none otherwise.
In addition to suffix features, we define features
that capture shape properties: capitalization, special
characters and word length. If a word in the test set
has a combination of feature values that does not oc-
cur in the training set, then it is assigned to the class
whose features are most similar. We described the
similarity measure and details of the shape features
in prior work (Mu?ller and Schu?tze, 2011). The shape
features are listed in Table 1.
4 Experimental Setup
Experiments are performed using srilm (Stolcke,
2002), in particular the Kneser-Ney (KN) and
generic class model implementations. Estimation of
optimal interpolation parameters is based on (Bahl
et al, 1991).
4.1 Baseline
Our baseline is a modified KN model (Chen and
Goodman, 1999).
4.2 Morphological class language model
We use a variation of the model proposed by Brown
et al (1992) that we developed in prior work on En-
glish (Mu?ller and Schu?tze, 2011). This model is a
class-based language model that groups words into
classes and replaces the word transition probability
by a class transition probability and a word emission
probability:
PC(wi|wi?1i?N+1) =
P (g(wi)|g(wi?1i?N+1)) ? P (wi|g(wi))
where g(w) is the class of word w and we write
g(wi . . . wj) for g(wi) . . . g(wj).
Our approach targets rare and unseen histories.
We therefore exclude all frequent words from clus-
tering on the assumption that enough training data
is available for them. Thus, clustering of words is
restricted to those below a certain token frequency
threshold ?. As described above, we simply group
all words with identical feature values into one class.
Words with a training set frequency above ? are
added as singletons. The class transition probabil-
ity P (g(wi)|g(wi?1i?N+1)) is estimated using Witten-
Bell smoothing.1
The word emission probability is defined as fol-
lows:
P (w|c) =
?
??
??
1 , N(w) > ?
N(w)P
w?c N(w) ?
?(c)
|c|?1 , ??N(w)>0
?(c) , N(w) = 0
where c = g(w) is w?s class and N(w) is the fre-
quency of w in the training set. The class-dependent
out-of-vocabulary (OOV) rate ?(c) is estimated on
held-out data. Our final model PM interpolates PC
with a modified KN model:
PM (wi|wi?N+1i?1 ) =
?(g(wi?1)) ? PC(wi|wi?N+1i?1 )
+(1? ?(g(wi?1))) ? PKN(wi|wi?N+1i?1 ) (1)
This model can be viewed as a generalization of
the simple interpolation ?PC + (1? ?)PW used by
Brown et al (1992) (where PW is a word n-gram
1Witten-Bell smoothing outperformed modified Kneser-Ney
(KN) and Good-Turing (GT).
388
is capital(w) first character of w is an uppercase letter
is all capital(w) ? c ? w : c is an uppercase letter
capital character(w) ? c ? w : c is an uppercase letter
appears in lowercase(w) ?capital character(w) ? w? ? ?T
special character(w) ? c ? w : c is not a letter or digit
digit(w) ? c ? w : c is a digit
is number(w) w ? L([+? ?][0? 9] (([., ][0? 9])|[0? 9]) ?)
Table 1: Shape features as defined by Mu?ller and Schu?tze (2011). ?T is the vocabulary of the training corpus T , w? is
obtained from w by changing all uppercase letters to lowercase and L(expr) is the language generated by the regular
expression expr.
model and PC a class n-gram model). For the set-
ting ? = ? (clustering of all words), our model is
essentially a simple interpolation of a word n-gram
and a class n-gram model except that the interpola-
tion parameters are optimized for each class instead
of using the same interpolation parameter ? for all
classes. We have found that ? = ? is never optimal;
it is always beneficial to assign the most frequent
words to their own singleton classes.
Following Yuret and Bic?ici (2009), we evaluate
models on the task of predicting the next word from
a vocabulary that consists of all words that occur
more than once in the training corpus and the un-
known word UNK. Performing this evaluation for
KN is straightforward: we map all words with fre-
quency one in the training set to UNK and then com-
pute PKN(UNK |h) in testing.
In contrast, computing probability estimates for
PC is more complicated. We define the vocabulary
of the morphological model as the set of all words
found in the training corpus, including frequency-1
words, and one unknown word for each class. We
do this because ? as we argued above ? morpholog-
ical generalization is only expected to be useful for
rare words, so we are likely to get optimal perfor-
mance for PC if we include all words in clustering
and probability estimation, including hapax legom-
ena. Since our testing setup only evaluates on words
that occur more than once in the training set, we ide-
ally would want to compute the following estimate
when predicting the unknown word:
PC(UNKKN |h) =?
{w:N(w)=1}
PC(w|h) +
?
c
PC(UNKc |h) (2)
where we distinguish the unknown words of the
morphological classes from the unknown word used
in evaluation and by the KN model by giving the lat-
ter the subscript KN.
However, Eq. 2 cannot be computed efficiently
and we would not be able to compute it in practical
applications that require fast language models. For
this reason, we use the modified class model P ?C in
Eq. 1 that is defined as follows:
P ?C(w|h) =
{ PC(w|h) , N(w) ? 1
PC(UNKg(w) |h), N(w) = 0
P ?C and ? by extension ? PM are deficient. This
means that the evaluation of PM we present below
is pessimistic in the sense that the perplexity reduc-
tions would probably be higher if we were willing to
spend additional computational resources and com-
pute Eq. 2 in its full form.
4.3 Distributional class language model
The most frequently used type of class-based lan-
guage model is the distributional model introduced
by Brown et al (1992). To understand the dif-
ferences between distributional and morphological
class language models, we compare our morpholog-
ical model PM with a distributional model PD that
has exactly the same form as PM; in particular, it
is defined by Equations (1) and (2). The only dif-
ference is that the classes are morphological for PM
and distributional for PD.
The exchange algorithm that was used by Brown
et al (1992) has very long running times for large
corpora in standard implementations like srilm. It
is difficult to conduct the large number of cluster-
ings necessary for an extensive study like ours using
standard implementations.
389
Language T/T ? #Sentences
S bg Bulgarian .0183 .0094 181,415
S cs Czech .0185 .0097 369,881
S pl Polish .0189 .0096 358,747
S sk Slovak .0187 .0088 368,624
S sl Slovene .0156 .0090 365,455
G da Danish .0086 .0077 1,428,620
G de German .0091 .0073 1,391,324
G en English .0028 .0023 1,460,062
G nl Dutch .0061 .0048 1,457,629
G sv Swedish .0090 .0095 1,342,667
E el Greek .0081 .0079 851,636
R es Spanish .0040 .0031 1,429,276
R fr French .0029 .0024 1,460,062
R it Italian .0040 .0030 1,389,665
R pt Portuguese .0042 .0032 1,426,750
R ro Romanian .0142 .0079 178,284
U et Estonian .0329 .0198 375,698
U fi Finnish .0231 .0183 1,394,043
U hu Hungarian .0312 .0163 364,216
B lt Lithuanian .0265 .0147 365,437
B lv Latvian .0182 .0086 363,104
Table 2: Statistics for the 21 languages. S = Slavic, G
= Germanic, E = Greek, R = Romance, U = Uralic, B
= Baltic. Type/token ratio (T/T) and # sentences for the
training set and OOV rate ? for the validation set. The
two smallest and largest values in each column are bold.
We therefore induce the distributional classes
as clusters in a whole-context distributional vector
space model (Schu?tze and Walsh, 2011), a model
similar to the ones described by Schu?tze (1992)
and Turney and Pantel (2010) except that dimension
words are immediate left and right neighbors (as op-
posed to neighbors within a window or specific types
of governors or dependents). Schu?tze and Walsh
(2011) present experimental evidence that suggests
that the resulting classes are competitive with Brown
classes.
4.4 Corpus
Our experiments are performed on the Europarl cor-
pus (Koehn, 2005), a parallel corpus of proceed-
ings of the European Parliament in 21 languages.
The languages are members of the following fam-
ilies: Baltic languages (Latvian, Lithuanian), Ger-
manic languages (Danish, Dutch, English, Ger-
man, Swedish), Romance languages (French, Ital-
ian, Portuguese, Romanian, Spanish), Slavic lan-
guages (Bulgarian, Czech, Polish, Slovak, Slovene),
Uralic languages (Estonian, Finnish, Hungarian)
and Greek. We only use the part of the corpus that
can be aligned to English sentences. All 21 corpora
are divided into training set (80%), validation set
(10%) and test set (10%). The training set is used for
morphological and distributional clustering and esti-
mation of class and KN models. The validation set
is used to estimate the OOV rates ? and the optimal
parameters ?, ? and ?. Table 2 gives basic statistics
about the corpus. The sizes of the corpora of lan-
guages whose countries have joined the European
community more recently are smaller than for coun-
tries who have been members for several decades.
We see that English and French have the lowest
type/token ratios and OOV rates; and the Uralic lan-
guages (Estonian, Finnish, Hungarian) and Lithua-
nian the highest. The Slavic languages have higher
values than the Germanic languages, which in turn
have higher values than the Romance languages ex-
cept for Romanian. Type/token ratio and OOV
rate are one indicator of how much improvement
we would expect from a language model with
a morphological component compared to a non-
morphological language model.2
5 Results and Discussion
We performed all our experiments with an n-gram
order of 4; this was the order for which the KN
model performs best for all languages on the vali-
dation set.
5.1 Morphological model
Using grid search, we first determined on the vali-
dation set the optimal combination of three param-
eters: (i) ? ? {100, 200, 500, 1000, 2000, 5000},
(ii) ? ? {50, 100, 200, 500} and (iii) segmentation
method. Recall that we only cluster words whose
frequency is below ? and only consider the ? most
2The tokenization of the Europarl corpus has a preference
for splitting tokens in unclear cases. OOV rates would be higher
for more conservative tokenization strategies.
4A two-tailed paired t-test on the improvements by language
shows that the morphological model significantly outperforms
the distributional model with p=0.0027. A test on the Germanic,
Romance and Greek languages yields p=0.19.
390
PPKN ??M ?? M? PPC PPM ?M ??D PPWC PPD ?D
S bg 74 200 50 f 103 69 0.07 500 141 71 0.04
S cs 141 500 100 f 217 129 0.08 1000 298 134 0.04
S pl 148 500 100 m 241 134 0.09 1000 349 141 0.05
S sk 123 500 200 f 186 111 0.10 1000 261 116 0.06
S sl 118 500 100 m 177 107 0.09 1000 232 111 0.06
G da 69 1000 100 r 89 65 0.05 2000 103 65 0.05
G de 100 2000 50 m 146 94 0.06 2000 150 94 0.06
G en 55 2000 50 f 73 53 0.03 5000 87 53 0.04
G nl 70 2000 50 r 100 67 0.04 5000 114 67 0.05
G sv 98 1000 50 m 132 92 0.06 2000 154 92 0.06
E el 80 1000 100 f 108 73 0.08 2000 134 74 0.07
R es 57 2000 100 m 77 54 0.05 5000 93 54 0.05
R fr 45 1000 50 f 56 43 0.04 5000 71 42 0.05
R it 69 2000 100 m 101 66 0.04 2000 100 66 0.05
R pt 62 2000 50 m 88 59 0.05 2000 87 59 0.05
R ro 76 500 100 m 121 70 0.07 1000 147 71 0.07
U et 256 500 100 m 422 230 0.10 1000 668 248 0.03
U fi 271 1000 500 f 410 240 0.11 2000 706 261 0.04
U hu 151 200 200 m 222 136 0.09 1000 360 145 0.03
B lt 175 500 200 m 278 161 0.08 1000 426 169 0.03
B lv 154 500 200 f 237 142 0.08 1000 322 147 0.05
Table 3: Perplexities on the test set for N = 4. S = Slavic, G = Germanic, E = Greek, R = Romance, U =
Uralic, B = Baltic. ??x, ?? and M? denote frequency threshold, suffix count and segmentation method optimal on the
validation set. The letters f, m and r stand for the frequency-based method, Morfessor and Reports. PPKN, PPC,
PPM, PPWC, PPD are the perplexities of KN, morphological class model, interpolated morphological class model,
distributional class model and interpolated distributional class model, respectively. ?x denotes relative improvement:
(PPKN?PPx)/PPKN. Bold numbers denote maxima and minima in the respective column.4
frequent suffixes. An experiment with the optimal
configuration was then run on the test set. The re-
sults are shown in Table 3. The KN perplexities vary
between 45 for French and 271 for Finnish.
The main result is that the morphological model
PM consistently achieves better performance than
KN (columns PPM and ?M), in particular for
Slavic, Uralic and Baltic languages and Greek. Im-
provements range from 0.03 for English to 0.11 for
Finnish.
Column ??M gives the threshold that is optimal for
the validation set. Values range from 200 to 2000.
Column ?? gives the optimal number of suffixes. It
ranges from 50 to 500. The morphologically com-
plex language Finnish seems to benefit from more
suffixes than morphologically simple languages like
Dutch, English and German, but there are a few lan-
guages that do not fit this generalization, e.g., Esto-
nian for which 100 suffixes are optimal.
The optimal morphological segmenter is given in
column M?: f = Frequency, r = Reports, m = Mor-
fessor. The most sophisticated segmenter, Morfes-
sor is optimal for about half of the 21 languages, but
Frequency does surprisingly well. Reports is opti-
mal for two languages, Danish and Dutch. In gen-
eral, Morfessor seems to have an advantage for com-
plex morphologies, but is beaten by Frequency for
Finnish and Latvian.
5.2 Distributional model
Columns PPD and ?D show the performance of the
distributional class language model. As one would
perhaps expect, the morphological model is superior
to the distributional model for morphologically com-
plex languages like Estonian, Finnish and Hungar-
ian. These languages have many suffixes that have
391
??+ ???? ?+ ?? ??+ ???? ?+ ?? ?M+ ??M? M+ M?
S bg 0.03 200 5000 0.01 50 500 f m
S cs 0.03 500 5000 100 500 f r
S pl 0.03 500 5000 0.01 100 500 m r
S sk 0.02 500 5000 200 500 0.01 f r
S sl 0.03 500 5000 0.01 100 500 m r
G da 0.02 1000 100 100 50 r f
G de 0.02 2000 100 50 500 m f
G en 0.01 2000 100 50 500 f r
G nl 0.01 2000 100 50 500 r f
G sv 0.02 1000 100 50 500 m f
E el 0.02 1000 100 100 500 0.01 f r
R es 0.02 2000 100 100 500 m r
R fr 0.01 1000 100 50 500 f r
R it 0.01 2000 100 100 500 m r
R pt 0.02 2000 100 50 500 m r
R ro 0.03 500 5000 100 500 m r
U et 0.02 500 5000 0.01 100 50 0.01 m r
U fi 0.03 1000 100 0.03 500 50 0.02 f r
U hu 0.03 200 5000 0.01 200 50 m r
B lt 0.02 500 5000 200 50 m r
B lv 0.02 500 5000 200 500 f r
Table 4: Sensitivity of perplexity values to the parameters (on the validation set). S = Slavic, G = Germanic, E =
Greek, R = Romance, U = Uralic, B = Baltic. ?x+ and ?x? denote the relative improvement of PM over the KN
model when parameter x is set to the best (x+) and worst value (x?), respectively. The remaining parameters are set
to the optimal values of Table 3. Cells with differences of relative improvements that are smaller than 0.01 are left
empty.
high predictive power for the distributional contexts
in which a word can occur. A morphological model
can exploit this information even if a word with an
informative suffix did not occur in one of the lin-
guistically licensed contexts in the training set. For
a distributional model it is harder to learn this type
of generalization.
What is surprising about the comparative perfor-
mance of morphological and distributional models is
that there is no language for which the distributional
model outperforms the morphological model by a
wide margin. Perplexity reductions are lower than
or the same as those of the morphological model
in most cases, with only four exceptions ? English,
French, Italian, and Dutch ? where the distributional
model is better by one percentage point than the
morphological model (0.05 vs. 0.04 and 0.04 vs.
0.03).
Column ??D gives the frequency threshold for the
distributional model. The optimal threshold ranges
from 500 to 5000. This means that the distributional
model benefits from restricting clustering to less fre-
quent words ? and behaves similarly to the morpho-
logical class model in that respect. We know of no
previous work that has conducted experiments on
frequency thresholds for distributional class models
and shown that they increase perplexity reductions.
5.3 Sensitivity analysis of parameters
Table 3 shows results for parameters that were opti-
mized on the validation set. We now want to analyze
how sensitive performance is to the three parame-
ters ?, ? and segmentation method. To this end, we
present in Table 4 the best and worst values of each
parameter and the difference in perplexity improve-
ment between the two.
Differences of perplexity improvement between
best and worst values of ?M range between 0.01
392
and 0.03. The four languages with the smallest
difference 0.01 are morphologically simple (Dutch,
English, French, Italian). The languages with the
largest difference (0.03) are morphologically more
complex languages. In summary, the frequency
threshold ?M has a comparatively strong influence
on perplexity reduction. The strength of the effect is
correlated with the morphological complexity of the
language.
In contrast to ?, the number of suffixes ? and
the segmentation method have negligible effect on
most languages. The perplexity reductions for dif-
ferent values of ? are 0.03 for Finnish, 0.01 for Bul-
garian, Estonian, Hungarian, Polish and Slovenian,
and smaller than 0.01 for the other languages. This
means that, with the exception of Finnish, we can
use a value of ? = 100 for all languages and be very
close to the optimal perplexity reduction ? either be-
cause 100 is optimal or because perplexity reduction
is not sensitive to choice of ?. Finnish is the only
language that clearly benefits from a large number
of suffixes.
Surprisingly, the performance of the morphologi-
cal segmentation methods is very close for 17 of the
21 languages. For three of the four where there is
a difference in improvement of ? 0.01, Frequency
(f) performs best. This means that Frequency is a
good segmentation method for all languages, except
perhaps for Estonian.
5.4 Impact of shape
The basic question we are asking in this paper is
to what extent the sequence of characters a word
is composed of can be exploited for better predic-
tion in language modeling. In the final analysis in
Table 5 we look at four different types of character
sequences and their contributions to perplexity re-
duction. The four groups are alphabetic character
sequences (W), numbers (N), single special charac-
ters (P = punctuation), and other (O). Examples for
O would be ?751st? and words containing special
characters like ?O?Neill?. The parameters used are
the optimal ones of Table 3. Table 5 shows that the
impact of special characters on perplexity is similar
across languages: 0.04 ? ?P ? 0.06. The same is
true for numbers: 0.23 ? ?N ? 0.33, with two out-
liers that show a stronger effect of this class: Finnish
?N = 0.38 and German ?N = 0.40.
?W ?P ?N ?O
S bg 0.07 0.04 0.28 0.16
S cs 0.09 0.04 0.26 0.33
S pl 0.10 0.05 0.23 0.22
S sk 0.10 0.05 0.25 0.28
S sl 0.10 0.04 0.28 0.28
G da 0.05 0.05 0.31 0.18
G de 0.06 0.05 0.40 0.18
G en 0.03 0.04 0.33 0.14
G nl 0.04 0.05 0.31 0.26
G sv 0.06 0.05 0.31 0.35
E el 0.08 0.05 0.33 0.14
R es 0.05 0.04 0.26 0.14
R fr 0.04 0.04 0.29 0.01
R it 0.04 0.05 0.33 0.02
R pt 0.05 0.05 0.28 0.39
R ro 0.08 0.04 0.25 0.17
U et 0.11 0.05 0.26 0.26
U fi 0.12 0.06 0.38 0.36
U hu 0.10 0.04 0.32 0.23
B lt 0.08 0.06 0.27 0.05
B lv 0.08 0.05 0.26 0.19
Table 5: Relative improvements of PM on the valida-
tion set compared to KN for histories wi?1i?N+1 grouped
by the type of wi?1. The possible types are alphabetic
word (W), punctuation (P), number (N) and other (O).
The fact that special characters and numbers be-
have similarly across languages is encouraging as
one would expect less crosslinguistic variation for
these two classes of words.
In contrast, ?true? words (those exclusively com-
posed of alphabetic characters) show more variation
from language to language: 0.03 ? ?W ? 0.12.
The range of variation is not necessarily larger than
for numbers, but since most words are alphabetical
words, class W is responsible for most of the differ-
ence in perplexity reduction between different lan-
guages. As before we observe a negative correlation
between morphological complexity and perplexity
reduction; e.g., Dutch and English have small ?W
and Estonian and Finnish large values.
We provide the values of ?O for completeness.
The composition of this catch-all group varies con-
siderably from language to language. For exam-
ple, many words in this class are numbers with al-
phabetic suffixes like ?2012-ben? in Hungarian and
393
words with apostrophes in French.
6 Summary
We have investigated an interpolation of a KN model
with a class language model whose classes are de-
fined by morphology and shape features. We tested
this model in a large crosslingual study of European
languages.
Even though the model is generic and we use
the same architecture and features for all languages,
the model achieves reductions in perplexity for all
21 languages represented in the Europarl corpus,
ranging from 3% to 11%, when compared to a KN
model. We found perplexity reductions across all
21 languages for histories ending with four different
types of word shapes: alphabetical words, special
characters, and numbers.
We looked at the sensitivity of perplexity reduc-
tions to three parameters of the model: ?, a thresh-
old that determines for which frequencies words are
given their own class; ?, the number of suffixes used
to determine class membership; and morphological
segmentation. We found that ? has a considerable
influence on the performance of the model and that
optimal values vary from language to language. This
parameter should be tuned when the model is used
in practice.
In contrast, the number of suffixes and the mor-
phological segmentation method only had a small
effect on perplexity reductions. This is a surprising
result since it means that simple identification of suf-
fixes by frequency and choosing a fixed number of
suffixes ? across languages is sufficient for getting
most of the perplexity reduction that is possible.
7 Future Work
A surprising result of our experiments was that the
perplexity reductions due to morphological classes
were generally better than those due to distributional
classes even though distributional classes are formed
directly based on the type of information that a lan-
guage model is evaluated on ? the distribution of
words or which words are likely to occur in se-
quence. An intriguing question is to what extent the
effect of morphological and distributional classes is
additive. We ran an exploratory experiment with
a model that interpolates KN, morphological class
model and distributional class model. This model
only slightly outperformed the interpolation of KN
and morphological class model (column PPM in Ta-
ble 3). We would like to investigate in future work if
the information provided by the two types of classes
is indeed largely redundant or if a more sophisticated
combination would perform better than the simple
linear interpolation we have used here.
Acknowledgments. This research was funded by
DFG (grant SFB 732). We would like to thank the
anonymous reviewers for their valuable comments.
References
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza,
Robert L. Mercer, and David Nahamoo. 1991. A fast
algorithm for deleted interpolation. In Eurospeech.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Comput. Linguist.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
NAACL-HLT.
Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist.
Stanley F. Chen and Joshua Goodman. 1999. An empir-
ical study of smoothing techniques for language mod-
eling. Computer Speech & Language.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM TSLP.
Mathias Creutz, Teemu Hirsima?ki, Mikko Kurimo, Antti
Puurula, Janne Pylkko?nen, Vesa Siivola, Matti Var-
jokallio, Ebru Arisoy, Murat Sarac?lar, and Andreas
Stolcke. 2007. Morph-based speech recognition
and modeling of out-of-vocabulary words across lan-
guages. ACM TSLP.
Samarth Keshava and Emily Pitler. 2006. A simpler,
intuitive approach to morpheme induction. In PASCAL
Morpho Challenge.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit.
Toma?s? Mikolov, Martin Karafia?t, Luka?s? Burget, Jan
C?ernocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In ICSLP.
Thomas Mu?ller and Hinrich Schu?tze. 2011. Improved
modeling of out-of-vocabulary words using morpho-
logical classes. In ACL.
394
Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modelling. Computer
Speech & Language.
Hinrich Schu?tze and Michael Walsh. 2011. Half-context
language models. Comput. Linguist.
Hinrich Schu?tze. 1992. Dimensions of meaning.
In ACM/IEEE Conference on Supercomputing, pages
787?796.
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In Interspeech.
Yee Whye Teh. 2006. A hierarchical bayesian language
model based on Pitman-Yor processes. In ACL.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
JAIR.
Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and An-
dreas Stolcke. 2004. Morphology-based language
modeling for Arabic speech recognition. In ICSLP.
E.W.D. Whittaker and P.C. Woodland. 2000. Particle-
based language modelling. In ICSLP.
Peng Xu and Frederick Jelinek. 2004. Random forests in
language modeling. In EMNLP.
Deniz Yuret and Ergun Bic?ici. 2009. Modeling morpho-
logically rich languages using split words and unstruc-
tured dependencies. In ACL-IJCNLP.
395
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 520?524, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
CodeX: Combining an SVM Classifier and Character N-gram Language
Models for Sentiment Analysis on Twitter Text
Qi Han, Junfei Guo and Hinrich Schu?tze
Institute for Nature Language Processing
University of Stuttgart
Stuttgart, Germany
{hanqi, guojf}@ims.uni-stuttgart.de
Abstract
This paper briefly reports our system for the
SemEval-2013 Task 2: sentiment analysis in
Twitter. We first used an SVM classifier with
a wide range of features, including bag of
word features (unigram, bigram), POS fea-
tures, stylistic features, readability scores and
other statistics of the tweet being analyzed,
domain names, abbreviations, emoticons in
the Twitter text. Then we investigated the ef-
fectiveness of these features. We also used
character n-gram language models to address
the problem of high lexical variation in Twit-
ter text and combined the two approaches to
obtain the final results. Our system is robust
and achieves good performance on the Twitter
test data as well as the SMS test data.
1 Introduction
The challenge of the SemEval-2013 Task 2 (Task
B) is the ?Message Polarity Classification? (Wilson
et al, 2013). Specifically, the task was to classify
whether a given message has positive, negative or
neutral sentiment; for messages conveying both pos-
itive and negative sentiment, whichever is stronger
should be chosen.
In recent years, text messaging and microblog-
ging such as tweeting has gained its popularity.
Since these short messages are often used not only
to discuss facts but also to share opinions and sen-
timents, sentiment analysis on this type of data has
lately become interesting. However, some features
of this type of data make natural language process-
ing challenging. For example, the messages are usu-
ally short and the language used can be very in-
formal, with misspellings, creative spellings, slang,
URLs and special abbreviations. Some research has
already been done attempting to address these prob-
lems, to enable sentiment analysis on this type of
data, in particular on Twitter data, and even to use
the outcome of sentiment analysis to make predic-
tions (Jansen et al, 2009; Barbosa and Feng, 2010;
Bifet and Frank, 2010; Davidov et al, 2010; Jiang et
al., 2011; Pak and Paroubek, 2010; Saif et al, 2012;
Tumasjan et al, 2010).
As the research mentioned above, our system used
a machine learning based approach for sentiment
analysis. Our system combines results from an SVM
classifier using a wide range of features as well as
votes derived from character n-gram language mod-
els to do the final prediction.
The rest of this paper is organized as follows. Sec-
tion 2 describes the features used for the SVM clas-
sifier. Section 3 describes how the votes from char-
acter n-gram language models were derived. Section
4 describes the details of our method. And finally
section 5 presents the results.
2 Features
We pre-processed the tweets as follows: i) tok-
enized the tweets using a tokenizer suitable for Twit-
ter data, which, for example, recognize emoticons
and hashtags; ii) replaced all URLs with the token
twitterurl; iii) replaced all Twitter usernames with
the token @twitterusername; iv) converted all to-
kens into lower case; v) replaced all sequences of
repeated characters by three characters, for example,
convert gooooood to goood, this way we recognize
520
the emphasized usage of the word; vi) expanded ab-
breviations with a dictionary,1 which we will refer
to as noslang dictionary; vii) appended neg to all
words from one position before a negation word to
the next punctuation mark.
We represented each given tweet using 6 feature
families:
? Lexical features (UG, BG): Number of times
each unigram appears in the tweet (UG); num-
ber of times each bigram appears in the tweet
(BG).
? POS features (POS U, POS B): Number of
times each POS appears in the tweet divided by
number of tokens of that tweet (POS U); num-
ber of times each POS bigram appears in the
tweet (POS B). To tag the tweet we used the
ark-twitter-nlp tagger.2
? Statistical features (STAT): Various readabil-
ity scores (ARI, Flesch Reading Ease, RIX,
LIX, Coleman Liau Index, SMOG Index, Gun-
ning Fog Index, Flesch-Kincaid Grade Level)
of the tweet; some simple statistics of the tweet
(average count of words per sentence, complex
word count, syllable count, sentence count,
word count, char count). We calculated the
statistics and scores after pre-processing step
vi). We then normalized these scores so that
they had mean 0 and standard deviation 1.
? Stylistic features (STY): Number of times
an emoticon appears in the tweet, number of
words which are written in all capital let-
ters, number of words containing characters
repeated consecutively more than three times,
number of words containing characters re-
peated consecutively more than four times. We
calculated these features after pre-processing
step i). We used the binarized and the logarith-
mically scaled version of these features.
? Abbreviation features (ABB): For every term
in the noslang dictionary, we checked whether
it was present in the tweet or not and used this
as a feature.
1http://www.noslang.com
2http://www.ark.cs.cmu.edu/TweetNLP/
? URL features (URL): We expanded the URLs
in the Twitter text and collected all the domain
names which the URLs in the training set point
to, and used them as binary features.
Feature sets UG, BG, POS U, POS B are com-
mon features for sentiment analysis (Pang et al,
2002). Remus (2011) showed that incorporat-
ing readability measures as features can improve
sentence-level subjectivity classification. Stylistic
features have also been used in sentiment analysis on
Twitter data (Go et al, 2010). Some abbreviations
express sentiment which is not apparent from word
level. For example lolwtime, which means laugh-
ing out loud with tears in my eyes, expresses positive
sentiment overall, but this does not follow directly at
the sentiment of individual words, so the feature set
ABB might be helpful. Finally, we conjecture that a
tweet including an URL pointing to youtube.com
is more likely to be subjective than a tweet including
an URL pointing to a news website.
3 Integrating votes from language models
based on character n-grams
Language Models can be used for text classification
tasks. Since the goal of the SemEval-2013 Task 2
(Task B) is to classify each tweet into one of the
three classes: positive, negative or neutral, a lan-
guage model approach can be used.
Emoticon-smoothed language models have been
used to do Twitter sentiment analysis (Liu et al,
2012). The language models used there were based
on words. However, there is evidence (Aisopos et
al., 2012; Raaijmakers and Kraaij, 2008) showing
that super-word character n-gram features can be
quite effective for sentiment analysis on short infor-
mal data. This is because noise and mis-spellings
tend to have smaller impact on substring patterns
than on word patterns. Our system used language
models based on character n-grams to improve the
performance of sentiment analysis on tweets.
For every tweet we constructed 3 sequences of
character-trigrams and 4 sequences of character-
four-grams. For instance, the tweet "Hello
World!" would have 7 corresponding substring
representations:
<s><s>H ell o W orl d!</s>,
<s>He llo Wo rld !</s></s>,
521
Hel lo Wor ld!,
<s><s><s>H ello Wor ld!</s>
<s><s>He llo Worl d!</s></s>,
<s>Hel lo W orld !</s></s></s>,
Hell o Wo rld!
where <s> means start of a sentence, </s> means
end of a sentence, means whitespace. Using the
corresponding sequences of character-trigrams from
all positive tweets in training set we trained a lan-
guage model LM+3 . To train the language model
we used Chen and Goodman?s modified Kneser-Ney
discounting for N-grams from the SRILM toolkit
(Stolcke, 2002). Given a new sequence of character-
trigrams derived from a positive tweet, it should
give a lower perplexity value than a language model
trained on sequences of character-trigrams from
negative tweets.
In this way we obtained 6 language models:
LM?3 from character-trigram sequences of neg-
ative tweets, LMN3 from character-trigram se-
quences of neutral tweets, LM+3 from character-
trigram sequences of positive tweets, LM?4 from
character-four-grams sequences of negative tweets,
LMN4 from character-four-gram sequences of neu-
tral tweets, LM+4 from character-four-gram se-
quences of positive tweets.
For every new tweet, we first obtain the 7 corre-
sponding substring representations. Then for each
substring representation, we calculate 3 votes from
the language models. For instance, for a sequence
of character-trigrams, we first calculate three per-
plexity valuesP?3 , P
N
3 , P
+
3 using language models
LM?3 , LM
N
3 , LM
+
3 then produce votes according
to the following discretization function:
vote(LMxn , LM
y
n) =
{
1 if P xn ? P
y
n ;
?1 else.
where n ? {3, 4} is the length of the character n-
gram, x, y ? {?,+, N} are class labels and P xn , P
y
n
are the corresponding perplexity values. In this way
we obtain 21 votes for every tweet. However, in the
final classification, every sentence got 42 votes, of
which 21 were derived from bigram language mod-
els of the substrings and 21 were from trigram lan-
guage models of these substrings.
Feature Sets Accuracy
UG,BG,POS U,POS B,STAT,STY,ABB,URL 0.692
BG,POS U,POS B,STAT,STY,ABB,URL 0.641
POS U,POS B,STAT,STY,ABB,URL 0.579
POS U,STAT,STY,ABB,URL 0.564
STAT,STY,ABB,URL 0.524
STY,ABB,URL 0.474
STY,URL 0.454
URL 0.441
Table 1: Cross validation average accuracy with differ-
ent feature sets. we started with all 8 feature sets and
removed feature sets one by one, where we always first
removed the feature set that resulted in the biggest drop
in accuracy.
4 Methods
In this section we describe the methods used by our
system.
Firstly, we did feature selection on all the features
described in Section 2. Using Mutual Information
(Shannon and Weaver, 1949) and 10-fold cross vali-
dation we chose the top 13,500 features. Using these
features we trained an SVM classifier with the train-
ing data. As the implementation of the SVM classi-
fier we used liblinear (Fan et al, 2008). The SVM
classifier was then used to produce initial predictions
for messages in the development set, the Twitter test
set and the SMS test set.
Then, we represented every message in the devel-
opment set, the Twitter test set and the SMS test
set using the 42 votes we described in Section 3
together with the predictions of the SVM classifier
we described above. Using the Bagging algorithm
from the WEKA machine learning toolkit (Hall et
al., 2009) and the development set data, we trained
a new classifier and used this classifier for the final
prediction on Twitter test data and SMS test data.
5 Results
5.1 Feature analysis
To study the effectiveness of different features, we
started with all 8 feature sets and removed feature
sets one by one, where we always first removed the
feature set that resulted in the biggest drop in accu-
racy. We did 10 fold cross validation on training set
522
Feature Sets Accuracy
POS U,POS B,STAT,STY,ABB,URL 0.579
POS B,STY,ABB,URL 0.571
POS U,STY,ABB,URL 0.557
STAT,STY,ABB,URL 0.524
STY,ABB,URL 0.474
Table 2: Cross validation average accuracy with further
combination of feature sets.
Accuracy F1 (pos, neg)
Majority Baseline 0.4123 0.2919
SVM Classifier 0.6612 0.5414
SVM + LM Votes 0.6457 0.5384
Table 3: Overall accuracy and average F1 score for posi-
tive and negative classes on Twitter test data.
and used average accuracy as a metric.
As we can see from Table 1, lexical features were
the most important features ? they counted for more
than 0.11 loss of accuracy when removed from the
features. POS features and statistical features were
also important, POS bigrams more so than POS uni-
grams. Stylistic, abbreviation and URL features, on
the contrary, seem to be only of moderate useful-
ness.
To further investigate the relationship between the
feature sets POS U, POS B and STAT, we did addi-
tional experiments. From Table 2, we can see that
removing all three feature sets caused a decrease
in accuracy to 0.47, including just one feature set
POS B, POS U or STAT resulted in accuracy above
0.57, 0.55 and 0.52 respectively. This shows that
all three feature sets were quite effective and POS B
was most useful. However, adding all of the three
feature sets only caused an increase in accuracy to
0.579, which suggests that they were highly corre-
lated.
Accuracy F1 (pos, neg)
Majority Baseline 0.2350 0.1902
SVM Classifier 0.6504 0.5811
SVM + LM Votes 0.6418 0.5670
Table 4: Overall accuracy and average F1 score for posi-
tive and negative classes on SMS test data.
5.2 Effectiveness of language model features
To evaluate the effectiveness of features derived
from language models of character n-grams, we
compared the performance of our SVM classifier
and that of the classifier combining the SVM clas-
sifier results and language model features.3 We per-
formed our experiments on both of the Twitter test
data and the SMS test data. The results in Table 3
and Table 4 suggested that in our current setup, lan-
guage model features were not very helpful.
Table 3 and Table 4 also show that our system
improved the performance greatly compared to Ma-
jority baseline system,4. Compared with other par-
ticipants in the SemEval-2013 Task 2, our system
achieved average performance on Twitter test data.
However, it has been the ninth best out of all 48 sys-
tems for the performance on SMS test data. This
shows that our system can be easily adapted to dif-
ferent contexts without a big drop in performance.
One reason for that might be that we did not use any
sentiment lexicon developed specifically for Twitter
data and we used high level features like the statisti-
cal features and POS features for our classification.
6 Conclusion
This paper briefly reports our system designed for
the SemEval-2013 Task 2: sentiment analysis in
Twitter. We first used an SVM classifier with a wide
range of features. We found that simple statistics
of the tweets, for example word count or readabil-
ity scores, can help in sentiment analysis on Twitter
text.
We then used character n-gram language mod-
els to address the problem of high lexical variation
in Twitter text and combined the two approaches
to obtain the final results. Although in our current
setup, features derived from character n-gram lan-
guage models do not perform very well, they may
benefit from a larger training data set.
Acknowledgments
This work was funded by DFG projects SFB 732.
We would like to thank our colleagues at IMS.
3We accidentally used feature set POS B two times in our
representation, but it didn?t change the results significantly.
4To be consistent with the evaluation metric, we chose the
majority class of positive and negative classes.
523
References
Fotis Aisopos, George Papadakis, Konstantinos Tserpes,
and Theodora Varvarigou. 2012. Content vs. context
for sentiment analysis: a comparative analysis over
microblogs. In Proceedings of the 23rd ACM confer-
ence on Hypertext and social media, HT ?12, pages
187?96, New York, NY, USA. ACM.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ?10,
pages 36?44, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Proceed-
ings of the 13th international conference on Discov-
ery science, DS?10, pages 1?15, Berlin, Heidelberg.
Springer-Verlag.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 241?249, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: a li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874, June.
Alec Go, Richa Bhayani, and L. Huang. 2010. Exploit-
ing the unique characteristics of tweets for sentiment
analysis. Technical report, Technical Report, Stanford
University.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as elec-
tronic word of mouth. J. Am. Soc. Inf. Sci. Technol.,
60(11):2169?2188, November.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent twitter sentiment clas-
sification. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, HLT ?11,
pages 151?160, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kun-Lin Liu, Wu-Jun Li, and Minyi Guo. 2012. Emoti-
con smoothed language models for twitter sentiment
analysis. In Twenty-Sixth AAAI Conference on Artifi-
cial Intelligence.
A. Pak and P. Paroubek. 2010. Twitter as a corpus for
sentiment analysis and opinion mining. LREC.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP,
pages 79?86.
Stephan Raaijmakers and Wessel Kraaij. 2008. A shal-
low approach to subjectivity classification. Proceed-
ings of ICWSM, pages 216?217.
Robert Remus. 2011. Improving sentence-level subjec-
tivity classification through readability measurement.
May. Proceedings of the 18th Nordic Conference of
Computational Linguistics NODALIDA 2011.
Hassan Saif, Yulan He, and Harith Alani. 2012. Seman-
tic sentiment analysis of twitter. In Philippe Cudr-
Mauroux, Jeff Heflin, Evren Sirin, Tania Tudorache,
Jrme Euzenat, Manfred Hauswirth, Josiane Xavier
Parreira, Jim Hendler, Guus Schreiber, Abraham Bern-
stein, and Eva Blomqvist, editors, The Semantic Web
ISWC 2012, number 7649 in Lecture Notes in Com-
puter Science, pages 508?524. Springer Berlin Heidel-
berg, January.
Claude E. Shannon and Warren Weaver. 1949. Mathe-
matical Theory of Communication. University of Illi-
nois Press.
Andreas Stolcke. 2002. SRILMAn extensible language
modeling toolkit. In In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing
(ICSLP 2002, page 901904.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with twitter: What 140 characters reveal about politi-
cal sentiment. Proceedings of the Fourth International
AAAI Conference on Weblogs and Social Media.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
524
