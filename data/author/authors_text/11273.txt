Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1033?1040
Manchester, August 2008
Chinese Term Extraction Using Minimal Resources 
Yuha
School of C
Science and Technology,  
Harbin In
Techn
Harbin 15
1983yang@gmail.com 
Qin L
Department of Computing,  
The Hon
Polytechn
Hong Ko
csluqin@comp.polyu.e
du.
Tieju
School of C
Science and Technology,  
Harbin In
Techn
Harbin 1
tjzhao@mtlab.hit.edu
.
 
ct 
This pap
term extraction nimal resources. 
A term candidate extraction algorithm is 
proposed to i tures of the 
1 
Ter st 
fun  domain. Term 
                                                
ng Yang 
omputer  
stitute of  
ology, 
0001, China 
u 
g Kong  
ic University, 
ng, China 
hk 
n Zhao 
omputer  
stitute of  
ology, 
50001, China 
cn 
Abstra
er presents a new approach for 
using mi
dentify fea
relatively stable and domain independent 
term delimiters rather than that of the 
terms. For term verification, a link 
analysis based method is proposed to 
calculate the relevance between term 
candidates and the sentences in the 
domain specific corpus from which the 
candidates are extracted. The proposed 
approach requires no prior domain 
knowledge, no general corpora, no full 
segmentation and minimal adaptation for 
new domains. Consequently, the method 
can be used in any domain corpus and it 
is especially useful for resource-limited 
domains. Evaluations conducted on two 
different domains for Chinese term 
extraction show quite significant 
improvements over existing techniques 
and also verify the efficiency and relative 
domain independent nature of the 
approach. Experiments on new term 
extraction also indicate that the approach 
is quite effective for identifying new 
terms in a domain making it useful for 
domain knowledge update. 
Introduction 
ms are the lexical units to represent the mo
damental knowledge of a
 
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
extraction is 
knowledge acq
an essential task in domain 
uisition which
lexicon update, domain onto
etc. Term extraction involves tw
tes by unithood calculation 
s a valid term. The second 
endent features of domain 
ter
s. 
Ot
 can be used for 
logy construction, 
o steps. The first 
step extracts candida
to qualify a string a
step verifies them through termhood measures 
(Kageura and Umino, 1996) to validate their 
domain specificity.  
Existing techniques extract term candidates 
mainly by two kinds of statistic based measures 
including internal association (e.g. Schone and 
Jurafsky, 2001) and context dependency (e.g. 
Sornlertlamvanich et al, 2000). These techniques 
are also used in Chinese term candidate 
extraction (e.g. Luo and Sun, 2003; Ji and Lu, 
2007). Domain dep
ms are used in a weighted manner to identify 
term boundaries. However, these algorithms 
always face the dilemma that fewer features are 
not enough to identify terms from non-terms 
whereas more features lead to more conflicts 
among selected features in a specific instance.  
Most term verification techniques use features 
on the difference in distribution of a term 
occurred within a domain and across domains, 
such as TF-IDF (Salton and McGill, 1983; Frank, 
1999) and Inter-Domain Entropy (Chang, 2005). 
Limited distribution information on term 
candidates in different documents are far from 
enough to distinguish terms from non-term
her researches attempted to use more direct 
information. The therm verification algorithm, 
TV_ConSem, proposed in (Ji and Lu, 2007) for 
Chinese calculate the percentage of context 
words in a domain lexicon using both frequency 
information and semantic information. However, 
this technique requires a large domain lexicon 
and relies heavily on both the size and the quality 
of the lexicon. Some supervised learning 
1033
approaches have been applied to protein/gene 
name recognition (Zhou et al, 2005) and Chinese 
new word identification (Li et al, 2004) using 
SVM classifiers (Vapnik, 1995) which also 
require large domain corpora and annotations, 
and intensive training is needed for a new domain. 
Current term extraction techniques (e.g. Frank 
et al, 1999; Chang, 2005; Ji and Lu, 2007) suffer 
from three major problems. The first problem is 
that these algorithms cannot identify certain 
kinds of terms such as the ones that have less 
statistical significance. The second problem is 
their dependency on full segmentation for 
Chinese text which is particularly vulnerable to 
ha
idates and the sentences in domain 
sp
s (terms for short) are more likely to 
be domain substantives. Words immediate before 
s, called predecessors and 
es
conne . These predecessors and 
Ch
ndle domain specific data (Huang et al, 2007). 
The third problem is their dependency on some a 
priori domain knowledge such as a domain 
lexicon making it difficult to be applied to a new 
domain.  
In this work, the proposed algorithm extracts 
candidates by identifying the relatively stable and 
domain independent term boundary markers 
instead of looking for features associated with the 
term candidate themselves. Furthermore, a novel 
algorithm for term verification is proposed using 
link analysis to calculate the relevance between 
term cand
ecific corpus to validate their domain 
specificity.  
The rest of the paper is organized as follows. 
Section 2 describes the proposed algorithms. 
Section 3 explains the experiments and the 
performance evaluation. Section 4 is the 
conclusion. 
2 Methodology 
2.1 Delimiters Based Term Candidate 
Extraction 
Generally speaking, sentences are constituted by 
substantives and functional words. Domain 
specific term
and after these term
succ sors of the terms, are likely to be either 
functional words or other general substantives 
cting terms
successors can be considered as markers of terms, 
and are referred to as term delimiters in this 
paper. In contrast to terms, delimiters are 
relatively stable and domain independent. Thus, 
they can be extracted more easily. Instead of 
looking for features associated with terms as in 
other works, this paper looks for features 
associated with term delimiters. That is, term 
delimiters are identified first. Words between 
delimiters are then taken as term candidates.  
The proposed delimiter identification based 
algorithm, referred to as TCE_DI (Term 
Candidate Extraction ? Delimiter Identification), 
extracts term candidates from a domain corpus 
by using a delimiter list, referred to as the DList. 
Given a DList, the algorithm TCE_DI itself is 
straight forward. For a given character string CS 
(CS = C1C2?Cn) shown in Figure 1, where Ci is a 
inese character. Suppose there are two 
delimiters D1 = Ci1?Cil and D2 = Cj1?Cjm in CS 
where D1 ? DList and D2 ? DList. The string CS 
is then segmented to five substrings: C1?Cib, 
Ci1?Cil, Cia?Cjb, Cj1?Cjm, and Cja?Cn. Since 
Ci1?Cil and Cj1?Cjm are delimiters, C1?Cib, 
Cia?Cjb, and Cja?Cn are regarded as term 
candidates as labeled by TC1, TC2 and TC3 in 
Figure 1, respectively. If there is no delimiter 
contained in CS, the whole string C1C2?Cn is 
regarded as one term candidate. 
Figure 1. Paradigm of Term Candidate Extraction 
DList can be obtained either from a delimiter 
training corpus or from a given  o l stop w rd ist. 
Given a delimiter training corpus, CorpusTraining, 
normally a domain specific corpus, and a domain 
lexicon Lexicon, DList can be obtained based on 
the following algorithm, referred to as DList_Ext 
St
withou
a stop  experts or from a 
ge
(DelimiterList Extraction Algorithm).   
ep 1: For each term Ti in Lexicon, mark Ti in 
CorpusTraining as a non-divisible lexical unit.  
Step 2: Segment remaining text in CorpusTraining.  
Step 3: Extracts predecessors and successors of 
all Ti as delimiter candidates. 
Step 4: Remove delimiter candidates that are 
contained in a Ti in Lexicon. 
Step 5: Rank delimiter candidates by frequency 
and the top NDI number of items are 
considered delimiters. 
The DList_Ext algorithm basically use known 
terms in a domain specific Lexicon to find the 
delimiters. It can be shown in the experiments 
later that Lexicon does not need to be 
comprehensive. Even if a small training corpus, 
CorpusTraining, is not available in a language 
t sufficient domain specific NLP resources, 
-word list produced by
neral corpus can serve as DList directly 
without using the DList_Ext algorithm. 
1034
2.2 Link Analysis Based Term Verification 
In a domain corpus, some sentences are domain 
relevant sentences which contain more domain 
specific information whereas others are general 
sentences which contain less domain information. 
A domain specific term is more likely to be 
contained in domain relevant sentences, which 
means that domain relevant sentences and 
ai  
, w(pn)
l numb
dom n specific terms have a mutually
reinforcing relationship. A novel algorithm, 
referred to as TV_LinkA (Term Verification ? 
Link Analysis) based the Hyperlink-Induced 
Topic Search (HITS) algorithm (Kleinberg, 1997) 
originally proposed for information retrieval, is 
proposed using link analysis to calculate the 
relevance between term candidates and the 
sentences in domain specific corpora for term 
verification.   
In TV_LinkA, a node p can either be a sentence 
or a term candidate. If a term candidate TermC is 
contained in a sentence Sen of the corpus 
CorpusExtract where the candidates were extracted, 
there is a directional link from Sen to TermC. This 
way, a graph for the candidates and the sentences 
in CorpusExtract can be constructed and the links 
between them indicate their relationships. A good 
hub  Corpu in sExtract is a sentence that contains 
many good authorities; a good authority is a term 
candidate that is contained in many good hubs. 
Each node p is associated with a non-negative 
authority weight Apw )(  and a non-negative hub 
weight Hpw )( . Link analysis in TV_LinkA 
makes use of the relationship between hubs and 
authorities via an iterative process to maintain 
and update authority/hub weights for each node 
of the graph.  
Let VA denote the authority vector (w(p1)A, 
w(p2)A,?, w(pn)A)  and VH denote the hub vector 
(w(p1)H, w(p2)H,? H), where n is the sum of 
the tota er of sentences and the total 
number of term candidates. Given weights VA and 
VH with a directional link p?q, the I operation(an 
in-pointer to a node) and the O operation(an out-
pointer to a node) update w(q)A and w(p)H as 
follows. 
I operation: ?
??
=
Eqp
HA w(p)w(q)          (1) 
O operation: ?
??
e calculated as follows. 
For i 
Apply the I operation to ( ), 
o . 
factor
=
Eqp
AH w(q)w(p)         (2) 
Let k be the iteration termination parameter and z 
be the vector (1, 1, 1,?, 1) , and VA and VH are 
initialized to AV0  = 
HV0  = z. Hubs and authorities 
can then b
= 1, 2,?, k 
A
iV 1- ,
H
iV 1-
btaining new AiV '
Apply the O operation to ( AiV ' ,
H
iV 1- ), 
obtaining new HiV ' . 
Normalize iV '  by dividing the 
A
normalization  ? 2)'( A(p)w  to 
'  by dividing the 
obtain AiV . 
Normalize V Hi
normalization factor ? 2)'( H(p)w  to 
End 
R
In sExt , term candidates with high 
authority in 
dom terms wh
high uments are m
likel
on this observation, the termhood of each 
candidate term TermC, denoted as TermhoodC, is 
calculated according to formula (3) defined 
be
obtain iV . 
eturn ( AkV , 
H
kV ) 
 Corpu ract
H
a few documents are likely to be 
ain specific ereas candidates with 
 authority in many doc ore 
y to be commonly used general words. Based 
low. 
)log()(
Cj
A
jC DF
D
w(C)Termhood ?=      (3) 
where Ajw(C)  is the authority of TermC in a 
document Dj of CorpusExtract, |D| is the total 
number of documents in CorpusExtract and DFC is 
the total number of documents in which TermC 
occurs. Term s
termhood val
C  are then ranked according to their 
ues TermhoodC, and the top ranked 
NTCList candidates are considered terms. NT
an algorithm parameter to be determined 
entally
e two sets of non-overlapping 
academic papers in the IT domain and 
CorpusIT_Small is identical to the corpus used in 
TV_ConSem(Ji and Lu, 2007). CorpusLegal_Small is 
a complete set of official Chinese criminal law 
articles. CorpusLegal_Large includes the complete set 
CList is 
experim . 
3 Performance Evaluation 
3.1 Data Preparation 
To evaluate the performance of the proposed 
algorithms for Chinese, experiments are 
conducted on four corpora of two different 
domains as listed in Table 1. CorpusIT_Small and 
CorpusIT_Large ar
1035
of ficial Chinese constitutional lof aw articles and 
d
Economics/Finance law articles (http://www.law-
lib.com/). Three domain lexicons used in the 
experiments are detailed in Table 2. LexiconIT is 
obtained according to the term extraction 
algorithm (Ji and Lu, 2007) with manual 
verification. LexiconLegal is extracted from 
CorpusLegal_Small by manual verification too. 
Because legal text covers a lot of different areas 
such finance, science, advertisement, etc., the 
actually legal specific terms are relatively small 
in size. LexiconPKU contains a total of 144K 
manually verified IT terms supplied by the 
Institute of Computational Linguistics, Peking 
University. LexiconPKU, is used as the standard 
term set for evaluation on the IT domain. 
CorpusIT_Small and LexiconIT are used to obtain the 
delimiter list of IT domain, DListIT. 
CorpusLegal_Small and LexiconLegal are used to 
obtain the elimiter list of legal domain, 
DListLegal. CorpusIT_Large and CorpusLegal_Large are 
used as open test data to evaluate the proposed 
algorithms in IT domain and legal domain, 
respectively.  
Corpus Domain Size Text type
CorpusIT_Small IT 77K Academic 
papers 
CorpusIT_Large IT 6.64M Academic 
papers 
Corpus 
Legal_Small
Legal 344K Law  
article 
Corpus 
Legal_Large
Legal 1.04M Law  
article 
Table 1. Different Corpora Used for Experiments 
Lexicon Domain Size Source 
LexiconIT IT 3,337 Corpus 
IT_Small
Lex 3
 
iconLegal Legal 94 Corpus 
Legal_Small
LexiconPKU IT 144K PKU 
Table 2. D rent Le se
xperime
rify that the approach works with a 
op word list without delimiter extraction, 
rd list, , is d a ence 
 the 494 general purpose stop words 
web www n) thou  
. 
 in the IT 
alua e 
follow formula: 
iffe
E
xicons U
nts 
d for 
To ve
simple st
a stop wo DListSW also use s refer
by taking
downloaded from a Chinese NLP resource 
site (
ion
.nlp.org.c wi t any
modificat
The performance of the algorithm
domain is ev ted by precision according to th
TCList
NewLexicon N+
TE N
N
recis =
where tes in 
term candidate xtracted by an 
ev
e verification of all the new terms 
is 
arked them as correct terms. As 
there is no reasonably large standard l
list available, the evaluation of the leg
p ion            (4) 
NTCList is the number of term candida
list TCList e
aluated algorithm, NLexicon denotes the number 
of term candidates in TCList contained in 
LexiconPKU, NNew denotes the number of extracted 
term candidates that are not in LexiconPKU, yet 
are considered correct. Thus, NNew is the number 
of newly discovered terms with respect to 
LexiconPKU. Th
carried out manually by two experts 
independently. A new term is considered correct 
if both experts m
egal term 
al domain 
in terms of precision is conducted manually. No 
evaluation on new term extraction is conducted. 
To evaluate the ability of the algorithms in 
identify new terms in the IT domain, another 
measurement is applied to the IT corpus against 
LexiconPKU based on the following formula: 
TCList
New
NTE N
N
R =                          (5) 
where TCList and NNew are the same as given in 
formula (4). A higher RNTE indicates that more 
extracted terms are outside of LexiconPKU and are 
thus considered new terms. This is similar to the 
measurements of out of vocabulary (OOV) in 
Chinese segmentation. A higher RNTE indicates 
the algorithm can be useful for domain 
knowledge update including lexicon expansion. 
3.2 Evaluation on Term Extraction 
For comparison, a statistical based term 
candidate extraction algorithm, TCE_SEF&CV 
with the best performance in 
using both internal association and external 
e 
; one is a 
(Ji and Lu, 2007) 
strength, is used as the reference algorithm for 
the evaluation of TCE_DI. A statistics based term 
verification algorithm, TV_ConSem (Ji and Lu, 
2007) using semantic information within a 
context window is used for the evaluation of 
TV_LinkA. LexiconPKU is also used in 
TV_ConSem. Two popular methods integrated 
without division of candidate extraction and 
verification steps are used for comparison. Th
first one is based on TF-IDF (Salton and McGill, 
1983 Frank et al, 1999). The second 
supervised learning approach based on a SVM 
classifier, SVMlight (Joachims, 1999). The 
features used by SVMlight are shown in Table 3. 
Two training sets are constructed for the SVM 
classifier. The first one includes 3,337 positive 
examples (LexiconIT) and 5,950 negative 
examples extracted from CorpusIT_Small. The 
second one includes 394 positive examples 
1036
(LexiconLegal) and 28,051 negative examples 
extracted from CorpusLegal_Small.  
No. Feature Explanation 
1 Percentage of the Chinese characters 
occurred in LexiconDomain
2 Frequency in the domain corpus 
3 Frequency in the general corpus 
4 Part of speech 
5 The length of Chinese characters in 
the candidate 
6 The length of non-Chinese 
characters in the candidate 
7 Contextual evidence 
Table 3. Features Used in the SVM Classifier 
perforFigure 2 shows the mance of the 
proposed TCE_ for term 
ex tio s 
for IT do IT
TCE_DIl tracted 
de iter  NDI = 
50 esp SW
word list 
DI and TV_LinkA 
trac n compared to the reference algorithm
main using CorpusIT_Large. TCE_DI  and 
egal indicate TCE_DI using ex
lim  lists DListIT and DListLegal with
I  simply uses the stop 0, r ectively. TCE_D
DListSW. 
0 1000 2000 3000 4000 5000
40
45
50
55
60
65
70
75
80
85
90
95
100
Pr
ec
is
io
n
Extracted Terms (N
TCList
)
 TCE_DI
IT
+TV_LinkA
 TCE_DI
Legal
+TV_LinkA
 TCE_DI +TV_L
SW
inkA
 TCE_SEF&CV+TV_LinkA
 TCE_DI
IT
+TV_ConSem
 TCE_SEF&CV+TV_ConSem
 TF-IDF
 SVM
Figure 2 Performance of Different Algorithms on 
IT Domain 
As shown in Figure 2, term extraction based 
on TCE_DIIT combined with TV_LinkA gives the 
best performance. It achieves 75.4% precision 
when the number of extracted terms NTCList 
reaches 5,000. The performance is 9.6% and 
29.4% higher in precision compared to TF-IDF 
and TCE_SEF&CV combined with TV_ConSem, 
respectively. These translate to improvements of 
ver 14.8% and 63.9%, respectively.  
When applying the same TV_LinkA algorithm 
for term verification, TCE_DI using different 
delimiter lists provide 24% better performance on 
average compared to the TCE_SEF&CV 
algorithm which translates to improvement of 
over 47%. The result from using delimiters of 
legal domain (DListLegal) to data in IT domain (as 
shown in TCE_DIlegal) is better on average than 
using a simple general stop word list. It should be 
ver, that TCE_DISW still performs 
much better than the reference algorithms, which 
means that delimiter based term candidate 
extraction algorithm can improve performance 
even without any domain specific training. When 
applying the same TCE_DIIT algorithm in term 
candidate extraction, TV_LinkA provides 10% 
higher performance compared to the TV_ConSem 
TV_LinkA using o word list without 
an
of
precision of o
noted, howe
algorithm which translates to improvement of 
over 15.3%. It is important to point out that 
nly the stop 
y domain specific knowledge performs better 
than TV_ConSem using a large domain lexicon. 
In other words, delimiter based extraction with 
link analysis use much less resources and still 
improve performance of TV_ConSem. 
The performance of TCE_DIIT or 
TCE_SEF&CV combined with TV_ConSem have 
an upward trend when more terms are extracted 
which seems to be against intuition. The principle 
 the TV_ConSem algorithm is that a candidate 
is considered a valid term if a majority of its 
context words already appear in the domain 
lexicon. General words are more likely to be 
ranked on top because they are commonly used 
which explains the low performance of 
TV_ConSem in the lower range of NTCList. When 
NTCList increases, more domain terms are included. 
Thus, there is an upward trend in precision. But, 
the upward trend reverts at around 4,500 because 
the measurement in percentage is too low to 
distinguish valid terms from non-term candidates.  
It is also interesting to point out that the simple 
TF-IDF algorithm which was rarely used in 
Chinese term extraction performs as well as the 
SVM classifier. The main reason is that the test 
corpus consists of academic papers. So, many 
terms are consistent and repeated a lot of times in 
different documents which accords with the idea 
of TF-IDF. Thus, TF-IDF performs relatively 
well because of the high-quality domain corpus. 
However, TF-IDF, as a statistics based algorithm 
suffers from similar problem as others based on 
1037
statistics. Thus it does not perform as well as the 
proposed TCE_DI and TV_LinkA algorithms. 
ac
Figure 3 shows that the proposed algorithms 
achieve similar performance on the legal domain. 
TCE_DILegal combined with TV_LinkA perform 
the best. The result from using IT domain 
delimiters (DListIT) in legal domain as shown in 
TCE_DIIT is better on average than using the 
general purpose stop list. This further proves that 
extracted delimiter list even from a different 
domain can be more effective than a general stop 
word list. When applying the same TV_LinkA 
algorithm for term verification, TCE_DI using 
different delimiter lists are better than all the 
reference algorithms. Without large lexicon in 
Chinese legal domain, the TV_ConSem algorithm 
does not even work.  TV_LinkA using no prior 
domain knowledge for term verification still 
hieves similar improvement compared to that 
of the IT domain where a comprehensive domain 
lexicon is available. 
70
80
90
100
0 1000 2000 3000 4000 5000
40
50
60
Extracted Terms (N
TCList
)
Pr
ec
is
io
n
 TCE_DI
IT
+TV_LinkA
 TCE_DI
Legal
+TV_LinkA
 TCE_DI
SW
+TV_LinkA
 TCE_SEF&CV+TV_LinkA
 TF-IDF
 SVM
 Figure 3. Performance of Different Algorithms 
on Legal Domain 
There are three main reasons for the 
performance improvements of the proposed 
TCE_DI and TV_LinkA algorithms. Firstly, the 
delimiters which are mainly functional words (e. 
g. ???(at/in), ???(or)) and general substantive 
(e.g. ???(be), ????(adopt)) can be extracted 
easily and are effective term boundary markers 
since they are quite domain independent and 
stable. Secondly, the granularity of domain 
specific terms extracted the proposed algorithm is 
much larger than words obtained by word 
segmentation. This keeps many noisy strings out 
of the term candidate set. Thus, the proposed 
delimiter based algorithm performs much better 
over segmentation based statistical methods. 
Thirdly, the proposed approach is not as sensitive 
to term frequency as other statistical based 
approaches because term candidates are 
identified without regards to the frequencies of 
the candidates. In the TV_LinkA algorithm, terms 
are verified by calculating the relevance between 
candidates and the sentences instead of the 
distributions of terms in different types of 
documents. Terms having low frequencies can be 
identified as long as they are in domain relevant 
sentences whereas in the previous approaches 
including TF-IDF, terms with less statistical 
significance are weeded out. For example, a long 
IT term ????????? (Hierarchical storage 
system) with a low frequency of 6 is extracted 
using the proposed approach. It cannot be 
i  
information is is term cannot 
be extracte
dentified by TF-IDF since the statistical
not significant. Th
d by the segmentation based 
algorithms either because general segmentor split 
long terms into pieces making them difficult to 
be reunited using term extraction techniques.  
It is interesting to know that the proposed 
approach not only achieves the best performance 
for both domains, it also achieves second best 
when using extracted delimiters from a different 
domain. The results confirm that delimiters are 
quite stable across domains and the relevance 
between candidates and sentences are efficient 
for distinguishing terms from non-terms in 
different domains. In fact, the proposed approach 
can be applied to different domains with minimal 
training or no training if resources are limited. 
3.3 Evaluation on New Term Extraction 
As LexiconPKU is the only ready-to-use domain 
lexicon, the evaluation on new term extraction is 
conducted on CorpusIT_Large only. Figure 4 shows 
the evaluation of the proposed algorithms 
compared to the reference algorithms in terms of 
RNTE, the ratio of new terms among all identified 
terms.  
It can be seen that the proposed algorithms 
TCE_DIIT combined with TV_LinkA is basically 
the top performer throughout the range. It can 
identify 4% (with respect to TCE_SEF&CV 
+TV_ConSem) to 27% (with respect to TF-IDF) 
more new terms when NTCList reaches 5,000 which 
translate to improvements of over 9% to 170%, 
respectively. The second best performer is 
TCE_DIlegal combined with TV_LinkA using 
delimiters of legal domain. In fact, it only 
underperforms in the lower range of NTCList 
1038
compared to TCE_DIIT. When NTCList reaches 
5,000, their performance is basically the same. 
However, the TCE_DISW algorithm using s
context words occur in the domain lexicon than 
that of other terms. Thus, new terms are actually 
ranked higher than other terms in TV_ConSem 
which explains its higher ability to identify new 
terms in the low range of NTCList. However, its 
performance drops in the high range of NTCList 
because the influence of context words 
diminishes in terms of percentage in the domain 
lexicon to distinguish terms from non-terms. 
Figure 4 also shows that TF-IDF and SVM 
perform the worst in new term extraction 
compared to other algorithms. TF-IDF has 
relatively low ability to identify new terms since 
new terms are not widely used and they do not 
repeat a lot of times in many documents. As  
SVM  is sensitive to training data, it is naturally 
not adaptive to new terms. 
All current Chinese term extraction algorithms 
rely on segmentation with comprehensive lexical 
knowledge and y
a  
problem. T xtraction 
pa
top 
wo
in 
mi ion 
 a 
rds performs much worse than using extracted 
delimiter lists as shown for TCE_DIIT and 
TCE_DIlegal. In the TCE_DI algorithm, character 
strings are split by delimiters and the remained 
parts are taken as term candidates. Generally 
speaking, if a new term contains a delimiter or a 
stop word as its component, it cannot be 
identified correctly. Consequently, if a new term 
contains a stop word as its component, it cannot 
be extracted correctly using TCE_DISW.  
However, new terms are less likely to conta
deli ters because the delimiter extract
algorithm DList_Ext would not consider
component as a delimiter if it is contained in a 
term in LexiconDomain. Consequently, TCE_DISW 
is less adaptive to domain specific data compared 
to TCE_DIIT and TCE_DIlegal. That is also why 
TCE_DISW picks up new terms much more slowly. 
0 1000 2000 3000 4000 5000
0
10
20
30
40
50
Pe
rc
en
ta
ge
 o
f N
ew
 T
er
m
s
Extracted Terms (N
TCList
)
 TCE_DI +TV_LinkA
IT
 TCE_DI
Legal
+TV_LinkA
 TCE_DI
SW
+TV_LinkA
 TCE_SEF&CV+TV_LinkA
 TCE_DI
IT
+TV_ConSem
 TCE_SEF&CV+TV_ConSem
 TF-IDF
 SVM
Figure 4. Performance of Different Algorithms 
for New Term Extraction 
It is interesting to know that TCE_DIIT 
combined with TV_ConSem identifies more new 
terms in the low range of NTCList. In the 
TV_ConSem algorithm, the major information 
used for term verification is the percentage of the 
context words appear in the domain lexicon. As 
discussed earlier in Section 3.2, TV_ConSem 
ranks commonly used general words higher than 
others which leads to the low precision of 
TV_ConSem for term extraction. A new term 
faces a similar scenario because more of its 
et Chinese segmentation 
lgorithms have the OOV (out of vocabulary)
his makes Chinese term e
rticularly vulnerable to new term extraction. 
The proposed approach, on the other hand, is 
based on delimiters which is more stable, domain 
independent, and OOV independent. Figure 4 
shows that TCE_DI and TV_LinkA using minimal 
training from different domains can extract much 
more new terms than previous techniques. In fact, 
the proposed approach can serve as a much better 
tool to identify new domain terms and can be 
quite effective for domain lexicon expansion. 
4 Conclusion 
In conclusion, this paper presents a robust term 
extraction approach using minimal resources. It 
includes a delimiter based algorithm for term 
candidate extraction and a link analysis based 
algorithm for term verification. The proposed 
approach is not sensitive to term frequency as the 
previous works. It requires no prior domain 
knowledge, no general corpora, no full 
segmentation, and minimal adaptation for new 
domains.  
Experiments for term extraction are conducted 
on IT domain and legal domain, respectively. 
Evaluations indicate that the proposed approach 
has a number of advantages. Firstly, the proposed 
approach can improve precision of term 
extraction quite significantly. Secondly, the fact 
that the proposed approach achieves the best 
performance on two different domains verifies its 
domain independent nature. The proposed 
approach using delimiters extracted from a 
1039
different domain also achieves the second best 
performance which indicates that the delimiters 
are quite stable and domain independent. The 
proposed approach still performs much better 
than the reference algorithms when using a 
general purpose stop word list, which means that 
the proposed approach can improve performance 
well even as a completely unsupervised approach 
without any training. Consequently, the results 
demonstrate that the proposed approach can be 
applied to different domains easily even without 
he proposed approach is 
R
 August 2002. 
a. 2002. A measure of term 
ed on the number of co-
inese Word 
Segmentation: Tokenization, Character 
n, or Wordbreak Identification. In 
Ka K., and B. Umino. 1996. Methods of 
Kl
onment. In Proceedings of the 9th 
Ji 
2 ? 74. 
Lu
 Measures. In 
M
e Identification and Semantic 
Na
Sc rafsky D. 2001. Is Knowledge-free 
So
ord 
Vl
Zh
training. Thirdly, t
particularly good for identifying new terms so 
that it can serve as an effective tool for domain 
lexicon expansion. 
Acknowledgements 
This work was done while the first author was 
working at the Hong Kong Polytechnic 
University supported by CERG Grant B-Q941 
and Central Research Grant: G-U297.
eferences 
Chang Jing-Shin. 2005. Domain Specific Word 
Extraction from Hierarchical Web Documents: A 
First Step toward Building Lexicon Trees from 
Web Corpora. In Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language Learning: 
64-71. 
Chien LF. 1999. Pat-tree-based adaptive keyphrase 
extraction for intelligent Chinese information 
retrieval. Information Processing and Management, 
vol.35: 501-521. 
Eibe Frank, Gordon. W. Paynter, Ian H. Witten, Carl 
Gutwin, and Craig G. Nevill-Manning. 1999. 
Domain-specific Keyphrase Extraction. In 
Proceedings of 16th International Joint Conference 
on Artificial Intelligence IJCAI-99: 668-673. 
Feng Haodi, Kang Chen, Xiaotie Deng , and Weimin 
Zheng, 2004. Accessor variety criteria for Chinese 
word extraction. Computational Linguistics, 
30(1):75-93. 
Hiroshi Nakagawa, and Tatsunori Mori. 2002. A 
simple but powerful automatic term extraction 
method. In COMPUTERM-2002 Proceedings of 
the 2nd International Workshop on Computational 
Term: 29-35. Taiwan,
Hisamitsu T., and Y. Niw
representativeness bas
occurring salient words. In Proceedings of the 19th 
COLING, 2002. 
Huang Chu-Ren, Petr ?Simon, Shu-Kai Hsieh, and 
Laurent Pr?evot. 2007. Rethinking Ch
Classificatio
Proceedings of the ACL 2007 Demo and Poster 
Sessions: 69?72. Joachims T. 2000. Estimating the 
Generalization Performance of a SVM Efficiently. 
In Proceedings of the International Conference on 
Machine Learning, Morgan Kaufman, 2000. 
geura 
automatic term recognition: a review. Term 
3(2):259-289. 
einberg J. 1997. Authoritative sources in a 
hyperlinked envir
ACM-SIAM Symposium on Discrete Algorithms: 
668-677. New Orleans, America, January 1997. 
Luning, and Qin Lu. 2007. Chinese Term Extraction 
Using Window-Based Contextual Information. In 
Proceedings of CICLing 2007, LNCS 4394: 6
Li Hongqiao, Chang-Ning Huang, Jianfeng Gao, and 
Xiaozhong Fan. The Use of SVM for Chinese New 
Word Identification. In Proceedings of the 1st 
International Joint Conference on Natural 
Language Processing ( IJCNL P2004): 723-732. 
Hainan Island, China, March 2004. 
o Shengfen, and Maosong Sun. 2003. Two-
Character Chinese Word Extraction Based on 
Hybrid of Internal and Contextual
Proceedings of the Second SIGHAN Workshop on 
Chinese Language Processing: 24-30. 
cDonald, David D. 1993. Internal and External 
Evidence in th
Categorization of Proper Names. In Proceedings of 
the Workshop on Acquisition of Lexical 
Knowledge from Text, pages 32--43, Columbus, 
OH, June. Special Interest Group on the Lexicon of 
the Association for Computational Linguistics. 
sreen AbdulJaleel and Yan Qu. 2005. Domain 
Term Extraction and Structuring via Link Analysis. 
In Proceedings of the AAAI '05 Workshop on Link 
Analysis: 39-46. 
Salton, G., and McGill, M.J. (1983). Introduction to 
Modern Information Retrieval. McGraw-Hill. 
hone, P. and Ju
Induction of Multiword Unit Dictionary Headwords 
a solved problem? In Proceedings of EMNLP2001. 
rnlertlamvanich V., Potipiti T., and Charoenporn T. 
2000. Automatic Corpus-based Thai W
Extraction with the C4.5 Learning Algorithm. In 
Proceedings of COLING 2000. 
adimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer, 1995. 
ou GD, Shen D, Zhang J, Su J, and Tan SH. 2005. 
Recognition of Protein/Gene Names from Text 
using an Ensemble of Classifiers. BMC 
Bioinformatics 2005, 6(Suppl 1):S7. 
1040
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 213?216,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Chinese Term Extraction Using Different Types of Relevance 
 
 
Yuhang Yang1, Tiejun Zhao1, Qin Lu2, Dequan Zheng1 and Hao Yu1
1School of Computer Science and Technology,  
Harbin Institute of Technology, Harbin 150001, China 
{yhyang,tjzhao,dqzheng,yu}@mtlab.hit.edu.cn 
2Department of Computing,  
The Hong Kong Polytechnic University, Hong Kong, China 
csluqin@comp.polyu.edu.hk 
 
  
 
Abstract 
This paper presents a new term extraction ap-
proach using relevance between term candi-
dates calculated by a link analysis based 
method. Different types of relevance are used 
separately or jointly for term verification. The 
proposed approach requires no prior domain 
knowledge and no adaptation for new domains. 
Consequently, the method can be used in any 
domain corpus and it is especially useful for 
resource-limited domains. Evaluations con-
ducted on two different domains for Chinese 
term extraction show significant improve-
ments over existing techniques and also verify 
the efficiency and relative domain independent 
nature of the approach. 
1 Introduction 
Terms are the lexical units to represent the most 
fundamental knowledge of a domain. Term ex-
traction is an essential task in domain knowledge 
acquisition which can be used for lexicon update, 
domain ontology construction, etc. Term extrac-
tion involves two steps. The first step extracts 
candidates by unithood calculation to qualify a 
string as a valid term. The second step verifies 
them through termhood measures (Kageura and 
Umino, 1996) to validate their domain specificity.  
Many previous studies are conducted on term 
candidate extraction. Other tasks such as named 
entity recognition, meaningful word extraction 
and unknown word detection, use techniques 
similar to that for term candidate extraction. But, 
their focuses are not on domain specificity. This 
study focuses on the verification of candidates by 
termhood calculation.  
Relevance between term candidates and docu-
ments is the most popular feature used for term 
verification such as TF-IDF (Salton and McGill, 
1983; Frank, 1999) and Inter-Domain Entropy 
(Chang, 2005), which are all based on the hy-
pothesis that ?if a candidate occurs frequently in 
a few documents of a domain, it is likely a term?. 
Limited distribution information of term candi-
dates in different documents often limits the abil-
ity of such algorithms to distinguish terms from 
non-terms. There are also attempts to use prior 
domain specific knowledge and annotated cor-
pora for term verification. TV_ConSem (Ji and 
Lu, 2007) calculates the percentage of context 
words in a domain lexicon using both frequency 
information and semantic information. However, 
this technique requires a domain lexicon whose 
size and quality have great impact on the per-
formance of the algorithm. Some supervised 
learning approaches have been applied to pro-
tein/gene name recognition (Zhou et al, 2005) 
and Chinese new word identification (Li et al, 
2004) using SVM classifiers (Vapnik, 1995) 
which also require large domain corpora and an-
notations. The latest work by Yang (2008) ap-
plied the relevance between term candidates and 
sentences by using the link analysis approach 
based on the HITS algorithm to achieve better 
performance. 
In this work, a new feature on the relevance 
between different term candidates is integrated 
with other features to validate their domain 
specificity. The relevance between candidate 
terms may be useful to identify domain specific 
terms based on two assumptions. First, terms are 
more likely to occur with other terms in order to 
express domain information. Second, term can-
didates extracted from domain corpora are likely 
213
to be domain specific. Previous work by (e.g. Ji 
and Lu, 2007) uses similar information by com-
paring the context to an existing large domain 
lexicon. In this study, the relevance between 
term candidates are iteratively calculated by 
graphs using link analysis algorithm to avoid the 
dependency on prior domain knowledge.  
The rest of the paper is organized as follows. 
Section 2 describes the proposed algorithms. 
Section 3 explains the experiments and the per-
formance evaluation. Section 4 concludes and 
presents the future plans. 
2 Methodology 
This study assumes the availability of term can-
didates since the focus is on term verification by 
termhood calculation. Three types of relevance 
are first calculated including (1) the term candi-
date relevance, CC; (2) the candidate to sentence 
relevance, CS; and the candidates to document 
relevance, CD. Terms are then verified by using 
different types of relevance. 
2.1 Relevance between Term Candidates 
Based on the assumptions that term candidates 
are likely to be used together in order to repre-
sent a particular domain concept, relevance of 
term candidates can be represented by graphs in 
a domain corpus. In this study, CC is defined as 
their co-occurrence in the same sentence of the 
domain corpus. For each document, a graph of 
term candidates is first constructed. In the graph, 
a node is a term candidate. If two term candi-
dates TC1 and TC2 occur in the same sentence, 
two directional links between TC1 to TC2 are 
given to indicate their mutually related. Candi-
dates with overlapped substrings are not removed 
which means long terms can be linked to their 
components if the components are also candi-
dates.  
After graph construction, the term candidate 
relevance, CC, is then iteratively calculated using 
the PageRank algorithm (Page et al 1998) origi-
nally proposed for information retrieval. PageR-
ank assumes that the more a node is connected to 
other nodes, it is more likely to be a salient node. 
The algorithm assigns the significance score to 
each node according to the number of nodes link-
ing to it as well as the significance of the nodes. 
The PageRank calculation PR of a node A is 
shown as follows:  
)
)(
)(
...
)(
)(
)(
)(
()1()(
2
2
1
1
t
t
BC
BPR
BC
BPR
BC
BPR
ddAPR ++++?=
(1) 
where B1, B2,?, Bt are all nodes linked to node A; 
C(Bi) is the number of outgoing links from node 
Bi; d is the factor to avoid loop trap in the 
graphic structure. d is set to 0.85 as suggested in 
(Page et al, 1998). Initially, all PR weights are 
set to 1. The weight score of each node are ob-
tained by (1), iteratively. The significance of 
each term candidate in the domain specific cor-
pus is then derived based on the significance of 
other candidates it co-occurred with. The CC 
weight of term candidate TCi is given by its PR 
value after k iterations, a parameter to be deter-
mined experimentally. 
2.2 Relevance between Term Candidates 
and Sentences 
A domain specific term is more likely to be con-
tained in domain relevant sentences. Relevance 
between term candidate and sentences, referred 
to as CS, is calculated using the TV_HITS (Term 
Verification ? HITS) algorithm proposed in 
(Yang et al, 2008) based on  Hyperlink-Induced 
Topic Search (HITS) algorithm (Kleinberg, 
1997). In TV_HITS, a good hub in the domain 
corpus is a sentence that contains many good 
authorities; a good authority is a term candidate 
that is contained in many good hubs.  
In TV_HITS, a node p can either be a sentence 
or a term candidate. If a term candidate TC is 
contained in a sentence Sen of the domain corpus, 
there is a directional link from Sen to TC. 
TV_HITS then makes use of the relationship be-
tween candidates and sentences via an iterative 
process to update CS weight for each TC.  
Let VA(w(p1)A, w(p2)A,?, w(pn)A) denote the 
authority vector and VH(w(p1)H, w(p2)H,?, w(pn)H) 
denote the hub vector. VA and VH are initialized 
to (1, 1,?, 1). Given weights VA and VH with a 
directional link p?q, w(q)A and w(p)H are up-
dated by using the I operation(an in-pointer to a 
node) and the O operation(an out-pointer to a 
node) shown as follows. The CS weight of term 
candidate TCi is given by its w(q)A value after 
iteration. 
I operation:          (2) ?
??
=
Eqp
HA w(p)w(q)
O operation:         (3) ?
??
=
Eqp
AH w(q)w(p)
2.3 Relevance between Term Candidates 
and Documents 
The relevance between term candidates and 
documents is used in many term extraction algo-
214
rithms. The relevance is measured by the TF-IDF 
value according to the following equations: 
)IDF(TC)TF(TC)TFIDF(TC iii ?=      (4) 
)
)(
log()(
i
i TCDF
D
TCIDF =             (5) 
where TF(TCi) is the number of times term can-
didate TCi occurs in the domain corpus, DF(TCi) 
is the number of documents in which TCi occurs 
at least once, |D| is the total number of docu-
ments in the corpus, IDF(TCi) is the inverse 
document frequency which can be calculated 
from the document frequency. 
2.4 Combination of Relevance 
To evaluate the effective of the different types of 
relevance, they are combined in different ways in 
the evaluation. Term candidates are then ranked 
according to the corresponding termhood values 
Th(TC) and the top ranked candidates are con-
sidered terms.  
For each document Dj in the domain corpus 
where a term candidate TCi occurs, there is CCij 
weight and a CSij weight. When features CC and 
CS are used separately, termhood ThCC(TCi) and 
ThCS(TCi) are calculated by averaging CCij and 
CSij, respectively. Termhood of different combi-
nations are given in formula (6) to (9). R(TCi) 
denotes the ranking position of TCi.  
)(TCR)(TCR
)(TCTh
iCSiCC
iCSCC
11 +=+    (6) 
)log()()(
Cj
ijiCDCC DF
D
CCTCTh ?=+     (7) 
)log()()(
Cj
ijiCDCS DF
D
CSTCTh ?=+     (8) 
)(TCR)(TCR
TCTh
iCDCSiCDCC
iCDCSCC
++
++ += 11)( (9) 
3 Performance Evaluation 
3.1 Data Preparation 
To evaluate the performance of the proposed 
relevance measures for Chinese in different do-
mains, experiments are conducted on two sepa-
rate domain corpora CorpusIT and CorpusLegal., 
respectively. CorpusIT includes academic papers 
of 6.64M in size from Chinese IT journals be-
tween 1998 and 2000. CorpusLegal includes the 
complete set of official Chinese constitutional 
law articles and Economics/Finance law articles 
of 1.04M in size (http://www.law-lib.com/).  
For comparison to previous work, all term 
candidates are extracted from the same domain 
corpora using the delimiter based algorithm 
TCE_DI (Term Candidate Extraction ? Delimiter 
Identification) which is efficient according to 
(Yang et al, 2008). In TCE_DI, term delimiters 
are identified first. Words between delimiters are 
then taken as term candidates. 
The performances are evaluated in terms of 
precision (P), recall (R) and F-value (F). Since 
the corpora are relatively large, sampling is used 
for evaluation based on fixed interval of 1 in 
each 10 ranked results. The verification of all the 
sampled data is carried out manually by two ex-
perts independently. To evaluate the recall, a set 
of correct terms which are manually verified 
from the extracted terms by different methods is 
constructed as the standard answer. The answer 
set is certainly not complete. But it is useful as a 
performance indication for comparison since it is 
fair to all algorithms. 
3.2 Evaluation on Term Extraction 
For comparison, three reference algorithms are 
used in the evaluation. The first algorithm is 
TV_LinkA which takes CS and CD into consid-
eration and performs well (Yang et al, 2008). 
The second one is a supervised learning ap-
proach based on a SVM classifier, SVMlight 
(Joachims, 1999). Internal and external features 
are used by SVMlight. The third algorithm is the 
popular used TF-IDF algorithm. All the refer-
ence algorithms require no training except 
SVMlight. Two training sets containing thousands 
of positive and negative examples from IT do-
main and legal domain are constructed for the 
SVM classifier. The training and testing sets are 
not overlapped. 
Table 1 and Table 2 show the performance of 
the proposed algorithms using different features 
for IT domain and legal domain, respectively. 
The algorithm using CD alone is the same as the 
TF-IDF algorithm. The algorithm using CS and 
CD is the TV_LinkA algorithm.  
Algorithms Precision 
(%) 
Recall 
(%) 
F-value 
(%) 
SVM 63.6 49.5 55.6 
CC 47.1 36.5 41.2 
CS 65.6 51 57.4 
CD(TF-IDF) 64.8 50.4 56.7 
CC+CS 80.4 62.5 70.3 
CC+CD 49 38.1 42.9 
CS+CD 
(TV_LinkA) 
75.4 58.6 66 
CC+CS+CD 82.8 64.4 72.4 
Table 1. Performance on IT Domain 
215
 Algorithms Precision 
(%) 
Recall 
(%) 
F-value 
(%) 
SVM 60.1 54.2 57.3 
CC 45.2 40.3 42.6 
CS 70.5 40.1 51.1 
CD(TF-IDF) 59.4 52.9 56 
CC+CS 64.2 49.9 56.1 
CC+CD 48.4 43.1 45.6 
CS+CD 
(TV_LinkA) 
67.4 60.1 63.5 
CC+CS+CD 70.2 62.6 66.2 
Table 2. Performance on Legal Domain 
Table 1 and Table 2 show that the proposed 
algorithms achieve similar performance on both 
domains. The proposed algorithm using all three 
features (CC+CS+CD) performs the best. The 
results confirm that the proposed approach are 
quite stable across domains and the relevance 
between candidates are efficient for improving 
performance of term extraction in different do-
mains. The algorithm using CC only does not 
achieve good performance. Neither does CC+CS. 
The main reason is that the term candidates used 
in the experiments are extracted using the 
TCE_DI algorithm which can extract candidates 
with low statistical significance. TCE_DI pro-
vides a better compromise between recall and 
precision. CC alone is vulnerable to noisy candi-
dates since it relies on the relevance between 
candidates themselves. However, as an addi-
tional feature to the combined use of CS and CD 
(TV_LinkA), improvement of over 10% on F-
value is obtained for the IT domain, and 5% for 
the legal domain. This is because the noise data 
are eliminated by CS and CD, and CC help to 
identify additional terms that may not be statisti-
cally significant.  
4 Conclusion and Future Work 
In conclusion, this paper exploits the relevance 
between term candidates as an additional feature 
for term extraction approach. The proposed ap-
proach requires no prior domain knowledge and 
no adaptation for new domains. Experiments for 
term extraction are conducted on IT domain and 
legal domain, respectively. Evaluations indicate 
that the proposed algorithm using different types 
of relevance achieves the best performance in 
both domains without training.  
In this work, only co-occurrence in a sentence 
is used as the relevance between term candidates. 
Other features such as syntactic relations can 
also be exploited. The performance may be fur-
ther improved by using more efficient combina-
tion strategies. It would also be interesting to 
apply this approach to other languages such as 
English. 
Acknowledgement: The project is partially sup-
ported by the Hong Kong Polytechnic University 
(PolyU CRG G-U297) 
References 
Chang Jing-Shin. 2005. Domain Specific Word Ex-
traction from Hierarchical Web Documents: A 
First Step toward Building Lexicon Trees from 
Web Corpora. In Proc of the 4th SIGHAN Work-
shop on Chinese Language Learning: 64-71. 
Eibe Frank, Gordon. W. Paynter, Ian H. Witten, Carl 
Gutwin, and Craig G. Nevill-Manning. 1999. Do-
main-specific Keyphrase Extraction. In Proc.of 
16th Int. Joint Conf. on AI,  IJCAI-99: 668-673. 
Joachims T. 2000. Estimating the Generalization Per-
formance of a SVM Efficiently. In Proc. of the Int 
Conf. on Machine Learning, Morgan Kaufman, 
2000. 
Kageura K., and B. Umino. 1996. Methods of auto-
matic term recognition: a review. Term 3(2):259-
289. 
Kleinberg J. 1997. Authoritative sources in a hyper-
linked environment. In Proc. of the 9th ACM-SIAM 
Symposium on Discrete Algorithms: 668-677. New 
Orleans, America, January 1997. 
Ji Luning, and Qin Lu. 2007. Chinese Term Extrac-
tion Using Window-Based Contextual Information. 
In Proc. of CICLing 2007, LNCS 4394: 62 ? 74. 
Li Hongqiao, Chang-Ning Huang, Jianfeng Gao, and 
Xiaozhong Fan. The Use of SVM for Chinese New 
Word Identification. In Proc. of the 1st Int.Joint 
Conf. on NLP (IJCNLP2004): 723-732. Hainan Is-
land, China, March 2004. 
Salton, G., and McGill, M.J. (1983). Introduction to 
Modern Information Retrieval. McGraw-Hill. 
S. Brin, L. Page. The anatomy of a large-scale hyper-
textual web search engine. The 7th Int. World Wide 
Web Conf, Brisbane, Australia, April 1998, 107-
117. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer, 1995. 
Yang Yuhang, Qin Lu, Tiejun Zhao. (2008). Chinese 
Term Extraction Using Minimal Resources. The 
22nd Int. Conf. on Computational Linguistics (Col-
ing 2008). Manchester, Aug., 2008, 1033-1040. 
Zhou GD, Shen D, Zhang J, Su J, and Tan SH. 2005. 
Recognition of Protein/Gene Names from Text us-
ing an Ensemble of Classifiers. BMC Bioinformat-
ics 2005, 6(Suppl 1):S7. 
216
