Story Link Detection based on Dynamic Information Extending
Xiaoyan Zhang Ting Wang Huowang Chen
Department of Computer Science and Technology, School of Computer,
National University of Defense Technology
No.137, Yanwachi Street, Changsha, Hunan 410073, P.R.China
{zhangxiaoyan, tingwang, hwchen}@nudt.edu.cn
Abstract
Topic Detection and Tracking refers to au-
tomatic techniques for locating topically re-
lated materials in streams of data. As the
core technology of it, story link detection is
to determine whether two stories are about
the same topic. To overcome the limitation
of the story length and the topic dynamic
evolution problem in data streams, this pa-
per presents a method of applying dynamic
information extending to improve the per-
formance of link detection. The proposed
method uses previous latest related story to
extend current processing story, generates
new dynamic models for computing the sim-
ilarity between the current two stories. The
work is evaluated on the TDT4 Chinese cor-
pus, and the experimental results indicate
that story link detection using this method
can make much better performance on all
evaluation metrics.
1 Introduction
Topic Detection and Tracking (TDT) (Allan, 2002)
refers to a variety of automatic techniques for dis-
covering and threading together topically related
material in streams of data such as newswire or
broadcast news. Such automatic discovering and
threading could be quite valuable in many appli-
cations where people need timely and efficient ac-
cess to large quantities of information. Supported
by such technology, users could be alerted with new
events and new information about known events. By
examining one or two stories, users define the topic
described in them. Then with TDT technologies
they could go to a large archive, find all the stories
about this topic, and learn how it evolved.
Story link detection, as the core technology de-
fined in TDT, is a task of determining whether two
stories are about the same topic, or topically linked.
In TDT, a topic is defined as ?something that hap-
pens at some specific time and place? (Allan, 2002).
Link detection is considered as the basis of other
event-based TDT tasks, such as topic tracking, topic
detection, and first story detection. Since story link
detection focuses on the streams of news stories,
it has its specific characteristic compared with the
traditional Information Retrieval (IR) or Text Clas-
sification task: new topics usually come forth fre-
quently during the procedure of the task, but nothing
about them is known in advance.
The paper is organized as follows: Section 2 de-
scribes the procedure of story link detection; Section
3 introduces the related work in story link detection;
Section 4 explains a baseline method which will
be compared with the proposed dynamic method in
Section 5; the experiment results and analysis are
given in Section 6; finally, Section 7 concludes the
paper.
2 Problem Definition
In the task definition of story link detection (NIST,
2003), a link detection system is given a se-
quence of time-ordered news source files S =
?S1, S2, S3, . . . , Sn? where each Si includes a set
of stories, and a sequence of time-ordered story
pairs P = ?P1, P2, P3, . . . , Pm? where Pi =
40
(si1, si2), si1 ? Sj , si2 ? Sk, 1 ? i ? m, 1 ? j ?
k ? n. The system is required to make decisions on
all story pairs to judge if they describe a same topic.
We formalize the procedure for processing a pair
of stories as follows:
For a story pair Pi = (si1, si2):
1. Get background corpus Bi of Pi. According to
the supposed application situation and the cus-
tom that people usually look ahead when they
browse something, in TDT research the system
is usually allowed to look ahead N (usually 10)
source files when deciding whether the current
pair is linked. So Bi = {S1, S2, S3, . . . , Sl},
where
l =
{
k + 10 , si2 ? Sk and (k + 10) ? n
n , si2 ? Sk and (k + 10) > n .
2. Produce the representation models (Mi1,Mi2)
for two stories in Pi. M = {(fs, ws) | s ? 1},
where fs is a feature extracted from a story and
ws is the weight of the feature in the story. They
are computed with some parameters counted
from current story and the background.
3. Choose a similarity function F and computing
the similarity between two models. If t is a pre-
defined threshold and F (Mi1,Mi2) ? t, then
stories in Pi are topically linked.
3 Related Work
A number of works has been developed on story link
detection. It can be classified into two categories:
vector-based methods and probabilistic-based meth-
ods.
The vector space model is widely used in IR and
Text Classification research. Cosine similarity be-
tween document vectors with tf?idf term weighting
(Connell et al, 2004) (Chen et al, 2004) (Allan et
al., 2003) is also one of the best technologies for link
detection. We have examined a number of similarity
measures in story link detection, including cosine,
Hellinger and Tanimoto, and found that cosine sim-
ilarity produced outstanding results. Furthermore,
(Allan et al, 2000) also confirms this conclusion
among cosine, weighted sum, language modeling
and Kullback-Leibler divergence in its story link de-
tection research.
Probabilistic-based method has been proven to be
very effective in several IR applications. One of its
attractive features is that it is firmly rooted in the the-
ory of probability, thereby allowing the researcher
to explore more sophisticated models guided by the
theoretical framework. (Nallapati and Allan, 2002)
(Lavrenko et al, 2002) (Nallapati, 2003) all ap-
ply probability models (language model or relevance
model) for story link detection. And the experiment
results indicate that the performances are compara-
ble with those using traditional vector space models,
if not better.
On the basis of vector-based methods, this paper
represents a method of dynamic information extend-
ing to improve the performance of story link detec-
tion. It makes use of the previous latest topically re-
lated story to extend the vector model of current be-
ing processed story. New dynamic models are gen-
erated for computing the similarity between two sto-
ries in current pair. This method resolves the prob-
lems of information shortage in stories and topic dy-
namic evolution in streams of data.
Before introducing the proposed method, we first
describe a method which is implemented with vector
model and cosine similarity function. This straight
and classic method is used as a baseline to be com-
pared with the proposed method.
4 Baseline Story Link Detection
The related work in story link detection shows that
vector representation model with cosine function
can be used to build the state-of-the-art story link de-
tection systems. Many research organizations take
this as their baseline system (Connell et al, 2004)
(Yang et al, 2002). In this paper, we make a similar
choice.
The baseline method represents each story as a
vector in term space, where the coordinates repre-
sent the weights of the term features in the story.
Each vector terms (or feature) is a single word plus
its tag which is produced by a segmenter and part of
speech tagger for Chinese. So if two tokens with
same spelling are tagged with different tags, they
will be taken as different terms (or features). It is
notable that in it is independent between processing
any two comparisons the baseline method.
41
4.1 Preprocessing
A preprocessing has been performed for TDT Chi-
nese corpus. For each story we tokenize the text, tag
the generated tokens, remove stop words, and then
get a candidate set of terms for its vector model. Af-
ter that, the term-frequency for each token in the
story and the length of the story will also be ac-
quired. In the baseline and dynamic methods, both
training and test data are preprocessed in this way.
The segmenter and tagger used here is ICTCLAS
1 . The stop word list is composed of 507 terms. Al-
though the term feature in the vector representation
is the word plus its corresponding tag, we will ig-
nore the tag information when filtering stop words,
because almost all the words in the list should be
filtered out whichever part of speech is used to tag
them.
4.2 Feature Weighting
One important issue in the vector model is weight-
ing the individual terms (features) that occur in the
vector. Most IR systems employed the traditional
tf ? idf weighting, which also provide the base for
the baseline and dynamic methods in this paper. Fur-
thermore, this paper adopts a dynamic way to com-
pute the tf ? idf weighting:
wi(fi, d) = tf(fi, d) ? idf(fi)
tf = t/(t+ 0.5 + 1.5dl/dlavg)
idf = log((N + 0.5)/df)/log(N + 1)
where t is the term frequency in a story, dl is the
length of a story, dlavg is the average length of sto-
ries in the background corpus, N is the number of
stories in the corpus, df is the number of the stories
containing the term in the corpus.
The tf shows how much a term represents the
story, while the idf reflects the distinctive ability
of distinguishing current story from others. The
dynamic attribute of the tf ? idf weighting lies in
the dynamic computation of dlavg, N and df . The
background corpus used for statistics is incremen-
tal. As more story pairs are processed, more source
files could be seen, and the background is expand-
ing as well. Whenever the size of the background
1http://sewm.pku.edu.cn/QA/reference/ICTCLAS/FreeICT-
CLAS/
has changed, the values of dlavg, N and df will up-
date accordingly. We call this as incremental tf ?idf
weighting. A story might have different term vectors
in different story pairs.
4.3 Similarity Function
Another important issue in the vector model is de-
termining the right function to measure the similar-
ity between two vectors. We have firstly tried three
functions: cosine, Hellinger and Tanimoto, among
which cosine function performs best for its substan-
tial advantages and the most stable performance. So
we consider the cosine function in baseline method.
Cosine similarity, as a classic measure and con-
sistent with the vector representation, is simply an
inner product of two vectors where each vector is
normalized to the unit length. It represents cosine
of the angle between two vector models M1 =
{(f1i, w1i), i ? 1} and M2 = {(f2i, w2i), i ? 1}.
cos(M1,M2) = (?(w1i ? w2i))/
?
(?w21i)(?w22i)
Cosine similarity tends to perform best at full di-
mensionality, as in the case of comparing two sto-
ries. Performance degrades as one of the vectors be-
comes shorter. Because of the built-in length nor-
malization, cosine similarity is less dependent on
specific term weighting.
5 Dynamic Story Link Detection
5.1 Motivation
Investigation on the TDT corpus shows that news
stories are usually short, which makes that their rep-
resentation models are too sparse to reflect topics
described in them. A possible method of solving
this problem is to extend stories with other related
information. The information can be synonym in
a dictionary, related documents in external corpora,
etc. However, extending with synonym is mainly
adding repetitious information, which can not define
the topics more clearly. On the other hand, topic-
based research should be real-sensitive. The corpora
in the same period as the test corpora are not easy
to gather, and the number of related documents in
previous period is very few. So it is also not feasi-
ble to extend the stories with related documents in
other corpora. We believe that it is more reason-
able that the best extending information may be the
42
story corpus itself. Following the TDT evaluation
requirement, we will not use entire corpus at a time.
Instead, when we process current pair of stories, we
utilize all the stories before the current pair in the
story corpus.
In addition, topics described by stories usually
evolve along with time. A topic usually begins with
a seminal event. After that, it will focus mainly on
the consequence of the event or other directly re-
lated events as the time goes. When the focus in
later stories has changed, the words used in them
may change remarkably. Keeping topic descrip-
tions unchanged from the beginning to the end is
obviously improper. So topic representation mod-
els should also be updated as the topic emphases in
stories has changed. Formerly we have planed to use
related information to extend a story to make up the
information shortage in stories. Considering more
about topic evolution, we extend a story with its lat-
est related story. In addition, up to now almost all
research in story link detection takes the hypothe-
sis that whether two stories in one pair are topically
linked is independent of that in another pair. But we
realize that if two stories in a pair describe a same
topic, one story can be taken as related information
to extend another story in later pairs. Compared with
extending with more than one story, extending only
with its latest related story can keep representation
of the topic as fresh as possible, and avoid extend-
ing too much similar information at the same time,
which makes the length of the extended vector too
long. Since the vector will be renormalized, a too
big length means evidently decreasing the weight
of an individual feature which will instead cause a
lower cosine similarity. This idea has also been con-
firmed by the experiment showing that the perfor-
mance extending with one latest related story is su-
perior to that extending with more than one related
story, as described in section 6.3. The experiment re-
sults also show that this method of dynamic informa-
tion extending apparently improves the performance
of story link detection.
5.2 Method Description
The proposed dynamic method is actually the base-
line method plus dynamic information extending.
The preprocessing, feature weighting and similarity
computation in dynamic method are similar as those
in baseline method. However, the vector representa-
tion for a story here is dynamic. This method needs a
training corpus to get the extending threshold decid-
ing whether a story should be used to extend another
story in a pair. We split the sequence of time-ordered
story pairs into two parts: the former is for training
and the later is for testing. The following is the pro-
cessing steps:
1. Preprocess to create a set of terms for repre-
senting each story as a term vector, which is
same as baseline method.
2. Run baseline system on the training corpora
and find an optimum topically link threshold.
We take this threshold as extending threshold.
The topically link threshold used for making
link decision in dynamic method is another pre-
defined one.
3. Along with the ordered story pairs in the test
corpora, repeat a) and b):
(a) When processing a pair of stories Pi =
(si1, si2), if si1 or si2 has an extending
story, then update the corresponding vec-
tor model with its related story to a new
dynamic one. The generation procedure
of dynamic vector will be described in
next subsection.
(b) Computing the cosine similarity between
the two dynamic term vectors. If it ex-
ceeds the extending threshold, then si1 and
si2 are the latest related stories for each
other. If one story already has an extend-
ing story, replace the old one with the new
one. So a story always has no more than
one extending story at any time. If the
similarity exceeds topically link threshold,
si1 and si2 are topically linked.
From the above description, it is obvious that dy-
namic method needs two thresholds, one for making
extending decision and the other for making link de-
cision. Since in this paper we will focus on the op-
timum performance of systems, the first threshold is
more important. But topically link threshold is also
necessary to be properly defined to approach a bet-
ter performance. In the baseline method, term vec-
tors are dynamic because of the incremental tf ? idf
43
weighting. However, dynamic information extend-
ing is another more important reason in the dynamic
method. Whenever a story has an extending story, its
vector representation will update to include the ex-
tending information. Having the extending method,
the representation model can have more information
to describe the topic in a story and make the topic
evolve along with time. The dynamic method can
define topic description clearer and get a more accu-
rate similarity between stories.
5.3 Dynamic Vector Model
In the dynamic method, we have tried two ways for
the generation of dynamic vector models: increment
model and average model. Supposing we use vector
model M1 = {(f1i, w1i), i ? 1} of story s1 to ex-
tend vector model M2 = {(f2i, w2i), i ? 1} of story
s2, M2 will change to representing the latest evolv-
ing topic described in current story after extending.
1. Increment Model: For each term f1i in M1, if
it also occurs as f2j in M2, then w2j will not
change, otherwise (f1i, w1i) will be added into
M2. This dynamic vector model only takes in-
terest in the new information that occurs only in
M1. For features both occurred in M1 and M2,
the dynamic model will respect to their original
weights.
2. Average Model: For each term f1i in M1, if
it also occurs as f2j in M2, then w2j = 0.5 ?
(w1i+w2j), otherwise (f1i, w1i) will be added
into M2. This dynamic model will take account
of all information in M1. So the difference be-
tween those two dynamic models is the weight
recalculation method of the feature occurred in
both M1 and M2.
Both the above two dynamic models can take ac-
count of information extending and topic evolution.
Increment Model is closer to topic description since
it is more dependent on latest term weights, while
Average Model makes more reference to the cen-
troid concept. The experiment results show that dy-
namic method with Average Model is a little supe-
rior to that with Increment Model.
6 Experiment and Discussion
6.1 Experiment Data
To evaluate the proposed method, we use the Chi-
nese subset of TDT4 corpus (LDC, 2003) devel-
oped by the Linguistic Data Consortium (LDC) for
TDT research. This subset contains 27145 stories
all in Chinese from October 2000 through January
2001, which are gathered from news, broadcast or
TV shows.
LDC totally labeled 40 topics on TDT4 for 2003
evaluation. There are totally 12334 stories pairs
from 1151 source files in the experiment data. The
answers for these pairs are based on 28 topics of
these topics, generated from the LDC 2003 anno-
tation documents. The first 2334 pairs are used for
training and finding extending threshold of dynamic
method. The rest 10000 pairs are testing data used
for comparing performances of baseline and the dy-
namic methods.
6.2 Evaluation Measures
The work is measured by the TDT evaluation soft-
ware, which could be referred to (Hoogma, 2005)
for detail. Here is a brief description. The goal of
link detection is to minimize the cost due to errors
caused by the system. The TDT tasks are evaluated
by computing a ?detection cost?:
Cdet = Cmiss?Pmiss?Ptarget+Cfa?Pfa?Pnon?target
where Cmiss is the cost of a miss, Pmiss is the es-
timated probability of a miss, Ptarget is the prior
probability under which a pair of stories are linked,
Cfa is the cost of a false alarm, Pfa is the estimated
probability of a false alarm, and Pnon?target is the
prior probability under which a pair of stories are
not linked. A miss occurs when a linked story pair is
not identified as being linked by the system. A false
alarm occurs when the pair of stories that are not
linked are identified as being linked by the system.
A target is a pair of linked stories; conversely a non-
target is a pair of stories that are not linked. For the
link detection task these parameters are set as fol-
lows: Cmiss is 1, Ptarget is 0.02, and Cfa is 0.1. The
cost for each topic is equally weighted (usually the
cost of topic-weighted is the mainly evaluation pa-
rameter) and normalized so that for a given system,
the normalized value (Cdet)norm can be no less than
44
one without extracting information from the source
data:
(Cdet)norm = Cdetmin(CmissPtarget, CfaPnon?target)
(Cdet)overall = ?i(Cidet)norm/#topics
where the sum is over topics i. A detection curve
(DET curve) is computed by sweeping a threshold
over the range of scores, and the minimum cost over
the DET curve is identified as the minimum detec-
tion cost or min DET. The topic-weighted DET cost
is dependent on both a good minimum cost and a
good method for selecting an operating point, which
is usually implemented by selecting a threshold. A
system with a very low min DET cost can have a
much larger topic-weighted DET score. Therefore,
we focus on the minimum DET cost for the experi-
ments.
6.3 Experiment Results
In this paper, we have tried three methods for story
link detection: the baseline method described in
Section 4 and two dynamic methods with different
dynamic vectors introduced in Section 5. The fol-
lowing table gives their evaluation results.
metrics baseline dynamic 1 dynamic 2
Pmiss 0.0514 0.0348 0.0345
Pfa 0.0067 0.0050 0.0050
Clinkmin 0.0017 0.0012 0.0012
Clinknorm 0.0840 0.0591 0.0588
Table 1: Experiment Results of Baseline System and
Dynamic Systems
In the table, Clinkmin is the minimum
(Cdet)overall, DET Graph Minimum Detection
Cost (topic-weighted), Clinknorm is the normal-
ized minimum (Cdet)overall, the dynamic 1 is the
dynamic method which uses Increment Model and
the dynamic 2 is the dynamic method which uses
Average Model. We can see that the proposed two
dynamic methods are both much better than base-
line method on all four metrics. The ClinkNorm
of dynamic 1 and 2 are improved individually by
27.2% and 27.8% as compared to that of baseline
method. The difference between two dynamic
methods is due to different in the Pmiss. However,
it is too little to compare the two dynamic systems.
We also make additional experiments in which a
story is extended with all of its previous related
stories. The minimum (Cdet)overall is 0.0614 for
the system using Increment Model, and 0.0608 for
the system using Average Model. Although the
performances are also much superior to baseline, it
is still a little poorer than that with only one latest
related story, which confirm the ideal described in
section 5.1.
Figure 1, 2 and 3 show the detail evaluation in-
formation for individual topic on Minimum Norm
Detection Cost, Pmiss and Pfa. From Figure 1 we
know these two dynamic methods have improved the
performance on almost all the topic, except topic 12,
26 and 32. Note that detection cost is a function of
Pmiss and Pfa. Figure 2 shows that both two dy-
namic methods reduce the false alarm rates on all
evaluation topics. In Figure 3 there are 20 topics
on which the miss rates remain zero or unchange.
The dynamic methods reduce the miss rates on 5
topics. However, dynamic methods get relatively
poorer results on topic 12, 26 and 32 . Altogether
dynamic methods can notably improve system per-
formance on evaluation metrics of both individual
and weighted topic, especially the false alarm rate,
but on some topics, it gets poorer results.
Further investigation shows that topic 12, 26 and
32 are about Presidential election in Ivory Coast
on October 25, 2000, Airplane Crash in Chiang
Kai Shek International Airport in Taiwan on Octo-
ber 31, 2000, and APEC Conference on Novem-
ber 12-15, 2000 at Brunei. After analyzing those
story pairs with error link decision, we can split
them into two sets. One is that two stories in a pair
are general linked but not TDT specific topically
linked. Here general linked means that there are
many common words in two stories, but the events
described in them happened in different times or dif-
ferent places. For example, Airplane Crash is a gen-
eral topic, while Airplane Crash in certain location
at specification time is a TDT topic. The other is
that two stories in a pair are TDT topically linked
while they describe the topic from different perspec-
tives. In this condition they will have few common
words. These may be due to that the information
extracted from stories is still not accurate enough
to represent them. It also may be because of the
45
00.05
0.1
0.15
0.2
0.25
0.3
0.35
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
Topic ID
N
o
r
m
 
C
l
i
n
k
Norm Clink(Baseline)
Norm Clink(Dynamic 1)
Norm Clink(Dynamic 2)
 
Figure 1: Normalized Minimum Detection Cost for individual topic
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
Topic ID
F
a
l
s
e
 
A
l
a
r
m
 
p
r
o
b
a
b
i
l
i
t
y
Pfa(Baseline)
Pfa(Dynamic 1)
Pfa(Dynamic 2)
 
Figure 2: Pfa for individual topic
0
0.05
0.1
0.15
0.2
0.25
0.3
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
Topic ID
M
i
s
s
 
p
r
o
b
a
b
i
l
i
t
y
Pmiss(Baseline)
Pmiss(Dynamic 1)
Pmiss(Dynamic 2)
 
Figure 3: Pmiss for individual topic
46
deficiency of vector model itself. Furthermore, we
know that the extending story is chosen by cosine
similarity, which results that the extending story and
the extended story are usually topically linked from
the same perspectives, seldom from different per-
spectives. Therefore the method of information ex-
tending may sometimes turn the above first problem
worse and have no impact on the second problem.
So mining more useful information or making more
use of other useful resources to solve these problems
will be the next work. In addition, how to repre-
sent this information with a proper model and seek-
ing better or more proper representation models for
TDT stories are also important issues. In a word,
the method of information extending has been veri-
fied efficient in story link detection and can provide a
reference to improve the performance of some other
similar systems whose data must be processed seri-
ally, and it is also hopeful to combined with other
improvement technologies.
7 Conclusion
Story link detection is a key technique in TDT re-
search. Though many approaches have been tried,
there are still some characters ignored. After analyz-
ing the characters and deficiency in TDT stories and
story link detection, this paper presents a method of
dynamic information extending to improve the sys-
tem performance by focus on two problems: infor-
mation deficiency and topic evolution. The exper-
iment results indicate that this method can effec-
tively improve the performance on both miss and
false alarm rates, especially the later one. How-
ever, we should realize that there are still some prob-
lems to solve in story link detection. How to com-
pare general topically linked stories and how to com-
pare stories describing a TDT topic from different
angles will be very vital to improve system perfor-
mance. The next work will focus on mining more
and deeper useful information in TDT stories and
exploiting more proper models to represent them.
Acknowledgement
This research is supported by the National Natu-
ral Science Foundation of China (60403050), Pro-
gram for New Century Excellent Talents in Uni-
versity (NCET-06-0926) and the National Grand
Fundamental Research Program of China under
Grant(2005CB321802).
References
James Allan, Victor Lavrenko, Daniella Malin, and Rus-
sell Swan. 2000. Detections, bounds, and timelines:
Umass and tdt?3. In Proceedings of Topic Detection
and Tracking (TDT?3), pages 167?174.
J. Allan, A. Bolivar, M. Connell, S. Cronen-Townsend,
A Feng, F. Feng, G. Kumaran, L. Larkey, V. Lavrenko,
and H. Raghavan. 2003. Umass tdt 2003 research
summary. In proceedings of TDT workshop.
James Allan, editor. 2002. Topic Detection and Track-
ing: Event-based Information Organization. Kluwer
Academic Publishers, Norvell, Massachusetts.
Francine Chen, Ayman Farahat, and Thorsten Brants.
2004. Multiple similarity measures and source-pair
information in story link detection. In HLT-NAACL,
pages 313?320.
Margaret Connell, Ao Feng, Giridhar Kumaran, Hema
Raghavan, Chirag Shah, and James Allan. 2004.
Umass at tdt 2004. In TDT2004 Workshop.
Niek Hoogma. 2005. The modules and methods of topic
detection and tracking. In 2nd Twente Student Confer-
ence on IT.
Victor Lavrenko, James Allan, Edward DeGuzman,
Daniel LaFlamme, Veera Pollard, and Stephen
Thomas. 2002. Relevance models for topic detec-
tion and tracking. In Proceedings of Human Language
Technology Conference (HLT), pages 104?110.
LDC. 2003. Topic detection and tracking - phase 4.
Technical report, Linguistic Data Consortium.
Ramesh Nallapati and James Allan. 2002. Capturing
term dependencies using a language model based on
sentence trees. In Proceedings of the eleventh interna-
tional conference on Information and knowledge man-
agement, pages 383?390. ACM Press.
Ramesh Nallapati. 2003. Semantic language models for
topic detection and tracking. In HLT-NAACL.
NIST. 2003. The 2003 topic detection and tracking task
definition and evaluation plan. Technical report, Na-
tional Institute of Standards and Technology(NIST).
Yiming Yang, Jian Zhang, Jaime Carbonell, and Chun
Jin. 2002. Topic-conditioned novelty detection. In
Proceedings of the eighth ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 688?693. ACM Press.
47
