CRFs-Based Named Entity Recognition Incorporated with Heuristic 
Entity List Searching 
Fan Yang 
National Laboratory of 
Pattern Recognition 
Institute of Automation, 
Chinese Academy of 
Sciences  
fyang@nlpr.ia.ac.cn 
Jun Zhao 
National Laboratory of 
Pattern Recognition 
Institute of Automation, 
Chinese Academy of 
Sciences  
jzhao@nlpr.ia.ac.cn
Bo Zou 
National Laboratory of 
Pattern Recognition 
Institute of Automation, 
Chinese Academy of 
Sciences  
bzou@nlpr.ia.ac.cn
 
 
Abstract 
Chinese Named entity recognition is one of 
the most important tasks in NLP. Two 
kinds of Challenges we confront are how to 
improve the performance in one corpus and 
keep its performance in another different 
corpus. We use a combination of statistical 
models, i.e. a language model to recognize 
person names and two CRFs models to 
recognize Location names and 
Organization names respectively. We also 
incorporate an efficient heuristic named 
entity list searching process into the 
framework of statistical model in order to 
improve both the performance and the 
adaptability of the statistical NER system. 
We participate in the NER tests on open 
tracks of MSRA. The testing results show 
that our system can performs well. 
1 Introduction 
Named Entity Recognition (NER) is one of the 
most important tasks in NLP, and acts as a critical 
role in some language processing applications, 
such as Information Extraction and Integration, 
Text Classification etc. Many efforts have been 
paid to improve the performance of NER. 
NER task in Chinese has some differences from 
in English as follows. 1) There is no space between 
Chinese characters, which make boundary 
recognition more difficult. 2) In English, a 
capitalized letter at the beginning position of a 
word implies that the word is a part of a named 
entity. However, this kind of characteristic does 
not exist in Chinese. 
In the paper, we will focus on two kinds of 
problems. 1) How to improve the performance of 
Chinese NER in one corpus, which contains 
boosting precision rate, recall rate and F-measure 
rate. 2) How to enhance the  adaptability of a 
Chinese NER system, which means that a system 
can get a good performance on a testing set which 
has many differences from the training set. To 
solve the first problem, we should select a good 
model and adjust parameters carefully. But there is 
no framework that can solve the second problem 
completely.  
Our goal is to find a way to solve these two 
problems. We select a language model to recognize 
Person names, and two CRFs models are used to 
recognize Location and Organization separately.  
We also try to incorporate a large-scale named 
entity list into the statistical model, where a 
heuristic searching method is developed to match 
the entities in the list quickly and efficiently. 
2 Framework of NER System 
The Input of the system is a raw text. We will 
apply some pre-processing such as code 
transformation. Then the heuristic searching will 
be executed to find the appearance of the entities in 
the named entity list. After that, two CRFs that 
have been trained before will be used to recognize 
Location and Organization based on the result of 
word segmentation, and a language model will be 
used to find Person names. All the results will be 
integrated at last. 
171
Sixth SIGHAN Workshop on Chinese Language Processing
 
Figure 1.  System Frameworks 
3 System Details 
3.1 Using heuristic method to search entity 
list 
NER task meets many difficulties which come 
from the complexity of the construction of named 
entities. Named entities have flexible internal 
styles and external environments. Building a good 
model to describe the condition precisely will have 
many troubles. The statistical models we common-
ly used have some shortcomings, especially when 
they are adapted to a corpus of new domains or 
styles. We try to use an improved searching 
method to make up the relatively poor adaptability 
of the statistical models. The heuristic searching 
method is more flexible especially in the following 
two aspects. 1) Abbreviation can be matched. 2) 
Suffix in location and organization is universal, but 
it should not be taken in count when we search 
named entities. 
The framework of the algorithm can be briefly 
described as follows: 
 
1) Building an inverse index using Chinese characters 
as key term; 
2) Using the text as a query to search for entities; 
3) When comes terminal condition, a heuristic 
function is invoked to determine whether the 
character sequence is an entity; 
4) When comes creation condition, a heuristic 
function is invoked to judge whether a new entity is 
created; 
5) The labeled sequence is output. 
Table 1. Heuristic Searching Method 
One advantage of heuristic searching method is 
that the heuristic function can be set to fit a special 
corpus. The heuristic searching method we used in 
Bakeoff-4 is as follows:  
Un-segmenting test 
Person recognition 
Heuristic searching 
Location 
recognition 
Organization 
recognition 
Word segmentation 
Output results 
z Ignoring the suffix key word in Location and 
Organization names. For example 
??? ??/Tongfang /Corporation? and 
???/Tongfang? will get same score under 
this heuristic rule. 
z Ignoring the Location name as a prefix in an 
Organization name. For example, 
???/Ameri ??can /General Motors ? and 
???/General Motors? will get same score 
under this heuristic rule 
z Taking Abbreviation rules in consideration. 
For example??? ??/Peking /University? 
can be abbreviated as ?? ?/Bei /Da? rather 
than ???/Peking? or ???/University? 
Heuristic searching method also has such advan-
tages as follows: 
It is easy to be expanded to a corpus of new 
domain or style. We only need to add the entities 
in the new domain into list 
Searching method will improve the recall 
performance remarkably 
But the precision will be reduced for the ambi-
guities, i.e. whether a sequence that matches an 
entity in the list really constructs an entity in the 
text. We will disambiguate it using statistical 
models. 
3.2 Conditional Random Fields Model 
Conditional Random Fields (CRFs) is an 
undirected graphical model that encodes a 
conditional probability distribution using a given 
set of features. Currently it is widely used as a 
discriminate model for sequence labeling: 
1 2
1
1
( | ) exp( ( , , ))
k
i i c c
c C i
P Y X f y y X
Z
?
? =
= ??     (1) 
CRFs is considered to be a very effective model 
to resolve the issue of sequence labeling for the 
following characteristics: 
Because it uses a non-greedy whole sentence 
joint labeling method, high accuracy rate can be 
guaranteed and bias labeling can be avoided. 
Any types of features can be integrated in the 
model flexibly. 
Over-fitting can be avoided to some extent by 
integrating a priori with training data. 
172
Sixth SIGHAN Workshop on Chinese Language Processing
As a discriminate model, CRFs inherits the 
advantages of both Hidden Markov Model (HMM) 
and Maximum Entropy Markov Model (MEMM) 
as well. 
3.3 Person names recognize 
We use language model to recognize Personal 
names. We use character-based rather than word-
based model to avoid the word segmentation errors. 
We construct a context model and an entity model 
to respectively describe external and internal 
features of Personal names. The details of the 
model are as follows: 
We use a tri-gram model as the context model: 
( ) ( )? --?
m
1i
1i2ii wcwc|wcPWCP
=
               (2) 
Entity model: 
( )
( ) ( )( ) ( )( )
1 1
1 1 1
2
1 2 1
1
1
2
| |
| | , | ,
i ik i ik
i il iki l i k
k
wc wc i wc wc k k
k
wc wc l wc wc k wc
l
P w w wc P w w B M M E
P w B P w M w P w E w
? ?
?
?
?
=
? ?? ?= ??
? ? ??

" " " ?? (3) 
Where B means the beginning of the entity, M 
means middle, E means end. 
Some expert knowledge is employed to assist 
the recognition process of language model. 
z A Chinese family name list (476) and a 
Japanese family name list (9189) are used to 
restrict and select the generated candidates. 
z A list of commonly used character in Russian 
and European name. 
z Constrain of name length: A Chinese name 
cannot contain more than 8 characters. 
3.4 Location names recognition 
Location names have some composition characters. 
1) There may be some key words as suffix, such as: 
??/Shi, ?/Zheng, ?/Hu, ?/Shan? etc. 2) Other 
parts of Location names are always OOV words, 
such as ????/Dagang Village, ???/Fuzi 
Temple?.  So the right boundary of Location can 
be determined easily. The mainly problem in 
Location recognition is on abbreviation, such as 
?????/JinJiLuYu? is the combination of four 
location abbreviations. In our system, CRFs model 
can be supported by the heuristic searching method 
because it can match the abbreviation of entity in 
list. Using the searching method can boost the 
recall rate of location recognition significantly. We 
construct the recognition model based on the word-
segmented texts. 
To construct a CRFs model, we select the 
following features: 
z A list of key word suffix is used to trigger the 
recognition processing. 
z Using a list of indication words to restrict the 
boundary. 
z Heuristic searching method is used to assist 
Location recognition. 
The features we used in CRFs model is followed: 
 
W0 Current Word 
W-1?W-2 , W1?W2 Two words before and 
behind 
W-1W0?W0W1 Bi-gram Features 
POS0 POS tag 
PRE-1?PRE0?PRE1 Pre-Position reference 
words 
SRE-1?SRE0?SRE1 Suf-Position reference 
words 
Key Has Key suffix 
DIC In Dictionary 
Table 2. Features used in Location recognition 
Statement: 
The indication words used in Location 
recognition include ?for-indicate? and ?back-
indicate? words, where ?for-indicate? denotes the 
indicating words that occur as the left neighbor of 
the candidate Location named entity, while ?back-
indicate? denotes the indicating words that occur 
as the right neighbor. ?for-indicate? and ?back-
indicate? words are got from the training corpus. 
We calculate the mutual information between 
neighbor words and location entity, and get the top 
N words as indication words. 
( , )
( , ) ( , ) log
( ) ( )
p x y
MI x y p x y
p x p y
=     (4) 
We select 1216 for-indicate words and 1227 
back-indicate words. We also get 607 key words as 
location name suffix. 
3.5 Organization names recognition 
Organization name recognition is the most difficult 
part in NER task. The difficulties are as follows. 1) 
The composition of Organization name is very 
complex. For example: ??? ??/Dalian /Shide 
??/Group?, the first words in the entity is a 
location name. The second is a phonetic name 
173
Sixth SIGHAN Workshop on Chinese Language Processing
which is also an OOV word and the last one is a 
key word as suffix.2) The boundary of organizat-
ion name is hard to be classified, and the length of 
organization names is dynamitic. 3) Organization 
names are easily confused as Location names. We 
must use contextual information to determine its 
type. 4) To recognize the abbreviation of an 
organization is also a difficult task. So we choose 
the following features to solve the above problems. 
z A list of key word suffix is used to trigger the 
recognition processing. 
z Using indication words to define the boundary 
of organization. 
z Heuristic searching method is used to assist 
Location recognition. 
The features we used in the CRFs model are the 
same as used in Location model. We use the 
mutual information to select 513 for-indicate 
words and 1195 back-indicate words from training 
corpus. The number of key suffix words is 3129. 
4 Experiments 
We participate in the SigHAN Microsoft Research 
Asia (MSRA) corpus in open track. The table 3 is 
the official result of NER by our system. 
 
 R P F 
Person 0.9657 0.9574 0.9615 
Location 0.9593 0.9769 0.968 
Organization 0.8778 0.9338 0.9049 
overall 0.9377 0.9603 0.9489 
Table 3.  SigHAN MSRA corpus test results 
The training corpora we used comes from 1) 
1998 People?s Daily corpus; 2) the training corpus 
supplied by MSRA for SigHAN bakeoff 4. These 
two corpora have many difference and we focus on 
how to get a good performance both on training 
corpus and testing corpus. We select some general 
features and get assistance from the heuristic 
searching method. A good list is very important, 
which has been proved by the experimental data. 
We collect nearly 1 million personal names, 40 
thousand location names and more than 300,000 
organization names. 
5 Conclusion 
In the paper, we give a presentation to our Chinese 
Named Entity Recognition System. It uses a 
language mode to recognize personal names, and 
two CRFs models to find Location and 
organization separately. We also have a flexible 
heuristic searching method to match entity in 
named entity list with text characters sequence. 
Our system achieves a good result in the open 
NER track of MSRA corpus.  
Acknowledgement 
The work is supported by the National High 
Technology Development 863 Program of China 
under Grants no. 2006AA01Z144, the National 
Natural Science Foundation of China under Grants 
No. 60673042, the Natural Science Foundation of 
Beijing under Grants no. 4052027, 4073043. 
References 
J. Lafferty, A. McCallum, and F. Pereira. 2001. 
Conditional random fields: Probabilistic models for 
segmenting and labeling sequence data, In Proc. 
ICML 2001.  
F?Sha, F. Pereira. Shallow parsing with conditional 
random  fields. In Proc. NAACL 2003 
HP Zhang, HK Yu, DY Xiong, Q Liu and Liu Qun. 
HHMM-based Chinese Lexical Analyzer ICTCLAS. 
In Proc. Second of SIGHAN Workshop on Chinese 
Language Processing 2003. 
Youzheng Wu, Jun Zhao, Bo Xu. Chinese Named Entity 
Recognition Model Based on Multiple Features. In 
Proceedings of HLT/EMNLP 2005 
Youzheng Wu, Jun Zhao, Bo Xu. Chinese Named Entity 
Recognition Combining a Statistical Model with 
Human Knowledge. In Proceedings of ACL2003 
Workshop on Multilingual and Mixed-language 
Named Entity Recognition 
 
174
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of ACL-08: HLT, pages 541?549,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
 
Chinese-English Backward Transliteration Assisted with Mining Mono-lingual Web Pages    Fan Yang, Jun Zhao, Bo Zou, Kang Liu, Feifan Liu  National Laboratory of Pattern Recognition   Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China {fyang,jzhao,bzou,kliu,ffliu}@nlpr.ia.ac.cn     
Abstract 
In this paper, we present a novel backward transliteration approach which can further as-sist the existing statistical model by mining monolingual web resources. Firstly, we em-ploy the syllable-based search to revise the transliteration candidates from the statistical model. By mapping all of them into existing words, we can filter or correct some pseudo candidates and improve the overall recall. Secondly, an AdaBoost model is used to re-rank the revised candidates based on the in-formation extracted from monolingual web pages. To get a better precision during the re-ranking process, a variety of web-based in-formation is exploited to adjust the ranking score, so that some candidates which are less possible to be transliteration names will be as-signed with lower ranks. The experimental re-sults show that the proposed framework can significantly outperform the baseline translit-eration system in both precision and recall. 1 Introduction* The task of Name Entity (NE) translation is to translate a name entity from source language to target language, which plays an important role in machine translation and cross-language informa-tion retrieval (CLIR). Transliteration is a subtask in NE translation, which translates NEs based on the phonetic similarity. In NE translation, most person names are transliterated, and some parts of location names or organization names also need to be trans-literated. Transliteration has two directions: for-ward transliteration which transforms an original name into target language, and backward translit-eration which recovers a name back to its original expression. For instance, the original English per-                                                           *Contact: Jun ZHAO, jzhao@nlpr.ia.ac.cn.  
son name ?Clinton? can be forward transliterated to its Chinese expression ??/ke ?/lin?/dun? and the backward transliteration is the inverse process-ing. In this paper, we focus on backward translit-eration from Chinese to English. Many previous researches have tried to build a transliteration model using statistical approach [Knight and Graehl, 1998; Lin and Chen, 2002; Virga and Khudanpur, 2003; Gao, 2004]. There are two main challenges in statistical backward trans-literation: First, statistical transliteration approach selects the most probable translations based on the knowledge learned from the training data. This approach, however, does not work well when there are multiple standards [Gao, 2004]. Second, back-ward transliteration is more challenging than for-ward transliteration as it is required to disambiguate the noises introduced in the forward transliteration and estimate the original name as close as possible [Lin and Chen, 2002]. One of the most important causes in introducing noises is that: some silent syllables in original names have been missing when they are transliterated to target lan-guage. For example, when ?Campbell? is translit-erated into ??/kan?/bei?/er?, the ?p? is missing.  In order to make up the disadvantages of statisti-cal approach, some researchers have been seeking for the assistance of web resource. [Wang et al, 2004; Cheng et al, 2004; Nagata et al, 2001; Zhang et al 2005] used bilingual web pages to ex-tract translation pairs. Other efforts have been made to combine a statistical transliteration model with web mining [Al-Onaizan and Knight, 2002; Long Jiang et al 2007]. Most of these methods need bilingual resources. However, those kinds of resources are not readily available in many cases. Moreover, to search for bilingual pages, we have to depend on the performance of search engines. We can?t get Chinese-English bilingual pages when the input is a Chinese query. Therefore, the existing 
541
 
assistance approaches using web-mining to assist transliteration are not suitable for Chinese to Eng-lish backward transliteration. Thus in this paper, we mainly focus on the fol-lowing two problems to be solved in transliteration. Problem I: Some silent syllables are missing in English-Chinese forward transliteration. How to recover them effectively and efficiently in back-ward transliteration is still an open problem. Problem II: Statistical transliteration always chooses the translations based on probabilities. However, in some cases, the correct translation may have lower probability. Therefore, more stud-ies are needed on combination with other tech-niques as supplements. Aiming at these two problems, we propose a method which mines monolingual web resources to assist backward transliteration. The main ideas are as follows. We assume that for every Chinese en-tity name which needs to be backward transliter-ated to an English original name, the correct transliteration exists somewhere in the web. What we need to do is to find out the answers based on the clues given by statistical transliteration results. Different from the traditional methods which ex-tract transliteration pairs from bilingual pages, we only use monolingual web resources. Our method has two advantages. Firstly, there are much more monolingual web resources available to be used. Secondly, our method can revise the transliteration candidates to the existing words before the subse-quent re-ranking process, so that we can better mine the correct transliteration from the Web. Concretely, there are two phases involved in our approach. In the first phase, we split the result of transliteration into syllables, and then a syllable-based searching processing can be employed to revise the result in a word list generated from web pages, with an expectation of higher recall of trans-literation. In the second phase, we use a revised word as a search query to get its contexts and hit information, which are integrated into the AdaBoost classifier to determine whether the word is a transliteration name or not with a confidence score. This phase can readjust the candidate?s score to a more reasonable point so that precision of transliteration can be improved. Table 1 illustrates how to transliterate the Chinese name ??/a?/jia?/xi? back to ?Agassi?.  Chinese name Transliteration results Revised Candidate Re-rank Results 
??? a  jia xi Agassi 
aggasi agahi agacy agasie ? 
agasi agathi agathe agassi ? 
agassi agasi agache agga ? Table 1. An example of transliteration flow The experimental results show that our approach improves the recall from 41.73% to 59.28% in open test when returning the top-100 results, and the top-5 precision is improved from 19.69% to 52.19%. The remainder of the paper is structured as fol-lows. Section 2 presents the framework of our sys-tem. We discuss the details of our statistical transliteration model in Section 3. In Section 4, we introduce the approach of revising and re-ranking the results of transliteration. The experiments are reported in Section 5. The last section gives the conclusion and the prediction of future work. 2 System Framework  Our system has three main modules. 
  Figure 1. System framework 1) Statistical transliteration: This module re-ceives a Chinese Pinyin sequence as its input, and output the N-best results as the transliteration can-didates.  2) Candidate transliteration revision through syllable-based searching: In the module, a transliteration candidate is transformed into a syllable query. We use a syllable-based searching strategy to select the revised candidate from a huge word list. Each word in the list is indexed by sylla-bles, and the similarity between the word and the query is calculated. The most similar words are returned as the revision results. This module guar-
Monolingual web pages 
Words list 
Chinese name 
Statistical model 
Transliteration candidates Syllable-based search 
Revised candidates Re-ranking phase 
Final results Search engine 
542
 
antees the transliteration candidates are all existing words. 3) Revised candidate re-ranking in web pages: In the module, we search the revised candi-dates to get their contexts and hit information which we can use to score the probability of being a transliteration name. This phase doesn?t generate new candidates, but re-rank the revised candidate set to improve the performance in top-5. Under this framework, we can solve the two problems of statistical model mentioned above.  (1) The silent syllables will be given lower weights in syllable-based search, so the missing syllables will be recovered through selecting the most similar existing words which can contain some silent syllables.  (2) The query expansion technology can recall more potential transliteration candidates by ex-panding syllables to their ?synonymies?. So the mistakes introduced when selecting syllables in statistical transliteration will be corrected through giving suitable weights to synonymies.  Through the revision phase, the results of statis-tical model which may have illegal spelling will be mapped to its most similar existing words. That can improve the recall. In re-ranking phase, the revised candidate set will be re-ranked to put the right answer on the top using hybrid information got from web resources. So the precision of trans-literation will be improved. 3 Statistical Transliteration Model We use syllables as translation units to build a sta-tistical Chinese-English backward transliteration model in our system. 3.1 Traditional Statistical Translation Model [P. Brown et al, 1993] proposed an IBM source-channel model for statistical machine translation (SMT). When the channel output f= f1,f2 ?. fn ob-served, we use formula (1) to seek for the original sentence e=e1,e2 ?. en with the most likely poste-riori. 
' argmax ( | ) argmax ( | ) ( )
e e
e P e f P f e P e= =
      (1) The translation model ( | )P f e  is estimated from a paired corpus of foreign-language sentences and their English translations. The language model ( )P e  is trained from English texts. 
3.2 Our Transliteration Model The alignment method is the base of statistical transliteration model. There are mainly two kinds of alignment methods: phoneme-based alignment [Knight and Graehl, 1998; Virga and Khudanpur, 2003] and grapheme-based alignment [Long Jiang, 2007]. In our system, we adopt the syllable-based alignment from Chinese pinyin to English syllables, where the syllabication rules mentioned in [Long Jiang et al, 2007] are used. For example, Chinese name ??/xi ?/er ?/dun? and its backward transliteration ?Hilton? can be aligned as follows. ?Hilton? is split into syllable sequence as ?hi/l/ton?, and the alignment pairs are ?xi-hi?, ?er-l?, ?dun-ton?.  Based on the above alignment method, we can get our statistical Chinese-English backward trans-literation model as, 
argmax ( | ) ( )
E
E p PY ES p ES=
            (2) Where, PY is a Chinese Pinyin sequence, ES is a English syllables sequence, ( | )p PY ES  is the probability of translating ES into PY, ( )p E S  is the generative probability of a English syllable lan-guage model. 3.3 The Difference between Backward Trans-literation and Traditional Translation Chinese-English backward transliteration has some differences from traditional translation. 1) We don?t need to adjust the order of sylla-bles when transliteration.  2) The language model in backward translitera-tion describes the relationship of syllables in words. It can?t work as well as the language model de-scribing the word relationship in sentences. We think that the crucial problem in backward transliteration is selecting the right syllables at every step. It?s very hard to obtain the exact an-swer only based on the statistical transliteration model. We will try to improve the statistical model performance with the assistance of mining web resources. 4 Mining Monolingual Web Pages to As-sist Backward Transliteration  In order to get assistance from monolingual Web resource to improve statistical transliteration, our 
543
 
method contains two main phases: ?revision? and ?re-ranking?. In the revision phase, transliteration candidates are revised using syllable-based search in the word list, which are generated by collecting the existing words in web pages. Because the proc-ess of named entity recognition may lose some NEs, we will reserve all the words in web corpus without any filtering. The revision process can im-prove the recall through correcting some mistakes in the transliteration results of statistical model. In the re-ranking phase, we search every revised candidate on English pages, score them according to their contexts and hit information so that the right answer will be given a higher rank.  4.1 Using Syllable-based Retrieval to Revise Transliteration Candidates In this section, we will propose two methods re-spectively for the two problems of statistical model mentioned in section 1.  4.1.1  Syllable-based retrieval model When we search a transliteration candidate tci in the word list, we firstly split it into syllables {es1,es2,?..esn}. Then this syllable sequence is used as a query for syllable-based searching.  We define some notions here. ? Term set T={t1,t2?.tk} is an orderly set of all syllables which can be viewed as terms.  ? Pinyin set P={py1,py2?.pyk} is an orderly set of all Pinyin.  ? An input word can be represented by a vec-tor of syllables {es1,es2,?..esn}.  We calculate the similarity between a translitera-tion result and each word in the list to select the most similar words as the revised candidates. The {es1,es2,?..,esn} will be transformed into a vector Vquery={t1,t2?.tk} where ti represents the ith term in T. The value of ti is equal to 0 if the ith term doesn?t appear in query. In the same way, the word in list can also be transformed into vector represen-tation. So the similarity can be calculated as the inner product between these two vectors.  We don?t use tf and idf conceptions as traditional information retrieval (IR) to calculate the terms? weight. We use the weight of ti to express the ex-pectation probability of ith term having pronuncia-tion. If the term has a lower probability of having pronunciation, its weight is low. So when we searching, the missing silent syllables in the results 
of statistical transliteration model can be recovered because such syllables have little impact on simi-larity measurement. The formula we used is as fol-lows. 
( , )
/
query word
word py
V V
Sim query word
L L
?
=
            (3) 
The numerator is the inner product of two vec-tors. The denominator is the length of word Lword divided by the length of Chinese pinyin sequence Lpy. In this formula, the more syllables in one word, the higher score of inner production it may get, but the word will get a loss for its longer length. The word which has the shortest length and the highest syllable hitting ratio will be the best. Another difference from traditional IR is how to deal with the order of the words in a query. Ac-cording to transliteration, the similarity must be calculated under the limitation of keeping order, which can?t be satisfied by current methods. We use the algorithm like calculating the edit distance between two words. The syllables are viewed as the units which construct a word. The edit distance calculation finds the best matching with the least operation cost to change one word to another word by using deletion/addition/insertion operations on syllables. But the complexity will be too high to afford if we calculate the edit distance between a query and each word in the list. So, we just calcu-late the edit distance for the words which get high score without the order limitation. This trade off method can save much time but still keep perform-ance. 4.1.2  Mining the Equivalent through Syllable Expansion In most collections, the same concept may be re-ferred to using different words. This issue, known as synonymy, has an impact on the recall of most information retrieval systems. In this section, we try to use the expansion technology to solve prob-lem II. There are three kinds of expansions to be explained below.  Syllable expansion based on phonetic similar-ity: The syllables which correspond to the same Chinese pinyin can be viewed as synonymies. For example, the English syllables ?din? and ?tin? can be aligned to the same Chinese pinyin ?ding?. Given a Chinese pinyin sequence {py1,py2,?..pyn} as the input of transliteration model, for every pyi, there are a set of syllables 
544
 
{es1, es2 ?.. esk} which can be selected as its translation. The statistical model will select the most probable one, while others containing the right answer are discarded. To solve this problem, we expand the query to take the synonymies of terms into consideration. We create an expansion set for each Chinese pinyin. A syllable esi will be selected into the expansion set of pyj based on the alignment probability P(esi|pyj) which can be ex-tracted from the training corpus. The phonetic similarity expansion is based on the input Chinese Pinyin sequence, so it?s same for all candidates. Syllable expansion based on syllable similar-ity: If two syllables have similar alignment prob-ability with every pinyin, we can view these two syllables as synonymy. Therefore, if a syllable is in the query, its synonymies should be contained too. For example, ?fea? and ?fe? can replace each other. To calculate the similarity, we first obtain the alignment probability P(pyj|esk) of every syllable. Then the distance between any two syllables will be calculated using formula (4). 
1
1
( , ) ( | ) ( | )
N
j k i j i k
i
Sim es es P py es P py es
N
=
=
?
(4) This formula is used to evaluate the similarity of two syllables in alignment. The expansion set of the ith syllable can be generated by selecting the most similar N syllables. This kind of expansion is conducted upon the output of statistical translitera-tion model. Syllable expansion based on syllable edit dis-tance: The disadvantage of last two expansions is that they are entirely dependent on the training set. In other word, if some syllables haven?t appeared in the training corpus, they will not be expanded. To solve the problem, we use the method of expan-sion based on edit distance. We use edit distance to measure the similarity between two syllables, one is in training set and the other is absent. Because the edit distance expansion is not very relevant to pronunciation, we will give this expansion method a low weight in combination. It works when new syllables arise.  Combine the above three strategies: We will combine the three kinds of expansion method to-gether. We use the linear interpolation to integrate them. The formulas are follows.   
(1 )
pre sy ed
S S S S? ? ?= ? + +                (5) 
(1 )
pre py ed
S S S S? ? ?= ? + +                (6) 
where Spre is the score of exact matching, Ssy is the score of expansion based on syllables similarity and Spy based on phonetic similarity. We will ad-just these parameters to get the best performance. The experimental results and analysis will be re-ported in section 5.3. 4.2 Re-Ranking the Revised Candidates Set using the Monolingual Web Resource In the first phase, we have generated the revised candidate set {rc1,rc2,?,rcn} from the word list us-ing the transliteration results as clues. The objec-tive is to improve the overall recall. In the second phase, we try to improve the precision, i.e. we wish to re-rank the candidate set so that the correct an-swer will be put in a higher rank. [Al-Onaizan et al, 2002] has proposed some methods to re-score the transliteration candidates. The limitation of their approach is that some can-didates are propbale not existing words, with which we will not get any information from web. So it can only re-rank the transliteration results to improve the precision of top-5. In our work, we can improve the recall of transliteration through the revising process before re-ranking. In this section, we employ the AdaBoost frame-work which integrates several kinds of features to re-rank the revised candidate set. The function of the AdaBoost classifier is to calculate the probabil-ity of the candidate being a NE. Then we can re-rank the revised candidate set based on the score. The features used in our system are as follows. NE or not: Using rci as query to search for monolingual English Web Pages, we can get the context set {Ti1, Ti2??Tin} of rci. Then for every Tik, we use the named entity recognition (NER) software to determine whether rci is a NE or not. If rci is recognized as a NE in some Tik, rci will get a score. If rci can?t be recognized as NE in any con-texts, it will be pruned. The hit of the revised candidate: We can get the hit information of rci from search engine. It is used to evaluate the importance of rci. Unlike [Al-Onaizan et al, 2002], in which the hit can be used to eliminate the translation results which contain illegal spelling, we just use hit number as a feature. The limitation of compound NEs: When trans-literating a compound NE, we always split them into several parts, and then combine their translit-eration results together. But in this circumstance, 
545
 
every part can add a limitation in the selection of the whole NE. For example: ??/xi?/la?/li ?  ?/ke?/lin?/dun? is a compound name. ??/xi?/la?/li? can be transliterate to ?Hilary? or ?Hilaly? and ??/ke?/lin?/dun? can be transliterate to ?Clinton? or ?Klinton?. But the combination of ?Hilary?Clinton? will be selected for it is the most common combination. So the hit of combination query will be extracted as a feature in classifier. Hint words around the NE: We can take some hint words around the NE into the query, in order to add some limitations to filter out noisy words. For example: ??? (president)? can be used as hint word for ???? (Clinton)?. To find the hint words, we first search the Chinese name in Chi-nese web pages. The frequent words can be ex-tracted as hint words and they will be translated to English using a bilingual dictionary. These hint words are combined with the revised candidates to search English web pages. So, the hit of the query will be extracted as feature. The formula of AdaBoost is as follow. 
1
( ) ( ( ))
T
t t
t
H x sign h x?
=
=
?
                 (7) 
Where 
t
?  is the weight for the ith weak classifier 
( )
t
h x . 
t
?  can be calculated based on the precision of its corresponding classifier. 5 Experiments We carry out experiments to investigate how much the revision process and the re-ranking process can improve the performance compared with the base-line of statistical transliteration model. We will also evaluate to which extents we can solve the two problems mentioned in section 1 with the as-sistance of Web resources. 5.1 Experimental data The training corpus for statistical transliteration model comes from the corpus of Chinese <-> Eng-lish Name Entity Lists v 1.0 (LDC2005T34). It contains 565,935 transliteration pairs. Ruling out those pairs which are not suitable for the research on Chinese-English backward transliteration, such as Chinese-Japanese, we select a training set which contains 14,443 pairs of Chinese-European & American person names. In the training set, 1,344 
pairs are selected randomly as the close test data. 1,294 pairs out of training set are selected as the open test data. To set up the word list, a 2GB-sized collection of web pages is used. Since 7.42% of the names in the test data don?t appear in the list, we use Google to get the web page containing the ab-sent names and add these pages into the collection. The word list contains 672,533 words. 5.2 Revision phase vs. statistical approach Using the results generated from statistical model as baseline, we evaluate the revision module in recall first. The statistical transliteration model works in the following 4 steps: 1) Chinese name are transformed into pinyin representation and the English names are split into syllables. 2) The GIZA++1 tool is invoked to align pinyin to sylla-bles, and the alignment probabilities ( | )P py es are obtained. 3) Those frequent sequences of syllables are combined as phrases. For example, ?be/r/g???berg?, ?s/ky???sky?. 4) Camel 2  de-coder is executed to generate 100-best candidates for every name. We compare the statistical transliteration results with the revised results in Table 2. From Table 2 we can find that the recall of top-100 after revision is improved by 13.26% in close test set and 17.55% in open test set. It proves that the revision module is effective for correcting the mistakes made in statistical transliteration model. Transliteration results Revised results  close open close open Top1 33.64% 9.41% 27.15% 11.04% Top5 40.37% 13.38% 42.83% 19.69% Top10 47.79% 17.56% 56.98% 26.52% Top20 61.88% 25.44% 71.05% 37.81% Top50 66.49% 36.19% 82.16% 46.22% Top100 72.52% 41.73% 85.78% 59.28% Table 2. Statistical model vs. Revision module To show the effects of the revision on the two above-mentioned problems in which the statistical model does not solve well: the losing of silent syl-lables and the selection bias problem, we make a statistics of the improvements with a measurement of ?correction time?. For a Chinese word whose correct transliteration appears in top-100 candidates only if it has been 
                                                           1 http://www.fjoch.com/GIZA++.html 2 http://www.nlp.org.cn 
546
 
revised, we count the ?correction time?. For exam-ple, when ?Argahi? is revised to ?Agassi? the cor-rection time is ?1? for Problem II and ?1? for Problem I, because in ?hi?? ?si? the syllable is expanded, and in ?si? ??ssi? an ?s? is added.   Close test Open test Problem I 0.6931 0.7853 Problem II 0.9264 1.1672 Table 3. Average time of correction This measurement reflects the efficiency of the revision of search strategy, in contrast to those spelling correction techniques in which several operations of ?add? and ?expand? are inevitable. It has proved that the more an average correction time is, the more efficient our strategy is.  
?
???
???
???
???
???
???
???
???
???
?
? ? ? ? ? ? ? ? ?
??????? ?????????  Figure 2. Length influence in recall comparison The recall of the statistical model relies on the length of English name in some degree. It is more difficult to obtain an absolutely correct answer for longer names, because they may contain more si-lent and confused syllables. However, through the revision phase, this tendency can be effectively alleviated. In Figure 2, we make a comparison be-tween the results of the statistical model and the revision module with the changing of syllable?s length in open test. The curves demonstrate that the revision indeed prevents the decrease of recall for longer names. 5.3 Parameter setting in the revision phase We will show the experimental results when set-ting different parameters for query expansion. In the expansion based on phonetic similarity, for every Chinese pinyin, we select at most 20 sylla-bles to create an expansion set. We set 0.1? =  in formula (5). The results are shown in the columns labeled ?exp1? in Table 4. From the results we can conclude that, we get the best performance when 
0.4? = . That means the performance is best when the weight of exact 
matching is a little larger than the weight of fuzzy matching. We can also see that, higher weight of exact matching will lead to low recall, while higher weight of fuzzy matching will bring noise in. The expansion method based on syllable similar-ity is also evaluated. For every syllable, we select at most 15 syllables to create the expansion set. We set 0.1? = . The results are shown in the columns labeled ?exp2? in Table 4. From the results we can conclude that, we get the best performance when 0.5? = . It means that we can?t put emphasis on any matching methods. Comparison with the expansion based on phonetic similarity, the performance is poorer. It means that the expansion based on phonetic similarity is more suitable for revising transliteration candidates. 5.4 Revision phase vs. re-ranking phase After the phase of revising transliteration candi-dates, we re-rank the revised candidate set with the assistance of monolingual web resources. In this section, we will show the improvement in preci-sion after re-ranking. We have selected four kinds of features to inte-grate in the AdaBoost framework. To determine whether the candidate is NE or not in its context, we use the software tool Lingpipe3. The queries are sent to google, so that we can get the hit of queries and the top-10 snippets will be extracted as context. The comparison of revision results and re-ranking results is shown as follows. Revised results Re-ranked results  close open close open Top1 27.15% 11.04% 58.08% 38.63% Top5 42.83% 19.69% 76.35% 52.19% Top10 56.98% 26.52% 83..92% 54.33% Top20 71.05% 37.81% 83.92% 57.61% Top50 82.16% 46.22% 83.92% 57.61% Top100 85.78% 59.28% 85.78% 59.28% Table 5. Revision results vs. Re-ranking results From these results we can conclude that, after re-ranking phase, the noisy words will get a lower 
                                                           3 http://www.alias-i.com/lingpipe/ 
547
 
0.2? =  0.3? =   0.4? =  0.5? =  0.6? =  0.7? =  0.8? =   exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 Top1 13.46 13.32 13.79 13.61 11.04 12.70 11.65 10.93 10.83 11.25 9.62 10.63 8.73 10.18 Top5 21.58 19.59 23.27 20.17 19.69 18.28 21.07 17.25 22.05 16.84 17.90 16.26 17.38 15.34 Top10 27.39 22.71 28.41 24.73 26.52 22.93 26.83 21.81 27.26 20.39 24.38 21.20 25.42 18.20 Top20 35.23 34.88 35.94 29.49 37.81 31.57 38.59 33.04 36.52 31.72 35.25 29.75 34.65 27.62 Top50 43.91 40.63 43.75 40.85 46.22 41.46 48.72 42.79 45.48 40.49 41.57 39.94 42.81 38.07 Top100 53.76 48.47 54.38 52.04 59.28 53.15 57.36 53.46 55.19 51.83 55.63 49.52 53.41 47.15 Table 4.  Parameters Experiment rank. Through the revision module, we get both higher recall and higher precision than statistical transliteration model when at most 5 results are returned. We also use the average rank and average recip-rocal rank (ARR) [Voorhees and Tice, 2000] to evaluate the improvement. ARR is calculated as       
1
1 1
( )
M
i
ARR
M R i
=
=
?
                             (8) where ( )R i  is the rank of the answer of ith test word. M is the size of test set. The higher of ARR, the better the performance is. The results are shown as Table 6. Statistical  model Revision  module Re-rank  Module  close open close open close open Average rank 37.63 70.94 24.52 58.09 16.71 43.87 ARR 0.3815 0.1206 0.3783 0.1648 0.6519 0.4492 Table 6. ARR and AR evaluation The ARR after revision phase is lower than the statistical model. Because the goal of revision module is to improve the recall as possible as we can, some noisy words will be introduced in. The noisy words will be pruned in re-ranking module. That is why we get the highest ARR value at last. So we can conclude that the revision module im-proves recall and re-ranking module improves pre-cision, which help us get a better performance than pure statistical transliteration model 6 Conclusion In this paper, we present a new approach which can revise the results generated from statistical transliteration model with the assistance of mono-lingual web resource. Through the revision process, the recall of transliteration results has been im-proved from 72.52% to 85.78% in the close test set and from 41.73% to 59.28% in open test set, re-spectively. We improve the precision in re-ranking phase, the top-5 precision can be improved to 76.35% in close test and 52.19% in open test. The 
promising results show that our approach works pretty well in the task of backward transliteration. In the future, we will try to improve the similar-ity measurement in the revision phase. And we also wish to develop a new approach using the transliteration candidates to search for their right answer more directly and effectively. Acknowledgments The work is supported by the National High Tech-nology Development 863 Program of China under Grants no. 2006AA01Z144, the National Natural Science Foundation of China under Grants No. 60673042, the Natural Science Foundation of Bei-jing under Grants no. 4073043. References  Yaser Al-Onaizan and Kevin Knight. 2002. Translating named entities using monolingual and bilingual re-sources. In Proc.of ACL-02.  Kevin Knight and Jonathan Graehl. 1998. Machine Transliteration. Computational Linguistics 24(4). Wei-Hao Lin and Hsin-His Chen. 2002 Backward Ma-chine Transliteration by Learning Phonetic Similarity. In Proc. Of the 6th CoNLL Donghui Feng, Yajuan Lv, and Ming Zhou. 2004. A New Approach for English-Chinese Named Entity Alignment. In Proc. of EMNLP-2004. Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng Niu, 2007. Named Entity Translation with Web Min-ing and Transliteration. In Proc. of IJCAI-2007. Wei Gao. 2004. Phoneme-based Statistical Translitera-tion of Foreign Name for OOV Problem. A thesis of Master. The Chinese University of Hong Kong. Ying Zhang, Fei Huang, Stephan Vogel. 2005. Mining translations of OOV terms from the web through cross-lingual query expansion. SIGIR 2005. Pu-Jen Cheng, Wen-Hsiang Lu, Jer-Wen Teng, and Lee-Feng Chien. 2004 Creating Multilingual Transla-tion Lexicons with Regional Variations Using Web Corpora. In Proc. of ACL-04 Masaaki Nagata, Teruka Saito, and Kenji Suzuki. 2001. Using the Web as a Bilingual Dictionary. In Proc. of ACL 2001 Workshop on Data-driven Methods in Machine Translation. 
548
 
Paola Virga and Sanjeev Khudanpur. 2003. Translitera-tion of proper names in cross-lingual information re-trieval. In Proc. of the ACL workshop on Multi-lingual Named Entity Recognition. Jenq-Haur Wang, Jei-Wen Teng, Pu-Jen Cheng, Wen-Hsiang Lu, Lee-Feng Chien. 2004. Translating un-known cross-lingual queries in digital libraries using a web-based approach. In Proc. of JCDL 2004. E.M.Voorhees and D.M.Tice. 2000. The trec-8 question answering track report. In Eighth Text Retrieval Con-ference (TREC-8) 
549
