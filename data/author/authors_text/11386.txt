Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 132?135,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Estimating probability of correctness for ASR N-Best lists
Jason D. Williams and Suhrid Balakrishnan
AT&T Labs - Research, Shannon Laboratory, 180 Park Ave., Florham Park, NJ 07932, USA
{jdw,suhrid}@research.att.com
Abstract
For a spoken dialog system to make good
use of a speech recognition N-Best list, it is
essential to know how much trust to place
in each entry. This paper presents a method
for assigning a probability of correctness to
each of the items on the N-Best list, and to
the hypothesis that the correct answer is not
on the list. We find that both multinomial lo-
gistic regression and support vector machine
models yields meaningful, useful probabili-
ties across different tasks and operating con-
ditions.
1 Introduction
For spoken dialog systems, speech recognition er-
rors are common, and so identifying and reducing
dialog understanding errors is an important problem.
One source of potentially useful information is the
N-Best list output by the automatic speech recog-
nition (ASR) engine. The N-Best list contains N
ranked hypotheses for the user?s speech, where the
top entry is the engine?s best hypothesis. When the
top entry is incorrect, the correct entry is often con-
tained lower down in the N-Best list. For a dialog
system to make use of the N-Best list, it is useful to
estimate the probability of correctness for each en-
try, and the probability that the correct entry is not
on the list. This paper describes a way of assigning
these probabilities.
2 Background and related work
To begin, we formalize the problem. The user takes
a communicative action u, saying a phrase such as
?Coffee shops in Madison New Jersey?. Using a lan-
guage model g, the speech recognition engine pro-
cesses this audio and outputs an ordered list of N
hypotheses for u, u? = {u?1, . . . u?N}, N ? 2. To
the N-Best list we add the entry u??, where u = u??
indicates that u does not appear on the N-Best list.
The ASR engine also generates a set of K recog-
nition features f = [f1, . . . , fK ]. These features
might include properties of the lattice, word confu-
sion network, garbage model, etc. The aim of this
paper is to estimate a model which accurately as-
signs the N + 1 probabilities P (u = u?n|u?, f) for
n ? {?, 1, . . . , N} given u? and f . The model also
depends on the language model g, but we don?t in-
clude this conditioning in our notation for clarity.
In estimating these probabilities, we are most
concerned with the estimates being well-calibrated.
This means that the probability estimates we ob-
tain for events should accurately represent the em-
pirically observed proportions of those events. For
example, if 100 1-best recognitions are assigned a
probability of 60%, then approximately 60 of those
100 should in fact be the correct result.
Recent work proposed a generative model of the
N-Best list, P (u?, f |u) (Williams, 2008). The main
motivation for computing a generative model is
that it is a component of the update equation used
by several statistical approaches to spoken dialog
(Williams and Young, 2007). However, the diffi-
culty with a generative model is that it must estimate
a joint probability over all the features, f ; thus, mak-
ing use of many features becomes problematic. As
a result, discriminative approaches often yield bet-
ter results. In our work, we propose a discrimina-
tive approach and focus on estimating the probabil-
ities conditioned on the features. Additionally, un-
der some further fairly mild assumptions, by apply-
ing Bayes Rule our model can be shown equivalent
to the generative model required in the dialog state
update. This is a desirable property because dialog
systems using this re-statement have been shown to
work in practice (Young et al, 2009).
Much past work has assigned meaningful proba-
132
bilities to the top ASR hypothesis; the novelty here
is assigning probabilities to all the entries on the list.
Also, our task is different to N-Best list re-ranking,
which seeks to move more promising entries toward
the top of the list. Here we trust the ordering pro-
vided by the ASR engine, and only seek to assign
meaningful probabilities to the elements.
3 Model
Our task is to estimate P (u = u?n|u?, f) for n ?
{?, 1, . . . , N}. Ideally we could view each element
on the N-Best list as its own class and train an
(N+1)-class regression model. However this is dif-
ficult for two reasons. First, the number of classes is
variable: ASR results can have different N-Best list
lengths for different utterances. Second, we found
that the distribution of items on the N-Best list has
a very long tail, so it would be difficult to obtain
enough data to accurately estimate late position class
probabilities.
As a result, we model the probability P in two
stages: first, we train a (discriminative) model Pa to
assign probabilities to just three classes: u = u??,
u = u?1, and u ? u?2+, where u?2+ = {u?2, . . . , u?N}.
In the second stage, we use a separate probability
model Pb to distribute mass over the items in u?2+:
P (u?n = u|u?, f) = (1)
?
?
?
?
?
Pa(u = u?1|f) if n = 1,
Pa(u ? u?2+|f)Pb(u = u?n|f) if n > 1,
Pa(u = u??|f) if n = ?
To model Pa, multinomial logistic regression
(MLR) is a natural choice as it yields a well-
calibrated estimator for multi-class problems. Stan-
dard MLR can over-fit when there are many features
in comparison to the number of training examples;
to address this we use ridge regularized MLR in our
experiments below (Genkin et al, 2005).
An alternative to MLR is support vector machines
(SVMs). SVMs are typically formulated including
regularization; however, their output scores are gen-
erally not interpretable as probabilities. Thus for Pa,
we use an extension which re-scales SVM scores to
yield well-calibrated probabilities (Platt, 1999).
Our second stage model Pb, distributes mass
over the items in the tail of the N-best list (n ?
0%
20%
40%
60%
80%
100%
0% 20% 40% 60% 80% 100%
Cu
m
ul
at
ive
 p
ro
ba
bi
lity
Fractional position in N-Best list  (n/N) of correct entry
N-Best lists with N < 100 entries
N-Best lists with N >= 100 entries
All N-Best lists
Model (Beta distribution)
Figure 1: Empirical cumulative distribution of cor-
rect recognitions for N-Best lists, and the Beta dis-
tribution model for Pb on 1, 000 business search ut-
terances (Corpus 1 training set, from Section 4.)
{2, . . . , N}). In our exploratory analysis of N-Best
lists, we noticed a trend that facilitates modeling this
distribution. We observed that the distribution of the
fraction of the correction position n/N was rela-
tively invariant to N . For example, for both short
(N < 100) and long (N ? 100) lists, the proba-
bility that the answer was in the top half of the list
was very similar (see Figure 1). Thus, we chose a
continuous distribution in terms of the fractional po-
sition n/N as the underlying distribution in our sec-
ond stage model. Given the domain of the fractional
position [0, 1], we chose a Beta distribution. Our fi-
nal second stage model is then an appropriately dis-
cretized version of the underlying Beta, namely, Pb:
Pb(u = u?n|f) = Pb(u = u?n|N) =
Pbeta(
n? 1
N ? 1;?, ?) ? Pbeta(
n? 2
N ? 1;?, ?)
where Pbeta(x;?, ?) is the standard Beta cumula-
tive distribution function parametrized by ? and ?.
Figure 1 shows an illustration. In summary, our
method requires training the three-class regression
model Pa, and estimating the Beta distribution pa-
rameters ? and ?.
4 Data and experiments
We tested the method by applying it to three cor-
pora of utterances from dialog systems in the busi-
ness search domain. All utterances were from
133
Corpus WCN SVM MLR
1 -0.714 -0.697 -0.703
2 -0.251 -0.264 -0.222
3 -0.636 -0.605 -0.581
Table 1: Mean log-likelihoods on the portion of the
test set with the correct answer on the N-Best list.
None of the MLR nor SVM results differ signifi-
cantly from the WCN baseline at p < 0.02.2
users with real information needs. Corpus 1 con-
tained 2, 000 high-quality-audio utterances spoken
by customers using the Speak4It application, a
business search application which operates on mo-
bile devices, supporting queries containing a listing
name and optionally a location.1 Corpus 2 and 3
contained telephone-quality-audio utterances from
14, 000 calls to AT&T?s ?411? business directory
listing service. Corpus 2 contained locations (re-
sponses to ?Say a city and state?); corpus 3 con-
tained listing names (responses to ?OK what list-
ing??). Corpus 1 was split in half for training and
testing; corpora 2 and 3 were split into 10, 000 train-
ing and 4, 000 testing utterances.
We performed recognition using the Watson
speech recognition engine (Goffin et al, 2005), in
two configurations. Configuration A uses a sta-
tistical language model trained to recognize busi-
ness listings and optionally locations, and acous-
tic models for high-quality audio. Configuration B
uses a rule-based language model consisting of all
city/state pairs in the USA, and acoustic models for
telephone-quality audio. Configuration A was ap-
plied to corpora 1 and 3, and Configuration B was
applied to corpus 2. This experimental design is in-
tended to test our method on both rule-based and
statistical language models, as well as matched and
mis-matched acoustic and language model condi-
tions.
We used the following recognition features in f :
f1 is the posterior probability from the best path
through the word confusion network, f2 is the num-
ber of segments in the word confusion network,
f3 is the length of the N-Best list, f4 is the aver-
age per-frame difference in likelihood between the
1http://speak4it.com
22-tailed Wilcoxon Signed-Rank Test; 10-way partitioning.
Corpus WCN SVM MLR
1 -1.12 -0.882 -0.890
2 -0.821 -0.753 -0.734
3 -1.00 -0.820 -0.824
Table 2: Mean log-likelihoods on the complete test
set. All MLR and SVM results are significantly bet-
ter than the WCN baseline (p < 0.0054).2
highest-likelihood lattice path and a garbage model,
and f5 is the average per-frame difference in likeli-
hood between the highest-likelihood lattice path and
the maximum likelihood of that frame on any path
through the lattice. Features are standardized to the
range [?1, 1] and MLR and SVM hyperparameters
were fit by cross-validation on the training set. The
? and ? parameters were fit by maximum likelihood
on the training set.
We used the BMR toolkit for regularized multi-
nomial logistic regression (Genkin et al, 2005), and
the LIB-SVM toolkit for calibrated SVMs (Chang
and Lin, 2001).
We first measure average log-likelihood the mod-
els assign to the test sets. As a baseline, we use the
posterior probability estimated by the word confu-
sion network (WCN), which has been used in past
work for estimating likelihood of N-Best list entries
(Young et al, 2009). However, the WCN does not
assign probability to the u = u?? case ? indeed, this
is a limitation of using WCN posteriors. So we re-
ported two sets of results. In Table 1, we report the
average log-likelihood given that the correct result
is on the N-Best list (higher values, i.e., closer to
zero are better). This table includes only the items
in the test set for which the correct result appeared
on the N-Best list (that is, excluding the u = u??
cases). This table compares our models to WCNs
on the task for which the WCN is designed. On this
task, the MLR and SVM methods are competitive
with WCNs, but not significantly better.
In Table 2, we report average log-likelihood for
the entire test set. Here the WCNs use a fixed
prior for the u = u?? case, estimated on the training
sets (u = u?? class is always assigned 0.284; other
classes are assigned 1 ? 0.284 = 0.716 times the
WCN posterior). This table compares our models
to WCNs on the task for which our model is de-
signed. Here, the MLR and SVM models yielded
134
020
40
60
80
100
120
140
160
180
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
Nu
m
be
r o
f e
nt
rie
s
Fr
ac
tio
n 
co
rre
ct
Regression-assigned probability
Perfect calibration (left axis)
MLR Calibration (left axis)
Number of entries (right axis)
Figure 2: Calibration and histogram of probabilities
assigned by MLR on corpus 1 (test set).
significantly better results than the WCN baseline.
We next investigated the calibration properties of
the models. Results for the MLR model on the
u = u?1 class from corpus 1 test set are shown in
Figure 2. This illustrates that the MLR model is rel-
atively well-calibrated and yields broadly distributed
probabilities. Results for the SVM were similar, and
are omitted for space.
Finally we investigated whether the models
yielded better accept/reject decisions than their in-
dividual features. Figure 3 shows the MLR model
a receiver operating characteristic (ROC) curve for
corpus 1 test set for the u = u?1 class. This con-
firms that the MLR model produces more accurate
accept/reject decisions than the individual features
alone. Results for the SVM were similar.
5 Conclusions
This paper has presented a method for assigning
useful, meaningful probabilities to elements on an
ASR N-Best list. Multinomial logistic regression
(MLR) and support vector machines (SVMs) have
been tested, and both produce significantly better
models than a word confusion network baseline, as
measured by average log likelihood. Further, the
models appear to be well-calibrated and yield a bet-
ter indication of correctness than any of its input fea-
tures individually.
In dialog systems, we are often more interested in
the concepts than specific words, so in future work,
we hope to assign probabilities to concepts. In the
0%
10%
20%
30%
40%
50%
0% 10% 20% 30% 40%
Tr
ue
 A
cc
ep
ts
False Accepts
MLR-assigned probability
n=1 posterior from word confusion network (f )
Average delta to best frame in lattice (f )
Average delta to garbage model (f )
1
5
4
Figure 3: ROC curve for MLR and the 3 most infor-
mative input features on corpus 1 (test set).
meantime, we are applying the method to our dialog
systems, to verify their usefulness in practice.
References
CC Chang and CJ Lin, 2001. LIBSVM: a library for sup-
port vector machines. http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
A Genkin, DD Lewis, and D Madigan, 2005.
BMR: Bayesian Multinomial Regression Soft-
ware. http://www.stat.rutgers.edu/
?madigan/BMR/.
V Goffin, C Allauzen, E Bocchieri, D Hakkani-Tur,
A Ljolje, S Parthasarathy, M Rahim, G Riccardi, and
M Saraclar. 2005. The AT&T Watson speech recog-
nizer. In Proc ICASSP, Philadelphia.
JC Platt. 1999. Probabilistic outputs for support vector
machines and comparisons to regularized likelihood
methods. In Advances in Large Margin Classifiers,
pages 61?74. MIT Press.
JD Williams and SJ Young. 2007. Partially observable
Markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(2):393?422.
JD Williams. 2008. Exploiting the ASR N-best by track-
ing multiple dialog state hypotheses. In Proc ICSLP,
Brisbane.
SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatzmann,
B Thomson, and K Yu. 2009. The hidden information
state model: a practical framework for POMDP-based
spoken dialogue management. Computer Speech and
Language. To appear.
135
