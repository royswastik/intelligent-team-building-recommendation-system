Proceedings of the EACL 2009 Workshop on Language Technologies for African Languages ? AfLaT 2009, pages 1?8,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Collecting and evaluating speech recognition corpora
for nine Southern Bantu languages
Jaco Badenhorst, Charl van Heerden, Marelie Davel and Etienne Barnard
HLT Research Group, Meraka Institute, CSIR, South Africa
jbadenhorst@csir.co.za, mdavel@csir.co.za
cvheerden@csir.co.za, ebarnard@csir.co.za
Abstract
We describe the Lwazi corpus for auto-
matic speech recognition (ASR), a new
telephone speech corpus which includes
data from nine Southern Bantu lan-
guages. Because of practical constraints,
the amount of speech per language is
relatively small compared to major cor-
pora in world languages, and we report
on our investigation of the stability of
the ASR models derived from the corpus.
We also report on phoneme distance mea-
sures across languages, and describe initial
phone recognisers that were developed us-
ing this data.
1 Introduction
There is a widespread belief that spoken dialog
systems (SDSs) will have a significant impact in
the developing countries of Africa (Tucker and
Shalonova, 2004), where the availability of alter-
native information sources is often low. Tradi-
tional computer infrastructure is scarce in Africa,
but telephone networks (especially cellular net-
works) are spreading rapidly. In addition, speech-
based access to information may empower illiter-
ate or semi-literate people, 98% of whom live in
the developing world.
Spoken dialog systems can play a useful role
in a wide range of applications. Of particular im-
portance in Africa are applications such as ed-
ucation, using speech-enabled learning software
or kiosks and information dissemination through
media such as telephone-based information sys-
tems. Significant benefits can be envisioned if
information is provided in domains such as agri-
culture (Nasfors, 2007), health care (Sherwani et
al., ; Sharma et al, 2009) and government ser-
vices (Barnard et al, 2003). In order to make
SDSs a reality in Africa, technology components
such as text-to-speech (TTS) systems and auto-
matic speech recognition (ASR) systems are re-
quired. The latter category of technologies is the
focus of the current contribution.
Speech recognition systems exist for only a
handful of African languages (Roux et al, ; Seid
and Gambck, 2005; Abdillahi et al, 2006), and
to our knowledge no service available to the gen-
eral public currently uses ASR in an indigenous
African language. A significant reason for this
state of affairs is the lack of sufficient linguistic
resources in the African languages. Most impor-
tantly, modern speech recognition systems use sta-
tistical models which are trained on corpora of
relevant speech (i.e. appropriate for the recogni-
tion task in terms of the language used, the pro-
file of the speakers, speaking style, etc.) This
speech generally needs to be curated and tran-
scribed prior to the development of ASR systems,
and for most applications speech from a large
number of speakers is required in order to achieve
acceptable system performance. On the African
continent, where infrastructure such as computer
networks is less developed than in countries such
as America, Japan and the European countries, the
development of such speech corpora is a signifi-
cant hurdle to the development of ASR systems.
The complexity of speech corpus development
is strongly correlated with the amount of data that
is required, since the number of speakers that need
to be canvassed and the amount of speech that
must be curated and transcribed are major fac-
tors in determining the feasibility of such devel-
opment. In order to minimise this complexity, it
is important to have tools and guidelines that can
be used to assist in designing the smallest cor-
pora that will be sufficient for typical applications
of ASR systems. As minimal corpora can be ex-
tended by sharing data across languages, tools are
also required to indicate when data sharing will be
beneficial and when detrimental.
1
In this paper we describe and evaluate a new
speech corpus of South African languages cur-
rently under development (the Lwazi corpus) and
evaluate the extent in which computational anal-
ysis tools can provide further guidelines for ASR
corpus design in resource-scarce languages.
2 Project Lwazi
The goal of Project Lwazi is to provide South
African citizens with information and informa-
tion services in their home language, over the
telephone, in an efficient and affordable manner.
Commissioned by the South African Department
of Arts and Culture, the activities of this three year
project (2006-2009) include the development of
core language technology resources and compo-
nents for all the official languages of South Africa,
where, for the majority of these, no prior language
technology components were available.
The core linguistic resources being developed
include phoneme sets, electronic pronunciation
dictionaries and the speech and text corpora re-
quired to develop automated speech recognition
(ASR) and text-to-speech (TTS) systems for all
eleven official languages of South Africa. The
usability of these resources will be demonstrated
during a national pilot planned for the third quar-
ter of 2009. All outputs from the project are be-
ing released as open source software and open
content(Meraka-Institute, 2009).
Resources are being developed for all nine
Southern Bantu languages that are recognised as
official languages in South Africa (SA). These lan-
guages are: (1) isiZulu (zul1) and isiXhosa (xho),
the two Nguni languages most widely spoken in
SA. Together these form the home language of
41% of the SA population. (2) The three Sotho
languages: Sepedi (nso), Setswana (tsn), Sesotho
(sot), together the home language of 26% of the
SA population. (3) The two Nguni languages less
widely spoken in SA: siSwati (ssw) and isiNde-
bele (nbl), together the home language of 4% of
the SA population. (4) Xitsonga (tso) and Tshiv-
enda (ven), the home languages of 4% and 2% of
the SA population, respectively (Lehohla, 2003).
(The other two official languages of South Africa
are Germanic languages, namely English (eng)
and Afrikaans (afr).)
For all these languages, new pronunciation dic-
1After each language name, the ISO 639-3:2007 language
code is provided in brackets.
tionaries, text and speech corpora are being de-
veloped. ASR speech corpora consist of ap-
proximately 200 speakers per language, produc-
ing read and elicited speech, recorded over a tele-
phone channel. Each speaker produced approxi-
mately 30 utterances, 16 of these were randomly
selected from a phonetically balanced corpus and
the remainder consist of short words and phrases:
answers to open questions, answers to yes/no
questions, spelt words, dates and numbers. The
speaker population was selected to provide a bal-
anced profile with regard to age, gender and type
of telephone (cellphone or landline).
3 Related work
Below, we review earlier work relevant to the de-
velopment of speech recognisers for languages
with limited resources. This includes both ASR
system design (Sec. 3.1) and ASR corpus design
(Sec. 3.2). In Sec. 3.3, we also review the ana-
lytical tools that we utilise in order to investigate
corpus design systematically.
3.1 ASR for resource-scarce languages
The main linguistic resources required when de-
veloping ASR systems for telephone based sys-
tems are electronic pronunciation dictionaries, an-
notated audio corpora (used to construct acous-
tic models) and recognition grammars. An ASR
audio corpus consists of recordings from multi-
ple speakers, with each utterance carefully tran-
scribed orthographically and markers used to indi-
cate non-speech and other events important from
an ASR perspective. Both the collection of ap-
propriate speech from multiple speakers and the
accurate annotation of this speech are resource-
intensive processes, and therefore corpora for
resource-scarce languages tend to be very small
(1 to 10 hours of audio) when compared to the
speech corpora used to build commercial systems
for world languages (hundreds to thousands of
hours per language).
Different approaches have been used to best
utilise limited audio resources when developing
ASR systems. Bootstrapping has been shown to
be a very efficient technique for the rapid devel-
opment of pronunciation dictionaries, even when
utilising linguistic assistants with limited phonetic
training (Davel and Barnard, 2004).
Small audio corpora can be used efficiently by
utilising techniques that share data across lan-
2
guages, either by developing multilingual ASR
systems (a single system that simultaneously
recognises different languages), or by using addi-
tional source data to supplement the training data
that exists in the target language. Various data
sharing techniques for language-dependant acous-
tic modelling have been studied, including cross-
language transfer, data pooling, language adap-
tation and bootstrapping (Wheatley et al, 1994;
Schultz and Waibel, 2001; Byrne et al, 2000).
Both (Wheatley et al, 1994) and (Schultz and
Waibel, 2001) found that useful gains could be
obtained by sharing data across languages with
the size of the benefit dependent on the similar-
ity of the sound systems of the languages com-
bined. In the only cross-lingual adaptation study
using African languages (Niesler, 2007), similar
gains have not yet been observed.
3.2 ASR corpus design
Corpus design techniques for ASR are generally
aimed at specifying or selecting the most appro-
priate subset of data from a larger domain in order
to optimise recognition accuracy, often while ex-
plicitly minimising the size of the selected corpus.
This is achieved through various techniques that
aim to include as much variability in the data as
possible, while simultaneously ensuring that the
corpus matches the intended operating environ-
ment as accurately as possible.
Three directions are primarily employed: (1)
explicit specification of phonotactic, speaker and
channel variability during corpus development, (2)
automated selection of informative subsets of data
from larger corpora, with the smaller subset yield-
ing comparable results, and (3) the use of active
learning to optimise existing speech recognition
systems. All three techniques provide a perspec-
tive on the sources of variation inherent in a speech
corpus, and the effect of this variation on speech
recognition accuracy.
In (Nagroski et al, 2003), Principle Component
Analysis (PCA) is used to cluster data acousti-
cally. These clusters then serve as a starting point
for selecting the optimal utterances from a train-
ing database. As a consequence of the clustering
technique, it is possible to characterise some of the
acoustic properties of the data being analysed, and
to obtain an understanding of the major sources of
variation, such as different speakers and genders
(Riccardi and Hakkani-Tur, 2003).
Active and unsupervised learning methods can
be combined to circumvent the need for tran-
scribing massive amounts of data (Riccardi and
Hakkani-Tur, 2003). The most informative untran-
scribed data is selected for a human to label, based
on acoustic evidence of a partially and iteratively
trained ASR system. From such work, it soon be-
comes evident that the optimisation of the amount
of variation inherent to training data is needed,
since randomly selected additional data does not
necessarily improve recognition accuracy. By fo-
cusing on the selection (based on existing tran-
scriptions) of a uniform distribution across differ-
ent speech units such as words and phonemes, im-
provements are obtained (Wu et al, 2007).
In our focus on resource-scarce languages, the
main aim is to understand the amount of data that
needs to be collected in order to achieve accept-
able accuracy. This is achieved through the use
of analytic measures of data variability, which we
describe next.
3.3 Evaluating phoneme stability
In (Badenhorst and Davel, 2008) a technique is
developed that estimates how stable a specific
phoneme is, given a specific set of training data.
This statistical measure provides an indication of
the effect that additional training data will have on
recognition accuracy: the higher the stability, the
less the benefit of additional speech data.
The model stability measure utilises the Bhat-
tacharyya bound (Fukunaga, 1990), a widely-used
upper bound of the Bayes error. If Pi and pi(X)
denote the prior probability and class-conditional
density function for class i, respectively, the Bhat-
tacharyya bound ? is calculated as:
? =
?
P1P2
? ?
p1(X)p2(X)dX (1)
When both density functions are Gaussian with
mean ?i and covariance matrix ?i, integration of
? leads to a closed-form expression for ?:
? =
?
P1P2e??(1/2) (2)
where
?(1/2) = 18(?2 ? ?1)
T
[?1 +?2
2
]?1(?2 ? ?1)
+ 12 ln
|?1+?22 |?|?1||?2|
(3)
is referred to as the Bhattacharyya distance.
3
In order to estimate the stability of an acous-
tic model, the training data for that model is sep-
arated into a number of disjoint subsets. All sub-
sets are selected to be mutually exclusive with re-
spect to the speakers they contain. For each sub-
set, a separate acoustic model is trained, and the
Bhattacharyya bound between each pair of mod-
els calculated. By calculating both the mean of
this bound and the standard deviation of this mea-
sure across the various model pairs, a statistically
sound measure of model estimation stability is ob-
tained.
4 Computational analysis of the Lwazi
corpus
We now report on our analysis of the Lwazi
speech corpus, using the stability measure de-
scribed above. Here, we focus on four languages
(isiNdebele, siSwati, isiZulu and Tshivenda) for
reasons of space; later, we shall see that the other
languages behave quite similarly.
4.1 Experimental design
For each phoneme in each of our target lan-
guages, we extract all the phoneme occurrences
from the 150 speakers with the most utterances per
phoneme. We utilise the technique described in
Sec. 3.3 to estimate the Bhattacharyya bound both
when evaluating phoneme variability and model
distance. In both cases we separate the data for
each phoneme into 5 disjoint subsets. We calcu-
late the mean of the 10 distances obtained between
the various intra-phoneme model pairs when mea-
suring phoneme stability, and the mean of the
25 distances obtained between the various inter-
phoneme model pairs when measuring phoneme
distance.
In order to be able to control the number of
phoneme observations used to train our acoustic
models, we first train a speech recognition system
and then use forced alignment to label all of the
utterances using the systems described in Sec. 5.
Mel-frequency cepstral coefficients (MFCCs) with
cepstral mean and variance normalisation are used
as features, as described in Sec. 5.
4.2 Analysis of phoneme variability
In an earlier analysis of phoneme variability of
an English corpus (Badenhorst and Davel, 2008),
it was observed that similar trends are observed
when utilising different numbers of mixtures in
a Gaussian mixture model. For both context de-
pendent and context independent models similar
trends are also observed. (Asymptotes occur later,
but trends remain similar.) Because of the limited
size of the Lwazi corpus, we therefore only report
on single-mixture context-independent models in
the current section.
As we also observe similar trends for phonemes
within the same broad categories, we report on
one or two examples from several broad categories
which occur in most of our target languages. Us-
ing SAMPA notation, the following phonemes are
selected: /a/ (vowels), /m/ (nasals), /b/ and /g/
(voiced plosives) and /s/ (unvoiced fricatives), af-
ter verifying that these phonemes are indeed rep-
resentative of the larger groups.
Figures 1 and 2 demonstrate the effects of vari-
able numbers of phonemes and speakers, respec-
tively, on the value of the mean Bhattacharyya
bound. This value should approach 0.5 for a model
fully trained on a sufficiently representative set of
data. In Fig. 1 we see that the various broad cate-
gories of sounds approach the asymptotic bound in
different ways. The vowels and nasals require the
largest number of phoneme occurrences to reach
a given level, whereas the fricatives and plosives
converge quite rapidly (With 10 observations per
speaker, both the fricatives and plosives achieve
values of 0.48 or better for all languages, in con-
trast to the vowels and nasals which require 30 ob-
servations to reach similar stability). Note that we
employed 30 speakers per phoneme group, since
that is the largest number achievable with our pro-
tocol.
For the results in Fig. 2, we keep the number
of phoneme occurrences per speaker fixed at 20
(this ensures that we have sufficient data for all
phonemes, and corresponds with reasonable con-
vergence in Fig. 1). It is clear that additional
speakers would still improve the modelling ac-
curacy for especially the vowels and nasals. We
observe that the voiced plosives and fricatives
quickly achieve high values for the bound (close
to the ideal 0.5).
Figures 1 and 2 ? as well as similar figures for
the other phoneme classes and languages we have
studied ? suggest that all phoneme categories re-
quire at least 20 training speakers to achieve rea-
sonable levels of convergence (bound levels of
0.48 or better). The number of phoneme observa-
tions required per speaker is more variable, rang-
4
Figure 1: Effect of number of phoneme utterances per speaker on mean of Bhattacharyya bound for
different phoneme groups using data from 30 speakers
ing from less than 10 for the voiceless fricatives
to 30 or more for vowels, liquids and nasals. We
return to these observations below.
4.3 Distances between languages
In Sec. 3.1 it was pointed out that the simi-
larities between the same phonemes in different
languages are important predictors of the bene-
fit achievable from pooling the data from those
languages. Armed with the knowledge that sta-
ble models can be estimated with 30 speakers per
phoneme and between 10 and 30 phonemes oc-
currences per speaker, we now turn to the task of
measuring distances between phonemes in various
languages.
We again use the mean Bhattacharyya bound
to compare phonemes, and obtain values between
all possible combinations of phonemes. Results
are shown for the isiNdebele phonemes /n/ and /a/
in Fig. 3. As expected, similar phonemes from
the different languages are closer to one another
than different phonemes of the same language.
However, the details of the distances are quite re-
vealing: for /a/, siSwati is closest to the isiN-
debele model, as would be expected given their
close linguistic relationship, but for /n/, the Tshiv-
enda model is found to be closer than either of
the other Nguni languages. For comparative pur-
poses, we have included one non-Bantu language
(Afrikaans), and we see that its models are indeed
significantly more dissimilar from the isiNdebele
model than any of the Bantu languages. In fact,
the Afrikaans /n/ is about as distant from isiNde-
bele /n/ as isiNdebele and isiZulu /l/ are!
5 Initial ASR results
In order to verify the usability of the Lwazi cor-
pus for speech recognition, we develop initial
ASR systems for all 11 official South African
languages. A summary of the data statistics for
the Bantu languages investigated is shown in Tab.
1, and recognition accuracies achieved are sum-
marised in Tab. 2. For these tests, data from 30
speakers per language were used as test data, with
the remaining data being used for training.
Although the Southern Bantu languages are
tone languages, our systems do not encode tonal
5
Figure 2: Effect of number of speakers on mean of Bhattacharyya bound for different phoneme groups
using 20 utterances per speaker
Language total # # speech # distinct
minutes minutes phonemes
isiNdebele 564 465 46
isiXhosa 470 370 52
isiZulu 525 407 46
Tshivenda 354 286 38
Sepedi 394 301 45
Sesotho 387 313 44
Setswana 379 295 34
siSwati 603 479 39
Xitsonga 378 316 54
N-TIMIT 315 - 39
Table 1: A summary of the Lwazi ASR corpus:
Bantu languages.
information, since tone is unlikely to be impor-
tant for small-to-medium vocabulary applications
(Zerbian and Barnard, 2008).
As the initial pronunciation dictionaries were
developed to provide good coverage of the lan-
guage in general, these dictionaries did not cover
the entire ASR corpus. Grapheme-to-phoneme
rules are therefore extracted from the general
dictionaries using the Default&Refine algorithm
(Davel and Barnard, 2008) and used to generate
missing pronunciations.
We use HTK 3.4 to build a context-dependent
cross-word HMM-based phoneme recogniser with
triphone models. Each model had 3 emitting
states with 7 mixtures per state. 39 features are
used: 13 MFCCs together with their first and sec-
ond order derivatives. Cepstral Mean Normali-
sation (CMN) as well as Cepstral Variance Nor-
malisation (CMV) are used to perform speaker-
independent normalisation. A diagonal covariance
matrix is used; to partially compensate for this in-
correct assumption of feature independence semi-
tied transforms are applied. A flat phone-based
language model is employed throughout.
As a rough benchmark of acceptable phoneme-
recognition accuracy, recently reported results ob-
tained by (Morales et al, 2008) on a similar-sized
telephone corpus in American English (N-TIMIT)
are also shown in Tab. 2. We see that the Lwazi
results compare very well with this benchmark.
An important issue in ASR corpus design is
6
Figure 3: Effective distances in terms of the mean of the Bhattacharyya bound between a single phoneme
(/n/-nbl top and /a/-nbl bottom) and each of its closest matches within the set of phonemes investigated.
Language % corr % acc avg # total #
phons speakers
isiNdebele 74.21 65.41 28.66 200
isiXhosa 69.25 57.24 17.79 210
isiZulu 71.18 60.95 23.42 201
Tshivenda 76.37 66.78 19.53 201
Sepedi 66.44 55.19 16.45 199
Sesotho 68.17 54.79 18.57 200
Setswana 69.00 56.19 20.85 207
siSwati 74.19 64.46 30.66 208
Xitsonga 70.32 59.41 14.35 199
N-TIMIT 64.07 55.73 - -
Table 2: Initial results for South African ASR sys-
tems. The column labelled ?avg # phonemes? lists
the average number of phoneme occurrences for
each phoneme for each speaker.
the trade-off between the number of speakers and
the amount of data per speaker (Wheatley et al,
1994). The figures in Sec. 4.2 are not conclusive
on this trade-off, so we have also investigated the
effect of reducing either the number of speakers
or the amount of data per speaker when training
the isiZulu and Tshivenda recognisers. As shown
in Fig. 4, the impact of both forms of reduction
is comparable across languages and different de-
grees of reduction, in agreement with the results
of Sec. 4.2.
These results indicate that we now have a firm
Figure 4: The influence of a reduction in training
corpus size on phone recognition accuracy.
baseline to investigate data-efficient training meth-
ods such as those described in Sec. 3.1.
6 Conclusion
In this paper we have introduced a new tele-
phone speech corpus which contains data from
nine Southern Bantu languages. Our stability anal-
ysis shows that the speaker variety as well as
the amount of speech per speaker is sufficient to
achieve acceptable model stability, and this con-
clusion is confirmed by the successful training of
phone recognisers in all the languages. We con-
firm the observation in (Badenhorst and Davel,
2008) that different phone classes have different
7
data requirements, but even for the more demand-
ing classes (vowels, nasals, liquids) our amount of
data seems sufficient. Our results suggest that sim-
ilar accuracies may be achievable by using more
speech from fewer speakers ? a finding that may
be useful for the further development of speech
corpora in resource-scarce languages.
Based on the proven stability of our models, we
have performed some preliminary measurements
of the distances between the phones in the dif-
ferent languages; such distance measurements are
likely to be important for the sharing of data across
languages in order to further improve ASR accu-
racy. The development of real-world applications
using this data is currently an active topic of re-
search; for that purpose, we are continuing to in-
vestigate additional methods to improve recogni-
tion accuracy with such relatively small corpora,
including cross-language data sharing and effi-
cient adaptation methods.
References
Nimaan Abdillahi, Pascal Nocera, and Jean-Franois
Bonastre. 2006. Automatic transcription of Somali
language. In Interspeech, pages 289?292, Pitts-
burgh, PA.
J.A.C. Badenhorst and M.H. Davel. 2008. Data re-
quirements for speaker independent acoustic mod-
els. In PRASA, pages 147?152.
E. Barnard, L. Cloete, and H. Patel. 2003. Language
and technology literacy barriers to accessing govern-
ment services. Lecture Notes in Computer Science,
2739:37?42.
W. Byrne, P. Beyerlein, J. M. Huerta, S. Khudanpur,
B. Marthi, J. Morgan, N. Peterek, J. Picone, D. Ver-
gyri1, and W. Wang. 2000. Towards language inde-
pendent acoustic modeling. In ICASSP, volume 2,
pages 1029?1032, Istanbul, Turkey.
M. Davel and E. Barnard. 2004. The efficient cre-
ation of pronunication dictionaries: human factors
in bootstrapping. In Interspeech, pages 2797?2800,
Jeju, Korea, Oct.
M. Davel and E. Barnard. 2008. Pronunciation predi-
cation with Default&Refine. Computer Speech and
Language, 22:374?393, Oct.
K. Fukunaga. 1990. Introduction to Statistical Pattern
Recognition. Academic Press, Inc., 2nd edition.
Pali Lehohla. 2003. Census 2001: Census in brief.
Statistics South Africa.
Meraka-Institute. 2009. Lwazi ASR corpus. Online:
http://www.meraka.org.za/lwazi.
N. Morales, J. Tejedor, J. Garrido, J. Colas, and D.T.
Toledano. 2008. STC-TIMIT: Generation of a
single-channel telephone corpus. In LREC, pages
391?395, Marrakech, Morocco.
A. Nagroski, L. Boves, and H. Steeneken. 2003. In
search of optimal data selection for training of auto-
matic speech recognition systems. ASRU workshop,
pages 67?72, Nov.
P. Nasfors. 2007. Efficient voice information services
for developing countries. Master?s thesis, Depart-
ment of Information Technology, Uppsala Univer-
sity.
T. Niesler. 2007. Language-dependent state clustering
for multilingual acoustic modeling. Speech Commu-
nication, 49:453?463.
G. Riccardi and D. Hakkani-Tur. 2003. Active and
unsupervised learning for automatic speech recog-
nition. In Eurospeech, pages 1825?1828, Geneva,
Switzerland.
J.C. Roux, E.C. Botha, and J.A. du Preez. Develop-
ing a multilingual telephone based information sys-
tem in african languages. In LREC, pages 975?980,
Athens, Greece.
T. Schultz and A. Waibel. 2001. Language-
independent and language-adaptive acoustic model-
ing for speech recognition. Speech Communication,
35:31?51, Aug.
Hussien Seid and Bjrn Gambck. 2005. A speaker inde-
pendent continuous speech recognizer for Amharic.
In Interspeech, pages 3349?3352, Lisboa, Portugal,
Oct.
A. Sharma, M. Plauche, C. Kuun, and E. Barnard.
2009. HIV health information access using spoken
dialogue systems: Touchtone vs. speech. Accepted
at IEEE Int. Conf. on ICTD.
J. Sherwani, N. Ali, S. Mirza, A. Fatma, Y. Memon,
M. Karim, R. Tongia, and R. Rosenfeld. Healthline:
Speech-based access to health information by low-
literate users. In IEEE Int. Conf. on ICTD, pages
131?139.
R. Tucker and K. Shalonova. 2004. The Local Lan-
guage Speech Technology Initiative. In SCALLA
Conf., Nepal.
B. Wheatley, K. Kondo, W. Anderson, and
Y. Muthusumy. 1994. An evaluation of cross-
language adaptation for rapid HMM development
in a new language. In ICASSP, pages 237?240,
Adelaide.
Y. Wu, R. Zhang, and A. Rudnicky. 2007. Data selec-
tion for speech recognition. ASRU workshop, pages
562?565, Dec.
S. Zerbian and E. Barnard. 2008. Phonetics of into-
nation in South African Bantu languages. Southern
African Linguistics and Applied Language Studies,
26(2):235?254.
8
