Proceedings of NAACL HLT 2009: Short Papers, pages 277?280,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Fast decoding for open vocabulary spoken term detection
1B. Ramabhadran,1A. Sethy, 2J. Mamou?1 B. Kingsbury, 1 U. Chaudhari
1IBM T. J. Watson Research Center
Yorktown Heights,NY
2IBM Haifa Research Labs
Mount Carmel,Haifa
Abstract
Information retrieval and spoken-term detec-
tion from audio such as broadcast news, tele-
phone conversations, conference calls, and
meetings are of great interest to the academic,
government, and business communities. Mo-
tivated by the requirement for high-quality in-
dexes, this study explores the effect of using
both word and sub-word information to find
in-vocabulary and OOV query terms. It also
explores the trade-off between search accu-
racy and the speed of audio transcription. We
present a novel, vocabulary independent, hy-
brid LVCSR approach to audio indexing and
search and show that using phonetic confu-
sions derived from posterior probabilities es-
timated by a neural network in the retrieval
of OOV queries can help in reducing misses.
These methods are evaluated on data sets from
the 2006 NIST STD task.
1 Introduction
Indexing and retrieval of speech content in vari-
ous forms such as broadcast news, customer care
data and on-line media has gained a lot of interest
for a wide range of applications from market in-
telligence gathering, to customer analytics and on-
line media search. Spoken term detection (STD) is
a key information retrieval technology which aims
open vocabulary search over large collections of
spoken documents. An approach for solving the out-
of-vocabulary (OOV) issues (Saraclar and Sproat,
2004) consists of converting speech into phonetic,
?TThe work done by J. Mamou was partially funded by the
EU projects SAPIR and HERMES
syllabic or word-fragment transcripts and represent-
ing the query as a sequence of phones, syllables or
word-fragments respectively. Popular approaches
include subword decoding (Clements et al, 2002;
Mamou et al, 2007; Seide et al, 2004; Siohan and
Bacchiani, 2005) and representations enhanced with
phone confusion probabilities and approximate sim-
ilarity measures (Chaudhari and Picheny, 2007).
2 Fast Decoding Architecture
The first step in converting speech to a searchable in-
dex involves the use of an ASR system that produces
word, word-fragment or phonetic transcripts. In
this paper, the LVCSR system is a discriminatively
trained speaker-independent recognizer using PLP-
derived features and a quinphone acoustic model
with approximately 1200 context dependent states
and 30000 Gaussians. The acoustic model is trained
on 430 hours of audio from the 1996 and 1997 En-
glish Broadcast News Speech corpus (LDC97S44,
LDC98S71) and the TDT4 Multilingual Broadcast
News Speech corpus (LDC2005S11).
The language model used for decoding is a tri-
gram model with 84087 words trained on a collec-
tion of 335M words from the following data sources:
Hub4 Language Model data, EARS BN03 closed
captions and GALE Broadcast news and conversa-
tions data. A word-fragment language model is built
on this same data after tokenizing the text to frag-
ments using a fragment inventory of size 21000. A
greedy search algorithm assigns the longest possi-
ble matching fragment first and iteratively uses the
next longest possible fragment until the entire pro-
nunciation of the OOV term has been represented
277
0 5 10 15 20 25 30
30
40
50
60
70
80
90
Real Time Factor
W
ER
Figure 1: Speed vs WER
by sub-word units.
The speed and accuracy of the decoding are con-
trolled using two forms of pruning. The first is the
standard likelihood-based beam pruning that is used
in many Viterbi decoders. The second is a form
of Gaussian shortlisting in which the Gaussians in
the acoustic model are clustered into 1024 clusters,
each of which is represented by a single Gaussian.
When the decoder gets a new observation vector, it
computes the likelihood of the observation under all
1024 cluster models and then ranks the clusters by
likelihood. Observation likelihoods are then com-
puted only for those mixture components belonging
to the top maxL1 clusters; for components outside
this set a default, low likelihood is used. To illus-
trate the trade-offs in speed vs. accuracy that can
be achieved by varying the two pruning parame-
ters, we sweep through different values for the pa-
rameters and measure decoding accuracy, reported
as word error rate (WER), and decoding speed, re-
ported as times faster than real time (xfRT). For ex-
ample, a system that operates at 20xfRT will require
one minute of time (measured as elapsed time) to
process 20 minutes of speech. Figure 1 illustrates
this effect on the NIST 2006 Spoken Term Detec-
tion Dev06 test set.
3 Lucene Based Indexing and Search
The main difficulty with retrieving information from
spoken data is the low accuracy of the transcription,
particularly on terms of interest such as named en-
tities and content words. Generally, the accuracy
of a transcript is measured by its word error rate
(WER), which is characterized by the number of
substitutions, deletions, and insertions with respect
to the correct audio transcript. Mamou (Mamou
et al, 2007) presented the enhancement in recall
and precision by searching on word confusion net-
works instead of considering only the 1-best path
word transcript. We used this model for searching
in-vocabulary queries.
To handle OOV queries, a combination of
word and phonetic search was presented by
Mamou (Mamou et al, 2007). In this paper, we ex-
plore fuzzy phonetic search extending Lucene1, an
Apache open source search library written in Java,
for indexing and search. When searching for these
OOVs in word-fragment indexes, they are repre-
sented phonetically (and subsequently using word-
fragments) using letter-to-phoneme (L2P) rules.
3.1 Indexing
Each transcript is composed of basic units (e.g.,
word, word-fragment, phones) associated with a be-
gin time, duration and posterior probability. An
inverted index is used in a Lucene-based indexing
scheme. Each occurrence of a unit of indexing u in
a transcript D is indexed on its timestamp. If the
posterior probability is provided, we store the confi-
dence level of the occurrence of u at the time t that
is evaluated by its posterior probability Pr(u|t,D).
Otherwise, we consider its posterior probability to
be one. This representation allows the indexing of
different types of transcripts into a single index.
3.2 Retrieval
Since the vocabulary of the ASR system used to gen-
erate the word transcripts is known, we can easily
identify IV and OOV parts of the query. We present
two different algorithms, namely, exact and fuzzy
search on word-fragment transcripts. For search
on word-fragment or phonetic transcripts, the query
terms are converted to their word-fragment or pho-
netic representation.
Candidate lists of each query unit are extracted
from the inverted index. For fuzzy search, we re-
trieve several fuzzy matches from the inverted in-
dex for each unit of the query using the edit distance
weighted by the substitution costs provided by the
confusion matrix. Only the matches whose weighted
1http://lucene.apache.org/
278
edit distance is below a given threshold are returned.
We use a dynamic programming algorithm to incor-
porate the confusion costs specified in the matrix
in the distance computation. Our implementation is
fail-fast since the procedure is aborted if it is discov-
ered that the minimal cost between the sequences is
greater than a certain threshold.
The score of each occurrence aggregates the pos-
terior probability of each indexed unit. The occur-
rence of each unit is also weighted (user defined
weight) according to its type, for example, a higher
weight can be assigned to word matches instead of
word-fragment or phonetic matches. Given the na-
ture of the index, a match for any query term cannot
span across two consecutively indexed units.
3.3 Hybrid WordFragment Indexing
For the hybrid system we limited the word portion
of the ASR system?s lexicon to the 21K most fre-
quent (frequency greater than 5) words in the acous-
tic training data. This resulted in roughly 11M
(3.1%) OOV tokens in the hybrid LM training set
and 1127(2.5%) OOV tokens in the evaluation set.
A relative entropy criterion described in (Siohan and
Bacchiani, 2005) based on a 5-gram phone language
model was used to identify fragments. We selected
21K fragments to complement the 21K words result-
ing in a composite 42K vocabulary. The language
model text (11M (3.1%) fragment tokens and 320M
word tokens) was tokenized to contain words and
word-fragments (for the OOVs) and the resulting hy-
brid LM was used in conjunction with the acoustic
models described in Section 2.
4 Neural Network Based Posteriors for
Fuzzy Search
In assessing the match of decoded transcripts with
search queries, recognition errors must be accounted
for. One method relies on converting both the de-
coded transcripts and queries into phonetic represen-
tations and modeling the confusion between phones,
typically represented as a confusion matrix. In this
work, we derive this matrix from broadcast news de-
velopment data. In particular, two systems: HMM
based automatic speech recognition (ASR) (Chaud-
hari and Picheny, 2007) and a neural network based
acoustic model (Kingsbury, 2009), are used to ana-
lyze the data and the results are compared to produce
confusion estimates.
Let X = {xt} represent the input feature frames
and S the set of context dependent HMM states.
Associated with S is a many to one map M from
each member sj ? S to a phone in the phone set
pk ? P. This map collapses the beginning, mid-
dle, and end context dependent states to the central
phone identity. The ASR system is used to generate
a state based alignment of the development data to
the training transcripts. This results in a sequence
of state labels (classes) {st}, st ? S , one for each
frame of the input data. Note that the aligned states
are collapsed to the phone identity with M, so the
frame class labels are given by {ct}, ct ? P.
Corresponding to each frame, we also use the
state posteriors derived from the output of a Neu-
ral Network acoustic model and the prior probabil-
ities computed on the training set. Define Xt =
{. . . , xt, . . .} to be the sub-sequence of the input
speech frames centered around time index t. The
neural network takes Xt as input and produces
lt(sj) = y(sj|Xt)? l(sj), sj ? S
where y is the neural network output and l is the
prior probability, both in the log domain. Again, the
state labels are mapped using M, so the above pos-
terior is interpreted as that for the collapsed phone:
lt(sj) ? lt(M(sj)) = lt(pj), pj = M(sj).
The result of both analyses gives the following set of
associations:
c0 ? l0(p0), l0(p1), l0(p2), . . .
c1 ? l1(p0), l1(p1), l1(p2), . . .
.
.
ct ? lt(p0), lt(p1), lt(p2), . . .
Each log posterior li(pj) is converted into a count
ni,j = ceil[N ? eli(pj)],
where N is a large constant, i ranges over the
time index, and j ranges over the context dependent
states. From the counts, the confusion matrix entries
are computed. The total count for each state is
nj(k) =
?
i:ci=pj
ni,k,
279
where k is an index over the states.
?
????
n1(1) n1(2) . . .
n2(1) n2(2) . . .
.
.
?
????
The rows of the above matrix correspond to the ref-
erence and the columns to the observations. By nor-
malizing the rows, the entries can be interpreted as
?probability? of an observed phone (indicated by the
column) given the true phone.
5 Experiments and Results
The performance of a spoken term detection system
is measured using DET curves that plot the trade-off
between false alarms (FAs) and misses. This NIST
STD 2006 evaluation metric used Actual/Maximum
Term Weighted Value (ATWV/MTWV) that allows
one to weight FAs and Misses per the needs of the
task at hand (NIST, 2006).
Figure 2 illustrates the effect of speed on ATWV
on the NIST STD 2006 Dev06 data set using 1107
query terms. As the speed of indexing is increased to
many times faster than real time, the WER increases,
which in turn decreases the ATWV measure. It can
be seen that the use of word-fragments improves
the performance on OOV queries thus making the
combined search better than simple word search.
The primary advantage of using a hybrid decoding
scheme over a separate word and fragment based
decoding scheme is the speed of transforming the
audio into indexable units. The blue line in the fig-
ure illustrates that when using a hybrid setup, the
same performance can be achieved at speeds twice
as fast. For example, with the combined search
on two different decodes, an ATWV of 0.1 can be
achieved when indexing at a speed 15 times faster
than real time, but with a hybrid system, the same
performance can be reached at an indexing speed 30
times faster than real time. The ATWV on the hybrid
system also degrades gracefully with faster speeds
when compared to separate word and word-fragment
systems. Preliminary results indicate that fuzzy
search on one best output gives the same ATWV
performance as exact search (Figure 2) on consen-
sus output. Also, a closer look at the retrieval results
of OOV terms revealed that many more OOVs are
retrieved with the fuzzy search.
0 5 10 15 20 25 30 35
?0.4
?0.2
0
0.2
0.4
0.6
0.8
1
Real Time Factor
AT
W
V
 
 
exactWord
exactWordAndFrag
exactHybrid
Figure 2: Effect of WER on ATWV. Note that the cuves
for exactWord and exactWordAndFrag lie on top of each
other.
6 CONCLUSION
In this paper, we have presented the effect of rapid
decoding on a spoken term detection task. We
have demonstrated that hybrid systems perform well
and fuzzy search with phone confusion probabilities
help in OOV retrieval.
References
U. V. Chaudhari and M. Picheny. 2007. Improvements in
phone based audio search via constrained match with
high order confusion estimates. In Proc. of ASRU.
M. Clements, S. Robertson, and M. S. Miller. 2002.
Phonetic searching applied to on-line distance learning
modules. In Proc. of IEEE Digital Signal Processing
Workshop.
B. Kingsbury. 2009. Lattice-based optimization
of sequence classification criteria for neural-network
acoustic modeling. In Proc. of ICASSP.
J. Mamou, B. Ramabhadran, and O. Siohan. 2007. Vo-
cabulary independent spoken term detection. In Proc.
of ACM SIGIR.
NIST. 2006. The spoken term de-
tection (STD) 2006 evaluation plan.
http://www.nist.gov/speech/tests/std/docs/std06-
evalplan-v10.pdf.
M. Saraclar and R. Sproat. 2004. Lattice-based search
for spoken utterance retrieval. In Proc. HLT-NAACL.
F. Seide, P. Yu, C. Ma, and E. Chang. 2004. Vocabulary-
independent search in spontaneous speech. In Proc. of
ICASSP.
O. Siohan and M. Bacchiani. 2005. Fast vocabulary in-
dependent audio search using path based graph index-
ing. In Proc. of Interspeech.
280
