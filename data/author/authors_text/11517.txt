Proceedings of the 12th European Workshop on Natural Language Generation, pages 98?101,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
A Hearer-oriented Evaluation of Referring Expression Generation ?
Imtiaz H. Khan, Kees van Deemter, Graeme Ritchie, Albert Gatt, Alexandra A. Cleland
University of Aberdeen, Aberdeen, Scotland, United Kingdom
{i.h.khan,k.vdeemter,g.ritchie,a.gatt,a.cleland}@abdn.ac.uk
Abstract
This paper discusses the evaluation of a
Generation of Referring Expressions algo-
rithm that takes structural ambiguity into
account. We describe an ongoing study
with human readers.
1 Introduction
In recent years, the NLG community has seen a
substantial number of studies to evaluate Gener-
ation of Referring Expressions (GRE) algorithms,
but it is still far from clear what would constitute
an optimal evaluation method. Two limitations
stand out in the bulk of existing work. Firstly,
most existing evaluations are essentially speaker-
oriented, focussing on the degree of ?human-
likeness? of the generated descriptions, disre-
garding their effectiveness (e.g. Mellish and Dale
(1998), Gupta and Stent (2005), van Deemter et al
(2006), Belz and Kilgarriff (2006), Belz and Re-
iter (2006), Paris et al (2006), Viethen and Dale
(2006), Gatt and Belz (2008)). The limited num-
ber of exceptions to this rule indicate that the dif-
ferences between the two approaches to evaluation
can be substantial (Gatt and Belz, 2008). Sec-
ondly, most evaluations have focussed on the se-
mantic content of the generated descriptions, as
produced by the Content Determination stage of
a GRE algorithm; this means that linguistic re-
alisation (i.e. the choice of words and linguistic
constructions) is usually not addressed (exceptions
are: Stone and Webber (1998), Krahmer and The-
une (2002), Siddharthan and Copestake (2004)).
Our aim is to build GRE algorithms that produce
referring expressions that are of optimal benefit to
a hearer. That is, we are interested in generating
descriptions that are easy to read and understand.
But the readability and intelligibility of a descrip-
tion can crucially depend on the way in which it is
? This work is supported by a University of Aberdeen
Sixth Century Studentship, and EPSRC grant EP/E011764/1.
worded. This happens particularly when there is
potential for misunderstanding, as can happen in
the case of attachment and scope ambiguities.
Suppose, for example, one wants to make it
clear that all radical students and all radical teach-
ers are in agreement with a certain idea. It might
be risky to express this as ?the radical students and
teachers are agreed?, since the reader1 might be
inclined to interpret this as pertaining to all teach-
ers rather than only the radical ones. For this rea-
son, a GRE program might opt for the longer noun
phrase ?the radical students and the radical teach-
ers?. But because this expression is lengthier, the
choice involves a compromise between compre-
hensibiliity and brevity, a special case of a diffi-
cult trade-off that is typical of generation as well
as interpretation of language (van Deemter, 2004).
We previously reported the design of an algo-
rithm (based on an earlier work on expressions re-
ferring to sets (Gatt, 2007)), which was derived
from experiments in which readers were asked to
express their preference between different descrip-
tions and to respond to instructions which used a
variety of phrasings (Khan et al, 2008). Here we
discuss the issues that arise when such an algo-
rithm is evaluated in terms of its benefits for read-
ers.
2 Summary of the algorithm
In order to study specific data, we have focussed
on the construction illustrated in Section 1 above:
potentially ambiguous Noun Phrases of the gen-
eral form the Adj Nouni and Nounj . For such
phrases, there are potentially two interpretations:
wide scope (Adj modifies both Nouni and Nounj)
or narrow scope (Adj modifies Nouni but not
Nounj).
Our algorithm starts from an unambiguous set-
theoretic formula over lexical items (i.e. words
1In this paper, we use the word reader and hearer inter-
changeably.
98
have already been chosen), and thus has to choose
between a number of different realisations. The
possible phrasings for the wide scope meaning are:
(1) the Adj Noun1 and Noun2, (2) the Adj Noun2
and Noun1, (3) the Adj Noun1 and the Adj Noun2,
and (4) the Adj Noun2 and the Adj Noun1. For nar-
row scope, the possibilities are: (1) the Adj Noun1
and Noun2, (2) the Noun2 and Adj Noun1, (3) the
Adj Noun1 and the Noun2, and (4) the Noun2 and
the Adj Noun1. For our purposes, (1) and (2) are
designated as ?brief?, (3) and (4) as ?non-brief?
(that is, ?brevity? has a specialised sense involv-
ing the presence/absence of ?the? and possibly Adj
before the second Noun). Importantly, the ?non-
brief? expressions are syntactically unambiguous,
but the ?brief? NPs are potentially ambiguous, and
hence are the focus of attention in this work.
Our algorithm is based on certain specific hy-
potheses (from the earlier experiments) which
make crucial use of corpus data concerning the
frequency of two types of collocations: the col-
location between an adjective and a noun, and the
collocation between two nouns. At a broader level,
we hypothesise: the most likely reading of an NP
can be predicted using corpus data (Word Sketches
(Kilgarriff, 2003)). The more specific hypotheses
derive from earlier work by Kilgarriff (2003) and
Chantree et al (2006), and were further developed
and tested in our previous experiments. The cen-
tral idea is that this statistical information can be
used to predict a ?most likely? scoping (and hence
interpretation) for the adjective in the ?brief? (i.e.
potentially ambiguous) NPs. We define an NP to
be predictable if our model predicts a single read-
ing for it; otherwise it is unpredictable. Hence, all
?non-brief? NPs are predictable (being unambigu-
ous), but only some of the ?brief? ones are pre-
dictable.
In a nutshell, the model underlying our algo-
rithm prefers predictable expressions to unpre-
dictable ones, but if several of the expressions are
predictable then brief expressions are preferred
over non-brief.
3 Aims of the study
We want to find out whether our generator
makes the best possible choices (for hearers) from
amongst the different ways in which a given de-
scription can be realised. But although our al-
gorithm uses sophisticated strategies for avoiding
noun phrases that it believes to be liable to mis-
understanding, misunderstandings cannot be ruled
out, and if a hearer misunderstands a noun phrase
then secondary aspects such as reading (and/or
comprehension) speed are of little consequence.
We therefore plan first to find out the likelihood of
misunderstanding. For this reason, we will report
on the degree of accuracy, as a percentage of times
that a participant?s understanding of an expression
that we label as predictable fails to match the in-
terpretation assigned by our model. Additionally,
we shall statistically test two hypotheses:
Comprehension Accuracy 1: Predictable ex-
pressions are more often interpreted in
agreement than in disagreement with the
model.
Comprehension Accuracy 2: There is more
agreement among participants on the inter-
pretation of predictable expressions than of
unpredictable expressions.
We will not only test the comprehensibility of the
expressions generated by our algorithm, but their
readability and intelligibility as well. This is nec-
essary because the experiments which led to the
algorithm design considered only certain aspects
of the hearer?s reaction to NPs (e.g. metalinguistic
judgements about a participant?s preferences) and
we wish to check these comprehensibility/brevity
facets from a different, perhaps psycholinguisti-
cally more valid, perspective. It is also necessary
because avoidance of misunderstandings is not the
only decisive factor: if several of the expressions
are predictable then our algorithm chooses be-
tween them by preferring brevity. But why is brief
better than non-brief? Taking readability and intel-
ligibility together as ?processing speed?, our third
hypothesis is:
Processing speed: Subjects process
predictable brief expressions more
quickly than predictable non-brief ones.
Confirmation of this hypothesis would be a strong
indication that our algorithm is on the right track,
particularly if the degree of accuracy (see above)
turns out to be high. Processing speed is a com-
plex concept, but we could decompose it as ?read-
ing speed? and ?comprehension speed?, permitting
us to examine reading and comprehension sepa-
rately. We intend to see what evidence there is for
the following additional propositions, which will
be tested solely to aid our understanding.
99
Reading Speed:
RS1: Subjects read predictable brief NPs more
quickly than unpredictable brief ones.
RS2: Subjects read unpredictable brief NPs more
quickly than predictable non-brief ones.
RS3: Subjects read predictable brief NPs more
quickly than predictable non-brief ones.
Comprehension Speed:
CS1: Subjects comprehend predictable brief NPs
more quickly than unpredictable brief ones.
CS2: Subjects comprehend predictable non-brief
NPs more quickly than unpredictable brief ones.
CS3: Subjects do not comprehend predictable
non-brief NPs more quickly than predictable brief
ones.
(Remember that, in our restricted set of NPs, a
phrase cannot be both ?unpredictable? and ?non-
brief?.) Rejection of any of these statements will
not count against our algorithm.
4 Sketch of experimental procedure
Participants will be presented with a sequence of
trials (on a computer screen), each of which con-
sists of a lead-in sentence followed by a target sen-
tence and a comprehension question that relates to
the two sentences together. The target sentence
might for example say ?the radical students and
teachers were waving their hands?. The compre-
hension question in this case could be ?Were the
moderate teachers waving their hands??. As both
the target sentence and the comprehension ques-
tion make use of definite NPs (e.g. ?the moderate
teachers?), it is necessary to ensure any presuppo-
sitions about the existence of the referent set are
met, without biasing the answer. For this reason,
the target sentence is preceded by a lead-in sen-
tence to establish the existence of the sets within
the discourse (here, ?there were radical and mod-
erate people in a rally?).
Given this set-up we are confident that we
can identify, from a participant?s yes/no answer,
whether the NP in the target sentence was assigned
a narrow-scope or a wide-scope reading for the ad-
jective. The computer will record the participant?s
response as well as the length of time that the par-
ticipant took to answer the question. We will use
Linger2 for presentation of stimuli. Pilots sug-
gest that the complexity of the trials makes it ad-
visable to use masked sentence-based self-paced
2http://tedlab.mit.edu/?dr/Linger/
reading, in which every press of the space bar re-
veals the next sentence and the previous sentence
is replaced by dashes.
The choice of nouns and adjectives (to construct
NPs) is motivated by the fact that there is a bal-
anced distribution of NPs in each of the follow-
ing three classes. Wide scope class is the one for
which our model predicts a wide-scope reading;
narrow scope class is the one for which our model
predicts a narrow-scope reading; and ambiguous
class is the one for which our model fails to pre-
dict a single reading (Khan et al, 2008).
5 Issues emerging from this study
The design of this experiment raised some difficult
questions, some quite unexpected:
1. The quality of the output of a generation al-
gorithm might appear to be a simple and well-
understood concept. However, output quality is
multi-faceted, because an expression may be easy
to read but difficult to process semantically, or the
other way round. A thorough output evaluation
should address both aspects of quality, in our view.
2. If both reading and understanding are ad-
dressed, this raises the question of how these
two dimensions should be traded off against each
other. If one algorithm?s output was read more
quickly than that of another, but understood more
slowly than the second, which of the two should be
preferred? Perhaps there is a legitimate role here
for metalinguistic judgments after all, in which
participants are asked to express their preference
between expressions (see Paraboni et al (2006) for
discussion)? An alternative point of view is that
these questions are impossible to answer indepen-
dent of a realistic setting in which participants ut-
ter sentences with a concrete communicative pur-
pose in mind. If utterances were made in order to
accomplish a concrete task (e.g., to win a game)
then task-based evaluation would be possible.
3. Even though this paper has not focussed on de-
tails of experimental design and analysis, one diffi-
culty is worth mentioning: given the grammatical
options between which the generator is choosing,
only three types of situations are represented: a de-
scription can be brief and predictable (e.g. using
?the old men and women? to convey wide scope,
since the adjective is predicted by our algorithm
to have wide scope), brief and unpredictable (e.g.
?the rowing boats and ships? for wide scope, given
100
a prediction of narrow scope), or non-brief and
predictable (e.g. ?the old men and the old women?
for wide scope). It might appear that there exists
a fourth option: non-brief and unpredictable. But
this is ruled out by our technical sense of ?non-
brief?: as noted earlier, ?non-brief? NPs do not
have the scope ambiguity. Because of this ?miss-
ing cell?, it will not be possible to analyse our data
using an ANOVA test, which would have automat-
ically taken care of all possible interactions be-
tween comprehensibility and brevity. A number
of different tests will be used instead, with Bon-
ferroni corrections where necessary.
6 Conclusion
Human-based evaluation is gaining considerable
popularity in the NLG community. Whereas eval-
uation of GRE has mostly been speaker-oriented,
the present paper has explored a plan for an ex-
perimental hearer-oriented evaluation. The main
conclusion is that hearer-based evaluation is diffi-
cult because the quality of a generated expression
can be measured in different ways, whose results
cannot be assumed to match. One factor we have
not examined is the notion of fluency: it is possible
that our algorithm will sometimes choose a word
order (e.g. ?the women and old men?) that is rela-
tively infrequent, and therefore lacking in fluency.
Such situations might lead to longer reading times.
References
A. Belz and A. Kilgarriff. 2006. Shared-task evalu-
ations in HLT: Lessons for NLG. In Proceedings
of the 4th International Conference on Natural Lan-
guage Generation, pages 133?135.
A. Belz and E. Reiter. 2006. Comparing automatic
and human evaluation of NLG systems. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 313?320, Trento, Italy, 3-7 April.
F. Chantree, B. Nuseibeh, A. de Roeck, and A. Willis.
2006. Identifying nocuous ambiguities in require-
ments specifications. In Proceedings of 14th IEEE
International Requirements Engineering conference
(RE?06), Minneapolis/St. Paul, Minnesota, U.S.A.
A. Gatt and A. Belz. 2008. Attribute selection for re-
ferring expression generation: New algorithms and
evaluation methods. In Proceedings of the 5th Inter-
national Conference on NLG.
A. Gatt. 2007. Generating Coherent References to
Multiple Entities. Ph.D. thesis, University of Ab-
erdeen, Aberdeen, Scotland.
S. Gupta and A. Stent. 2005. Automatic evaluation
of referring expression generation using corpora. In
Proceedings of the Workshop on Using Corpora for
Natural Language Generation, pages 1?6.
I. H. Khan, K. van Deemter, and G. Ritchie. 2008.
Generation of referring expressions: Managing
structural ambiguities. In Proceedings of the 22nd
International Conference on Computational Lin-
guistics (COLING-8), pages 433?440, Manchester.
A. Kilgarriff. 2003. Thesauruses for natural language
processing. In Proceedings of NLP-KE, pages 5?13,
Beijing, China.
E. Krahmer and M. Theune. 2002. Efficient context-
sensitive generation of referring expressions. In
K. van Deemter and R. Kibble, editors, Information
Sharing: Reference and Presupposition in Language
Generation and Interpretation, CSLI Publications,
pages 223?264.
C. Mellish and R. Dale. 1998. Evaluation in the
context of natural language generation. Computer
Speech and Language, 12(4):349?373.
I. Paraboni, J. Masthoff, and K. van Deemter. 2006.
Overspecified reference in hierarchical domain:
measuring the benefits for readers. In Proceedings
of the Fourth International Conference on Natural
Language Generation(INLG), pages 55?62.
C. Paris, N. Colineau, and R. Wilkinson. 2006. Eval-
uations of NLG systems: Common corpus and tasks
or common dimensions and metrics? In Proceed-
ings of the 4th International Conference on Natural
Language Generation, pages 127?129.
A. Siddharthan and A. Copestake. 2004. Generating
referring expressions in open domains. In Proceed-
ings of the 42nd Meeting of the Association for Com-
putational Linguistics Annual Conference (ACL-04).
M. Stone and B. Webber. 1998. Textual economy
through close coupling of syntax and semantics. In
Proceedings of the Ninth International Workshop on
Natural Language Generation, pages 178?187, New
Brunswick, New Jersey.
K. van Deemter, I. van der Sluis, and A. Gatt. 2006.
Building a semantically transparent corpus for the
generation of referring expressions. In Proceedings
of the 4th International Conference on Natural Lan-
guage Generation, pages 130?132.
K. van Deemter. 2004. Towards a probabilistic version
of bidirectional OT syntax and semantics. Journal
of Semantics, 21(3):251?281.
J. Viethen and R. Dale. 2006. Towards the evaluation
of referring expression generation. In Proceedings
of the 4th Australasian Language Technology Work-
shop, pages 115?122, Sydney, Australia.
101
