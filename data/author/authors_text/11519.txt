Proceedings of the EACL 2009 Workshop on Cognitive Aspects of Computational Language Acquisition, pages 42?50,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Darwinised Data-Oriented Parsing ? Statistical NLP with added Sex and Death Dave Cochran Cognitive Systems Group, School of Computer Science,  University of St. Andrews davec@cs.st-andrews.ac.uk   Abstract We present the Darwinised Data-Oriented Parsing algorithm, an incre-mental, dy-namic form of Data-Oriented Parsing, in which exemplars are used as replicators, subject to a selection pressure towards gen-eralisability.1 1 Introduction Data-Oriented Parsing (DOP) is a state-of-the art approach to both supervised and unsupervised parsing (Bod1992, 1998, 2006a, 2006b, 2007a, 2007b, Zollman and Sima?an 2005), which has mostly been developed within a technologically-oriented computer science context. Recent work has highlighted some interesting cognitive prop-erties of the Data-Oriented approach (Borensztajn, Zuidema & Bod 2008, Bod 2008). However, these studies have mostly focused on the static properties of the DOP probability model. Here, we present the first attempt at a dynamic, incremental Data-Oriented model which can address the time course of language learning, rather than just the outcome; Dar-winised DOP. 2 Data-Oriented Parsing 2.1 Supervised DOP Data-Oriented Parsing (DOP) is a paradigm in Natural Language Processing in which linguistic knowledge is represented as fragmentable, re-combinable exemplars of concrete previous ex-perience, usually in the form of trees. What cru-cially distinguishes DOP from other approaches is the fact that fragments of arbitrary size are                                                  1 The author thanks Rens Bod, Mark-Jan Nederhof and three anonymous reviewers for exceedingly helpful com-ments, sug-gestions and references. 
used, ranging, in the case of the usual tree-structures, from depth-1 context-free rewrite rules to entire trees, and all points in between; this gives it the power to pick up on whatever statistical patterns are present in the data, to a considerable extent bypassing of the researcher?s theoretical prejudices. Moreover, it allows these regularities to be exploited without being repre-sented. DOP was first proposed by Scha (1990), and implemented and developed by Bod (1992, 1998). The simplest manifestation of DOP is DOP1, as described in Bod (1998 p12-23 and 40-50), though more sophisticated versions exist. The parser uses a large corpus of natural language strings annotated with labeled, ordered tree-structures, divided into a training corpus and a smaller corpus against which the parser is tested. The parser uses every possible subtree (of unlim-ited depth) of all the available trees, constrained only by the following wellformedness criteria (Fig. 1).  ? Every subtree must be of at least depth 1. ? Sister relationships must be preserved: that is, either all or none of the daughters of a given node may be extracted, but not only some. 
 Figure 1: Well-formed subtrees of a parse of ?John likes Mary? 
42
 Fig 2: A sequence of substitutions comprising a derivation of ?John likes Mary?. ?is the opera-tor for leftmost non-terminal leaf-node substitu-tion. The parser is given test corpus strings and builds up new parse-trees for these using the fragments available to it from the training corpus (Fig. 2), starting with a fragment with an S-node at the top, and then, for each nonterminal leaf-node, working rightwards, substituting in addi-tional subtrees, the topmost node of which must carry the same label as the node to be substituted. (see figure 2.2). In DOP research it is necessary to distinguish between parses and derivations. A parse is the tree structure expressed over a string; a deriva-tion is the particular sequence of subtree substi-tutions by which it was constructed. When pars-ing with probabilistic context-free grammars (PCFG?s, see Manning and Sch?tze1999, pp.381-405; note that a PCFG is equivalent to a DOP grammar in which subtree depth has been restricted to 1), there is a one-to-one mapping between parses and derivations, because all non-terminal nodes (nodes which have daughters in the completed parse) must be substitution sites. In DOP, subtrees can be of any depth, and so in any given derivation, any subset of the non-terminal nodes could have been substitution sites, while the remainder will not have been. As such, if a parse contains N many non-terminal nodes, it will have 2N many derivations.  For each subtree t, its probability P(t) is its to-tal frequency |t| of occurrence in the training cor-pus over the summed corpus frequency of sub-trees with the same root node;2   (1) ?where r(t)and r(t') are the node-labels on the root-nodes of subtreest and t'.                                                  2 Note that although, beside the node-label on the substitu-tion site, the input to be parsed is also a constraint on the selection of subtrees for substitutions, these constraints are not factored in to the calculation of probabilities. That is to say, the probability used is that of the subtree, not the sub-stitution. 
The probability of a derivation is the product of the probabilities of its subtrees (note that ?is the notation for the substitution operation; thus t1???tn is the sequence of substitutions, which together comprise the derivation);   (2) And the probability of a parse T is the sum of the probabilities of its possible derivations D;   (3)  The output of the parser is, in theory, the most probable parse. In practice, there are issues of computational complexity that prevent this from being calculated directly; instead, a Monte-Carlo sample is taken. Furthermore, although the num-ber of subtrees for any given tree increases exponentially with tree-size, it is possible to reduce DOP trees to a stochastically equivalent PCFG which expands linearly with tree-size; for details see Goodman 1996, 2003) Bod (ibid p.54) reports accuracies of 85% on the ATIS3 corpus for DOP1. However, better results are achieved with later, more sophisti-cated versions of DOP ? the current state of the art is DOP* (Zollman and Sima?an 2005), which selects parses on the basis of the shortest deriva-tion, and only uses probabilities to tie-break if there is more than one shortest derivation; this approach overcomes the problems of statistical consistency and bias which Johnson (1998) pointed out as afflicting DOP1. 2.2 Unsupervised DOP Unsupervised DOP (UDOP, Bod 2006b, 2007a, 2007b) extends the DOP approach to bootstrap language without recourse to a training corpus of manually annotated language data as a representation of ?prior experience?. UDOP ex-pands on DOP?s maximalist, all-subtrees, ap-proach by using all subtrees of all possible (bi-nary) trees to provide the parser with a resource of subtrees for the construction of derivations (which may be stored within a chart in quadratic space: Bod 1998, pp.40-8). Not only is this shown to achieve state-of-the art results com-pared to other unsupervised parsing methods (Bod2006b, 2007a), it is also shown to outper-form state-of-the-art supervised parsing tech-niques when evaluated as a language model for a practical application (Machine Translation), rather than using the rather academical and the-                                                 3 Air Transport Information System ? part of the Penn Tree-bank. 
43
ory-laden standard of agreement with the judge-ments of manual annotations on a corpus (Bod 2007b). In the UDOP* implementation of UDOP, all subtrees of all possible trees are extracted from the training corpus, and from this exemplar-base, the shortest derivation (in the fashion of DOP*) for each string is calculated (again, the complex-ity of the task may be reined in using Goodman?s PCFG-reduction, 1996, 2003). The set of trees that results from this is then converted again to subtrees and used as a Stochastic Tree Substitu-tion Grammar, which is then used to parse the test strings. In UML-DOP, another implementa-tion, this last step is iterated over the training data until there is negligible reduction in cross-entropy.  Bod (2007a) shows that these methods applied to miniature ?toy? corpora can be used to explain linguistically interesting phenomena, such as long-distance agreement and ?move-ment?, as emerging out of simpler structures without themselves being found in either experi-ence or ?hardwired? grammar. However, the batch learning methods noted above are cogni-tively implausible. This is not a criticism of the approach in itself; Batterman (2005) shows that idealisations, however unrealistic in themselves, are necessary in scientific modeling in order to explain universalities which more ?realistic? models would miss. However, UDOP cannot itself model the time-course of developmental learning processes, which are by nature incre-mental. It is to make good this deficit that we have developed the Darwinised DOP (DDOP) approach. 2.3 Darwinised DOP Darwinised Data-Oriented Parsing is a new unsupervised parsing algorithm which allows the time-course of pattern-learning to be modeled. Unlike previous DOP algorithms, it begins with a completely empty training set; it is fed strings one by one, and its own outputs are entered into the training set. In so doing, it exploits a hitherto underexploited property of exemplar-based sys-tems; when an exemplar (subtree) is reused in producing the system?s eventual output, this out-put contains a new copy of the reused exemplar which is inserted back into the exemplar-base upon which the algorithm operates; thus, exem-plars become replicators; packets of information coupled to mechanisms by which new copies of themselves are generated.Furthermore, exem-plars which are able to be used more often ? those that are more highly generalisable - are 
likely to make more new copies of themselves; thus we find a selection pressure favouring gen-eralisability.4It is also worth nothing that because the subsequent sampling of subtrees from a stored tree can and most likely will cross-cut the substitution-sites of the original derivations by which the stored tree was created, replication of subtrees is recombinant; exemplars do not just reproduce, they reproduce sexually.  Trees have a limited lifespan, and are erased from memory after K many parses have been generated following their creation. This serves two functions; firstly, if the dataset is small, the training data may be iterated through several times, but because DDOP uses the DOP* short-est derivation method, it must be prevented from seeing strings for which it already has a complete parse in memory, or else it will simply return the parse it gave before, which of course can be gen-erated in a single-step derivation. Secondly, death is an essential component of evolving sys-tems. Without death, maladaptive and primitive forms are allowed to remain in the system, still reproducing, albeit at a slower rate than newer, more highly evolved replicators. As mentioned above, DDOP uses DOP*?s shortest-derivation method of parsing, but be-cause it begins with an empty exemplar-base, it needs to have a backoff behaviour, in case it en-counters a situation, at some point in a deriva-tion, where no subtree can be found in the exem-plar-base which would allow the derivation to continue. When a backoff subtree is generated, the following information only is used: ? The node-label l of the substitution sitet. This does not affect the probability model, but determines the node-label of the root of the new subtree. In fact, this feature is redundant in all the versions of the model tested so far, as the number of available node labels has been limited to one. ? The substring wx ? wx?, of the total string being parsed, which represents the largest possible substring capable of eventually 
                                                 4 This notion of exemplars as evolving replicators is not without precedent; see Batali 1994; however, Batali?s model concerned the evolution of a shared linguistic code in a population of agents, rather than exploiting this property for the individual learning of a preexisting language. We are also indebted to Kirby?s (1999, for instance) insight that evolving linguistic systems will favour generalisability, though again Kirby?s models concern evolution over glos-sogenetic, generational time-scales. 
44
being daughters of the node at the substi-tution site. We have tested two versions of DDOP, Non-Folding DDOP (NF-DDOP) and Folding DDOP (F-DDOP), each of which use different backoff procedures: RANDOM and FLAT, respectively. The other difference between the two versions of DDOP concerns the interpretation of flat struc-tures in parses (here taken to mean context-free rewrite rules with an arity of three or greater) ? internal nodes with three or more immediate daughters. In NF-DDOP, the RANDOM backoff procedure generates subtrees as a random sample of the complete set of all subtrees of all possible parses of substring wi ? wi? with root node t. A description of how this is calculated can be found at http://www.cs.st-andrews.ac.uk/backoff.pdf. No distinction is made between flat and deep structures. F-DDOP, by contrast, takes flat structures to be an indica-tor uncertaintyregarding actual, lower-arity struc-ture, and therefore as a shorthand for a set of possible lower-arity structures, or ?foldings?, wherein the high-arity context free rewrite rules (subtree of depth one) is replaced with a subtree of equal or greater depth, with the same root and frontier. In order to reduce computational load, this set of allowable foldings is limited to sub-trees of depth one or two, in the case of the depth two foldings further limited to foldings contain-ing no more than one internal node5. By way of example, figure 3 shows all the allowable fold-ings of a 4-ary subtree. Derivations proceed one substitution at a time. DOP* subtrees extracted from the training data are always used provided there exists at least one which fits the string being parsed. If at any point no such subtree can be found, then a backoff subtree is generated at that step only. A Monte Carlo sample of N derivations is generated (in the simulations reported here, N = 1500). In or-der to introduce an element of mutation to the process (crucial in any evolving system), a single derivation consisting only of backoff subtrees is occasionally added to the sample, at a probability given by p(AllBackoff), which is a parameter set before the run commences, where 0 ? p(AllBackoff) ? 1. Following the procedure of                                                  5 Note that although this limitation excludes possible parses which may be linguistically pertinent from the immediate folding event (in the case of structures of arity greater than 3), any flat structures which remain can, when re-used, be folded again, allowing the entire space of possible folding to be explored eventually. 
DOP*, the shortest derivation in the sample is selected as the output parse. If there is more than one parse with the shortest derivation, DOP1 probabilities are applied to tie-break, following equations 1-3 above. The chosen parse is added to the exemplar-base. Note that because, if an all-backoff derivation is included in the sample,  it is assessed for derivation-length and probability just like all the rest, this mutation procedure can only actually introduce novel structures if the random derivation is shorter than all the others in the sample, because only then does it avoid the probability-based tiebreak, which precisely pe-nalizes novelty and favours well-known struc-tures. Initially, because the exemplar-base is empty, the backoff behaviour is the only behaviour, which, over time, gives way to mostly using cor-pus trees. Importantly, whenever the outputted tree is derived from subtrees taken from mem-ory, the output tree contains new copies of all those subtrees. In this way, subtrees replicate and more generalisable subtrees are selected. 3 Tests & Results 3.1 Test 1: Six-line toy corpus The first test each of the versions of DDOP were subjected to was a very simple toy corpus, con-sisting of six three-word sentences; Corpus Oracle The dog barks. Watch the dog. The dog eats. The cat barks. Watch the cat. The cat eats. 
[The dog] barks. Watch [the dog.] [The dog] eats. [The cat] barks. Watch [the cat.] [The cat] eats. Table 1: six-line corpus 
a)  b)  c)  
d)  e)  f)  Figure 3: 3(a-f) show the allowable foldings of the 4-ary subtree in 3(a). 
45
This initial task was simply to recognise that ?the cat? and ?the dog? are constituents. The ad-vantage of initially testing the models on this very simple toy corpus is that it is very easy to analyse what the model is doing. The toy corpus contains two sentence types, V-NP and NP-V (V and N for short), each of which may be parse as left-branching, flat or right branching (L, F and R, respectively). After the first three parses on its run, the parser always has parses of n-1 of the n sentences in memory; thus each step in an itera-tion through the data, each stored parse exempli-fies one of the 3 possible parse-structures, giving 3n-1 possible memory-states for each step, and n(3n-1) possible memory states overall. Of the memory-states possible at any step, nine repre-sent a consistent assignment of parse-structures (L/F/R) to sentence-types (V/N), and of these only VR-NL represents a successful outcome. Inconsistent states are never stable; in the ab-sence of mutation they are inaccessible and if accessed as a result of mutation the parser will return to a consistent state within one iteration. Barring multiple mutations it is possible to calcu-late which consistent state that will be. We call the n states of a consistent assignment (one state for each step), plus all the inconsistent states that lead predictably into it, a ?territory? within the state-space. If the states of a consistent assign-ment always predictably lead into states in the same territory at the next step, the territory will form a cyclical trajectory through state-space, and will be stable barring a disruptive mutation; if they do not, they will lead into the territory of another consistent assignment. A mutation while the parser is on a cyclical trajectory will have one of the following outcomes: ? Parser moves into an inconsistent state of the same territory, returns to cycle. ? Parser moves into an inconsistent state of another territory, changes cycle. For NF-DDOP, the model was first probed us-ing a state-space model of all 108 of the possible memory-states of the parser based on a four-line version of the above corpus, minus the lines ?the dog eats? and ?the cat eats?. This analysis found that eight of the nine consistent assignments are stable cycles, withVR-NR being the only excep-tion. VR-NR states always run off to the ?true? VR-NL assignment. This means that a mutation in the VR-NL cycle has a greater likelihood of being non-disruptive (returning to VR-NL) than a mutation in any other cycle, and a disruptive 
mutation in another cycle is more likely to result in a shift to VR-NL than to any other cycle. However, with seven other cyclical territories to compete with, the parser still spends the majority of its time in states other than VR-NL; VR-NL is more robust than the other cyclical modes, but no modes are wholly robust. This finding was first calculated a priori based on the state-space model, then confirmed empirically. Tests with the full 6-line corpus found further destabiliza-tionof VR-NL (Chart 1 below). 
 Chart 1: Performance of NF-DDOP on the six-line test corpus 
 Chart 2: F1-Scores for F-DDOP on the six-line mini-corpus 
 Chart 3: F1 Scores for F-DDOP on the six-line mini-corpus using the RANDOM backoff routine. Each data point averages over the 20 parses, sampled at 10-parse intervals. In contrast, for F-DDOP, VR-NL is the only accessible territory, and all otherconsistent as-signments run off to it, with the result that VR-NL is completely robust, and the parser?s per-formance on the six-line toy corpus holds fast to F1-Scores of 100%, as shown in Chart 2 below. 
0?
20?
40?
60?
80?
100?
120?
1?
63
3?
12
65
?
18
97
?
25
29
?
31
61
?
37
93
?
44
25
?
50
57
?
56
89
?
63
21
?
69
53
?
75
85
?
82
17
?
88
49
?
94
81
?
10
11
3?
10
74
5?
11
37
7?
Precision? Recall? F1?Score?
0 
20 
40 
60 
80 
100 
120 
20 
5440 
10860 
16280 
21700 
27120 
32540 
37960 
43380 
48800 
54220 
F1
-S
co
re
 
0?
50?
100?
150?
1?
30
1?
60
1?
90
1?
12
01
?
15
01
?
18
01
?
21
01
?
24
01
?
27
01
?
30
01
?
33
01
?
36
01
?
39
01
?
42
01
?
45
01
?
48
01
?
51
01
?
54
01
?
57
01
?
46
This is because any state containing flat parse-structures runs off to VR-NL, and states contain-
ing VL and/or NR are inaccessible, because the FLATback-off always yields VF and NF. This is made clearer by comparison with a hybrid DDOP, in which F-DDOP is combined with the RANDOM back-off routine. Here we find that VL-NL is accessible and cyclical, reducing the robustness of VR-NL. 3.2 Test 2: 20-line toy corpus The second round of tests used a slightly larger and more complex corpus ? this time of 20 sen-tences, varying between 3 and 11 words in length. Again, notable differences were found between NF-DDOP and F-DDOP. Charts 4 and 5 show the performance of the model on this cor-pus. In both cases, the performance of the model was not especially great, with only NF-DDOP showing an overall trend towards improvement over the course of the run; however, the differ-ence in mean F1-score is small; 41.6 for NF-DDOP compared to 37.7 for F-DDOP. Most puzzling of all was the wild instability of F-
DDOP on this test, especially when contrasted with its robustness in the simpler test. Chart 6 shows a histogram of the changes in F1-score between adjacent datapoints in the preceding two tests. Note that the values for F-DDOP are rather less sharply peaked and have rather fat-ter tails. Clearly, further research is required to make sense of this. More interestingly, a second test was done on F-DDOP in which the model was subject to limitations on attention, which were loosened as time went on: specifically, for the first 700 iterations through the data, it ignored all but the 3-word sentences, then continued in 700-iteration blocks, each with an attention span one word greater than the previous, until it could see the whole corpus. Not only was the overall per-formance much better, it also showed much greater stability, as can be seen in Chart 7above 4 Discussion UDOP has the unrestricted ability to sample the entire range of possible subtrees of possible parses of the entirety of whatever dataset it is presented with, and has a limitless scope to re-visit and reconsider past judgements on the basis of new information. As such, we would not ex-pect DDOP to exceed it in power; having access only to a limited subset of the UDOP subtree-set, and strictly limited powers to revisit and reana-lyze past judgements, it is fair to assume that at best the performance of UDOP represents a theo-retical upper limit of the power of DDOP. As 
 Chart 4: NF-DDOP performance on the 20-line toy corpus. 
 Chart 5: F-DDOP performance on the 20-line toy corpus  
0?
20?
40?
60?
80?
100?
1?
18
6?
37
1?
55
6?
74
1?
92
6?
11
11
?
12
96
?
14
81
?
16
66
?
18
51
?
20
36
?
22
21
?
24
06
?
25
91
?
27
76
?
29
61
?
31
46
?
33
31
?
35
16
?
37
01
?
Precision? Recall?
F1?Score? Linear(Precision)?
Linear(Recall)? Linear(F1?Score)?
0?
20?
40?
60?
80?
100?
1?
41
2?
82
3?
12
34
?
16
45
?
20
56
?
24
67
?
28
78
?
32
89
?
37
00
?
41
11
?
45
22
?
49
33
?
53
44
?
57
55
?
61
66
?
65
77
?
69
88
?
73
99
?
Precission? Recall?
F1?Score? Linear(Precission)?
Linear(Recall)? Linear(F1?Score)?
 Chart 6: frequency distribution of differences be-tween F1-score at adjacent datapoints in the tests of NF-DDOP and F-DDOP on the 20-line toy corpus. 
 Chart 7: F-DDOP performance with gradually in-creasing attention span  
0%?
10%?
20%?
30%?
40%?
?22????20?
?20?????18?
?18????16?
?16????14?
?14????12?
?12????10?
?10????8?
?8????6?
?6????4?
?4????2?
?2???0?
0???2?
2???4?
4???6?
6???8?
8???10?
10???12?
12???14?
14???16?
16???18?
18???20?
20???22?
NF?DDOP? F?DDOP?
0 
20 
40 
60 
80 
100 
120 
1 
36
7 
73
3 
10
99
 
14
65
 
18
31
 
21
97
 
25
63
 
29
29
 
32
95
 
36
61
 
40
27
 
43
93
 
47
59
 
51
25
 
54
91
 
58
57
 
62
23
 
65
89
 
69
55
 
73
21
 
76
87
 
80
53
 
Precission Recall 
F1 Score Increase Attention 
47
such, the questions we must attend to are as fol-lows; how far and in what way can we limit the sampling and reanalyzing power of unsupervised DOP systems, and still approach the convergence properties of UDOP in the limit of learning? And if this is possible, can the restrictions adduced be illuminatingly related to empirical work in De-velopmental and Evolutionary Linguistics? DDOP in its original form underperformed because it was an unsystematic search through too large a parameter space for too small an error minimum. The population of exemplars is not best conceptualized as an assembly of individu-als in a species, who, in competing with their conspecifics, enhance the genetic health of the whole; rather, they are best seen as an assembly of species (though, like bacteria, promiscuously engaged in lateral gene transfer), competing for space in a shifting landscape of ecological niches defined by statistical patterns in the language data. From the viewpoint of an individual exem-plar, the ecology of this landscape is as much defined by its competitors and potential mates as by the language-data itself. The problem is the selfishness of replicators described by Dawkins (1976, Williams 1966) in the Selfish Gene. Evo-lution is a dumb, blind process, and selection concerns only the individual replicator in its im-mediate fitness landscape. However two modifications to the original DDOP algorithm, which can both be independ-ently motivated in terms of cognitive realism, have been shown to usefully constrain the avail-ability of niches so as to produce more favour-able results. The first, the introduction of the Folding pro-cedure changes the nature of the algorithm?s ini-tial assumptions regarding unknown structure; rather than assigning an arbitrary structure to everything on first sight, no initial structure is assumed, allowing the learner to pick recurring motifs out of the data as they are spotted. (see Saffran, Aslin and Newport 1996 on the rapidity with which infants can pick out motifs). The ana-logue of this behaviour, in terms of evolutionary biology, is phenotypic plasticity, whereby the genotype of a species provides for multiple pos-sible developmental pathways, to multiple phe-notypes, modulated in selectionally advanta-geous ways by environmental conditions. (West-Eberhard 2003). We may consider the case of higher-arity structures in F-DDOP to be the only instance in DDOP of a separation between geno-type and phenotype; in the case of the ternary structures in the six-line corpus, the underlying 
flat structure is analogous to the genotype, while the flat, left-branching and right-branching sub-trees extracted from it in subsequent derivations may be understood as alternative phenotypes. This sort of adaptive plasticity allows ?popula-tions to move into new adaptive zones without abandoning old ones ? [a]lternative phenotypes enable condition sensitive evolutionary experi-mentation within populations? (West-Eberhard 2003, p.392). These variations in phenotype arise out of an interaction between ?genotype? and environment (DeWitt and Scheiner 2004b) ?understood as both the incoming stream of lan-guage inputs and the accumulated store of other exemplars ? in a manner that responds adaptively to the frequencies of linguistic patterns in the environment. Secondly, we introduced the assumption of an initiallimitation on the length of sentences the parser is exposed to, which expands with matura-tion. It is known that adults? speech to infants tends to be characterized by shorter, simpler ut-terances than normal adult speech (Cameron-Faulkner, Lieven and Tomasello 2003); further-more, children?s speech is characterized by a gradual increase in Mean Length of Utterance (MLU: calculated as morphemes-per-utterance) with maturation (Brown 1973: p270-5), and it has been shown that MLU is a better predictor of comprehension of syntactically complex adult utterances than chronological age (de Villiers and de Villiers, 1973; see also Elman 1993 for further computational work on the payoff of ?starting small?). This not only improved overall performance substantially, but also achieved a much-needed gain in stability. 5 Conclusion We have seen that, by building in additional cognitively realistic assumptions, the overall per-formance of DDOP, both in terms of average parse quality, and diachronic stability is consid-erably enhanced; this in itself should be taken as a prima facie indication of DDOP?s promise as a platform for developmental cognitive modeling. We also note with interest a possible resonance between the current model and neural models of development as evolution at the ontogenetic scale: Edelman?s (1987) ?Neural Darwinism?, or more saliently Fernando, Karishma and Szath-m?ry?s (2008) work on neural-developmental evolution with true replication.Future research in DDOP will investigate the role of the global properties of the exemplar base in determining 
48
the evolutionary dynamicsin relation to which exemplars compete and die, and the success or failure of Data-Oriented models of learning and cognition. References Batali, J. 1994.?Innate biases and critical periods: Combining evolution and learning in the acquisi-tion of syntax?. In R. Brooks and P. Maes, editors, Artificial Life IV. 160-171. Cambridge, MA: MIT Press. Batterman, R., 2005. ?Critical Phenomena and Break-ing Drops: Infinite Idealizations in Physics?, Stud-ies In History and Philosophy of Modern Physics, 36:225-244. Bod, R., 1992. A Computational Model of Language Performance: Data-Oriented Parsing. In Proc. COLING 1992, 855--859. Stroudsburg: Associa-tion for Computational Linguistics.  Bod, R. (1998).Beyond Grammar: An Experience-Based Theory of Language. Stanford: CSLI Publications.  Bod, R., 2006a. Exemplar-Based Syntax: How to Get Productivity from Examples. The Linguistic Re-view23, 291-320. Bod, R., 2006b.An All-Subtrees Approach to Unsu-pervised Parsing.In Proc. ACL 2006, 865-872. Stroudsburg: Association for Computational Lin-guistics.  Bod, R., 2007a. Is the End of Supervised Parsing in Sight? In Proc. ACL 2007, 400-407.Stroudsburg: Association for Computational Linguistics.  Bod, R., 2007b.A Linguistic Investigation into U-DOP. In Proc. Workshop on Cognitive Aspects of Computational Language Acquisition ACL 2007, 1-8. Stroudsburg: Association for Computational Linguistics.  Bod, R., 2008. From Exemplar to Grammar: A Prob-abilistic Analogy-based Model of Language Learn-ing. Accepted for publication in Cognitive Science. Bod, R., R. Scha and K. Sima?an, (eds.), 2003.Data-Oriented Parsing. Stanford, CA: Centre for the Study of Language and Information. Borensztajn, G., Zuidema, W., &Bod, R., 2008. Chil-dren?s grammars grow more abstract with age-evidence from an automatic procedure for identify-ing the productive units of language. In Proc. Cog-Sci 2008, 47-51. Austin: Cognitive Science Society.  Brown, R., 1973. A First Language: The Early Stages. Cambridge, Mass.: Harvard University Press. Cameron-Faulkner, T., E. Lieven and M. Tomasello, 2003. A construction-based analysis ofchild di-rected speech. Cognitive Science 23, 843-873. 
Dawkins, R, 1976. The Selfish Gene, Oxford: Oxford University Press. De Villiers, J. and P. de Villiers, 1973. Development of the Use of Word Order in Comprehension, Journal of Psycholinguistic Research, 2; 331-341. DeWitt, T, and S. Scheiner, 2004a.Phenotypic Plas-ticity: Functional and Conceptual Approaches. Ox-ford: Oxford University Press. DeWitt, T. and S. Scheiner, 2004b. ?Phenotypic Variation from Single Genotypes: A Primer?, in DeWitt and Scheiner 2004a, 1-9. Edelman, G. 1987.The Theory of Neuronal Group Selection. New York NY: Basic Books. Elman, J. 1993. ?Learning and development in neural networks: The importance of starting small?. Cog-nition, 48(1):71-99. Fernando C, K. Karishma and E. Szathm?ry.2008. ?Copying and Evolution of Neuronal Topol-ogy?.PLoS ONE 3(11): e3775. Goodman, J., 1996. Efficient algorithms for parsing the DOP model.In Proc. EMNLP 1996, 143-152. Stroudsburg: Association for Computational Linguistics Goodman, J., 2003. ?Efficient parsing of DOP with PCFG-reductions?.InBod, Scha and Sima?an 2003, 125-146. Johnson, M. 1998."The DOP Estimation Method is Biased and Inconsistent", Computational Linguis-tics, 28:71?76. Kirby, S. 1999. ?Learning, bottlenecks, and infnity: a working model of the evolution of syntactic com-munication?, Proc. AISB'99 Symposium on Imita-tion in Animals and Artifacts, 55-63. Society of the Study of Artificial Intelligence and the Simulation of Behaviour. Manning, C. and H. Sch?tze, 1999. Foundations of Statistical Natural Language Process-ing.Cambridge, Mass; The MIT Press. Saffran, J., R. Aslin, and E. Newport.1996. ?Statisti-cal learning by 8-month-old infants?.Science, 274:1926?1928. Scha, R., 1990. ?Taaltheorie en Taaltechnologie: Competence en Performance?, in Q. de Kort and G. Leerdam (eds.), Computertoepassingen in de Neer-landistiek, Almere: LandelijkeVereniging van Neerlandici. West-Eberhard, M, 2003.Developmental Plasticity and Evolution, Oxford: Oxford University Press. Williams, G., 1966.Adaptation and Natural Selection. Princeton: Princeton University Press. Zollmann, A., and K. Sima'an, 2005. A consistent and efficient estimator for data-oriented pars-
49
ing.Journal of Automata, Languages and Combi-natorics 10, 367-388. 
50
