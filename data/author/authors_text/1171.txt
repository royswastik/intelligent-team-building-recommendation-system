Domain Specific Word Extraction from Hierarchical Web Documents:
A First Step Toward Building Lexicon Trees from Web Corpora
Jing-Shin Chang
Department of Computer Science & Information Engineering
National Chi-Nan University
1, University Road, Puli, Nantou, Taiwan 545, ROC.
jshin@csie.ncnu.edu.tw
Abstract
Domain specific words and ontological
information among words are important
resources for general natural language
applications. This paper proposes a
statistical model for finding domain
specific words (DSW?s) in particular
domains, and thus building the
association among them. When applying
this model to the hierarchical structure
of the web directories node-by-node, the
document tree can potentially be
converted into a large semantically
annotated lexicon tree. Some
preliminary results show that the current
approach is better than a conventional
TF-IDF approach for measuring domain
specificity. An average precision of
65.4% and an average recall of 36.3%
are observed if the top-10% candidates
are extracted as domain-specific words.
1 Domain Specific Words and Lexicon
Trees as Important NLP Resources
Domain specific words (DSW?s) are important
?anchoring words? for natural language
processing applications that involve word sense
disambiguation (WSD). It is appreciated that
multi-sense words appearing in the same
document tend to be tagged with the same word
sense if they belong to the same common domain
in the semantic hierarchy (Yarowsky, 1995). The
existence of some DSW?s in a document will
therefore be a strong evidence of a specific sense
for words within the document. For instance, the
existence of ?basketball? in a document would
strongly suggest the ?sport? sense of the word
???? (?Pistons?), rather than its ?mechanics?
sense. It is also a personal belief that DSW-based
sense disambiguation, document classification
and many similar applications would be easier
than sense-based models since sense-tagged
documents are rare while domain-aware training
documents are abundant on the Web. DSW
identification is therefore an important issue.
On the other hand, the semantics hierarchy
among words (especially among sets of domain
specific words) as well as the membership of
domain specific words are also important
resources for general natural language processing
applications, since the hierarchy will provide
semantic links and ontological information (such
as ?is-A? and ?part-of? relationships) for words,
and, domain specific words belonging to the
same domain may have the ?synonym? or
?antonym? relationships. A hierarchical lexicon
tree (or a network, in general) (Fellbaum, 1998;
Jurafsky and Martin, 2000), indicative of sets of
highly associated domain specific words and
their hierarchy, is therefore invaluable for NLP
applications.
Manually constructing such a lexicon
hierarchy and acquiring the associated words for
each node in the hierarchy, however, is most
likely unaffordable both in terms of time and cost.
In addition, new words (or new usages of words)
are dynamically produced day by day. For
instance, the Chinese word ???? (pistons) is
more frequently used as the ?sport? or
?basketball? sense (referring to the ?Detroit
64
Pistons?) in Chinese web pages rather than the
?mechanics? or ?automobile? sense. It is
therefore desirable to find an automatic and
inexpensive way to construct the whole
hierarchy.
Since the hierarchical web pages provide
semantic tag information (explicitly from the
HTML/XML tags or implicitly from the
directory names) and useful semantic links, it is
desirable that the lexicon construction process
could be conducted using the web corpora.
Actually, the directory hierarchy of the Web can
be regarded as a kind of classification tree for
web documents, which assigns an implicit hidden
tag (represented by the directory name) to each
document and hence the embedded domain
specific words. Converting such a hierarchy into
a lexicon tree is therefore feasible, provided that
we can remove non-specific terms from the
associated document sets.
For instance, the domain-specific words for
documents under the ?sport? hierarchy are likely
to be tagged with a ?sport? tag. These tags, in
turn, can be used in various word sense
disambiguation (WSD) tasks and other hot
applications like anti-spamming mail filters.
Such rich annotation provides a useful
knowledge source for mining various semantic
links among words.
We therefore will explore a non-conventional
view for constructing a lexicon tree from the web
hierarchy, where domain-specific word
identification turns out to be a key issue and the
first step toward such a construction process. An
inter-domain entropy (IDE) measure will be
proposed for this purpose.
2 Conventional Clustering View for
Constructing Lexicon Trees
One conventional way to construct the lexicon
hierarchy from web corpora is to collect the
terms in all web documents and measure the
degree of word association between word pairs
using some well-known association metrics
(Church and Hanks, 1989; Smadja et al, 1996)
as the distance measure. Terms of high
association are then clustered bottom-up using
some clustering techniques to build the hierarchy.
The clustered hierarchy is then submitted to
lexicographers to assign a semantic label to each
sub-cluster. The cost will be reduced in this way,
but could still be unaffordable. Besides, it still
depends on the lexicographers to assign
appropriate semantic tags to the list of highly
associated words.
There are several disadvantages with this
approach. Firstly, the hierarchical relationship
among the web documents, and hence the
embedded DSW?s, is lost during the document
collection process, since the words are collected
without considering where they come from in the
document hierarchy. The loss of such
hierarchical information implies that the
clustered one will not match human perception
quite well. Secondly, the word association metric
and the clustering criteria used by the clustering
algorithm are not directly related to human
perception. Therefore, the lexicographers may
not be able to adjust the clustered hierarchy
comfortably. Thirdly, most clustering algorithms
merge terms in a binary way; this may not match
human perception as well. As far as the
computation cost is concerned, computation of
word association based on pairwise word
association metrics will be time consuming.
Actually, such an approach may not be the
only option today, thanks to the large number of
web documents, which are natively arranged in a
hierarchical manner.
3 Lexicon Tree Construction as
Domain Specific Word Detection
from Web Hierarchy
Since the web documents virtually form an
extremely huge document classification tree, we
propose here a simple approach to convert it into
a lexicon tree, and assign implicit semantic tags
to the domain specific words in the web
documents automatically.
This simple approach is inspired by the fact
that most text materials (webpages) in websites
are already classified in a hierarchical manner;
the hierarchical directory structures implicitly
suggest that the domain specific terms in the text
65
materials of a particular subdirectory are closely
related to a common subject, which is identified
by the name of the subdirectory.
If we can detect domain specific words within
each document, and remove words that are
non-specific, and tag the DSW?s thus acquired
with the directory name (or any appropriate tag),
then we virtually get a hierarchical lexicon tree.
In such a tree, each node is semantically linked
by the original web document hierarchy, and
each node has a set of domain specific words
associated with it.
For instance, a subdirectory entitled
?HQWHUWDLQPHQW? LV OLNHO\ WRKDYH D ODUJH QXPEHU
of web pages containing domain specific terms
OLNH ?VLQJHU? ?SRS VRQJV? ?URFN 	 UROO?
?Ah-Mei? (nickname of a pop song singer),
?DOEXP?DQGVRRQ6LQFHWKHVHZRUGVDUHKLJKO\
associated with the ?HQWHUWDLQPHQW? domain, we
will be able to collect the domain specific words
of the ?entertainment? domain from such a
directory.
In the extraction process, the directory names
can be regarded as implicit sense labels or
implicit semantic tags (which may be different
from linguistically motivated semantic tags), and
the action to put the web pages into properly
named directories can be regarded as an implicit
tagging process by the webmasters. And, the
hierarchical directory itself provides information
on the hierarchy of the semantic tags.
From a well-organized web site, we will then
be able to acquire an implicitly tagged corpus
from that site. Thanks to the webmasters, whose
daily work include the implicit tagging of the
corpora in their websites, there is almost no cost
to extract DSW?s from such web corpora. This
idea actually extends equally well for other
Internet resources, such as news groups and BBS
articles, that are associated with hierarchical
group names. Extending the idea to well
organized book chapters, encyclopedia and
things like that would not be surprised too.
The advantages of such a construction
process, by removing non-specific terms, are
many folds. First, the original hierarchical
structure reflects human perception on document
(and term) classification. Therefore, the need for
adjustment may be rare, and the lexicographers
may be more comfortable to adjust the hierarchy
even if necessary. Second, the directory names
may have higher correlation with linguistically
motivated sense tags than those assigned by a
clustering algorithm, since the web hierarchy was
created by a human tagger (i.e., the webmaster).
As far as the computation cost is concerned,
pairwise word association computation is now
replaced by the computation of ?domain
specificity? of words against domains. The
reduction is significant, from O(|W|x|W|) to
O(|W|x|D|), where |W| and |D| represent the
vocabulary size and number of domains,
respectively.
4 Domain Specific Word Extraction as
the Key Technology: An
Inter-Domain Entropy Approach
Since the terms (words or compound words)
in the documents include general terms as well as
domain-specific terms, the only problem then is
an effective model to exclude those
domain-independent terms from the implicit
tagging process. The degree of domain
independency can be measured with the
inter-domain entropy (IDE) as will be defined in
the following DSW (Domain-Specific Word)
Extraction Algorithm. Intuitively, a term that
distributes evenly in all domains is likely to be
independent of any domain. We therefore weight
such terms less probable as DSW?s. The method
can be summarized in the following algorithm:
Domain-Specific Word Extraction &
Lexicon Tree Construction Algorithm:
Step1 (Data Collection): Acquire a large
collection of web documents using a web
spider while preserving the directory
hierarchy of the documents. Strip unused
markup tags from the web pages.
Step2 (Word Segmentation or Chunking):
Identify word (or compound word)
boundaries in the documents by applying a
word segmentation process, such as
66
(Chiang et al, 1992; Lin et al, 1993), to
Chinese-like documents (where word
boundaries are not explicit) or applying a
compound word chunking algorithms to
English-like documents (where word
boundaries are clear) in order to identify
interested word entities.
Step3 (Acquiring Normalized Term Frequencies
for all Words in Various Domains): For
each subdirectory d j , find the number of
occurrences nij of each term wi in all
the documents, and derive the normalized
term frequency f n Nij ij j / by
normalizing nij with the total document
size, N nj ij
i
{? , in that directory. The
directory is then associated with a set of
 !w d fi j ij, , tuples, where wi is the
i-th words of the complete word list for all
documents, d j is the j-th directory name
(refer to as the domain hereafter), and
f n Nij ij j / is the normalized relative
frequency of occurrence of wi in domain
d j .
Step4 (Identifying Domain-Independent Terms):
Domain-independent terms are identified as
those terms which distributed evenly in all
domains. That is, terms with large
Inter-Domain Entropy (IDE) defined as
follows:
  logi i ij ij
j
ij
ij
ij
j
H H w P P
f
P
f
{ { 
{
?
?
Terms whose IDE?s are above a threshold
are likely to be removed from the lexicon
tree since such terms are unlikely to be
associated with any particular domain.
Terms with a low IDE, on the other hand,
may be retained in a few domains with high
normalized frequencies.
To appreciate the fact that a high frequency
term may be more important in a domain, the
IDE is further weighted by the term
frequency in the particular domain when
deciding whether a term should be removed.
Currently, the weighting method is the same
as the conventional TF-IDF method
(Baeza-Yates and Ribeiro-Neto, 1998;
Jurafsky and Martin, 2000) for information
retrieval. In brief, a word with entropy Hi
can be think of as a term that spreads in
2**Hi domains on average. The equivalent
number of domains a term could be found
then can be equated to 2 iHiNd  . The term
weight for wi in the j-th domain can then
be estimated as:
2logij ij
i
NW n
Nd
? ? u ? ?? ?
where N is the total number of domains.
Unlike the conventional TF-IDF method,
however, the expected number of domains
that a term could be found is estimated by
considering its probabilistic distribution,
instead of simple counting.
Step5 (Output): Sort the words in each domain
by decreasing weights, Wij, and output the
top-k% candidates as the domain specific
words of the domain. The percentage (k)
can be determined empirically, or based on
other criteria, such as their classification
performance in a DSW-based text classifier
(Chang, 2005). The directory tree now
represents a hierarchical classification of the
domain specific terms for different domains.
Since the document tree may not be really
perfect, we have the option to adjust the
hierarchy or the sets of words associated with
each node, after eliminating domain-independent
terms from the directory tree. The terms can be
further clustered into highly associated word lists,
with other association metrics. On the other hand,
we can further move terms that are less specific
to the current domain upward toward the root.
This action will associate such terms with a
slightly more general domain. All these issues
will be left as our future works.
67
However, the current method is independent
of the source web hierarchy. Given a web
organized as an encyclopedium of biology, the
current method is likely to find out the living
species associated with each node of the
underlying taxonomy automatically. With more
and more well organized web sites of various
kinds of knowledge online, the problems with
imperfect web hierarchy will hopefully become a
less important issue.
5 Evaluation
To see how the above algorithm could be useful
as a basis for building a large lexicon tree from
web pages, some preliminary results will be
examined in this section.
A large collection of Chinese web pages was
collected from a local news site. The size of the
HTML web pages amounts to about 200M bytes
in 138 subdomains (including the most specific
domains at the leaf nodes and their ancestor
domains). About 16,000 unique words are
identified after word segmentation is applied to
the text parts.
It was observed, from some small sample
domains, that only around 10% of the words in
each subdomain are deemed domain specific.
(The percentages, however, may vary from
domain to domain.) The large vocabulary size
and the small percentage of DSW?s suggest that
the domain specific word identification task is
not an easy one.
Table 1 shows a list of highly associated
domain-specific words of low inter-domain
entropies and their domain names. (Literal
English translation for each term is enclosed in
the parenthesis.) They are sampled from 4 out of
138 subdomains. The domain names virtually act
as the semantic tags for such word lists. The tags,
being extracted from manually created directory,
well reflect the senses of the words in each
subdomains.
Table 1 shows that many domain-specific
words can really be extracted with the proposed
approach in their respect domains. For instance,
the word ?pitcher? (????) is specifically used
in the ?baseball? domain. The domain specific
words and their domain tags are well associated.
As a result of such association, low
inter-domain entropy words in the same domain
are also highly correlated. For instance, the term
???? for calling a Japanese baseball team
?manager? is specifically used with????
? ? (Japanese professional baseball team),
instead of a Chinese team, where ?manager? is
called differently.
In addition, new usages of words, such as
??? (Pistons)? with the ?basketball? sense,
could also be identified by the current approach.
Furthermore, it was also observed that many
irrelevant words (such as those words in the
webmasters? advertisement) are rejected as the
DSW candidates automatically since they have
very high inter-domain entropies.
One can also find interesting lexical
relations (Fellbaum, 1998) among the domain
tags and domain specific words, form Table 1,
such as:
Hypernym/Hyponym: athlete ( ? ? ) vs.
baseball game (???); car (??) vs.
small car (???).
Has-Member/Member-Of: baseball team (??)
vs. manager (??), pitcher (??).
Has-Part/Part-Of: car (??) vs. engine cover
(???), tank (??), safety system (??
??), trunk (???).
Antonym: shot (??) vs. defense (??).
Such lexical relations are, in general,
interesting to lexical database builders.
Furthermore, for data driven applications, such
fine details are unlikely to be listed in a general
purpose lexical database. ([WUDFWLQJ'6:?s with
the inter-domain entropy (IDE) metric is
therefore well founded.
68
In order to have a quantitative evaluation, we
have inspected a few domains of small sizes
(each containing about 300 unique words or less)
for a preliminary estimation. The top-10%
candidates with lowest inter-domain entropy,
weighted by their term frequencies in their
respect domains, are evaluated. (The 10%
threshold is selected arbitrarily.) Table 2 shows
the results in terms of precision (P), recall (R)
and F-measure (F). The column with the
?#Words? label shows the numbers of unique
words used in the 5 domains.
Since it is difficult sometimes to have a
consistent judgment on ?domain specificity?, the
estimation could vary drastically on other
domains by other persons. For this reason, the
degree of domain specificity is ranked from 0
(irrelevant) to 5 (absolutely specific to the
domain) points. Therefore, when computing the
precision and recall measures, a completely
?correct? answer should have a grading point ?5?.
Fortunately, most terms are assigned the grading
point 5, with a few less certain cases assigned ?3?
or ?4?.
Baseball Broadcast-TV Basketball Car
????
(Japanese professional
baseball)
????
(cable TV)
??
(one minute)
???
(Kilo-c.c.)
???
(baseball games)
??
(the Dong Fong TV
Station)
??
(three seconds)
???
(small car)
??
(warm up)
??
(start to work)
???
(girl?s teams)
??
(used car)
?? (athlete) ??? (on air) ?? (fold; clip) ??? (engine cover)
??
(time table)
??? (radio-tv
office)
?? ?? (tank)
?? (cost) ?? ?? (foul) ??
?? (baseball team) ?? (Ho-Hsin TV
Station)
?? (shot) ???? (market
atmosphere)
?? (manager) ??? (government
information office)
??? (male team) ??? (destination)
?? (practicing) ?? ?? (defense) ?? (car delivery)
?? (Hsin-Lung team) ?? (channel) ???(championship) ??
(of the same grade)
??(course; diamond) ??
(TV)
?? (fullback) ????
(co-development)
?? (pitcher) ??(movie) ??
(Pistons team)
???? (safety system)
?? (season) ??
(hot)
?? (national male
team)
?? (luggage)
?? (schedule) ?? (video) ??(Wallace) ??? (trunk)
?? (the Sun team) ?? (entertainment) ?? (Philadelphia) ?? (c.c.)
Table 1. Sampled domain specific words with low entropies.
69
Domain #Words R P F
Baseball 149 29.7 68.0 41.3
Basketball 277 26.2 60.7 36.6
Broadcast
-TV
161 47.6 50.0 48.8
Education 255 40.0 81.5 53.7
Health
-care
263 38.2 66.9 48.6
(Average) 36.3 65.4 45.8
Table 2. Performance of the Top-10% DSW
candidate lists in 5 sample domains.
Table 2 shows that, by only gathering the first
10% of the word lists, we can identify about 36%
of the embedded domain specific words, and the
precision is as high as 65%. Therefore, we can
identify significant amount of DSW?s about
every 1.5 entries from the top-10% list of low
entropy words.
Since the TF-IDF (term frequency-inverse
document frequency) approach (Baeza-Yates and
Ribeiro-Neto, 1998; Jurafsky and Martin 2000) is
widely used in information retrieval applications
for indexing important terms within documents,
it can also be applied to identify domain specific
words in various domains. To make a
comparison, the TF-IDF term weighting method
is also applied to the same corpus. The
?baseball? domain is then inspected for their
differences. It turns out that the top-10%
candidate lists of both methods show the same
performance. However, the IDE measure appears
to reach the highest precision faster than the
TF-IDF approach. Furthermore, the IDE measure
has a better top-20% performance than that of the
TF-IDF approach as listed in Table 3.
Model R P F
IDE 48.3 55.3 51.6
TF-IDF 44.8 51.3 47.8
Table 3. Comparison of the top-20% candidate
list performance between IDE and TFIDF
-based approaches.
Although it is not sure whether the superiority
of the IDE approach will retain when examining
larger corpora, it does have its advantages in
indicating the ?degree of specificity?. In
particular, the degree of domain specificity of a
term is estimated by considering the
cross-domain probability distribution of the term
in the current IDE-based approach. Instead, the
TF-IDF approaches only count the number of
domains a term was found as a measure of
randomness. The IDE approach is therefore
gaining a little bit performance than a TF-IDF
model.
The results partially confirm our expectation
to build a large semantically annotated lexicon
tree from the web pages using the implicit tags
associated with the web directory hierarchy and
removing non-specific words from web
documents.
6 Error Sources and Future Works
In spite of some encouraging results, we also
observed some adverse effects in using the single
inter-domain entropy metric to identify domain
specific words. For instance, some non-specific
words may also have low entropy simply because
they appear in only one domain (IDE=0). Since
such words cannot be distinguished from real
?GRPDLQ-VSHFLILF? ZRUGV there should be other
knowledge sources to reduce false alarms of this
kind (known as the Type II errors.)
On the other hand, some multiple sense words
may have too many senses such that they are
considered non-specific in each domain
(although the sense is unique in each respect
domain). This is a typical Type I error we have
observed.
As a result, further refinement of the purely
statistical model is required to improve the
precision of the current approach. Currently, we
prefer a co-training approach inspired by the
works in (Chang and Su, 1997; Chang, 1997),
which is capable of augmenting a single IDE-like
metric with other information sources.
We have also assumed that the directories of
all web sites are well organized in the sense that
the domain labels (directory names) are
appropriate representatives of the documents
under the directories. This assumption is not
always satisfied since it depends on the site
RZQHUV?YLHZVRQWKHGRFXPHQWV7KHUHDUHJRRG
70
chances that the hierarchies differ from site to
site. Therefore, we may need some measures of
site similarity, and approaches to unify the
different hierarchies and naming policies as well.
The answers to such problems are not yet clear.
However, we believe that the hierarchy of the
directories (even though not well named) had
substantially reduces the cost for lexicogrophers
who want to build a large semantically annotated
lexicon tree. And the whole process will become
more and more automatic as we refine the above
model against more and more data.
7 Concluding Remarks
The major contribution of the proposed model is
to extract highly associated sets of
domain-specific words, and keeping their
hierarchical links with other sets of domain
specific words at low cost. These sets of highly
associated domain specific words can thus be
used directly for sense disambiguation and
similar applications. The proposed model takes
advantages of the rich web text resources and the
implicit semantic tags implied in the directory
hierarchy for web documents. Therefore, the
requirement for manual tagging is negligible.
The extracted lists of DSW?s are not only useful
for word sense disambiguation but also useful as
a basis for constructing lexicon databases with
rich semantic links. So far, an average precision
of 65.4% and an average recall of 36.3% are
observed if the top-10% candidates are extracted
as domain-specific words. And it outperforms the
TF-IDF method for term weighting in the current
task.
Acknowledgements
Part of the work was supported by the National
Science Council (NSC), ROC, under the contract
NSC 90-2213-E-260-015-.
References
Christiane Fellbaum (Ed.). 1998. WordNet: An
Electronic Lexical Database. MIT Press,
Cambridge, MA.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to
Natural Language Processing, Computational
Linguistics, and Speech Recognition, Prentice-Hall,
NJ, USA.
David Yarowsky. 1995. "Unsupervised Word Sense
Disambiguation Rivaling Supervised Methods," in
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics, pp.
189-196, MIT, MA, USA.
Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. "Translating Collocations
for Bilingual Lexicons: A Statistical Approach,"
Computational Linguistics, 22(1):1-38.
Jing-Shin Chang and Keh-Yih Su. 1997. "An
Unsupervised Iterative Method for Chinese New
Lexicon Extraction", International Journal of
Computational Linguistics and Chinese Language
Processing (CLCLP), 2(2):97-148.
Jing-Shin Chang. 1997. Automatic Lexicon
Acquisition and Precision-Recall Maximization for
Untagged Text Corpora, Ph.D. dissertation,
Department of Electrical Engineering, National
Tsing-Hua University, Hsinchu, Taiwan, R.O.C.
Jing-Shin Chang. 2005. "Web Document
Classification into a Hierarchical Document Tree
Based on Domain Specific Words", submitted.
Ken Church and Patrick Hanks. 1989. "Word
Association Norms, Mutual Information, and
Lexicography," Proc. 27th Annual Meeting of the
ACL, pp. 76-83, University of British Columbia,
Vancouver, British Columbia, Canada.
Ming-Yu Lin, Tung-Hui Chiang and Keh-Yih Su.
1993. "A Preliminary Study on Unknown Word
Problem in Chinese Word Segmentation,"
Proceedings of ROCLING VI, pp. 119-142.
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval, Addison Wesley,
New York.
Tung-Hui Chiang, Jing-Shin Chang, Ming-Yu Lin and
Keh-Yih Su. 1992. "Statistical Models for Word
Segmentation and Unknown Word Resolution,"
Proceedings of ROCLING-V, pp. 123-146, Taipei,
Taiwan, R.O.C.
71
Page 1
A Preliminary Study on Probabilistic Models for Chinese Abbreviations 
Jing-Shin Chang 
Department of Computer Science & 
Information Engineering 
National Chi-Nan University 
Puli, Nantou, Taiwan, ROC. 
jshin@csie.ncnu.edu.tw 
Yu-Tso Lai 
Department of Computer Science & 
Information Engineering 
National Chi-Nan University 
Puli, Nantou, Taiwan, ROC. 
s0321521@ncnu.edu.tw 
Abstract 
Chinese abbreviations are widely used in 
the modern Chinese texts. They are a 
special form of unknown words, including 
many named entities. This results in 
difficulty for correct Chinese processing. 
In this study, the Chinese abbreviation 
problem is regarded as an error recovery 
problem in which the suspect root words 
are the ?errors? to be recovered from a set 
of candidates. Such a problem is mapped
to an HMM-based generation model for 
both abbreviation identification and root
word recovery, and is integrated as part of
a unified word segmentation model when 
the input extends to a complete sentence. 
Two major experiments are conducted to 
test the abbreviation models. In the first 
experiment, an attempt is made to guess 
the abbreviations of the root words. An 
accuracy rate of 72% is observed. In 
contrast, a second experiment is 
conducted to guess the root words from 
abbreviations. Some submodels could 
achieve as high as 51% accuracy with the 
simple HMM-based model. Some 
quantitative observations against heuristic 
abbreviation knowledge about Chinese 
are also observed. 
1 Introduction 
The modern Chinese language is a highly 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 1 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
abbreviated one due to the mixed uses of ancient 
single character words as well as modern 
multi-character words and compound words. The 
abbreviated form and root form are used 
interchangeably everywhere in the current Chinese 
articles. Some news articles may contain about 
20% of sentences that have suspect abbreviated 
words in them ?Lai 2003?. Since abbreviations 
cannot be enumerated in a dictionary, it forms a 
special class of unknown words, many of which 
originate from named entities. Many other open
class words are also abbreviatable. This particular 
class thus introduces complication for Chinese
language processing, including the fundamental 
word segmentation process ?Chiang 1992, Lin
1993, Chang 1997? and many word-based 
applications. For instance, a keyword-based 
information retrieval system may requires the two 
forms, such as ?
? and ?
? 
??legislators??, in order not to miss any relevant 
documents. The Chinese word segmentation 
process is also significantly degraded by the 
existence of unknown words ?Chiang 1992?, 
including unknown abbreviations. 
There are many heuristics for Chinese
abbreviations. Such heuristics, however, can easily 
break ?Sproat 2002?. Currently, only some
quantitative approaches ?Huang 1994a, 94b? are 
available in predicting the presentation of an 
abbreviation. Since such formulations regard the 
word segmentation process and abbreviation 
identification as two independent processes, they
probably cannot optimize the identification 
process jointly with the word segmentation process, 
and thus may lose the useful contextual 
information. Some class-based segmentation 
models ?Sun 2002, Gao 2003? well integrate the 
identification of some regular non-lexicalized units 
?such as named entities?. However, the 
abbreviation process can be applied to almost all 
word forms ?or classes of words?. Therefore, this
particular word formation process may have to be 
handled as a separate layer in the segmentation 
process. 
To resolve the Chinese abbreviation problems 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 2 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
and integrate its identification into the word
segmentation process, this study proposes to 
regard the abbreviation problem in the word
segmentation process as an ?error recovery? 
problem in which the suspect root words are the 
?errors? to be recovered from a set of candidates 
according to some generation probability criteria. 
This idea implies that an HMM-based model for 
identifying Chinese abbreviations could be 
effective in either identifying the existence of an 
abbreviation or the recovery of the root words 
Page 2
from an abbreviation. We therefore start with a 
unified word segmentation model so that both 
processes can be handled at the same time, and 
when the input is reduced to a single abbreviated 
word, the model can be equally useful for 
recovering its root. 
As a side effect of using HMM-based
formulation, we expect that a large abbreviation 
dictionary could be derived from a large corpus or 
from web documents through the training process 
of the unified word segmentation model 
automatically. 
Section 2 will show our HMM models and the 
three abbreviation problems correspond to the 
three basic HMM problems. Section 3 will show 
the experiment setup. Section 4 will examine the 
experiments to guess abbreviations from root or 
vice versa. 
2 Chinese Abbreviation Models
2.1 An Error Recovery Paradigm 
To resolve the abbreviation problems, first note 
that the most common action one would take when 
encountering an abbreviation is to find its 
candidate roots ?probably from a large 
abbreviation dictionary if available or from an 
ordinary dictionary with some educated guesses?, 
and then identify the most probable one. This 
process is identical to the operation of many 
spelling correction models, which generate the
candidate corrections according to a reversed word 
formation process, then justify the best candidate. 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 3 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
Such an analogy indicates that we may use an 
HMM model ?Rabiner 1993?, which is good at 
finding the best unseen state sequence, for error 
recovery. There will be a direct map between the 
two paradigms if we regard the observed input 
character sequence as our ?observation sequence?, 
and regard the unseen word candidates as the 
underlying ?state sequence?. 
Given these mappings, we will be able to use 
many standard processing approaches for HMM 
when we have to answer some interesting 
questions ?including root word recovery?. Among 
all interesting questions for an HMM, we have 
three basic questions to ask the model ?Rabiner 
1993?, namely the output probability of an output 
sequence, the best underlying state sequence and 
the best parameters given a training corpus. 
If we can ask the HMM for abbreviation the 
same questions, then we will also be able to 
answer the question on ?1? what is the likelihood 
that a string is an abbreviation, ?2? what are the 
best underlying root words for an input character
string that contains abbreviations, and ?3? how to 
estimate the model parameters automatically given 
a corpus. 
The first question is related to the problem of 
generating an appropriate abbreviation from a root 
word; the second question is linked to finding the
best underlying roots from an abbreviated string, 
and the third question have a direct link to the 
construction of an abbreviation dictionary 
automatically from a corpus. For now we will not 
explore this third question, but leave it to a
research that would be launched in the near future. 
The most interesting question to ask is, of 
course, the second question in the Chinese 
tokenization process. Therefore, we will start with 
a unified word segmentation model, which has the 
capability to handle abbreviation problem jointly 
with the word segmentation process. 
2.2 HMM-Q2: Unified Word Segmentation 
Model for Abbreviation Recovery 
To integrate the abbreviation process into the word
segmentation model, firstly we can regard the 
segmentation model as finding the best underlying 
words 
m
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 4 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
mw
w
w
,
,
1
1
??
?which include only 
base/root forms?, given the surface string of 
characters 
n
n
c
c
c
,
,
1
1
??
?which may contain 
abbreviated forms of compound words.? The 
segmentation process is then equivalent to finding
the best word sequence
*
w
such that: 
?
?
?
? ? ?
?
? ?
?
?
?
=
?
?
?
?
?
=
?
=
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 5 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
=i
i
n
m
m
n
m
m
n
m
m
c
w
m
i
i
i
i
i
c
w
w
m
m
n
c
w
w
n
m
c
w
w
w
w
P
w
c
P
w
P
w
c
P
c
w
P
w
,1
1
:
1
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 6 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
1
1
:
1
1
:
|
|
max
arg
|
max
arg
|
max
arg
*
1
1
1
1
1
1
1
1
1
Equation 1. Unified Word Segmentation 
Model for Abbreviation Recovery 
where 
i
c
refers to the surface form of 
i
w
, which 
could be in an abbreviated or non-abbreviated ?or 
any transformed? form of 
i
w
. The last equality 
assumes that the generation of an abbreviation is 
independent of context, and the language model is 
a word-based bigram model. Such assumptions 
can be adapted to different submodels for word 
segmentation ?Chiang 1992? as appropriate. 
Furthermore, in many cases, the underlying word
i
w
will be a compound word consisting of other 
constituent words 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 7 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
ij
w
?e.g., ?
??. And, 
the probability 
?
?
i
i
w
c
P |
is not always 1 or 0, 
since the constituents may be abbreviated 
Page 3
differently in different context, making the
mapping of the compound ambiguous. For 
instance, some people may prefer to abbreviate ?
? ?Industrial Technology Research 
Institute; ITRI? into ?
? ?IRI? while other 
may prefer an abbreviation of ?
? ?ITI?. 
Notice that, this equation is equivalent to the 
formulation for an HMM ?Hidden Markov Model? 
?Rabiner 1993? to find the best ?state? sequence 
given the observation symbols. The parameters 
?
?
1
|
?i
i
w
w
P
and 
?
?
i
i
w
c
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 8 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
P |
represent the
transition probability and the ?word-wise? output 
probability of an HMM, respectively; and, the
formulations for 
? ?
m
w
P
1
and 
?
?
m
n
w
c
P
1
1
|
are 
the respective ?language model? of the Chinese 
language and the ?generation model? for the 
abbreviated words ?i.e., the ?abbreviation model?
in the current context?. The ?state? sequence in the 
word segmentation case is characterized by the
root forms
m
m
w
w
w
,
,
1
1
??
, or the hidden words; 
and, the ?observation symbols? are characterized 
here by 
m
n
n
c
c
c
c
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 9 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
c,
,
,
,
1
1
1
??
??
, where the
surface form 
? ?
? ?
i
e
i
b
i
c
c ??
is a chunk of characters 
beginning at the b?i?-th character and ending at the 
e?i?-th character. 
Such an analogy with an HMM enables us to 
estimate the model parameters using an 
unsupervised training method that is directly 
ported 
from 
the 
forward-backward 
or 
Baum-Welch re-estimation formula ?Rabiner 1993? 
or a generic EM algorithm ?Dempster 1977?. 
Note also that, while the above formulation is 
intended for finding root words in a sentence, with
the help of contextual words, we can also apply the 
same formulation to a single abbreviated word 
?likely to have a compound word as its root in 
many cases? to find the most likely constituent 
words, without the help of surrounding words, but 
with the help of contextual constraints among its 
constituents. 
2.2.1. Language Model 
The word transition probability 
?
?
1
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 10 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
|
?i
i
w
w
P
used in the language model is used to provide 
contextual constraints among root words. It may 
not be reliably estimated when the language has a 
large vocabulary and when the training corpus is 
small. To resolve this problem, we can back-off 
the bigram word transition probability to a 
unigram word probability using Katz?s method 
?Katz 1987? for rare bigrams. We can, of course, 
use other smoothing methods to acquire reliable 
parameters. The smoothing issues, however, are 
not the main focus of this preliminary study. 
2.2.2 Generation Model for Abbreviations 
In the perfect case where all words are 
lexicalized, rendering all surface forms identical to 
their ?root? forms and all words are known to the 
system dictionary, we will have
?
?
|
1
i
i
P c w =
, 
1,
i
m
? =
, and Equation 1 is no more than a word
bigram model for word segmentation ?Chiang 
1992?. In the presence of unknown words ?e.g., 
abbreviations being one of such entities?, however, 
we can no longer ignore the generation probability 
?
?
i
i
w
c
P |
. 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 11 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
For example, if 
i
c
is ?
? then 
i
w
could 
be the compound word ?
? ?Taiwan 
University? or ?
? ?Taiwan Major 
League?. In this case, the parameters in P?
|
? x P? |
? x P? |
? and P?
|
? 
x P? |
? x P? |
? will indicate how 
likely ?
? is an abbreviation, and which of the 
above two compounds is the root form of the 
abbreviation. Therefore, we need a method for 
estimating the probabilities between the 
abbreviations and their root forms ?many of which
are compound words with other constituents?. 
2.3 Applying Abbreviation Models 
There are two problems to use the unified model 
which takes abbreviated words into account. First 
of all, since the word lattice is constructed from all 
possible
m
m
w
w
w
,
,
1
1
??
, how can we construct it 
without really knowing the candidate base forms 
of 
i
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 12 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
cin advance? We don?t really want to 
randomly combine all possible root forms, which 
is not affordable in computational cost. Therefore, 
we have to make some smarter choices. Second, 
how 
to 
compute 
the 
abbreviation 
?output/generation? probability 
?
?
i
i
w
c
P |
once 
the lattice is constructed with candidate root 
words? 
2.3.1 Candidate Root Word Generation 
The first problem can be resolved if we choose 
some highly probable constituents 
w
that would 
generate each individual characters 
ij
c
in 
i
c
independently, and allow such Top-N candidates 
to form part of the complete word lattice. That is, 
Page 4
for each individual character 
ij
c
, we choose its 
Top-N candidates according to: 
?
?
? ?
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 13 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
|
ij
P c w P w
i
. 
The probability 
?
?
|
ij
P c w
here represents the 
character-wise generation probability of a single 
character from its corresponding root word. Notice 
that, after we apply the word segmentation model 
Equation 1 to the word lattice, some of the above
candidates may be preferred and others be 
discarded, by consulting the neighboring words 
and their transition probabilities. This makes the 
abbreviation model jointly optimized in the word 
segmentation process, instead of being optimized 
independent of context. 
2.3.2 Abbreviation Probability 
The second problem can be resolved using the 
following equation if 
i
w
can be segmented into 
iL
i
i
w
w
w
,
,
1
??
, each constituent corresponding 
to a character in 
? ?
? ?
i
e
i
b
i
c
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 14 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
c ??
: 
?
?
? ?
?
?
?
? ?
?
? ? ? ?
? ?
? ?
1
? ?
1
, 1
1,
|
|
|
|
1
e i
iL
i
i
b i
i
b i
j
ij
ij
i j
j
L
P c w
P c
w
P c
w
P w w
L e i b i
L i
+ ?
?
=
=
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 15 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
=?
=
?
+ ??
?
Equation 2. Abbreviation Probability. 
In other words, we use the transition 
probability between constituent words and the 
character-wise 
generation 
probabilities 
of 
individual characters from a constituent word to 
estimate the global generation probability of the 
abbreviated form. 
2.3.3 Simplified Abbreviation Models 
It is sometimes simply not efficient to save all 
pairs of root compounds and their respective 
abbreviations in an abbreviation dictionary. 
Therefore, it is desirable to simplify the
abbreviation probability by using some simpler 
features for Chinese abbreviation words. For 
instance, it is known that many 4-character 
compound words will be abbreviated as 
2-character abbreviations ?such as the case for the 
<
, 
> pair.? It was also known 
heuristically that many such 4-character words are 
abbreviated by reserving the first and the third 
characters, which can be represented by a ?1010? 
bit pattern, where a ?1? means to reserve the
respective character and a ?0? means to delete it. 
Therefore, a reasonable simplification for the 
abbreviation model is to introduce the length and 
the bit pattern for abbreviation operations as 
additional features into the abbreviation model. If 
this is the case, we will have the following 
augmented abbreviation model. 
?
?
1
1
1
1
|
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 16 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
? , , | , ?
? | ?
?
| ?
? | ?
m
n
m
n
P c w
P c bit m r n
P c r
P bit n P m n
=
??
?
?
1
1
: surface characters.
: root word characters.
where
: length of surface characters.
: length of root word characters.
bit: bit pattern of abbreviation 
m
n
c
r
m
n
Equation 3. Abbreviation Probability using 
Abbreviation Pattern and Length Features. 
All these three terms can be combined freely to
produce as many as 7 sub-models for the 
abbreviation model. Note, the first term 
?
?
n
m
r
c
1
1
|
Pr
plays the same role as the older 
notation of 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 17 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
??
w
c |
Pr
, which could means a pair 
of <abbreviation, root> or be evaluated as the
product of the per-character generation 
probabilities and the sub-constituent transition 
probabilities as outlined in Equation 2. This term 
can of course be ignored from the above 
augmented abbreviation model so that only very
simple length and position features are used for 
abbreviation handling. 
3 Data and Parameter Estimation 
An abbreviation dictionary containing the
word-abbreviation pairs is required to test the 
proposed models. Unfortunately, a large Chinese
abbreviation dictionary is not available. Therefore, 
we have to collect some of the generic 
abbreviations, and make others manually from 
some named entity lists. Almost half of our 
collection comes from the Ministry of Education 
of the ROC. ?http://www.edu.tw/clc/dict/?. ?In a 
future plan, a large abbreviation dictionary will be
built automatically by using the proposed models.? 
Eventually, we got 1547 root-abbreviation pairs. 
Among them, 1235 pairs are considered simple
and 312 pairs are ?tough? in the sense that they 
violate some model assumptions. For instance, we 
required that a root in a compound word be 
mapped to at least one character in its abbreviation 
?not to a null string?, and we also assume that the 
word cannot be mapped to a character that is not
part of the word. ?For example, AB can be 
abbreviated as A or B but not C.? Some tough 
words will actually map substrings to null strings; 
Page 5
others may be recursively abbreviated; and yet 
others may change the word order ?as in 
abbreviating ?
? as ?
? 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 18 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
instead of ?
?.?. As a result, the tough pairs 
will not be handled correctly with current models. 
To simplify the task, only the 1235 simple pairs 
are tested for evaluation. They are further divided 
randomly into a training set of 986 pairs ?80%? 
and a test set of 249 pairs ?20%?. Since the corpus
size is not large, the compound words are also 
manually segmented into their constituents in
order to know the true alignments between each 
character of the abbreviation with its root form in 
the compound word. Admittedly, such an 
extremely small training set causes serious data
sparseness problem during training. Therefore, the 
evaluated performance in this preliminary report
will be highly underestimated. 
The parameters are estimated in the 
unsupervised mode using a standard EM algorithm 
or the re-estimation method as conventional HMM 
models would do ?Rabiner 1993?. In addition, the 
manually segmented dictionary also allows us to 
estimate the model parameters in the supervised
mode. 
The unsupervised training will automatically 
align each character in the abbreviations to its root 
form in the full words. It is observed that 65.5% of 
the training set dictionary pairs will be aligned 
correctly. Other pairs are aligned partially correct. 
Note that parameters 
?
?
n
m
P |
and 
?
?
n
bit
P
|
can be estimated using maximum likelihood 
estimation by directly consulting the abbreviation 
dictionary since they are only related to word 
length and character position. It is interesting, in 
the first place, to check these types of parameters 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 19 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
quantitatively to see if they reveal some 
abbreviation heuristics recognized by native 
Chinese speakers. The high frequency patterns, 
which are much more frequent than the ones 
ranked in lower places, are listed in Table 1 and 
Table 2. 
P?m|n? Score Examples 
P?1|2? 1.00 ? |
?, ? |
? 
P?2|3? 0.67 ?
|
?, ?
|
? 
P?2|4? 0.95 ?
|
?, 
?
|
? 
P?3|5? 0.73 ?
|
?, 
?
|
? 
P?3|6? 0.70 ?
|
?, 
?
|
? 
P?3|7? 0.76 ?
|
?, 
?
|
? 
Table 1. High Frequency Abbreviation 
Patterns [by lengths] 
P(bit|n) 
Score
Examples 
P?10|2?
0.87
? |
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 20 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
?,? |
? 
P?101|3?
0.44
?
|
?, 
?
|
? 
P?1010|4?
0.56
?
|
?, 
?
|
? 
P?10101|5?
0.66
?
|
?, 
?
|
? 
P?101001|6?
0.51
?
|
?,
?
|
?
P?1010001|7?
0.55
?
|
?, 
?
|
? 
P?10101010|8?
0.21
?
|
?, 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 21 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
?|
? 
Table 2. High Frequency Abbreviation Patterns 
[by P?bit|n?] 
Table 1 shows how word lengths will change 
during the abbreviation process, and Table 2
shows which characters will be deleted from the 
root of a particular length. The tables
quantitatively support some general heuristics for 
native Chinese speaker. For instance, most words 
will be abbreviated by deleting about half the 
characters in the words, as shown in Table 1. The 
data also shows that the first character in a 
two-character word will be retained in most cases, 
and the first and the third characters in a 
4-character word will be retained in 56% of the 
cases. However, the tables also shows that around 
50% of the cases cannot be uniquely determined 
simply by consulting the word length for its
abbreviated form. This does suggest the necessity
of an abbreviation model for resolving this kind of
unknown words and named entities. 
4 Experiments and Analysis 
The unified model can be applied to a whole 
sentence which contains abbreviations during
word segmentation. When the input is reduced to a 
single abbreviated word ?or compound?, it can also 
be applied to recover the underlying root 
constituent words ?without consulting contextual 
words?. In this paper, we will only focus on the 
abbreviation word recovery problems. 
Two major experiments are conducted. The 
first experiment is to guess the most likely 
abbreviation form for a word using various feature 
combinations; the second is to guess the root word 
Page 6
from an abbreviation. The following sections will 
give more details. 
4.1 Guessing Abbreviations from Roots 
The main task of this experiment is to guess the 
most probable abbreviation forms for the
unabbreviated words in a word list. The 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 22 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
abbreviation forms of a word can be enumerated 
by arbitrarily retaining some characters of this root 
word and deleting others. For example, the word 
?
@? has six possible abbreviated forms: ? ?, 
? ?, ?@?, ?
?, ? @? and ? @?. In general, 
if we have a root word of length L, there could be 
2
2 ?
L
possible abbreviations for this root word 
?excluding the word itself and the null string?. 
The best possible abbreviation form 
*
c
for an 
input word 
i
w
can be determined as the one with 
the highest generation probability 
?
?
|
i
i
P c w
, i.e., 
?
?
*
argmax
|
i
i
c
c
P c w
=
. 
The 
generation 
probability for a candidate 
i
c
, in turn, can be 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 23 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
estimated by summing up all probabilities of 
alignments between each character 
i j
c
in 
i
c
and the suspect constituent words 
i j
w
in 
i
w
. In 
other words, we have 
?
?
?
?
?
?
all alignments
|
,A |
|
i
i
i
i
A
A
i
i
A
P c w
P c
w
P c w
?
=
??
?
?
where 
?
?
|
A
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 24 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
i
i
P c w
is the generation probability for a 
known alignment A, which can be estimated as in 
Equation 2. 
For simplicity, we assume that each character 
in 
i
c
will be mapped to a substring 
i j
w
in 
i
w
. 
In other words, we assume that the mapping 
between the constituents is 1-1, and no 1-0 or 0-1 
mapping is possible. ?In future works, such a 
constraint could be removed.? Also, we will 
assume that 
i j
w
should at least contain the 
character that is aligned to it. ?This is not always 
true for Chinese abbreviations. For example, ?
? can be abbreviated with its ancient location 
name ? ?, which does not appear anywhere in its 
root.? 
There is also a normalization issue in 
computing the probability of a particular alignment. 
In general, a shorter string may be preferred as the 
best abbreviation simply because it multiplies less
probability factors when estimating the alignment
probability. To reduce this effect, we intentionally 
scale down, by a normalization factor, the 
generation probabilities for those alignments that 
map a complete word into a single character. In 
fact, there are only about 10% of such alignments, 
and many of which are mapping a two-character
word into a single character ?which can be 
compensated by the large Pr?1|2? factor in the
model. This simple normalization approach 
actually improves the test set performance greatly. 
The following table shows the test set
performance for using different features in the 
abbreviation probability as given in Equation 3. 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 25 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
?The training set performance ranges from 94% to 
98%, which suggests a good fit to the training 
data.? 
Feature 
Unsupervised Supervised 
P?c|w?xP?wi|wi-1? 1 1 1 1 1 1 1 1 
P?bit|n? 
1 1 0 0 1 1 0 0 
P?m|n? 
1 0 1 0 1 0 1 0 
Accuracy 
Rate?%? 
68 68 61 60 72 70 61 58
Table 3. Test Set Performance for 
Abbreviation Generation with Combined 
Features. 
Each column shows the test set performance for 
a submodel, which is identified by the features 
used for estimating the probability. The label ?1?
?or ?0?? indicates that the feature at the first 
column is used ?or unused? in the submodel. For 
instance, the submodel of the second column 
??111?? uses all the features, including the word 
transition 
probability, 
word-to-abbreviation 
probability, probability for mapping n character 
word to a particular abbreviation bit pattern 
P?bit|n?, and the probability for mapping 
n-character words into m-character abbreviations. 
It is seen that supervised training acquires a 
little better performance than its unsupervised ?EM? 
counterpart. Although not shown in this table, it is 
observed that the word transition probability and 
word-to-abbreviation probability in general should 
be used to get better performance. The table also 
shows that the other two features based on 
character positions and word lengths provide 
additional help. In particular, P?bit|n? seems to be
more helpful than P?m|n? since it contains detailed 
information for retaining characters at particular
positions. 
The best performance is about 72% when 
supervised training is used and all the three types 
of features are used for estimating the abbreviation 
probability. 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 26 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
Page 7
4.2 Guessing Roots from Abbreviations 
In this experiment, we are given an 
abbreviation list; the goal is to guess the best root
words of the abbreviations in the list. The 
parameters used here are acquired from human 
tagged alignments in a supervised manner. 
To find the best root candidates of an 
abbreviated compound word, we need to find the 
candidate root words for each input character first. 
The candidate root words can be found from the 
training set whose generation probability 
?
?
i
i
w
c
P |
is non-zero. The Top-N candidates can 
then be picked up as described earlier. 
For instance, if we want to find the root words 
of the abbreviation ?
?, and the probabilities 
P? |
? and P? |
? are non-zero, then we 
have the chance to recover the abbreviation ?
? back to the correct compound word ?
? , which consists of the candidate root words 
?
? and ?
? for the input characters ? ? 
and ? ? respectively. 
Unfortunately, the limited abbreviation 
dictionary we have is highly sparse. Among the 
249 abbreviations in the test set, only 144 ?58%? of 
them have their candidate root words available in 
the training set. The other 105 abbreviations ?42%? 
cannot be recovered since each of them has at least
one character whose candidate cannot be 
discovered from the training set. For this reason,
we will limit ourselves to the performance of the 
?trainable? test set consisting of the 144 
abbreviations, in order to factor out the sparseness
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 27 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
problem pertaining to the training corpus. 
Under such a restricted environment, we have 
tested various submodels to see how different 
language models and simple smoothing affect the 
results of this error recovery process. The results 
are summarized in the following table: 
LM 
SM? Top-N TR?%? TS?%? Best? 
bigram No 
all 
90.9 
35 
2 
69.6 
43 
2 
1 
46.0 
45 
1 
unigram No 
all 
44.2 
44 
bigram Yes 
all 
90.7 
51 
1 
2 
69.6 
51 
1 
1 
46.0 
45 
Table 4. Abbreviation Recovery Performance. 
Notations: LM: Language Model, SM?: Apply 
Smoothing?, Top-N: maximum number of Top-N 
candidate root words for each character, TR: 
Training Set Accuracy Rate, TS: Test Set 
Accuracy, Best?: Best TS Performance among all 
N?s? ?1 = yes, 2= rank 2? 
The bigram language model uses 
?
?
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 28 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
1
|
?i
i
w
w
P
in the unified HMM model while the unigram
model uses 
? ?
i
w
P
instead. Both of them use 
maximum likelihood estimation over the manually 
tagged abbreviation-root pairs when smoothing is 
not applied. When smoothing is applied, the 
smoothed bigram probability is acquired by 
linearly interpolating the unigram and bigram
probabilities with an equal weight ?0.5?. The above 
table indicates that using the less complicated 
unigram model generally improve the test set 
performance significantly ?from 35% to 44%?. If 
the model parameters are smoothed, the 
improvement is even greater. Such results can be 
well expected in the current environment where 
the training data is very sparse. 
Overall, the best test set performance is about
51% when using a smoothed bigram language
model; and this can be achieved by using at most 2
Top-N candidate root words while constructing the 
underlying word lattice. This suggests that we 
don?t really need to wildly enumerate all possible 
candidate root words for each input character with
this model. 
5. Concluding Remarks 
Chinese abbreviations, a special form of 
unknown words and named entities, are widely 
seen in the modern Chinese texts. This results in 
difficulty for correct Chinese processing. In this 
preliminary study, the Chinese abbreviation 
problem is modeled as an error recovery problem
in which the suspect root words are to be
recovered from a set of candidates. An 
HMM-based model is thus used for Chinese in
either abbreviation identification, or in the 
recovery of the root words from an abbreviation. 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 29 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
By extending a simple abbreviation string into a
whole text involving abbreviations, it can also be 
applied to the Chinese word segmentation for 
identifying abbreviations in a text, or for 
bootstrapping an abbreviation dictionary from a 
text corpus. 
With the proposed model, the abbreviated 
forms can be guessed from root words at about 
72% correction. The recovery of the root words 
from abbreviations is conducted at about 51% 
accuracy rate. Although further improvement is 
possible, the preliminary results are encouraging. 
In the near future, bootstrapping a large 
abbreviation dictionary from web text by applying 
the proposed models is planned. This should 
partially resolve the data sparseness problems. 
Such models will also be integrated into a Chinese 
Page 8
word segmentation model to partially resolve the 
unknown word and named entity identification 
problems in the tokenization process. It is expected 
that more applications will rely on such models for 
Chinese processing. 
References 
Chang, Jing-Shin and Keh-Yih Su, 1997. ?An 
Unsupervised Iterative Method for Chinese 
New Lexicon Extraction?, International Journal 
of Computational Linguistics and Chinese
Language Processing (CLCLP), 2?2?: 97-148. 
Chiang, Tung-Hui, Jing-Shin Chang, Ming-Yu Lin 
and Keh-Yih Su, 1992. ?Statistical Models for 
Word Segmentation and Unknown Word 
Resolution,? Proceedings of ROCLING-V, 
pages 123-146, Taipei, Taiwan, ROC. 
Dempster, A. P., N. M. Laird, and D. B. Rubin, 
1977. ?Maximum Likelihood from Incomplete 
Data via the EM Algorithm?, Journal of the 
Royal Statistical Society, 39 ?b?: 1-38. 
Gao, Jianfeng, Mu Li, Chang-Ning Huang, 2003. 
?Improved Source-Channel Models for Chinese 
Word Segmentation,? Proc. ACL 2003, pages 
272-279. 
Huang, Chu-Ren, Kathleen Ahrens, and Keh-Jiann 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 30 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
Chen, 1994a. ?A data-driven approach to 
psychological reality of the mental lexicon: Two 
studies on Chinese corpus linguistics.? In
Language and its Psychobiological Bases, 
Taipei. 
Huang, Chu-Ren, Wei-Mei Hong, and Keh-Jiann 
Chen, 1994b. ?Suoxie: An information based 
lexical rule of abbreviation.? In Proceedings of 
the Second Pacific Asia Conference on Formal 
and Computational Linguistics II, pages 49?52,
Japan. 
Katz, Slava M., 1987. ?Estimation of Probabilities 
from Sparse Data for the Language Model 
Component of a Speech Recognizer,? IEEE 
Trans. ASSP-35 (3). 
Lai, Yu-Tso, 2003. A Probabilistic Model for 
Chinese Abbreviations, Master Thesis, National 
Chi-Nan University, ROC. 
Lin, Ming-Yu, Tung-Hui Chiang and Keh-Yih Su, 
1993. ?A Preliminary Study on Unknown Word 
Problem in Chinese Word Segmentation,? 
Proceedings of ROCLING VI, pages 119-142. 
Rabiner, L., and B.-H., Juang, 1993. Fundamentals 
of Speech Recognition, Prentice-Hall. 
Sun, Jian, Jianfeng Gao, Lei Zhang, Ming Zhou 
and Chang-Ning Huang, 2002. ?Chinese named 
entity identification using class-based language 
model,? Proc. of COLING 2002, Taipei, ROC. 
Sproat, Richard, 2002. ?Corpus-Based Methods in
Chinese 
Morphology?, 
Pre-conference 
Tutorials, COLING-2002, Taipei, Taiwan, 
ROC. 
25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 31 -
shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 17?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Mining Atomic Chinese Abbreviation Pairs: A Probabilistic Model for
Single Character Word Recovery
Jing-Shin Chang
Department of Computer Science &
Information Engineering
National Chi-Nan University
Puli, Nantou, Taiwan, ROC.
jshin@csie.ncnu.edu.tw
Wei-Lun Teng
Department of Computer Science &
Information Engineering
National Chi-Nan University
Puli, Nantou, Taiwan, ROC.
S3321512@ncnu.edu.tw
Abstract
An HMM-based Single Character
Recovery (SCR) Model is proposed in
this paper to extract a large set of
?atomic abbreviation pairs?from a large
text corpus. By an ?atomic abbreviation
pair,?it refers to an abbreviated word
and its root word (i.e., unabbreviated
form) in which the abbreviation is a
single Chinese character.
This task is interesting since the
abbreviation process for Chinese
compound words seems to be
?compositional?; in other words, one
can often decode an abbreviated word,
such as ????(Taiwan University),
character-by-character back to its root
form. With a large atomic abbreviation
dictionary, one may be able to recover
multiple-character abbreviations more
easily.
With only a few training iterations, the
acquisition accuracy of the proposed
SCR model achieves 62% and 50 %
precision for training set and test set,
respectively, from the ASWSC-2001
corpus.
1 Introduction
Chinese abbreviations are widely used in the
modern Chinese texts. They are a special form
of unknown words, which cannot be
exhaustively enumerated in an ordinary
dictionary. Many of them originated from
important lexical units such as named entities.
However, the sources for Chinese abbreviations
are not solely from the noun class, but from most
major categories, including verbs, adjectives
adverbs and others. No matter what lexical or
syntactic structure a string of characters could be,
one can almost always find a way to abbreviate
it into a shorter form. Therefore, it may be
necessary to handle them beyond a class-based
model. Furthermore, abbreviated words are
semantically ambiguous. For example, ????
can be the abbreviation for??????or???
???; on the opposite direction, multiple
choices for abbreviating a word are also possible.
For instance,??????may be abbreviated as
????, ????or ?????. This results in
difficulty for correct Chinese processing and
applications, including word segmentation,
information retrieval, query expansion, lexical
translation and much more. An abbreviation
model or a large abbreviation lexicon is
therefore highly desirable for Chinese language
processing.
Since the smallest possible Chinese lexical
unit into which other words can be abbreviated
is a single character, identifying the set of
multi-character words which can be abbreviated
into a single character is especially interesting.
Actually, the abbreviation of a compound word
can often be acquired by the principle of
composition. In other words, one can decompose
a compound word into its constituents and then
concatenate their single character equivalents to
form its abbreviated form. The reverse process
to predict the unabbreviated form from an
abbreviation shares the same compositional
property.
The Chinese abbreviation problem can be
regarded as an error recovery problem in which
the suspect root words are the ?errors? to be 
recovered from a set of candidates. Such a
problem can be mapped to an HMM-based
generation model for both abbreviation
identification and root word recovery; it can also
17
be integrated as part of a unified word
segmentation model when the input extends to a
complete sentence. As such, we can find the
most likely root words, by finding those
candidates that maximizes the likelihood of the
whole text. An abbreviation lexicon, which
consists of the root-abbreviation pairs, can thus
be constructed automatically.
In a preliminary study (Chang and Lai, 2004),
some probabilistic models had been developed
to handle this problem by applying the models to
a parallel corpus of compound words and their
abbreviations, without knowing the context of
the abbreviation pairs. In this work, the same
framework is extended and a method is proposed
to automatically acquire a large abbreviation
lexicon for indivisual characters from web texts
or large corpora, instead of building abbreviation
models based on aligned abbreviation pairs of
short compound words. Unlike the previous task,
which trains the abbreviation model parameters
from a list of known abbreviation pairs, the
current work aims at extracting abbreviation
pairs from a corpus of free text, in which the
locations of prospective abbreviations and full
forms are unknown and the correspondence
between them is not known either.
In particular, a Single Character Recovery
(SCR) Model is exploited in the current work to
extract ?atomic abbreviation pairs?from a large
text corpus. With only a few training iterations,
the acquisition accuracy achieves 62% and 50 %
precision for training set and test set from the
ASWSC-2001 corpus.
1.1 Chinese Abbreviation Problems
The modern Chinese language is a highly
abbreviated one due to the mixed uses of ancient
single character words as well as modern
multi-character words and compound words.
The abbreviated form and root form are used
interchangeably everywhere in the current
Chinese articles. Some news articles may
contain as high as 20% of sentences that have
suspect abbreviated words in them (Lai, 2003).
Since abbreviations cannot be enumerated in a
dictionary, it forms a special class of unknown
words, many of which originate from named
entities. Many other open class words are also
abbreviatable. This particular class thus
introduces complication for Chinese language
processing, including the fundamental word
segmentation process (Chiang et al, 1992; Lin
et al, 1993; Chang and Su, 1997) and many
word-based applications. For instance, a
keyword-based information retrieval system may
requires the two forms, such as ?????and
???????(?Academia Sinica?), in order not
to miss any relevant documents. The Chinese
word segmentation process is also significantly
degraded by the existence of unknown words
(Chiang et al, 1992), including unknown
abbreviations.
There are some heuristics for Chinese
abbreviations. Such heuristics, however, can
easily break (Sproat, 2002). Unlike English
abbreviations, the abbreviation process of the
Chinese language is a very special word
formation process. Almost all characters in all
positions of a word can be omitted when used
for forming an abbreviation of a compound word.
For instance, it seems that, by common
heuristics, ?most?Chinese abbreviations could
be derived by keeping the first characters of the
constituent words of a compound word, such as
transforming ??????into ????, ????
??into ????and ????(?)?????into
????. Unfortunately, it is not always the case.
For example, we can transform ??????into
????, ??????into ????, and, for very
long compounds like ????????into ??
???(Sproat, 2002). Therefore, it is very
difficult to predict the possible surface forms of
Chinese abbreviations and to guess their base
(non-abbreviated) forms heuristically.
P(bit|n) Score Examples
P(10|2) 0.87 (?|??),(?|??)
P(101|3) 0.44 (??|???),
(??|???)
P(1010|4) 0.56 (??|????),
(??|????)
P(10101|5) 0.66 (???|?????),
(???|?????)
P(101001|6) 0.51 (???|??????),
(???|??????)
P(1010001|7) 0.55 (???|???????),
(???|???????)
P(10101010|8) 0.21 (????|???????
?),
( ????|???????
?)
Table 1. High Frequency Abbreviation Patterns
[by P(bit|n)] (Chang and Lai, 2004)
18
The high frequency abbreviation patterns
revealed in (Chang and Lai, 2004) further break
the heuristics quantitatively. Table 1 lists the
distribution of the most frequent abbreviation
patterns for word of length 2~8 characters.
The table indicates which characters will be
deleted from the root of a particular length (n)
with a bit ?0?; on the other hand, a bit ?1? means
that the respective character will be retained.
This table does support some general heuristics
for native Chinese speaker quantitatively. For
instance, there are strong supports that the first
character in a two-character word will be
retained in most cases, and the first and the third
characters in a 4-character word will be retained
in 56% of the cases. However, the table also
shows that around 50% of the cases cannot be
uniquely determined by character position
simply by consulting the word length of the
un-abbreviated form. This does suggest the
necessity of either an abbreviation model or a
large abbreviation lexicon for resolving this kind
of unknown words and named entities.
There are also a large percentage (312/1547)
of ?tough? abbreviation paterns (Changand Lai,
2004), which are considered ?tough?in the sense
that they violate some simple assumptions, and
thus cannot be modeled in a simple way. For
instance, some tough words will actually be
recursively abbreviated into shorter and shorter
lexical forms; and others may change the word
order (as in abbreviating ?????????as
?????instead of ?????.). As a result, the
abbreviation process is much more complicated
than a native Chinese speaker might think.
1.2 Atomic Abbreviation Pairs
Since the abbreviated words are created
continuously through the abbreviation of new
(mostly compound) words, it is nearly
impossible to construct a complete abbreviation
lexicon. In spite of the difficulty, it is interesting
to note that the abbreviation process for Chinese
compound words seems to be ?compositional?.
In other words, one can often decode an
abbreviated word, such as ????(?Taiwan
University?), character-by-character back to its
root form ??????by observing that ???
can be an abbreviation of ????and ???can
be an abbreviation of ????and??????is
a frequently observed character sequence.
Since character is the smallest lexical unit for
Chinese, no further abbreviation into smaller
units is possible. We therefore use ?atomic
abbreviation pair?to refer to an abbreviated
word and its root word (i.e., unabbreviated form)
in which the abbreviation is a single Chinese
character.
On the other hand, abbreviations of
multi-character compound words may be
synthesized from single characters in the
?atomic abbreviation pairs?. If we are able to
identify all such ?atomic abbreviation pairs?,
where the abbreviation is a single character, and
construct such an atomic abbreviation lexicon,
then resolving multiple character abbreviation
problems, either by heuristics or by other
abbreviation models, might become easier.
Furthermore, many ancient Chinese articles
are composed mostly of single-character words.
Depending on the percentage of such
single-character words in a modern Chinese
article, the article will resemble to an ancient
Chinese article in proportional to such a
percentage. As another application, an effective
single character recovery model may therefore
be transferred into an auxiliary translation
system from ancient Chinese articles into their
modern versions. This is, of course, an overly
bold claim since lexical translation is not the
only factor for such an application. However, it
may be consider as a possible direction for
lexical translation when constructing an
ancient-to-modern article translation system.
Also, when a model for recovering atomic
translation pair is applied to the?single character
regions?of a word segmented corpus, it is likely
to recover unknown abbreviated words that are
previously word-segmented incorrectly into
individual characters.
An HMM-based Single Character Recovery
(SCR) Model is therefore proposed in this paper
to extract a large set of ?atomic abbreviation
pairs?from a large text corpus.
1.3 Previous Works
Currently, only a few quantitative approaches
(Huang et al, 1994a; Huang et al, 1994b) are
available in predicting the presence of an
abbreviation. There are essentially no prior arts
for automatically extracting atomic abbreviation
pairs. Since such formulations regard the word
segmentation process and abbreviation
19
identification as two independent processes, they
probably cannot optimize the identification
process jointly with the word segmentation
process, and thus may lose some useful
contextual information. Some class-based
segmentation models (Sun et al, 2002; Gao et
al., 2003) well integrate the identification of
some regular non-lexicalized units (such as
named entities). However, the abbreviation
process can be applied to almost all word forms
(or classes of words). Therefore, this particular
word formation process may have to be handled
as a separate layer in the segmentation process.
To resolve the Chinese abbreviation
problems and integrate its identification into the
word segmentation process, (Chang and Lai,
2004) proposes to regard the abbreviation
problem in the word segmentation process as an
?eror recovery? problem in which the suspect 
root words are the ?erors? to be recovered from 
a set of candidates according to some generation
probability criteria. This idea implies that an
HMM-based model for identifying Chinese
abbreviations could be effective in either
identifying the existence of an abbreviation or
the recovery of the root words from an
abbreviation.
Since the parameters of an HMM-like model
can usually be trained in an unsupervised
manner, and the ?output probabilities? known to 
the HMM framework will indicate the likelihood
for an abbreviation to be generated from a root
candidate, such a formulation can easily be
adapted to collect highly probable
root-abbreviation pairs. As a side effect of using
HMM-based formulation, we expect that a large
abbreviation dictionary could be derived
automatically from a large corpus or from web
documents through the training process of the
unified word segmentation model.
In this work, we therefore explore the
possibility of using the theories in (Chang and
Lai, 2004) as a framework for constructing a
large abbreviation lexicon consisting of all
Chinese characters and their potential roots. In
the following section, the HMM models as
outlined in (Chang and Lai, 2004) is reviewed
first. We then described how to use this
framework to construct an abbreviation lexicon
automatically. In particular, a Single Character
Recovery (SCR) Model is exploited for
extracting possible root (un-abbreviated) forms
for all Chinese characters.
2 Chinese Abbreviation Models
2.1 Unified Word Segmentation Model for
Abbreviation Recovery
To resolve the abbreviation recovery problem,
one can identify some root candidates for
suspect abbreviations (probably from a large
abbreviation dictionary if available or from an
ordinary dictionary with some educated guesses),
and then confirm the most probable root by
consulting local context. This process is
identical to the operation of many error
correction models, which generate the candidate
corrections according to a reversed word
formation process, then justify the best
candidate.
Such an analogy indicates that we may use an
HMM model (Rabiner and Juang, 1993), which
is good at finding the best unseen state sequence,
for root word recovery. There will be a direct
map between the two paradigms if we regard the
observed input character sequence as our
?observation sequence?, and regard the unseen 
word candidates as the underlying ?state 
sequence?.
To integrate the abbreviation process into the
word segmentation model, firstly we can regard
the segmentation model as finding the best
underlying words m
m www ,,11 ?? (which
include only base/root forms), given the surface
string of characters n
n ccc ,,11 ?? (which may
contain abbreviated forms of compound words.)
The segmentation process is then equivalent to
finding the best (un-abbreviated) word
sequence *w? such that:
? ?
? ? ? ?
? ? ? ??
?
?
?
?
?
?
??
??
?
ii
nmm
nmm
nmm
cw
mi
iiii
cww
mmn
cww
nm
cww
wwPwcP
wPwcP
cwPw
?
?
?
,1
1
:
111
:
11
:
||maxarg
|maxarg
|maxarg*
111
111
111
Equation 1. Unified Word Segmentation
Model for Abbreviation Recovery
20
where ic
?
refers to the surface form of iw ,
which could be in an abbreviated or
non-abbreviated root form of iw . The last
equality assumes that the generation of an
abbreviation is independent of context, and the
language model is a word-based bigram model.
If no abbreviated words appears in real text,
such that all surface forms are identical to their
?root? forms, we will have ? ?| 1i iP c w ?? ,
1,i m?? , and Equation 1 is simply a word
bigram model for word segmentation (Chiang et
al., 1992). In the presence of abbreviations,
however, the generation probability ? ?ii wcP |?
can no longer be ignored, since the probability
? ?ii wcP |? is not always 1 or 0.
As an example, if two consecutive ic
?
are
???and ???then their roots, iw , could be ??
??plus ???? (Taiwan University) or ????
plus ????? (Taiwan Major League). In this 
case, the parameters in P(??|??) x P(?|?
?) x P(?|??) and P(???|??) x P(?|?
?) x P(?|???) wil indicate how likely ??
?? is an abbreviation, and which of the above 
two compounds is the root form.
Notice that, this equation is equivalent to an
HMM (Hidden Markov Model) (Rabiner and
Juang, 1993) normally used to find the best
?state? sequence given the observation symbols. 
The parameters ? ?1| ?ii wwP and ? ?ii wcP |?
represent the transition probability and the
(word-wise) output probability of an HMM,
respectively; and, the formulations for ? ?mwP 1
and ? ?mn wcP 11 | are the respective ?language 
model? of the Chinese language and the
?generation model? for the abbreviated words
(i.e., the ?abbreviation model?in the current
context). The ?state? sequence in this case is
characterized by the hidden root forms
m
m www ,,11 ?? ; and, the ?observation 
symbols? are characterized by
mn
n ccccc ???? ,,,, 111 ?? , where the surface
form ??
??ie
ibi cc ?
?
is a chunk of characters
beginning at the b(i)-th character and ending at
the e(i)-th character.
The word-wise transition probability
? ?1| ?ii wwP in the language model is used to
provide contextual constraints among root words
so that the underlying word sequence forms a
legal sentence with high probability.
Notice that, after applying the word
segmentation model Equation 1 to the word
lattice, some of the above candidates may be
preferred and others be discarded, by consulting
the neighboring words and their transition
probabilities. This makes the abbreviation model
jointly optimized in the word segmentation
process, instead of being optimized independent
of context.
2.2 Simplified Abbreviation Models
Sometimes, it is not desirable to use a
generation probability that is based on the
root-abbreviation pairs, since the number of
parameters will be huge and estimation error due
to data sparseness might be high. Therefore, it is
desirable to simplify the abbreviation probability
by using some simpler features in the model. For
instance, many 4-character compound words are
abbreviated as 2-character abbreviations (such as
in the case for the <????, ??> pair.) It
was also known that many such 4-character
words are abbreviated by preserving the first and
the third characters, which can be represented by
a ?1010? bit patern, where the ?1? or ?0?means
to preserve or delete the respective character.
Therefore, a reasonable simplification for the
abbreviation model is to introduce the length and
the positional bit pattern as additional features,
resulting in the following augmented model for
the abbreviation probability.
? ? 1 1
1 1
| ( , , | , )
( | ) ( | ) ( | )
m n
m n
P c w P c bit m r n
P c r P bit n P m n
?
? ? ?
?
1
1
: surface characters.
: root word characters.
where : length of surface characters.
: length of root word characters.
bit: bit pattern of abbreviation
m
n
c
r
m
n
?
?
??
?
?
?
??
Equation 2. Abbreviation Probability using
Abbreviation Pattern and Length Features.
All these three terms can be combined freely
to produce as many as 7 sub-models for the
21
abbreviation model. Note, the first term
? ?nm rc 11 |Pr plays the same role as the older
notation of ? ?wc |Pr ? . To use the simple length
and position features, this term can be unused in
the above augmented abbreviation model.
3 The Single Character Recovery (SCR)
Model
As mentioned earlier, many multiple
character words are frequently abbreviated into a
single Chinese character. Compound words
consisting of a couple of such multiple character
words are then abbreviated by concatenating all
the single character abbreviations. This means
that those N-to-1 abbreviation patterns may form
the basis for the underlying Chinese abbreviation
process. The other M-to-N abbreviation patterns
might simply be a composition of such basic
N-to-1 abbreviations. The N-to-1 abbreviation
patterns can thus be regarded as the atomic
abbreviation pairs.
Therefore, it is interesting to apply the
abbreviation recovery model to acquire all basic
N-to-1 abbreviation patterns, in the first place,
so that abbreviations of multi-character words
can be detected and predicted more easily.
Such a task can be highly simplified if each
character in a text corpus is regarded as an
abbreviated word whose root form is to be
recovered. In other words, the surface form ic
?
in Equation 1 is reduced to a single character.
The abbreviation recovery model based on this
assumption will be referred to as the SCR
Model.
The root candidates for each single character
will form a word lattice, and each path of the
lattice will represent a non-abbreviated word
sequence. The underlying word sequence that is
most likely to produce the input character
sequence will then be identified as the best word
sequence. Once the best word sequence is
identified, the model parameters can be
re-estimated. And the best word sequence is
identified again. Such process is repeated until
the best sequence no more changes. In addition,
the corresponding <root, abbreviation> pairs will
be extracted as atomic abbreviation pairs, where
all the abbreviations are one character in size.
While it is overly simplified to use this SCR
model for conducting a general abbreviation
enhanced word segmentation process (since not
all single characters are abbreviated words), the
single character assumption might still be useful
for extracting roots of real single-character
abbreviations. The reason is that one only care to
use the contextual constraints around a true
single character abbreviation for matching its
root form against the local context in order to
confirm that the suspect root did conform to its
neighbors (with a high language model score).
An alternative to use a two-stage recovery
strategy, where the first stage applies a baseline
word segmentation model to identify most
normal words and a second stage to identify and
recover incorrectly segmented single characters,
is currently under investigation with other modes
of operations. For the present, the SCR model is
tested first as a baseline.
The HMM-based recovery models enables us
to estimate the model parameters using an
unsupervised training method that is directly
ported from the Baum-Welch re-estimation
formula (Rabiner and Juang, 1993) or a generic
EM algorithm (Dempster et al, 1977). Upon
convergence, we should be able to acquire a
large corpus of atomic abbreviation pairs from
the text corpus.
If a word-segmented corpus is available, we
can also use such re-estimation methods for
HMM parameter training in an unsupervised
manner, but with initial word transition
probabilities estimated in a supervised manner
from the seed.
The initial candidate <root, abbreviation>
pairs are generated by assuming that all
word-segmented words in the training corpus are
potential roots for each of its single-character
constituents. For example, if we have ????
and????as two word-segmented tokens, then
the abbreviation pairs <?, ??>, <?, ??>,
<?, ??> and <?, ??> will be generated.
Furthermore, each single character by default is
its own abbreviation.
To estimate the abbreviation probabilities,
each abbreviation pair is associated with a
frequency count of the root in the word
segmentation corpus. This means that each
single-character abbreviation candidate is
22
equally weighted. The equal weighting strategy
may not be absolutely true (Chang and Lai,
2004). In fact, the character position and word
length features may be helpful as mentioned in
Equation 2. The initial probabilities are
therefore weighted differently according to the
position of the character and the length of the
root. The weighting factors are directly acquired
from a previous work in (Chang and Lai, 2004).
Before the initial probabilities are estimated,
Good-Turning smoothing (Katz, 1987) is applied
to the raw frequency counts of the abbreviation
pairs.
4 Experiments
To evaluate the SCR Model, the Academia
Sinica Word Segmentation Corpus (dated July,
2001), ASWSC-2001 (CKIP, 2001), is adopted
for parameter estimation and performance
evaluation. Among the 94 files in this balanced
corpus, 83 of them (13,086KB) are randomly
selected as the training set and 11 of them
(162KB) are used as the test set.
Table 3 shows some examples of atomic
abbreviation pairs acquired from the training
corpus. The examples here partially justify the
possibility to use the SCR model for acquiring
atomic abbreviation pairs from a large corpus.
The iterative training process converges
quickly after 3~4 iterations. The numbers of
unique abbreviation patterns for the training and
test sets are 20,250 and 3,513, respectively.
Since the numbers of patterns are large, a rough
estimate on the acquisition accuracy rates is
conducted by randomly sampling 100 samples of
the <root, abbreviation> pairs. The pattern is
then manually examined to see if the root is
correctly recovered. The precision is estimated
as 50% accuracy for the test set, and 62% for the
training set on convergence. Although a larger
sample may be necessary to get a better estimate,
the preliminary result is encouraging. Figures
1~4 demonstrate the curve of convergence for
the iterative training process in terms of pattern
number and accuracy.
SCR Model Training Set ??pattern??
20307
20252 20250 20250
20220
20240
20260
20280
20300
20320
1 2 3 4
????
Figure 1. Numbers of abbreviation patterns in
each iteration. (Training Set)
SCR Model Training Set???
29%
45%
62% 62%
0%
20%
40%
60%
80%
1 2 3 4
????
Figure 2. Acquisition accuracy for each iteration.
(Training Set)
SCR Model Test Set ??pattern??
3521
3518
3515
3513
3512
3513
3514
3515
3516
3517
3518
3519
3520
3521
3522
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5
????
Figure 3. Numbers of abbreviation patterns in
each iteration. (Test Set)
Abbr : Root Example Abbr : Root Example
?:?? ?? ?:?? ??
?:?? ?? ?:?? ??
?:?? ?? ?:?? ??
?:?? ?? ?:?? ??
?:?? ?? ?:?? ???
?:?? ?? ?:??? ??
?:?? ?? ?:?? ??
?:?? ?? ?:?? ???
?:?? ?? ?:?? ??
?:?? ?? ?:?? ???
?:?? ?? ?:?? ???
?:?? ?? ?:?? ??
?:?? ??? ?:??? ??
?:?? ?? ?:?? ??
?:?? ?? ?:?? ??
Table 3. Examples of atomic abbreviation pairs.
23
SCR Model Test Set ???
20% 22%
33%
50%
0%
10%
20%
30%
40%
50%
60%
1 2 3 4
????
Figure 4. Acquisition accuracy for each iteration.
(Test Set)
5 Concluding Remarks
Chinese abbreviations, which form a special
kind of unknown words, are widely seen in the
modern Chinese texts. This results in difficulty
for correct Chinese processing. In this work, we
had applied a unified word segmentation model
developed in a previous works (Chang and Lai,
2004), which was able to handle the kind of
?erors? introduced by the abbreviation process. 
An iterative training process is developed to
automatically acquire an abbreviation dictionary
for single-character abbreviations from large
corpora. In particular, a Single Character
Recovery (SCR) Model is exploited. With only a
few training iterations, the acquisition accuracy
achieves 62% and 50 % precision for training set
and test set from the ASWSC-2001 corpus. For
systems that choose to lexicalize such lexicon
entities, the automatically constructed
abbreviation dictionary will be an invaluable
resource to the systems. And, the proposed
recovery model looks encouraging.
References
Chang, Jing-Shin and Keh-Yih Su, 1997. ?An
Unsupervised Iterative Method for Chinese New
Lexicon Extraction?, International Journal of
Computational Linguistics and Chinese Language
Processing (CLCLP), 2(2): 97-148.
Chang, Jing-Shin and Yu-Tso Lai, 2004. ?A 
Preliminary Study on Probabilistic Models for
Chinese Abbreviations.? Proceedings of the Third
SIGHAN Workshop on Chinese Language
Learning, pages 9-16, ACL-2004, Barcelona,
Spain.
Chiang, Tung-Hui, Jing-Shin Chang, Ming-Yu Lin
and Keh-Yih Su, 1992. ?Statistical Models for
Word Segmentation and Unknown Word
Resolution,?Proceedings of ROCLING-V, pages
123-146, Taipei, Taiwan, ROC.
CKIP 2001, Academia Sinica Word Segmentation
Corpus, ASWSC-2001, (??????????),
Chinese Knowledge Information Processing Group,
Acdemia Sinica, Tiapei, Taiwan, ROC.
Dempster, A. P., N. M. Laird, and D. B. Rubin, 1977.
?Maximum Likelihood from Incomplete Data via 
the EM Algorithm?, Journal of the Royal
Statistical Society, 39 (b): 1-38.
Gao, Jianfeng, Mu Li, Chang-Ning Huang, 2003.
?Improved Source-Channel Models for Chinese
Word Segmentation,? Proc. ACL 2003, pages
272-279.
Huang, Chu-Ren, Kathleen Ahrens, and Keh-Jiann
Chen, 1994a. ?A data-driven approach to
psychological reality of the mental lexicon: Two
studies on Chinese corpus linguistics.? In 
Language and its Psychobiological Bases, Taipei.
Huang, Chu-Ren, Wei-Mei Hong, and Keh-Jiann
Chen, 1994b. ?Suoxie: An information based
lexical rule of abbreviation.? In Proceedings of the
Second Pacific Asia Conference on Formal and
Computational Linguistics II, pages 49?52, Japan.
Katz, Slava M., 1987. ?Estimation of Probabilities
from Sparse Data for the Language Model
Component of a Speech Recognizer,?IEEE Trans.
ASSP-35 (3).
Lai, Yu-Tso, 2003. A Probabilistic Model for
Chinese Abbreviations, Master Thesis, National
Chi-Nan University, ROC.
Lin, Ming-Yu, Tung-Hui Chiang and Keh-Yih Su,
1993. ?A Preliminary Study on Unknown Word
Problem in Chinese Word Segmentation,?
Proceedings of ROCLING VI, pages 119-142.
Rabiner, L., and B.-H., Juang, 1993. Fundamentals of
Speech Recognition, Prentice-Hall.
Sun, Jian, Jianfeng Gao, Lei Zhang, Ming Zhou and
Chang-Ning Huang, 2002. ?Chinese named entity 
identification using class-based language model,? 
Proc. of COLING 2002, Taipei, ROC.
Sproat, Richard, 2002. ?Corpus-Based Methods in
Chinese Morphology?, Pre-conference Tutorials,
COLING-2002, Taipei, Taiwan, ROC.
24
