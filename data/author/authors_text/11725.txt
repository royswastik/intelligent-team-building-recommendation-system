Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 69?77,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Reordering Model Using Syntactic Information of a Source Tree for
Statistical Machine Translation
Kei Hashimoto?1, Hirohumi Yamamoto?2?3, Hideo Okuma?2?4,
Eiichiro Sumita?2?4, and Keiichi Tokuda?1?2
?1Nagoya Institute of Technology Department of Computer Science and Engineering
/ Gokiso-cho Syouwa-ku Nagoya-city Aichi Japan
?2National Institute of Information and Communications Technology
?3Kinki University School of Science and Engineering Department of Informaiton
?4ATR Spoken Language Communication Research Labs.
Abstract
This paper presents a reordering model us-
ing syntactic information of a source tree for
phrase-based statistical machine translation.
The proposed model is an extension of IST-
ITG (imposing source tree on inversion trans-
duction grammar) constraints. In the pro-
posed method, the target-side word order is
obtained by rotating nodes of the source-side
parse-tree. We modeled the node rotation,
monotone or swap, using word alignments
based on a training parallel corpus and source-
side parse-trees. The model efficiently sup-
presses erroneous target word orderings, espe-
cially global orderings. Furthermore, the pro-
posed method conducts a probabilistic evalu-
ation of target word reorderings. In English-
to-Japanese and English-to-Chinese transla-
tion experiments, the proposed method re-
sulted in a 0.49-point improvement (29.31 to
29.80) and a 0.33-point improvement (18.60
to 18.93) in word BLEU-4 compared with
IST-ITG constraints, respectively. This indi-
cates the validity of the proposed reordering
model.
1 Introduction
Statistical machine translation has been wiedely ap-
plied in many state-of-the-art translation systems. A
popular statistical machine translation paradigms is
the phrase-based model (Koehn et al, 2003; Och
and Ney, 2004). In phrase-based statistical ma-
chine translation, errors in word reordering, espe-
cially global reordering, are one of the most se-
rious problems. To resolve this problem, many
word-reordering constraint techniques have been
proposed. These techniques are categorized into
two types. The first type is linguistically syntax-
based. In this approach, tree structures for the source
(Quirk et al, 2005; Huang et al, 2006), target (Ya-
mada and Knight, 2000; Marcu et al, 2006), or both
(Melamed, 2004) are used for model training. The
second type is formal constraints on word permuta-
tions. IBM constraints (Berger et al, 1996), the lex-
ical word reordering model (Tillmann, 2004), and
inversion transduction grammar (ITG) constraints
(Wu, 1995; Wu, 1997) belong to this type of ap-
proach. For ITG constraints, the target-side word
order is obtained by rotating nodes of the source-
side binary tree. In these node rotations, the source
binary tree instance is not considered. Imposing
a source tree on ITG (IST-ITG) constraints (Ya-
mamoto et al, 2008) is an extension of ITG con-
straints and a hybrid of the first and second type of
approach. IST-ITG constraints directly introduce a
source sentence tree structure. Therefore, IST-ITG
can obtain stronger constraints for word reordering
than the original ITG constraints. For example, IST-
ITG constraints allows only eight word orderings for
a four-word sentence, even though twenty-two word
orderings are possible with respect to the original
ITG constraints. Although IST-ITG constraints ef-
ficiently suppress erroneous target word orderings,
the method cannot assign the probability to the tar-
get word orderings.
This paper presents a reordering model using syn-
tactic information of a source tree for phrase-based
statistical machine translation. The proposed re-
ordering model is an extension of IST-ITG con-
69
straints. In the proposed method, the target-side
word order is obtained by rotating nodes of a source-
side parse-tree in a similar fashion to IST-ITG con-
straints. We modeled the rotating positions, mono-
tone or swap, from word alignments of a training
parallel corpus and source-side parse-trees. The pro-
posed method conducts a probabilistic evaluation of
target word orderings using syntactic information of
the source tree.
The rest of this paper is organized as follows.
Section 2 describes the previous approach to re-
solving erroneous word reordering. In Section 3,
the reordering model using syntactic information of
a source tree is presented. Section 4 shows ex-
perimental results. Finally, Section 5 presnts the
summary and some concluding remarks and future
works.
2 Previous Works
First, we introduce two previous studies on related
word reordering constraints, ITG and IST-ITG con-
straints.
2.1 ITG Constraints
In one-to-one word-alignment, the source word fi
is translated into the target word ei. The source
sentence [f1, f2, ? ? ? , fN ] is translated into the tar-
get sentence which is the reordered target word se-
quence [e1, e2, ? ? ? , eN ]. The number of reorderings
is N !. When ITG constraints are introduced, this
combination N ! can be reduced in accordance with
the following constraints.
? All possible binary tree structures are generated
from the source word sequence.
? The target sentence is obtained by rotating any
node of the binary trees.
When N = 4, the ITG constraints can reduce
the number of combinations from 4! = 24 to
22 by rejecting the combinations [e3, e1, e4, e2]
and [e2, e4, e1, e3]. For a four-word sentence, the
search space is reduced to 92% (22/24), but for
a 10-word sentence, the search space is only 6%
(206,098/3,628,800) of the original full space.
2.2 IST-ITG Constraints
In ITG constraints, the source-side binary tree in-
stance is not considered. Therefore, if a source sen-
tence tree structure is utilized, stronger constraints
than the original ITG constraints can be created.
IST-ITG constraints directly introduce a source sen-
tence tree structure. The target sentence is obtained
with the following constraints.
? A source sentence tree structure is generated
from the source sentence.
? The target sentence is obtained by rotating any
node of the source sentence tree structure.
By parsing the source sentence, the parse-tree is
obtained. After parsing the source sentence, a
bracketed sentence is obtained by removing the
node syntactic labels; this bracketed sentence can
then be converted into a tree structure. For example,
the parse-tree ?(S1 (S (NP (DT This)) (VP (AUX
is) (NP (DT a) (NN pen)))))? is obtained from the
source sentence ?This is a pen,? which consists of
four words. By removing the node syntactic labels,
the bracketed sentence ?((This) ((is) ((a) (pen))))?
is obtained. Such a bracketed sentence can be used
to produce constraints. If IST-ITG constraints is
applied, the number of word orderings in N = 4
is reduced to 8, down from 22 with ITG cn-
straints. For example, for the source-side bracketed
tree ?((f1f2) (f3f4)),? the eight target sequences
[e1, e2, e3, e4], [e2, e1, e3, e4], [e1, e2, e4, e3],
[e2, e1, e4, e3], [e3, e4, e1, e2], [e3, e4, e2, e1],
[e4, e3, e1, e2], and [e4, e3, e2, e1] are accepted. For
the source-side bracketed tree ?(((f1f2) f3) f4),?
the eight sequences [e1, e2, e3, e4], [e2, e1, e3, e4],
[e3, e1, e2, e4], [e3, e2, e1, e4], [e4, e1, e2, e3],
[e4, e2, e1, e3], [e4, e3, e1, e2], and [e4, e3, e2, e1] are
accepted. When the source sentence tree structure
is a binary tree, the number of word orderings is
reduced to 2N?1. The parsing results sometimes do
not produce binary trees. In this case, some subtrees
have more than two child nodes. For a non-binary
subtree, any reordering of child nodes is allowed. If
a subtree has three child nodes, six reorderings of
the nodes are accepted.
In phrase-based statistical machine translation, a
source ?phrase? is translated into a target ?phrase?.
However, with IST-ITG constraints, ?word? must be
70
used for the constraint unit since the parse unit is a
?word?. To absorb different units between transla-
tion models and IST-ITG constraints, a new limita-
tion for word reordering is applied.
? Word ordering that destroys a phrase is not al-
lowed.
When this limitation is applied, the translated word
ordering is obtained from the bracketed source sen-
tence tree by reordering the nodes in the tree, which
is the same as for one-to-one word-alignment.
3 Reordering Model Using Syntactic
Information of the Source Tree
In this section, we present a new reordering model
using syntactic information of a source-side parse-
tree.
3.1 Abstract of Proposed Method
The IST-ITG constraints method efficiently sup-
presses erroneous target word orderings. However,
IST-ITG constraints cannot evaluate the accuracy of
the target word orderings; i.e., IST-ITG constraints
assign an equal probability to all target word order-
ings. This paper proposes a reordering model us-
ing syntactic information of the source tree as an
extension of IST-ITG constraints. The proposed re-
ordering model conducts a probabilistic evaluation
of target word orderings using syntactic information
of the source-side parse-tree.
In the proposed method, the target-side word or-
der is obtained by rotating nodes of the source-
side parse-tree in a similar fashion to IST-ITG con-
straints. Reordering probabilities are assigned to
each subtree of source-side parse-tree S by reorder-
ing the positions into two types: monotone and
swap. If the subtree has more than two child nodes,
the number of child node order is more than two.
However, we assume the child node order other than
monotone to be swap. The source-side parse-tree
S consists of subtrees {s1, s2, ? ? ? , sK}, where K
is the number of subtrees included in the source-
side parse-tree. The subtree sk is which is repre-
sented by the parent node?s syntactic label and the
order, from sentence head to sentence tail, of the
child node?s syntactic labels. For example, Fig-
ure 1 shows a source-side parse-tree for a four-word
Source-side parse-tree
Source sentence
S
NP VP
NPAUX
DT NN
Figure 1: Example of a source-side parse-tree fo a four-
word source sentence consisting of three subtrees.
source sentence consisting of three subtrees. In Fig-
ure 1, the subtrees s1, s2, and s3 are represented by
S+NP+VP, VP+AUX+NP, and NP+DT+NN, re-
spectively. Each subtree has a probability P (t | sk),
where t is monotone (m) or swap (s). The proba-
bility of the target word reordering is calculated as
follows.
Pr =
K?
k=1
P (t | sk) (1)
Each target candidate is assigned the different re-
ordering probability by Equation (1). Since the pro-
posed reordering model uses the syntactic labels,
which is not considered in IST-ITG constraints, the
different parse-tree assigns the different reordering
probability. The proposed model is effective for
global word reordering, because reordering proba-
bilities are also assigned to higher-level subtrees of
the source-side parse-tree.
3.2 Training of the Proposed Model
We modeled monotone or swap node rotating auto-
matically from word alignments of a training paral-
lel corpus and source-side parse-trees. The training
algorithm for the proposed reordering model is as
follows.
1. The training process begins with a word-
aligned corpus. We obtained the word align-
ments using Koehn et al?s method (2003),
71
32
2,34
2,3,41
Figure 2: Example of a source-side parse-tree with word
alignments using the training algorithm of the proposed
model.
which is based on Och and Ney?s work (2004).
This involves running GIZA++ (Och and Ney,
2003) on the corpus in both directions, and ap-
plying refinement rules (the variant they desig-
nate is ?final-and?) to obtain a single many-to-
many word alignment for each sentence.
2. Source-side parse-trees are created using a
source language phrase structure parser, which
annotates each node with a syntactic label. A
source-side parse-tree consists of several sub-
trees with syntactic labels. For example, the
parse-tree ?(S1 (S (NP (DT This)) (VP (AUX
is) (NP (DT a) (NN pen)))))? is obtained from
the source sentence ?This is a pen? which con-
sists of four words.
3. Word alignments and source-side parse-trees
are combined. Leaf nodes are assigned target
word positions obtained from word alignments.
Via the bottom-up process, target word posi-
tions are assigned to all nodes. For example,
in Figure 2, the left-side (sentence head) child
node of subtree s2 is assigned the target word
position ?4,? and the right-side (sentence tail)
child node is assigned the target word positions
?2? and ?3,? which are assigned to the child
nodes of subtree s3.
4. The monotone and swap reordering positions
are checked and counted for each subtree. By
Subtree type Monotone probability
S+PP+,+NP+VP+. 0.764
PP+IN+NP 0.816
NP+DT+NN+NN 0.664
VP+AUX+VP 0.864
VP+VBN+PP 0.837
NP+NP+PP 0.805
NP+DT+JJ+NN 0.653
NP+DT+JJ+VBP+NN 0.412
NP+DT+NN+CC+VB 0.357
Table 1: Example of proposed reordering models.
comparing the target word positions, which are
assigned in the above step, the reordering posi-
tion is determined. If the target word position
of the left-side child node is smaller than one of
the right-side child node, the reordering posi-
tion determined as monotone. For example, in
Figure 2, the subtrees s1, s2 and s3 are mono-
tone, swap, and monotone, respectively.
5. The reordering probability of the subtree can
be directly estimated by counting the reorder-
ing positions in the training data.
P (t | s) = ct(s)?
t ct(s)
(2)
where ct(s) is the count of reordering positon t
included all training samples for the subtree s.
The parsing results sometimes do not produce bi-
nary trees. For a non-binary subtree, any reorder-
ing of child nodes is allowed. However, the pro-
posed reordering model assumes that reordering po-
sitions are only two, monotone and swap. That
is, the reordering position which the order of child
nodes do not change is monotone, and the other po-
sitions are swap. Therefore, the probability of swap
P (s | sk) is derived from the probability of mono-
tone P (m | sk) as follows.
P (s | sk) = 1.0 ? P (m | sk) (3)
Table 1 shows the example of proposed reordering
models.
If a subtree is represented by a binary-tree, there
are L3 possible subtrees, where L is the number of
72
Figure 3: Example of a target word order which is not
derived from rotating the nodes of source-side parse trees.
syntactic labels. However, in the possible subtrees,
there are subtrees observed only a few times in train-
ing sentences, especially when the subtree consists
of more than three child nodes. Although a large
number of subtree models can capture variations in
the training samples, too many models lead to the
over-fitting problem. Therefore, subtrees where the
number of training samples is less than a heuristic
threshold and unseen subtrees are clustered to deal
with the data sparseness problem for robust model
estimations.
After creating word alignments of a training par-
allel corpus, there are target word orders which are
not derived from rotating nodes of source-side parse-
trees. Figure 3 shows a sample which is not derived
from rotating nodes. Some are due to linguistic rea-
sons, structual differences such as negation (French
?ne...pas? and English ?not?), adverb, modal and so
on. Others are due to non-linguistic reasons, er-
rors of automatic word alignments, syntactic anal-
ysis, or human translation (Fox, 2002). The pro-
posed method discards such problematic cases. In
Figure 3, the subtree s1 is then removed from train-
ing samples, and the subtrees s2 and s3 are used as
training samples.
3.3 Decoding Using the Proposed Reordering
Model
In this section, we describe a one-pass phrase-based
decoding algorithm that uses the proposed reorder-
ing model in the decoder. The translation target sen-
tence is sequentially generated from left (sentence
Figure 4: Example of a target candidate including a
phrase.
head) to right (sentence tail), and all reordering is
conducted on the source side. To introduce the pro-
posed reordering model into the decoder, the target
candidate must be checked for whether the reorder-
ing position of a subtree is either monotone or swap
whenever a new phrase is selected to extend a target
candidate. The checking algorithm is as follows.
1. For old translation candidates, the subtree s,
which includes both translated and untranslated
words, and its untranslated part u are calcu-
lated.
2. When a new target phrase e? is generated, the
source phrase f? and the untranslated part u cal-
culated in the above step are compared. If the
source phrase f? does not include the untrans-
lated part u and is not included u, the new can-
didate is rejected.
3. In the accepted candidate, the reordering po-
sitions for all subtrees included the source side
parse-tree are checked by comparing the source
phrase f? with the source phrase sequence used
before.
Subtrees checked reordering positions are assigned a
probability?monotone or swap?by the proposed re-
ordering model, and the target word order is evalu-
ated by Equation (1).
Phrase-based statistical machine translation uses
a ?phrase? as the translation unit. However, the pro-
posed reordering model needs a ?word? order. Be-
cause ?word? alignments form the source phrase to
target phrase are not clear, we cannot determine the
73
Figure 5: Example of a non-binary subtree including a
phrase.
reordering position of subtree included in a phrase.
Therefore, in the decoding process using the pro-
posed reordering model, we define that higher prob-
ability, monotone or swap, are assigned to subtrees
included in a source phrase. For example, in Fig-
ure 4, the source sentence [[f1, f2], f3, f4] is trans-
lated into the target sentence [[e1, e2], e4, e3], where
[f1, f2] and [e1, e2] are used as phrases. Then, the
source phrase [f1, f2] includes the subtree s2. If the
monotone probabilities of subtrees s1, s2, and s3 are
0.8, 0.4 and 0.7, the proposed reordering probabil-
ity is 0.8 ? 0.6 ? 0.3 = 0.144. If a source phrase
is [f1, f2, f3, f4] and a source-side parse-tree has the
same tree structure used in Figure 4, the subtrees s1,
s2, and s3 are assigned higher reordering probabili-
ties. If the source phrase [f1, f2, f3, f4] used in Fig-
ure 4, the subtrees s1, s2, and s3 are assigned higher
reordering probabilities.
Non-binary subtrees are often observed in the
source-side parse-tree. When a source phrase f? is
included in a non-binary subtree and does not in-
clude a non-binary subtree, we cannot determine the
reordering position. For example, the reordering po-
sition of subtree s2 in Figure 5, which includes the
phrase [f3, f4], can not be determined. In this case,
we define that such subtrees are also to be assigned
a higher probability.
4 Experiments
To evaluate the proposed model, we conducted two
experiments: English-to-Japanese and English-to-
Chinese translation.
English Japanese
Train Sentences 1.0M
Words 24.6M 24.6M
Dev Sentences 2.0K
Words 50.1K 58.7K
Test Sentences 2.0K
Words 49.5K 58.0K
Table 2: Statistics of training, development and test cor-
pus for E-J translation.
4.1 English-to-Japanese Paper Abstract
Translation Experiments
The first experiment was the English-to-Japanese
(E-J) translation. Table 2 shows the training, de-
velopment and test corpus statistics. JST Japanese-
English paper abstract corpus consists of 1.0M
parallel sentences were used for model training.
This corpus was constructed from 2.0M Japanese-
English paper abstract corpus belongs to JST by
NICT using the method of Uchiyama and Isahara
(2007). For phrase-based translation model training,
we used the GIZA++ toolkit (Och and Ney, 2003),
and 1.0M bilingual sentences. For language model
training, we used the SRI language model toolkit
(Stolcke, 2002), and 1.0M sentences for the trans-
lation model training. The language model type was
word 5-gram smoothed by Kneser-Ney discounting
(Kneser and Ney, 1995). To tune the decoder pa-
rameters, we conducted minimum error rate training
(Och, 2003) with respect to the word BLEU score
(Papineni et al, 2002) using 2.0K development sen-
tence pairs. The test set with 2.0K sentences is used.
In the evaluation and development sets, a single ref-
erence was used. For the creation of English sen-
tence parse trees and segmentation of the English,
we used the Charniak parser (Charniak, 2000). We
used Chasen for segmentation of the Japanese sen-
tences. For decoding, we used an in-house decoder
that is a close relative of the Moses decoder. The
performance of this decoder was configured to be
the same as Moses. Other conditions were the same
as the default conditions of the Moses decoder.
In this experiment, the following three methods
were compared.
? Baseline : The IBM constraints and the lexi-
cal reordering model were used for target word
74
Baseline IST-ITG Proposed
BLEU 27.87 29.31 29.80
Table 3: BLEU score results for E-J translation. (1-
reference)
reordering.
? IST-ITG : The IST-ITG constraints, the IBM
constraints, and the lexical reordering model
were used for target word reordering.
? Proposed : The proposed reordering model,
the IBM constraints, and the lexical reordering
model were used for target word reordering.
During minimum error training, each method used
each reordering model and reordering constraint.
The proposed reordering model are trained from
1.0M bilingual sentences for the translation model
training. The amount of available training samples
represented by subtrees was 9.8M. In the available
training samples, there were 54K subtree types. The
heuristic threshold was 10, and subtrees with train-
ing samples of less than 10 were clustered. The pro-
posed reordering model consisted of 5,960 subtrees
types and one clustered model ?other?. The models
not including ?other? covered 99.29% of all training
samples.
The BLEU scores are presented in Table 3.
In comparing ?Baseline? method with ?IST-ITG?
method, the improvement in BLEU was a 1.44-
point. Furthermore, in comparing ?IST-ITG?
method with ?Proposed? method, the improvement
in BLEU was a 0.49-point. Both the IST-ITG con-
straints and the proposed reordering model fixed the
phrase position for the global reorderings. How-
ever, the proposed method can conduct a probabilis-
tic evaluation of target word reorderings which the
IST-ITG constraints cannot. Therefore, ?Proposed?
method resulted in a better BLEU.
4.2 NIST MT08 English-to-Chinese
Translation Experiments
Next, we conducted English-to-Chinese (E-C) news-
paper translation experiments for different lan-
guage pairs. The NIST MT08 evaluation campaign
English-to-Chinese translation track was used for
the training and evaluation corpora. Table 4 shows
English Chinese
Train Sentences 4.6M
Words 79.6M 73.4M
Dev Sentences 1.6K
Words 46.4K 39.0K
Test Sentences 1.9K
Words 45.7K 47.0K (Ave.)
Table 4: Statistics of training, development and test cor-
pus for E-C translation.
Baseline IST-ITG Proposed
BLEU 17.54 18.60 18.93
Table 5: BLEU score results for E-C translation. (4-
reference)
the training, development and test corpus statistics.
For the translation model training, we used 4.6M
bilingual sentences. For the language model train-
ing, we used 4.6M sentences which are used for
the translation model training. The language model
type was word 3-gram smoothed by Kneser-Ney
discounting. A development set with 1.6K sen-
tences was used as evaluation data in the Chinese-to-
English translation track for the NIST MT07 eval-
uation campaign. A single reference was used in
the development set. The evaluation set with 1.9K
sentences is the same as the MT08 evaluation data,
with 4 references. In this experiment, the compared
methods were the same as in the E-J experiment.
The proposed reordering model are trained from
4.6M bilingual sentences for the translation model
training. The amount of available training samples
represented by subtrees was 39.6M. In the available
training samples, there were 193K subtree types.
As in the E-J experiments, the heuristic threshold
was 10. The proposed reordering model consisted
of 18,955 subtree types and one clustered model
?other.? The models not including ?other? covered
99.45% of all training samples.
The BLEU scores are presented in Table 5.
In comparing ?Baseline? method with ?IST-ITG?
method, the improvement in BLEU was a 1.06-
point. In comparing ?IST-ITG? method with ?Pro-
posed? method, the improvement in BLEU was a
0.33-point. As in the E-J experiments, ?Proposed?
method performed the highest BLEU. We demon-
75
strated that the proposed method is effective for mul-
tiple language pairs. However, the improvement
of BLEU score in E-C translation is smaller than
the improvement in E-J translation, because English
and Chinese are similar sentence structures, such as
SVO-languages (Japanese is SOV-language). When
the sentence structures are different, the proposed re-
ordering model is effective.
5 Conclusion
This paper proposed a new word reordering model
using syntactic information of a source tree for
phrase-based statistical machine translation. The
proposed model is an extension of the IST-ITG con-
straints. In both IST-ITG constraints and the pro-
posed method, the target-side word order is obtained
by rotating nodes of the source-side tree structure.
Both the IST-ITG constraints and the proposed re-
ordering model fix the phrase position for the global
reorderings. However, the proposed method can
conduct a probabilistic evaluation of target word re-
orderings which the IST-ITG constraints cannot. In
E-J and E-C translation experiments, the proposed
method resulted in a 0.49-point improvement (29.31
to 29.80) and a 0.33-point improvement (18.60 to
18.93) in word BLEU-4 compared with IST-ITG
constraints, respectively. This indicates the validity
of the proposed reordering model.
Future work will focus on a reduction of com-
putational cost of decoding including the proposed
reordering model, and a simultaneous training of
translation and reordering models. Moreover, we
will deal with difference between source and target
in multi level like in Gally et al (2004).
The improvement could clearly be seen from vi-
sual inspection of the output, a few examples of
which are presented in the following Appendix.
A Samples from the English-to-Japanese
Translation
A.1 Sentence 1
Source: Aggravation was obvious from the latter
half of March to the end of April, and he contracted
the disease in February to the beginning of May.
Baseline: ?????????????????
????????????????
Reference: ?????????????????
????????????
Proposed: ?????????????????
????????????????
A.2 Sentence 2
Source: The value of TF, on the other hand, was
higher in the reverse order, indicating that high ox-
idation rate causes severe defects on the surface of
Ni crystallites.
Baseline: ?????????????????
??????????????????????
??????????????
Reference: ?????????????????
??????????????????????
?????????
Proposed: ?????????????????
??????????????????????
?????????????
A.3 Sentence 3
Source: After diagnosing the pleural effusion and
ascites, vein catheter was left in place under the echo
guide, and after removing the pleural effusion and
ascites, OK-432 was administered locally.
Baseline: ??????????????????
??????????????????????
?????????????????
Reference: ?????????????????
??????????????????????
???????????????????
Proposed: ?????????????????
??????????????????????
???????????????????????
A.4 Sentence 4
Source: From result of the consideration, it was
pointed that radiation from the loop elements was
weak.
Baseline: ?????????????????
?????????????
Reference: ?????????????????
???????????
Proposed: ?????????????????
???????????
76
References
Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra,
Vincent J. Della Pietra, Andrew S. Kehler, and Robert
L. Mercer 1996. Language translation apparatus
and method of using context-based translation models.
United States patent, patent number 5510981.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of NAACL 2000, pages 132?
139.
Chasen
http://chasen-legacy.sourceforge.jp/
Heidi J. Fox, 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of EMNLP, pages
304?311.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL-04.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical Syntax-Directed Translation with Extended
Domain of Locality. In Proceedings of AMTA.
Japanese-English paper abstract corpus
http://www.jst.go.jp
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language model In Proceed-
ings of ICASSP 1995, pages 181?184.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, pages 127?133.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical Machine
Translation with Syntactified Target Language Phrases
In Proceedings of EMNLP2006, pages 44?52.
Dan Melamed. 2004. Statistical machine translation by
parsing In Proceedings of ACL, pages 653?660.
Moses
http://www.statmt.org/moses/
Franz josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1), pages 19?51.
Franz josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Franz josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4), pages 417?
449.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of ACL, pages 271?279.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Andreas Stolcke. 2002. SRILM - An Ex-
tensible Language Model Toolkit In Pro-
ceedings of ICSLP2002, pages 901?904.
http://www.speech.sri.com/projects/srilm/
Christopher Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Proceed-
ings of HLT-NAACL, pages 101?104.
Masao Uchiyama and Hitoshi Isahara. 2007. 2007. A
japanese-english patent parallel corpus. In MT sum-
mit XI, pages 475?482.
Dekai Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation, bracket-
ing, and alignment of parallel corpora. In Proceedings
of IJCAI, pages 1328?1334.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguiatics, 23(3), pages 377?403.
Kenji Yamada and Kevin Knight. 2000. A syntax-based
statistical translation model In Proceedings of ACL,
pages 523?530.
Hirofumi Yamamoto, Hideo Okuma, and Eiichiro
Sumita. 2008. Imposing Constraints from the Source
Tree on ITG Constraints for SMT. In Proceedings of
ACL : HLT Second Workshop on Syntax and Structure
in Statistical Translation (SSST-2), pages 1?9.
77
