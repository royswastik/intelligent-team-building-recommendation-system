Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 540?548,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Semi-supervised Learning for Automatic Prosodic Event Detection
Using Co-training Algorithm
Je Hun Jeon and Yang Liu
Computer Science Department
The University of Texas at Dallas, Richardson, TX, USA
{jhjeon,yangl}@hlt.utdallas.edu
Abstract
Most of previous approaches to automatic
prosodic event detection are based on su-
pervised learning, relying on the avail-
ability of a corpus that is annotated with
the prosodic labels of interest in order to
train the classification models. However,
creating such resources is an expensive
and time-consuming task. In this paper,
we exploit semi-supervised learning with
the co-training algorithm for automatic de-
tection of coarse level representation of
prosodic events such as pitch accents, in-
tonational phrase boundaries, and break
indices. We propose a confidence-based
method to assign labels to unlabeled data
and demonstrate improved results using
this method compared to the widely used
agreement-based method. In addition, we
examine various informative sample selec-
tion methods. In our experiments on the
Boston University radio news corpus, us-
ing only a small amount of the labeled data
as the initial training set, our proposed la-
beling method combined with most confi-
dence sample selection can effectively use
unlabeled data to improve performance
and finally reach performance closer to
that of the supervised method using all the
training data.
1 Introduction
Prosody represents suprasegmental information in
speech since it normally extends over more than
one phoneme segment. Prosodic phenomena man-
ifest themselves in speech in different ways, in-
cluding changes in relative intensity to emphasize
specific words or syllables, variations of the fun-
damental frequency range and contour, and subtle
timing variations, such as syllable lengthening and
insertion of pause. In spoken utterances, speakers
use prosody to convey emphasis, intent, attitude,
and emotion. These are important cues to aid the
listener for interpretation of speech. Prosody also
plays an important role in automatic spoken lan-
guage processing tasks, such as speech act detec-
tion and natural speech synthesis, because it in-
cludes aspect of higher level information that is
not completely revealed by segmental acoustics or
lexical information.
To represent prosodic events for the categorical
annotation schemes, one of the most popular label-
ing schemes is the Tones and Break Indices (ToBI)
framework (Silverman et al, 1992). The most im-
portant prosodic phenomena captured within this
framework include pitch accents (or prominence)
and prosodic phrase boundaries. Within the ToBI
framework, prosodic phrasing refers to the per-
ceived grouping of words in an utterance, and
accent refers to the greater perceived strength or
emphasis of some syllables in a phrase. Cor-
pora annotated with prosody information can be
used for speech analysis and to learn the relation-
ship between prosodic events and lexical, syntac-
tic and semantic structure of the utterance. How-
ever, it is very expensive and time-consuming to
perform prosody labeling manually. Therefore,
automatic labeling of prosodic events is an attrac-
tive alternative that has received attention over the
past decades. In addition, automatically detecting
prosodic events also benefits many other speech
understanding tasks.
Many previous efforts on prosodic event de-
tection were supervised learning approaches that
used acoustic, lexical, and syntactic cues. How-
ever, the major drawback with these methods is
that they require a hand-labeled training corpus
and depend on specific corpus used for training.
Limited research has been conducted using unsu-
pervised and semi-supervised methods. In this pa-
per, we exploit semi-supervised learning with the
540
Figure 1: An example of ToBI annotation on a sentence ?Hennessy will be a hard act to follow.?
co-training algorithm (Blum and Mitchell, 1998)
for automatic prosodic event labeling. Two dif-
ferent views according to acoustic and lexical-
syntactic knowledge sources are used in the co-
training framework. We propose a confidence-
based method to assign labels to unlabeled data
in training iterations and evaluate its performance
combined with different informative sample se-
lection methods. Our experiments on the Boston
Radio News corpus show that the use of unla-
beled data can lead to significant improvement
of prosodic event detection compared to using
the original small training set, and that the semi-
supervised learning result is comparable with su-
pervised learning with similar amount of training
data.
The remainder of this paper is organized as fol-
lows. In the next section, we provide details of
the corpus and the prosodic event detection tasks.
Section 3 reviews previous work briefly. In Sec-
tion 4, we describe the classification method for
prosodic event detection, including the acoustic
and syntactic prosodic models, and the features
used. Section 5 introduces the co-training algo-
rithm we used. Section 6 presents our experiments
and results. The final section gives a brief sum-
mary along with future directions.
2 Corpus and tasks
In this paper, our experiments were carried out
on the Boston University Radio News Corpus
(BU) (Ostendorf et al, 2003) which consists
of broadcast news style read speech and has
ToBI-style prosodic annotations for a part of the
data. The corpus is annotated with orthographic
transcription, automatically generated and hand-
corrected part-of-speech (POS) tags, and auto-
matic phone alignments.
The main prosodic events that we are concerned
to detect automatically in this paper are phrasing
and accent (or prominence). Prosodic phrasing
refers to the perceived grouping of words in an ut-
terance, and prominence refers to the greater per-
ceived strength or emphasis of some syllables in
a phrase. In the ToBI framework, the pitch accent
tones (*) are marked at every accented syllable and
have five types according to pitch contour: H*, L*,
L*+H, L+H*, H+!H*. The phrase boundary tones
are marked at every intermediate phrase boundary
(L-, H-) or intonational phrase boundary (L-L%,
L-H%, H-H%, H-L%) at certain word boundaries.
There are also the break indices at every word
boundary which range in value from 0 through
4, where 4 means intonational phrase boundary, 3
means intermediate phrase boundary, and a value
under 3 means phrase-medial word boundary. Fig-
ure 1 shows a ToBI annotation example for a sen-
tence ?Hennessy will be a hard act to follow.? The
first and second tiers show the orthographic infor-
mation such as words and syllables of the utter-
ance. The third tier shows the accents and phrase
boundary tones. The accent tone is located on each
accented syllable, such as the first syllable of word
?Hennessy.? The boundary tone is marked on ev-
ery final syllable if there is a prosodic boundary.
For example, there are intermediate phrase bound-
aries after words ?Hennessy? and ?act?, and there
is an intonational phrase boundary after word ?fol-
low.? The fourth tier shows the break indices at the
end of every word.
The detailed representation of prosodic events
in the ToBI framework creates a serious sparse
data problem for automatic prosody detection.
This problem can be alleviated by grouping ToBI
labels into coarse categories, such as presence or
absence of pitch accents and phrasal tones. This
also significantly reduces ambiguity of the task. In
this paper, we thus use coarse representation (pres-
ence versus absence) for three prosodic event de-
tection tasks:
541
? Pitch accents: accent mark (*) means pres-
ence.
? Intonational phrase boundaries (IPB): all of
the IPB tones (%) are grouped into one cate-
gory.
? Break indices: value 3 and 4 are grouped to-
gether to represent that there is a break. This
task is equivalent to detecting the presence of
intermediate and intonational phrase bound-
aries.
These three tasks are binary classification prob-
lems. Similar setup has also been used in other
previous work.
3 Previous work
Many previous efforts on prosodic event detec-
tion used supervised learning approaches. In the
work by Wightman and Ostendorf (1994), binary
accent, IPB, and break index were assigned to
syllables based on posterior probabilities com-
puted from acoustic evidence using decision trees,
combined with a bigram model of accent and
boundary patterns. Their method achieved an
accuracy of 84% for accent, 71% for IPB, and
84% for break index detection at the syllable
level. Chen et al (2004) used a Gaussian mix-
ture model for acoustic-prosodic information and
neural network based syntactic-prosodic model
and achieved pitch accent detection accuracy of
84% and IPB detection accuracy of 90% at the
word level. The experiments of Ananthakrish-
nan and Narayanan (2008) with neural network
based acoustic-prosodic model and a factored n-
gram syntactic model reported 87% accuracy on
accent and break index detection at the syllable
level. The work of Sridhar et al (2008) using a
maximum entropy model achieved accent and IPB
detection accuracies of 86% and 93% on the word
level.
Limited research has been done in prosodic
detection using unsupervised or semi-supervised
methods. Ananthakrishnan and Narayanan (2006)
proposed an unsupervised algorithm for prosodic
event detection. This algorithm was based on clus-
tering techniques to make use of acoustic and syn-
tactic cues and achieved accent and IPB detec-
tion accuracies of 77.8% and 88.5%, compared
with the accuracies of 86.5% and 91.6% with su-
pervised methods. Similarly, Levow (2006) tried
clustering based unsupervised approach on ac-
cent detection with only acoustic evidence and
reported accuracy of 78.4% for accent detection
compared with 80.1% using supervised learning.
She also exploited a semi-supervised approach us-
ing Laplacian SVM classification on a small set of
examples. This approach achieved 81.5%, com-
pared to 84% accuracy for accent detection in a
fully supervised fashion.
Since Blum and Mitchell (1998) proposed co-
training, it has received a lot of attention in the re-
search community. This multi-view setting applies
well to learning problems that have a natural way
to divide their features into subsets, each of which
are sufficient to learn the target concept. Theo-
retical and empirical analysis has been performed
for the effectiveness of co-training such as Blum
and Mitchell (1998), Goldman and Zhou (2000),
Nigam and Ghani (2000), and Dasuta et al (2001).
More recently, researchers have begun to explore
ways of combing ideas from sample selection with
that of co-training. Steedman et al (2003) ap-
plied co-training method to statistical parsing and
introduced sample selection heuristics. Clark et
al. (2003) and Wang et al (2007) applied co-
training method in POS tagging using agreement-
based selection strategy. Co-testing (Muslea et
al., 2000), one of active learning approaches, has
a similar spirit. Like co-training, it consists of
two classifiers with redundant views and compares
their outputs for an unlabeled example. If they
disagree, then the example is considered as a con-
tention point, and therefore a good candidate for
human labeling.
In this paper, we apply co-training algorithm
to automatic prosodic event detection and propose
methods to better select samples to improve semi-
supervised learning performance for this task.
4 Prosodic event detection method
We model the prosody detection problem as a clas-
sification task. We separately develop acoustic-
prosodic and syntactic-prosodic models accord-
ing to information sources and then combine the
two models. Our previous supervised learning ap-
proach (Jeon and Liu, 2009) showed that a com-
bined model using Neural Network (NN) classifier
for acoustic-prosodic evidence and Support Vector
Machine (SVM) classifier for syntactic-prosodic
evidence performed better than other classifiers.
We therefore use NN and SVM in this study. Note
542
that our feature extraction is performed at the syl-
lable level. This is straightforward for accent de-
tection since stress is defined associated with syl-
lables. In the case of IPB and break index detec-
tion, we use only the features from the final syl-
lable of a word since those events are associated
with word boundaries.
4.1 The acoustic-prosodic model
The most likely sequence of prosodic events P ? =
{p?1, . . . , p?n} given the sequence of acoustic evi-
dences A = {a1, . . . , an} can be found as follow-
ing:
P ? = arg max
P
p(P |A)
? arg max
P
n
?
i=1
p(pi|ai) (1)
where ai = {a1i , . . . , ati} is the acoustic feature
vector corresponding to a syllable. Note that this
assumes that the prosodic events are independent
and they are only dependent on the acoustic obser-
vations in the corresponding locations.
The primary acoustic cues for prosodic events
are pitch, energy and duration. In order to reduce
the effect by both inter-speaker and intra-speaker
variation, both pitch and energy values were nor-
malized (z-value) with utterance specific means
and variances. The acoustic features used in our
experiments are listed below. Again, all of the fea-
tures are computed for a syllable.
? Pitch range (4 features): maximum pitch,
minimum pitch, mean pitch, and pitch range
(difference between maximum and minimum
pitch).
? Pitch slope (5 features): first pitch slope, last
pitch slope, maximum plus pitch slope, max-
imum minus pitch slope, and the number of
changes in the pitch slope patterns.
? Energy range (4 features): maximum en-
ergy, minimum energy, mean energy, and
energy range (difference between maximum
and minimum energy).
? Duration (3 features): normalized vowel du-
ration, pause duration after the word final syl-
lable, and the ratio of vowel durations be-
tween this syllable and the next syllable.
Among the duration features, the pause dura-
tion and the ratio of vowel durations are only used
to detect IPB and break index, not for accent de-
tection.
4.2 The syntactic-prosodic model
The prosodic events P ? given the sequence of lex-
ical and syntactic evidences S = {s1, . . . , sn} can
be found as following:
P ? = arg max
P
p(P |S)
? arg max
P
n
?
i=1
p(pi|?(si)) (2)
where ?(si) is chosen such that it contains lexi-
cal and syntactic evidence from a fixed window of
syllables surrounding location i.
There is a very strong correlation between the
prosodic events in an utterance and its lexical and
syntactic structure. Previous studies have shown
that for pitch accent detection, the lexical features
such as the canonical stress patterns from the pro-
nunciation dictionary perform better than the syn-
tactic features, while for IPB and break index de-
tection, the syntactic features such as POS work
better than the lexical features. We use different
feature types for each task and the detailed fea-
tures are as follows:
? Accent detection: syllable identity, lexical
stress (exist or not), word boundary informa-
tion (boundary or not), and POS tag. We
also include syllable identity, lexical stress,
and word boundary features from the previ-
ous and next context window.
? IPB and Break index detection: POS tag, the
ratio of syntactic phrases the word initiates,
and the ratio of syntactic phrases the word
terminates. All of these features from the pre-
vious and next context windows are also in-
cluded.
4.3 The combined model
The two models above can be coupled as a classi-
fier for prosodic event detection. If we assume that
the acoustic observations are conditionally inde-
pendent of the syntactic features given the prosody
labels, the task of prosodic detection is to find the
optimal sequence P ? as follows:
P ? = arg max
P
p(P |A,S)
543
? arg max
P
p(P |A)p(P |S)
? arg max
P
n
?
i=1
p(pi|ai)?p(pi|?(si)) (3)
where ? is a parameter that can be used to adjust
the weighting between syntactic and the acoustic
model. In our experiments, the value of ? is esti-
mated based on development data.
5 Co-training strategy for prosodic event
detection
Co-training (Blum and Mitchell, 1998) is a semi-
supervised multi-view algorithm that uses the ini-
tial training set to learn a (weak) classifier in each
view. Then each classifier is applied to all the
unlabeled examples. Those examples that each
classifier makes the most confident predictions are
selected and labeled with the estimated class la-
bels and added to the training set. Based on the
new training set, a new classifier is learned in each
view, and the whole process is repeated for some
iterations. At the end, a final hypothesis is cre-
ated by combining the predictions of the classifiers
learned in each view.
As described in Section 4, we use two classi-
fiers for the prosodic event detection task based
on two different information sources: one is the
acoustic evidence extracted from the speech signal
of an utterance; the other is the lexical and syn-
tactic evidence such as syllables, words, POS tags
and phrasal boundary information. These are two
different views for prosodic event detection and fit
the co-training framework.
The general co-training algorithm we used is
described in Algorithm 1. Given a set L of labeled
data and a set U of unlabeled data, the algorithm
first creates a smaller pool U? containing u unla-
beled data. It then iterates in the following proce-
dure. First, we use L to train two distinct classi-
fiers: the acoustic-prosodic classifier h1, and the
syntactic classifier h2. These two classifiers are
used to examine the unlabeled set U? and assign
?possible? labels. Then we select some samples
to add to L. Finally, the pool U? is recreated from
U at random. This iteration continues until reach-
ing the defined number of iterations or U is empty.
The main issue of co-training is to select train-
ing samples for next iteration so as to minimize
noise and maximize training utility. There are two
issues: (1) the accurate self-labeling method for
unlabeled data and (2) effective heuristics to se-
Algorithm 1 General co-training algorithm.
Given a set L of labeled training data and a set
U of unlabeled data
Randomly select U? from U, |U?|=u
while iteration < k do
Use L to train classifiers h1 and h2
Apply h1 and h2 to assign labels for all ex-
amples in U?
Select n self-labeled samples and add to L
Remove these n samples from U
Recreate U? by choosing u instances ran-
domly from U
end while
lect more informative examples. We investigate
different approaches to address these issues for
the prosodic event detection task. The first is-
sue is how to assign possible labels accurately.
The general method is to let the two classifiers
predict the class for a given sample, and if they
agree, the hypothesized label is used. However,
when this agreement-based approach is used for
prosodic event detection, we notice that there is
not only difference in the labeling accuracy be-
tween positive and negative samples, but also an
imbalance of the self-labeled positive and negative
examples (details in Section 6). Therefore we be-
lieve that using the hard decisions from the two
classifiers along with the agreement-based rule is
not enough to label the unlabeled samples. To ad-
dress this problem, we propose an approximated
confidence measure based on the combined classi-
fier (Equation 3). First, we take a squared root of
the classifier?s posterior probabilities for the two
classes, denoted as score(pos) and score(neg),
respectively. Our proposed confidence is the dis-
tance between these two scores. For example, if
the classifier?s hypothesized label is positive, then:
Positive confidence=score(pos)-score(neg)
Similarly if the classifier?s hypothesis is negative,
we calculate a negative confidence:
Negative confidence=score(neg)-score(pos)
Then we apply different thresholds of confi-
dence level for positive and negative labeling. The
thresholds are chosen based on the accuracy distri-
bution obtained on the labeled development data
and are reestimated at every iteration. Figure 2
shows the accuracy distribution for accent detec-
tion according to different confidence levels in the
first iteration. In Figure 2, if we choose 70% label-
ing accuracy, the positive confidence level is about
544
0 0.2 0.4 0.6 0.8 1
0.2
0.4
0.6
0.8
1
Confidence level
A
cc
ur
ac
y
Figure 2: Approximated confidence level and la-
beling accuracy on accent detection task.
0.1 and the negative confidence level is about 0.8.
In our confidence-based approach, the samples
with a confidence level higher than these thresh-
olds are assigned with the classifier?s hypothesized
labels, and the other samples are disregarded.
The second problem in co-training is how to
select informative samples. Active learning ap-
proaches, such as Muslea et al (2000), can gener-
ally select more informative samples, for example,
samples for which two classifiers disagree (since
one of two classifiers is wrong) and ask for human
labels. Co-training approaches cannot, however,
use this selection method since there is a risk to
label the disagreed samples. Usually co-training
selects samples for which two classifiers have the
same prediction but high difference in their con-
fidence measures. Based on this idea, we applied
three sampling strategies on top of our confidence-
based labeling method:
? Random selection: randomly select samples
from those that the two classifiers have dif-
ferent posterior probabilities.
? Most confident selection: select samples that
have the highest posterior probability based
on one classifier, and at the same time there
is certain posterior probability difference be-
tween the two classifiers.
? Most different selection: select samples that
have the most difference between the two
classifiers? posterior probabilities.
The first strategy is appropriate for base classi-
fiers that lack the capability of estimating the pos-
terior probability of their predictions. The second
is appropriate for base classifiers that have high
classification accuracy and also with high poste-
rior probability. The last one is also appropriate
for accurate classifiers and expected to converge
utter. word syll Speaker
Test Set 102 5,448 8,962 f1a, m1b
Development Set 20 1,356 2,275 f2b, f3b
Labeled set L 5 347 573 m2b, m3b
Unlabeled set U 1,027 77,207 129,305 m4b
Table 1: Training and test sets.
faster since big mistakes of one of the two classi-
fiers can be fixed. These sample selection strate-
gies share some similarity with those in previous
work (Steedman et al, 2003).
6 Experiments and results
Our goal is to determine whether the co-training
algorithm described above could successfully use
the unlabeled data for prosodic event detection. In
our experiment, 268 ToBI labeled utterances and
886 unlabeled utterances in BU corpus were used.
Among labeled data, 102 utterances of all f1a and
m1b speakers are used for testing, 20 utterances
randomly chosen from f2b, f3b, m2b, m3b, and
m4b are used as development set to optimize pa-
rameters such as ? and confidence level thresh-
old, 5 utterances are used as the initial training
set L, and the rest of the data is used as unlabeled
set U, which has 1027 unlabeled utterances (we
removed the human labels for co-training exper-
iments). The detailed training and test setting is
shown in Table 1.
First of all, we compare the learning curves us-
ing our proposed confidence-based method to as-
sign possible labels with the simple agreement-
based random selection method. We expect that if
self-labeling is accurate, adding new samples ran-
domly drawn from these self-labeled data gener-
ally should not make performance worse. For this
experiment, in every iteration, we randomly se-
lect the self-labeled samples that have at least 0.1
difference between two classifiers? posterior prob-
abilities. The number of new samples added to
training is 5% of the size of the previous training
data. Figure 3 shows the learning curves for accent
detection. The number of samples in the x-axis
is the number of syllables. The F-measure score
using the initial training data is 0.69. The dark
solid line in Figure 3 is the learning curve of the
supervised method when varying the size of the
training data. Compared with supervised method,
our proposed relative confidence-based labeling
method shows better performance when there is
545
5,000 10,000 15,000
0.55
0.6
0.65
0.7
0.75
0.8
0.85
# of samples
F?
m
ea
su
re
 
 
Supervised
Agreement based
Confidence based
Figure 3: The learning curve of agreement-based
and our proposed confidence-based random selec-
tion methods for accent detection.
Confidence Agreement
Accent
detection
% of P samples 47% 38%
P sample error 0.17 0.09
N sample error 0.12 0.22
IPB
detection
% of P samples 46% 19%
P sample error 0.12 0.01
N sample error 0.18 0.53
Break
detection
% of P samples 50% 25%
P sample error 0.15 0.03
N sample error 0.17 0.42
Table 2: Percentage of positive samples, and
averaged error rate for positive (P) and nega-
tive (N) samples for the first 20 iterations using
the agreement-based and our confidence labeling
methods.
less data, but after some iteration, the performance
is saturated earlier. However, the agreement-based
method does not yield any performance gain, in-
stead, its performance is much worse after some
iteration. The other two prosodic event detection
tasks also show similar patterns.
To analyze the reason for this performance
degradation using the agreement-based method,
we compare the labels of the newly added samples
in random selection with the reference annotation.
Table 2 shows the percentage of the positive sam-
ples added for the first 20 iterations, and the av-
erage labeling error rate of those samples for the
self-labeled positive and negative classes for two
methods. The agreement-based random selection
added more negative samples that also have higher
error rate than the positive samples. Adding these
samples has a negative impact on the classifier?s
performance. In contrast, our confidence-based
approach balances the number of positive and neg-
ative samples and significantly reduces the error
5,000 10,000 15,000
0.65
0.7
0.75
0.8
# of samples
F?
m
ea
su
re
 
 
Supervised
Random
Most confident
Most different
Figure 4: The learning curve of 3 sample selection
methods for accent detection.
rates for the negative samples as well, thus leading
to performance improvement.
Next we evaluate the efficacy of the three sam-
ple selection methods described in Section 5,
namely, random, most confident, and most dif-
ferent selections. Figure 4 shows the learning
curves for the three selection methods for accent
detection. The same configuration is used as in
the previous experiment, i.e., at least 0.1 posterior
probability difference between the two classifiers,
and adding 5% of new samples in each iteration.
All of these sample selection approaches use the
confidence-based labeling. For comparison, Fig-
ure 4 also shows the learning curve for supervised
learning when varying the training size. We can
see from the figure that compared to random selec-
tion, the most confident selection method shows
similar performance in the first few iterations, but
its performance continues to increase and the sat-
uration point is much later than random selection.
Unlike the other two sample selection methods,
most different selection results in noticeable per-
formance degradation after some iteration. This
difference is caused by the high self-labeling er-
ror rate of selected samples. Both random and
most confident selections perform better than su-
pervised learning at the first few iterations. This is
because the new samples added have different pos-
terior probabilities by the two classifiers, and thus
one of the classifiers benefits from these samples.
Learning curves for the other two tasks (break
index and IPB detection) show similar pattern for
the random and most different selection methods,
but some differences in the most confident selec-
tion results. For the IPB task, the learning curve of
the most confident selection fluctuates somewhat
in the middle of the iterations with similar per-
formance to random selection, however, afterward
the performance is better than random selection.
546
5,000 10,000 15,000 20,000 25,000
0.68
0.7
0.72
0.74
0.76
0.78
0.8
# of samples
F?
m
ea
su
re
 
 
Supervised
5 utterances
10 utterances
20 utterances5 utterances
10 utterances
20 utterances
Figure 5: The learning curves for accent detection
using different amounts of initial labeled training
data.
For the break index detection, the learning curve
of most different selection increases more slowly
than random selection at the beginning, but the sat-
uration point is much later and therefore outper-
forms the random selection at the later iterations.
We also evaluated the effect of the amount of
initial labeled training data. In this experiment,
most confident selection is used, and the other con-
figurations are the same as the previous experi-
ment. The learning curve for accent detection is
shown in Figure 5 using different numbers of utter-
ances in the initial training data. The arrow marks
indicate the start position of each learning curve.
As we can see, the learning curve when using 20
utterances is slightly better than the others, but
there is no significant performance gain according
to the size of initial labeled training data.
Finally we compared our co-training perfor-
mance with supervised learning. For supervised
learning, all labeled utterances except for the test
set are used for training. We used most confi-
dent selection with proposed self-labeling method.
The initial training data in co-training is 3% of
that used for supervised learning. After 74 iter-
ations, the size of samples of co-training is similar
to that in the supervised method. Table 3 presents
the results of three prosodic event detection tasks.
We can see that the performance of co-training for
these three tasks is slightly worse than supervised
learning using all the labeled data, but is signifi-
cantly better than the original performance using
3% of hand labeled data.
Most of the previous work for prosodic event
detection reported their results using classification
accuracy instead of F-measure. Therefore to bet-
ter compare with previous work, we present be-
low the accuracy results in our approach. The co-
training algorithm achieves the accuracy of 85.3%,
Accent IPB Break
Supervised 0.82 0.74 0.77
Co-
training
Initial training (3%) 0.69 0.59 0.62
After 74 iterations 0.80 0.71 0.75
Table 3: The results (F-measure) of prosodic
event detection for supervised and co-training ap-
proaches.
90.1%, and 86.7% respectively for accent, intona-
tional phrase boundary, and break index detection,
compared with 87.6%, 92.3%, and 88.9% in su-
pervised learning. Although the test condition is
different, our result is significantly better than that
of other semi-supervised approaches of previous
work and comparable with supervised approaches.
7 Conclusions
In this paper, we exploit the co-training method
for automatic prosodic event detection. We intro-
duced a confidence-based method to assign possi-
ble labels to unlabeled data and evaluated the per-
formance combined with informative sample se-
lection methods. Our experimental results using
co-training are significantly better than the origi-
nal supervised results using the small amount of
training data, and closer to that using supervised
learning with a large amount of data. This sug-
gests that the use of unlabeled data can lead to sig-
nificant improvement for prosodic event detection.
In our experiment, we used some labeled data
as development set to estimate some parameters.
For the future work, we will perform analysis
of loss function of each classifier in order to es-
timate parameters without labeled development
data. In addition, we plan to compare this to other
semi-supervised learning techniques such as ac-
tive learning. We also plan to use this algorithm
to annotate different types of data, such as sponta-
neous speech, and incorporate prosodic events in
spoken language applications.
Acknowledgments
This work is supported by DARPA under Contract
No. HR0011-06-C-0023. Distribution is unlim-
ited.
References
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. Proceedings of
547
the Workshop on Computational Learning Theory,
pp. 92-100.
C. W. Wightman and M. Ostendorf. 1994. Automatic
labeling of prosodic patterns. IEEE Transactions on
Speech and Audio Processing, Vol. 2(4), pp. 69-481.
G. Levow. 2006. Unsupervised and semi-supervised
learning of tone and pitch accent. Proceedings of
HLT-NAACL, pp. 224-231.
I. Muslea, S. Minton and C. Knoblock. 2000. Selec-
tive sampling with redundant views. Proceedings of
the 7th International Conference on Artificial Intel-
ligence, pp. 621-626.
J. Jeon and Y. Liu. 2009. Automatic prosodic event
detection using syllable-base acoustic and syntactic
features. Proceeding of ICASSP, pp. 4565-4568.
K. Chen, M. Hasegawa-Johnson, and A. Cohen. 2004.
An automatic prosody labeling system using ANN-
based syntactic-prosodic model and GMM-based
acoustic prosodic model. Proceedings of ICASSP,
pp. 509-512.
K. Nigam and R. Ghani. 2000 Analyzing the effec-
tiveness and applicability of Co-training Proceed-
ings 9th International Conference on Information
and Knowledge Management, pp. 86-93.
K. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf,
C. Wightman, P. Price, J. Pierrehumbert, and J.
Hirschberg. 1992. ToBI: A standard for labeling
English prosody. Proceedings of ICSLP, pp. 867-
870.
M. Steedman, S. Baker, S. Clark, J. Crim, J. Hocken-
maier, R. Hwa, M. Osborne, P. Ruhlen, A. Sarkar
2003. CLSP WS-02 Final Report: Semi-Supervised
Training for Statistical Parsing.
M. Ostendorf, P. J. Price and S. Shattuck-Hunfnagel.
1995. The Boston University Radio News Corpus.
Linguistic Data Consortium.
S. Ananthakrishnan and S. Narayanan. 2006. Com-
bining acoustic, lexical, and syntactic evidence for
automatic unsupervised prosody labeling. Proceed-
ings of ICSLP, pp. 297-300.
S. Ananthakrishnan and S. Narayanan. 2008. Auto-
matic prosodic event detection using acoustic, lex-
ical and syntactic evidence. IEEE Transactions on
Audio, Speech and Language Processing, Vol. 16(1),
pp. 216-228.
S. Clark, J. Currant, and M. Osborne. 2003. Bootstrap-
ping POS taggers using unlabeled data. Proceedings
of CoNLL, pp. 49-55.
S. Dasupta, M. L. Littman, and D. McAllester. 2001.
PAC generalization bounds for co-training. Ad-
vances in Neural Information Processing Systems,
Vol. 14, pp. 375-382.
S. Goldman and Y. Zhou. 2000. Enhancing supervised
learning with unlabeled data. Proceedings of the
Seventeenth International Conference on Machine
Learning, pp. 327-334.
V. K. Rangarajan Sridhar, S. Bangalore, and S.
Narayanan. 2008. Exploiting acoustic and syntactic
features for automatic prosody labeling in a maxi-
mum entropy framework. IEEE Transactions on Au-
dio, Speech, and Language processing, pp. 797-811.
W. Wang, Z. Huang, and M. Harper. 2007. Semi-
supervised learning for part-of-speech tagging of
Mandarin transcribed speech. Proceeding of
ICASSP, pp. 137-140.
548
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 732?741,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
N-Best Rescoring Based on Pitch-accent Patterns
Je Hun Jeon1 Wen Wang2 Yang Liu1
1Department of Computer Science, The University of Texas at Dallas, USA
2Speech Technology and Research Laboratory, SRI International, USA
{jhjeon,yangl}@hlt.utdallas.edu, wwang@speech.sri.com
Abstract
In this paper, we adopt an n-best rescoring
scheme using pitch-accent patterns to improve
automatic speech recognition (ASR) perfor-
mance. The pitch-accent model is decoupled
from the main ASR system, thus allowing us
to develop it independently. N-best hypothe-
ses from recognizers are rescored by addi-
tional scores that measure the correlation of
the pitch-accent patterns between the acoustic
signal and lexical cues. To test the robustness
of our algorithm, we use two different data
sets and recognition setups: the first one is En-
glish radio news data that has pitch accent la-
bels, but the recognizer is trained from a small
amount of data and has high error rate; the sec-
ond one is English broadcast news data using
a state-of-the-art SRI recognizer. Our experi-
mental results demonstrate that our approach
is able to reduce word error rate relatively by
about 3%. This gain is consistent across the
two different tests, showing promising future
directions of incorporating prosodic informa-
tion to improve speech recognition.
1 Introduction
Prosody refers to the suprasegmental features of nat-
ural speech, such as rhythm and intonation, since
it normally extends over more than one phoneme
segment. Speakers use prosody to convey paralin-
guistic information such as emphasis, intention, atti-
tude, and emotion. Humans listening to speech with
natural prosody are able to understand the content
with low cognitive load and high accuracy. How-
ever, most modern ASR systems only use an acous-
tic model and a language model. Acoustic informa-
tion in ASR is represented by spectral features that
are usually extracted over a window length of a few
tens of milliseconds. They miss useful information
contained in the prosody of the speech that may help
recognition.
Recently a lot of research has been done in au-
tomatic annotation of prosodic events (Wightman
and Ostendorf, 1994; Sridhar et al, 2008; Anan-
thakrishnan and Narayanan, 2008; Jeon and Liu,
2009). They used acoustic and lexical-syntactic
cues to annotate prosodic events with a variety of
machine learning approaches and achieved good
performance. There are also many studies us-
ing prosodic information for various spoken lan-
guage understanding tasks. However, research using
prosodic knowledge for speech recognition is still
quite limited. In this study, we investigate leverag-
ing prosodic information for recognition in an n-best
rescoring framework.
Previous studies showed that prosodic events,
such as pitch-accent, are closely related with acous-
tic prosodic cues and lexical structure of utterance.
The pitch-accent pattern given acoustic signal is
strongly correlated with lexical items, such as syl-
lable identity and canonical stress pattern. There-
fore as a first study, we focus on pitch-accent in this
paper. We develop two separate pitch-accent de-
tection models, using acoustic (observation model)
and lexical information (expectation model) respec-
tively, and propose a scoring method for the cor-
relation of pitch-accent patterns between the two
models for recognition hypotheses. The n-best list
is rescored using the pitch-accent matching scores
732
combined with the other scores from the ASR sys-
tem (acoustic and language model scores). We show
that our method yields a word error rate (WER) re-
duction of about 3.64% and 2.07% relatively on two
baseline ASR systems, one being a state-of-the-art
recognizer for the broadcast news domain. The fact
that it holds across different baseline systems sug-
gests the possibility that prosody can be used to help
improve speech recognition performance.
The remainder of this paper is organized as fol-
lows. In the next section, we review previous work
briefly. Section 3 explains the models and features
for pitch-accent detection. We provide details of our
n-best rescoring approach in Section 4. Section 5
describes our corpus and baseline ASR setup. Sec-
tion 6 presents our experiments and results. The last
section gives a brief summary along with future di-
rections.
2 Previous Work
Prosody is of interest to speech researchers be-
cause it plays an important role in comprehension
of spoken language by human listeners. The use
of prosody in speech understanding applications has
been quite extensive. A variety of applications
have been explored, such as sentence and topic seg-
mentation (Shriberg et al, 2000; Rosenberg and
Hirschberg, 2006), word error detection (Litman et
al., 2000), dialog act detection (Sridhar et al, 2009),
speaker recognition (Shriberg et al, 2005), and emo-
tion recognition (Benus et al, 2007), just to name a
few.
Incorporating prosodic knowledge is expected
to improve the performance of speech recogni-
tion. However, how to effectively integrate prosody
within the traditional ASR framework is a difficult
problem, since prosodic features are not well de-
fined and they come from a longer region, which is
different from spectral features used in current ASR
systems. Various research has been conducted try-
ing to incorporate prosodic information in ASR. One
way is to directly integrate prosodic features into
the ASR framework (Vergyri et al, 2003; Ostendorf
et al, 2003; Chen and Hasegawa-Johnson, 2006).
Such efforts include prosody dependent acoustic and
pronunciation model (allophones were distinguished
according to different prosodic phenomenon), lan-
guage model (words were augmented by prosody
events), and duration modeling (different prosodic
events were modeled separately and combined with
conventional HMM). This kind of integration has
advantages in that spectral and prosodic features are
more tightly coupled and jointly modeled. Alterna-
tively, prosody was modeled independently from the
acoustic and language models of ASR and used to
rescore recognition hypotheses in the second pass.
This approach makes it possible to independently
model and optimize the prosodic knowledge and to
combine with ASR hypotheses without any modi-
fication of the conventional ASR modules. In or-
der to improve the rescoring performance, various
prosodic knowledge was studied. (Ananthakrishnan
and Narayanan, 2007) used acoustic pitch-accent
pattern and its sequential information given lexi-
cal cues to rescore n-best hypotheses. (Kalinli and
Narayanan, 2009) used acoustic prosodic cues such
as pitch and duration along with other knowledge
to choose a proper word among several candidates
in confusion networks. Prosodic boundaries based
on acoustic cues were used in (Szaszak and Vicsi,
2007).
We take a similar approach in this study as the
second approach above in that we develop prosodic
models separately and use them in a rescoring
framework. Our proposed method differs from pre-
vious work in the way that the prosody model is used
to help ASR. In our approach, we explicitly model
the symbolic prosodic events based on acoustic and
lexical information. We then capture the correla-
tion of pitch-accent patterns between the two differ-
ent cues, and use that to improve recognition perfor-
mance in an n-best rescoring paradigm.
3 Prosodic Model
Among all the prosodic events, we use only pitch-
accent pattern in this study, because previous stud-
ies have shown that acoustic pitch-accent is strongly
correlated with lexical items, such as canonical
stress pattern and syllable identity that can be eas-
ily acquired from the output of conventional ASR
and pronunciation dictionary. We treat pitch-accent
detection as a binary classification task, that is, a
classifier is used to determine whether the base unit
is prominent or not. Since pitch-accent is usually
733
carried by syllables, we use syllables as our units,
and the syllable definition of each word is based
on CMU pronunciation dictionary which has lexi-
cal stress and syllable boundary marks (Bartlett et
al., 2009). We separately develop acoustic-prosodic
and lexical-prosodic models and use the correlation
between the two models for each syllable to rescore
the n-best hypotheses of baseline ASR systems.
3.1 Acoustic-prosodic Features
Similar to most previous work, the prosodic features
we use include pitch, energy, and duration. We also
add delta features of pitch and energy. Duration in-
formation for syllables is derived from the speech
waveform and phone-level forced alignment of the
transcriptions. In order to reduce the effect by both
inter-speaker and intra-speaker variation, both pitch
and energy values are normalized (z-value) with ut-
terance specific means and variances. For pitch, en-
ergy, and their delta values, we apply several cate-
gories of 12 functions to generate derived features.
? Statistics (7): minimum, maximum, range,
mean, standard deviation, skewness and kurto-
sis value. These are used widely in prosodic
event detection and emotion detection.
? Contour (5): This is approximated by taking
5 leading terms in the Legendre polynomial
expansion. The approximation of the contour
using the Legendre polynomial expansion has
been successfully applied in quantitative pho-
netics (Grabe et al, 2003) and in engineering
applications (Dehak et al, 2007). Each term
models a particular aspect of the contour, such
as the slope, and information about the curva-
ture.
We use 6 duration features, that is, raw, normal-
ized, and relative durations (ms) of the syllable and
vowel. Normalization (z-value) is performed based
on statistics for each syllable and vowel. The rela-
tive value is the difference between the normalized
current duration and the following one.
In the above description, we assumed that the
event of a syllable is only dependent on its observa-
tions, and did not consider contextual effect. To al-
leviate this restriction, we expand the features by in-
corporating information about the neighboring sylla-
bles. Based on the study in (Jeon and Liu, 2010) that
evaluated using left and right contexts, we choose to
use one previous and one following context in the
features. The total number of features used in this
study is 162.
3.2 Lexical-prosodic Features
There is a very strong correlation between pitch-
accent in an utterance and its lexical information.
Previous studies have shown that the lexical fea-
tures perform well for pitch-accent prediction. The
detailed features for training the lexical-prosodic
model are as follows.
? Syllable identity: We kept syllables that appear
more than 5 times in the training corpus. The
other syllables that occur less are collapsed into
one syllable representation.
? Vowel phone identity: We used vowel phone
identity as a feature.
? Lexical stress: This is a binary feature to rep-
resent if the syllable corresponds to a lexical
stress based on the pronunciation dictionary.
? Boundary information: This is a binary feature
to indicate if there is a word boundary before
the syllable.
For lexical features, based on the study in (Jeon
and Liu, 2010), we added two previous and two fol-
lowing contexts in the final features.
3.3 Prosodic Model Training
We choose to use a support vector machine (SVM)
classifier1 for the prosodic model based on previous
work on prosody labeling study in (Jeon and Liu,
2010). We use RBF kernel for the acoustic model,
and 3-order polynomial kernel for the lexical model.
In our experiments, we investigate two kinds
of training methods for prosodic modeling. The
first one is a supervised method where models are
trained using all the labeled data. The second is
a semi-supervised method using co-training algo-
rithm (Blum and Mitchell, 1998), described in Algo-
rithm 1. Given a set L of labeled data and a set U of
unlabeled data with two views, it then iterates in the
1LIBSVM ? A Library for Support Vector Machines, loca-
tion: http://www.csie.ntu.edu.tw/?cjlin/libsvm/
734
Algorithm 1 Co-training algorithm.
Given:
- L: labeled examples; U: unlabeled examples
- there are two views V1 and V2 on an example x
Initialize:
- L1=L, samples used to train classifiers h1
- L2=L, samples used to train classifiers h2
Loop for k iterations
- create a small pool U? choosing from U
- use V1(L1) to train classifier h1
and V2(L2) to train classifier h2
- let h1 label/select examples Dh1 from U?
- let h2 label/select examples Dh2 from U?
- add self-labeled examples Dh1 to L2
and Dh2 to L1
- remove Dh1 and Dh2 from U
following procedure. The algorithm first creates a
smaller pool U? containing unlabeled data from U. It
uses Li (i = 1, 2) to train two distinct classifiers: the
acoustic classifier h1, and the lexical classifier h2.
We use function Vi (i = 1, 2) to represent that only
a single view is used for training h1 or h2. These two
classifiers are used to make predictions for the unla-
beled setU?, and only when they agree on the predic-
tion for a sample, their predicted class is used as the
label for this sample. Then among these self-labeled
samples, the most confident ones by one classifier
are added to the data set Li for training the other
classifier. This iteration continues until reaching the
defined number of iterations. In our experiment, the
size of the pool U? is 5 times of the size of training
data Li, and the size of the added self-labeled ex-
ample set, Dhi , is 5% of Li. For the newly selected
Dhi , the distribution of the positive and negative ex-
amples is the same as that of the training data Li.
This co-training method is expected to cope with
two problems in prosodic model training. The first
problem is the different decision patterns between
the two classifiers: the acoustic model has relatively
higher precision, while the lexical model has rela-
tively higher recall. The goal of the co-training al-
gorithm is to learn from the difference of each clas-
sifier, thus it can improve the performance as well
as reduce the mismatch of two classifiers. The sec-
ond problem is the mismatch of data used for model
training and testing, which often results in system
performance degradation. Using co-training, we can
use the unlabeled data from the domain that matches
the test data, adapting the model towards test do-
main.
4 N-Best Rescoring Scheme
In order to leverage prosodic information for bet-
ter speech recognition performance, we augment the
standard ASR equation to include prosodic informa-
tion as following:
W? = argmax
W
p(W |As, Ap)
= argmax
W
p(As, Ap|W )p(W ) (1)
where As and Ap represent acoustic-spectral fea-
tures and acoustic-prosodic features. We can further
assume that spectral and prosodic features are con-
ditionally independent given a word sequence W ,
therefore, Equation 1 can be rewritten as following:
W? ? argmax
W
p(As|W )p(W )p(Ap|W ) (2)
The first two terms stand for the acoustic and lan-
guage models in the original ASR system, and the
last term means the prosody model we introduce. In-
stead of using the prosodic model in the first pass de-
coding, we use it to rescore n-best candidates from
a speech recognizer. This allows us to train the
prosody models independently and better optimize
the models.
For p(Ap|W ), the prosody score for a word se-
quence W , in this work we propose a method to es-
timate it, also represented as scoreW?prosody(W ).
The idea of scoring the prosody patterns is that there
is some expectation of pitch-accent patterns given
the lexical sequence (W ), and the acoustic pitch-
accent should match with this expectation. For in-
stance, in the case of a prominent syllable, both
acoustic and lexical evidence show pitch-accent, and
vice versa. In order to maximize the agreement be-
tween the two sources, we measure how good the
acoustic pitch-accent in speech signal matches the
given lexical cues. For each syllable Si in the n-best
list, we use acoustic-prosodic cues (ai) to estimate
the posterior probability that the syllable is promi-
nent (P), p(P |ai). Similarly, we use lexical cues (li)
735
to determine the syllable?s pitch-accent probability
p(P |li). Then the prosody score for a syllable Si is
estimated by the match of the pitch-accent patterns
between acoustic and lexical information using the
difference of the posteriors from the two models:
scoreS?prosody(Si) ? 1? | p(P |ai) ? p(P |li) | (3)
Furthermore, we take into account the effect due
to varying durations for different syllables. We no-
tice that syllables without pitch-accent have much
shorter duration than the prominent ones, and the
prosody scores for the short syllables tend to be
high. This means that if a syllable is split into two
consecutive non-prominent syllables, the agreement
score may be higher than a long prominent syllable.
Therefore, we introduce a weighting factor based on
syllable duration (dur(i)). For a candidate word se-
quence (W) consisting of n syllables, its prosodic
score is the sum of the prosodic scores for all the
syllables in it weighted by their duration (measured
using milliseconds), that is:
scoreW?prosody(W ) ?
n
?
i=1
log(scoreS?prosody(Si)) ? dur(i) (4)
We then combine this prosody score with the
original acoustic and language model likelihood
(P (As|W ) and P (W ) in Equation 2). In practice,
we need to weight them differently, therefore, the
combined score for a hypothesis W is:
Score(W ) = ? ? scoreW?prosody(W )
+ scoreASR(W ) (5)
where scoreASR(W ) is generated by ASR systems
(composed of acoustic and language model scores)
and ? is optimized using held out data.
5 Data and Baseline Systems
Our experiments are carried out using two different
data sets and two different recognition systems as
well in order to test the robustness of our proposed
method.
The first data set is the Boston University Radio
News Corpus (BU) (Ostendorf et al, 1995), which
consists of broadcast news style read speech. The
BU corpus has about 3 hours of read speech from
7 speakers (3 female, 4 male). Part of the data has
been labeled with ToBI-style prosodic annotations.
In fact, the reason that we use this corpus, instead of
other corpora typically used for ASR experiments,
is because of its prosodic labels. We divided the
entire data corpus into a training set and a test set.
There was no speaker overlap between training and
test sets. The training set has 2 female speakers (f2
and f3) and 3 male ones (m2, m3, m4). The test set is
from the other two speakers (f1 and m1). We use 200
utterances for the recognition experiments. Each ut-
terance in BU corpus consists of more than one sen-
tences, so we segmented each utterance based on
pause, resulting in a total number of 713 segments
for testing. We divided the test set roughly equally
into two sets, and used one for parameter tuning and
the other for rescoring test. The recognizer used for
this data set was based on Sphinx-32. The context-
dependent triphone acoustic models with 32 Gaus-
sian mixtures were trained using the training par-
tition of the BU corpus described above, together
with the broadcast new data. A standard back-off tri-
gram language model with Kneser-Ney smoothing
was trained using the combined text from the train-
ing partition of the BU, Wall Street Journal data, and
part of Gigaword corpus. The vocabulary size was
about 10K words and the out-of-vocabulary (OOV)
rate on the test set was 2.1%.
The second data set is from broadcast news (BN)
speech used in the GALE program. The recognition
test set contains 1,001 utterances. The n-best hy-
potheses for this data set are generated by a state-of-
the-art SRI speech recognizer, developed for broad-
cast news speech (Stolcke et al, 2006; Zheng et
al., 2007). This system yields much better perfor-
mance than the first one. We also divided the test
set roughly equally into two sets for parameter tun-
ing and testing. From the data used for training the
speech recognizer, we randomly selected 5.7 hours
of speech (4,234 utterances) for the co-training al-
gorithm for the prosodic models.
For prosodic models, we used a simple binary
representation of pitch-accent in the form of pres-
ence versus absence. The reference labels are de-
2CMU Sphinx - Speech Recognition Toolkit, location:
http://www.speech.cs.cmu.edu/sphinx/tutorial.html
736
rived from the ToBI annotation in the BU corpus,
and the ratio of pitch-accented syllables is about
34%. Acoustic-prosodic and lexical-prosodic mod-
els were separately developed using the features de-
scribed in Section 3. Feature extraction was per-
formed at the syllable level from force-aligned data.
For the supervised approach, we used those utter-
ances in the training data partition with ToBI labels
in the BU corpus (245 utterances, 14,767 syllables).
For co-training, the labeled data from BU corpus is
used as initial training, and the other unlabeled data
from BU and BN are used as unlabeled data.
6 Experimental Results
6.1 Pitch-accent Detection
First we evaluate the performance of our acoustic-
prosodic and lexical-prosodic models for pitch-
accent detection. For rescoring, not only the ac-
curacies of the two individual prosodic models are
important, but also the pitch-accent agreement score
between the two models (as shown in Equation 3)
is critical, therefore, we present results using these
two metrics. Table 1 shows the accuracy of each
model for pitch-accent detection, and also the av-
erage prosody score of the two models (i.e., Equa-
tion 3) for positive and negative classes (using ref-
erence labels). These results are based on the BU
labeled data in the test set. To compare our pitch ac-
cent detection performance with previous work, we
include the result of (Jeon and Liu, 2009) as a ref-
erence. Compared to previous work, the acoustic
model achieved similar performance, while the per-
formance of lexical model is a bit lower. The lower
performance of lexical model is mainly because we
do not use part-of-speech (POS) information in the
features, since we want to only use the word output
from the ASR system (without additional POS tag-
ging).
As shown in Table 1, when using the co-training
algorithm, as described in Section 3.3, the over-
all accuracies improve slightly and therefore the
prosody score is also increased. We expect this im-
proved model will be more beneficial for rescoring.
6.2 N-Best Rescoring
For the rescoring experiment, we use 100-best hy-
potheses from the two different ASR systems, as de-
Accuracy(%) Prosody score
Acoustic Lexical Pos Neg
Supervised 83.97 84.48 0.747 0.852
Co-training 84.54 84.99 0.771 0.867
Reference 83.53 87.92 - -
Table 1: Pitch accent detection results: performance of
individual acoustic and lexical models, and the agreement
between the twomodels (i.e., prosody score for a syllable,
Equation 3) for positive and negative classes. Also shown
is the reference result for pitch accent detection from Jeon
and Liu (2009).
scribed in Section 5. We apply the acoustic and lex-
ical prosodic models to each hypothesis to obtain its
prosody score, and combine it with ASR scores to
find the top hypothesis. The weights were optimized
using one test set and applied to the other. We report
the average result of the two testings.
Table 2 shows the rescoring results using the first
recognition system on BU data, which was trained
with a relatively small amount of data. The 1-
best baseline uses the first hypothesis that has the
best ASR score. The oracle result is from the best
hypothesis that gives the lowest WER by compar-
ing all the candidates to the reference transcript.
We used two prosodic models as described in Sec-
tion 3.3. The first one is the base prosodic model us-
ing supervised training (S-model). The second is the
prosodic model with the co-training algorithm (C-
model). For these rescoring experiments, we tuned
? (in Equation 5) when combining the ASR acous-
tic and language model scores with the additional
prosody score. The value in parenthesis in Table 2
means the relative WER reduction when compared
to the baseline result. We show the WER results for
both the development and the test set.
As shown in Table 2, we observe performance
improvement using our rescoring method. Using
the base S-model yields reasonable improvement,
and C-model further reduces WER. Even though the
prosodic event detection performance of these two
prosodic models is similar, the improved prosody
score between the acoustic and lexical prosodic
models using co-training helps rescoring. After
rescoring using prosodic knowledge, the WER is re-
duced by 0.82% (3.64% relative). Furthermore, we
notice that the difference between development and
737
WER (%)
1-best baseline 22.64
S-model
Dev 21.93 (3.11%)
Test 22.10 (2.39%)
C-model
Dev 21.76 (3.88%)
Test 21.81 (3.64%)
Oracle 15.58
Table 2: WER of the baseline system and after rescoring
using prosodic models. Results are based on the first ASR
system.
test data is smaller when using the C-model than S-
model, which means that the prosodic model with
co-training is more stable. In fact, we found that
the optimal value of ? is 94 and 57 for the two
folds using S-model, and is 99 and 110 for the C-
model. These verify again that the prosodic scores
contribute more in the combination with ASR likeli-
hood scores when using the C-model, and are more
robust across different tuning sets. Ananthakrish-
nan and Narayanan (2007) also used acoustic/lexical
prosodic models to estimate a prosody score and re-
ported 0.3% recognition error reduction on BU data
when rescoring 100-best list (their baseline WER is
22.8%). Although there is some difference in experi-
mental setup (data, classifier, features) between ours
and theirs, our S-model showed comparable perfor-
mance gain and the result of C-model is significantly
better than theirs.
Next we test our n-best rescoring approach using a
state-of-the-art SRI speech recognizer on BN data to
verify if our approach can generalize to better ASR
n-best lists. This is often the concern that improve-
ments observed on a poor ASR system do not hold
for better ASR systems. The rescoring results are
shown in Table 3. We can see that the baseline per-
formance of this recognizer is much better than that
of the first ASR system (even though the recogni-
tion task is also harder). Our rescoring approach
still yields performance gain even using this state-
of-the-art system. The WER is reduced by 0.29%
(2.07% relative). This error reduction is lower than
that in the first ASR system. There are several pos-
sible reasons. First, the baseline ASR performance
is higher, making further improvement hard; sec-
ond, and more importantly, the prosody models do
not match well to the test domain. We trained the
prosody model using the BU data. Even though co-
training is used to leverage unlabeled BN data to re-
duce data mismatch, it is still not as good as using
labeled in-domain data for model training.
WER (%)
1-best baseline 13.77
S-model
Dev 13.53 (1.78%)
Test 13.55 (1.63%)
C-model
Dev 13.48 (2.16%)
Test 13.49 (2.07%)
Oracle 9.23
Table 3: WER of the baseline system and after rescoring
using prosodic models. Results are based on the second
ASR system.
6.3 Analysis and Discussion
We also analyze what kinds of errors are reduced
using our rescoring approach. Most of the error re-
duction came from substitution and insertion errors.
Deletion error rate did not change much or some-
times even increased. For a better understanding of
the improvement using the prosody model, we ana-
lyzed the pattern of corrections (the new hypothesis
after rescoring is correct while the original 1-best is
wrong) and errors. Table 4 shows some positive and
negative examples from rescoring results using the
first ASR system. In this table, each word is asso-
ciated with some binary expressions inside a paren-
thesis, which stand for pitch-accent markers. Two
bits are used for each syllable: the first one is for
the acoustic-prosodic model and the second one is
for the lexical-prosodic model. For both bits, 1 rep-
resents pitch-accent, and 0 indicates none. These
hard decisions are obtained by setting a threshold of
0.5 for the posterior probabilities from the acoustic
or lexical models. For example, when the acoustic
classifier predicts a syllable as pitch-accented and
the lexical one as not accented, ?10? marker is as-
signed to the syllable. The number of such pairs of
pitch-accent markers is the same as the number of
syllables in a word. The bold words indicate correct
words and italic means errors. As shown in the pos-
itive example of Table 4, we find that our prosodic
model is effective at identifying an erroneous word
when it is split into two words, resulting in dif-
ferent pitch-accent patterns. Language models are
738
Positive example
1-best : most of the massachusetts
(11 ) (10) (00) (11 00 01 00)
rescored : most other massachusetts
(11 ) (11 00) (11 00 01 00)
Negative example
1-best : robbery and on a theft
(11 00 00) (00) (10) (00) (11)
rescored : robbery and lot of theft
(11 00 00) (00) (11) (00) (11)
Table 4: Examples of rescoring results. Binary expressions inside the parenthesis below a word represent pitch-accent
markers for the syllables in the word.
not good at correcting this kind of errors since both
word sequences are plausible. Our model also intro-
duces some errors, as shown in the negative exam-
ple, which is mainly due to the inaccurate prosody
model.
We conducted more prosody rescoring experi-
ments in order to understand the model behavior.
These analyses are based on the n-best list from the
first ASR system for the entire test set. In the first
experiment, among the 100 hypotheses in n-best list,
we gave a prosody score of 0 to the 100th hypothe-
sis, and used automatically obtained prosodic scores
for the other hypotheses. A zero prosody score
means the perfect agreement given acoustic and lex-
ical cues. The original scores from the recognizer
were combined with the prosodic scores for rescor-
ing. This was to verify that the range of the weight-
ing factor ? estimated on the development data (us-
ing the original, not the modified prosody scores for
all candidates) was reasonable to choose proper hy-
pothesis among all the candidates. We noticed that
27% of the times the last hypothesis on the list was
selected as the best hypothesis. This hypothesis has
the highest prosodic scores, but lowest ASR score.
This result showed that if the prosodic models were
accurate enough, the correct candidate could be cho-
sen using our rescoring framework.
In the second experiment, we put the reference
text together with the other candidates. We use the
same ASR scores for all candidates, and generated
prosodic scores using our prosody model. This was
to test that our model could pick up correct candi-
date using only the prosodic score. We found that
for 26% of the utterances, the reference transcript
was chosen as the best one. This was significantly
better than random selection (i.e., 1/100), suggest-
ing the benefit of the prosody model; however, this
percentage is not very high, implying the limitation
of prosodic information for ASR or the current im-
perfect prosodic models.
In the third experiment, we replaced the 100th
candidate with the reference transcript and kept its
ASR score. When using our prosody rescoring ap-
proach, we obtained a relative error rate reduction
of 6.27%. This demonstrates again that our rescor-
ing method works well ? if the correct hypothesis is
on the list, even though with a low ASR score, us-
ing prosodic information can help identify the cor-
rect candidate.
Overall the performance improvement we ob-
tained from rescoring by incorporating prosodic in-
formation is very promising. Our evaluation using
two different ASR systems shows that the improve-
ment holds even when we use a state-of-the-art rec-
ognizer and the training data for the prosody model
does not come from the same corpus. We believe
the consistent improvements we observed for differ-
ent conditions show that this is a direction worthy of
further investigation.
7 Conclusion
In this paper, we attempt to integrate prosodic infor-
mation for ASR using an n-best rescoring scheme.
This approach decouples the prosodic model from
the main ASR system, thus the prosodic model can
be built independently. The prosodic scores that we
use for n-best rescoring are based on the matching
of pitch-accent patterns by acoustic and lexical fea-
tures. Our rescoring method achieved a WER reduc-
tion of 3.64% and 2.07% relatively using two differ-
ent ASR systems. The fact that the gain holds across
different baseline systems (including a state-of-the-
739
art speech recognizer) suggests the possibility that
prosody can be used to improve speech recognition
performance.
As suggested by our experiments, better prosodic
models can result in more WER reduction. The per-
formance of our prosodic model was improved with
co-training, but there are still problems, such as the
imbalance of the two classifiers? prediction, as well
as for the two events. In order to address these prob-
lems, we plan to improve the labeling and selec-
tion method in the co-training algorithm, and also
explore other training algorithms to reduce domain
mismatch. Furthermore, we are also interested in
evaluating our approach on the spontaneous speech
domain, which is quite different from the data we
used in this study.
In this study, we used n-best rather than lattice
rescoring. Since the prosodic features we use in-
clude cross-word contextual information, it is not
straightforward to apply it directly to lattices. In
our future work, we will develop models with only
within-word context, and thus allowing us to explore
lattice rescoring, which we expect will yield more
performance gain.
References
Sankaranarayanan Ananthakrishnan and Shrikanth
Narayanan. 2007. Improved speech recognition using
acoustic and lexical correlated of pitch accent in a
n-best rescoring framework. Proc. of ICASSP, pages
65?68.
Sankaranarayanan Ananthakrishnan and Shrikanth
Narayanan. 2008. Automatic prosodic event detec-
tion using acoustic, lexical and syntactic evidence.
IEEE Transactions on Audio, Speech, and Language
Processing, 16(1):216?228.
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2009. On the syllabification of phonemes. Proc. of
NAACL-HLT, pages 308?316.
Stefan Benus, Agust??n Gravano, and Julia Hirschberg.
2007. Prosody, emotions, and whatever. Proc. of In-
terspeech, pages 2629?2632.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. Proc. of the
Workshop on Computational Learning Theory, pages
92?100.
Ken Chen and Mark Hasegawa-Johnson. 2006. Prosody
dependent speech recognition on radio news corpus
of American English. IEEE Transactions on Audio,
Speech, and Language Processing, 14(1):232? 245.
Najim Dehak, Pierre Dumouchel, and Patrick Kenny.
2007. Modeling prosodic features with joint fac-
tor analysis for speaker verification. IEEE Transac-
tions on Audio, Speech, and Language Processing,
15(7):2095?2103.
Esther Grabe, Greg Kochanski, and John Coleman. 2003.
Quantitative modelling of intonational variation. Proc.
of SASRTLM, pages 45?57.
Je Hun Jeon and Yang Liu. 2009. Automatic prosodic
events detection suing syllable-based acoustic and syn-
tactic features. Proc. of ICASSP, pages 4565?4568.
Je Hun Jeon and Yang Liu. 2010. Syllable-level promi-
nence detection with acoustic evidence. Proc. of Inter-
speech, pages 1772?1775.
Ozlem Kalinli and Shrikanth Narayanan. 2009. Contin-
uous speech recognition using attention shift decoding
with soft decision. Proc. of Interspeech, pages 1927?
1930.
Diane J. Litman, Julia B. Hirschberg, and Marc Swerts.
2000. Predicting automatic speech recognition perfor-
mance using prosodic cues. Proc. of NAACL, pages
218?225.
Mari Ostendorf, Patti Price, and Stefanie Shattuck-
Hufnagel. 1995. The Boston University radio news
corpus. Linguistic Data Consortium.
Mari Ostendorf, Izhak Shafran, and Rebecca Bates.
2003. Prosody models for conversational speech
recognition. Proc. of the 2nd Plenary Meeting and
Symposium on Prosody and Speech Processing, pages
147?154.
Andrew Rosenberg and Julia Hirschberg. 2006. Story
segmentation of broadcast news in English, Mandarin
and Arabic. Proc. of HLT-NAACL, pages 125?128.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tu?r,
and Go?khan Tu?r. 2000. Prosody-based automatic seg-
mentation of speech into sentences and topics. Speech
Communication, 32(1-2):127?154.
Elizabeth Shriberg, Luciana Ferrer, Sachin S. Kajarekar,
Anand Venkataraman, and Andreas Stolcke. 2005.
Modeling prosodic feature sequences for speaker
recognition. Speech Communication, 46(3-4):455?
472.
Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore,
and Shrikanth S. Narayanan. 2008. Exploiting acous-
tic and syntactic features for automatic prosody label-
ing in a maximum entropy framework. IEEE Trans-
actions on Audio, Speech, and Language Processing,
16(4):797?811.
Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore,
and Shrikanth Narayanan. 2009. Combining lexi-
cal, syntactic and prosodic cues for improved online
740
dialog act tagging. Computer Speech and Language,
23(4):407?422.
Andreas Stolcke, Barry Chen, Horacio Franco, Venkata
Ramana Rao Gadde, Martin Graciarena, Mei-Yuh
Hwang, Katrin Kirchhoff, Arindam Mandal, Nelson
Morgan, Xin Lin, Tim Ng, Mari Ostendorf, Kemal
So?nmez, Anand Venkataraman, Dimitra Vergyri, Wen
Wang, Jing Zheng, and Qifeng Zhu. 2006. Recent in-
novations in speech-to-text transcription at SRI-ICSI-
UW. IEEE Transactions on Audio, Speech and Lan-
guage Processing, 14(5):1729?1744. Special Issue on
Progress in Rich Transcription.
Gyorgy Szaszak and Klara Vicsi. 2007. Speech recogni-
tion supported by prosodic information for fixed stress
languages. Proc. of TSD Conference, pages 262?269.
Dimitra Vergyri, Andreas Stolcke, Venkata R. R. Gadde,
Luciana Ferrer, and Elizabeth Shriberg. 2003.
Prosodic knowledge sources for automatic speech
recognition. Proc. of ICASSP, pages 208?211.
Colin W. Wightman and Mari Ostendorf. 1994. Auto-
matic labeling of prosodic patterns. IEEE Transaction
on Speech and Auido Processing, 2(4):469?481.
Jing Zheng, Ozgur Cetin, Mei-Yuh Hwang, Xin Lei, An-
dreas Stolcke, and Nelson Morgan. 2007. Combin-
ing discriminative feature, transform, and model train-
ing for large vocabulary speech recognition. Proc. of
ICASSP, pages 633?636.
741
