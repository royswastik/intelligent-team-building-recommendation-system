Proceedings of the Workshop on BioNLP, pages 97?105,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
 
Towards Automatic Generation of Gene Summary 
 
 
Feng Jin Minlie Huang 
Dept. Computer Science and Technology Dept. Computer Science and Technology 
Tsinghua University Tsinghua University 
Beijing 100084, China Beijing 100084, China 
jinfengfeng@gmail.com aihuang@tsinghua.edu.cn 
Zhiyong Lu Xiaoyan Zhu 
National Center for Biotechnology Information Dept. Computer Science and Technology 
National Library of Medicine Tsinghua University 
Bethesda, 20894, USA Beijing 100084, China 
luzh@ncbi.nlm.nih.gov zxy-dcs@tsinghua.edu.cn 
 
  
  
 
 
Abstract 
In this paper we present an extractive system that au-
tomatically generates gene summaries from the biomed-
ical literature. The proposed text summarization system 
selects and ranks sentences from multiple MEDLINE 
abstracts by exploiting gene-specific information and 
similarity relationships between sentences. We evaluate 
our system on a large dataset of 7,294 human genes and 
187,628 MEDLINE abstracts using Recall-Oriented 
Understudy for Gisting Evaluation (ROUGE), a widely 
used automatic evaluation metric in the text summariza-
tion community. Two baseline methods are used for 
comparison. Experimental results show that our system 
significantly outperforms the other two methods with 
regard to all ROUGE metrics. A demo website of our 
system is freely accessible at 
http://60.195.250.72/onbires/summary.jsp.  
1 Introduction 
Entrez Gene is a database for gene-centric infor-
mation maintained at the National Center for Bio-
technology Information (NCBI). It includes genes 
from completely sequenced genomes (e.g. Homo 
sapiens). An important part of a gene record is the 
summary field (shown in Table 1), which is a small 
piece of text that provides a quick synopsis of what 
is known about the gene, the function of its en-
coded protein or RNA products, disease associa-
tions, genetic interactions, etc. The summary field, 
when available, can help biologists to understand 
the target gene quickly by compressing a huge 
amount of knowledge from many papers to a small 
piece of text. At present, gene summaries are gen-
erated manually by the National Library of Medi-
cine (NLM) curators, a time- and labor-intensive 
process. A previous study has concluded that ma-
nual curation is not sufficient for annotation of ge-
nomic databases (Baumgartner et al, 2007). 
Indeed, of the 5 million genes currently in Entrez 
Gene, only about 20,000 genes have a correspond-
ing summary. Even in humans, arguably the most 
important species, the coverage is modest: only 26% 
of human genes are curated in this regard. The goal 
of this work is to develop and evaluate computa-
tional techniques towards automatic generation of 
gene summaries. 
To this end, we developed a text summarization 
system that takes as input MEDLINE documents 
related to a given target gene and outputs a small 
set of genic information rich sentences. Specifical-
ly, it first preprocesses and filters sentences that do 
 
97
 Gene Number  of 
Abstracts 
GO terms Human-writtenSummary 
EFEMP1 26 calcium ion binding 
protein binding 
extracellular region 
proteinaceous extracellu-
lar matrix 
This gene spans approximately 18 kb of genomic DNA and consists of 12 ex-
ons. Alternative splice patterns in the 5\' UTR result in three transcript variants 
encoding the same extracellular matrix protein. Mutations in this gene are asso-
ciated with Doyne honeycomb retinal dystrophy. 
IL20RA 15 blood coagulation 
receptor activity 
integral to membrane 
membrane 
The protein encoded by this gene is a receptor for interleukin 20 (IL20), a cyto-
kine that may be involved in epidermal function. The receptor of IL20 is a hete-
rodimeric receptor complex consisting of this protein and interleukin 20 
receptor beta (IL20B). This gene and IL20B are highly expressed in skin. The 
expression of both genes is found to be upregulated in Psoriasis. 
Table1. Two examples of human-written gene summaries 
not include enough informative words for gene 
summaries. Next, the remaining sentences are 
ranked by the sum of two individual scores: a) an 
authority score from a lexical PageRank algorithm 
(Erkan and Radev, 2004) and b) a similarity score 
between the sentence and the Gene Ontology (GO) 
terms with which the gene is annotated (To date, 
over 190,000 genes have two or more associated 
GO terms). Finally, redundant sentences are re-
moved and top ranked sentences are nominated for 
the target gene.  
In order to evaluate our system, we assembled a 
gold standard dataset consisting of handwritten 
summaries for 7,294 human genes and conducted 
an intrinsic evaluation by measuring the amount of 
overlap between the machine-selected sentences 
and human-written summaries. Our metric for the 
evaluation was ROUGE1, a widely used intrinsic 
summarization evaluation metric. 
2 Related Work 
Summarization systems aim to extract salient text 
fragments, especially sentences, from the original 
documents to form a summary. A number of me-
thods for sentence scoring and ranking have been 
developed. Approaches based on sentence position 
(Edmundson, 1969), cue phrase (McKeown and 
Radev, 1995), word frequency (Teufel and Moens, 
1997), and discourse segmentation (Boguraev and 
Kennedy, 1997) have been reported. Radev et al 
(Radev et al, 2004) developed an extractive multi-
document summarizer, MEAD, which extracts a 
summary from multiple documents based on the 
document cluster centroid, position and first-
sentence overlap. Recently, graph-based ranking 
methods, such as LexPageRank (Erkan and Radev, 
2004) and TextRank (Mihalcea and Tarau, 2004), 
                                                          
1 http://haydn.isi.edu/ROUGE/ 
have been proposed for multi-document summari-
zation. Similar to the original PageRank algorithm, 
these methods make use of similarity relationships 
between sentences and then rank sentences accord-
ing to the ?votes? or ?recommendations? from 
their neighboring sentences. 
Lin and Hovy (2000) first introduced topic sig-
natures which are topic relevant terms for summa-
rization. Afterwards, this technique was 
successfully used in a number of summarization 
systems (Hickl et al, 2007, Gupta and Nenkova et 
al., 2007). In order to improve sentence selection, 
we adopted the idea in a similar way to identify 
terms that tend to appear frequently in gene sum-
maries and subsequently filter sentences that in-
clude none or few such terms. 
Compared with newswire document summariza-
tion, much less attention has been paid to summa-
rizing MEDLINE documents for genic information. 
Ling et al (Ling et al, 2006 and 2007) presented 
an automatic gene summary generation system that 
constructs a summary based on six aspects of a 
gene, such as gene products, mutant phenotype, etc. 
In their system, sentences were ranked according 
to a) the relevance to each category (namely the 
aspect), b) the relevance to the document where 
they are from; and c) the position where sentences 
are located. Although the system performed well 
on a small group of genes (10~20 genes) from Fly-
base, their method relied heavily on high-quality 
training data that is often hard to obtain in practice.  
Yang et al reported a system (Yang et al, 2007 
and 2009) that produces gene summaries by focus-
ing on gene sets from microarray experiments. 
Their system first clustered gene set into functional 
related groups based on free text, Medical Subject 
Headings (MeSH?) and Gene Ontology (GO) fea-
tures. Then, an extractive summary was generated 
for each gene following the Edmundson paradigm 
98
 (Edmundson, 1969). Yang et al also presented 
evaluation results based on human ratings of eight 
gene summaries.  
Another related work is the second task of Text 
REtrieval Conference 2  (TREC) 2003 Genomics 
Track. Participants in the track were required to 
extract GeneRIFs from MEDLINE abstracts 
(Hersh and Bhupatiraju, 2003). Many teams ap-
proached the task as a sentence classification prob-
lem using GeneRIFs in the Entrez database as 
training data (Bhalotia et al, 2003; Jelier et al, 
2003). This task has also been approached as a sin-
gle document summarization problem (Lu et al, 
2006).  
The gene summarization work presented here 
differs from the TREC task in that it deals with 
multiple documents. In contrast to the previously 
described systems for gene summarization, our 
approach has three novel features. First, we are 
able to summarize all aspects of gene-specific in-
formation as opposed to a limited number of prede-
termined aspects. Second, we exploit a lexical 
PageRank algorithm to establish similarity rela-
tionships between sentences. The importance of a 
sentence is based not only on the sentence itself, 
but also on its neighbors in a graph representation. 
Finally, we conducted an intrinsic evaluation on a 
large publicly available dataset. The gold standard 
assembled in this work makes it possible for com-
parisons between different gene summarization 
systems without human judgments.  
3 Method 
To determine if a sentence is extract worthy, we 
consider three different aspects: (1) the number of 
salient or informative words that are frequently 
used by human curators for writing gene summa-
ries; (2) the relative importance of a sentence to be 
included in a gene summary; (3) the gene-specific 
information that is unique between different genes.  
Specifically, we look for signature terms in 
handwritten summaries for the first aspect. Ideally, 
computer generated summaries should resemble 
handwritten summaries. Thus the terms used by 
human curators should also occur frequently in 
automatically generated summaries. In this regard, 
we use a method similar to Lin and Hovy (2000) to 
identify signature terms and subsequently use them 
                                                          
2 http://ir.ohsu.edu/genomics/ 
to discard sentences that contain none or few such 
terms. For the second aspect, we adopt a lexical 
PageRank method to compute the sentence impor-
tance with a graph representation. For the last as-
pect, we treat each gene as having its own 
properties that distinguish it from others. To reflect 
such individual differences in the machine-
generated summaries, we exploit a gene?s GO an-
notations as a surrogate for its unique properties 
and look for their occurrence in abstract sentences.  
Our gene summarization system consists of 
three components: a preprocessing module, a sen-
tence ranking module, and a redundancy removal 
and summary generation module. Given a target 
gene, the preprocessing module retrieves corres-
ponding MEDLINE abstracts and GO terms ac-
cording to the gene2pubmed and gene2go data 
provided by Entrez Gene. Then the abstracts are 
split into sentences by the MEDLINE sentence 
splitter in the LingPipe3 toolkit. The sentence rank-
ing module takes these as input and first filters out 
some non-informative sentences. The remaining 
sentences are then scored according to a linear 
combination of the PageRank score and GO relev-
ance score.  Finally, a gene summary is generated 
after redundant sentences are removed. The system 
is illustrated in Figure 1 and is described in more 
detail in the following sections.  
 
 
Figure 1. System overview  
3.1 Signature Terms Extraction 
There are signature terms for different topic texts 
(Lin and Hovy, 2000). For example, terms such as 
eat, menu and fork that occur frequently in a cor-
pus may signify that the corpus is likely to be 
                                                          
3 http://alias-i.com/lingpipe/ 
Abstracts 
Sentence Segmentation 
Tokenization 
Stemming 
Signature Filtering 
PageRank Scoring 
GO Scoring 
Redundancy Removal 
GO Terms Summary 
99
 about cooking or restaurants. Similarly, there are 
signature terms for gene summaries. 
We use the Pearson?s chi-square test (Manning 
and Sch?tze, 1999) to extract topic signature terms 
from a set of handwritten summaries by comparing 
the occurrence of terms in the handwritten summa-
ries with that of randomly selected MEDLINE ab-
stracts. Let R denote the set of handwritten 
summaries and R denote the set of randomly se-
lected abstracts from MEDLINE. The null hypo-
thesis and alternative hypothesis are as follows:  
0H : ( | ) ( | )i iP t R p P t R= =   
1 1 2H : ( | ) ( | )i iP t R p p P t R= ? =   
The null hypothesis says that the term it appears 
in R and in R with an equal probability and it is 
independent from R . In contrast, the alternative 
hypothesis says that the term it is correlated with
R . We construct the following 2-by-2 contingency 
table:  
 
 R  R  
it  11O  12O  
it  21O  22O  
Table 2. Contingency table for the chi-square test. 
 
where 
11O : the frequency of term it occurring in R ; 
12O : the frequency of it occurring in R ; 
21O  : the frequency of term i it t? occurring in R ; 
22O :  the frequency of it in R .  
Then the Pearson?s chi-square statistic is computed 
by  
22
2
, 1
( )ij ij
i j ij
O E
X
E
=
?
=?  
where ijO is the observed frequency and ijE is the 
expected frequency.  
In our experiments, the significance level is set 
to 0.001, thus the corresponding chi-square value 
is 10.83. Terms with 2X value above 10.83 would 
be selected as signature terms. In total, we obtained 
1,169 unigram terms. The top ranked (by 2X value) 
signature terms are listed in Table 3. Given the set 
of signature terms, sentences containing less than 3 
signature terms are discarded. This parameter was 
determined empirically during the system devel-
opment.  
 
protein 
gene 
encode 
family 
transcription 
member 
variant 
domain 
splice 
subunit 
receptor 
isoform 
alternative 
bind 
involve 
Table 3. A sample of unigram topic signature terms. 
3.2 Lexical PageRank Scoring 
The lexical PageRank algorithm makes use of the 
similarity between sentences and ranks them by 
how similar a sentence is to all other sentences. It 
originates from the original PageRank algorithm 
(Page et al, 1998) that is based on the following 
two hypotheses:  
(1) A web page is important if it is linked by many 
other pages.  
(2) A web page is important if it is linked by im-
portant pages.  
The algorithm views the entire internet as a large 
graph in which a web page is a vertex and a di-
rected edge is connected according to the linkage. 
The salience of a vertex can be computed by a ran-
dom walk on the graph. Such graph-based methods 
have been widely adapted to such Natural Lan-
guage Processing (NLP) problems as text summa-
rization and word sense disambiguation. The 
advantage of such graph-based methods is obvious: 
the importance of a vertex is not only decided by 
itself, but also by its neighbors in a graph represen-
tation.  The random walk on a graph can imply 
more global dependence than other methods. Our 
PageRank scoring method consists of two steps: 
constructing the sentence graph and computing the 
salience score for each vertex of the graph.  
Let { |1 }iS s i N= ? ? be the sentence collec-
tion containing all the sentences to be summarized. 
According to the vector space model (Salton et al, 
1975), each sentence is  can be represented by a 
vector is
G
with each component being the weight of 
a term in is . The weight associated with a term w  
is calculated by ( )* ( )tf w isf w , where ( )tf w is the 
frequency of the term w in sentence is and ( )isf w
100
 is the inverse sentence frequency 4  of term w :
( ) 1 log( / )wisf w N n= + , where N is the total 
number of sentences in S  and wn is the number of 
sentences containing w .The similarity score be-
tween two sentences is computed using the inner 
product of the corresponding sentence vectors, as 
follows:  
( , )
|| || || ||
i j
i j
i j
s s
sim s s
s s
?
=
?
G G
G G  
Taking each sentence as a vertex, and the simi-
larity score as the weight of the edge between two 
sentences, a sentence graph is constructed. The 
graph is fully connected and undirected because 
the similarity score is symmetric.  
The sentence graph can be modeled by an adja-
cency matrix M , in which each element corres-
ponds to the weight of an edge in the graph. Thus
[ ]ij N NM ?=M is defined as:  
,
|| || || ||
0,
i j
i jij
s s
if i j
s sM
otherwise
??
??
?= ???
G G
G G
 
We normalize the row sum of matrix M  in or-
der to assure it is a stochastic matrix such that the 
PageRank iteration algorithm is applicable. The 
normalized matrix is: 
1 1
, 0
0,
N N
ij ij ij
j jij
M M if M
M
otherwise
= =
?
??
= ???
? ? . 
Using the normalized adjacency matrix, the sa-
lience score of a sentence is is computed in an 
iterative manner:  
1
(1 )
( ) ( )
N
i j ji
j
d
score s d score s M
N
=
?
= ? ? +?   
where d is a damping factor that is typically be-
tween 0.8 and 0.9 (Page et al, 1998).  
If we use a column vector p to denote the sa-
lience scores of all the sentences in S , the above 
equation can be written in a matrix form as follows:  
[ (1 ) ]Tp d d p= ? + ? ? ?M U  
                                                          
4 Isf is equivalent to idf if we view each sentence as a docu-
ment. 
where U is a square matrix with all elements being 
equal to 1/ N . The component (1 )d? ?U can be 
considered as a smoothing term which adds a small 
probability for a random walker to jump from the 
current vertex to any vertex in the graph. This 
guarantees that the stochastic transition matrix for 
iteration is irreducible and aperiodic. Therefore the 
iteration can converge to a stable state.  
In our implementation, the damping factor d is 
set to 0.85 as in the PageRank algorithm (Page et 
al., 1998). The column vector p is initialized with 
random values between 0 and 1. After the algo-
rithm converges, each component in the column 
vector p corresponds to the salience score of the 
corresponding sentence. This score is combined 
with the GO relevance score to rank sentences. 
3.3 GO Relevance Scoring 
Up to this point, our system considers only gene-
independent features, in both sentence filtering and 
PageRank-based sentence scoring. These features 
are universal across different genes. However, each 
gene is unique because of its own functional and 
structural properties. Thus we seek to include 
gene-specific features in this next step.  
The GO annotations provide one kind of gene-
specific information and have been shown to be 
useful for selecting GeneRIF candidates (Lu et al, 
2006). A gene?s GO annotations include descrip-
tions in three aspects: molecular function; biologi-
cal process; and cellular component. For example, 
the human gene AANAT (gene ID 15 in Entrez 
Gene) is annotated with the GO terms in Table 4. 
 
GO ID GO term 
GO:0004059 aralkylamine N-acetyltransferase activi-
ty 
GO:0007623 circadian rhythm 
GO:0008152 metabolic process 
GO:0008415 acyltransferase activity 
GO:0016740 transferase activity 
Table 4. GO terms for gene AANAT 
 
The GO relevance score is computed as follows: 
first, the GO terms and the sentences are both 
stemmed and stopwords are removed. For example, 
the GO terms in Table 4 are processed into a set of 
stemmed words: aralkylamin, N, acetyltransferas, 
activ, circadian, rhythm, metabol, process, acyl-
transferas and transferas.  
101
 Second, the total number of occurrence of the 
GO terms appearing in a sentence is counted. Fi-
nally, the GO relevance score is computed as the 
ratio of the total occurrence to the sentence length. 
The entire process can be illustrated by the follow-
ing pseudo codes: 
 
1 tokenize and stem the GO terms; 
2 tokenize and stem all the sentences, remove stop 
words; 
3 for each sentence is , 1,...,i N=  
( ) 0
i
GOScore s =  
for each word w  in is  
if w in the GO term set 
( )
i
GOScore s ++ 
end if 
end for 
( ) ( ) / ( )
i i i
GOScore s GOScore s length s=  
end for  
 
where ( )ilength s is the number of distinct non-stop 
words in is . For each sentence is , the GO relev-
ance score is combined with the PageRank score to 
get the overall score (? is a weight parameter be-
tween 0 and 1; see Section 4.2 for discussion): 
( ) ( ) (1 ) ( )i i iscore s PRScore s GOScore s? ?= ? + ? ? . 
3.4 Redundancy Removal  
A good summary contains as much diverse infor-
mation as possible for a gene, while with as little 
redundancy as possible. For many well-studied 
genes, there are thousands of relevant papers and 
much information is redundant. Hence it is neces-
sary to remove redundant sentences before produc-
ing a final summary.  
We adopt the diversity penalty method (Zhang 
et al, 2005; Wan and Xiao, 2007) for redundancy 
removal. The idea is to penalize the candidate sen-
tences according to their similarity to the ones al-
ready selected. The process is as follows:  
(1) Initialize two sets, A ?= ,
{ | 1, 2,..., }iB s i K= =  containing all the extracted 
sentences;  
(2)  Sort the sentences in B by their scores in des-
cending order;  
(3) Suppose is is the top ranked sentence in B , 
move it from B to A . Then we penalize the re-
maining sentences in B as follows: 
For each sentence js  in B , j i?  
( ) ( ) ( , ) ( )j j j i iScore s Score s sim s s Score s?= ? ? ?  
where 0? > is the penalty degree factor, 
( , )j isim s s  is the similarity between is and js .  
(4) Repeat steps 2 and 3 until enough sentences 
have been selected. 
4 Results and Discussion 
4.1 Evaluation Metrics 
Unlike the newswire summarization, there are no 
gold-standard test collections available for evaluat-
ing gene summarization systems. The two previous 
studies mentioned in Section 2 both conducted ex-
trinsic evaluations by asking human experts to rate 
system outputs. Although it is important to collect 
direct feedback from the users, involving human 
experts makes it difficult to compare different 
summarization systems and to conduct large-scale 
evaluations (both studies evaluated nothing but a 
small number of genes). In contrast, we evaluated 
our system intrinsically on a much larger dataset 
consisting of 7,294 human genes, each with a pre-
existing handwritten summary downloaded from 
the NCBI?s FTP site5.  
The handwritten summaries were used as refer-
ence summaries (i.e. a gold standard) to compare 
with the automatically generated summaries. Al-
though the length of reference summaries varies, 
the majority of these summaries contain 80 to 120 
words. To produce a summary of similar length, 
we decided to select five sentences consisting of 
about 100 words. 
For the intrinsic evaluation of a large number of 
summaries, we made use of the ROUGE metrics 
that has been widely used in automatic evaluation 
of summarization systems (Lin and Hovy, 2003; 
Hickl et al, 2007). It provides a set of evaluation 
metrics to measure the quality of a summary by 
counting overlapping units such as n-grams or 
word sequences between the generated summary 
and its reference summary.  
                                                          
5 ftp://ftp.ncbi.nih.gov/gene/DATA/ASN_BINARY/ 
102
 We computed three ROUGE measures for each 
summary, namely ROUGE-1 (unigram based), 
ROUGE-2 (bigram based) and ROUGE-SU4 
(skip-bigram and unigram) (Lin and Hovy, 2003). 
Among them, ROUGE-1 has been shown to agree 
most with human judgments (Lin and Hovy, 2003). 
However, as biomedical concepts usually contain 
more than one word (e.g. transcription factor), 
ROUGE-2 and ROUGE-SU4 scores are also im-
portant for assessing gene summaries.  
4.2 Determining parameters for best perfor-
mance 
The two important parameters in our system ? the 
linear coefficient ? for the combination of Page-
Rank and GO scores and the diversity penalty de-
gree factor ? in redundancy removal ? are 
investigated in detail on a collection of 100 ran-
domly selected genes. First, by setting ? to values 
from 0 to 1 with an increment of 0.1 while holding 
?  steady at 0.7, we observed the highest ROUGE-
1score when ? was 0.8 (Figure 2). This suggests 
that the two scores (i.e. PageRank and GO score) 
complement to each other and that the PageRank 
score plays a more dominating role in the summed 
score. Next, we varied? gradually from 0 to 5 with 
an increment of 0.25 while holding ? steady at 
0.75.The highest ROUGE-1 score was achieved 
when? was 1.3 (Figure 3). For ROURE-2, the best 
performance was obtained when ? was 0.7 and ?
was 0.5. In order to balance ROUGE-1 and 
ROUGE-2 scores, we set ? to 0.75 and ? to 0.7 
for the remaining experiments.  
 
Figure 2. The blue line represents the changes in 
ROUGE-1 scores with different values of ? while ? is 
held at 0.7. 
 
Figure 3. The blue line represents the changes in 
ROUGE-1 scores with different values of ? while ? is 
held at 0.75. 
4.3 Comparison with other methods 
Because there are no publicly available gene sum-
marization systems, we compared our system with 
two baseline methods. The first is a well known 
publicly available summarizer - MEAD (Radev et 
al., 2004). We adopted the latest version of MEAD 
3.11 and used the default setting in MEAD that 
extracts sentences according to three features: cen-
troid, position and length. The second baseline ex-
tracts different sentences randomly from abstracts. 
Comparison results are shown in the following ta-
ble:  
 
System ROUGE-1 ROUGE-2 ROUGE-SU4
Our System 0.4725 0.1247 0.1828 
MEAD 0.3890 0.0961 0.1449 
Random 0.3434 0.0577 0.1091 
Table 5. Systems comparison on 7,294 genes. 
 
As shown in Table 5, our system significantly 
outperformed the two baseline systems in all three 
ROUGE measures. Furthermore, larger perfor-
mance gains are observed in ROUGE-2 and 
ROUGE-SU4 than in ROUGE-1. This is because 
many background words (e.g. gene, protein and 
enzyme) also appeared frequently as unigrams in 
randomly selected summaries. 
 
103
  
Figure 4. ROUGE-1 score distribution 
 
In Figure 4, we show that the majority of the 
summaries have a ROUGE-1 score greater than 0.4. 
Our further analysis revealed that almost half 
summaries with a low score (smaller than 0.3) ei-
ther lacked sufficient relevant abstracts, or the ref-
erence summary was too short or too long. In 
either case, only few overlapping words can be 
found when comparing the generated gene sum-
mary with the reference. The statistics for low 
ROUGE-1 score are listed in Table 6. We also note 
that almost half of the summaries that have low 
ROUGE-1 scores were due to other causes: mostly, 
machine generated summaries differ from human 
summaries in that they describe different function-
al aspects of the same gene product. Take the gene 
TOP2A (ID: 7153) for example. While both sum-
maries (handwritten and machine generated) focus 
on its encoded protein DNA topoisomerase, the 
handwritten summary describes the chromosome 
location of the gene whereas our algorithm selects 
statements about its gene expression when treated 
with a chemotherapy agent. We plan to investigate 
such differences further in our future work. 
 
Causes for Low Score Number of 
genes 
Few (?10) related abstracts 106 
Short reference summary (< 40 words) 27 
Long reference summary (> 150 words) 76 
Other 198 
Total 407 
Table 6. Statistics for low ROUGE-1 scores (<0.3) 
4.4 Results on various summary length 
Figure 5 shows the variations of ROUGE scores as 
the summary length increases. At all lengths and 
for both ROUGE-1 and ROUGE-2 measures, our 
proposed method performed better than the two 
baseline methods. By investigating the scores of 
different summary lengths, it can be seen that the 
advantage of our method is greater when the sum-
mary is short. This is of great importance for a 
summarization system as ordinary users typically 
prefer short content for summaries.  
 
 
Figure 5. Score variation for different summary length 
 
5 Conclusions and Future Work 
In this paper we have presented a system for gene-
rating gene summaries by automatically finding 
extract-worthy sentences from the biomedical lite-
rature. By using the state-of-the-art summarization 
techniques and incorporating gene specific annota-
tions, our system is able to generate gene summa-
ries more accurately than the baseline methods. 
Note that we only evaluated our system for human 
genes in this work. More summaries are available 
for human genes than other organisms, but our me-
thod is organism-independent and can be applied 
to any other species. 
This research has implications for real-world 
applications such as assisting manual database cu-
ration or updating existing gene records. The 
ROUGE scores in our evaluation show comparable 
performance to those in the newswire summariza-
tion (Hickl et al, 2007). Nonetheless, there are 
further steps necessary before making our system 
output readily usable by human curators. For in-
stance, human curators are generally in favor of 
sentences presented in a coherent order. Thus, in-
formation-ordering algorithms in multi-document 
summarization need to be investigated. We also 
plan to study the guidelines and scope of the cura-
tion process, which may provide additional impor-
tant heuristics to further refine our system output.  
Acknowledgments 
104
 The work is supported by NSFC project No. 
60803075, Chinese 973 Project No. 
2007CB311003. ZL is supported by the Intramural 
Program of the National Institutes of Health. The 
authors are grateful to W. John Wilbur and G. 
Craig Murray for their help on the early version of 
this manuscript.  
References 
W. A. Baumgartner, B. K. Cohen, L. M. Fox, G. Ac-
quaah-Mensah, L. Hunter. 2007. Manual Curation Is 
Not Sufficient for Annotation of Genomic Databases. 
Bioinformatics, Vol. 23, No. 13. (July 2007), pp. i41-
48. 
G. Bhalotia, P. I. Nakov, A. S. Schwartz and M. A. 
Hearst, BioText Team Report for the TREC 2003 
Genomics Track. In Proceedings of TREC 2003.  
B. Boguraev and C. Kennedy. 1997. Salience-based 
Content Characterization of Text Documents. In Pro-
ceedings of Workshop on Intelligent Scalable Text 
Summarization (ACL97/EACL97), pp. 2-9. 
J. Carbonell and J. Goldstein. 1998. The Use of MMR, 
Diversity-based Reranking for Reordering Docu-
ments and Producing Summaries. In ACM SIGIR, 
pages 335?336, August. 
H. P. Edmundson. 1969. New Methods in Automatic 
Extracting. Journal of the ACM (JACM) archive Vo-
lume 16,  Issue 2  (April 1969) Pages: 264 ? 285. 
G. Erkan and D. R. Radev. 2004. LexPageRank: Pres-
tige in Multi-Document Text Summarization. In Pro-
ceedings of 2004 Conference on Empirical Methods 
in Natural Language Processing (EMNLP 2004), 
Barcelona, Spain. 
S. Gupta, A.Nenkova and D.Jurafsky. 2007. Measuring 
Importance and Query Relevance in Topic-focused 
Multi-document Summarization. Proceedings of 
ACL 2007 short papers, Prague, Czech Republic. 
W. Hersh and R. T. Bhupatiraju. 2003. TREC Genomics 
track Overview. In Proceedings of TheTwelfth Text 
REtrieval Conference, 2003. 
A. Hickl, K. Roberts and F. Lacatusu. 2007. LCC's 
GISTexter at DUC 2007: Machine Reading for Up-
date Summarization. 
R. Jelier, M. Schuemie, C. Eijk, M. Weeber, E. Mulli-
gen, B. Schijvenaars, B. Mons, J. Kors. Searching for 
geneRIFs: Concept-based Query Expansion and 
Bayes Classification. In Proceedings of TREC 2003.  
C. Lin and E. Hovy. 2000. The Automated Acquisition 
of Topic Signatures for Text Summarization. In Pro-
ceedings of the COLING Conference. 
C. Lin and E. Hovy. 2003. Automatic Evaluation of 
Summaries Using N-gram Co-Occurrence Statistics. 
In HLT-NAACL, pages 71?78. 
X. Ling, J. Jiang, X. He, Q. Mei, C. Zhai and B. Schatz. 
2006. Automatically Generating Gene Summaries 
from Biomedical Literature. Proceedings of the Pa-
cific Symposium on Biocomputing 2006. 
X. Ling, J. Jiang, X. He, Q. Mei, C. Zhai and B. Schatz. 
2007. Generating Gene Summaries from Biomedical 
Literature: A Study of Semi-Structured Summariza-
tion. Information Processing and Management 43, 
2007, 1777-1791. 
Z. Lu, K. B. Cohen and L. Hunter. 2006. Finding Ge-
neRIFs via Gene Ontology Annotations. Pac Symp-
Biocomput. 2006:52-63. 
C. Manning and H. Sch?tze. 1999. Foundations of Sta-
tistical Natural Language Processing. Chapter 5, MIT 
Press. Cambridge, MA: May 1999. 
K. R. McKeown and D. R. Radev. 1995. Generating 
Summaries of Multiple News Articles. In Proceed-
ings, ACM Conference on Research and Develop-
ment in Information Retrieval SIGIR'95, pages 74?
82. 
R. Mihalcea and P. Tarau. TextRank: Bringing Order 
into Texts, in Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP 2004), Barcelona, Spain, July 2004. 
M. Newman. 2003. The Structure and Function of 
Complex Networks. SIAM Review 45.167?256 
(2003). 
L. Page, S. Brin, R. Motwani and T. Winograd. The 
PageRank Citation Ranking: Bringing Order to the 
Web. Technical report, Stanford University, Stanford, 
CA, 1998. 
D. R. Radev, H. Jing, M. Stys and D. Tam. 2004. Cen-
troid-based Summarization of Multiple Documents. 
Information Processing and Management, 40:919?
938. 
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector 
Space Model for Automatic Indexing. Communica-
tions of the ACM, vol. 18, nr.11, pages 613?620. 
S. Teufel and M. Moens. 1997. Sentence Extraction as a 
Classification Task. Workshop ?Intelligent and scala-
ble Text summarization?, ACL/EACL 1997. 
X. Wan and J. Xiao. 2007. Towards a Unified Approach 
Based on Affinity Graph to Various Multi-document 
Summarizations. ECDL 2007: 297-308.  
J. Yang, A. M. Cohen, W. Hersh. Automatic Summari-
zation of Mouse Gene Information by Clustering and 
Sentence Extraction from MEDLINE Abstracts. 
AMIA 2007 Annual Meeting. Nov. 2007 Chicago, IL. 
J. Yang, A. M. Cohen, W. Hersh. 2008. Evaluation of a 
Gene Information Summarization System by Users 
During the Analysis Process of Microarray Datasets. 
In BMC Bioinformatics 2009 10(Suppl 2):S5. 
B. Zhang, H. Li, Y. Liu, L. Ji, W. Xi, W. Fan, Z. Chen, 
W. Ma. 2005. Improving Web Search Results Using 
Affinity Graph. The 28th Annual International ACM 
SIGIR Conference (SIGIR'2005), August 2005.  
105
Coling 2010: Poster Volume, pages 525?533,
Beijing, August 2010
A Comparative Study on Ranking and Selection Strategies for 
Multi-Document Summarization 
Feng Jin, Minlie Huang, Xiaoyan Zhu 
State Key Laboratory of Intelligent Technology and Systems 
Tsinghua National Laboratory for Information Science and Technology 
Dept. of Computer Science and Technology, Tsinghua University 
jinfengfeng@gmail.com,{aihuang,zxy-dcs}@tsinghua.edu.cn
Abstract 
This paper presents a comparative study 
on two key problems existing in extrac-
tive summarization: the ranking problem 
and the selection problem. To this end, 
we presented a systematic study of 
comparing different learning-to-rank al-
gorithms and comparing different selec-
tion strategies. This is the first work of 
providing systematic analysis on these 
problems. Experimental results on two 
benchmark datasets demonstrate three 
findings: (1) pairwise and listwise learn-
ing-to-rank algorithms outperform the 
baselines significantly; (2) there is no 
significant difference among the learn-
ing-to-rank algorithms; and (3) the in-
teger linear programming selection 
strategy generally outperformed Maxi-
mum Marginal Relevance and Diversity 
Penalty strategies. 
1 Introduction 
As the rapid development of the Internet, docu-
ment summarization has become an important 
task since document collections are growing 
larger and larger. Document summarization, 
which aims at producing a condensed version of 
the original document(s), helps users to acquire 
information that is both important and relevant 
to their information need.  So far, researchers 
have mainly focused on extractive methods 
which choose a set of salient textual units to 
form a summary.  Such textual units are typical-
ly sentences, sub-sentences (Gillick and Favre, 
2009), or excerpts (Sauper and Barzilay, 2009).  
Almost all extractive summarization methods 
face two key problems: the first problem is how 
to rank textual units, and the second one is how 
to select a subset of those ranked units. The 
ranking problem requires systems model the 
relevance of a textual unit to a topic or a query. 
In this paper, the ranking problem refers to ei-
ther sentence ranking or concept ranking. Con-
cepts can be unigrams, bigrams, semantic con-
tent units, etc., although in our experiment, only 
bigrams are used as concepts. The selection 
problem requires systems improve diversity or 
remove redundancy so that more relevant in-
formation can be covered by the summary as its 
length is limited. As our paper focuses on ex-
tractive summarization, the selection problem 
refers to selecting sentences. However, the se-
lection framework presented here is universal 
for selecting arbitrary textual units, as discussed 
in Section 4. 
There have been a variety of studies to ap-
proach the ranking problem. These include both 
unsupervised sentence ranking (Luhn, 1958; 
Radev and Jing, 2004, Erkan and Radev, 2004), 
and supervised methods (Ouyang et al, 2007; 
Shen et al, 2007; Li et al, 2009). Even given a 
list of ranked sentences, it is not trivial to select 
a subset of sentences to form a good summary 
which includes diverse information within a 
length limit. Three common selection strategies 
have been studied to address this problem: Max-
imum Marginal Relevance (MMR) (Carbonell 
and Goldstein, 1998), Diversity Penalty (DiP) 
(Wan, 2007), and integer linear programming 
(ILP) (McDonald, 2007; Gillick and Favre, 
2009). As different methods were often eva-
luated on different datasets, it is of great value 
to systematically compare ranking and selection 
strategies on the same dataset. However, to the 
best of our knowledge, there is still no work to 
compare different ranking strategies or compare 
different selection strategies.  
In this paper, we presented a comparative 
study on the ranking problem and the selection 
525
problem for extractive summarization. We 
compared three genres of learning-to-rank me-
thods for ranking sentences or concepts: SVR, a 
pointwise ranking algorithm; RankNet, a pair-
wise learning-to-rank algorithm; and ListNet, a 
listwise learning-to-rank algorithm. We adopted 
an ILP framework that is able to select sen-
tences based on sentence ranking or concept 
ranking. We compared it with other selection 
strategies such as MMR and Diversity Penalty. 
We conducted our comparative experiments on 
the TAC 2008 and TAC 2009 datasets, respec-
tively. Our contributions are two-fold: First, to 
the best of our knowledge, this is the first work 
of presenting systematic and in-depth analysis 
on comparing ranking strategies and comparing 
selection strategies. Second, this is the first 
work using pairwise and liswise learning-to-
rank algorithms to perform concept (word bi-
gram) ranking for extractive summarization.  
The rest of this paper is organized as follows. 
We introduce the related work in Section 2. In 
Section 3, we present three ranking algorithms, 
SVR, RankNet, and ListNet. We describe the 
sentence selection problem with an ILP frame-
work described in Section 4. We introduce fea-
tures in Section 5. Evaluation and experiments 
are presented in Section 6. Finally, we conclude 
this paper in Section 7. 
2 Related Work 
A number of extractive summarization studies 
used unsupervised methods with surface fea-
tures, linguistic features, and statistical features 
to guide sentence ranking (Edmundson, 1969; 
McKeown and Radev, 1995; Radev et al, 2004; 
Nekova et al, 2006). Recently, graph-based 
ranking methods have been proposed for sen-
tence ranking and scoring, such as LexRank 
(Erkan and Radev, 2004) and TextRank (Mihal-
cea and Tarau, 2004).  
There are also a variety of studies on su-
pervised learning methods for sentence ranking 
and selection. Kupiec et al (1995) developed a 
naive Bayes classifier to decide whether a sen-
tence is worthy to extract. Recently, Conditional 
Random Field (CRF) and Structural SVM have 
been employed for single document summariza-
tion (Shen et al, 2007; Li et al, 2009).  
Besides ranking sentences directly, there are 
some approaches that select sentences based on 
concept ranking. Radev et al (2004) used cen-
troid words whose tf*idf scores are above a 
threshold. Filatova and Hatzivassiloglou (2004) 
used atomic event as concept. Moreover, sum-
marization evaluation metrics such as Basic 
Element (Hovy et al, 2006), ROUGE (Lin and 
Hovy, 2003) and Pyramid (Passonneau et al, 
2005) are all counting the concept overlap be-
tween generated summaries and human-written 
summaries.  
Another important issue existing in extractive 
summarization is to find an optimal sentence 
subset which can cover diverse information. 
Maximal Marginal Relevance (MMR) (Carbo-
nell and Goldstein, 1998) and Diversity Penalty 
(Wan, 2007) are most widely used approaches 
to reduce redundancy. The two methods are es-
sentially based on greedy search. By contrast, 
ILP based approaches view summary generation 
as a global optimization problem. McDonald 
(2007) proposed a sentence-level ILP solution. 
Sauper and Barzilay (2009) presented an ex-
cerpt-level ILP method to generate Wikipedia 
articles. Gillick and Favre (2009) proposed a 
concept-level ILP, but they used document fre-
quency to score concepts (bigrams), without any 
learning process. Some recent studies (Gillick 
and Favre, 2009; Martins and Smith, 2009) also 
modeled sentence selection and compression 
jointly using ILP. Our ILP framework proposed 
here is based on these studies. Although various 
selection strategies have been proposed, there is 
no work to systematically compare these strate-
gies yet. 
Learning to rank attracts much attention in 
the information retrieval community recently. 
Pointwise, pairwise and listwise learning-to-
rank approaches have been extensively studied 
(Liu, 2009). Some of those have been applied to 
document summarization, such as SVR 
(Ouyang et al, 2007), classification SVM 
(Wang et al, 2007), and RankNet (Svore et al, 
2007). Again, there is no work to systematically 
compare these ranking algorithms. To the best 
of our knowledge, this is the first time that a 
listwise learning-to-rank algorithm, ListNet 
(Cao et al, 2007), is adapted to document sum-
marization in this paper. Moreover, pairwise 
and listwise learning-to-rank algorithms have 
never been used to perform concept ranking for 
extractive summarization.  
526
3 Ranking Sentences or Concepts 
Given a query and a collection of relevant doc-
uments, an extractive summarization system is 
required to generate a summary consisting of a 
set of text units (usually sentences). The first 
problem we need to consider is to determine the 
importance of these sentences according to the 
input query. We approach this ranking problem 
in two ways: the first way is to score sentences 
directly using learning-to-rank algorithms, and 
thus the goal of summarization is to select a 
subset of sentences, considering both relevance 
and redundancy. The second way is to score 
concepts within the document collection, and 
then the summarization task is to select a sen-
tence subset that can cover those important con-
cepts maximally. The problem of sentence se-
lection will be described in Section 4.  
Suppose the relevant document collection for 
a query q is Dq. From this collection, we obtain 
a set of sentences or concepts (e.g., word bi-
grams), S={s1,s2,?,sn} or C={c1,c2,?, cn}. Be-
fore training, each si or ci is associated with a 
gold standard score, yi. A feature vector, xj= 
?(sj/cj,q,Dq), is constructed for each sentence or 
concept. The learning algorithm will learn a 
ranking function f(xj) from a collection of 
query-document pairs {(qi,Dqi)|i= 1, 2,?,m}.  
We investigated three learning-to-rank me-
thods to learn f(xj). The first one is a pointwise 
ranking algorithm, support vector regression 
(SVR). This algorithm treats sentences (or con-
cepts) independently. The second method is a 
pairwise ranking algorithm, RankNet, which 
learns a ranking function from a list of sentence 
(or concept) pairs. Each pair is labeled as 1 if 
the first sentence si (or concept ci) ranks ahead 
of the second sj (or cj), and 0 otherwise. 
The listwise ranking algorithm, ListNet, 
learns the ranking function f(xj) in a different 
way. A list of sentences (or concepts) is treated 
as a whole. Both RankNet and ListNet take into 
account the dependency between sentences (or 
concepts). 
3.1  Support Vector Regression  
Support Vector Regression (SVR), a generaliza-
tion of the classical SVM formulation, attempts 
to learn a regression model. SVR has been ap-
plied to summarization in (Ouyang et al, 2007; 
Metzler and Kanungo, 2008). In our work, we 
train the SVR model to fit the gold standard 
score of each sentence or concept.  
Formally, the objective of SVR is to minim-
ize the following objective: 
2
, ,
1 1
|| || ( ( ))
2
i
i i
x
w b
w C v L y f x
N
?
?
?( ) =
? ?? ?? ?
+ ? + ?? ?? ?? ?? ?? ??
(1) 
where L(x)=|x|-? if x > ? and otherwise L(x)=0; 
yi is the gold standard score of xi; f(x) =wTx+b, 
the predicted score of x; C and v are two para-
meters; and N is the total number of training 
examples.  
3.2 RankNet  
RankNet is a pairwise learning-to-rank method 
(Burges et al, 2005). In this algorithm, training 
examples are handled pair by pair. Given a pair 
of feature vectors (xi, xj), the gold standard 
probability ijP is set to be 1 if the label of the 
pair is 1, which means xi ranks ahead of xj. The 
gold standard probability is 0 if the label of the 
pair is 0. Then the predicted probability Pij, 
which defines the probability of xi ranking 
ahead of xj by the model, is represented as a lo-
gistic function:  
exp( ( ) ( ))
1 exp( ( ) ( ))
i j
ij
i j
f x f x
P
f x f x
?
=
+ ?
            (2) 
where f(x) is the ranking function. The objective 
of the algorithm is to minimize the cross entro-
py between the gold standard probability and 
the predicted probability, which is defined as 
follows: 
( ) log (1 ) log(1 )ij ij ij ij ijC f P P P P= ? ? ? ?     (3) 
A three-layer neural network is used as the 
ranking function, as follows:  
3 32 2 21 2 3( ) ( ( ) )n ij jk nk j i
j k
f x g w g w x b b= + +? ?
 
 (4) 
where for weights w and bias b, the superscripts 
indicate the node layer while the subscripts in-
dicate the node indexes within each layer. And 
xnk is the k-th component of input feature vector 
xn. Then a gradient descent method is used to 
learn the parameters. For details, refer to 
(Burges et al, 2005). 
3.3 ListNet 
ListNet takes a list of items as input in the learn-
ing process. More specifically, suppose we have 
527
a list of feature vectors (x1, x2,?, xn) and each 
feature vector xi has an gold standard score yi, 
which has been assigned before training. Ac-
cordingly, we have a list of gold standard scores 
(y1, y2,?,yn). We also have a list of scores as-
signed by the algorithm during training, say, 
(f(x1), f(x2),?, f(xn)). Given a score list 
S={s1,s2,?,sn}, the probability that xj will rank 
the first place among the n items is defined as 
follows: 
1 1
( ) exp( )
( )
( ) exp( )
j j
s n n
k kk k
s s
P j
s s
= =
?
= =
?? ?
        (5) 
It is easy to prove that (Ps(1), Ps(2), ?, Ps(n)) is 
a probability distribution, as the sum of them 
equals to 1. Therefore, the cross entropy can be 
used to define the loss between the gold stan-
dard distribution Py(j) and the distribution Pf(j), 
as follows:  
1
( , ) ( ) log ( )
n
y f
j
L y f P j P j
=
= ??               (6) 
where y represents the gold standard score list  
(y1, y2,?,yn) and f=(f(x1), f(x2),?, f(xn)) is the 
score list output by the ranking algorithm.  
The function f is defined as a linear function, 
as follows: 
( ) Tw i if x w x=                          (7) 
Then the gradient of loss function L(y,f) with 
respect to the parameter vector w can be calcu-
lated as follows:  
1
1
1
( )( , )
( )
( )1
exp( ( ))
exp( ( ))
n
w jw
y j
j
n
w j
w jn
jw jj
f xL y f
w P x
w w
f x
f x
wf x
=
=
=
??? = = ?
? ?
?
+
?
?
??
 (8) 
 
During training, w is updated in a gradient des-
cent manner: w=w -??w and ? is the learning 
rate. For details, refer to (Cao et al, 2007). 
4 ILP-based Selection Framework 
After we have a way of ranking sentences or 
concepts, we face a sentence selection problem: 
selecting an optimal subset of sentences as the 
final summary. To integrate sentence/concept 
ranking, we adopted an integer linear program-
ming (ILP) framework to find the optimal sen-
tence subset (Filatova and Hatzivassiloglou, 
2004; McDonald, 2007; Gillick and Favre, 2009; 
Takamura and Okumura, 2009). ILP is a global 
optimization problem whose objective and con-
straints are linear in a set of integer variables.  
Formally, we define the problem of sentence 
selection as follows: 
maximize:  ( )*  xi i
i
f x z
? ?? ?? ??          (9) 
. .     * | |       uj j
j
z u Lims t ??  
       * ( , ) ,          u xj i
j
z I i j z i? ??  
      ( )* ( , )   ,    x xi j i jz z sim x x i j?+ < ?  
        , {0,1},      ,x ui jz z i j? ?  
where: 
xi ? the representation unit, such as a sentence 
or a concept. We term it representation unit be-
cause the summary quality is represented by the 
set of included xi; 
f(xi) - the ranking function given by the learn-
ing-to-rank algorithms; 
uj - the selection unit, for instance, a sentence in 
this paper. |uj| is the number of words in uj; 
x
iz - the indicator variable which denotes the 
presence or absence of xi in the summary; 
u
jz - the indicator variable which denotes inclu-
sion or exclusion of uj; 
I(i, j) - a  binary constant indicating that wheth-
er xi appears in uj. It is either 1 or 0; 
Lim - the length limit; 
sim(xi, xj) - a similarity measure for considering 
the redundancy; 
? - the redundancy threshold.  
The first constraint indicates the length limit. 
The second constraint asserts that if a represen-
tation unit xi is included in a summary, at least 
one selection unit that contains xi must be se-
lected. The third constraint considers redundan-
cy. If the representation unit is sentence, the 
similarity measure is defined as tf*idf similarity, 
and ?/2 is the similarity threshold, which was 
set to be 1 here. For concepts, the similarity 
measure can be defined as  
1,    
( , )
0,    otherwise
i j
i j
x x
sim x x
=?
= ?? .
 
However, other definition is also feasible, de-
pending on what has been selected as represen-
tation unit. 
528
Note that this framework is very general. If 
the representation unit xi is a sentence, the rank-
ing function is defined on sentence. Thus the 
ILP framework will find a set of sentences that 
can optimize the total scores of selected sen-
tences, subject to several constraints. If the re-
presentation unit is a concept, the ranking func-
tion measures the importance of a concept to be 
included in a summary. Thus the goal of ILP is 
to find a set of sentences by maximizing the 
scores of concepts covered by those selected 
sentences. 
 
Dq relevant document collection in response 
to query q 
d one single document 
wi unigram 
wiwi+1 bigram 
S sentence 
tfd(wi) the frequency of wi occurring in d 
dfD(wi) the number of documents containing wi 
in collection D 
Table 1. Notations for features. 
5 Features 
To facilitate the following description, some 
notations are defined in Table 1. In our dataset, 
each query has a title and narrative to precisely 
define an information need. The following is a 
query example from the TAC 2008 test dataset:  
<topic id = "D0801A">  
 <title> Airbus A380 </title> 
 <narrative> 
Describe developments in the production and 
launch of the Airbus A380. 
 </narrative> 
</topic> 
Features for sentence ranking and concept 
ranking are listed in the following. We use word 
bigrams as concept here. 
Sentence Features 
(1) Cluster frequency: ( )
qi
D iw S
tf w
??  
(2) Title frequency: ( )
i
d iw S
tf w
??  where d is a 
new document that consists of all the titles of 
documents in Dq.  
(3) Query frequency: ( )
i
d iw S
tf w
??  where d is 
a document consisting of the title and narrative 
fields of the current topic.  
(4) Theme frequency: ( )
qi i
D iw S w T
tf w
? ? ??  
where T is the top 10% frequent unigram words 
in Dq. 
(5) Document frequency of bigrams in the sen-
tence: 
1
1( )
i i
D i iw w S
df w w
+
+?? .  
(6) PageRank score: as described in (Mihalcea 
and Tarau, 2004), each sentence in Dq is a node 
in the graph and the cosine similarity between a 
pair of sentences is used as edge weight. 
Concept Features 
(1) Cluster frequency: 1( )qD i itf w w + , the fre-
quency of 1i iw w + occurring in Dq.  
(2) Title frequency: 1( )d i itf w w + , where d is a 
document consisting of all the titles of docu-
ments in Dq. 
(3) Query Frequency: the frequency of the bi-
gram occurring in the topic title and narrative. 
(4) Average term frequency: 
 1( )/ | |
q
d i i qd D
tf w w D+?? . |Dq| is the number of 
documents in the set. 
(5) Document frequency: the document fre-
quency of this bigram. 
(6) Minimal position: the minimal position of 
this bigram relative to the document length.  
(7) Average position: the average position of 
this bigram in collection Dq . 
6 Experimental Results 
6.1 Data Preprocessing 
We conducted experiments on the TAC 2008 
and TAC 2009 datasets. The task requires pro-
ducing a 100-word summary for each query (al-
so termed topic sometimes). There are 48 que-
ries in TAC 2008 and 44 queries in TAC 2009. 
A query example has been given in Section 5. 
Relevant documents for these queries have been 
specified. And four human-written summaries 
were supplied as reference summaries for each 
query. 
We segmented the relevant documents into 
sentences using the LingPipe toolkit 1  and 
stemmed words using the Porter Stemmer. 
Word bigrams are used as concepts in this paper. 
If the two words in a bigram are both stop-
words, the bigram will be discarded. The sen-
                                                 
1 http://alias-i.com/lingpipe/index.html 
529
tence features and bigram features are then cal-
culated. As our focus is on comparing different 
ranking strategies and selection strategies, we 
did not apply any sophisticated linguistic or se-
mantic processing techniques (as pre- or post-
processing). Thus we did not compare our re-
sults to those submitted to the TAC conferences.  
We train the learning algorithms on one data-
set and then evaluate the algorithms on the other. 
The generated summaries are evaluated using 
the ROUGE toolkit (Lin and Hovy, 2003).  
6.2 Preparing Training Samples 
As our work includes both sentence ranking and 
concept ranking, we need to establish two types 
of training data. Fortunately, we are able to do 
this based on the reference summaries and an-
notation results provided by the TAC confe-
rences.  
For the sentence ranking problem, we com-
pute the average ROUGE-1 score for each sen-
tence by comparing it to the four reference 
summaries for each query. This score is treated 
as the gold-standard score. In ListNet, these 
scores are directly used (see formula (5)). While 
in RankNet, the sentences for a query are 
grouped into 10 bins according to their 
ROUGE-1 scores, and then we extract sentences 
from different bins respectively to form a pair. 
We assume that a sentence in a higher scored 
bin should rank ahead of those sentences in 
lower scored bins.  
As for the concept ranking problem, gold-
standard scores are obtained from the human 
annotated Pyramid data. The weight of each 
semantic content unit (SCU) is the number of 
reference summaries in which the SCU appears. 
So straightforwardly, the gold-standard score of 
a bigram is the largest weight of all SCUs that 
contain the bigram. And if a bigram does not 
occur in any SCU, its score will be 0. Thus the 
bigram scores belong to the set {0,1,2,3,4} as 
there are four human-written summaries for 
each query. These scores are directly used in 
ListNet (see formula (5)). And in RankNet, bi-
gram pairs are constructed according to the 
gold-standard scores.  
6.3 Learning Parameters 
For SVR, the radial basis kernel function is em-
ployed and the optimal values for parameters C, 
v and g (for the kernel) are found using the gri-
dregression.py tool provided by LibSVM 
(Chang and Lin, 2001) with a 5-fold cross vali-
dation on the training set.  
RankNet applies a three-layer (one hidden 
layer) neural network with only one node in the 
output layer, as described in (Burges et al, 
2005). The number of hidden neurons was em-
pirically set to be 10. The learning rate was set 
to 0.001 for sentence ranking and 0.01 for bi-
gram ranking.  
As for ListNet, the learning rate for sentence 
ranking and concept ranking are both set to be 
0.1 empirically.  
6.4 Comparing Ranking Strategies 
In this section, we compared different ranking 
strategies for both sentence ranking and concept 
ranking. The sentence selection strategies were 
fixed to the ILP selection framework as shown 
in Section 4. We chose ILP as the selection 
strategy because we want to compare our sys-
tem with the following two methods (as base-
lines): 
(1) SENT_ILP: A sentence-level method pro-
posed by McDonald (2007) with ILP formula-
tion. We implemented the query-focused ver-
sion of the formulae as TAC 2008 and 2009 
required query-focused summarization. 
(2) DF_ILP: A concept-level ILP method using 
document frequency to score word bigrams 
(Gillick and Favre, 2009), without any learning 
process.  
The differences between our framework and 
SENT_ILP are: a) SENT_ILP used a redundan-
cy factor in the objective function whereas we 
modeled redundancy as constraints; b) 
SENT_ILP used tf*idf similarity to compute 
relevance scores whereas we used learning algo-
rithms.  
The ROUGE-1 and ROUGE-2 measures for 
each method are presented in Table 2 and Table 
3. Note that the performance on the TAC 2008 
dataset was obtained from the models that were 
trained on the TAC 2009 dataset. Then, the da-
tasets were interchanged for training and testing, 
respectively. Different learning-to-rank strate-
gies (SVR, RankNet, ListNet) do not show sig-
nificant differences between one and another, 
but they all outperform SENT_ILP substantially 
(p-value < 0.0001). And for concept ranking, 
RankNet and ListNet both achieve significantly 
better ROUGE-2 results (p-value < 0.005) than 
530
DF_ILP. This infers that considering more fea-
tures will have better results than using docu-
ment frequency to score concepts. The Wilcox-
on signed-rank test (Wilcoxon, 1945) is used for 
significance tests in our experiment. A good 
ranking strategy for modeling relevance is im-
portant for extractive summarization. RankNet 
which used a three-layer network (non-linear 
function) as the ranking function performs 
slightly better than ListNet which is based on a 
linear ranking function.  
 
Dataset Method ROUGE-1 ROUGE-2
TAC 
2008 
SVR 0.35086 0.08447 
RankNet 0.36025 0.09291 
ListNet 0.35365 0.09129 
SENT_ILP 0.31546 0.06500 
TAC 
2009 
SVR 0.36125 0.09659 
RankNet 0.36216 0.09778 
ListNet 0.35480 0.09126 
SENT_ILP 0.31962 0.07034 
Table 2. Results of sentence ranking strategies. 
 
Dataset Method ROUGE-1 ROUGE-2
TAC 2008 
SVR 0.36555 0.10291 
RankNet 0.37564 0.11213 
ListNet 0.36863 0.10660 
DF_ILP 0.36922 0.10373 
TAC 2009 
SVR 0.37126 0.10698 
RankNet 0.37513 0.11364 
ListNet 0.37499 0.11313 
DF_ILP 0.36347 0.10156 
Table 3. Results of concept ranking strategies. 
 
It is worth noting that Pyramid annotations 
may not cover all important bigrams, partly be-
cause SCUs in reference summaries have been 
rephrased by human annotators. Note that we 
simply extract original sentences to form a 
summary, thus it is possible that a bigram which 
is important in the original sentences does not 
appear in any rephrased SCUs at all. Such bi-
grams will have a gold-standard score of 0, 
which is erroneous supervision. For example, 
the bigrams hurricane katrina in topic D0804A 
about Katrina pet rescue and life support in 
D0806A about Terri Schiavo case are not anno-
tated in any SCUs, but these bigrams are both 
key terms for the topics.  
6.5 Comparing Selection Strategies 
In order to study the influence of different selec-
tion strategies, we compare the ILP selection 
strategy (as introduced in Section 4) with other 
popular selection strategies, based on the same 
sentence ranking algorithm (we chose sentence-
level RankNet). The baselines to be compared 
are as follows:  
(1) MMR: As shown in (Carbonell and 
Goldstein, 1998), the formula of MMR is: 
{ }1 2arg max ( , ) (1 ) max ( , )
i j
i i js R S s S
MMR D q s D s s? ?
? ? ?
= ? ?
 
where q is the given query; R is the set of all 
sentences; S is the set of already included sen-
tences; D1 is the normalized ranking score f(xi) 
of si, and D2 is the cosine similarity of the fea-
ture vectors for si  and sj. Our implementation 
was similar to the MMR strategy in the 
MEAD2summarizer. 
(2) DiP: Diversity penalty which penalizes the 
score of candidate sentences according to the 
already selected ones (Wan, 2007). 
 
Dataset Method ROUGE-1 ROUGE-2
TAC 2008
ILP 0.36025 0.09291 
MMR 0.35459 0.09086 
DiP 0.35263 0.08689 
TAC 2009
ILP 0.36216 0.09778 
MMR 0.35148 0.08881 
DiP 0.34714 0.08672 
Table 4. Comparing selection strategies. 
 
The corresponding ROUGE scores are pre-
sented in Table 4. ILP outperforms other selec-
tion strategies significantly on the TAC 2009 
dataset (both ILP vs. MMR and ILP vs. DiP). 
Although improvements are observed with ILP 
on the TAC 2008 dataset, the difference is not 
significant (using ILP vs. using MMR). MMR is 
comparable to DiP as they are both based on 
greedy search in nature.  
To investigate the difference between these 
strategies, we present in-depth analysis here. 
First, the average length of summaries generat-
ed by ILP is 97.1, while that by MMR and DiP 
are 95.5 and 92.7, respectively. Note that the 
required summary length is 100 and that more 
words can potentially cover more information. 
Thus, ILP can generate summaries with more 
information. This is because ILP is a global op-
timization algorithm, subject to the length con-
straint. Second, the average rank of sentences 
selected by ILP is 12.6, while that by MMR and 
                                                 
2 http://www.summarization.com/mead/ 
531
DiP is about 5, which is substantially different. 
ILP can search down the ranked list while the 
other two methods tend to only select the very 
top sentences. Third, there are 4.1 sentences on 
average in each ILP-generated summary, while 
the number for MMR and DiP generated sum-
maries are 2.7 and 2.5, respectively. Thus ILP 
tend to select shorter sentences than MMR and 
DiP. This may help reduce redundancy as long-
er sentences may contain more topic irrelevant 
clauses or phrases.  
6.6 Discussions 
Interestingly, although the learning-to-rank al-
gorithms combined with the ILP selection strat-
egy perform well in summarization, the perfor-
mance is still far from that of manual summari-
zation. In this study, we investigate the upper 
bound performance. We used the presented ILP 
framework to generate summaries based on the 
gold-standard scores, rather than the scores giv-
en by the learning algorithms. In other words, 
f(xi) in formula (9) is replaced by the gold-
standard scores. The ROUGE results are shown 
in Table 5. We also listed the best/worst/average 
ROUGE scores of human summaries in TAC by 
comparing one human summary (as generated 
summary) to the other three human summaries 
(as reference summaries). These results are sub-
stantially better than those by the learning algo-
rithms. Sentence- and concept- level ranking 
produces very close results to best human sum-
maries. Some ROUGE-2 scores are even higher 
than those of human summaries. This is reason-
able as human annotators may have difficulty in 
organizing content when there are many docu-
ments and sentences. The results reflect that 
there is a remarkable gap between the gold-
standard scores and the learned scores.  
 
Dataset Method ROUGE-1 ROUGE-2
TAC 
2008 
Sentence-level 0.44216 0.14842 
Concept-level 0.42222 0.16018 
Human Best 0.44220 0.13079 
Human Average 0.41417 0.11606 
Human Worst 0.38005 0.10736 
TAC 
2009 
Sentence-level 0.45500 0.15565 
Concept-level 0.43526 0.17118 
Human Best 0.45663 0.14864 
Human Average 0.44443 0.12680 
Human Worst 0.39652 0.11109 
Table 5. Upper bound performance. 
7 Conclusion and Future Work 
We presented systematic and extensive analysis 
on studying two key problems in extractive 
summarization: the ranking problem and the 
selection problem. We compared three genres of 
learning-to-rank algorithms for the ranking 
problem, and investigated ILP, MMR, and Di-
versity Penalty strategies for the selection prob-
lem. To the best of our knowledge, this is the 
first work of presenting systematic comparison 
and analysis on studying these problems. We 
also at the first time proposed to use learning-to-
rank algorithms to perform concept ranking for 
extractive summarization.  
Our future work will focus on: (1) exploiting 
more features that can reflect summary quality; 
(2) optimizing summarization evaluation me-
trics directly with new learning algorithms. 
Acknowledgments 
This work was partly supported by the Chinese 
Natural Science Foundation under grant No. 
60973104 and No. 60803075, and with the aid 
of a grant from the International Development 
Research Center, Ottawa, Canada IRCI project 
from the International Development.  
References 
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, 
Matt Deeds, Nicole Hamilton and Greg Hullender. 
2005. Learning to Rank Using Gradient Descent. 
In Proceedings of the 22nd International Confe-
rence on Machine Learning.  
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai and 
Hang Li. 2007. Learning to Rank: from Pairwise 
Approach to Listwise Approach. In Proceedings 
of ICML 2007.  
Jaime Carbonell and Jade Goldstein. 1998. The Use 
of MMR, Diversity-Based Reranking for Reorder-
ing Documents and Producing Summaries. In 
Proceedings of SIGIR, August 1998, pp. 335 - 336.  
Chih-Chung Chang and Chih-Jen Lin. 2001. 
LIBSVM: a Library for Support Vector Machines. 
Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
H. P. Edmundson. 1969. New Methods in Automatic 
Extracting. Journal of the ACM (JACM) Archive,  
Volume 16, Issue 2 (April 1969) Pages: 264 - 285.  
G. Erkan and Dragomir R. Radev. 2004. LexPage-
Rank: Prestige in Multi-Document Text Summa-
532
rization. In Proceedings of EMNLP 2004, Barce-
lona, Spain.  
Elena Filatova and Vasileios Hatzivassiloglou. 2004. 
Event-based Extractive Summarization. In Pro-
ceedings of ACL Workshop on Summarization, 
volume 111. 
Dan Gillick and Benoit Favre. 2009. A Scalable 
Global Model for Summarization. In Proceedings 
of the Workshop on Integer Linear Programming 
for Natural Language Processing.  
Eduard Hovy, Chin-yew Lin, Liang Zhou and Juni-
chi Fukumoto. 2006. Automated Summarization 
Evaluation with Basic Elements. In Proceedings 
of the Fifth Conference on Language Resources 
and Evaluation.  
Julian Kupiec, Jan Pedersen and Francine Chen. 
1995. A Trainable Document Summarizer. In 
Proceedings of SIGIR'95, pages 68 - 73, New 
York, USA.  
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha 
and Yong Yu. 2009. Enhancing Diversity, Cover-
age and Balance for Summarization through 
Structure Learning. In Proceedings of the 18th In-
ternational Conference on World Wide Web.  
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
Occurrence Statistics. In Proceedings of HLT-
NAACL, pages 71-78. 
Tie-Yan Liu. 2009. Learning to Rank for Informa-
tion Retrieval, Foundation and Trends on Infor-
mation Retrieval.  Now Publishers.  
H.P. Luhn. 1958. The Automatic Creation of Litera-
ture Abstracts. In IBM Journal of Research and 
Development, Vol. 2, No. 2, pp. 159-165, April 
1958. 
Andr? F. T. Martins and Noah A. Smith. 2009. 
Summarization with a Joint Model for Sentence 
Extraction and Compression. In Proceedings of 
the Workshop on Integer Linear Programming for 
Natural Langauge Processing\. 
Ryan McDonald. 2007. A Study of Global Inference 
Algorithms in Multi-Document Summarization. In 
Proceedings of the 29th ECIR.  
Kathleen McKeown and Dragomir R. Radev. 1995. 
Generating Summaries of Multiple News Articles. 
In Proceedings of SIGIR'95, pages 74?82.  
Donald Metzler and Tapas Kanungo. 2008. Machine 
Learned Sentence Selection Strategies for Query-
Biased Summarization. SIGIR Learning to Rank 
Workshop.  
Rada Mihalcea and Paul Tarau. 2004. TextRank: 
Bringing Order into Texts. In Proceedings of 
EMNLP 2004, Barcelona, Spain, July 2004.  
Ani Nenkova, Lucy Vanderwende and Kathleen 
McKeown. 2006. A Compositional Context Sensi-
tive Multi-document Summarizer: Exploring the 
Factors that Influence Summarization. In Pro-
ceedings of SIGIR 2006.  
You Ouyang, Sujian Li, Wenjie Li. 2007. Develop-
ing Learning Strategies for Topic-based Summa-
rization. In Proceedings of the sixteenth ACM 
Conference on Information and Knowledge Man-
agement, 2007. 
Rebecca J. Passonneau, Ani Nenkova, Kathleen 
McKeown and Sergey Sigelman. 2005. Applying 
the Pyramid Method in DUC 2005. DUC 2005 
Workshop.  
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, 
and Daniel Tam. 2004. Centroid-based Summari-
zation of Multiple Documents. Information 
Processing and Management, 40:919?938.  
Christina Sauper and Regina Barzilay. 2009. Auto-
matically Generating Wikipedia Articles: A Struc-
ture-Aware Approach. In Proceedings of ACL 
2009.  
Dou Shen, Jian-Tao Sun, Hua Li, QiangYang and 
Zheng Chen. 2007. Document Summarization Us-
ing Conditional Random Fields. In IJCAI, pages 
2862 - 2867, 2007.  
Krysta Svore, Lucy Vanderwende, and Chris Burges. 
2007. Enhancing Single-Document Summariza-
tion by Combining RankNet and Third-Party 
Sources. In Proceedings of EMNLP-CoNLL 
(2007), pp. 448-457.. 
Hiroya Takamura and Manabu Okumura. Text 
Summarization Model Based on Maximum Cov-
erage Problem and its Variant. In Proceedings 
EACL, 2009.  
Xiaojun Wan and Jianguo Xiao. 2007. Towards a 
Unified Approach Based on Affinity Graph to 
Various Multi-document Summarizations. ECDL 
2007, 297-308.  
Changhu Wang, Feng Jing, Lei Zhang and Hong-
Jiang Zhang. 2007. Learning Query-Biased Web 
Page Summarization. In Proceedings of the six-
teenth ACM Conference on Information and 
Knowledge Management.  
Frank Wilcoxon. 1945. Individual comparisons by 
ranking methods. Biometrics, 1, 80-83. 
533
