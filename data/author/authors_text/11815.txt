Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 459?467,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Projecting Parameters for Multilingual Word Sense Disambiguation 
 
Mitesh M. Khapra Sapan Shah Piyush Kedia Pushpak Bhattacharyya 
Department of Computer Science and Engineering 
Indian Institute of Technology, Bombay 
Powai, Mumbai ? 400076, 
Maharashtra, India. 
{miteshk,sapan,charasi,pb}@cse.iitb.ac.in 
 
Abstract 
We report in this paper a way of doing Word 
Sense Disambiguation (WSD) that has its ori-
gin in multilingual MT and that is cognizant 
of the fact that parallel corpora, wordnets and 
sense annotated corpora are scarce re-
sources. With respect to these resources, lan-
guages show different levels of readiness; 
however a more resource fortunate language 
can help a less resource fortunate language. 
Our WSD method can be applied to a lan-
guage even when no sense tagged corpora for 
that language is available. This is achieved by 
projecting wordnet and corpus parameters 
from another language to the language in 
question. The approach is centered around a 
novel synset based multilingual dictionary and 
the empirical observation that within a domain 
the distribution of senses remains more or less 
invariant across languages. The effectiveness 
of our approach is verified by doing parameter 
projection and then running two different 
WSD algorithms. The accuracy values of ap-
proximately 75% (F1-score) for three lan-
guages in two different domains establish the 
fact that within a domain it is possible to cir-
cumvent the problem of scarcity of resources 
by projecting parameters like sense distribu-
tions, corpus-co-occurrences, conceptual dis-
tance, etc. from one language to another. 
1 Introduction 
Currently efforts are on in India to build large scale 
Machine Translation and Cross Lingual Search 
systems in consortia mode. These efforts are large, 
in the sense that 10-11 institutes and 6-7 languages 
spanning the length and breadth of the country are 
involved.  The approach taken for translation is 
transfer based which needs to tackle the problem of 
word sense disambiguation (WSD) (Sergei et. al., 
2003).  Since 90s machine learning based ap-
proaches to WSD using sense marked corpora have 
gained ground (Eneko Agirre & Philip Edmonds, 
2007). However, the creation of sense marked cor-
pora has always remained a costly proposition. 
Statistical MT has obviated the need for elaborate 
resources for WSD, because WSD in SMT hap-
pens implicitly through parallel corpora (Brown et. 
al., 1993). But parallel corpora too are a very cost-
ly resource.  
The above situation brings out the challenges 
involved in Indian language MT and CLIR. Lack 
of resources coupled with the multiplicity of Indian 
languages severely affects the performance of sev-
eral NLP tasks. In the light of this, we focus on the 
problem of developing methodologies that reuse 
resources. The idea is to do the annotation work 
for one language and find ways of using them for 
another language. 
Our work on WSD takes place in a multilingual 
setting involving Hindi (national language of India; 
500 million speaker base), Marathi (20 million 
speaker base), Bengali (185 million speaker base) 
and Tamil (74 million speaker base). The wordnet 
of Hindi and sense marked corpora of Hindi are 
used for all these languages. Our methodology 
rests on a novel multilingual dictionary organiza-
tion and on the idea of ?parameter projection? from 
Hindi to the other languages. Also the domains of 
interest are tourism and health. 
The roadmap of the paper is as follows. Section 
2 describes related work. In section 3 we introduce 
the parameters essential for domain-specific WSD. 
Section 4 builds the case for parameter projection. 
Section 5 introduces the Multilingual Dictionary 
Framework which plays a key role in parameter 
projection. Section 6 is the core of the work, where 
we present parameter projection from one language 
to another. Section 7 describes two WSD algo-
rithms which combine various parameters for do-
459
main-specific WSD. Experiments and results are 
presented in sections 8 and 9. Section 10 concludes 
the paper. 
2 Related work 
Knowledge based approaches to WSD such as 
Lesk?s algorithm (Michael Lesk, 1986), Walker?s 
algorithm (Walker D. & Amsler R., 1986), concep-
tual density (Agirre Eneko & German Rigau, 1996) 
and random walk algorithm (Mihalcea Rada, 2005) 
essentially do Machine Readable Dictionary loo-
kup. However, these are fundamentally overlap 
based algorithms which suffer from overlap sparsi-
ty, dictionary definitions being generally small in 
length.  
Supervised learning algorithms for WSD are 
mostly word specific classifiers, e.g., WSD using 
SVM (Lee et. al., 2004), Exemplar based WSD 
(Ng Hwee T. & Hian B. Lee, 1996) and decision 
list based algorithm (Yarowsky, 1994). The re-
quirement of a large training corpus renders these 
algorithms unsuitable for resource scarce languag-
es. 
Semi-supervised and unsupervised algorithms 
do not need large amount of annotated corpora, but 
are again word specific classifiers, e.g., semi-
supervised decision list algorithm (Yarowsky, 
1995) and Hyperlex (V?ronis Jean, 2004)). Hybrid 
approaches like WSD using Structural Semantic 
Interconnections (Roberto Navigli & Paolo Velar-
di, 2005) use combinations of more than one 
knowledge sources (wordnet as well as a small 
amount of tagged corpora). This allows them to 
capture important information encoded in wordnet 
(Fellbaum, 1998) as well as draw syntactic genera-
lizations from minimally tagged corpora.  
At this point we state that no single existing so-
lution to WSD completely meets our requirements 
of multilinguality, high domain accuracy and 
good performance in the face of not-so-large 
annotated corpora. 
3 Parameters for WSD  
We discuss a number of parameters that play a 
crucial role in WSD. To appreciate this, consider 
the following example: 
 
The river flows through this region to meet the sea. 
 
The word sea is ambiguous and has three senses as 
given in the Princeton Wordnet (PWN): 
S1: (n) sea (a division of an ocean or a large body 
of salt water partially enclosed by land) 
S2: (n) ocean, sea (anything apparently limitless in 
quantity or volume) 
S3: (n) sea (turbulent water with swells of consi-
derable size) "heavy seas" 
Our first parameter is obtained from Domain 
specific sense distributions. In the above example, 
the first sense is more frequent in the tourism do-
main (verified from manually sense marked tour-
ism corpora). Domain specific sense distribution 
information should be harnessed in the WSD task. 
The second parameter arises from the domin-
ance of senses in the domain. Senses are ex-
pressed by synsets, and we define a dominant 
sense as follows: 
 
A few dominant senses in the Tourism domain are 
{place, country, city, area}, {body of water}, {flo-
ra, fauna}, {mode of transport} and {fine arts}. In 
disambiguating a word, that sense which belongs 
to the sub-tree of a domain-specific dominant 
sense should be given a higher score than other 
senses. The value of this parameter (?) is decided 
as follows: 
? = 1; if the candidate synset is a dominant synset 
? = 0.5; if the candidate synset belongs to the sub-
tree of a dominant synset 
? = 0.001; if the candidate synset is neither a do-
minant synset nor belongs to the sub-tree of a do-
minant synset. 
Our third parameter comes from Corpus co-
occurrence. Co-occurring monosemous words as 
well as already disambiguated words in the con-
text help in disambiguation. For example, the word 
river appearing in the context of sea is a mono-
semous word. The frequency of co-occurrence of 
river with the ?water body? sense of sea is high in 
the tourism domain. Corpus co-occurrence is cal-
A synset node in the wordnet hypernymy 
hierarchy is called Dominant if the syn-
sets in the sub-tree below the synset are 
frequently occurring in the domain cor-
pora. 
460
culated by considering the senses which occur in a 
window of 10 words around a sense. 
Our fourth parameter is based on the semantic 
distance between any pair of synsets in terms of 
the shortest path length between two synsets in the 
wordnet graph. An edge in the shortest path can be 
any semantic relation from the wordnet relation 
repository (e.g., hypernymy, hyponymy, meronymy, 
holonymy, troponymy etc.). 
For nouns we do something additional over and 
above the semantic distance. We take advantage of 
the deeper hierarchy of noun senses in the wordnet 
structure. This gives rise to our fifth and final pa-
rameter which arises out of the conceptual dis-
tance between a pair of senses. Conceptual 
distance between two synsets S1 and S2 is calcu-
lated using Equation (1), motivated by Agirre Ene-
ko & German Rigau (1996). 
 
Concep-
tual 
Distance    
(S1, S2) 
 
 
 
= 
Length of the path between (S1, 
S2) in terms of hypernymy hie-
rarchy 
Height of the lowest common 
ancestor of S1 and S2 in the word-
net hierarchy 
 
 
 (1) 
The conceptual distance is proportional to the 
path length between the synsets, as it should be. 
The distance is also inversely proportional to the 
height of the common ancestor of two sense nodes, 
because as the common ancestor becomes more 
and more general the conceptual relatedness tends 
to get vacuous (e.g., two nodes being related 
through entity which is the common ancestor of 
EVERYTHING, does not really say anything 
about the relatedness). 
To summarize, our various parameters used for 
domain-specific WSD are: 
Wordnet-dependent parameters  
? belongingness-to-dominant-concept 
? conceptual-distance 
? semantic-distance 
Corpus-dependent parameters 
? sense distributions 
? corpus co-occurrence. 
In section 7 we show how these parameters are 
used to come up with a scoring function for WSD. 
4 Building a case for Parameter Projec-
tion   
Wordnet-dependent parameters depend on the 
graph based structure of Wordnet whereas the 
Corpus-dependent parameters depend on various 
statistics learnt from a sense marked corpora. Both 
the tasks of (a) constructing a wordnet from scratch 
and (b) collecting sense marked corpora for mul-
tiple languages are tedious and expensive. An im-
portant question being addressed in this paper is: 
whether the effort required in constructing seman-
tic graphs for multiple wordnets and collecting 
sense marked corpora can be avoided? Our find-
ings seem to suggest that by projecting relations 
from the wordnet of a language and by projecting 
corpus statistics from the sense marked corpora of 
the language we can achieve this end. Before we 
proceed to discuss the way to realize parameter 
projection, we present a novel dictionary which 
facilitates this task. 
5 Synset based multilingual dictionary  
Parameter projection as described in section 4 rests 
on a novel and effective method of storage and use 
of dictionary in a multilingual setting proposed by 
Mohanty et. al. (2008). For the purpose of current 
discussion, we will call this multilingual dictionary 
framework MultiDict. One important departure 
from traditional dictionary is that synsets are 
linked, and after that the words inside the syn-
sets are linked. The basic mapping is thus be-
tween synsets and thereafter between the words.  
 
Concepts L1 
(Eng-
lish) 
L2 (Hindi) L3 (Mara-
thi) 
04321: a 
youthful 
male per-
son 
{male
child, 
boy} 
{????? ladkaa, 
????  baalak,  
????? 
bachchaa}  
{?????  mulgaa , 
?????  porgaa , 
???  por } 
Table 1: Multilingual Dictionary Framework 
Table 1 shows the structure of MultiDict, with one 
example row standing for the concept of boy. The 
first column is the pivot describing a concept with 
a unique ID. The subsequent columns show the 
words expressing the concept in respective lan-
guages (in the example table above, English, Hindi 
and Marathi). Thus to express the concept ?04321: 
a youthful male person?, there are two lexical ele-
ments in English, which constitute a synset. Cor-
respondingly, the Hindi and Marathi synsets 
contain 3 words each. 
461
It may be noted that the central language whose 
synsets the synsets of other languages link to is 
Hindi. This way of linking synsets- more popularly 
known as the expansion approach- has several ad-
vantages as discussed in (Mohanty et. al., 2008). 
One advantage germane to the point of this paper 
is that the synsets in a particular column automati-
cally inherit the various semantic relations of the 
Hindi wordnet (Dipak Narayan et. al., 2000), 
which saves the effort involved in reconstructing 
these relations for multiple languages. 
After the synsets are linked, cross linkages are 
set up manually from the words of a synset to the 
words of a linked synset of the central language. 
The average number of such links per synset per 
language pair is approximately 3. These cross-
linkages actually solve the problem of lexical 
choice in translating from text of one language to 
another. 
Thus for the Marathi word ?????  {mulagaa} de-
noting ?a youthful male person?, the correct lexi-
cal substitute from the corresponding Hindi synset 
is ????? {ladakaa} (Figure 1). One might argue that 
any word within the synset could serve the purpose 
of translation. However, the exact lexical substitu-
tion has to respect native speaker acceptability.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Cross linked synset members for the 
concept: a youthful male person 
We put these cross linkages to another use, as 
described later. 
Since it is the MultiDict which is at the heart of 
parameter projection, we would like to summarize 
the main points of this section. (1) By linking with 
the synsets of Hindi, the cost of building wordnets 
of other languages is partly reduced (semantic rela-
tions are inherited). The wordnet parameters of 
Hindi wordnet now become projectable to other 
languages. (2) By using the cross linked words in 
the synsets, corpus parameters become projectable 
(vide next section).  
6 Parameter projection using MultDict  
6.1 P(Sense|Word) parameter 
Suppose a word (say, W) in language L1 (say, Ma-
rathi) has k senses. For each of these k senses we 
are interested in finding the parameter P(Si|W)- 
which is the probability of sense Si given the word 
W expressed as: 
? ??  ?) =  
#(??  ,?)
 #(??  ,?)?  
 
where ?#? indicates ?count-of?. Consider the exam-
ple of two senses of the Marathi word ???? 
{saagar}, viz., sea and abundance and the corres-
ponding cross-linked words in Hindi (Figure 2 be-
low): 
     Marathi            Hindi 
 
 
 
 
 
 
Figure 2: Two senses of the Marathi word ???? 
(saagar), viz., {water body} and {abundance}, and 
the corresponding cross-linked words in Hindi1. 
The probability P({water body}|saagar) for Mara-
thi is  
#({????? ????}, ??????)
#({????? ????}, ??????) + #({?????????}, ??????)
 
 
We propose that this can be approximated by the 
counts from Hindi sense marked corpora by replac-
ing saagar with the cross linked Hindi words sa-
mudra and saagar, as per Figure 2: 
 
#({water body}, samudra)
#({water body}, samudra) + #({abundance}, saagar)
 
                                                          
1 Sense_8231 shows the same word saagar for both Marathi 
and Hindi. This is not uncommon, since Marathi and Hindi are 
sister languages. 
????? 
/MW1 
mulagaa,  
????? 
/MW2 
poragaa, 
??? /MW3 
pora  
 
  ????? 
/HW1 
ladakaa,  
???? 
/HW2 
baalak, 
????? /HW3 
bachcha, 
???? /HW4 
choraa  
 
 
 
male-child 
/HW1, 
 
boy 
/HW2  
 
 
 
 
Marathi Synset Hindi Synset   English Synset 
Sense_2650 
Sense_8231 
 
saagar (sea) 
{water body} 
saagar (sea) 
{abundance} 
samudra (sea) 
{water body} 
saagar (sea) 
{abundance} 
462
Thus, the following formula is used for calculat-
ing the sense distributions of Marathi words using 
the sense marked Hindi corpus from the same do-
main: 
? ?? ?) =  
#(??  , ?????_??????_?????_????)
 #(??  , ?????_??????_?????_????)?  
           (2) 
Note that we are not interested in the exact sense 
distribution of the words, but only in the relative 
sense distribution.  
To prove that the projected relative distribution 
is faithful to the actual relative distribution of 
senses, we obtained the sense distribution statistics 
of a set of Marathi words from a sense tagged Ma-
rathi corpus (we call the sense marked corpora of a 
language its self corpora). These sense distribu-
tion statistics were compared with the statistics for 
these same words obtained by projecting from a 
sense tagged Hindi corpus using Equation (2).  The 
results are summarized in Table 2. 
Sr. 
No 
Marathi 
Word 
Synset P(S|word) 
as learnt 
from 
sense 
tagged 
Marathi 
corpus 
P(S|word) as 
projected 
from sense 
tagged 
Hindi cor-
pus 
1 ???? ?? 
(kimat)  
{ worth } 0.684 0.714 
{ price }  0.315 0.285 
2 ????? 
(rasta)  
 
{ roadway } 0.164 0.209 
{road, 
route} 
0.835 0.770 
3 ????? 
(thikan) 
{ land site, 
place} 
0.962 0.878 
{ home } 0.037 0.12 
4 ???? 
(saagar) 
{water 
body} 
1.00 1.00 
{abun-
dance} 
0 0 
Table 2: Comparison of the sense distributions of 
some Marathi words learnt from Marathi sense 
tagged corpus with those projected from Hindi 
sense tagged corpus. 
The fourth row of Table 2 shows that whenever 
???? (saagar) (sea) appears in the Marathi tourism 
corpus there is a 100% chance that it will appear in 
the ?water body? sense and 0% chance that it will 
appear in the sense of ?abundance?. Column 5 
shows that the same probability values are ob-
tained using projections from Hindi tourism cor-
pus. Taking another example, the third row shows 
that whenever ????? (thikaan) (place, home) ap-
pears in the Marathi tourism corpus there is a much 
higher chance of it appearing in the sense of 
?place? (96.2%) then in the sense of ?home? 
(3.7%). Column 5 shows that the relative proba-
bilities of the two senses remain the same even 
when using projections from Hindi tourism corpus 
(i.e. by using the corresponding cross-linked words 
in Hindi). To quantify these observations, we cal-
culated the average KL divergence and Spearman?s 
correlation co-efficient between the two distribu-
tions. The KL divergence is 0.766 and Spearman?s 
correlation co-efficient is 0.299. Both these values 
indicate that there is a high degree of similarity 
between the distributions learnt using projection 
and those learnt from the self corpus. 
6.2 Co-occurrence parameter 
Similarly, within a domain, the statistics of co-
occurrence of senses remain the same across lan-
guages. For example, the co-occurrence of the Ma-
rathi synsets {???? (akash) (sky), ????? (ambar) 
(sky)} and {??? (megh) (cloud), ???? (abhra) 
(cloud)} in the Marathi corpus remains more or 
less same as (or proportional to) the co-occurrence 
between the corresponding Hindi synsets in the 
Hindi corpus.   
Sr. No Synset Co-
occurring 
Synset 
P(co-
occurrence) 
as learnt 
from sense 
tagged 
Marathi 
corpus 
P(co-
occurrence) 
as learnt 
from sense 
tagged 
Hindi 
corpus 
1 {???, ?????} 
{small bush} 
{???, ?????, 
?????, ?????, 
???, ????}  
{tree} 
0.125 0.125 
2 {???, ????} 
{cloud} 
{????, 
????, 
?????}  
{sky} 
0.167 0.154 
3 {?????, ?^???, 
?^???, 
??????}  
{geographical 
area} 
{????, 
???}  
{travel} 
0.0019 0.0017 
Table 3: Comparison of the corpus co-occurrence 
statistics learnt from Marathi and Hindi Tourism 
corpus. 
463
Table 3 shows a few examples depicting similarity 
between co-occurrence statistics learnt from Mara-
thi tourism corpus and Hindi tourism corpus. Note 
that we are talking about co-occurrence of synsets 
and not words. For example, the second row shows 
that the probability of co-occurrence of the synsets 
{cloud} and {sky} is almost same in the Marathi 
and Hindi corpus. 
7 Our algorithms for WSD 
We describe two algorithms to establish the use-
fulness of the idea of parameter projection. The 
first algorithm- called iterative WSD (IWSD-) is 
greedy, and the second based on PageRank algo-
rithm is exhaustive. Both use scoring functions that 
make use of the parameters detailed in the previous 
sections.  
7.1 Iterative WSD (IWSD) 
We have been motivated by the Energy expression 
in Hopfield network (Hopfield, 1982) in formulat-
ing a scoring function for ranking the senses. Hop-
field Network is a fully connected bidirectional 
symmetric network of bi-polar (0/1 or +1/-1) neu-
rons. We consider the asynchronous Hopfield 
Network. At any instant, a randomly chosen neu-
ron (a) examines the weighted sum of the input, (b) 
compares this value with a threshold and (c) gets to 
the state of 1 or 0, depending on whether the input 
is greater than or less than or equal to the thre-
shold. The assembly of 0/1 states of individual 
neurons defines a state of the whole network. Each 
state has associated with it an energy, E, given by 
the following expression 
 
? = ????? +  ???
?
?>?
?
?=1
????  
 
(3) 
 
where, N is the total number of neurons in the net-
work, ??   and ??  are the activations of neurons i and 
j respectively and ???  is the weight of the connec-
tion between neurons i and j.  Energy is a funda-
mental property of Hopfield networks, providing 
the necessary machinery for discussing conver-
gence, stability and such other considerations. 
The energy expression as given above cleanly 
separates the influence of self-activations of neu-
rons and that of interactions amongst neurons to 
the global macroscopic property of energy of the 
network.  This fact has been the primary insight for 
equation (4) which was proposed to score the most 
appropriate synset in the given context. The cor-
respondences are as follows:   
 
Neuron ? Synset 
Self-activation ? Corpus Sense Distribu-
tion 
Weight of connec-
tion between two 
neurons 
 
? 
Weight as a function of 
corpus co-occurrence 
and Wordnet distance 
measures between syn-
sets 
 
?? = argmax
?
  ?? ? ?? +  ??? ? ?? ? ??
?  ? J
   4  
?????, 
  J = ??? ?? ????????????? ?????            
         ?? = ?????????????????????????????? (??)
   ??  = ? ??  | ????                                                   
 
 ??? =  ??????????????????  ?? , ??                    
                 ?  1 ????????????????????(?? , ?? )           
                   ?  1 ???????????????????????(?? , ?? )    
 
The component ?? ? ??  of the energy due to the self 
activation of a neuron can be compared to the cor-
pus specific sense of a word in a domain. The other 
component ??? ?  ?? ? ??  coming from the interaction 
of activations can be compared to the score of a 
sense due to its interaction in the form of corpus 
co-occurrence, conceptual distance, and wordnet-
based semantic distance with the senses of other 
words in the sentence. The first component thus 
captures the rather static corpus sense, whereas the 
second expression brings in the sentential context.  
Algorithm 1: performIterativeWSD(sentence) 
1. Tag all monosemous words in the sentence. 
2. Iteratively disambiguate the remaining words in the 
sentence in increasing order of their degree of polyse-
my. 
3. At each stage select that sense for a word which max-
imizes the score given by Equation (4) 
Algorithm1: Iterative WSD  
IWSD is clearly a greedy algorithm. It bases its 
decisions on already disambiguated words, and 
ignores words with higher degree of polysemy. For 
example, while disambiguating bisemous words, 
the algorithm uses only the monosemous words. 
464
7.2 Modified PageRank algorithm 
Rada Mihalcea (2005) proposed the idea of using 
PageRank algorithm to find the best combination 
of senses in a sense graph. The nodes in a sense 
graph correspond to the senses of all the words in a 
sentence and the edges depict the strength of inte-
raction between senses. The score of each node in 
the graph is then calculated using the following 
recursive formula: 
????? ?? =                                                                 
 1? d + d ?  
Wij
 WjkSk?Out  Si 
? Score Sj 
S j?In S i 
 
Instead of calculating Wij  based on the overlap 
between the definition of senses Si and S  as pro-
posed by Rada Mihalcea (2005), we calculate the 
edge weights using the following formula: 
 ??? =  ??????????????????  ?? , ??                    
                   ?  1 ???????????????????? ?? , ??             
                   
?  1 ??????????????????????? ?? , ??    
?  ? ??  | ?????                                                   
?  ? ??  | ?????                                                   
  
? = ??????? ??????  ????????? 0.85     
 
This formula helps capture the edge weights in 
terms of the corpus bias as well as the interaction 
between the senses in the corpus and wordnet. It 
should be noted that this algorithm is not greedy. 
Unlike IWSD, this algorithm allows all the senses 
of all words to play a role in the disambiguation 
process.  
8 Experimental Setup: 
We tested our algorithm on tourism corpora in 3 
languages (viz., Marathi, Bengali and Tamil) and 
health corpora in 1 language (Marathi) using pro-
jections from Hindi. The corpora for both the do-
mains were manually sense tagged. A 4-fold cross 
validation was done for all the languages in both 
the domains. The size of the corpus for each lan-
guage is described in Table 4. 
Language # of polysemous words 
(tokens) 
Tourism 
Domain 
Health 
Domain 
Hindi 50890 29631 
Marathi 32694 8540 
Bengali 9435  - 
Tamil 17868 - 
Table 4: Size of manually sense tagged corpora for 
different languages. 
 
Table 5 shows the number of synsets in MultiDict 
for each language. 
Language # of synsets in 
MultiDict 
Hindi 29833 
Marathi 16600 
Bengali 10732 
Tamil 5727 
Table 5: Number of synsets for each language 
 
Algorithm Language 
Marathi Bengali 
P  % R % F % P  % R % F % 
IWSD (training on self corpora; no parameter pro-
jection) 81.29 80.42 80.85 81.62 78.75 79.94 
IWSD (training on Hindi and reusing parameters  
for another language) 73.45 70.33 71.86 79.83 79.65 79.79 
PageRank (training on self corpora; no parameter 
projection) 79.61 79.61 79.61 76.41 76.41 76.41 
PageRank (training on Hindi and reusing parame-
ters  for another language) 71.11 71.11 71.11 75.05 75.05 75.05 
Wordnet Baseline 58.07 58.07 58.07 52.25 52.25 52.25 
Table 6: Precision, Recall and F-scores of IWSD, PageRank and Wordnet Baseline. Values are re-
ported with and without parameter projection. 
 
465
9 Results and Discussions 
Table 6 shows the results of disambiguation (preci-
sion, recall and F-score). We give values for two 
algorithms in the tourism domain: IWSD and Pa-
geRank. In each case figures are given for both 
with and without parameter projection. The word-
net baseline figures too are presented for the sake 
of grounding the results.  
Note the lines of numbers in bold, and compare 
them with the numbers in the preceding line. This 
shows the fall in accuracy value when one tries the 
parameter projection approach in place of self cor-
pora. For example, consider the F-score as given 
by IWSD for Marathi. It degrades from about 81% 
to 72% in using parameter projection in place of 
self corpora.  Still, the value is much more than the 
baseline, viz., the wordnet first sense (a typically 
reported baseline). 
Coming to PageRank for Marathi, the fall in ac-
curacy is about 8%. Appendix A shows the corres-
ponding figure for Tamil with IWSD as 10%. 
Appendix B reports the fall to be 11% for a differ-
ent domain- Health- for Marathi (using IWSD).  
In all these cases, even after degradation the per-
formance is far above the wordnet baseline. This 
shows that one could trade accuracy with the cost 
of creating sense annotated corpora.  
10 Conclusion and Future Work: 
Based on our study for 3 languages and 2 domains, 
we conclude the following: 
(i) Domain specific sense distributions- if 
obtainable- can be exploited to advantage. 
(ii) Since sense distributions remain same across 
languages, it is possible to create a disambiguation 
engine that will work even in the absence of sense 
tagged corpus for some resource deprived 
language, provided (a) there are aligned and cross 
linked sense dictionaries for the language in 
question and another resource rich language, (b) 
the domain in which disambiguation needs to be 
performed for the resource deprived language is 
the same as the domain for which sense tagged 
corpora is available for the resource rich language.  
(iii) Provided the accuracy reduction is not drastic, 
it may make sense to trade high accuracy for the 
effort in collecting sense marked corpora.  
It would be interesting to test our algorithm on 
other domains and other languages to conclusively 
establish the effectiveness of parameter projection 
for multilingual WSD.  
It would also be interesting to analyze the con-
tribution of corpus and wordnet parameters inde-
pendently. 
References  
Agirre Eneko & German Rigau. 1996. Word sense dis-
ambiguation using conceptual density. In Proceed-
ings of the 16th International Conference on 
Computational Linguistics (COLING), Copenhagen, 
Denmark. 
Dipak Narayan, Debasri Chakrabarti, Prabhakar Pande 
and P. Bhattacharyya. 2002. An Experience in Build-
ing the Indo WordNet - a WordNet for Hindi. First 
International Conference on Global WordNet, My-
sore, India. 
Eneko Agirre & Philip Edmonds. 2007. Word Sense 
Disambiguation Algorithms and Applications. Sprin-
ger Publications. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical 
Database. The MIT Press.  
Hindi Wordnet. 
http://www.cfilt.iitb.ac.in/wordnet/webhwn/ 
J. J. Hopfield. April 1982. "Neural networks and physi-
cal systems with emergent collective computational 
abilities", Proceedings of the National Academy of 
Sciences of the USA, vol. 79 no. 8 pp. 2554-2558. 
Lee Yoong K., Hwee T. Ng & Tee K. Chia. 2004. Su-
pervised word sense disambiguation with support 
vector machines and multiple knowledge sources. 
Proceedings of Senseval-3: Third International 
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, Barcelona, Spain, 137-140. 
Lin Dekang. 1997. Using syntactic dependency as local 
context to resolve word sense ambiguity. In Proceed-
ings of the 35th Annual Meeting of the Association 
for Computational Linguistics (ACL), Madrid, 64-71. 
Michael Lesk. 1986. Automatic sense disambiguation 
using machine readable dictionaries: how to tell a 
pine cone from an ice cream cone. In Proceedings of 
the 5th annual international conference on Systems 
documentation, Toronto, Ontario, Canada. 
Mihalcea Rada. 2005. Large vocabulary unsupervised 
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of 
the Joint Human Language Technology and Empiri-
466
cal Methods in Natural Language Processing Confe-
rence (HLT/EMNLP), Vancouver, Canada, 411-418. 
Ng Hwee T. & Hian B. Lee. 1996. Integrating multiple 
knowledge sources to disambiguate word senses: An 
exemplar-based approach. In Proceedings of the 34th 
Annual Meeting of the Association for Computation-
al Linguistics (ACL), Santa Cruz, U.S.A., 40-47. 
Peter F. Brown and Vincent J.Della Pietra and Stephen 
A. Della Pietra and Robert. L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics Vol 
19, 263-311. 
Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar 
Pande, Shraddha Kalele, Mitesh Khapra and Aditya 
Sharma. 2008. Synset Based Multilingual Dictionary: 
Insights, Applications and Challenges. Global Word-
net Conference, Szeged, Hungary, January 22-25. 
Resnik Philip. 1997. Selectional preference and sense 
disambiguation. In Proceedings of ACL Workshop 
on Tagging Text with Lexical Semantics, Why, What 
and How? Washington, U.S.A., 52-57. 
Roberto Navigli, Paolo Velardi. 2005. Structural Se-
mantic Interconnections: A Knowledge-Based Ap-
proach to Word Sense Disambiguation. IEEE 
Transactions On Pattern Analysis and Machine Intel-
ligence. 
Sergei Nirenburg, Harold Somers, and Yorick Wilks. 
2003. Readings in Machine Translation. Cambridge, 
MA: MIT Press. 
V?ronis Jean. 2004. HyperLex: Lexical cartography for 
information retrieval. Computer Speech & Language, 
18(3):223-252. 
Walker D. and Amsler R. 1986. The Use of Machine 
Readable Dictionaries in Sublanguage Analysis. In 
Analyzing Language in Restricted Domains, Grish-
man and Kittredge (eds), LEA Press, pp. 69-83. 
Yarowsky David. 1994. Decision lists for lexical ambi-
guity resolution: Application to accent restoration in 
Spanish and French. In Proceedings of the 32nd An-
nual Meeting of the association for Computational 
Linguistics (ACL), Las Cruces, U.S.A., 88-95. 
Yarowsky David. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics (ACL), 
Cambridge, MA, 189-196. 
 
 
Appendix A: Results for Tamil (Tourism 
Domain) 
Algorithm P  % R  % F % 
IWSD (training on 
Tamil) 89.50 88.18 88.83 
IWSD (training on 
Hindi and reusing  for 
Tamil) 84.60 73.79 78.82 
Wordnet Baseline 65.62 65.62 65.62 
Table 7: Tamil Tourism corpus using parameters 
projected from Hindi 
Appendix B: Results for Marathi (Health 
Domain) 
Algorithm 
Words 
P  % R  % F % 
IWSD (training on Mara-
thi) 84.28 81.25 82.74 
IWSD (training on Hindi 
and reusing  for Marathi) 75.96 67.75 71.62 
Wordnet Baseline 60.32 60.32 60.32 
Table 8: Marathi Health corpus parameters pro-
jected from Hindi 
 
467
