Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 11?18,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
An Extensible Crosslinguistic Readability Framework
Jesse Saba Kirchner
Department of Linguistics
UC Santa Cruz
1156 High Street
Santa Cruz, CA 95064
kirchner@ucsc.edu
Justin Nuger
Department of Linguistics
UC Santa Cruz
1156 High Street
Santa Cruz, CA 95064
jnuger@ucsc.edu
Yi Zhang
Baskin School of Engineering
UC Santa Cruz
1156 High Street, SOE 3
Santa Cruz, CA 95064
yiz@soe.ucsc.edu
Abstract
Automatic assessment of the readability
level (i.e., the relative linguistic complex-
ity) of documents in a large number of
languages is an important problem that
can be applied to many real-world appli-
cations, such as retrieving age-appropriate
search engine results for kids, construct-
ing automatic tutoring systems, and so on.
Unfortunately, existing readability label-
ing techniques have only been applied to
a very small number of languages. In this
paper, we present an extensible crosslin-
guistic readability framework based on the
use of parallel corpora to quickly create
readability software for thousands of lan-
guages, including languages for which no
linguists are available to define readability
rules or for which documents with read-
ability labels are lacking to train readabil-
ity models. To demonstrate our idea, we
developed a system based on the proposed
framework. This paper discusses the theo-
retical and practical issues involved in de-
signing such a system and presents the re-
sults of an experiment conducted with the
system.
1 Introduction
Automatically labeling the reading difficulty of an
arbitrary document is an important problem in sev-
eral human language technology applications. It
can, for example, be used in the next generation of
personalized information retrieval systems to find
documents tailored to children at different grade
levels. In a tutoring system, it can be used to find
online reading materials of the appropriate diffi-
culty level for students (Heilman et al, 2006).
Of the world?s more than 6,000 languages
(Grimes, 2005), readability classification software
exists for a striking few, and it is limited in cover-
age to languages spoken in countries with promi-
nent standing in global economics and politics.
A substantial number of the remaining languages
nevertheless have a sufficient corpus of digital
documents ? a number which may already be
in the hundreds and soon in the thousands (Pao-
lillo et al, 2005). A natural idea is to create
software to automatically predict readability lev-
els (henceforth ?RLs?) for these documents. Such
software has significant potential for applications
in different areas of research, such as creating web
search engines for kids speaking languages not
covered by existing readability software, as de-
scribed above.
There is much research on assessing the read-
ing difficulties of texts in a particular language,
and the existing work can be roughly classified as
falling under two approaches. The first approach
uses manually or semi-automatically crafted rules
designed by computational linguists who are fa-
miliar with the language in question (Anderson,
1981). The second approach learns readability
models for a particular language based on labeled
data (Collins-Thompson and Callan, 2004).
Unfortunately, existing approaches cannot be
easily extended to handle thousands of different
languages. The first approach, using rules de-
vised by computational linguists familiar with the
languages, is impractical because for many lan-
guages, especially minority or understudied lan-
guages, there are relatively few linguists suffi-
ciently familiar with the language to design such
software. Even if these linguists exist, it is un-
likely that a search engine company that wanted to
serve the whole world would have the resources to
11
hire all of them. The second approach, using ma-
chine learning techniques on labeled data, is very
expensive because it requires the support of edu-
cated speakers of each language to provide read-
ability labels for documents in the language. The
availability of such speakers cannot always be as-
sumed. Again, recruiting annotators for thousands
of different languages is not economically feasible
or practical for a company. An alternative strategy
that can scale to thousands of different languages
is needed.
In this paper, we propose a general framework
to solve this problem based on a parallel corpus
crawled from the web. To illustrate the idea, we
developed an Extensible Crosslinguistic Readabil-
ity system (henceforth ?ECR system?), which uses
a Cross-Lingual Information Retrieval (henceforth
?CLIR?) system that we call EXCLAIM. The ECR
system functions to create RL classification soft-
ware in any language with sufficient coverage in
the CLIR system. We also report the promising ?
though very preliminary ? results of an experi-
ment that tests a real-world application of this sys-
tem. Investigation of the basic assumptions and
generalization of parameters and evaluation met-
rics are left for future work.
The rest of this paper is organized as follows.
The problem setting is described in Section 2. The
architecture of our ECR system is explained in
Section 3. Our experimental design is laid out in
Section 4, followed by experimental result analy-
sis in Section 5. Section 6 gives an overview of
related work, and section 7 concludes.
2 Problem and Proposed Methodology
2.1 Existing Approaches to Readability
Classification
In traditional approaches to computational read-
ability classification, there is a variety of language-
specific system requirements needed in order to
perform the RL classification task. For some
languages, this task is relatively well-studied.
For example, the simple and widely-used Laes-
barhedsindex (henceforth ?LIX?) calculates RLs
for texts written in Western European languages1
with the following LIX formula:
RLD = wordssentences +
100 ? wordschar>6
words
1In practice, LIX may be substituted with other metrics,
such as Flesch-Kincaid.
where D is a document written in an unfamiliar
language, and RLD is the readability score of the
document D.
The above formula relies on specific parame-
ters which have been tuned to a certain set of lan-
guages. These include the total number of words
in D (words), the total number of sentences in D
(sentences), and the total number of words in D
with more than six characters (wordschar>6 ).
Although this formula may be successful in
RL classification for languages like English and
French (Bjo?rnsson and Ha?rd af Segerstad(1979),
Anderson (1981)), it remains essentially parochial
in the context of other languages because the pa-
rameters overfit the data from the Western Euor-
pean languages for which it was designed. Since
the LIX formula depends on measuring the num-
ber of characters in a word to find words greater
than 6, it is ineffective in determining the readabil-
ity of documents written in languages with differ-
ent writing systems, such as Chinese. This is due
to the fact that some languages, like Chinese, are
written with characters based on semantic mean-
ing rather than phonemes, as in English, and a
large number of Chinese words consist of just one
or two characters, regardless of semantic complex-
ity (Li and Thompson, 1981). In a similar vein,
many languages of the world (even some that use
phonemically-based writing systems) do not ad-
here to the implicit assumption of the LIX formula
that semantically ?complex? words are longer than
simpler words (Greenberg, 1954). In these lan-
guages, then, the same metric cannot be used as a
valid measure of RL difficulty of documents, since
word length does not correlate with semantic com-
plexity.
One recent alternative approach has been devel-
oped for readability labeling that uses multiple sta-
tistical language models (Collins-Thompson and
Callan, 2004). The idea is to train statistical lan-
guage models for each grade level automatically
from manually labeled training documents. How-
ever, even an approach like this is not scalable to
handle thousands of languages, since it is hard to
recruit annotators of all of these languages to man-
ually label the training data.
2.2 Proposed Solution
We propose a scalable solution to the problem of
labeling the readability of documents in many lan-
guages. The general idea is to combine CLIR
12
technology with off-the-shelf readability software
for at least one well-studied language, such as
English. First, off-the-shelf readability software
is used to assign RLs to a set of documents in
the source language, e.g. English, which serve as
training data. Second, a set of key terms is se-
lected from each group of documents correspond-
ing to a particular RL to construct a readability
model for that RL. Third, for each of these sets
of terms, the cross-lingual query-expansion com-
ponent of the CLIR system returns a semantically
relevant set of terms in the target language. Fi-
nally, these target-language term sets are used to
build the target-language RL models, which can
be used to assign RLs to documents in the tar-
get language, even if language-specific readability
classification software does not exist for that lan-
guage. This solution plausibly extends to any of
the languages covered by the CLIR system. It is
possible to create a CLIR system by crawling the
internet for parallel corpora, which exist for many
language pairs. As a result, the proposed solution
already has the potential to cover many different
languages.
The success of this method relies on the as-
sumption that readability levels remain fairly con-
stant across syntactically and semantically paral-
lel documents in the two languages in question,
or simply across documents typified by equivalent
key terms. This does not seem unreasonable: if the
same information is represented in two different
languages in semantically and structurally compa-
rable ways, it is likely that the reading difficulty of
the two texts should not differ much, if at all. If
this assumption is true, generation of readability
software really depends only on the availability of
a solid CLIR system, and the problem of requir-
ing trained computational linguists and native lan-
guage speakers to design the system is mitigated.
Figure 1 shows a simple process model of a sys-
tem for generating RL classifiers for various lan-
guages. A set of training documents from a source
language (i.e., the ?L1? in Figure 1) is assigned
RLs by the off-the-shelf RL classification soft-
ware R(L1). Using the source langauge files and
the RLs produced by R(L1), the ECR system pro-
duces a source language (L1) readability model.
Through the system interface, the CLIR system
(EXCLAIM) uses the L1 readability model to pro-
duce a target language (L2) readability model. The
system uses the L2 readability model to produce a
Figure 1: ECR Domain
new RL classifier R(L2) for the target language.
The newly developed classifier R(L2) can then be
used to classify documents in the L2.
3 System Architecture
To address any theoretical or empirical concerns
and questions about the proposed solution, includ-
ing those relating to the assumption that key term
equivalence correlates with RL equivalence, we
have developed an ECR system compatible with
an existing CLIR system and have proposed eval-
uation metrics for this system. We developed
the ECR system to meet the needs of two differ-
ent kinds of users. First, higher-level intermedi-
ate users can build RL classification software for
a given target language. Second, end users can
use the software to classify documents in that lan-
guage. In this section, we give a developer?s-eye
view of the system architecture (shown in Figure
2), making specific reference to the points at which
intermediate and end users may interact with the
system. For presentational clarity, we periodically
adopt the arbitrary assumption that the source lan-
guage is English, as this is the source language of
our experiment described in the following section.
The ECR system has three primary tasks. The
first task is to enable intermediate users to develop
RL classification model for the source language.
The second task is to provide the intermediate user
with a toolkit to construct language-specific soft-
ware that automatically tags documents in the tar-
get language with the appropriate RLs. The final
task is to provide an interface module for the end
13
Figure 2: ECR System design
user to utilize this software.
In order to approach the first task, one needs a
set of documents in a source language for which
off-the-shelf readability software is available. This
set of documents functions as a training data set;
if a user is trying to assign RLs to documents
in a particular domain ? e.g., forestry, medical,
leisure, etc. ? then (s)he can already help shape
the results of the system by providing domain-
relevant source langauge data at this stage. To
aid the intermediate user in obtaining RLs for this
set of data, the ECR system has a number of pa-
rameters that may be selected, based on different
models of RL-tagging ? for example, we selected
English as the source language and the aforemen-
tioned LIX formula due to its simplicity. The doc-
uments are then organized according to the gener-
ated RLs and separated into different RL groups.
At this point, the K most salient words are
extracted from each source language RL groups
(RLS) based on the following tf*idf term weight-
ing:2
wi ,j =
(
0.5 + 0.5 freqi,jmaxl freql,j
)
? logNni
2In principle, this choice is arbitrary and any other appro-
priate term-weighting formula could also be used.
The selected words RLS = {f1 , f2 , ...fK }
form the basis for constructing an RL classifica-
tion model for an unknown target language.
In order to construct a target language RL clas-
sification model, the cross-lingual query expan-
sion component of a CLIR system is necessary
to select semantically comparable and semanti-
cally related words in the target language. The
CLIR system we developed is called EXCLAIM,
or the EXtensible Cross-Linguistic Automatic
Information Machine. We constructed EXCLAIM
from a semantically (though not structurally) par-
allel corpus crawled from Wikipedia (Wikime-
dia Foundation, 1999). All Wikipedia articles with
both source and target language versions collec-
tively function as data to construct the CLIR com-
ponent. Due to Wikipedia?s coverage of a large
amount of languages (English being the language
with the largest collection of articles at the time
of writing), CLIR components for English paired
with a wide number of target languages was cre-
ated for EXCLAIM.
For each RLS, the query-expansion component
of EXCLAIM determines a set of corresponding
words for the target language RLT. Initially, each
word in RLS is matched with the source language
document in EXCLAIM for which it has the highest
tf*idf term weight. The M most salient terms in
the corresponding target language document (cal-
culated once again using the tf*idf formula) are
then added to RLT. Therefore, RLT contains no
more than K ? M terms. The total set of RLTs
form the base of the target language readability
classification model.
Using this model, the system generates target
language readability classification software on the
fly, which plugs into the system?s existing inter-
face module for end users. Through the module,
the end user can use the newly generated software
to determine RLs for a set of target language doc-
uments without requiring any specialized knowl-
edge of the languages or the software development
process.
4 Experimental Design
We conducted an experiment to demonstrate this
idea and to test our ECR system. Without loss
of generality, we chose English as our source lan-
guage and Chinese as our target language. While
Chinese is a major language for which it would
be relatively easy to find linguistic experts to write
14
readability rules and native speakers to label doc-
ument readability for training, our goal is not to
demonstrate that the proposed solution is the best
solution to build readability software for Chinese.
Instead, we chose these languages for the follow-
ing reasons. First, we are capable of reading both
languages and are thus able to judge the quality of
the ECR system. Second, publicly available En-
glish readability labeling software exists, and we
are not aware of such software for Chinese. Third,
we had access to a parallel set of documents that
could be used for the evaluation of our experiment.
Fourth, the many differences between English and
Chinese might demonstrate the applicability of our
system for a diverse set of languages. However,
the features that made Chinese a desirable target
language for us are not essential for the proposed
solution, and do not affect the extensibility of the
approach.
We created a test set using a collection of
Chinese-English parallel documents from the
medical domain (Chinese Community Health Re-
source Center, 2004). The set comprised 65 docu-
ments in English and their human-translated Chi-
nese translations. Although a typical user does
not need to have access to sets of bilingual doc-
uments for the system to run successfully, we cir-
cumvented both the lack of off-the-shelf Chinese
readability labeling software and the lack of la-
beled Chinese documents for the evaluation of the
results of our system by using a high quality trans-
lated parallel document set. Since RLs are rough
measures of semantic and structural complexity,
we assume they should be approximately if not
exactly the same for a given document and its
translation in a different language, an extension of
the ideas in Collins-Thompson and Callan (2004).
Based on this assumption, we can accurately com-
pare the RLs of the translated CCHRC Chinese
medical documents to the RLs of the original En-
glish documents, which we call the ?true RLs? of
the testing documents.
LIX-based RLs can be roughly mapped to grade
levels, e.g., a text that is classified with an RL of
8 is appropriate for the average 8th grade reader.
Since we can assign RLs to the English versions
of the 65 CCHRC documents, these RLs can serve
as targets to match when generating RLs for the
corresponding Chinese versions of the same docu-
ments.
An advantage of our system arises from a com-
plete vertical integration which allows a user with
knowledge of the eventual goal to help shape the
development of the target language RL classifica-
tion model and software. In our case, the target
language (Chinese) test set was from the medical
domain, so we selected the OHSU87 medical ab-
stract corpus as an English data set. We automati-
cally classified the OHSU87 documents using the
LIX mapping schema assigned by the UNIX Dic-
tion and Style tools,3 given in the following Table.
LIX Index RL LIX Index RL
Under 34.0 4 48.0-50.9 9
34.0-37.9 5 51.0-53.9 10
38.0-40.9 6 54.0-56.9 11
41.0-43.9 7 57.0 and over 12
44.0-47.9 8
Table 1: Mapping of LIX Index scores to RLs as
assigned by Diction
Then, we concatenated the English OHSU87 doc-
uments in each RL group. The tf*idf formula was
used to select the K English words most represen-
tative of each RL group.
Next, we automatically selected a set of Chi-
nese words for each RL class to create a corre-
sponding Chinese readability model by passing
each English word through the CLIR system, EX-
CLAIM, to retrieve the most relevant English doc-
ument in the Wikipedia corpus, where relevance
is measured using the tf*idf vector space model.
The top M Chinese words from the corresponding
Chinese document in the parallel Wikipedia cor-
pus were added to RLT. By repeating this pro-
cess for each word of each RL class, the Chinese
readability model was constructed. In our exper-
iment, we set K = 50 and M = 10 arbitrarily.
The ECR system then automatically generated the
subsequent RL classification software for Chinese.
Finally, we assigned a RL to each document in
the test set. At this point the procedure is essen-
tially similar to document retrieval task. Each RL
group?s set of words RLT was treated as a docu-
ment (dj ), and each test document to be labeled
was treated as a query (q). RLs were ranked based
on the cosine similarity between RLT and q. Fi-
nally, the top-ranked RL was assigned to each test
document.
3Available online at http://www.gnu.org/software
/diction/diction.html.
15
5 Empirical Results
The results are presented below in Table 2. The
RL assigned to each Chinese document is com-
pared to the ?true RL? of the English document, on
the assumption that translation does not affect the
readability level. Although only 7.8% of the RLs
were predicted accurately (i.e., the highest ranked
RL for the Chinese document corresponded iden-
tically to the RL of the translated English docu-
ment), over 50% were either perfectly accurate or
off by only one RL.
Correctly predicted RL 7.8%
RL off by 1 grade level 43.1%
RL off by 2 grade levels 18.4%
RL off by 3 grade levels 18.4%
RL off by 4 grade levels 6.1%
RL off by 5 grade levels 3.1%
RL off by 6 grade levels 0%
RL off by 7 grade levels 3.1%
RL off by 8 grade levels 0%
Table 2: Distribution of RLs as predicted by our
ECR system
This table motivates us to represent the results
in a more comprehensive fashion. Intuitively, the
system tends to succeed at assigning RLs near the
correct level, though not necessarily at the exact
level. To quantify this intuition, we used Root
Mean Squared Error (RMSE) to evaluate the ex-
perimental results. We compared our results to
two kinds of baseline RL assignments. The first
method was to randomly assign RLs 1000 times
and take the average of the RMSE obtained in
each assignment; this yielded an average RMSE
of 3.05. The second method used a fixed equal
distribution of the nine RLs, applying each RL to
each document an equal number of times, and tak-
ing the average of these results. This baseline re-
turned an average RMSE of 3.65. The average
RMSE of our ECR system?s performance on the
CCHRC Chinese documents is 2.48. This number
compares favorably against both of the baseline al-
gorithms.
Recall that the actual RL-tagging procedure has
been treated as a document retrieval task, using
Vector Space Cosine similarity. As such, RLs are
not simply ?picked out? for each document: each
document receives a cosine similarity score for
each RL, calculated on the basis of its similarity to
the language model word set constructed for each
RL. For the results above, only the top ranked RL
was considered, as this would be the RL yielded if
the user wanted a discrete numeric value to assign
to the text. If we allow for enough flexibility to se-
lect the better of the two top-ranked RLs assigned
to each document by our ECR system, the results
are as given in Table 3.
Correctly predicted RL 10.8%
RL off by 1 grade level 49.2%
RL off by 2 grade levels 27.7%
RL off by 3 grade levels 7.7%
RL off by 4 grade levels 1.5%
RL off by 5 grade levels 0%
RL off by 6 grade levels 3.1%
RL off by 7 grade levels 0%
RL off by 8 grade levels 0%
Table 3: RL Distribution (Best of Two Top-
Ranked RLs)
While this extra selection is certain to improve
the RMSE, what is surprising is the extent to
which the RMSE improves. Once again, RMSE
can be calculated in the following way. The two
top-ranked RLs for each document are taken into
consideration, and of these two RLs, the RL near-
est to the true RL is selected. Selecting the best of
the two top-ranked RLs causes the RMSE to drop
to 1.91.
6 Related Work
The method described above builds on recent work
that has exploited the web and parallel corpora to
develop language technologies for minority lan-
guages (Trosterud (2002), inter alia).
Yarowsky et al (2001) describe a system and
a set of algorithms for automatically deriving au-
tonomous monolingual POS-taggers, base noun-
phrase bracketers, named-entity taggers, and mor-
phological analyzers for an arbitrary target lan-
guage. Bilingual text corpora are treated with
existing text analysis tools for English, and their
output is projected onto the target language via
statistically derived word alignments. Their ap-
proach is especially interesting insofar as the sys-
tem does not require hand-annotation of target-
language training data or virtually any target-
language-specific knowledge or resources.
Martin et al (2003) present an English-Inuktitut
aligned parallel corpus, demonstrating superior
16
sentence alignment via Pointwise Mutual Informa-
tion (PMI). Their approach provides broad cov-
erage of cross-linguistic morphology, which has
implications for dictionary expansion tasks; prob-
lems encountered in dealing with the agglutina-
tive morphology of Inuktitut are suggestive of the
myriad issues arising from cross-language com-
parisons.
Rogati et al (2003) present an unsupervised
learning approach to building an Arabic stemmer,
modeled on statistical machine translation. The
authors use an English stemmer and a small par-
allel corpus as training resources, with no parallel
text necessary after the training phase. Additional
monolingual texts can be incorporated to improve
the stemmer by allowing it to adapt to a specific
domain.
While Yarowsky et al (2001), Martin et
al. (2003) and Rogati et al (2003) all focus
on aligned parallel corpora, our approach dif-
fers in that we use comparable documents from
Wikipedia are linked thematically on the basis
of semantic content alone: there is no presumed
structural or lexical alignment between parallel
documents. We have adapted the methods used
in conjunction with aligned parallel corpora for
use with non-aligned parallel corpora to handle
the task pursued by Collins-Thompson and Callan
(2004), which presents a new approach to predict-
ing the RLs of a document by evaluating readabil-
ity in terms of statistical language modeling. Their
approach employs multiple language models to es-
timate the most likely RL for each document.
This approach contrasts with other previous
monolingual methods of calculating readability,
such as Chall and Dale (1995), which assesses
the readability of texts by calculating the percent-
age of terms that do not appear on a 3,000 word
list that 80% of tested fourth-grade students were
able to read. Similarly, Stenner et al (1988) use
the word frequency information from a 5-million-
word corpus.
While our work has drawn from several tech-
niques employed in prior research, we have mainly
hybridized the technique of using parallel cor-
pus employed by Yarowsky (2001) and the lan-
guage modeling approach employed by Collins-
Thompson and Callan (2004). Our approach relies
on parallel corpora to build a readability classi-
fier for one language based on readability software
for another language. Rather than focusing on
language-specific readability classification based
on training data drawn from the same language
as the testing data (Collins-Thompson and Callan,
2004), we have constructed a radically extensible
tool that can easily create readability classifiers
for an arbitrary target language using training data
from a source language such as English. The result
is a system capable of allowing a user to construct
readability software for languages like Indonesian,
for example, even if that user does not speak In-
donesian ? this is possible due to the large paral-
lel English-Indonesian corpus on Wikipedia.
7 Conclusion
We have proposed a general framework to quickly
construct a standalone readability classifier for
an arbitrary (and possibly unfamiliar) language
using statistical language models based both on
monolingual and non-aligned parallel corpora. To
demonstrate the proposed idea, we developed an
Extensible Crosslingual Readability system. We
evaluated the system on the task of predicting
readability level of a set of Chinese medical docu-
ments. The experimental results show that the pre-
dicted RLs were correct or nearly correct for over
50% of the documents. This research is important
because it is the only technique we are aware of
that is capable of straightforwardly creating read-
ability labels for hundreds, or theoretically even
thousands, of different languages.
Although the general framework and architec-
ture of the proposed system are straightforward,
the details of implementation of the system mod-
ules could be further improved to achieve bet-
ter performance. For example, all target lan-
guage words are selected from a single ?best-
matching document? using EXCLAIM in this pa-
per. Further experimentation might discover a
better word selection module. Future work may
also reveal delineation points for over- and under-
specialized sets of training data. The OHSU87
data set was selected on the basis of its medical
domain coverage, however it may not have pro-
vided broad enough coverage of the appropriate
domain-independent vocabulary in the CCHRC
documents. And finally, we conducted the ex-
periment using our own CLIR system, EXCLAIM,
while other CLIR systems might yield better re-
sults.
17
Acknowledgements
The research reported here was partly supported
by NSF Grant #BCS-0846979 and the Institute of
Education Sciences, US Department of Education,
through Grant R305A00596 to the University of
California, Santa Cruz. Any opinions, findings,
conclusions or recommendations expressed in this
paper are the authors?, and do not necessarily re-
flect those of the sponsors.
References
Jonathan Anderson. 1981. Analysing the readability
of English and non-English texts in the classroom
with Lix. Paper presented at the Annual Meeting of
the Australian Reading Association.
C. H. Bjo?rnsson and Birgit Ha?rd af Segerstad. 1979.
Lix pa? Franska och tio andra spra?k. Pedagogiskt
centrum, Stockholms skolfo?rvaltning.
Jeanne S. Chall and Edgar Dale. 1995. Readability Re-
visited: The New Dale-Chall Readability Formula.
Brookline, Cambridge, Mass.
Chinese Community Health Resource Center. 2004.
CCHRC Medical Documents. Retrieved Decem-
ber 9, 2006, fromhttp://www.cchphmo.com/
cchrchealth/index E.html.
Kevyn Collins-Thompson and Jamie Callan. 2004.
A language modeling approach to predicting read-
ing difficulty. In Proceedings of HLT/NAACL 2004.
ACL.
Joseph H. Greenberg. 1954. A quantitative approach
to the morphological typology of language. In
Method and Perspective in Anthropology: Papers
in Honor of Wilson D. Wallis, pages 192?220, Min-
neapolis. University of Minnesota Press.
Barbara Grimes. 2005. Ethnologue: Languages of the
World, 15th ed. Summer Institute of Linguistics.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2006. Classroom
success of an intelligent tutoring system for lexical
practice and reading comprehension. In Proceed-
ings of the Ninth International Conference on Spo-
ken Language Processing.
Charles N. Li and Sandra Thompson. 1981. Mandarin
Chinese: A Functional Reference Grammar. Uni-
versity of California Press.
Joel Martin, Howard Johnson, Benoit Farley, and Anna
Maclachlan. 2003. Aligning and using an English-
Inuktitut parallel corpus. In Proceedings of the HLT-
NAACL 2003 workshop on building and using par-
allel texts: Data driven machine translation and be-
yond. ACL.
John Paolillo, Daniel Pimienta, and Daniel Prado.
2005. Measuring Linguistic Diversity on the Inter-
net. UNESCO, France.
Monica Rogati, Scott McCarley, and Yiming Yang.
2003. Unsupervised learning of arabic stemming us-
ing a parallel corpus. In Proceedings of the 41st an-
nual meeting of the Association for Computational
Linguistics. ACL.
A.J. Stenner, I. Horabin, D.R. Smith, and M. Smith.
1988. The Lexile Framework. Metametrics,
Durham, NC.
Trond Trosterud. 2002. Parallel corpora as tools for
investigating and developing minority languages. In
Parallel corpora, parallel worlds, pages 111?122.
Rodopi.
Wikimedia Foundation. 1999. Wikipedia, the
free encyclopedia. Retrieved May 8, 2006, from
http://en.wikipedia.org/.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the First International Conference
on Human Language Technology Research, pages
161?168.
18
