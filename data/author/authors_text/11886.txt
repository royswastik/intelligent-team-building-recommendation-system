Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 111?119,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Exploring Miscommunication and Collaborative Behaviour in Hu-
man-Robot Interaction
Theodora Koulouri
Department of Information Systems and 
Computing
Brunel University
Middlesex UB8 3PH
theodora.koulouri@brunel.ac.uk
Stanislao Lauria
Department of Information Systems 
and Computing
Brunel University
Middlesex UB8 3PH
stasha.lauria@brunel.ac.uk
Abstract
This paper presents the first step in de-
signing a speech-enabled robot that is ca-
pable of natural management of mis-
communication. It describes the methods
and results of two WOz studies, in which 
dyads of na?ve participants interacted in a 
collaborative task. The first WOz study 
explored human miscommunication 
management. The second study investi-
gated how shared visual space and moni-
toring shape the processes of feedback 
and communication in task-oriented inte-
ractions. The results provide insights for
the development of human-inspired and
robust natural language interfaces in ro-
bots.
1 Introduction
Robots are now escaping laboratory and indus-
trial environments and moving into our homes 
and offices. Research activities have focused on 
offering richer and more intuitive interfaces, 
leading to the development of several practical 
systems with Natural Language Interfaces
(NLIs).  However, there are numerous open chal-
lenges arising from the nature of the medium 
itself as well as the unique characteristics of 
Human-Robot Interaction (HRI).
1.1 Miscommunication in Human-Robot 
Interaction
HRI involves embodied interaction, in which 
humans and robots coordinate their actions shar-
ing time and space. As most speech-enabled ro-
bots remain in the labs, people are generally un-
aware of what robots can understand and do re-
sulting in utterances that are out of the functional 
and linguistic domain of the robot.  Physical co-
presence will lead people to make strong but 
misplaced assumptions of mutual knowledge
(Clark, 1996), increasing the use of underspeci-
fied referents and deictic expressions. Robots 
operate in and manipulate the same environment 
as humans, so failure to prevent and rectify errors 
has potentially severe consequences. Finally, 
these issues are aggravated by unresolved chal-
lenges with automatic speech recognition (ASR) 
technologies. In conclusion, miscommunication 
in HRI grows in scope, frequency and costs, im-
pelling researchers to acknowledge the necessity 
to integrate miscommunication in the design 
process of speech-enabled robots.
1.2 Aims of study
The goal of this study is two-fold; first, to incor-
porate ?natural? and robust miscommunication 
management mechanisms (namely, prevention 
and repair) into a mobile personal robot, which is 
capable of learning by means of natural language 
instruction (Lauria et al, 2001). Secondly, it 
aims to offer some insights that are relevant for
the development of NLIs in HRI in general. This
research is largely motivated by models of hu-
man communication. It is situated within the lan-
guage-as-action tradition and its approach is to 
explore and build upon how humans manage
miscommunication.
2 Method
We designed and performed two rounds of Wiz-
ard of Oz (WOz) simulations. Given that the 
general aim of the study is to determine how ro-
bots should initiate repair and provide feedback 
in collaborative tasks, the simulations departed
from the typical WOz methodology in that the 
wizards were also naive participants. The domain 
of the task is navigation. In particular, the user 
111
guided the robot to six designated locations in a 
simulated town. The user had full access to the 
map whereas the wizard could only see the sur-
rounding area of the robot. Thus, the wizard re-
lied on the user?s instructions on how to reach 
the destination. In this section we outline the aim 
and approach of each WOz study, the materials 
used and the experimental procedure. Sections 4 
and 5 focus on each study individually and their 
results.
2.1 The first WOz study
This study is a continuation of previous work by 
the authors (Koulouri and Lauria, 2009). In that
study, the communicative resources of the wizard 
were incrementally restricted, from ?normal? 
dialogue capabilities towards the capabilities of a 
dialogue system, in three experimental condi-
tions:
 The wizard simulates a super-intelligent 
robot capable of using unconstrained, 
natural language with the user (henceforth,
Unconstrained Condition). 
 The wizard can select from a list of de-
fault responses but can also ask for clarifi-
cation or provide task-related information 
(henceforth, Semi-Constrained condition).
 The wizard is restricted to choose from a 
limited set of canned responses similar to 
a typical spoken dialogue system (SDS).
The current study investigates the first two con-
ditions and presents new findings.
2.2 The second WOz study
The second round of WOz experiments explored 
the effects of monitoring and shared visual in-
formation on the dialogue. 
2.3 Set-up
A custom Java-based system was developed and 
was designed to simulate the existing prototype 
(the mobile robot). The system consisted of two 
applications which sent and received coordinates 
and dialogue and were connected using the 
TCP/IP protocol over a LAN. The system kept a 
log of the interaction and the robot?s coordinates. 
The user?s interface displayed the full map of 
the town (Figure 1). The dialogue box was below 
the map. Similar to an instant messaging applica-
tion, the user could type his/her messages and see 
the robot?s responses appearing on the lower part 
of the box. In the first WOz study, the user?s in-
terface included a small ?monitor? on the upper 
right corner of the screen that displayed the cur-
rent surrounding area of the robot, but not the 
robot itself. Then, for the purposes of the second 
study, this feature was removed (see Figure 1 in 
Appendix A).
Figure 1. The user?s interface.
The wizard?s interface was modified accord-
ing to the two experimental conditions. For both
conditions, the wizard could only see a fraction 
of the map- the area around the robot?s current 
position. The robot was operated by the wizard 
using the arrow keys on the keyboard. The dialo-
gue box of the wizard displayed the most recent
messages of both participants as well as a history 
of the user?s messages. The buttons on the right 
side of the screen simulated the actual robot?s 
ability to remember previous routes: the wizard 
clicked on the button that corresponded to a 
known route and the robot automatically ex-
ecuted. In the interface for the Unconstrained 
condition, the wizard could freely type and send 
messages (Figure 2).
Figure 2. The wizard?s interface in the Uncon-
strained condition.
In the version for the Semi-Constrained condi-
tion, the wizard could interact with the user in 
two ways: first, they could click on the buttons,
situated on the upper part of the dialogue box, to 
automatically send the canned responses, ?Hel-
112
lo?, ?Goodbye?, ?Yes?, ?No?, ?Ok? and the 
problem-signalling responses, ?What??, ?I don?t 
understand? and ?I cannot do that?. The second 
way was to click on the ?Robot Asks Question? 
and ?Robot Gives Info? buttons which allowed 
the wizard to type his/her own responses (see 
Figure 2 in Appendix A).
2.4 Procedure
A total of 32 participants were recruited, 16 users 
and 16 wizards. The participants were randomly 
assigned to the studies, experimental conditions 
and to the roles of wizard or user. The pairs were 
seated in different rooms equipped with a desk-
top PC. The wizards were given a short demon-
stration and a trial period to familiarise with the 
operation of the system and were also informed 
about whether the users would be able to monitor 
them. The users were told that they would inte-
ract with a robot via a computer interface; this 
robot was very fluent in understanding spatial 
language and could give appropriate responses, it 
could learn routes but had limited vision. The 
users were asked to begin each task whenever 
they felt ready by clicking on the links on their 
computer screen, start the interaction with ?Hel-
lo?, which opened the wizard?s application, and 
end it with ?Goodbye? which closed both appli-
cations. The participants received verbal and 
written descriptions of the experiment. They 
were not given any specific guidelines on how to 
interact or what routes to take. However, the us-
ers were asked not to use directions such as 
?north?, ?south?, ?up?, ?down? etc. The wizards 
were also advised not to try to ?sound like a ro-
bot?, but respond normally. 
3 Data analysis
The dialogues collected were annotated on the 
basis of dialogue acts (DAs), miscommunication, 
and task status. An example of an annotated di-
alogue is shown in Table 1.
ID (x,y@TIME)  
MESSAGE 
T.S. MISC HCRC TAG  
U1 (899,445@10:44:34)  
go  straight on. 
  instruct  
R2 (880,286@10:44:59)  
There is a wall straight on. 
 Rej explain Imp 
U3 (884,286@10:45:28)  
ok turn left here then take the 
third right 
  acknowledge/ 
instruct 
 
R4 (352,216@10:45:58)  
Before the bridge? 
 Non check Imp 
U5 (351,68@10:46:17)  
this is the wrong place 
WE Corr explain  
R6 (351,68@10:46:39)  
Wrong place. Should Robot go 
  acknowledge/ 
query-yn 
 
back? 
U7 (351,68@10:46:55)  
go forward and at the cros-
sroads keep going forward and 
the tube is at the end of the 
road 
  instruct  
R8 (351,0@10:47:14) 
 Out of bounds. 
WE  explain  
R9 (351,608@10:47:47)  
Where to go? 
   query-w   
U10 (364,608@10:48:12)  
the tube is in front of you 
  instruct  
R11 (402,547@10:48:23)  
Is it this one? 
BOT  query-yn  
U12 (402,547@10:49:7)  
yes it is. 
SUC  reply-y  
Table 1. Example of an annotated dialogue. ID 
denotes the speaker (User or Robot), T.S. stands
for task status and MISC for miscommunication.
3.1 Annotation of dialogue acts
The DAs in the corpus were annotated following 
the HCRC coding scheme (Carletta et al, 1996).
Motivated by Skantze (2005), the last column in 
Table 1 contains information on the explicitness 
of the response. This feature was only relevant 
for repair initiations by the wizards. For instance, 
responses like ?What?? and the the ones in Table 
3 were considered to be explicit (EX) signals of 
miscommunication, whereas lines 2 and 4 in the 
dialogue above were labelled as implicit (IMP).
3.2 Annotation of task execution status
The coordinates (x,y) of the robot?s position re-
corded for every exchanged message were placed 
on the map of the town (of dimensions 1024x600
pixels) allowing the analysts to retrace the 
movements of the robot. Wrong executions (WE) 
were determined by juxtaposing the user?s in-
struction with the robot?s execution, as indicated
by the coordinates. Back-on-Track (BOT) was 
tagged when the first user instruction after a 
wrong execution was executed correctly. Finally,
task success (SUC) was labelled when the robot 
reached the destination and it was confirmed by 
the user. 
3.3 Annotation of miscommunication
The annotation of instances of miscommunica-
tion in the dialogues is based on the definitions 
given by Hirst et al (1994). Miscommunication 
includes three categories of problems: misunder-
standings, non-understandings and misconcep-
tions. First, misunderstandings occur when the 
hearer obtains an interpretation which is not 
aligned to what the speaker intended him/her to 
obtain. In this study, without attempting to unveil 
the intention of the user, misunderstandings were 
113
tagged when the user (who was monitoring the 
understanding) signalled a wrong execution (see 
line 5 in Table 1). These correction tags (Corr) 
did not always coincide with wrong execution
tags, but were used when the user became aware 
of the error (after receiving visual or verbal in-
formation). Following the same definition, mis-
understandings were also tagged as rejections 
(tag: Rej) when the wizard expressed inability to 
execute the instruction (for instance, given the 
robot?s current location, as shown in line 2 in the 
dialogue), although he/she was able to interpret 
it. Secondly, non-understandings (tag: Non, line 
4) occurred when the wizards obtained no inter-
pretation at all or too many. Non-understandings 
also included cases in which wizards were uncer-
tain about their interpretation (as suggested by 
Gabsdil, 2003). Lastly, misconceptions happen 
when the beliefs of the interlocutors clash, and 
are outside the scope of this study.
4 First WOz study
Skantze (2005) and Williams and Young (2004) 
performed variations of WOz studies to explore 
how humans handle ASR errors, using a real or 
simulated speech recogniser. They discovered 
that even after highly inaccurate recognition out-
put, the participants rarely signalled non-
understanding explicitly. Accordingly, the expe-
rimental hypothesis of the present study is that 
wizards in both conditions will not choose expli-
cit responses to signal miscommunication (such 
as ?I don?t understand? or ?What??) but res-
ponses that contribute with information.
ASR is a major source of errors in SDS. But as 
miscommunication is ubiquitous in interaction, 
there are many other sources of ambiguity that 
give rise to problematic understanding. Thus, for 
the current purposes of this work, it was decided 
that ASR would have an overwhelming effect on 
the interaction that might prevent the observation 
of other interesting dialogue phenomena. 
This section describes further work on the Un-
constrained and Semi-Constrained conditions 
(see Section 2.1). Twenty participants were re-
cruited and randomly allocated to each condition.
4.1 Results 
Analysis of the dialogues of the Unconstrained 
condition reinforced previous findings and con-
firmed the experimental hypothesis. In particular, 
wizards never used explicit repairs, but preferred 
to describe their location, request clarification 
and further instructions. Integrating finer classi-
fication of clarification requests (CRs) and the 
original dialogue act tagging, the DAs used by 
the wizards to signal non-understandings and 
rejections were categorised as shown in Table 2.
Dialogue Act Explanation 
Explain The wizard gives description of robot?s location. 
E.g., ?I crossed the bridge.?, ?I am at a cross-
road?. 
Check This category covers CRs. The corpus contained 
two types of CRs: first, task-level reformulations 
(as in line 4 in Table 1), which reformulate the 
utterance on the basis of its effects on the task, 
showing the wizard?s subjective understanding 
(Gabsdil, 2003). Second, alternative CRs which 
occur when the wizard gives two alternative 
interpretations, trying to resolve referential 
ambiguity. For instance, ?back to the bridge or 
to the factory?, to resolve ?go back to last loca-
tion?.  
Query-w The wizard asks for further instructions. E.g., 
?Please give me further instructions.? 
Explain+Query-
w 
A combo of actions; the wizard provides infor-
mation on location and asks for further instruc-
tions. E.g., ?crossroads, now where?? 
Table 2.Wizard DAs after miscommunication.
Figure 3 illustrates the distribution of these re-
sponses to signal non-understandings and rejec-
tions (columns labelled ?Uncons-NON? and 
?Uncons-REJ?, respectively). Evidently, there is 
a much greater variety of CRs than the two CR 
types reported here, as described in the work of 
Purver (2006) and Schlangen (2004). However, 
for a navigation task and having excluded ASR 
errors, problems occurred mainly in the meaning 
recognition level (explained below) and aimed 
for reference resolution.
Figure 3. Use of strategies to signal non-
understandings or rejections, for either condition.
In conclusion, wizards in the Unconstrained 
condition did not directly signal problems in un-
derstanding but, instead, they attempted to ad-
vance the dialogue by providing task-related in-
formation in either the form of CRs or simple 
statements. The study contributes to the findings 
presented in Skantze (2005) and Williams and 
Young (2004) in that it demonstrates the use of 
similar strategies to deal with different sources of 
problems.
114
In the Semi-Constrained condition, a degree of 
restrain and control over the error handling ca-
pacity of the wizards was introduced. In particu-
lar, the wizards could explicitly signal communi-
cation problems in the utterance, meaning and 
action level using three predefined responses.
This is inspired by the models of Clark (1996)
and Allwood (1995), according to which, mis-
communication can occur in any of these levels 
and people select repair initiations that point to 
the source of the problem. The model (adapted 
from Mills and Healey, 2006) and the responses 
are schematically shown in Table 3 below.
Levels of Communication Wizard Responses 
Level 1 Securing Attention - 
Level 2 Utterance Recognition ?What?? 
Level 3 Meaning Recognition ?Sorry, I don?t understand.? 
Level 4 Action Recognition ?I cannot do that.? 
Table 3. Levels of communication.
Moreover, based on the classification of the 
wizard?s error handling strategies in the Uncon-
strained condition (Table 2), we collapsed the 
observed strategies in two categories of re-
sponses which resulted in adding two more error 
handling buttons; namely, the button denoted as 
?Robot Asks Question? corresponded to the 
?Check? and ?Query-w? strategies. The ?Robot 
Gives Info? was associated with ?Explain?. This 
clear labelling of error handling actions pre-
sented to the wizards of the Semi-Constrained 
condition aimed to ?coerce? them to use the 
strategies in a more transparent way. This could 
allow us a glimpse to the mechanisms and proc-
esses underlying human miscommunication 
management.
Analysis of the dialogues revealed that in the 
Semi-Constrained condition wizards employed
both explicit and implicit strategies. Figure 4
shows the distribution of explicit and implicit 
responses to signal non-understandings and re-
jections. Figure 3 shows the frequency of each
implicit strategy to signal non-understandings
(Semi-NON) and rejections (Semi-REJ).
The initial prediction was that wizards will not 
use explicit signals of problems in the dialogue. 
This was contradicted by the results. It can be 
argued that the physical presence of the buttons 
and the less effort required account for this phe-
nomenon. On the other hand, it is also plausible 
to assume that these strategies matched what the
wizards wanted to say. Finally, there were no 
significant differences between conditions in 
terms of user experience, task success and time 
on task (as reported in Koulouri and Lauria, 
2009).
Figure 4. Occurrence of implicit and explicit 
miscommunication signals (Semi-Constrained).
4.2 Discussion and future work
The findings of this study could be extrapolated 
to HRI. Classification of the responses of the 
wizards resulted in a limited set of error signal-
ling strategies. Therefore, in the presence of mis-
communication the robot could use the static, 
explicit strategies. But these strategies alone are 
inadequate (as shown by Koulouri and Lauria, 
2009). They need to be supplemented, but not 
entirely replaced, with dynamic error handling 
strategies; namely, posing relevant questions and 
providing descriptions of location. Yet this en-
tails several challenges. Gabsdil (2003) identifies 
the complexity of adding clarification requests to 
systems with deep semantic processing. With 
regard to alternative clarifications, systems 
would need to generate two alternative interpre-
tations for one referent. Task-level reformula-
tions would also require the system to have the 
capability to identify the effects of all possible 
executions of the instruction. As a next step, we 
will focus on issues concerning the implementa-
tion of such functionality.
Schlangen (2004) suggests that ?general-
purpose? repair initiations, such as ?What??,
which request repetition of the whole utterance,
are more severe for the dialogue compared to 
reprise fragments (e.g., ?Turn where??) that ac-
cept part of the utterance. Mills and Healey 
(2006) also found that ?What?s? were more dis-
ruptive to the dialogue than reprise fragments.
Guided by these insights, our current work looks 
at how each error strategy affects the subsequent 
unfolding of the dialogue.
5 The second WOz study
Research in human communication has shown 
that in task-oriented interactions visual informa-
tion has a great impact on dialogue patterns and 
improves performance in the task. In particular, 
Gergle at al. (2004), Clark and Krych (2004) and 
115
Brennan (2005) explored different communica-
tion tasks and compared a condition, in which 
visual and verbal information was available, with
a speech-only condition. In their experiments, a 
person gave instructions to another participant on 
how to complete a task. Their findings seem to 
resonate. In terms of time for task completion 
and number of words per turn, the interactions in 
the visual information condition were more effi-
cient. The physical actions of the person follow-
ing the instructions functioned as confirmations 
and substituted for verbal grounding. Regarding
errors, no significant differences were observed 
between visual and speech-only conditions. Mo-
tivated by these findings in human-human inte-
raction, the second study aims to identify the dif-
ferences in the processes of communication de-
pending on whether the user can or cannot moni-
tor the actions of the robot. 
5.1 Experimental design
The study followed a between-subjects factorial 
design. Experiments were performed for four 
different conditions, as illustrated in Table 4. The 
conditions ?Monitor, Unconstrained? and ?Moni-
tor, Semi-Constrained? were the same as in the 
first study. Five pairs of participants were re-
cruited to each of the Monitor Conditions and 
three pairs to each of the No Monitor Conditions. 
Unconstrained Semi-Constrained 
Monitor 
Monitor, Uncon-
strained 
Monitor, Semi-
Constrained 
No Monitor 
No Monitor, Uncon-
strained 
No Monitor, Semi-
Constrained 
Table 4. The design of the 2nd study.
5.2 Results
The data collection resulted in 96 dialogues, 93 
of which were used in the analysis. The data 
were analysed using a two-way ANOVA. All 
effects that were found to be significant were 
verified by T-tests. The efficiency of interaction 
was determined using the following measures: 
time per task, number of turns, words, miscom-
munication-tagged turns, wrong executions and 
task success.
Time per task: The second column of Table 5
displays the average completion time per task in
the four conditions. As expected, a main effect of 
the Monitor factor was found (F=4.879, df=1,11, 
p<0.05). Namely, when the user could monitor 
the robot?s area the routes were completed faster. 
The interaction effect between factors was also 
marginally significant (F=4.225, df=1,11, p<0.1); 
pairs in the No Monitor, Semi-Constrained con-
dition could not compensate for the lack of visual 
information and took longer for each task. 
Number of turns and words: The aforemen-
tioned studies correlate task efficiency with 
number of turns and words. In terms of the mean 
number of turns per interaction, no significant 
differences were found across the groups.   Nev-
ertheless, we measured the number of words 
used per task and in accordance with previous 
research, we observed that pairs in the No Moni-
tor conditions used more words (F=4.602, 
df=1,11, p=0.05). However, it was the wizards 
under the No Monitor conditions that had to be 
more ?talkative? and descriptive (F=10.324, 
df=1,11, p<0.01).  Figure 5 shows the ?word-
possession? rates attributed to wizards in the four 
conditions. Moreover, there seems to be a differ-
ence (F=4.397, df=1,11, p=0.05) in the mean 
number of words per turn. In particular, when the 
wizards? actions were visible to the users, the 
wizards required fewer words per turn. There is 
also an interaction effect showing more signifi-
cant differences between the Monitor, Semi-
Constrained condition and the No Monitor, 
Semi-Constrained condition (F=5.970, df=1,11, 
p<0.05); in the former, wizards managed with 
less than 2 words per utterance, taking full ad-
vantage of the luxury of the buttons and the fact 
that they were supervised. In the latter, wizards
used more than 6 words per turn.
Figure 5. Words used by wizards over total.
Frequency of miscommunication: We meas-
ured the number of turns that were tagged as con-
taining miscommunication. Surprisingly, mis-
communication rates were much lower in the No 
Monitor conditions (F=13.316, df=1,11, p<0.01) 
and not in the conditions in which the user could 
check at all times the actions and understanding 
of the robot. The same pattern was found for us-
er-initiated and robot-initiated miscommunica-
tion. The rates of miscommunication are in-
cluded in the third column of Table 5.
Wrong executions: Analysis of number of
wrong executions per task reveals a similar ef-
fect; wrong executions occurred much less fre-
quently when the wizards were not supervised by 
116
the users (F=6.046, df=1,11, p<0.05).  They 
made on average 1 mistake per task, whereas the 
average number of wrong executions for the 
pairs in the Monitor conditions was 5 (fourth 
column in Table 5).
Task success rates: There were no differences 
in the number of interrupted or aborted tasks.
Condition 
Time per 
Task 
(min) 
Miscommunication 
Turns/Total Turns 
#Wrong 
Executions 
per Task 
Mon, Uncons 4.57 8.21% 4.2 
Mon, Semi 4.63 8.82% 5.8 
No Mon, Uncons 5.67 2.55% 1.0 
No Mon, Semi 7.41 1.71% 0.7 
Table 5. Summary of results (mean values).
5.3 Discussion and future work
These results are consistent with previous re-
search.  The conditions in which the user could 
see exactly what the robot saw and did resulted 
in faster task completion and shorter dialogues. 
However, a finding emerged which was not ex-
pected based on the aforementioned studies: in 
the conditions in which users could not monitor 
the robot?s actions, the wizards were more accu-
rate, leading to low occurrence of wrong execu-
tions and miscommunication (see column 3 and 4 
in Table 5). The ?least collaborative effort? is 
balanced and compromised against the need to 
ensure understanding. Thus, wizards provided 
rich and timely feedback to the users in order to 
compensate for the lack of visual information.
This feedback acted in a proactive way and pre-
vented miscommunication and wrong executions. 
In the Monitor conditions, asymmetries in per-
ceived responsibility and knowledge between the 
participants could have encouraged wizards to be 
less cautious to act. In other words, as the user 
had access to the full map and the location of the 
wizard, the wizard felt less ?obliged? to contri-
bute to the interaction. However, due to the com-
plex nature of the task, unless the wizard could 
sufficiently communicate the relevant position of 
the robot, the directions of the user would more 
likely be incorrect.  It could also be assumed that 
since visual feedback is instant, the users were 
also more inclined to issue commands in a ?trial 
and error? process. Irrespectively to the underly-
ing motives, these findings show that despite 
higher costs in time and word count, linguistic 
resources were adequate for completing complex 
tasks successfully. The findings also resonate
with the collaborative view of communication. 
The wizards adapted their behaviour in response 
to variations in the knowledge state of their part-
ners and made up for the lack of visual informa-
tion with rich verbal descriptions of their loca-
tions.
We are currently performing more experi-
ments to balance the data sets of the study and 
validate the initial results. Moreover, a fine-
grained analysis of the dialogues is under way 
and focuses on the linguistic content of the inte-
ractions. The aim is identical to the first WOz 
study, that is, to identify the strategies of the wi-
zards in the presence and absence of visual in-
formation.
These results have important implications for 
HRI. As in human collaborative interaction, the 
robot?s communicative actions have direct im-
pact on the actions of the users. In real-world 
settings, there will be situations in which the us-
ers cannot monitor the robot?s activities or their 
information and knowledge are either con-
strained or outdated. Robots that can dynamically 
determine and provide appropriate feedback 
could help the users avoid serious errors. Never-
theless, this is not a straightforward process; pro-
viding excessive, untimely feedback compromis-
es the ?naturalness? and efficiency of the interac-
tion. The amount and placement of feedback 
should be decided upon several knowledge 
sources, combined in a single criterion that is 
adaptive within and between interactions. These 
issues are the object of our future work and im-
plementation. 
6 Concluding remarks
One of the most valuable but complex processes
in the design of a NLI for a robot is enacting a 
HRI scenario to obtain naturally-occurring data 
which is yet generalisable and relevant for the 
future implementation of the system. The present 
study recreated a navigation scenario in which 
non-experienced users interacted with and taught
a mobile robot. It also simulated two different 
setups which corresponded to the realistic situa-
tions of supervised and unsupervised interaction.
The current trend in the fields of linguistics and
robotics is the unified investigation of spatial 
language and dialogue (Coventry et al, 2009). 
Exploring dialogue-based navigation of a robot, 
our study aimed to contribute to this body of re-
search. It can be argued that there were limita-
tions in the simulation as compared to the expe-
rimental testing of a real system and, thus, the 
study was primarily explorative. However, it 
yielded natural dialogues given that naive ?con-
federates? and no dialogue script were used. The 
data analysis was more qualitative than quantita-
117
tive and followed established methods from pre-
vious research. Finally, the results of the study
matched and extended these findings and pro-
vided useful information for the next version of 
the system as well as some insight into the 
processes of conversation and social psychology.
The next step in our research is to develop the 
dialogue manager of the robot to incorporate the 
feedback and miscommunication management 
strategies, as observed in the collected data. This 
holds the promise for a robust NLI that can han-
dle uncertainties arising from language and the 
environment. However, miscommunication in 
HRI reaches beyond preventing and repairing
recognition errors. Mills and Healey (2008) 
demonstrate that miscommunication does not 
inhibit but, on the contrary, it facilitates semantic 
coordination. Martinovsky and Traum (2003) 
suggest that through miscommunication, people 
gain awareness of the state and capabilities of 
each other. Miscommunication, thus, is seen as 
an opportunity for communication. Under this 
light, natural miscommunication management is 
not only the end, but also the means to shape and 
advance HRI, so that robots are not tools but 
partners that play a positive, practical and long-
lasting role in human life.
References
Bilyana Martinovsky and David Traum. 2003.The 
Error Is the Clue: Breakdown in Human-Machine 
Interaction.  In Proceedings of the ISCA Workshop 
on Error Handling in Dialogue Systems.
Dan Bohus and Alexander I. Rudnicky. 2005. Sorry, I 
Didn?t Catch That! ? An Investigation of Non-
understanding Errors and Recovery Strategies. In 
Proceedings of SIGdial2005. Lisbon, Portugal.
Darren Gergle, Robert E. Kraut and Susan E. Fussell. 
2004.  Language Efficiency and Visual Technolo-
gy: Minimizing Collaborative Effort with Visual 
Information. Journal of Language and Social Psy-
chology, 23(4):491-517. Sage Publications, CA. 
David Schlangen. 2004. Causes and Strategies for 
Requesting Clarification in dialogue. In  Proceed-
ings of the 5th Workshop of the ACL SIG on Dis-
course and Dialogue (SIGdial04), Boston, USA.
Gabriel Skantze. 2005. Exploring Human Error Re-
covery Strategies: Implications for Spoken Dialo-
gue Systems. Speech Communication, 45(3):207-
359.
Graeme Hirst, Susan McRoy, Peter Heeman, Philip 
Edmonds, Diane Horton.1994. Repairing Conver-
sational Misunderstandings and Nonunderstand-
ings. Speech Communication 15:213?230.
Gregory Mills and Patrick G. T. Healey. 2008. Nego-
tiation in Dialogue: Mechanisms of Alignment. In 
Proceedings of the 8th SIGdial workshop on Dis-
course and Dialogue, Columbus, OH, USA.
Gregory Mills and Patrick G. T. Healey. 2006. Clari-
fying Spatial Descriptions: Local and Global Ef-
fects on Semantic Co-ordination. In Procs. of the
10th Workshop on the Semantics and Pragmatics of 
Dialogue.
Herbert H. Clark. 1996. Using Language. Cambridge 
University Press, Cambridge, UK.
Herbert H. Clark and Meredyth A.  Krych. 2004. 
Speaking While Monitoring Addressees for Under-
standing. Journal of Memory and Language,
50:62-81.
Jason D. Williams and Steve Young. 2004. Characte-
rizing Task-Oriented Dialog Using a Simulated 
ASR Channel. ICSLP. Jeju, South Korea.
Jean Carletta, Amy Isard, Stephen Isard, Jacqueline
Kowtko, Gwyneth Doherty-Sneddon and Anne H. 
Anderson.1996. HCRC Dialogue Structure Coding
Manual(HCRC/TR-82). Human Communication 
Research Centre, University of Edinburgh.
Jens Allwood. 1995. An Activity based Approach to 
Pragmatics. Gothenburg Papers in Theoretical Lin-
guistics, 76, G?teborg University, Sweden.
Kenny Coventry, Thora Tenbrink and John Bateman, 
2009. Spatial Language and Dialogue: Navigating 
the Domain. In K. Coventry, T. Tenbrink, and J. 
Bateman (Eds.) Spatial Language and Dialogue. 1-
8. Oxford University Press. Oxford, UK.
Malte Gabsdil. 2003. Clarification in Spoken Dialo-
gue Systems. In: Proceedings of 2003 AAAI Spring 
Symposium on Natural Language Generation in 
Spoken and Written Dialogue, Stanford, USA.  
Matthew Purver. 2006. CLARIE: Handling Clarifica-
tion Requests in a Dialogue System.  Research on 
Language and Computation . 4(2-3):259-288.
Stanislao Lauria, Guido Bugmann, Theocharis Kyria-
cou, Johan  Bos and Ewan Klein. 2001. Training 
Personal Robots Using Natural Language Instruc-
tion. IEEE Intelligent Systems. 38?45.
Susan E. Brennan . 2005. How Conversation is 
Shaped by Visual and Spoken Evidence. In J. 
Trueswell & M. Tanenhaus (Eds.) Approaches to 
Studying World-situated Language Use: Bridging 
the Language-as-product and Language-action 
Traditions. 95-129. MIT Press, Cambridge, MA.
Theodora Koulouri and Stanislao Lauria. 2009. A 
WOz Framework for Exploring Miscommunication 
in HRI, In Procs. of the AISB Symposium on New 
Frontiers in Human-Robot Interaction. Edinburgh, 
UK.
118
Appendix A. Screenshot images of the in-
terface
Figure 1. The interface of the user without the 
monitor (as used in the second WOz study).
Figure 2. The interface of the wizard in the Semi-
Constrained condition.
119
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 95?98,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Route Communication in Dialogue: a Matter of Principles 
 
 
Theodora Koulouri 
Department of Information Systems  
and Computing 
Brunel University 
Middlesex UB8 3PH 
theodora.koulouri@brunel.ac.uk 
Stanislao Lauria 
Department of Information Systems 
and Computing 
Brunel University 
Middlesex UB8 3PH 
stasha.lauria@brunel.ac.uk 
 
  
Abstract 
The present study uses the dialogue paradigm 
to explore route communication. It revolves 
around the analysis of a corpus of route in-
structions produced in real-time interaction 
with the follower. It explores the variation in 
forming route instructions and the factors that 
contribute in it. The results show that visual 
co-presence influences the performance, con-
versation patterns and configuration of instruc-
tions. Most importantly, the results suggest an 
analogy between the choices of instruction-
givers and the communicative actions of their 
partners. 
1.1 Spatial language in dialogue 
  The main question this paper attempts to ad-
dress is how people produce route instructions in 
dialogue. The current zeitgeist in language re-
search and dialogue system development seems 
to be the unified investigation of spatial language 
and dialogue (Coventry et al, 2009). Indicative 
of the growing prioritisation of dialogue in the 
study of spatial language are the on-going re-
search efforts within the MapTask1 project and 
the GIVE challenge2 .  
1.2 A framework for the analysis of route 
instructions 
The study uses CORK (Communication of Route 
Knowledge, (Allen 2000)), a framework which 
provides a component-based analysis of route 
instructions. The CORK taxonomy differentiates 
between instructions that are directives (action 
statements with verbs of movement) and descrip-
tive statements (with state-of-being verbs, like 
?be? and ?see?). Descriptives present a static pic-
                                                 
1 http://www.hcrc.ed.ac.uk/maptask/ 
2 http://www.give-challenge.org/research/ 
ture of spatial relations and provide the follower 
the opportunity to verify his position or reorient 
himself. The taxonomy also considers elements 
that provide specificity and distinguishing infor-
mation about environmental features, called de-
limiters. Within this framework, Allen (2000) 
describes a set of principles pertaining to the 
configuration of route descriptions. Namely, 
people concentrate descriptives and delimiters on 
points along the route that offer for uncertainty 
(like crossroads). Moreover, the selection and 
placement of these components depends on the 
characteristics of the environment and the per-
ceived needs of the follower. Evidence from em-
pirical work supports the framework, reporting 
that errors in navigation increased when the route 
directions violated these principles. Nevertheless, 
the applicability of the suggested principles has 
only been tested in scenarios in which the direc-
tions were produced beforehand by either the 
experimenters or a separate group of subjects. 
1.3 The effect of visual information  
Studies exploring the effect of visual information 
on task-oriented interaction converge on that vis-
ual feedback leads to more efficient interactions 
and influences the conversational patterns be-
tween participants (Clark and Krych, 2004; Ger-
gle et al, 2004; Koulouri and Lauria, 2009). 
These phenomena are generally attributed to the 
ease of establishing ?common ground? when 
visual feedback is available. However, to the au-
thors? knowledge, most related studies have fo-
cused on high-level analysis of dialogue acts and 
many aspects of how interlocutors adapt their 
linguistic choices remain undefined.  
1.4 Aims and hypotheses of study 
The present study provides an empirical account 
of route instructions, as they emerge in real-time 
interaction with the follower. We offer the fol-
95
lowing tentative hypotheses. Since visual co-
presence facilitates grounding of information, it 
is expected to have a major effect on how route 
instructions are configured. Next, putting addi-
tional emphasis on the inter-individual processes 
involved in language use, this study aims to test 
whether the linguistic options mobilised by the 
instructor ultimately depend upon the contribu-
tions of the follower. 
2 Methods 
A study was designed to elicit natural route in-
structions in a restricted context. Pairs of partici-
pants collaborated in a navigation task, in a ?Wi-
zard-of-Oz? set-up. The instructors provided in-
structions to navigate their partners to designated 
locations in a simulated town, being under the 
impression that they were interacting with a 
software agent (robot). The study manipulated 
two factors; i) availability of visual information 
on follower?s actions and ii), follower?s interac-
tive capacity. With regard to the first factor, there 
were two conditions in which the ability to moni-
tor the actions of the ?robot? was either removed 
or provided. The second factor also involved two 
conditions. In the first condition, the followers 
could interact using unconstrained language 
(henceforth, ?Free? condition). In the second 
condition (henceforth, ?Constrained? condition), 
a set of predetermined responses available to the 
followers aimed to coerce them towards more 
?automated? contributions; for instance, ?open-
ended? repairs such as ?What??, which provide 
no specific information on the source of the 
problem. However, the followers were still able 
to be interactive if they wished so, by clicking 
the relevant buttons to request clarification or 
provide location information.  
The study followed a between-subjects fac-
torial design. A total of 56 students were allo-
cated in the four conditions: Monitor-Free, 
Monitor-Constrained, No Monitor-Free, No 
Monitor-Constrained. The experimental proce-
dure is described in detail in (Koulouri and 
Lauria, 2009). 
2.1 Set-up 
The experiment relied on a custom-built sys-
tem which supported the interactive simulation 
and enabled real-time direct text communication 
between the pairs. The interfaces consisted of a 
graphical display and a dialogue box.  
The interface of the instructor displayed the 
full map of the simulated town (Figure 1). On the 
upper right corner of the interface, there could be 
a small ?monitor?, in which the robot?s imme-
diate locality was displayed, but not the robot 
itself. The presence of the monitor feature de-
pended on the experimental condition.   
The followers? interface displayed a fraction 
of the map, the surroundings of the robot?s cur-
rent position. The robot was operated by the fol-
lower using the arrow keys on the keyboard. In 
the ?Free? conditions followers could freely type 
messages. In the ?Constrained? conditions, the 
followers needed to use the buttons on the inter-
face (Figure 2). 
 
Figure 1. The instructor?s interface in the Monitor 
conditions. The monitor window on the upper right 
corner was removed in No Monitor conditions. 
 
Figure 2. The follower?s interface in the Constrained 
conditions. In the Free conditions, there were no but-
tons and followers could freely type any message. 
2.2 Data analysis  
The analysis of the corpus of route instructions 
followed the CORK framework (Allen, 2000). 
Communicative statements were classified as 
Directives or Descriptives. These communica-
tive statements could contain references to envi-
ronmental features. The types of environmental 
features considered were: Locations (e.g., build-
ings or bridges), Pathways (e.g., streets), Choice 
Points (e.g., junctions) and Destination. Last, 
instructions can be composed of delimiters, 
which fall into four categories: 
1. Distance designations: e.g., ?...until you 
see a car park?. 
2. Direction designations: e.g., ?go left?. 
3. Relational terms: e.g., ?on your left?,  
4. Modifiers: e.g., ?big red bridge?, ?take 
the first/second/last road?. 
96
3 Results  
The experiment yielded a large corpus of 160 
dialogues, composed of 3,386 turns. 1,485 in-
structions were collected. First, the analysis con-
siders some common measures of efficiency. 
Next, the results of the component analysis of 
instructions are presented.  
3.1 Efficiency of interaction 
The number and length of turns and instructions 
and time needed to complete each task are typi-
cally used as measures of the efficiency of inter-
action. Additionally, fewer execution and under-
standing failures are taken as indicators of supe-
rior performance. 
Time, number of turns, words and instruc-
tions: The ANOVA performed on time per task 
showed no reliable significant differences among 
groups. On the other hand, significant effects 
were observed with regard to all other dependent 
variables. An interaction effect was revealed af-
ter analysis on numbers of turns (F(1, 24) = 
3.993, p = .05). Pairs in the Monitor-Free condi-
tion required less turns to complete the task 
compared to the other groups (column 1 of Table 
1). It seems however that instructors in both 
Monitor conditions were dominating the conver-
sational floor, having produced about 58% of the 
turns, compared to instructors in the No Monitor 
conditions (F(1, 24) = 5.303, p = .03). Neverthe-
less, it was not the case that instructors in Moni-
tor conditions were ?wordier?. The number of 
words was similar among all instructor groups. 
The results indicated that the total number of 
words required to complete a task was much 
lower in Monitor conditions (F(1, 24) = = 5.215, 
p = 0.03) (see column 3 in Table 1). Next, in-
structors in Monitor conditions gave more in-
structions to guide the followers to the destina-
tion (F(1, 24) = 3.494, p = .07).  However, these 
instructions were considerably shorter compared 
to the instructions provided by No Monitor in-
structors (F(1, 24) = 4.268, p = .05). All differ-
ences are amplified in the Monitor-Constrained 
group, in which more turns and instructions were 
needed but with fewer words and the ?turn pos-
session? of the instructor was the highest among 
the groups.  
Con-
dition 
#Turns 
per 
task 
%In-
struc-
tor 
Turns 
#Word
s per 
task 
#Words 
per 
Instruc-
tion 
#Instruc-
tions per 
task 
Miscom-
munica-
tion per 
task 
M-F 16.74 57.12% 87.33 4.70 9.08 1.14 
M-C 23.95 58.86% 65.02 3.01 11.73 2.05 
NM-F 23.63 52.28% 105.38 5.29 8.58 1.20 
NM-C 20.15 50.62% 100.35 5.07 7.68 0.69 
Table 1. Summary of Results (mean values).  
Frequency of miscommunication: Miscom-
munication was calculated by considering two 
measures: the number of execution errors and of 
follower turns that were tagged as expressing 
non-understanding. The ANOVA revealed an 
interaction effect (F (1, 24) = 4.012, p = .05). 
Striking differences were observed between the 
Monitor-Constrained group and the rest; in par-
ticular, followers in this condition were twice or 
three times more likely to fail to understand and 
execute instructions (see last column in Table 1). 
3.2 Component analysis of instructions 
This section presents the results of the analysis 
on inclusion of landmark references, types of 
delimiters and communicative statements.  
Landmark references: Instructors in both No 
Monitor conditions preferred to produce instruc-
tions that were anchored on landmarks, especial-
ly on 3D locations such as buildings (28% of 
instructions contained locations vs 14% in the 
Monitor conditions, (F (1, 24) = 12.034, p = 
.002)). On the other hand, Monitor instructors 
opted for simple action prescriptions. Particular-
ly, 75% of the instructions in the Monitor-
Constrained condition omitted any kind of refer-
ence (compared to an average of 42% in the oth-
er conditions).  
Delimiters: Category 2 delimiters that pro-
vided simple direction information were preva-
lent in Monitor conditions (F (1, 24) = 11.407, p 
= .002). Further, an interaction effect was found 
(F (1, 24) = 3.802, p = .01); the number of cate-
gory 2 delimiters almost doubles in the Monitor-
Constrained condition. On the contrary, the use 
of category 1 delimiters, which provide informa-
tion on the boundary of the route is very limited 
in the Monitor-Constrained condition (F (1, 24) = 
5.350, p = .03). The third category of delimiters 
includes terms that specify the relation between 
traveller and an environmental feature (?on your 
left?) or between environmental features. Again, 
the difference arises in the Monitor-Constrained 
condition, which included the lowest number of 
category 3 delimiters (marginal effect, F (1, 24) 
= 3.392, p = .07). Finally, the analysis performed 
on the frequency of category 4 delimiters did not 
yield any significant effect.  
Directive and descriptive communicative 
statements: An interaction effect was revealed 
with regard to the proportion of directives and 
97
descriptives in the corpus (F (1, 24) = 3.830, p = 
.06). The instructors in the Monitor-Constrained 
condition produced less descriptives, which give 
information about relations among features in the 
environment and tap perceptual experience (?you 
will see a bridge?). In particular, in the Monitor-
Constrained condition, 4.7% of instructions were 
descriptives, whereas the proportion of descrip-
tives averaged 10% in all other conditions. 
4 Discussion 
The results resonate with previous research. The 
actions of the followers served as an immediate, 
accurate and effortless indicator of their current 
state of understanding, making verbal feedback 
redundant. Monitor instructors could readily con-
firm their assumptions about the information re-
quirements of followers and used linguistics 
shortcuts and simpler instructions exactly at the 
moment needed. On the other hand, in the No 
Monitor condition, uncertainty about the position 
and movement of the robot created the need for 
elaborate and explicit instructions. The contribu-
tion of the present study lies on that it grounds 
these observations on quantitative analysis, using 
measures like words, turns and the relative fre-
quencies of certain types of instruction compo-
nents that vary in information value. Most impor-
tantly, it describes the specific ways in which 
instructors configure their directions in the pres-
ence/absence of visual information.  
The CORK framework predicts that route pro-
tocols which are rich in descriptives and rela-
tional terms are associated with more successful 
navigation, compared to simple directional ones. 
Our results partially meet this expectation, since 
large numbers of execution errors and non-
understandings were only observed in the Moni-
tor-Constrained condition, whereas miscommu-
nication rates were similar across the other 
groups. Indeed, this condition was found to gen-
erally differ from the rest. In particular, In the 
Monitor-Constrained condition, the dialogues 
were the shortest in terms of words. Instructors 
produced many but short instructions. The com-
ponent-based analysis revealed that they em-
ployed overwhelmingly more action-based in-
structions without landmark references and de-
scriptives. Boundary information on the route, 
frame of reference and spatial relations between 
environmental features were typically omitted. In 
both Constrained conditions, followers were ex-
pected to resort to a ?mechanical? interaction, as 
coerced by the presence of the predefined re-
sponses. Inspection of the dialogues revealed that 
followers in the Monitor-Constrained condition 
did so, given the precedence of visual feedback. 
This was not the case with No Monitor-
Constrained followers who needed to verbally 
ground information. Dialogue examples are pro-
vided in Table 2 below. 
I: turn around 
I: go straight 
ahead 
I: stop 
I: turn left here 
I: go ahead 
F: What? 
I: Go straight 
ahead 
I: Now keep going down the road until 
you see a car park 
F: I am in front of the car park 
I: turn right and walk till the end,  
along the road you will see a gym on your 
right 
F: yes gym to my right side 
I: good, keep going straight and  
you will see a factory on your left 
Table 2. Dialogue excerpts from the Monitor-
Constrained (column 1) and No Monitor-Constrained 
(column 2) conditions. 
Thus, we propose that the linguistic choices of 
the followers ?prime? the instructor?s own strat-
egies. In the Monitor-Constrained Condition, 
followers were less interactive, and gave fewer 
responses with lower information value. In har-
mony, their partners provided less elaborate in-
structions, which also lacked important informa-
tion and specificity.  
In conclusion, the findings confirm our initial 
hypotheses. Instructions are sensitive to condi-
tions of (visual) co-presence. Moreover, a direct 
link was identified between the way in which 
instructions and follower?s contributions are 
formulated. Following this lead, we are now fo-
cusing on a fine-grained analysis of the utter-
ances of the follower.  
References 
Darren Gergle, Robert E. Kraut and Susan E. Fussell. 
2004.  Language Efficiency and Visual Technolo-
gy: Minimizing Collaborative Effort with Visual 
Information. Journal of Language and Social Psy-
chology, 23(4):491-517. Sage Publications, CA.  
Gary L. Allen. 2000. Principles and Practices for 
Communicating Route Knowledge. Applied Cog. 
Psychology.14(4):333?359. 
Herbert H. Clark and Meredyth A. Krych. 2004. 
Speaking While Monitoring Addressees for Under-
standing. J. of Memory and Language, 50:62-81. 
Kenny Coventry, Thora Tenbrink and John Bateman, 
2009. Spatial Language and Dialogue: Navigating 
the Domain. In K. Coventry, T. Tenbrink, and J. 
Bateman (Eds.) Spatial Language and Dialogue. 1-
8. Oxford University Press. Oxford, UK. 
Theodora Koulouri and Stanislao Lauria. 2009. Ex-
ploring Miscommunication and Collaborative Be-
haviour in Human-Robot Interaction, SIGdial09.  
98
