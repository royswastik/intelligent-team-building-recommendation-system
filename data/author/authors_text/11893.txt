Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 21?24,
Suntec, Singapore, 3 August 2009. c?2009 ACL and AFNLP
MARS: Multilingual Access and Retrieval System with Enhanced 
Query Translation and Document Retrieval 
 
 
Lianhau Lee, Aiti Aw, Thuy Vu, Sharifah Aljunied Mahani, Min Zhang, Haizhou Li 
Institute for Infocomm Research 
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632 
{lhlee, aaiti, tvu, smaljunied, mzhang, hli} 
@i2r.a-star.edu.sg 
 
  
 
Abstract 
In this paper, we introduce a multilingual ac-
cess and retrieval system with enhanced query 
translation and multilingual document retrieval, 
by mining bilingual terminologies and aligned 
document directly from the set of comparable 
corpora which are to be searched upon by us-
ers. By extracting bilingual terminologies and 
aligning bilingual documents with similar con-
tent prior to the search process provide more 
accurate translated terms for the in-domain 
data and support multilingual retrieval even 
without the use of translation tool during re-
trieval time. This system includes a user-
friendly graphical user interface designed to 
provide navigation and retrieval of information 
in browse mode and search mode respectively.  
1 Introduction 
Query translation is an important step in the 
cross-language information retrieval (CLIR). 
Currently, most of the CLIR system relies on 
various kinds of dictionaries, for example Word-
Nets (Luca and Nurnberger, 2006; Ranieri et al, 
2004), in query translation. Although dictionaries 
can provide effective translation on common 
words or even phrases, they are always limited in 
the coverage. Hence, there is a need to expand 
the existing collections of bilingual terminologies 
through various means. 
Recently, there has been more and more re-
search work focus on bilingual terminology ex-
traction from comparable corpora. Some promis-
ing results have been reported making use of sta-
tistics, linguistics (Sadat et al, 2003), translitera-
tion (Udupa et al, 2008), date information (Tao 
and Zhai, 2005) and document alignment ap-
proach (Talvensaari et al, 2007). 
In this paper, we introduce our Multilingual 
Access and Retrieval System ? MARS which 
addresses the query translation issue by using in-
domain bilingual terminologies extracted directly 
from the comparable corpora which are to be 
accessed by users. And at the same time, bilin-
gual documents are paired up prior to the search 
process based on their content similarities to 
overcome the limitation of traditional keyword 
matching based on the translated terms. These 
would provide better retrieval experiences as not 
only more accurate in-domain translated term 
will be used to retrieve the documents but also 
provide a new perspective of multilingual infor-
mation retrieval to process the time-consuming 
multilingual document matching at the backend. 
The following sections of this paper will de-
scribe the system architecture and the proposed 
functionalities of the MARS system. 
2 MARS System 
The MARS system is designed to enhance query 
translation and document retrieval through min-
ing the underlying multilingual structures of 
comparable corpora via a pivot language. There 
are three reasons for using a pivot language. 
Firstly, it is appropriate to use a universal lan-
guage among potential users of different native 
languages. Secondly, it reduces the backend data 
processing cost by just considering the pair-wise 
relationship between the pivot language and any 
other languages. Lastly, the dictionary resources 
between the pivot language and all the other lan-
guages are more likely to be available than oth-
erwise. 
There are two main parts in this system, 
namely data processing and user interface. The 
data processing is an offline process to mine the 
underlying multilingual structure of the compa-
21
rable corpora to support retrieval. The structure 
of the comparable corpora is presented visually 
in the user interface under browse mode and 
search mode to facilitate navigation and retrieval 
of information respectively. 
3 Data Processing  
For demo purpose, three different language 
newspapers from the year 1995 to 2006 pub-
lished by Singapore Press Holding (SPH), 
namely Strait Times1 (English), ZaoBao2 (Chi-
nese) and Berita Harian3  (Malay), are used as 
comparable corpora. In these particular corpora, 
English is chosen as the pivot language and noun 
terms are chosen as the basic semantic unit as 
they represent a huge amount of significant in-
formation. Our strategy is to organize and ma-
nipulate the corpora in three levels of abstraction 
? clusters, documents and terms. And our key 
task over here is to find the underlying associa-
tions of documents or terminologies in each level 
across different languages. 
First, monolingual documents are grouped into 
clusters by k-means algorithm using simple word 
vectors. Then, monolingual noun terms are ex-
tracted from each cluster using linguistic patterns 
and filtered by occurrence statistics globally 
(within cluster) and locally (within document), so 
that they are good representatives for cluster as a 
whole as well as individual documents (Vu et al, 
2008). The extracted terms are then used in 
document clustering in a new cycle and the 
whole process is repeated until the result con-
verges. 
Next, cluster alignment is carried out between 
the pivot language (English) and the other lan-
guages (Chinese, Malay). Clusters can be con-
ceptualized as the collection of documents with 
the same themes (e.g. finance, politics or sports) 
and their alignments as the correspondents in the 
other languages. Since there may be overlaps 
among themes, e.g. finance and economy, each 
cluster is allowed to align to more than one clus-
ter with varying degree of alignment score. 
After that, document alignment is carried out 
between aligned cluster pairs (Vu et al, 2009). 
Note that the corpora are comparable, thus the 
aligned document pairs are inherently compara-
                                                 
1 http://www.straitstimes.com/ an English news agency in 
Singapore. Source ? Singapore Press Holdings Ltd. 
2 http://www.zaobao.com/ a Chinese news agency in 
Singapore. Source ? Singapore Press Holdings Ltd. 
3 http://cyberita.asia1.com.sg/ a Malay news agency in 
Singapore. Source ? Singapore Press Holdings Ltd. 
ble, i.e. they are similar in contents but not iden-
tical as translation pairs. Also as important to 
note that, document alignment harvested over 
here is independent of user query. In other 
words, document alignment is not simply deter-
mined by mere occurrence of certain keyword 
and its absence does not hinder documents to be 
aligned. Hence mining of document alignment 
beforehand improves document retrieval after-
ward. 
Finally, term alignment is likewise generated 
between aligned document pairs. The aligned 
terms are expected to be in-domain translation 
pairs since they are both derived from documents 
of similar contents, and thus they have similar 
contexts. By making use of the results provided 
by each other, document alignment and term 
alignment can be improved over iterations. 
All the mentioned processes are done offline 
and the results are stored in a relational database 
which will handle online queries generated in the 
user interface later on. 
4 User Interface  
As mentioned, there are two modes provided in 
the user interface to facilitate navigation and re-
trieval of information, namely browse mode and 
search mode. Both modes can be switched sim-
ply by clicking on the respective tabs in the user 
interface. In the following, the functionalities of 
the browse mode and the search mode will be 
explained in details. 
4.1 Browse Mode 
Browse mode provides a means to navigate 
through the complex structures underneath an 
overwhelming data with an easily-understood, 
user-friendly graphical interface. In the figure 1, 
the graph in the browse mode gives an overall 
picture of the distribution of documents in vari-
ous clusters and among the different language 
collections. The outer circles represent the lan-
guage repositories and the inner circles represent 
the clusters. The sizes of the clusters are depend-
ing on the number of contained documents and 
the color represents the dominant theme. The 
labels of the highlighted clusters, characterized 
by a set of five distinguished words, are shown in 
the tooltips next to them. By clicking on a clus-
ter, the links depicting the cluster alignments will 
show up. The links to the clusters in the other 
languages are all propagated through the pivot 
language. 
22
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1 Browse mode in the MARS System 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 2 Search mode in the MARS System 
23
The right hand side of the browse panel pro-
vides the detail information about the selected 
cluster using three sub-panels, i.e. top, middle 
and bottom. The top panel displays a list of ex-
tracted terms from the selected cluster. User may 
narrow down the list of interested terms by using 
the search-text column on top. By clicking on a 
term in the list, its translations in other lan-
guages, if any, will be displayed in the middle 
sub-panel and the document containing the term 
will be listed in the bottom sub-panel. The 
?Search? buttons next to the term translations 
provide a short-cut to jump to the search mode 
with the corresponding term translation being cut 
and pasted over. Last but not least, user may 
simply click on any document listed in the bot-
tom sub-panel to read the content of the docu-
ment and its aligned documents in a pop-up win-
dow. 
4.2 Search Mode 
Search mode provides a means for comprehen-
sive information retrieval. Refer to the figure 2, 
user may enter query in any of the selected lan-
guages to search for documents in all languages. 
The main difference is that query translation is 
done via bilingual terms extracted via the term 
alignment technology discussed earlier. For each 
retrieved document, documents with similar con-
tent in the other languages are also provided to 
supplement the searched results. This enables 
documents which are potentially relevant to the 
users be retrieved as some of these retrieved 
documents may not contain the translated terms 
at all. 
On top of the query translation, other informa-
tion such as related terms and similar terms to 
the query are shown at the tab panel on the right. 
Related terms are terms that correlate statistically 
with the query term and they are arranged by 
cluster, separated by dotted line in the list. Simi-
lar terms are longer terms that contains the query 
term in itself. Both the related terms and the 
similar terms provide user additional hints and 
guides to improve further queries. 
5 Conclusion  
The MARS system is developed to enable user to 
better navigate and search information from mul-
tilingual comparable corpora in a user-friendly 
graphical user interface. Query translation and 
document retrieval is enhanced by utilizing the 
in-domain bilingual terminologies and document 
alignment acquired from the comparable corpora 
itself, without limited by dictionaries and key-
word matching. 
Currently, the system only support simple 
query. Future work will improve on this to allow 
more general query. 
References  
Ernesto William De Luca, and Andreas Nurnberger. 
2006. A Word Sense-Oriented User Interface 
for Interactive Multilingual Text Retrieval, In 
Proceedings of the Workshop Information Re-
trieval, Hildesheim.  
M. Ranieri, E. Pianta, and L. Bentivogli. 2004. 
Browsing Multilingual Information with the 
MultiSemCor Web Interface, In Proceedings of 
the LREC-2004 Workshop ?The amazing utility of 
parallel and comparable corpora?, Lisban, Portu-
gal. 
Fatiha Sadat, Masatoshi Yoshikawa, Shunsuke Ue-
mura. 2003. Learning bilingual translations 
from comparable corpora to cross-language 
information retrieval: hybrid statistics-based 
and linguistics-based approach, In Proceedings 
of the 6th international workshop on Information 
Retrieval with Asian Languages, vol. 1: pp. 57-64. 
 Raghavendra Udupa, K. Saravanan, A. Kumaran, 
Jagadeesh Jagarlamudi. 2008. Mining named en-
tity transliteration equivalents from compara-
ble corpora. In Proceedings of the 17th ACM con-
ference on Information and knowledge manage-
ment. 
Tao Tao, and ChengXiang Zhai. 2005. Mining com-
parable bilingual text corpora for cross-
language information integration. In Proceed-
ings of the 11th ACM SIGKDD international con-
ference on Knowledge discovery in data mining. 
Tuomas Talvensaari, Jorma Laurikkala, Kalervo Jar-
velin, Martti Juhola, Heikki Keskustalo. 2007. 
Creating and exploiting a comparable corpus 
in cross-language information retrieval. ACM 
Transactions on Information System (TOIS), vol. 
25(1):  Article No 4. 
Thuy Vu, Aiti Aw, Min Zhang. 2008. Term extrac-
tion through unithood and termhood unifica-
tion. In Proceedings of the 3rd International Joint 
Conference on Natural Language Processing 
(IJCNLP-08), Hyderabad, India. 
Thuy Vu, Aiti Aw, Min Zhang. 2009. Feature-based 
Method for Document Alignment in Compara-
ble News Corpora. In Proceedings of the 12th 
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL-09), 
Athens, Greece. 
24
Coling 2010: Poster Volume, pages 639?646,
Beijing, August 2010
EM-based Hybrid Model for Bilingual Terminology Extraction 
from Comparable Corpora 
 
Lianhau Lee, Aiti Aw, Min Zhang, Haizhou Li 
Institute for Inforcomm Research 
{lhlee, aaiti, mzhang, hli}@i2r.a-star.edu.sg 
 
 
Abstract 
In this paper, we present an unsuper-
vised hybrid model which combines sta-
tistical, lexical, linguistic, contextual, 
and temporal features in a generic EM-
based framework to harvest bilingual 
terminology from comparable corpora 
through comparable document align-
ment constraint. The model is configur-
able for any language and is extensible 
for additional features. In overall, it pro-
duces considerable improvement in per-
formance over the baseline method. On 
top of that, our model has shown prom-
ising capability to discover new bilin-
gual terminology with limited usage of 
dictionaries. 
1 Introduction 
Bilingual terminology extraction or term align-
ment has been well studied in parallel corpora. 
Due to the coherent nature of parallel corpora, 
various statistical methods, like EM algorithm 
(Brown et. al., 1993) have been proven to be 
effective and have achieved excellent perform-
ance in term of precision and recall. The limita-
tion of parallel corpora in all domains and lan-
guages has led some researchers to explore 
ways to automate the parallel sentence extrac-
tion process from non-parallel corpora 
(Munteanu and Marcu, 2005; Fung and Cheung, 
2004) before proceeding to the usual term 
alignment extraction using the existing tech-
niques for parallel corpora. Nevertheless, the 
coverage is limited since parallel sentences in 
non-parallel corpora are minimal. 
Meanwhile, some researchers have started to 
exploit comparable corpora directly in a new 
manner. The motivations for such an approach 
are obvious: comparable corpora are abundantly 
available, from encyclopedia to daily newspa-
pers, and the human effort is reduced in either 
generating or collecting these corpora. If bilin-
gual terminology can be extracted directly from 
these corpora, evolving or emerging terminol-
ogies can be captured much faster than lexicog-
raphy and this would facilitate many tasks and 
applications in accessing cross-lingual informa-
tion. 
There remain challenges in term alignment 
for comparable corpora. The structures of texts, 
paragraphs and sentences can be very different. 
The similarity of content in two documents var-
ies through they talk about the same subject 
matter. Recent research in using transliteration 
(Udupa et. al., 2008; Knight and Graehl, 1998), 
context information (Morin et. al., 2007; Cao 
and Li, 2002; Fung, 1998), part-of-speech tag-
ging, frequency distribution (Tao and Zhai, 
2005) or some hybrid methods (Klementiev and 
Roth, 2006; Sadat et. al., 2003) have shone 
some light in dealing with comparable corpora. 
In particular, context information seems to be 
popular since it is ubiquitous and can be re-
trieved from corpora easily. 
In this paper, we propose an EM-based hy-
brid model for term alignment to address the 
issue. Through this model, we hope to discover 
new bilingual terminology from comparable 
corpora without supervision. In the following 
sections, the model will be explained in details. 
639
2 System Architecture 
It is expensive and challenging to extract bilin-
gual terminologies from a given set of compa-
rable corpora if they are noisy with very diverse 
topics. Thus the first thing we do is to derive the 
document association relationship between two 
corpora of different languages. To do this, we 
adopt the document alignment approach pro-
posed by Vu et. al. (2009) to harvest compara-
ble news document pairs. Their approach is re-
lying on 3 feature scores, namely Title-n-
Content (TNC), Linguistic Independent Unit 
(LIU), and Monolingual Term Distribution 
(MTD). In the nutshell, they exploit common 
words, numbers and identical strings in titles 
and contents as well as their distribution in time 
domain. Their method is shown to be superior 
to Tao and Zai (2005) which simply make use 
of frequency correlation of words. 
After we have retrieved comparable docu-
ment pairs, we tokenize these documents with 
prominent monolingual noun terms found 
within. We are interested only in noun terms 
since they are more informative and more im-
portantly they are more likely not to be covered 
by dictionary and we hope to find their transla-
tions through comparable bilingual corpora. We 
adopt the approach developed by Vu et. al. 
(2008). They first use the state-of-the-art C/NC-
Value method (Frantzi and Ananiadou, 1998) to 
extract terms based on the global context of the 
corpus, follow by refining the local terms for 
each document with a term re-extraction process 
(TREM) using Viterbi algorithm. 
 
 
Figure 1. The procedure of bilingual terminol-
ogy extraction from comparable documents.  
 
After these preprocesses, we have a set of 
comparable bilingual document pairs and a set 
of prominent monolingual noun terms for each 
monolingual document. The aim of our term 
alignment model is to discover new bilingual 
terminology formed from these monolingual 
terms across aligned document pairs (Figure.1). 
Like other approaches to comparable corpora, 
there exist many challenges in aligning bilingual 
terms due to the presence of noises and the sig-
nificant text-structure disparity across the com-
parable bilingual documents. To overcome this, 
we propose using both corpus-driven and non-
corpus-driven information, from which we draw 
various features and derive our hybrid model. 
These features are used to make initial guess on 
the alignment score of term pair candidates. Fig-
ure 2 shows the overall process of our term 
alignment model on comparable corpora. This 
model is language independent and it comprises 
several main components: 
? EM algorithm 
? Term alignment initialization 
? Mutual information (MI) & TScore res-
coring 
 
Figure 2. Term alignment model.  D = docu-
ment alignment score, L = lexical similarity, N 
= named entity similarity, C = context similar-
ity, T = temporal similarity, R = related term 
similarity. 
640
3 EM Algorithm 
We make two assumptions on the preprocesses 
that the extracted monolingual terms are good 
representatives of their source documents, and 
the document alignment scores derived from 
document alignment process are good indicators 
of how well the contents of various documents 
align. Hence, the logical implication suggests 
that the extracted terms from both well aligned 
documents could well be candidates of aligned 
term pairs. 
By reformulating the state-of-the-art EM-
based word alignment framework IBM model 1 
(Brown et. al., 1993), we can derive a term 
alignment model easily. In IBM word alignment 
model 1, the task is to find word alignment by 
using parallel sentences. In the reformulated 
model for term alignment, parallel sentences are 
replaced by comparable documents, character-
ized by document alignment score and their rep-
resentative monolingual terms. 
The significant advantage over the original 
IBM model 1 is the relaxation of parallel sen-
tences or parallel corpora, by incorporating an 
additional feature of document alignment score. 
We initialize the term alignment score of the 
corresponding term pair candidates with the 
document alignment score to reflect the confi-
dence level of document alignment. Other than 
that, we also employ a collection of feature 
similarity score: lexical similarity, named entity 
similarity, context similarity, temporal similar-
ity, and related term similarity, to term align-
ment initialization. We will explain this further 
in the next section. 
As we know, IBM model 1 will converge to 
the global maximum regardless of the initial 
assignment. This is truly good news for parallel 
corpora, but not for comparable corpora which 
contains a lot of noises. To prevent IBM model 
1 from overfitting, we choose to run ten itera-
tions (each iteration consists of one E-step and 
one M-step) for each cycle of EM in both e-f 
and f-e directions.  
After each cycle of EM process, we simply 
filter off the weak term alignment pairs of both 
directions with a high threshold (0.8) and popu-
late the lexicon database with the remaining 
pairs and use it to start another cycle of EM. 
The process repeats until no new term align-
ment pair is found. The EM algorithm for term 
alignment is shown as follow: 
 
Figure 3. EM algorithm for e-f direction, where 
e[k] = k-th aligned source document, f[k] = k-th 
aligned target document, e[k,i] = i-th term in 
e[k], f[k,j] = j-th term in f[k], a[i,j,k] = probabil-
ity of alignment from f[k,j] to e[k,i], t(f|e) = 
probability of alignment from term e to term f. 
4 Term Alignment Initialization 
We retrieve term alignment candidates by pair-
ing all possible combinations of extracted 
monolingual source terms and target terms 
across the aligned document pairs. Before each 
cycle of EM, we assign an initial term align-
ment score, t(f|e) to each of these term pair can-
didates. Basically, we initialize the term align-
ment score t(f|e) based on document alignment 
score (D), lexical similarity (L), named entity 
similarity (N), context similarity (C), temporal 
similarity (T), and related term similarity (R). 
The similarity calculations of the corpus-driven 
features (D, C, T, R) are derived directly from 
the corpus and require limited lexical resource. 
The non-corpus-driven features (L, N) make use 
of a small word based bilingual dictionary to 
measure their lexical relevancy. That makes our 
model not resource-demanding and it shows that 
our model can work under limited resource 
condition. 
All the above features contribute to the term 
alignment score t(f|e) independently, and we 
formulate their cumulative contributions as the 
following: 
Initialize t(f|e). 
for (iteration = 1 to 10) 
E step 
kjiallfor
ikejkft
ikejkft
kjia
i
,,,
]),[|],[(
]),[|],[(
],,[ ?=
 
M step 
),(,
),(
),(
)|(
),(],,,[),(
],[
,],[
:,,
feallfor
fetcount
fetcount
eft
feallforkjiafetcount
f
fjkf
eike
kji
?
?
=
=
==
 
End for.
641
)|()|()|()|(
)|()|()|(
,:),(
efRefTefCefN
efLEFDeft
FfEeFE
????
???
???
?= ?
??   
where, 
e = source term 
f  = target term 
E  = source document 
F   = target document 
D   = document alignment score 
L   = lexical similarity 
N   = named entity similarity 
C  = context similarity 
T   = temporal similarity 
R   = related term similarity 
  
 (1) 
 
This formula allows us to extend the model with 
additional features without affecting the existing 
configuration. 
4.1 Document Alignment Score (D) 
As explained in the Section 3, the relaxation on 
the requirement of parallel corpora in the new 
EM model leads to the incorporation of 
document alignment score. To indicate the 
confidence level of document alignment, we 
credit every aligned term pair candidate formed 
across the aligned documents with the 
corresponding document alignment score.  
Although it is not necessary, document 
alignment score is first normalized to the range 
of [0,1], with 1 indicates parallel alignment. 
4.2 Lexical Similarity (L) 
We design a simple lexical similarity measure-
ment of two terms based on word translation. 
Term pairs that share more than 50% of word 
translation pairs will be credited with lexical 
similarity of L0, where L0 is configurable con-
tribution weightage of lexical similarity. This 
provides us a primitive hint on term alignment 
without resorting to exhaustive dictionary 
lookup. 
??
? ?=
otherwise
efTifL
efL W
,1
5.0)|(,
)|( 0  
where L0 > 1 and  TW(f|e) is word translation 
score.  
(2)
 
4.3 Named Entity Similarity (N) 
Named entity similarity is a measure of prede-
fined category membership likelihood, such as 
person, location and organization. Term pairs 
that belong to the same NE categories will be 
credited with named entity similarity of N0, 
where N0 is a configurable weightage of named 
entity similarity. We use this similarity score to 
discover bilingual terms of same NE categories, 
yet not covered by bilingual dictionary. 
??
?=
otherwise
matchcategoriesNEifN
efN
,1
,
)|( 0  
where N0 > 1. 
(3)
 
4.4 Context Similarity (C) 
We assume that terms with similar contexts are 
likely to have similar meaning. Thus, we make 
use of context similarity to measure semantic 
similarity. Here, only k nearest content words 
(verbs, nouns, adjectives and adverbs) before or 
after the terms within the sentence boundary are 
considered as its contexts. The following shows 
the calculation of context similarity of two 
terms based on cosine similarity between their 
context frequency vectors before scaling to the 
range of [1, C0], where C0 is a configurable con-
tribution weightage of context similarity. As 
shown in the formula, the t(f?|e?) accounts for 
the translation probability from the source con-
text word to the target context word, hence the 
cosine similarity calculation is carried out in the 
target language domain. 
??
?
??
???
+=
)('
2
)('
2
)('
)('
0
)'()'(
)'|'()'()'(
)1(
1)|(
fcontextfecontexte
fcontextf
econtexte
ffreqefreq
eftffreqefreq
C
efC
 
where C0 > 1. 
(4)
  
4.5 Temporal Similarity (T) 
In temporal similarity, we make use of date in-
formation which is available in some corpus 
(e.g. news). We assume aligned terms are syn-
chronous in time, this is especially true for com-
parable news corpora (Tao and Zai, 2005). We 
642
use Discrete Fourier Transform (DFT) to trans-
form the distribution function of a term in dis-
crete time domain to a representative function in 
discrete frequency domain, which is usually 
known as ?spectrum?. We then calculate the 
power spectrum, which is defined as magnitude 
square of a spectrum. Power spectrum is sensi-
tive to the relative spacing in time (or frequency 
component), yet invariant to the shifting in time, 
thus it is most suitably to be used for pattern 
matching of time distribution. The temporal 
similarity is calculated based on cosine similar-
ity between the power spectrums of the two 
terms before scaling to the range of [1, T0], 
where T0 is a configurable contribution weight-
age of temporal similarity. 
 ( ) 1)(),(cos)1()|( 0 +?= kPkPineTefT fe             (5) 
where T0 > 1 and 
( )
2
1
0
2
2
22
)(
|)}({|)(
)()(
)()(
)(),(cos
?
??
?
?
=
??=
=
=
N
n
kn
N
i
x
xx
kk
k
enonFunctionDistributi
nonFunctionDistributiDFTkP
kvku
kvku
kvkuine
?
 
4.6 Related Term Similarity (R) 
Related terms are terms that correlate statisti-
cally in the same documents and they can be 
found by using mutual information or t-test in 
the monolingual corpus. Basically, related term 
similarity is a measure of related term likeli-
hood. Aligned terms are assumed to have simi-
lar related terms, hence related term similarity 
contributes to semantic similarity. The related 
term similarity is calculated based on weighted 
contribution from the related terms of the source 
term before scaling to the range of [1, R0], 
where R0 is a configurable contribution weight-
age of related terms similarity. 
 
1)|()1()|( 0 +?= efyRsimilaritRefR          (6) 
 where R0 > 1 and 
? ?
?
? ?
?=
Ff eRe
eRe
efvote
efvote
efyRsimilarit
)('
)('
)'|(
)'|(
)|(  
? ?
?
? ???
???
?
?
?
?
=
Ff
eeR
eRe
eeR
eRe
efvote
eeMIfew
efvote
eeMIfew
efvote
}]'{)'([
)("
}]'{)'([
)("
)"|(
)",(),"(
)"|(
)",(),"(
)'|(
 
???
?
???
?=
??
? ???=
)"()(
)",(
log)",(
,1
)()"(,5.1
),"(
epep
eep
eeMI
otherwise
fReTrif
few
 
 vote(f|e?) is initialized to 1 before it is com-
puted iteratively until it converges. R(e) is the 
set of related term of e and Tr(e) is the set of 
translated term of e. 
5 MI & TScore Rescoring 
We design the MI & TScore rescoring process 
to enhance the alignment score t(f|e) of e-f term 
pairs that have significant co-occurrence fre-
quencies in aligned document pairs, based on 
pointwise mutual information and TScore (or 
commonly known as t-test) of the terms. By 
using both measures concurrently, the associa-
tion relationship of a term pair can be assumed 
with higher confidence. On top of that, the asso-
ciation of a term pair can also be suggested by a 
much higher TScore value alone. In this rescor-
ing process, we scale up the alignment score 
t(f|e) of any term pair which is strongly associ-
ated by a constant factor. The following shows 
the mathematical expressions of what has been 
described, with M0 as the configurable scaling 
factor. 
 
Rescoring condition: 
andfeTScoreif 5.2),({[ ?          (7) 
)]','(6.0),(
)()'(
)()'(:)','(
feMIMaxfeMI
ffreqffreqor
efreqefreqfe = =
??  
thenfeTScoreor }5),( ?  
0)|()|( MefTefT ?=  
where M0 > 1 and 
N
fep
fpepfep
feTScore
2
),(
)()(),(
),(
?=  
),( feirNumberOfPaN =  
643
6 Experiment and Evaluation 
We conduct the experiment on articles from 
three newspapers of different languages pub-
lished by Singapore Press Holding (SPH), 
namely Straits Times1 (English), ZaoBao2 (Chi-
nese) and Berita Harian3 (Malay), in June 2006. 
There are 3187 English articles, 4316 Chinese 
articles and 1115 Malay articles. English is cho-
sen to be the source language and the remaining 
two languages as target languages. To analyze 
the effect of the quality of comparable docu-
ment in our term alignment model, we prepare 
two different input sets of document alignment, 
namely golden document alignment and auto-
mated document alignment for each source-
target language pair. The former is retrieved by 
linguistic experts who are requested to read the 
contents of the articles in the source and the tar-
get languages, and then match the articles with 
similar contents (e.g. news coverage on same 
story), while the latter is generated using unsu-
pervised method proposed by Vu et. al. (2009), 
mentioned in Section 2. 
In both cases of document alignments, only 
monolingual noun terms extracted automatically 
by program (Vu et. al., 2008) will be used as 
basic semantic unit. There are 23,107 unique 
English noun terms, 31,944 unique Chinese 
noun terms and 8,938 unique Malay noun terms 
extracted in overall. In average, there are 17.3 
noun term tokens extracted for each English 
document, 16.9 for Chinese document and 13.0 
for Malay document. Also note that the term 
alignment reference list is constructed based on 
these extracted monolingual terms under the 
constraints of document alignment. In other 
words, the linguistic experts are requested to 
match the extracted terms across aligned docu-
ment pairs (for both golden document alignment 
and automated document alignment sets respec-
tively). The numbers of comparable document 
pairs and the corresponding unique term align-
ment reference pairs are shown in Table 2. 
                                                 
1 http://www.straitstimes.com/ an English news 
agency in Singapore. Source ? Singapore Press 
Holdings Ltd. 
2 http://www.zaobao.com/ a Chinese news agency in 
Singapore. Source ? Singapore Press Holdings Ltd. 
3 http://cyberita.asia1.com.sg/ a Malay news agency 
in Singapore. Source ? Singapore Press Holdings 
Ltd. 
In the experiment, we will conduct the named 
entity recognition (NER) by using the devel-
oped system from the Stanford NLP Group, for 
English, and an in-house engine, for Chinese. 
Currently, there is no available NER engine for 
Malay.  
 
Dictionary E-C C-E E-M M-E 
Entry 23,979 71,287 28,496 18,935 
Table 1. Statistics of dictionaries, where E = English, 
C = Chinese, M = Malay. 
 
GoldenDocAlign AutomatedDocAlign 
Corpus Doc 
Align  
Term 
Align Ref 
Doc 
Align 
Term 
Align Ref 
ST-ZB 90 313 899 777 
ST-BH 42 113 475 358 
Table 2. Statistics of comparable document align-
ment pairs and term alignment reference pairs. 
 
For baseline, we make use of IBM model 1, 
modified in the same way which has been de-
scribed in the section 3, except that we treat all 
comparable documents as parallel sentences, i.e. 
document alignment score is 1. Precision and 
recall are used to evaluate the performance of 
the system. To achieve high precision, high 
thresholds are used in the system and they are 
kept constant throughout the experiments for 
consistency. To evaluate the capability of dis-
covering new bilingual terminology, we design 
a novelty metric, which is the ratio of the num-
ber of correct out-of-dictionary term alignment 
over the total number of correct term alignment. 
 
C
N
Novelty
G
C
Recall
T
C
Precision ===         (8) 
where, 
C = total number of correct term alignment result. 
T = total number of term alignment result. 
G = total number of term alignment reference. 
N     = total number of correct term alignment result 
that are out-of-dictionary. 
 
Table 3 shows the evaluation result of term 
alignment using EM algorithm with incremental 
feature setting. The particular order of setting is 
due to the implementation sequences and it is 
not expected to affect the result of analysis. 
We observe that the precision, recall and 
novelty of the system are comparatively higher 
when the golden document alignment is used 
instead of the automated document alignment.  
644
Table 3. Performance of term alignment using EM algorithm with incremental feature setting, where D = 
document alignment, L = lexical similarity, R = related term similarity, M = MI & TScore rescoring, N = 
named entity similarity, C = context similarity, T = temporal similarity.
 
This is expected since the golden document 
alignment provides document pairs with 
stronger semantic bonding. This also suggests 
that improving on the document alignment 
would further improve the term alignment re-
sult. 
It is noteworthy observation that the imple-
mented features improve the system precision 
and recall under various scenarios, although the 
degree of improvement varies from case to case. 
This shows the effectiveness of these features in 
the model.  
On the other hand, the novelty of the system 
is around 40%+ and 50%+ for ST-ZB and ST-
BH respectively (except for the automated 
document alignment in ST-BH scenarios). This 
suggests that the system can discover quite a 
large percentage of the correct bilingual termi-
nologies that do not exist in the lexicon initially. 
Compared with the baseline IBM model 1, 
there is an increase of 14.5% in precision, 
3.51% in recall and 2.9% in novelty for ST-ZB, 
using the golden document alignment. For ST-
BH, there is an even larger increase: 50% in 
precision, 7.96% in recall and 60% in novelty. 
7 Conclusion 
We have proposed an unsupervised EM-based 
hybrid model to extract bilingual terminology 
from comparable corpora through document 
alignment constraint. Our strategy is to make 
use of various information (corpus-driven and 
non-corpus-driven) to make initial guess on the 
semantic bonding of the term alignment candi-
dates before subjecting them to document 
alignment constraint through EM algorithm. 
The hybrid model allows inclusion of additional 
features without reconfigurations on existing 
features, this make it practically attractive. 
Moreover, the proposed system can be easily 
deployed in any language with minimal con-
figurations. 
We have successfully conducted the experi-
ments in English-Chinese and English-Malay 
comparable news corpora. The features em-
ployed in the model have shown incremental 
improvement in performance over the baseline 
method. In particular, the system shows im-
provement in the capability to discover new bi-
lingual terminology from comparable corpora 
even with limited usage of dictionaries. 
From the experiments, we have found that the 
quality of comparable bilingual documents is a 
GoldenDocAlign AutomatedDocAlign corpora Setting 
Precision Recall Novelty Precision Recall Novelty 
IBM 1 75.0% 1.92%  50.0% 22.2% 0.26% 50.0% 
(D) 75.0% 1.92% 50.0% 22.2% 0.26% 50.0% 
(D,L) 81.8% 2.88% 55.6% 33.3% 0.52% 25.0% 
(D,L,R) 81.8% 2.88% 55.6% 33.3% 0.52% 25.0% 
(D,L,R,M) 78.6% 3.51% 63.6% 35.7% 0.64% 40.0% 
(D,L,R,M,N) 88.2% 4.79% 53.3% 35.7% 0.64% 40.0% 
(D,L,R,M,N,C) 89.5% 5.43% 52.9% 33.3% 0.64% 40.0% 
ST-ZB 
(D,L,R,M,N,C,T) 89.5% (17/19) 
5.43% 
(17/313) 
52.9% 
(9/17) 
37.5% 
(6/16) 
0.77% 
(6/777) 
16.7%   
(1/6) 
IBM 1 33.3% 0.89% 0.00% 33.3% 0.78% 0.00% 
(D) 33.3% 0.89% 0.00% 33.3% 0.78% 0.00% 
(D,L) 75.0% 5.31% 50.0% 50.0% 1.94% 0.00% 
(D,L,R) 75.0% 5.31% 50.0% 50.0% 1.94% 0.00% 
(D,L,R,M) 75.0% 5.31% 50.0% 54.5% 2.33% 0.00% 
(D,L,R,M,N) 75.0% 5.31% 50.0% 54.5% 2.33% 0.00% 
(D,L,R,M,N,C) 83.3% 8.85% 60.0% 50.0% 1.94% 0.00% 
ST-BH 
(D,L,R,M,N,C,T) 83.3% (10/12) 
8.85% 
(10/113) 
60.0% 
(6/10) 
50.0% 
(5/10) 
1.94% 
(5/258) 
0.00% 
(0/5) 
645
major limiting factor to achieve good perform-
ance. In future, we want to explore ways to im-
prove on this. 
References 
R. Agrawal, C. Faloutsos, and A. Swami. 1993. Effi-
cient similarity search in sequence databases. In 
Proceedings of the 4th International Conference 
on Foundations of Data Organization and Algo-
rithms. Chicago, United States. 
P. F. Brown, V. S. A. Della Pietra, V. J. Della Pietra, 
and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2): 263-312. 
Yunbo Cao and Hang Li. 2002. Base Noun Phrase 
Translation Using Wed Data and the EM Algo-
rithm, Computational Linguistics, pp.1-7. 
Pascale Fung, 1998. A statistical view on bilingual 
lexicon extraction: From parallel corpora to non-
parallel corpora. Proceedings of AMTA, pp.1-17.  
Pascale Fung and Percy Cheung. 2004. Mining Very-
Non-Parallel Corpora: Parallel Sentence and 
Lexicon Extraction via Bootstrapping and EM, 
Proceedings of EMNLP, pp.57-63. 
Alexandre Klementiev and Dan Roth, 2006. Weakly 
Supervised Named Entity Transliteration and Dis-
covery from Multilingual Comparable Corpora. 
Computational Linguistics, pp. 817-824. 
K. Knight and J. Graehl. 1998. Machine translitera-
tion, Computational Linguistics, 24(4): 599-612.  
E. Morin, B. Daille, K. Takeuchi, K. Kageura. 2007. 
Bilingual Terminology Mining ? Using Brain, not 
brawn comparable corpora, Proceedings of ACL. 
Dragos Stefan Munteanu and Daniel Marcu. 2005. 
Improving Machine Translation Performance by 
Exploiting Non-Parallel Corpora. Computational 
Linguistics, 31(4): 477-504. 
Fatiha Sadat, Masatoshi Yoshikawa, Shunsuke Ue-
mura, 2003. Learning Bilingual Translations from 
Comparable Corpora to Cross-Language Infor-
mation Retrieval: Hybrid Statistics-based and 
Linguistics-based Approach. Proceedings of ACL, 
vol.11, pp.57-64. 
Tao Tao and Chengxiang Zhai. 2005. Mining com-
parable bilingual text corpora for cross-language 
information integration, Proceedings of ACM. 
Raghavendra Udupa, K. Saravanan, A. Kumaran, 
Jagadeesh Jagarlamudi. 2008. Mining named en-
tity transliteration equivalents from comparable 
corpora, Proceedings of ACM. 
Thuy Vu, Aiti Aw, Min Zhang, 2008. Term extrac-
tion through unithood and termhood unification. 
Proceedings of IJCNLP-08, Hyderabad, India. 
Thuy Vu, Aiti Aw, Min Zhang, 2009. Feature-based 
Method for Document Alignment in Comparable 
News Corpora. Proceedings of EACL-09, Athens, 
Greece. 
 
646
