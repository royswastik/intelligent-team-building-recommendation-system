Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 74?77,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Integrating High Precision Rules with Statistical Sequence Classifiers for
Accuracy and Speed
Wenhui Liao, Marc Light, and Sriharsha Veeramachaneni
Research and Development,Thomson Reuters
610 Opperman Drive, Eagan MN 55123
Abstract
Integrating rules and statistical systems is a
challenge often faced by natural language pro-
cessing system builders. A common sub-
class is integrating high precision rules with a
Markov statistical sequence classifier. In this
paper we suggest that using such rules to con-
strain the sequence classifier decoder results
in superior accuracy and efficiency. In a case
study of a named entity tagging system, we
provide evidence that this method of combina-
tion does prove efficient than other methods.
The accuracy was the same.
1 Introduction
Sequence classification lies at the core of several
natural language processing applications, such as
named entity extraction, Asian language segmen-
tation, Germanic language noun decompounding,
and event identification. Statistical models with a
Markov dependency have been successful employed
to perform these tasks, e.g., hidden Markov mod-
els (HMMs)(Rabiner, 1989) and conditional random
fields (CRFs)(Lafferty et al, 2001). These statistical
systems employ a Viterbi (Forney, 1973) decoder at
runtime to efficiently calculate the most likely la-
bel sequence based on the observed sequence and
model. Statistical machine translation systems make
use of similar decoders.
In many situations it is beneficial, and some-
times required, for these systems to respect con-
straints from high precision rules. And thus when
building working sequence labeling systems, re-
searchers/software engineers are often faced with
the task of combining these two approaches. In
this paper we argue for a particular method of com-
bining statistical models with Markov dependencies
and high precision rules. We outline a number of
ways to do this and then argue that guiding the de-
coder of the statistical system has many advantages
over other methods of combination.
But first, does the problem of combining multi-
ple approaches really happen? In our experience the
need arises in the following way: a statistical ap-
proach with a Markov component is chosen because
it has the best precision/recall characteristics and has
reasonable speed. However, a number of rules arise
for varied reasons. For example, the customer pro-
vides domain knowledge not present in the training
data or a particular output characteristic is more im-
portant that accuracy. Consider the following ficti-
tious but plausible situation: A named entity tagging
system is built using a CRF. The customer then pro-
vides a number of company names that cannot be
missed, i.e., false negatives for these companies are
catastrophic but false positives can be tolerated. In
addition, it is known that, unlike in the training data,
the runtime data will have a company name immedi-
ately before every ticker symbol. The question fac-
ing the builder of the system is how to combine the
CRF with rules based on the must-find company list
and the company-name-before-every-ticker-symbol
fact.
Similar situations arise for the other sequence tag-
ging situations mentioned above and for machine
translation. We suspect that even for non-language
applications, such as gene sequence labeling, similar
situations arise.
74
In the next section we will discuss a number of
methods for combining statistical systems and high
precision rules and argue for guiding the decoder
of the statistical model. Then in section 3, we de-
scribe an implementation of the approach and give
evidence that the speed benefits are substantial.
2 Methods for Combining a Markov
Statistical System and High Precision
Rules
One method of combination is to encode high preci-
sion rules as features and then train a new model that
includes these features. One advantage is that the
system stays a straightforward statistical system. In
addition, the rules are fully integrated into the sys-
tem allowing the statistical model weigh the rules
against other evidence. However, the model may
not give the rules high weight if training data does
not bear out their high precision or if the rule trig-
ger does not occur often enough in the training data.
Thus, despite a ?rule? feature being on, the system
may not ?follow? the rule in its result labeling. Also,
addition or modification of a rule would require a
retraining of the model for optimal accuracy. The
retraining process may be costly and/or may not be
possible in the operational environment.
Another method is to run both the statistical sys-
tem and the rules and then merge the resulting labels
giving preference to the labels resulting from the
high precision rules. The benefits are that the rules
are always followed. However, the statistical system
does not have the information needed to give an op-
timal solution based on the results of the high preci-
sion rules. In other words, the results will be incon-
sistent from the view of the statistical system; i.e., if
it had know what the rules were going to say, then it
would have calculated the remaining part of the label
sequence differently. In addition, the decoder con-
siders part of the label sequence search space that is
only going to be ruled out, pun intended, later.
Now for the preferred method: run the rules first,
then use their output to guide the decoder for the
statistical model. The benefits of this method are
that the rules are followed, the statistical system is
informed of constraints imposed by the rules and
thus the statistical system calculates optimal paths
given these constraints. In addition, the decoder
considers only those label sequences consistent with
these constraints, resulting in a smaller search space.
Thus, we would expect this method to produce both
a more accurate and a faster implementation.
Consider Figure 1 which shows a lattice that rep-
resents all the labeling sequences for the input ...
Microsoft on Monday announced a ... The possible
labels are O (out), P (person), C (company), L (lo-
cation) . Assume Microsoft is in a list of must-find
companies and that on and Monday are part of a rule
that makes them NOT names in this context. The
bold points are constraints from the high-precision
rules. In other words, only sequences that include
these bold points need to be considered.
Figure 1: Guiding decoding with high-precision rules
Figure 1 also illustrates how the constraints re-
duce the search space. Without constraints, the
search space includes 46 = 4096 sequences, while
with constraints, it includes only 43 = 64.
It should also be noted that we do not claim to
have invented the idea of constraining the decoder.
For example, in the context of active learning, where
a human corrects some of the errors made by a CRF
sequence classifier, (Culota et al, 2006) proposed a
constrained Viterbi algorithm that finds the path with
maximum probability that passes through the labels
assigned by the human. They showed that constrain-
ing the path to respect the human labeling consider-
ably improves the accuracy on the remaining tokens
in the sequence. Our contribution is noticing that
constraining the decoder is a good way to integrate
rule output.
3 A Case Study: Named Entity
Recognition
In this section, we flesh out the discussion of named
entity (NE) tagging started above. Since the entity
type of a word is determined mostly by the context
of the word, NE tagging is often posed as a sequence
75
classification problem and solved by Markov statis-
tical systems.
3.1 A Named Entity Recognition System
The system described here starts with a CRF which
was chosen because it allows for the use of numer-
ous and arbitrary features of the input sequence and
it can be efficiently trained and decoded. We used
the Mallet toolkit (McCallum, 2002) for training the
CRF but implemented our own feature extraction
and runtime system. We used standard features such
as the current word, the word to the right/left, ortho-
graphic shape of the word, membership in word sets
(e.g., common last names), features of neighboring
words, etc.
The system was designed to run on news wire text
and based on this data?s characteristics, we designed
a handful of high precision rules including:
Rule 1: if a token is in a must-tag list, this token
should be marked as Company no matter what the
context is.
Rule 2: if a capitalized word is followed by cer-
tain company suffix such as Ltd, Inc, Corp, etc., la-
bel both as Company.
Rule 3: if a token sequence is in a company list
and the length of the sequence is larger than 3, label
them as Company.
Rule 4: if a token does not include any uppercase
letters, is not pure number, and is not in an excep-
tions list, label it as not part of a name. (The ex-
ceptions list includes around 70 words that are not
capitalized but still could be an NE, such as al, at,
in, -, etc.)
Rule 5: if a token does not satisfy rule 4 but its
neighboring tokens satisfy rule 4, then if this token
is a time related word, label it as not part of a name.
(Example time tokens are January and Monday.)
The first three rules aim to find company names
and the last two to find tokens that are not part of a
name.
These rules are integrated into the system as de-
scribed in section 2: we apply the rules to the input
token sequence and then use the resulting labels, if
any, to constrain the Viterbi decoder for the CRF.
A further optimization of the system is based on
the following observation: features need not be cal-
culated for tokens that have already received labels
from the rules. (An exception to this is when fea-
tures are copied to a neighbor, e.g., the token to my
left is a number.) Thus, we do not calculate many
features of rule-labeled tokens. Note that feature ex-
traction can often be a major portion of the compu-
tational cost of sequence labeling systems (see Table
1(b))
3.2 Evidence of Computational Savings
Resulting from Our Proposed Method of
Integration
We compare the results when high-precision rules
are integrated into CRF for name entity extraction
(company, person, and location) in terms of both ac-
curacy and speed for different corpora. Three cor-
pora are used, CoNLL (CoNLL 2003 English shared
task official test set), MUC (Message Understanding
Conference), and TF (includes around 1000 news ar-
ticles from Thomson Financial).
Table 1(a) shows the results for each corpora re-
spectively. The baseline method does not use any
high-precision rules, the Post-corr uses the high-
precision rules to correct the labeling from the CRF,
and Constr-viti uses the rules to constrain the label
sequences considered by the Viterbi decoder. In gen-
eral, Constr-viti achieves slightly better precision
and recall.
(a)
(b)
Figure 2: (b) A test example : (a) without constraints; (b)
with constraints
To better understand how our strategy could im-
prove the accuracy, we did some analysis on the
76
Table 1: Experiment Results
Database Methods Precision Recall F1
CoNLL Baseline 84.38 83.02 83.69
Post-corr 85.87 84.86 85.36
Constr-viti 85.98 85.55 85.76
TF Baseline 88.39 82.42 85.30
Post-corr 87.69 88.30 87.99
Constr-viti 88.02 88.54 88.28
MUC Baseline 92.22 88.72 90.43
Post-Corr 91.28 88.87 90.06
Constr-viti 90.86 89.37 90.11
(a)Precision and Recall
Methods Rules Features Viterbi Overall
Baseline 0 0.78 0.22 1
Post-corr 0.08 0.78 0.22 1.08
Constr-vite 0.08 0.35 0.13 0.56
Baseline 0 0.85 0.15 1
Post-Corr 0.14 0.85 0.15 1.14
Constr-vite 0.14 0.38 0.1 0.62
Baseline 0 0.79 0.21 1
Post-corr 0.12 0.79 0.21 1.12
Constr-vite 0.12 0.36 0.12 0.60
(b)Time Efficiency
testing data. In one example as shown in Figure 2,
Steel works as an attorney, without high-precision
rules, Steel works is tagged as a company since it is
in our company list. Post-correction changes the la-
bel of works to O, but it is unable to fix Steel. With
our strategy, since works is pinned as O in the Vert-
ibi algorithm, Steel is tagged as Per. Thus, com-
pared to post-correction, the advantage of constrain-
ing Viterbi is that it is able to affect the whole path
where the token is, instead a token itself. However,
the improvements were not significant in our case
study. We have not done an error analysis. We can
only speculate that the high precision rules do not
have perfect precision and thus create a number of
errors that the statistical model would not have made
on its own.
We also measured how much the constrained
Viterbi method improves efficiency. We divide the
computational time to three parts: time in applying
rules, time in feature extraction, and time in Viterbi
computation. Table 1(b) lists the time efficiency. In-
stead using specific time unit (e.g. second), we use
ratio instead by assuming the overall time for the
baseline method is 1. As shown in the table, for
the three data sets, the overall time of our method
is 0.56, 0.62, and 0.60 of the time of the baseline
algorithm respectively. The post-correction method
is the most expensive one because of the extra time
spending in rules. Overall, the constrained Viterbi
method is substantially faster than the Baseline and
Post-corr methods in addition to being more accu-
rate.
4 Conclusions
The contribution of this paper is the repurposing of
the idea of constraining a decoder: we constrain the
decoder as a way to integrate high precision rules
with a statistical sequence classifier. In a case study
of named entity tagging, we show that this method
of combination does in fact increase efficiency more
than competing methods without any lose of ac-
curacy. We believe analogous situations exist for
other sequence classifying tasks such as Asian lan-
guage segmentation, Germanic language noun de-
compounding, and event identification.
References
Aron Culota, Trausti Kristjansson, Andrew McCallum,
and Paul Viola. 2006. Corrective feedback and per-
sistent learning for information extraction. Artificial
Intelligence Journal, 170:1101?1122.
G. D. Forney. 1973. The viterbi algorithm. Proceedings
of the IEEE, 61(3):268?278.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289.
A.K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
Lawrence R. Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, pages 257?286.
77
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 58?65,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Simple Semi-supervised Algorithm For
Named Entity Recognition
Wenhui Liao and Sriharsha Veeramachaneni
Research and Develpment,Thomson Reuters
610 Opperman Drive, Eagan MN 55123
{wenhui.liao, harsha.veeramachaneni}@thomsonreuters.com
Abstract
We present a simple semi-supervised learning
algorithm for named entity recognition (NER)
using conditional random fields (CRFs). The
algorithm is based on exploiting evidence that
is independent from the features used for a
classifier, which provides high-precision la-
bels to unlabeled data. Such independent ev-
idence is used to automatically extract high-
accuracy and non-redundant data, leading to a
much improved classifier at the next iteration.
We show that our algorithm achieves an aver-
age improvement of 12 in recall and 4 in pre-
cision compared to the supervised algorithm.
We also show that our algorithm achieves high
accuracy when the training and test sets are
from different domains.
1 Introduction
Named entity recognition (NER) or tagging is the
task of finding names such as organizations, persons,
locations, etc. in text. Since whether or not a word is
a name and the entity type of a name are determined
mostly by the context of the word as well as by the
entity type of its neighbors, NER is often posed as a
sequence classification problem and solved by meth-
ods such as hidden Markov models (HMM) and con-
ditional random fields (CRF).
Automatically tagging named entities (NE) with
high precision and recall requires a large amount of
hand-annotated data, which is expensive to obtain.
This problem presents itself time and again because
tagging the same NEs in different domains usually
requires different labeled data. However, in most
domains one often has access to large amounts of
unlabeled text. This fact motivates semi-supervised
approaches for NER.
Semi-supervised learning involves the utilization
of unlabeled data to mitigate the effect of insuf-
ficient labeled data on classifier accuracy. One
variety of semi-supervised learning essentially at-
tempts to automatically generate high-quality train-
ing data from an unlabeled corpus. Algorithms such
as co-training (Blum and Mitchell, 1998)(Collins
and Singer, 1999)(Pierce and Cardie, 2001) and
the Yarowsky algorithm (Yarowsky, 1995) make as-
sumptions about the data that permit such an ap-
proach.
The main requirement for the automatically gen-
erated training data in addition to high accuracy,
is that it covers regions in the feature space with
low probability density. Furthermore, it is neces-
sary that all the classes are represented according to
their prior probabilities in every region in the fea-
ture space. One approach to achieve these goals is
to select unlabeled data that has been classified with
low confidence by the classifier trained on the orig-
inal training data, but whose labels are known with
high precision from independent evidence. Here in-
dependence means that the high-precision decision
rule that classifies these low confidence instances
uses information that is independent of the features
used by the classifier.
We propose two ways of obtaining such inde-
pendent evidence for NER. The first is based on
the fact that multiple mentions of capitalized to-
kens are likely to have the same label and occur
in independently chosen contexts. We call this the
58
multi-mention property. The second is based on the
fact that entities such as organizations, persons, etc.,
have context that is highly indicative of the class,
yet is independent of the other context (e.g. com-
pany suffixes like Inc., Co., etc., person titles like
Mr., CEO, etc.). We call such context high precision
independent context.
Let us first look at two examples.
Example 1:
1) said Harry You, CEO of HearingPoint ....
2) For this year?s second quarter, You said the
company?s ...
The classifier tags ?Harry You? as person (PER)
correctly since its context (said, CEO) makes it an
obvious name. However, in the second sentence, the
classifier fails to tag ?You? as a person since ?You?
is usually a stopword. The second sentence is ex-
actly the type of data needed in the training set.
Example 2:
(1) Medtronic Inc 4Q profits rise 10 percent...
(2) Medtronic 4Q profits rise 10 percent...
The classifier tags ?Medtronic? correctly in the
first sentence because of the company suffix ?Inc?
while it fails to tag ?Medtronic? in the second
sentence since ?4Q profits? is a new pattern and
?Medtronic? is unseen in the training data. Thus the
second sentence is what we need in the training set.
The two examples have one thing in common. In
both cases, the second sentence has a new pattern
and incorrect labels, which can be fixed by using
either multi-mention or high-precision context from
the first sentence. We actually artificially construct
the second sentence to be added to the training set in
Example 2 although only the first sentence exists in
the unlabeled corpus.
By leveraging such independent evidence, our
algorithm can automatically extract high-accuracy
and non-redundant data for training, and thus ob-
tain an improved model for NER. Specifically, our
algorithm starts with a model trained with a small
amount of gold data (manually tagged data). This
model is then used to extract high-confidence data,
which is then used to discover low-confidence data
by using other independent features. These low-
confidence data are then added to the training data
to retrain the model. The whole process repeats
until no significant improvement can be achieved.
Our experiments show that the algorithm is not only
much better than the initial model, but also better
than the supervised learning when a large amount
of gold data are available. Especially, even when
the domain from which the original training data is
sampled is different from the domain of the testing
data, our algorithm still provides significant gains in
classification accuracy.
2 Related Work
The Yarowsky algorithm (Yarowsky, 1995), orig-
inally proposed for word sense disambiguation,
makes the assumption that it is very unlikely for two
occurrences of a word in the same discourse to have
different senses. This assumption is exploited by
selecting words classified with high confidence ac-
cording to sense and adding other contexts of the
same words in the same discourse to the training
data, even if they have low confidence. This allows
the algorithm to learn new contexts for the senses
leading to higher accuracy. Our algorithm also uses
multi-mention features. However, the application
of the Yarowsky algorithm to NER involves several
domain-specific choices as will become evident be-
low.
Wong and Ng (Wong and Ng, 2007) use the same
idea of multiple mentions of a token sequence be-
ing to the same named entity for feature engineer-
ing. They use a named entity recognition model
based on the maximum entropy framework to tag a
large unlabeled corpus. Then the majority tags of
the named entities are collected in lists. The model
is then retrained by using these lists as extra fea-
tures. This method requires a sufficient amount of
manually tagged data initially to work. Their paper
shows that, if the initial model has a low F-score,
the model with the new features leads to low F-score
too. Our method works with a small amount of gold
data because, instead of constructing new features,
we use independent evidence to enrich the training
data with high-accuracy and non-redundant data.
The co-training algorithm proposed by Blum and
Mitchell (Blum and Mitchell, 1998) assumes that
the features can be split into two class-conditionally
independent sets or ?views? and that each view is
sufficient for accurate classification. The classifier
built on one of the views is used to classify a large
unlabeled corpus and the data classified with high-
59
confidence are added to the training set on which
the classifier on the other view is trained. This pro-
cess is iterated by interchanging the views. The
main reason that co-training works is that, because
of the class-conditional independence assumptions,
the high-confidence data from one view, in addition
to being highly precise, is unbiased when added to
the training set for the other view. We could not
apply co-training for semi-supervised named entity
recognition because of the difficulty of finding infor-
mative yet class-conditionally independent feature
sets.
Collins et al(Collins and Singer, 1999) proposed
two algorithms for NER by modifying Yarowsky?s
method (Yarowsky, 1995) and the framework sug-
gested by (Blum and Mitchell, 1998). However, all
their features are at the word sequence level, instead
of at the token level. At the token level, the seed
rules they proposed do not necessarily work. In ad-
dition, parsing sentences into word sequences is not
a trivial task, and also not necessary for NER, in our
opinion.
Jiao et al propose semi-supervised conditional
random fields (Jiao et al, 2006) that try to maxi-
mize the conditional log-likelihood on the training
data and simultaneously minimize the conditional
entropy of the class labels on the unlabeled data.
This approach is reminiscent of the semi-supervised
learning algorithms that try to discourage the bound-
ary from being in regions with high density of unla-
beled data. The resulting objective function is no
longer convex and may result in local optima. Our
approach in contrast avoids changing the CRF train-
ing procedure, which guarantees global maximum.
3 Named Entity Recognition
As long as independent evidence exists for one type
of NE, our method can be directly applied to classify
such NE. As an example, we demonstrate how to ap-
ply our method to classify three types of NEs: orga-
nization (ORG), person (PER), and location (LOC)
since they are the most common ones. A non-NE is
annotated as O.
3.1 Conditional Random Fields for NER
We use CRF to perform classification in our frame-
work. CRFs are undirected graphical models trained
to maximize the conditional probability of a se-
quence of labels given the corresponding input se-
quence. Let X , X = x1...xN , be an input sequence,
and Y , Y = y1....yN , be the label sequence for the
input sequence. The conditional probability of Y
given X is:
P (Y |X) = 1Z(X) exp(
N?
n=1
?
k
?kfk(yn?1, yn, X, n))
(1)
where Z(X) is a normalization term, fk is a feature
function, which often takes a binary value, and ?k
is a learned weight associated with the feature fk.
The parameters can be learned by maximizing log-
likelihood ` which is given by
` = ?
i
logP (Yi|Xi)?
?
k
?2k
2?2k
(2)
where ?2k is the smoothing (or regularization) pa-
rameter for feature fk. The penalty term, used for
regularization, basically imposes a prior distribution
on the parameters.
It has been shown that ` is convex and thus a
global optimum is guaranteed (McCallum, 2003).
Inferring label sequence for an input sequence X
involves finding the most probable label sequence,
Y ? = argmax
Y
P (Y |X), which is done by the
Viterbi algorithm (Forney, 1973).
3.2 Features for NER
One big advantage of CRF is that it can naturally
represent rich domain knowledge with features.
3.2.1 Standard Features
Part of the features we used for our CRF classifier
are common features that are widely used in NER
(McCallum and Li, 2003), as shown below.
1) Lexicon. Each token is itself a feature.
2) Orthography. Orthographic information is
used to identify whether a token is capitalized, or
an acronym, or a pure number, or a punctuation, or
has mixed letters and digits, etc.
3) Single/multiple-token list. Each list is a collec-
tion of words that have a common sematic meaning,
such as last name, first name, organization, company
suffix, city, university, etc.
60
4) Joint features. Joint features are the conjunc-
tions of individual features. For example, if a token
is in a last name list and its previous token is in a
title list, the token will have a joint feature called as
Title+Name.
5) Features of neighbors. After extracting the
above features for each token, its features are then
copied to its neighbors (The neighbors of a token in-
clude the previous two and next two tokens) with a
position id. For example, if the previous token of a
token has a feature ?Cap@0?, this token will have a
feature ?Cap@-1?.
3.2.2 Label Features
One unique and important feature used in our al-
gorithm is called Label Features. A label feature
is the output label of a token itself if it is known.
We designed some simple high-precision rules to
classify each token, which take precedence over the
CRF. Specifically, if a token does not include any
uppercase letter, is not a number, and it is not in the
nocap list (which includes the tokens that are not
capitalized but still could be part of an NE, such as
al, at, in, -, etc), the label of this token is ?O?.
Table 1: An example of extracted features
Tokens Feature
Monday W=Monday@0 O@0
vice W=vice@0 O@0
chairman W=chairman@0 title@0 O@0
Goff W=Goff@0 CAP@0 Lastname@0
W=chairman@-1 title@-1 O@-1
W=vice@-2 O@-2
W=said@1 O@1 W=it@2 O@2
said W=said@0 O@0
the W=it@0 O@0
company W=company@0 O@0
In addition, if a token is surrounded by ?O? to-
kens and is in a Stopword list, or in a Time list (a
collection of date, time related tokens), or in a no-
cap list, or a nonNE list (a collection of tokens that
are unlikely to be an NE), or a pure number, its label
is ?O? as well. For example, in the sentence ?Ford
has said there is no plan to lay off workers?, all the
tokens except ?Ford? have ?O? labels. More rules
can be designed to classify NE labels. For example,
if a token is in an unambiguousORG list, it has a
label ?ORG?.
For any token with a known label, unless it is a
neighbor of a token with its label unknown (i.e., not
pretagged with high precision), its features include
only its lexicon and its label itself. No features will
be copied from its neighbors either. Table 1 gives
an example to demonstrate the features used in our
algorithm. For the sentence ?Monday vice chairman
Goff said the company ...?, only ?Goff? includes its
own features and features copied from its neighbors,
while most of the other tokens have only two fea-
tures since they are ?O? tokens based on the high-
precision rules.
Usually, more than half the tokens will be classi-
fied as ?O?. This strategy greatly saves feature ex-
traction time, training time, and inference time, as
well as improving the accuracy of the model. Most
importantly, this strategy is necessary in the semi-
supervised learning, which will be explained in the
next section.
4 Semi-supervised Learning Algorithm
Our semi-supervised algorithm is outlined in Ta-
ble 2. We assume that we have a small amount of
labeled data L and a classifier Ck that is trained on
L. We exploit a large unlabeled corpus U from the
test domain from which we automatically and grad-
ually add new training data D to L, such that L
has two properties: 1) accurately labeled, meaning
that the labels assigned by automatic annotation of
the selected unlabeled data are correct, and 2) non-
redundant, which means that the new data is from
regions in the feature space that the original training
set does not adequately cover. Thus the classifier Ck
is expected to get better monotonically as the train-
ing data gets updated.
Table 2: The semi-supervised NER algorithm
Given:
L - a small set of labeled training data
U - unlabeled data
Loop for k iterations:
Step 1: Train a classifier Ck based on L;
Step 2: Extract new data D based on Ck;
Step 3: Add D to L;
At each iteration, the classifier trained on the pre-
vious training data (using the features introduced in
the previous section) is used to tag the unlabeled
data. In addition, for each O token and NE seg-
ment, a confidence score is computed using the con-
61
strained forward-backward algorithm (Culotta and
McCallum, 2004), which calculates the LcX , the sum
of the probabilities of all the paths passing through
the constrained segment (constrained to be the as-
signed labels).
One way to increase the size of the training data
is to add all the tokens classified with high confi-
dence to the training set. This scheme is unlikely
to improve the accuracy of the classifier at the next
iteration because the newly added data is unlikely
to include new patterns. Instead, we use the high
confidence data to tag other data by exploiting inde-
pendent features.
? Tagging ORG
If a sequence of tokens has been classified as
?ORG? with high confidence score (> T )1,
we force the labels of other occurrences of the
same sequence in the same document, to be
?ORG? and add all such duplicate sequences
classified with low confidence to the training
data for the next iteration. In addition if a high
confidence segment ends with company suf-
fix, we remove the company suffix and check
the multi-mentions of the remaining segment
also. In addition to that, we reclassify the sen-
tence after removing the company suffix and
check if the labels are still the same with high-
confidence. If not, the sequence will be added
to the training data. As shown in Example 4,
?Safeway shares ticked? is added to training
data because ?Safeway? has low confidence af-
ter removing ?Inc.?.
Example 4:
High-confidence ORG: Safeway Inc. shares
ticked up ...
Low-confidence ORG:
1) Safeway shares ticked up ...
2) Wall Street expects Safeway to post earnings
...
? Tagging PER
If a PER segment has a high confidence score
and includes at least two tokens, both this seg-
1Through the rest of the paper, a high confidence score
means the score is larger than T. In our experiments, T is set
as 0.98. A low confidence score means the score is lower than
0.8.
ment and the last token of this segment are
used to find their other mentions. Similarly,
we force their labels to be PER and add them
to the training data if their confidence score is
low. However, if these mentions are followed
by any company suffix and are not classified
as ORG, their labels, as well as the company
suffix are forced to be ORG (e.g., Jefferies &
Co.). We require the high-confidence PER seg-
ment to include at least two tokens because the
classifier may confuse single-token ORG with
PER due to their common context. For ex-
ample, ?Tesoro proposed 1.63 billion purchase
of...?, Tesoro has high-confidence based on the
model, but it represents Tesoro Corp in the doc-
ument and thus is an ORG.
In addition, the title feature can be used simi-
larly as the company suffix features. If a PER
with a title feature has a high confidence score,
but has a low score after the title feature is
removed, the PER and its neighbors will be
put into training data after removing the title-
related tokens.
Example 5:
High-confidence PER:
1)Investor AB appoints Johan Bygge as CFO...
2)He is replacing Chief CEO Avallone...
Low-confidence PER:
1) Bygge is employed at...
2) He is replacing Avallone ...
(It is obvious for a human-being that Bygge is
PER because of the existence of ?employed?.
However, when the training data doesn?t in-
clude such cases, the classifier just cannot rec-
ognize it.)
? Tagging LOC
The same approach is used for a LOC segment
with a high confidence score. We force the la-
bels of its other mentions to be LOC and add
them to the training data if their confidence
score is low. Again, if any of these mentions
follows or is followed by an ORG segment with
a high confidence score, we force the labels to
be ORG as well. This is because when a LOC
is around an ORG, the LOC is usually treated
as part of an ORG, e.g., Google China.
62
Example 6:
High-confidence LOC: The former Soviet re-
public of Azerbaijan is...
Low-confidence PER:
Azerbaijan energy reserves better than...
Change LOC to ORG: shareholders of the
Chicago Board of Trade...
? Tagging O
Since all the NE segments added to the train-
ing data have low confidence scores based on
the original model, and especially since many
of them were incorrectly classified before cor-
rection, these segments form good training data
candidates. However, all of them are positive
examples. To balance the training data, we
need negative examples as well. If a token is
classified as ?O? with high confidence score
and does not have a label feature ?O?, this to-
ken will be used as a negative example to be
added to the training data.
Since the features of each token include the fea-
tures copied from its neighbors, in addition to those
extracted from the token itself, its neighbors need to
be added to the training set alo. If the confidence of
the neighbors are low, the neighbors will be removed
from the training data after copying their features to
the token of interest. If the confidence scores of the
neighbors are high, we further extend to the neigh-
bors of the neighbors until low-confidence tokens
are reached. We remove low-confidence neighbors
in order to reduce the chances of adding training ex-
amples with false labels.
Table 3: Step 2 of the semi-supervised algorithm
Step 2: Extract new data D based on Ck
i) Classify kth portion of U and compute confidence
scores;
ii) Find high-confidence NE segments and use them
to tag other low-confidence tokens
iii) Find qualified O tokens
iv) Extract selected NE and O tokens as well as
their neighbors
v) Shuffle part of the NEs in the extracted data
vi) Add extracted data to D
Now we have both negative and positive training
examples. However, one problem with the positive
data is that the same NE may appear too many times
since the multi-mention property is used. For ex-
ample, the word ?Citigroup? may appear hundreds
of times in recent financial articles because of the
subprime crisis. To account for this bias in the data
we randomly replace these NEs. Specifically, we
replace a portion of such NEs with NEs randomly
chosen from our NE lists. The size of the portion is
decided by the ratio of the NEs that are not in our
NE list over all the NEs in the gold data.
Table 3 summarizes the key sub-steps in Step 2
of the algorithm. At each step, more non-redundant
and high-accuracy data is added into the training set
and thus improves the model gradually.
5 Experiments
The data set used in the experiments is explained
in Table 4. Although we have 1000 labeled news
documents from the Thomson Financial (TF) News
source, only 60 documents are used as the initial
training data in our algorithm. For the evaluation,
the gold data was split into training and test sets as
appropriate. The toolbox we used for CRF is Mallet
(McCallum, 2002).
Table 4: Data source. Tokens include words, punctuation
and sentence breaks.
Gold Data 1000 docs from TF news
(around 330 tokens per doc)
Unlabeled Corpus 100,000 docs from TF news
0.5 0.6 0.7 0.8 0.9
93
94
95
96
97
98
Confidence Score Threshold
Acc
ura
cy
Figure 1: Token accuracy vs confidence score.
We first investigated our assumption that a high
confidence score indicates high classification accu-
racy. Figure 1 illustrates how accuracy varies as
CRF confidence score changes when 60 documents
63
are used as training data and the remaining are used
as testing data. When the threshold is 0.98, the token
accuracy is close to 99%. We believe this accuracy is
sufficiently high to justify using the high confidence
score to extract tokens with correct labels.
Table 5: Precision and recall of the automatically ex-
tracted training data
NE Precision% Recall% F-score%
LOC 94.5 96.8 95.6
ORG 96.6 93.4 94.9
PER 95.0 89.6 92.2
We wished to study the accuracy of our training
data generation strategy from how well it does on the
gold data. We treat the remaining gold data (except
the data trained for the initial model) as if they were
unlabeled, and then applied our data extraction strat-
egy on them. Table 5 illustrates the precision and re-
call for the three types of NEs of the extracted data,
which only accounts for a small part of the gold data.
The average F-score is close to 95%. Although the
precision and recall are not perfect, we believe they
are good enough for the training purpose, consider-
ing that human tagged data is seldom more accurate.
We compared the semi-supervised algorithm with
a supervised algorithm using the same features. The
semi-supervised algorithm starts with 60 labeled
documents (around 20,000 tokens) and ends with
around 1.5 million tokens. We trained the supervised
algorithm with two data sets: using only 60 docu-
ments (around 20,000 tokens) and using 700 doc-
uments (around 220,000 tokens) respectively. The
reason for the choice of the training set size is
the fact that 20,000 tokens are a reasonably small
amount of data for human to tag, and 220,000 tokens
are the amount usually used for supervised algo-
rithms (CoNLL 2003 English NER (Sang and Meul-
der, 2003) training set has around 220,000 tokens).
Table 6 illustrates the results when 300 docu-
ments are used for testing. As shown in Table 6,
starting with only 6% of the gold data, the semi-
supervised algorithm achieves much better results
than the semi-supervised algorithm when the same
amount of gold data is used. For LOC, ORG, and
PER, the recall increases 5.5, 16.8, and 8.2 respec-
tively, and the precision increases 2.4, 1.5, and 6.8
respectively. Even compared with the model trained
with 220,000 tokens, the semi-supervised learning
algorithm is better. Especially, for PER, the pre-
cision and recall increase 2.8 and 4.6 respectively.
Figure 2 illustrates how the classifier is improved at
each iteration in the semi-supervised learning algo-
rithm.
Table 6: Classification results. P/R represents Preci-
sion/Recall. The numbers inside the parentheses are the
result differences when the model trained from 60 docs is
used as baseline.
Training Data P/R(LOC) P/R(ORG) P/R(PER)
60 docs 88.1/85.6 86.0/64.2 74.5/81.2
700 docs 91.2/88.2 90.5/76.6 78.3/84.8
(3.1/3.6) (4.5/12.4) (3.8/3.6)
semi-supervised 90.5/91.1 87.5/81.0 81.1/89.4
(60 docs) (2.4/5.5) (1.5/16.8) (6.6/8.2)
1 2 3 4 5 6 7 8 9
82
83
84
85
Iteration
Ove
rall 
F?S
core
Figure 2: Overall F-score vs iteration numbers
Table 7 compares the results when the multi-
mention property is also used in testing as a high-
precision rule. Comparing Table 7 to Table 6, we
can see that with the same training data, using multi-
mention property helps improve classification re-
sults. However, this improvement is less than that
obtained by using this property to extract training
data thus improve the model itself. (For a fair com-
parison, the model used in the semi-supervised algo-
rithm in Table 6 only uses multi-mention property to
extract data.)
Our last experiment is to test how this method can
be used when the initial gold data and the testing
data are from different domains. We use the CoNLL
2003 English NER (Sang and Meulder, 2003) train-
ing set as the initial training data, and automatically
extract training data from the TF financial news cor-
pus. The CoNLL data is a collection of news wire
64
documents from the Reuters Corpus, while TF data
includes financial-related news only. Table 8 illus-
trates the results. As shown in the table, with only
CoNLL data, although it contains around 220,000
tokens, the results are not better than the results
when only 60 TF docs (Table 6) are used for train-
ing. This indicates that data from different domains
can adversely affect NER accuracy for supervised
learning. However, the semi-supervised algorithm
achieves reasonably high accuracy. For LOC, ORG,
and PER, the recall increases 16, 20.3, and 4.7 re-
spectively, and the precision increases 4.5, 5.5, and
4.7 respectively. Therefore our semi-supervised ap-
proach is effective for situation where the test and
training data are from different sources.
Table 7: Classification results when multi-mention prop-
erty (M) is used in testing
Trainig Data P/R(LOC) P/R(ORG) P/R(PER)
60 docs +M 89.9/87.6 82.4/71.4 78.2/87.3
700 docs+M 91.2/89.1 90.2/78.3 79.4/91.1
(1.3/1.5) (7.8/6.9) (1.2/3.8)
semi-supervised 90.0/91.0 86.6/82.4 81.3/90.6
+M (60 docs) (1.1/3.4) (4.2/11.0) (3.1/3.3)
Table 8: Classification results trained on CoNLL data and
test on TF data. Training data for the semi-supervised
algorithm are automatically extracted using both multi-
mention and high-precision context from TF corpus.
Training Data P/R(LOC) P/R(ORG) P/R(PER)
CoNLL 85.6/74.7 75.2/65.9 72.4/85.2
Semi-supervised 90.1/90.7 81.7/86.2 77.1/90.5
(CoNLL) (4.5/16) (5.5/20.3) (4.7/4.7)
6 Conclusion
We presented a simple semi-supervised learning al-
gorithm for NER using conditional random fields
(CRFs). In addition we proposed using high preci-
sion label features to improve classification accuracy
as well as to reduce training and test time.
Compared to other semi-supervised learning al-
gorithm, our proposed algorithm has several advan-
tages. It is domain and data independent. Although
it requires a small amount of labeled training data,
the data is not required to be from the same domain
as the one in which are interested to tag NEs. It can
be applied to different types of NEs as long as in-
dependent evidence exists, which is usually avail-
able. It is simple and, we believe not limited by the
choice of the classifier. Although we used CRFs in
our framework, other models can be easily incorpo-
rated to our framework as long as they provide accu-
rate confidence scores. With only a small amount of
training data, our algorithm can achieve a better NE
tagging accuracy than a supervised algorithm with a
large amount of training data.
References
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. Proceedings of the
Workshop on Computational Learning Theory, pages
92?100.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora.
A. Culotta and A. McCallum. 2004. Confidence estima-
tion for information extraction. HLT-NAACL.
G. D. Forney. 1973. The viterbi algorithm. Proceedings
of the IEEE, 61(3):268?278.
Feng Jiao, Shaojun Wang, Chi H. Lee, Russell Greiner,
and Dale Schuurmans. 2006. Semi-supervised condi-
tional random fields for improved sequence segmen-
tation and labeling. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics,
pages 209?216, July.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
CoNLL.
A.K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
Andrew McCallum. 2003. Efficiently inducing features
of conditional random fields.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In EMNLP.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Language-
independent named entity recognition. CoNLL, pages
142?147.
Yingchuan Wong and Hwee Tou Ng. 2007. One class
per named entity: Exploiting unlabeled text for named
entity recognition. IJCAI, pages 1763?1768.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Meeting of
the Association for Computational Linguistics, pages
189?196.
65
