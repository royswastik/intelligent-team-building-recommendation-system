A Unified Approach in Speech-to-Speech Translation: Integrating
Features of Speech Recognition and Machine Translation
Ruiqiang Zhang and Genichiro Kikui and Hirofumi Yamamoto
Taro Watanabe and Frank Soong and Wai Kit Lo
ATR Spoken Language Translation Research Laboratories
2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan
{ruiqiang.zhang, genichiro.kikui}@atr.jp
Abstract
Based upon a statistically trained speech
translation system, in this study, we try
to combine distinctive features derived from
the two modules: speech recognition and
statistical machine translation, in a log-
linear model. The translation hypotheses
are then rescored and translation perfor-
mance is improved. The standard trans-
lation evaluation metrics, including BLEU,
NIST, multiple reference word error rate
and its position independent counterpart,
were optimized to solve the weights of the
features in the log-linear model. The exper-
imental results have shown significant im-
provement over the baseline IBM model 4
in all automatic translation evaluation met-
rics. The largest was for BLEU, by 7.9%
absolute.
1 Introduction
Current translation systems are typically of a
cascaded structure: speech recognition followed
by machine translation. This structure, while
explicit, lacks some joint optimality in per-
formance since the speech recognition module
and translation module are running rather in-
dependently. Moreover, the translation module
of a speech translation system, a natural off-
spring of text-input based translation system,
usually takes a single-best recognition hypoth-
esis transcribed in text and performs standard
text-based translation. Lots of supplementary
information available from speech recognition,
such as N -best recognition recognition hypothe-
ses, likelihoods of acoustic and language models,
is not well utilized in the translation process.
The information can be effective for improving
translation quality if employed properly.
The supplementary information can be ex-
ploited by a tight coupling of speech recognition
and machine translation (Ney, 1999) or keeping
the cascaded structure unchanged but using an
integration model, log-linear model, to rescore
the translation hypotheses. In this study the
last approach was used due to its explicitness.
In this paper we intended to improve speech
translation by exploiting these information.
Moreover, a number of advanced features from
the machine translation module were also added
in the models. All the features from the speech
recognition and machine translation module
were combined by the log-linear models seam-
lessly.
In order to test our results broadly, we used
four automatic translation evaluation metrics:
BLEU, NIST, multiple word error rate and po-
sition independent word error rate, to measure
the translation improvement.
In the following, in section 2 we introduce the
speech translation system. In section 3, we de-
scribe the optimization algorithm used to find
the weight parameters in the log-linear model.
In section 4 we demonstrate the effectiveness
of our technique in speech translation experi-
ments. In the final two sections we discuss the
results and present our conclusions.
2 Feature-based Log-linear Models
in Speech Translation
The speech translation experimental system
used in this study illustrated in Fig. 1 is a typi-
cal, statistics-based one. It consists of two ma-
jor cascaded components: an automatic speech
recognition (ASR) module and a statistical ma-
chine translation (SMT) module. Additionally,
a third module, ?Rescore?, has been added to the
system and it forms a key component in the sys-
tem. Features derived from ASR and SMT are
combined in this module to rescore translation
candidates.
Without loss of its generality, in this paper
we use Japanese-to-English translation to ex-
plain the generic speech translation process. Let
X denote acoustic observations of a Japanese
X
utterance
recognized
text
target
translation
E
best
translation
J N1
ASR SMT
E
NxK
1
Rescore
Figure 1: Current framework of speech transla-
tion
utterance, typically a sequence of short-time
spectral vectors received at a frame rate of ev-
ery centi-second. It is first recognized as a
Japanese sentence, J . The recognized sentence
is then translated into a corresponding English
sentence, E.
The conversion from X to J is performed in
the ASR module. Based on Bayes? rule, P (J |X)
can be written as
P (J |X) = Pam(X|J)Plm(J)/P (X)
where Pam(X|J) is the acoustic model likeli-
hood of the observations given the recognized
sentence J ; Plm(J), the source language model
probability; and P (X), the probability of all
acoustic observations.
In the experiment we generated a set of N -
best hypotheses, JN1 = {J1, J2, ? ? ? , JN} 1 and
each Ji is determined by
Ji = arg maxJ??i
Pam(X|J)Plm(J)
where ?i is the set of all possible source sen-
tences excluding all higher ranked Jk?s, 1 ? k ?
i ? 1.
The conversion from J to E in Fig. 1 is
the machine translation process. According
to the statistical machine translation formal-
ism (Brown et al, 1993), the translation process
is to search for the best sentence E? such that
E? = arg max
E
P (E|J) = arg max
E
P (J |E)P (E)
where P (J |E) is a translation model charac-
terizing the correspondence between E and J ;
P (E), the English language model probability.
In the IBM model 4, the translation model
P (J |E) is further decomposed into four sub-
models:
? Lexicon Model ? t(j|e): probability of a
word j in the Japanese language being
translated into a word e in the English lan-
guage.
1Hereafter, J1 is called the single-best hypothesis of
speech recognition; JN1 , the N -best hypotheses.
? Fertility model ? n(?|e): probability of
a English language word e generating ?
words.
? Distortion model ? d: probability of distor-
tion, which is decomposed into the distor-
tion probabilities of head words and non-
head words.
? NULL translation model ? p1: a fixed prob-
ability of inserting a NULL word after de-
termining each English word.
In the above we listed seven features: two
from ASR (Pam(X|J), Plm(J)) and five from
SMT (P (E), t(j|e), n(?|e), d, p1).
The third module in Fig. 1 is to rescore trans-
lation hypotheses from SMT by using a feature-
based log-linear model. All translation can-
didates output through the speech recognition
and translation modules are re-evaluated by us-
ing all relevant features and searching for the
best translation candidate of the highest score.
The log-linear model used in our speech trans-
lation process, P (E|X), is
P?(E|X) =
exp(?Mi=1 ?ifi(X,E))?
E? exp(
?M
i=1 ?ifi(X,E?))
? = {?M1 }
(1)
In the Eq. 1, fi(X,E) is the logarithm value
of the i-th feature; ?i is the weight of the i-
th feature. Integrating different features in the
equation results in different models. In the ex-
periments performed in section 4, four different
models will be trained by increasing the number
of features successively to investigate the effect
of different features for improving speech trans-
lation.
In addition to the above seven features, the
following features are also incorporated.
? Part-of-speech language models: English
part-of-speech language models were used.
POS dependence of a translated English
sentence is an effective constraint in prun-
ing English sentence candidates. In our ex-
periments 81 part-of-speech tags and a 5-
gram POS language model were used.
? Length model P (l|E, J): l is the length
(number of words) of a translated English
sentence.
? Jump weight: Jump width for adjacent
cepts in Model 4 (Marcu and Wong, 2002).
? Example matching score: The translated
English sentence is matched with phrase
translation examples. A score is derived
based on the count of matches (Watanabe
and Sumita, 2003).
? Dynamic example matching score: Similar
to the example matching score but phrases
were extracted dynamically from sentence
examples (Watanabe and Sumita, 2003).
Altogether, we used M(=12) different fea-
tures. In section 3, we review Powell?s algo-
rithm (Press et al, 2000) as our tool to opti-
mize model parameters, ?M1 , based on different
objective translation metrics.
3 Parameter Optimization Based
on Translation Metrics
The denominator in Eq. 1 can be ignored since
the normalization is applied equally to every hy-
pothesis. Hence, the choice of the best transla-
tion, E?, out of all possible translations, E, is
independent of the denominator,
E? = arg max
E
M?
i=1
?ilogPi(X,E) (2)
where we write features, fi(X,E), explicitly in
logarithm, logPi(X,E).
The effectiveness of the model in Eq. 2 de-
pends upon the parameter optimization of the
parameter set ?M1 , with respect to some objec-
tively measurable but subjectively relevant met-
rics.
Suppose we have L speech utterances and
for each utterance, we generate N best speech
recognition hypotheses. For each recogni-
tion hypothesis, K English language transla-
tion hypotheses are generated. For the l-th
input speech utterance, there are then Cl =
{El1 , ? ? ? , ElN?K} translations. All L speech ut-
terances generate L?N?K translations in to-
tal.
Our goal is to minimize the translation ?dis-
tortion? between the reference translations, R,
and the translated sentences, E? .
?M1 = optimize D(E? ,R) (3)
where E? = {E?1, ? ? ? , E?L} is a set of translations
of all utterances. The translation E?l of the l-
th utterance is produced by the (Eq. 2), where
E ? Cl.
Let R = {R1, ? ? ? , RL} be the set of transla-
tion references for all utterances. Human trans-
lators paraphrased 16 reference sentences for
each utterance, i.e., Rl contains 16 reference
candidates for the l-th utterance.
D(E? ,R) is a translation ?distortion? or an
objective translation assessment. The following
four metrics were used specifically in this study:
? BLEU (Papineni et al, 2002): A weighted
geometric mean of the n-gram matches be-
tween test and reference sentences multi-
plied by a brevity penalty that penalizes
short translation sentences.
? NIST : An arithmetic mean of the n-gram
matches between test and reference sen-
tences multiplied by a length factor which
again penalizes short translation sentences.
? mWER (Niessen et al, 2000): Multiple ref-
erence word error rate, which computes the
edit distance (minimum number of inser-
tions, deletions, and substitutions) between
test and reference sentences.
? mPER: Multiple reference position inde-
pendent word error rate, which computes
the edit distance without considering the
word order.
The BLEU score and NIST score are calcu-
lated by the tool downloadable 2.
Because the objective function in the model
(Eq. 3) is not smoothed function, we used Pow-
ell?s search method to find a solution. The Pow-
ell?s algorithm used in this work is similar as the
one from (Press et al, 2000) but we modified the
line optimization codes, a subroutine of Powell?s
algorithm, with reference to (Och, 2003).
Finding a global optimum is usually difficult
in a high dimensional vector space. To make
sure that we had found a good local optimum,
we restarted the algorithm by using various ini-
tializations and used the best local optimum as
the final solution.
4 Experiments
4.1 Corpus & System
The data used in this study was the Basic
Travel Expression Corpus (BTEC) (Kikui et al,
2003), consisting of commonly used sentences
listed in travel guidebooks and tour conversa-
tions. The corpus were designed for developing
multiple language speech-to-speech translation
systems. It contains four different languages:
Chinese, Japanese, Korean and English. Only
Japanese-English parallel data was used in this
2http://www.nist.gov/speech/tests/mt/
Table 1: Training, development and test data
from Basic Travel Expression Corpus(BTEC)
Japanese English
Train Sentences 162,318
Words 1,288,767 949,377
Dev. Sentences 510
Words 4015 2983
Test Sentences 508
Words 4112 2951
study. The speech data was recorded by multi-
ple speakers and was used to train the acoustic
models, while the text database was used for
training the language and translation models.
The standard BTEC training corpus, the first
file and the second file from BTEC standard test
corpus #01 were used for training, development
and test respectively. The statistics of corpus is
shown in table 1.
The speech recognition engine used in the ex-
periments was an HMM-based, large vocabu-
lary continuous speech recognizer. The acoustic
HMMs were triphone models with 2,100 states
in total, using 25 dimensional, short-time spec-
trum features. In the first and second pass of
decoding, a multiclass word bigram of a lexicon
of 37,000 words plus 10,000 compound words
was used. A word trigram was used in rescor-
ing the results.
The machine translation system is a graph-
based decoder (Ueffing et al, 2002). The first
pass of the decoder generates a word-graph, a
compact representation of alternative transla-
tion candidates, using a beam search based on
the scores of the lexicon and language mod-
els. In the second pass an A* search traverses
the graph. The edges of the word-graph, or
the phrase translation candidates, are gener-
ated by the list of word translations obtained
from the inverted lexicon model. The phrase
translations extracted from the Viterbi align-
ments of the training corpus also constitute the
edges. Similarly, the edges are also created from
dynamically extracted phrase translations from
the bilingual sentences (Watanabe and Sumita,
2003). The decoder used the IBM Model 4
with a trigram language model and a 5-gram
part-of-speech language model. The training of
IBM model 4 was implemented by the GIZA++
package (Och and Ney, 2003).
4.2 Model Training
In order to quantify translation improvement by
features from speech recognition and machine
translation respectively, we built four log-linear
models by adding features successively. The
four models are:
? Standard translation model(stm): Only
features from the IBM model 4 (M=5) de-
scribed in section 2 were used in the log-
linear models. We did not perform parame-
ter optimization on this model. It is equiv-
alent to setting all the ?M1 to 1. This model
was the standard model used in most sta-
tistical machine translation system. It is
referred to as the baseline model.
? Optimized standard translation models
(ostm): This model consists of the same
features as the previous model ?stm? but
the parameters were optimized by Powell?s
algorithm. We intended to exhibit the ef-
fect of parameter optimization by compar-
ing this model with the baseline ?stm?.
? Optimized enhanced translation models
(oetm): We incorporated additional trans-
lation features described in section 2 to
enrich the model ?ostm?. In this model
the number of the total features, M , is 10.
Model parameters were optimized. We in-
tended to show how much the enhanced
features can improve translation quality.
? Optimized enhanced speech translation
models (oestm): Features from speech
recognition, likelihood scores of acoustic
and language models, were incorporated
additionally into the model ?oetm?. All
the 12 features described in section 2 were
used. Model parameters were optimized.
To optimize ? parameters of the log-linear
models, we used the development data of 510
speech utterances. We adopted an N -best
hypothesis approach (Och, 2003) to train ?.
For each input speech utterance, N?K candi-
date translations were generated, where N is
the number of generated recognition hypothe-
ses and K is the number of translation hypothe-
ses. A vector of dimension M , corresponding to
multiple features used in the translation model,
was generated for each translation candidate.
The Powell?s algorithm was used to optimize
these parameters. We used a large K to ensure
that promising translation candidates were not
Table 2: Comparisons of single-best and N -best
hypotheses of speech recognition performance
in terms of word accuracy, sentence accuracy,
insertion, deletion and substitution error rates
word sent ins del sub
acc(%) acc(%) (%) (%) (%)
single-best 93.5 78.7 2.0 0.8 3.6
N -best 96.1 87.0 1.2 0.3 2.2
pruned out. In the training, we set N=100 and
K=1, 000.
By using different objective translation eval-
uation metrics described in section 3, for each
model we obtained four sets of optimized pa-
rameters with respect to BLEU, NIST, mWER
and mPER metrics, respectively.
4.3 Translation Improvement by
Additional Features
All 508 utterances in the test data were used to
evaluate the models. Similar to processing the
development data, the speech recognizer gen-
erated N -best (N=100) recognition hypothe-
ses for each test speech utterance. Table 2
shows speech recognition results of the test data
set in single-best and N -best hypotheses. We
observed that over 8% sentence accuracy im-
provement was obtained from the single-best to
the N -best recognition hypotheses. The recog-
nized sentences were then translated into corre-
sponding English sentences. 1,000 such trans-
lation candidates were produced for each recog-
nition hypothesis. These candidates were then
rescored by each of the four models with four
sets of optimized parameters obtained in the
training respectively. The candidates with the
best score were chosen.
The best translations generated by a model
were evaluated by the translation assessment
metrics used to optimize the model parameters
in the development. The experimental results
are shown in Table 3.
In the experiments we changed the number
of speech recognition hypotheses, N , to see how
translation performance is changed as N . We
found that the best translation was achieved
when a relatively smaller set of hypotheses,
N=5, was used. Hence, the values in Table 3
were obtained when N was set to 5.
We test each model by employing the single-
best recognition hypothesis translations and
the N -best recognition hypothesis translations.
Table 3: Translation improvement from the
baseline model(stm) to the optimized enhanced
speech translation model(oestm): Models are
optimized using the same metric as shown in
the columns. Numbers are in percentage except
NIST score.
BLEU NIST mWER mPER
Single-best recognition hypothesis translation
stm 54.2 7.5 39.8 34.8
ostm 59.0 8.9 36.2 34.0
oetm 59.2 9.9 34.3 31.5
N -best recognition hypothesis translation
stm 55.5 7.3 39.8 35.4
ostm 61.1 8.8 36.4 33.9
oetm 61.1 10.0 34.0 31.1
oestm 62.1 10.2 33.7 29.4
The single-best translation was from the trans-
lation of the single best hypotheses of the speech
recognition and the N -best hypothesis trans-
lation was from the translations of all the hy-
potheses produced by speech recognition.
In Table 3, we observe that a large improve-
ment is achieved from the baseline model ?stm?
to the final model ?oestm?. The BLEU, NIST,
mWER, mPER scores are improved by 7.9%,
2.7, 6.1%, 5.4% respectively. Note that a high
value of BLEU and NIST score means a good
translation while a worse translation for mWER
and mPER. Consistent performance improve-
ment was achieved in the single-best and N -
best recognition hypotheses translations. We
observed that the improvement were due to the
following reasons:
? Optimization. Models with optimized pa-
rameters yielded a better translation than
the models with unoptimized parameters.
It can be seen by comparing the model
?stm? with the model ?ostm? for both the
single-best and the N -best results.
? N -best recognition hypotheses. In major-
ity of the cells in Table 3, translation per-
formance of the N -best recognition is bet-
ter than of the corresponding single-best
recognition. N -best BLEU score of ?ostm?
improved over the single-best of ?ostm? by
2.1%. However, NIST score is indifferent
to the change. It appears that NIST score
is insensitive to detect slight translation
changes.
Table 4: Translation improvement of incorrectly
recognized utterances from single-best(oetm) to
N -best(oestm)
BLEU NIST mWER mPER
single-best 29.0 6.1 59.7 51.8
N -best 36.3 7.2 54.4 47.9
? Enhanced features. Translation perfor-
mance is improved steadily when more fea-
tures are incorporated into the log-linear
models. Translation performance of model
?oetm? is better than model ?ostm? be-
cause more effective translation features
are used. Model ?oestm? is better than
model ?oetm? due to its enhanced speech
recognition features. It confirms that our
approach to integrate features from speech
recognition and translation features works
very well.
4.4 Recognition Improvement of
Incorrectly Recognized Sentences
In previous experiments we demonstrated that
speech translation performance was improved
by the proposed enhanced speech translation
model ?oestm?. In this section we want to show
that this improvement is because of the signifi-
cant improvement of incorrectly recognized sen-
tences when N -best recognition hypotheses are
used.
We carried out the following experiments.
Only incorrectly recognized sentences were ex-
tracted for translation and re-scored by the
model ?oetm? for the single-best case and the
model ?oestm? for the N -best case. The trans-
lation results are shown in Table 4. Translation
of incorrectly recognized sentences are improved
significantly as shown in the table.
Because we used N -best recognition hypothe-
ses, the log-linear model chose the recogni-
tion hypothesis among the N hypotheses which
yielded the best translation. As a result, speech
recognition could be improved if the higher ac-
curate recognition hypotheses was chosen for
translation. This effect can be observed clearly
if we extracted the chosen recognition hypothe-
ses of incorrectly recognized sentences. Table 5
shows the word accuracy and sentence accuracy
of the recognition hypotheses selected by the
translation module. The sentence accuracy of
incorrectly recognized sentences was improved
by 7.5%. The word accuracy was also improved.
Table 5: Recognition accuracy of incorrectly
recognized utterance improved by N -best hy-
pothesis translation.
word acc. (%) sent. acc. (%)
single-best 74.6 0
N -best BLEU 76.4 7.5
mWER 75.9 6.5
5 Discussions
As regards to integrating speech recognition
with translation, a coupling structure (Ney,
1999) was proposed as a speech translation in-
frastructure that multiplies acoustic probabili-
ties with translation probabilities in a one-step
decoding procedure. But no experimental re-
sults have been given on whether and how this
coupling structure improved speech translation.
(Casacuberta et al, 2002) used a finite-state
transducer where scores from acoustic infor-
mation sources and lexicon translation models
were integrated together. Word pairs of source
and target languages were tied in the decoding
graph. However, this method was only tested
for a pair of similar languages, i.e., Spanish to
English. For translating between languages of
different families where the syntactic structures
can be quite different, like Japanese and En-
glish, rigid tying of word pair still remains to be
shown its effectiveness for translation.
Our approach is rather general, easy to imple-
ment and flexible to expand. In the experiments
we incorporated features from acoustic models
and language models. But this framework is
flexible to include more effective features. In-
deed, the proposed speech translation paradigm
of log-linear models have been shown effective in
many applications (Beyerlein, 1998) (Vergyri,
2000) (Och, 2003).
In order to use speech recognition features,
the N -best speech recognition hypotheses were
needed. Using N -best could bear computing
burden. However, our experiments have shown
a smaller N seems to be adequate to achieve
most of the translation improvement without
significant increasing of computations.
6 Conclusion
In this paper we presented our approach of in-
corporating both speech recognition and ma-
chine translation features into a log-linear
speech translation model to improve speech
translation.
Under this new approach, translation perfor-
mance was significantly improved. The perfor-
mance improvement was confirmed by consis-
tent experimental results and measured by us-
ing various objective translation metrics. In
particular, BLEU score was improved by 7.9%
absolute.
We show that features derived from speech
recognition: likelihood of acoustic and language
models, helped improve speech translation. The
N -best recognition hypotheses are better than
the single-best ones when they are used in trans-
lation. We also show that N -best recogni-
tion hypothesis translation can improve speech
recognition accuracy of incorrectly recognized
sentences.
The success of the experiments owes to the
use of statistical machine translation and log-
linear models so that various of effective fea-
tures can be jointed and balanced to output the
optimal translation results.
Acknowledgments
We would like to thank for assistance from Ei-
ichiro Sumita, Yoshinori Sagisaka, Seiichi Ya-
mamoto and the anonymous reviewers.
The research reported here was supported in
part by a contract with the National Institute
of Information and Communications Technol-
ogy of Japan entitled ?A study of speech dia-
logue translation technology based on a large
corpus?.
References
Peter Beyerlein. 1998. Discriminative model
combination. In Proc.of ICASSP?1998, vol-
ume 1, pages 481?484.
Peter F. Brown, Vincent J. Della Pietra,
Stephen A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical
machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Francisco Casacuberta, Enrique Vidal, and
Juan M. Vilar. 2002. Architectures for
speech-to-speech translation using finite-state
models. In Proc. of speech-to-speech trans-
lation workshop, pages 39?44, Philadelphia,
PA, July.
Genichiro Kikui, Eiichiro Sumita, Toshiyuki
Takezawa, and Seiichi Yamamoto. 2003. Cre-
ating corpora for speech-to-speech transla-
tion. In Proc.of EUROSPEECH?2003, pages
381?384, Geneva.
Daniel Marcu and William Wong. 2002. A
phrase-based, joint probability model for sta-
tistical machine translation. In Proc. of
EMNLP-2002, Philadelphia, PA, July.
Hermann Ney. 1999. Speech translation: Cou-
pling of recognition and translation. In Proc.
of ICASSP?1999, volume 1, pages 517?520,
Phoenix, AR, March.
Sonja Niessen, Franz J. Och, Gregor Leusch,
and Hermann Ney. 2000. An evaluation tool
for machine translation: Fast evaluation for
machine translation research. In Proc.of the
LREC (2000), pages 39?45, Athens, Greece,
May.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguis-
tics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In
Proc. of ACL?2003, pages 160?167.
Kishore A. Papineni, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu: A
method for automatic evaluation of machine
translation. In Proc. of ACL?2002, pages
311?318, Philadelphia, PA, July.
William H. Press, Saul A. Teukolsky,
William T. Vetterling, and Brian P. Flan-
nery. 2000. Numerical Recipes in C++.
Cambridge University Press, Cambridge,
UK.
Nicola Ueffing, Franz Josef Och, and Hermann
Ney. 2002. Generation of word graphs in sta-
tistical machine translation. In Proc. of the
Conference on Empirical Methods for Natu-
ral Language Processing (EMNLP02), pages
156?163, Philadelphia, PA, July.
Dimitra Vergyri. 2000. Use of word level side
information to improve speech recognition. In
Proc. of the IEEE International Conference
on Acoustics, Speech and Signal Processing,
2000.
Taro Watanabe and Eiichiro Sumita. 2003.
Example-based decoding for statistical ma-
chine translation. In Machine Translation
Summit IX, pages 410?417, New Orleans,
Louisiana.
	

		
	
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 265?268,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Automatic Story Segmentation using a Bayesian Decision Framework 
for Statistical Models of Lexical Chain Features 
 
 Wai-Kit Lo Wenying Xiong Helen Meng 
 The Chinese University The Chinese University The Chinese University 
 of Hong Kong, of Hong Kong, of Hong Kong, 
 Hong Kong, China Hong Kong, China Hong Kong, China  
 wklo@se.cuhk.edu.hk wyxiong@se.cuhk.edu.hk hmmeng@se.cuhk.edu.hk 
  
 
Abstract 
This paper presents a Bayesian decision 
framework that performs automatic story 
segmentation based on statistical model-
ing of one or more lexical chain features. 
Automatic story segmentation aims to lo-
cate the instances in time where a story 
ends and another begins. A lexical chain 
is formed by linking coherent lexical 
items chronologically. A story boundary 
is often associated with a significant 
number of lexical chains ending before it, 
starting after it, as well as a low count of 
chains continuing through it. We devise a 
Bayesian framework to capture such be-
havior, using the lexical chain features of 
start, continuation and end. In the scoring 
criteria, lexical chain starts/ends are 
modeled statistically with the Weibull 
and uniform distributions at story boun-
daries and non-boundaries respectively. 
The normal distribution is used for lexi-
cal chain continuations. Full combination 
of all lexical chain features gave the best 
performance (F1=0.6356). We found that 
modeling chain continuations contributes 
significantly towards segmentation per-
formance. 
1 Introduction 
Automatic story segmentation is an important 
precursor in processing audio or video streams in 
large information repositories. Very often, these 
continuous streams of data do not come with 
boundaries that segment them into semantically 
coherent units, or stories. The story unit is 
needed for a wide range of spoken language in-
formation retrieval tasks, such as topic tracking, 
clustering, indexing and retrieval. To perform 
automatic story segmentation, there are three 
categories of cues available: lexical cues from 
transcriptions, prosodic cues from the audio 
stream and video cues such as anchor face and 
color histograms. Among the three types of cues, 
lexical cues are the most generic since they can 
work on text and multimedia sources. Previous 
approaches include TextTiling (Hearst 1997) that 
monitors changes in sentence similarity, use of 
cue phrases (Reynar 1999) and Hidden Markov 
Models (Yamron 1998). In addition, the ap-
proach based on lexical chaining captures the 
content coherence by linking coherent lexical 
items (Morris and Hirst 1991, Hirst and St-Onge 
1998). Stokes (2004) discovers boundaries by 
chaining up terms and locating instances of time 
where the count of chain starts and ends (boun-
dary strength) achieves local maxima. Chan et al 
(2007) enhanced this approach through statistical 
modeling of lexical chain starts and ends. We 
further extend this approach in two aspects: 1) a 
Bayesian decision framework is used; 2) chain 
continuations straddling across boundaries are 
taken into consideration and statistically modeled. 
2 Experimental Setup 
Experiments are conducted using data from the 
TDT-2 Voice of America Mandarin broadcast. 
In particular, we only use the data from the long 
programs (40 programs, 1458 stories in total), 
each of which is about one hour in duration.  The 
average number of words per story is 297. The 
news programs are further divided chronologi-
cally into training (for parameter estimation of 
the statistical models), development (for tuning 
decision thresholds) and test (for performance 
evaluation) sets, as shown in Figure 1. Automatic 
speech recognition (ASR) outputs that are pro-
vided in the TDT-2 corpus are used for lexical 
chain formation. 
265
The story segmentation task in this work is to 
decide whether a hypothesized utterance boun-
dary (provided in the TDT-2 data based on the 
speech recognition result) is a story boundary. 
Segmentation performance is evaluated using the 
F1-measure. 
20 hour 10 hour 10 hour
Feb.20th,1998 Mar.4th,1998 Mar.17th,1998 Apr.4th,1998
Training Set Development Set Test Set
697 stories 385 stories 376 stories
 
Figure 1: Organization of the long programs in TDT-2 
VOA Mandarin for our experiments. 
3 Approach 
Our approach considers utterance boundaries that 
are labeled in the TDT-2 corpus and classifies 
them either as a story boundary or non-boundary. 
We form lexical chains from the TDT-2 ASR 
outputs by linking repeated words. Since words 
may also repeat across different stories, we limit 
the maximum distance between consecutive 
words within the lexical chain. This limit is op-
timized according to the approach in (Chan et al 
2007) based on the training data. The optimal 
value is found to be 130.9sec for long programs. 
We make use of three lexical chain features: 
chain starts, continuations and ends. At the be-
ginning of a story, new words are introduced 
more frequently and hence we observe many lex-
ical chain starts. There is also tendency of many 
lexical chains ending before a story ends. As a 
result, there is a higher density of chain starts and 
ends in the proximity of a story boundary. Fur-
thermore, there tends to be fewer chains strad-
dling across a story boundary. Based on these 
characteristics of lexical chains, we devise a sta-
tistical framework for story segmentation by 
modeling the distribution of these lexical chain 
features near the story boundaries. 
3.1 Story Segmentation based on a Single 
Lexical Chain Feature 
Given an utterance boundary with the lexical 
chain feature, X, we compare the conditional 
probabilities of observing a boundary, B, or non-
boundary, B , as  
 <> )|()|( XBPXBP . (1) 
where X is a single chain feature, which may be 
the chain start (S), chain continuation (C), or 
chain end (E). 
By applying the Bayes? theorem, this can be 
rewritten as a likelihood ratio test, 
 
B
x
XP
BXP ?
)|(
)|( <>
 
(2) 
for which the decision threshold 
is )(/)( BPBPx =? , dependent on the a priori 
probability of observing boundary or a non-
boundary. 
3.2 Story Segmentation based on Combined 
Chain Features 
When multiple features are used in combination, 
we formulate the problem as  
 ),,|(),,|( CESBPCESBP <> . (3) 
By assuming that the chain features are condi-
tionally independent of one another (i.e., 
P(S,C,E|B) = P(S|B) P(C|B) P(E|B)), the formu-
lation can be rewritten as a likelihood ratio test 
 
<> SECBCPBEPBSP
BCPBEPBSP ?
)|()|()|(
)|()|()|(
.
 
(4) 
4 Modeling of Lexical Chain Features 
4.1 Chain starts and ends 
We follow (Chan et al 2007) to model the lexi-
cal chain starts and ends at a story boundary with 
a statistical distribution. We apply a window 
around the candidate boundaries (same window 
size for both chain starts and ends) in our work. 
Chain features falling outside the window are 
excluded from the model. Figure 2 shows the 
distribution when a window size of 20 seconds is 
used. This is the optimal window size when 
chain start and end features are combined. 
0-2-4-6-8-10-12-14-16-18-20 2 4 6 8 10 12 14 16 18 20
10
20
30
40
50
Offset from story boundary in second
Number of lexical chain features
Fitted Weibull dist. for 
lexical chain ends
Frequency of lexical 
chain features
Fitted Weibull dist. for 
lexical chain starts
x
 
Figure 2: Distribution of chain starts and ends at 
known story boundaries. The Weibull distribution is 
used to model these distributions. 
We also assume that the probability of seeing 
a lexical chain start / end at a particular instance 
is independent of the starts / ends of other chains. 
As a result, the probability of seeing a sequence 
of chain starts at a story boundary is given by the 
product of a sequence of Weibull distributions 
 
?
=
??
???
???
??
???
?=
s
k
iN
i
tk
i e
tk
BSP
1
1
)|( ??? , (5) 
266
where S is the sequence of time with chain starts 
(S=[t1, t2, ? ti, ? tNs]), ks is the shape, ?s is the 
scale for the fitted Weibull distribution for chain 
starts, Ns is the number of chain starts. The same 
formulation is similarly applied to chain ends. 
Figure 3 shows the frequency of raw feature 
points for lexical chain starts and ends near utter-
ance boundaries that are non-story boundaries. 
Since there is no obvious distribution pattern for 
these lexical chain features near a non-story 
boundary, we model these characteristics with a 
uniform distribution. 
 
2 4 6 8 10 12 14 16
0.02
0.04
0.06
0.08
0-2-4-6-8-10-12-14-16
0.1
Relative frequency of chain starts / ends
Offset from utterance boundary in seconds
(non-story boundaries only)
Lexical chain starts / ends
Fitted uniform dist. for 
lexical chain starts
x
Fitted uniform dist. for 
lexical chain ends
 
Figure 3: Distribution of chain starts and ends at ut-
terance boundaries that are non-story boundaries. 
4.2 Chain continuations 
Figure 4 shows the distributions of chain contin-
uations near story boundary and non-story boun-
dary. As one may expect, there are fewer lexical 
chains that straddle across a story boundary (the 
curve of )|( BCP ) when compared to a non-story 
boundary (the curve of )|( BCP ). Based on the 
observations, we model the probability of occur-
rence of lexical chains straddling across a given 
story boundary or non-story boundary by a nor-
mal distribution. 
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
P
ro
ba
bi
lit
y
0 5 10 15 20 25
Number of chain continuations straddling across an 
utterance boundary
Story boundary, )|( BCP
Non-story boundary, )|( BCP
Relative frequency of lexical chain 
continuation at an utterance boundaryx
Fitted distribution at story boundary
Fitted distribution at non-story boundary
P
ro
ba
bi
lit
y
 
Figure 4: Distributions of chain continuations at story 
boundaries and non-story boundaries. 
5 Story Segmentation based on Combi-
nation of Lexical Chain Features 
We trained the parameters of the Weibull distri-
bution for lexical chain starts and ends at story 
boundaries, the uniform distribution for lexical 
chain start / end at non-story boundary, and the 
normal distribution for lexical chain continua-
tions. Instead of directly using a threshold as 
shown in Equation (2), we optimize on the para-
meter n, which is the optimal number of top scor-
ing utterance boundaries that are classified as 
story boundaries in the development set. 
5.1 Using Bayesian decision framework 
We compare the performance of the Bayesian 
decision framework to the use of likelihood only 
P(X|B) as shown in Figure 5. The results demon-
strate consistent improvement in F1-measure 
when using the Bayesian decision framework. 
0
0.2
0.4
0.6
F1
-m
ea
su
re
)|( BSP )|( BEP
)|(
)|(
BSP
BSP
)|(
)|(
BEP
BEP
F1
-m
ea
su
re
 
Figure 5: Story segmentation performance in F1-
measure when using single lexical chain features. 
5.2 Modeling multiple features jointly 
0
0.2
0.4
0.6
0.8
F1
-m
ea
su
re
(a) (b) (c) (d) (e) (f) (g) (h)
)|(
)|(
(c)
BEP
BEP
)|(
)|(
(d)
BCP
BCP
)|()|(
)|()|(
(e)
BEPBSP
BEPBSP
)|()|(
)|()|(
(f)
BCPBSP
BCPBSP
)|()|(
)|()|(
(g)
BCPBEP
BCPBEP
)|()|()|(
)|()|()|(
(h)
BCPBEPBSP
BCPBEPBSP
)|(
)|(
(b)
BSP
BSP
]2007[),(core (a) ChanESS
F1
-m
ea
su
re
 
Figure 6: Results of F1-measure comparing the seg-
mentation results using different statistical models of 
lexical chain features. 
We further compare the performance of various 
scoring methods including single and combined 
lexical chain features. The baseline result is ob-
tained using a scoring function based on the like-
lihoods of seeing a chain start or end at a story 
boundary (Chan et al 2007) which is denoted as 
Score(S, E). Performance from other methods 
based on the same dataset can be referenced from 
Chan et al 2007 and will not be repeated here. 
The best story segmentation performance is 
achieved by combining all lexical chain features 
which achieves an F1-measure of 0.6356. All 
improvements have been verified to be statisti-
cally significant (?=0.05). By comparing the re-
sults of (e) to (h), (c) to (g), and (b) to (f), we can 
see that lexical chain continuation feature contri-
butes significantly and consistently towards story 
segmentation performance. 
267
5.3 Analysis 
Utterance boundary
(occurs at 664 second in document VOM19980317_0900_1000, 
which is not a story boundary)
time
5 10-5-10
11 chain continuations:
W1[??], W2[??], W3[??], W4[???], W5[??],
W6[??], W7[??], W8[??], W9[??], W10[??], W11[??]
15-15
W 1
5
[??
]
W 1
6
[??
]
W 1
7
[??
?]
W 1
8
[??
]
W 1
9
[??
]
W 2
0
[??
]
W 2
1
[??
]
W 1
2
[??
]
W 1
3
[??
]
W 1
4
[??
]
ts1 ts2 ts3 ts4 ts5 ts6 ts7te1te2te3
 
Figure 7: Lexical chain starts, ends and continuations 
in the proximity of a non-story boundary. Wi[xxxx] 
denotes the i-th Chinese word ?xxxx?. 
Figure 7 shows an utterance boundary that is a 
non-story boundary. There is a high concentra-
tion of chain starts and ends near the boundary 
which leads to a misclassification if we only 
combine chain starts and ends for segmentation. 
However, there are also a large number of chain 
continuations across the utterance boundary, 
which implies that a story boundary is less likely. 
The full combination gives the correct decision. 
Utterance boundary
(occurs at 2014 second in document 
VOM19980319_0900_1000, which is a story boundary)
time
10 201020
ts1 ts3te4te5te6 te1te2te3 ts2
6 chain continuations:
W1[???], W2[??], W3[???],
W4[??], W5[??, W6[??]
W 1
3
[?
??
??
]
W 1
4
[?
??
?]
W 1
5
[?
?]
W 1
2
[?
?]
W 1
1
[?
?]
W 1
0
[?
??
]
W 9
[?
?]
W 8
[?
?]
W 7
[?
?]
 
Figure 8: Lexical chain starts, ends and continuations 
in the proximity of a story boundary. 
Figure 8 shows another example where an ut-
terance boundary is misclassified as a non-story 
boundary when only the combination of lexical 
chain starts and ends are used. Incorporation of 
the chain continuation feature helps rectify the 
classification. 
From these two examples, we can see that the 
incorporation of chain continuation in our story 
segmentation framework can complement the 
features of chain starts and ends.  In both exam-
ples above, the number of chain continuations 
plays a crucial role in correct identification of a 
story boundary. 
6 Conclusions 
We have presented a Bayesian decision frame-
work that performs automatic story segmentation 
based on statistical modeling of one or more lex-
ical chain features, including lexical chain starts, 
continuations and ends. Experimentation shows 
that the Bayesian decision framework is superior 
to the use of likelihoods for segmentation. We 
also experimented with a variety of scoring crite-
ria, involving likelihood ratio tests of a single 
feature (i.e. lexical chain starts, continuations or 
ends), their pair-wise combinations, as well as 
the full combination of all three features. Lexical 
chain starts/ends are modeled statistically with 
the Weibull and normal distributions for story 
boundaries and non-boundaries. The normal dis-
tribution is used for lexical chain continuations. 
Full combination of all lexical chain features 
gave the best performance (F1=0.6356). Model-
ing chain continuations contribute significantly 
towards segmentation performance. 
Acknowledgments 
This work is affiliated with the CUHK MoE-
Microsoft Key Laboratory of Human-centric Compu-
ting and Interface Technologies. We would also like 
to thank Professor Mari Ostendorf for suggesting the 
use of continuing chains and Mr. Kelvin Chan for 
providing information about his previous work. 
References  
Chan, S. K. et al 2007. ?Modeling the Statistical Be-
haviour of Lexical Chains to Capture Word Cohe-
siveness for Automatic Story Segmentation?, Proc. 
of INTERSPEECH-2007.  
Hearst, M. A. 1997. ?TextTiling: Segmenting Text 
into Multiparagraph Subtopic Passages?, Computa-
tional Linguistics, 23(1), pp. 33?64. 
Hirst, G. and St-Onge, D. 1998. ?Lexical chains as 
representations of context for the detection and 
correction of malapropisms?, WordNet: An Elec-
tronic Lexical Database, pp. 305?332. 
Morris, J. and Hirst, G. 1991. ?Lexical cohesion com-
puted by thesaural relations as an indicator of the 
structure of text?, Computational Linguistics, 
17(1), pp. 21?48. 
Reynar, J.C. 1999, ?Statistical models for topic seg-
mentation?, Proc. 37th annual meeting of the ACL, 
pp. 357?364. 
Stokes, N. 2004. Applications of Lexical Cohesion 
Analysis in the Topic Detection and Tracking Do-
main, PhD thesis, University College Dublin.  
Yamron, J.P. et al 1998, ?A hidden Markov model 
approach to text segmentation and event tracking?, 
Proc. ICASSP 1998, pp. 333?336. 
268
