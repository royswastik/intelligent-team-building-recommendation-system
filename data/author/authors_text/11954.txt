Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 19?27,
Suntec, Singapore, 7 August 2009.
c?2009 ACL and AFNLP
Wiktionary and NLP: Improving synonymy networks
Emmanuel Navarro
IRIT, CNRS &
Universit? de Toulouse
navarro@irit.fr
Franck Sajous
CLLE-ERSS, CNRS &
Universit? de Toulouse
sajous@univ-tlse2.fr
Bruno Gaume
CLLE-ERSS & IRIT, CNRS &
Universit? de Toulouse
gaume@univ-tlse2.fr
Laurent Pr?vot
LPL, CNRS &
Universit? de Provence
laurent.prevot@lpl-aix.fr
Hsieh ShuKai
English Department
NTNU, Taiwan
shukai@gmail.com
Kuo Tzu-Yi
Graduate Institute of Linguistics
NTU, Taiwan
tzuyikuo@ntu.edu.tw
Pierre Magistry
TIGP, CLCLP, Academia Sinica,
GIL, NTU, Taiwan
pmagistry@gmail.com
Huang Chu-Ren
Dept. of Chinese and Bilingual Studies
Hong Kong Poly U. , Hong Kong.
churenhuang@gmail.com
Abstract
Wiktionary, a satellite of the Wikipedia
initiative, can be seen as a potential re-
source for Natural Language Processing.
It requires however to be processed be-
fore being used efficiently as an NLP re-
source. After describing the relevant as-
pects of Wiktionary for our purposes, we
focus on its structural properties. Then,
we describe how we extracted synonymy
networks from this resource. We pro-
vide an in-depth study of these synonymy
networks and compare them to those ex-
tracted from traditional resources. Fi-
nally, we describe two methods for semi-
automatically improving this network by
adding missing relations: (i) using a kind
of semantic proximity measure; (ii) using
translation relations of Wiktionary itself.
Note: The experiments of this paper are based on Wik-
tionary?s dumps downloaded in year 2008. Differences may
be observed with the current versions available online.
1 Introduction
Reliable and comprehensive lexical resources con-
stitute a crucial prerequisite for various NLP tasks.
However their building cost keeps them rare. In
this context, the success of the Princeton Word-
Net (PWN) (Fellbaum, 1998) can be explained by
the quality of the resource but also by the lack of
serious competitors. Widening this observation to
more languages only makes this observation more
acute. In spite of various initiatives, costs make
resource development extremely slow or/and re-
sult in non freely accessible resources. Collabo-
rative resources might bring an attractive solution
to this difficult situation. Among them Wiktionary
seems to be the perfect resource for building com-
putational mono-lingual and multi-lingual lexica.
This paper focuses therefore on Wiktionary, how
to improve it, and on its exploitation for creating
resources.
In next section, we present some relevant infor-
mation about Wiktionary. Section 3 presents the
lexical graphs we are using and the way we build
them. Then we pay some attention to evaluation
(?4) before exploring some tracks of improvement
suggested by Wiktionary structure itself.
2 Wiktionary
As previously said, NLP suffers from a lack of
lexical resources, be it due to the low-quality or
non-existence of such resources, or to copyrights-
related problems. As an example, we consider
French language resources. Jacquin et al (2002)
highlighted the limitations and inconsistencies
from the French EuroWordnet. Later, Sagot and
Fi?er (2008) explained how they needed to re-
course to PWN, BalkaNet (Tufis, 2000) and other
resources (notably Wikipedia) to build WOLF, a
free French WordNet that is promising but still a
very preliminary resource. Some languages are
straight-off purely under-resourced.
The Web as Corpus initiative arose (Kilgarriff
and Grefenstette, 2003) as an attempt to design
tools and methodologies to use the web for over-
coming data sparseness (Keller and Lapata, 2002).
Nevertheless, this initiative raised non-trivial tech-
nical problems described in Baroni et al (2008).
Moreover, the web is not structured enough to eas-
ily and massively extract semantic relations.
In this context, Wiktionary could appear to be
a paradisiac playground for creating various lexi-
19
cal resources. We describe below the Wiktionary
resource and we explain the restrictions and prob-
lems we are facing when trying to exploit it. This
description may complete few earlier ones, for ex-
ample Zesch et al (2008a).
2.1 Collaborative editing
Wiktionary, the lexical companion to Wikipedia,
is a collaborative project to produce a free-content
multilingual dictionary.
1
As the other Wikipedia?s
satellite projects, the resource is not experts-led,
rather filled by any kind of users. The might-be
inaccuracy of the resulting resource has lengthily
been discussed and we will not debate it: see Giles
(2005) and Britannica (2006) for an illustration
of the controversy. Nevertheless, we think that
Wiktionary should be less subject (so far) than
Wikipedia to voluntary misleading content (be it
for ideological, commercial reasons, or alike).
2.2 Articles content
As one may expect, a Wiktionary article
2
may (not
systematically) give information on a word?s part
of speech, etymology, definitions, examples, pro-
nunciation, translations, synonyms/antonyms, hy-
pernyms/hyponyms, etc.
2.2.1 Multilingual aspects
Wiktionary?s multilingual organisation may be
surprising and not always meet one?s expectations
or intuitions. Wiktionaries exist in 172 languages,
but we can read on the English language main
page, ?1,248,097 entries with English definitions
from over 295 languages?. Indeed, a given wik-
tionary describes the words in its own language
but also foreign words. For example, the English
article moral includes the word in English (adjec-
tive and noun) and Spanish (adjective and noun)
but not in French. Another example, boucher,
which does not exist in English, is an article of the
English wiktionary, dedicated to the French noun
(a butcher) and French verb (to cork up).
A given wiktionary?s ?in other languages? left
menu?s links, point to articles in other wiktionar-
ies describing the word in the current language.
For example, the Fran?ais link in the dictionary
article of the English wiktionary points to an arti-
cle in the French one, describing the English word
dictionary.
1
http://en.wiktionary.org/
2
What article refers to is more fuzzy than classical entry
or acceptance means.
2.2.2 Layouts
In the following paragraph, we outline wik-
tionary?s general structure. We only consider
words in the wiktionary?s own language.
An entry consists of a graphical form and a cor-
responding article that is divided into the follow-
ing, possibly embedded, sections:
? etymology sections separate homonyms when
relevant;
? among an etymology section, different parts
of speech may occur;
? definitions and examples belong to a part of
speech section and may be subdivided into sub-
senses;
? translations, synonyms/antonyms and hy-
pernyms/hyponyms are linked to a given part of
speech, with or without subsenses distinctions.
In figure 1 is depicted an article?s layout example.
Figure 1: Layout of boot article (shortened)
About subsenses, they are identified with an in-
dex when first introduced but they may appear as
a plain text semantic feature (without index) when
used in relations (translations, synonyms, etc.). It
is therefore impossible to associate the relations
arguments to subsenses. Secondly, subsense index
appears only in the current word (the source of the
relation) and not in the target word?s article it is
linked to (see orange French N. and Adj., Jan. 10,
2008
3
).
A more serious issue appears when relations are
shared by several parts of speech sections. In Ital-
3
http://fr.wiktionary.org/w/index.php?
title=orange&oldid=2981313
20
ian, both synonyms and translations parts are com-
mon to all words categories (see for example car-
dinale N. and Adj., Apr. 26, 2009
4
).
2.3 Technical issues
As Wikipedia and the other Wikimedia Founda-
tion?s projects, the Wiktionary?s content manage-
ment system relies on the MediaWiki software
and on the wikitext. As stated in Wikipedia?s
MetaWiki article, ?no formal syntax has been de-
fined? for the MediaWiki and consequently it is
not possible to write a 100% reliable parser.
Unlike Wikipedia, no HTML dump is available
and one has to parse the Wikicode. Wikicode
is difficult to handle since wiki templates require
handwritten rules that need to be regularly up-
dated. Another difficulty is the language-specific
encoding of the information. Just to mention one,
the target language of a translation link is iden-
tified by a 2 or 3 letters ISO-639 code for most
languages. However in the Polish wiktionary the
complete name of the language name (angielski,
francuski, . . . ) is used.
2.4 Parsing and modeling
The (non-exhaustive) aforementioned list of diffi-
culties (see ?2.2.2 and ?2.3) leads to the following
consequences:
? Writing a parser for a given wiktionary is
possible only after an in-depth observation of its
source. Even an intensive work will not prevent
all errors as long as (i) no syntax-checking is made
when editing an article and (ii) flexibility with the
?tacitly agreed? layout conventions is preserved.
Better, flexibility is presented as a characteristic of
the framework:
?[. . . ] it is not a set of rigid rules. You may
experiment with deviations, but other editors
may find those deviations unacceptable, and
revert those changes. They have just as much
right to do that as you have to make them.
5
?
Moreover, a parser has to be updated every new
dump, as templates, layout conventions (and so
on) may change.
?Writing parsers for different languages is not a
simple adjustment, rather a complete overhaul.
? When extracting a network of semantic rela-
tions from a given wiktionary, some choices are
more driven by the wiktionary inner format than
scientific modelling choices. An illustration fol-
4
http://it.wiktionary.org/w/index.php?
title=cardinale&oldid=758205
5
http://en.wiktionary.org/wiki/WT:ELE
lows in ?3.2. When merging information extracted
from several languages, the homogenisation of the
data structure often leads to the choice of the poor-
est one, resulting in a loss of information.
2.5 The bigger the better?
Taking advantage of colleagues mastering various
languages, we studied the wiktionary of the fol-
lowing languages: French, English, German, Pol-
ish and Mandarin Chinese. A first remark con-
cerns the size of the resource. The official num-
ber of declared articles in a given wiktionary in-
cludes a great number of meta-articles which are
not word entries As of April 2009, the French wik-
tionary reaches the first rank
6
, before the English
one. This can be explained by the automated im-
port of public-domain dictionaries articles (Littr?
1863 and Dictionnaire de l?Acad?mie Fran?aise
1932-1935). Table 1 shows the ratio between the
total number of articles and the ?relevant? ones
(numbers based on year 2008 snapshots).
Total Meta
?
Other
??
Relevant
fr 728,266 25,244 369,948 337,074 46%
en 905,963 46,202 667,430 192,331 21%
de 88,912 7,235 49,672 32,005 36%
pl 110,369 4,975 95,241 10,153 9%
zh 131,752 8,195 112,520 1,037 0.7%
?
templates definitions, help pages, user talks, etc.
??
other languages, redirection links, etc.
Table 1: Ratio of ?relevant? articles in wiktionaries
By ?relevant?, we mean an article about a word
in the wiktionary?s own language (e.g. not an
article about a French word in the English Wik-
tionary). Among the ?relevant? articles, some
are empty and some do not contain any transla-
tion nor synonym link. Therefore, before deciding
to use Wiktionary, it is necessary to compare the
amount of extracted information contribution and
the amount of work required to obtain it .
3 Study of synonymy networks
In this section, we study synonymy networks built
from different resources. First, we introduce
some general properties of lexical networks (?3.1).
Then we explain how we build Wiktionary?s syn-
onymy network and how we analyse its proper-
ties. In ?3.3, we show how we build similar graphs
from traditional resources for evaluation purposes.
3.1 Structure of lexical networks
In the following sections, a graph G = (V,E)
is defined by a set V of n vertices and a set
E ? V
2
of m edges. In this paper, V is
6
http://meta.wikimedia.org/wiki/List_
of_Wiktionaries
21
a set of words and E is defined by a relation
E
R
7?? E : (w
1
, w
2
) ? E if and only if w
1
R
? w
2
.
Most of lexical networks, as networks extracted
from real world, are small worlds (SW) net-
works. Comparing structural characteristics of
wiktionary-based lexical networks to some stan-
dard resource should be done according to well-
known properties of SW networks (Watts and
Strogatz, 1998; Barabasi et al, 2000; Newman,
2003; Gaume et al, 2008). These properties are:
? Edge sparsity: SW are sparse in edges
m = O(n) or m = O(n log(n))
? Short paths: in SW, the average path length
(L)
7
is short. Generally there is at least one short
path between any two nodes.
? High clustering: in SW, the clustering coef-
ficient (C) that expresses the probability that two
distinct nodes adjacent to a given third one are ad-
jacent, is an order of magnitude higher than for
Erdos-Renyi (random) graphs: C
SW
 C
random
;
this indicates that the graph is locally dense, al-
though it is globally sparse.
?Heavy-tailed degree distribution: the distri-
bution of the vertices incidence degrees follows a
power law in a SW graph. The probability P (k)
that a given node has k neighbours decreases as a
power law, P (k) ? k
a
(a being a constant charac-
teristic of the graph). Random graphs conforms to
a Poisson Law.
3.2 Wiktionary?s network
Graph extraction Considering what said in
?2.2.2 and ?2.4, we made the following choices:
8
? Vertices: a vertex is built for each entry?s part
of speech.
? Parts of speech: when modeling the links
from X (X having for part of speech Pos
X
) to
one of its synonyms Y , we assume that Pos
Y
=
Pos
X
, thus building vertex Pos
Y
.Y.
? Subsenses: subsenses are flattened. First, the
subsenses are not always mentioned in the syn-
onyms section. Second, if we take into account
the subsenses, they only appear in the source of the
relation. For example, considering in figure 1 the
relation boot
syn
??? kick (both nouns), and given the
10 subsenses for boot and the 5 ones for kick, we
should build 15 vertices. And we should then add
7
Average length of the shortest path between any two
nodes.
8
These choices can clearly be discussed from a linguis-
tic point of view and judged to be biased. Nevertheless, we
adopted them as a first approximation to make the modelling
possible.
all the links between the mentioned boot?s sub-
senses and the 5 kick?s existing subsenses. This
would lead to a high number of edges, but the
graph would not be closer to the reality. The way
subsenses appear in Wiktionary are unpredictable.
"Subsenses" correspond sometimes to homonyms
or clear-cut senses of polysemous words, but can
also correspond to facets, word usage or regu-
lar polysemy. Moreover, some entries have no
subsenses distinction whereas it would be wor-
thy. More globally, the relevance of discrete word
senses has been seriously questioned, see (Victorri
and Fuchs, 1996) or (Kilgarriff, 1997) for very
convincing discussions. Two more practical rea-
sons led us to this choice. We want our method to
be reproducible for other languages and some wik-
tionaries do not include subsenses. At last, some
gold standard resources (eg. Dicosyn) have their
subsenses flattened too and we want to compare
the resources against each other.
? Edges: wiktionary?s synonymy links are ori-
ented but we made the graph symmetric. For ex-
ample, boot does not appear in kick?s synonyms.
Some words even appear as synonyms without be-
ing an entry of Wiktionary.
From the boot example (figure 1), we extract ver-
tices {N.boot, V.boot}, build {N.buskin,
N.kick, V.kick} and we add the follow-
ing (symmetrized) edges: N.boot?N.buskin,
N.boot?N.kick and V.boot?V.kick.
Graph properties By observing the table 2, we
can see that the graphs of synonyms extracted
from Wiktionary are all typical small worlds. In-
deed their l
lcc
remains short, their C
lcc
is always
greater or equal than 0.2 and their distribution
curves of the vertices incidence degree is very
close to a power law (a least-square method gives
always exponent a
lcc
? ?2.35 with a confidence
r
2
lcc
always greater than 0.89). It can also be seen
that the average incidence k
lcc
ranges from 2.32
to 3.32.
9
It means that no matter which language
9
It is noteworthy that the mean incidence of vertices is al-
most always the same (close to 2.8) no matter the graph size
is. If we assume that all wiktionary?s graphs grow in a similar
way but at different speed rates (after all it is the same frame-
work), graphs (at least their statistical properties) from differ-
ent languages can be seen as snapshots of the same graph at
different times. This would mean that the number of graphs
edges tends to grow proportionally with the number of ver-
tices. This fits with the dynamic properties of small worlds
(Steyvers and Tenenbaum, 2005). It means that for a wik-
tionary system, even with many contributions, graph density
is likely to remain constant and we will see that in compar-
ison to traditional lexical resources this density is quite low.
22
graph n m n
lcc
m
lcc
k
lcc
l
lcc
C
lcc
a
lcc
r
2
lcc
fr-N 18017 9650 3945 4690 2.38 10.18 0.2 -2.03 0.89
fr-A 5411 2516 1160 1499 2.58 8.86 0.23 -2.04 0.95
fr-V 3897 1792 886 1104 2.49 9.84 0.21 -1.65 0.91
en-N 22075 11545 3863 4817 2.49 9.7 0.24 -2.31 0.95
en-A 8437 4178 2486 3276 2.64 8.26 0.2 -2.35 0.95
en-V 6368 3274 2093 2665 2.55 8.33 0.2 -2.01 0.93
de-N 32824 26622 12955 18521 2.86 7.99 0.28 -2.16 0.93
de-A 5856 6591 3690 5911 3.2 6.78 0.24 -1.93 0.9
de-V 5469 7838 4574 7594 3.32 5.75 0.23 -1.92 0.9
pl-N 8941 4333 2575 3143 2.44 9.85 0.24 -2.31 0.95
pl-A 1449 731 449 523 2.33 7.79 0.21 -1.71 0.94
pl-V 1315 848 601 698 2.32 5.34 0.2 -1.61 0.92
n: number of vertices m: number of edges
k: avg. number of neighbours per vertex l: avg. path length between vertices
C: clustering rate a: power law exponent with r
2
confidence
_
lcc
: denotes on largest connected component
Table 2: Wiktionary synonymy graphs properties
or part of speech, m = O(n) as for most of SW
graphs (Newman, 2003; Gaume et al, 2008).
3.3 Building synonymy networks from
known standards
WordNet There are many possible ways for
building lexical networks from PWN. We tried
several methods but only two of them are worth
to be mentioned here. The graphs we built have
words as vertices, not synsets or senses. A first
straightforward method (method A) consists in
adding an edge between two vertices only if the
corresponding words appear as elements of the
same synset. This method produced many discon-
nected graphs of various sizes. Both the compu-
tational method we planned to use and our intu-
itions about such graphs were pointing towards a
bigger graph that would cover most of the lexical
network.
We therefore decided to exploit the hypernymy
relation. Traditional dictionaries indeed propose
hypernyms when one look for synonyms of very
specific terms, making hypernymy the closest re-
lation to synonymy at least from a lexicographic
viewpoint. However, adding all the hypernymy re-
lations resulted in a network extremely dense in
edges with some vertices having a high number of
neighbours. This was due to the tree-like organi-
sation of WordNet that gives a very special impor-
tance to higher nodes of the tree.
In the end we retained method B that consists in
adding edges in following cases:
? if two words belong to the same synset;
? if a word only appears in a synset that is a leaf
of the tree and contains only this word, then cre-
ate edges linking to words included in the hyper-
nym(s) synset.
We would like to study the evolution through time of wik-
tionaries, however this is outside the scope of this paper.
Therefore when a vertice w do not get any neigh-
bour according to method A, method B adds edges
linking w to words included in the hypernym(s)
synset of the synset {w}. We only added hyper-
nyms for the leaves of the tree in order to keep our
relations close to the synonymy idea. This idea has
already been exploited for some WordNet-based
semantic distances calculation taking into account
the depth of the relation in the tree (Leacock and
Chodorow, 1998).
Dicosyn graphs Dicosyn is a compilation of
synonym relations extracted from seven dictionar-
ies (Bailly, Benac, Du Chazaud, Guizot, Lafaye,
Larousse and Robert):
10
there is an edge r ? s if
and only if r and s have the same syntactic cate-
gory and at least one dictionary proposes s being
a synonym in the dictionary entry r. Then, each
of the three graphs (Nouns, Verbs, Adjectives) ob-
tained is made symmetric (dicosyn-fr-N, dicosyn-
fr-V and dicosyn-fr-A).
Properties of the graphs extracted Table 3
sums-up the structural properties of the synonyms
networks built from standard resources.
We can see that all the synonymy graphs ex-
tracted from PWN or Dicosyn are SW graphs.
Indeed their l
lcc
remains short, their C
lcc
is al-
ways greater or equal than 0.35 and their distri-
bution curves of the vertices incidence degree is
very close to a power law (a least-square method
gives always exponent a
lcc
near of ?2.30 with a
confidence r
2
lcc
always greater than 0.85). It can
also be observed that no matter the part of speech,
the average incidence of Dicosyn-based graphs is
always lower than WordNet ones.
10
Dicosyn has been first produced at ATILF, before being
corrected at CRISCO laboratory.
(http://elsap1.unicaen.fr/dicosyn.html)
23
graph n m n
lcc
m
lcc
k
lcc
l
lcc
C
lcc
a
lcc
r
2
lcc
pwn-en-N-A 117798 104929 12617 28608 4.53 9.89 0.76 -2.62 0.89
pwn-en-N-B 117798 168704 40359 95439 4.73 7.79 0.72 -2.41 0.91
pwn-en-A-A 21479 22164 4406 11276 5.12 9.08 0.75 -2.32 0.85
pwn-en-A-B 21479 46614 15945 43925 5.51 6.23 0.78 -2.09 0.9
pwn-en-V-A 11529 23019 6534 20806 6.37 5.93 0.7 -2.34 0.87
pwn-en-V-B 11529 40919 9674 39459 8.16 4.66 0.64 -2.06 0.91
dicosyn-fr-N 29372 100759 26143 98627 7.55 5.37 0.35 -2.17 0.92
dicosyn-fr-A 9452 42403 8451 41753 9.88 4.7 0.37 -1.92 0.92
dicosyn-fr-V 9147 51423 8993 51333 11.42 4.2 0.41 -1.88 0.91
Table 3: Gold standard?s synonymy graphs properties
4 Wiktionary graphs evaluation
Coverage and global SW analysis By compar-
ing tables 2 and 3, one can observe that:
? The lexical coverage of Wiktionary-based syn-
onyms graphs is always quantitatively lower than
those of standard resources although this may
change. For example, to horn (in PWN), absent
from Wiktionary in 2008, appeared in 2009. At
last, Wiktionary is more inclined to include some
class of words such as to poo (childish) or to
prefetch, to google (technical neologisms).
? The average number of synonyms for an en-
try of a Wiktionary-based resource is smaller than
those of standard resources. For example, com-
mon synonyms such as to act/to play appear in
PWN and not in Wiktionary. Nevertheless, some
other appear (rightly) in Wiktionary: to reduce/to
decrease, to cook/to microwave.
? The clustering rate of Wiktionary-based
graphs is always smaller than those of standard re-
sources. This is particularly the case for English.
However, this specificity might be due to differ-
ences between the resources themselves (Dicosyn
vs. PWN) rather than structural differences at the
linguistic level.
Evaluation of synonymy In order to evaluate
the quality of extracted synonymy graphs from
Wiktionary, we use recall and precision measure.
The objects we compare are not simple sets but
graphs (G = (V ;E)), thus we should compare
separately set of vertices (V ) and set of edges (E).
Vertices are words and edges are synonymy links.
Vertices evaluation leads to measure the resource
(a) English Wiktionary vs. Wordnet
Precision Recall
Nouns 14120/22075 = 0.64 14120/117798 = 0.12
Adj. 5874/8437 = 0.70 5874/21479 = 0.27
Verbs 5157/6368 = 0.81 5157/11529 = 0.45
(b) French Wiktionary vs. Dicosyn
Precision Recall
Nouns 10393/18017 = 0.58 10393/29372 = 0.35
Adj. 3076/5411 = 0.57 3076/9452 = 0.33
Verbs 2966/3897 = 0.76 2966/9147 = 0.32
Table 4: Wiktionary coverage
coverage whereas edges evaluation leads to mea-
sure the quality of the synonymy links in Wik-
tionary resource.
First of all, the global picture (table 4) shows
clearly that the lexical coverage is rather poor. A
lot of words included in standard resources are not
included yet in the corresponding wiktionary re-
sources. Overall the lexical coverage is always
lower than 50%. This has to be kept in mind while
looking at the evaluation of relations shown in ta-
ble 5. To compute the relations evaluation, each
resource has been first restricted to the links be-
tween words being present in each resource.
About PWN, since every link added with
method A will also be added with method B, the
precision of Wiktionary-based graphs synonyms
links will be always lower for "method A graphs"
than for "method B graphs". Precision is rather
good while recall is very low. That means that a
lot of synonymy links of the standard resources
are missing within Wiktionary. As for Dicosyn,
the picture is similar with even better precision but
very low recall.
5 Exploiting Wiktionary for improving
Wiktionary
As seen in section 4, Wiktionary-based resources
are very incomplete with regard to synonymy. We
propose two tasks for adding some of these links:
Task 1: Adding synonyms to Wiktionary by
taking into account its Small World characteristics
for proposing new synonyms.
(a) English wiktionary vs. Wordnet
Precision Recall
Nouns (A) 2503/6453 = 0.39 2503/11021 = 0.23
Nouns (B) 2763/6453 = 0.43 2763/18440 = 0.15
Adj. (A) 786/3139 = 0.25 786/5712 = 0.14
Adj. (B) 1314/3139 = 0.42 1314/12792 = 0.10
Verbs (A) 866/2667 = 0.32 866/10332 = 0.08
Verbs (B) 993/2667 = 0.37 993/18725 = 0.05
(b) French wiktionary vs. Dicosyn
Precision Recall
Nouns 3510/5075 = 0.69 3510/44501 = 0.08
Adj. 1300/1677 = 0.78 1300/17404 = 0.07
Verbs 899/1267 = 0.71 899/23968 = 0.04
Table 5: Wiktionary synonymy links precision & recall
24
Task 2: Adding synonyms to Wiktionary by
taking into account the translation relations.
We evaluate these two tasks against the bench-
marks presented in section 3.2.
5.1 Improving synonymy in Wiktionary by
exploiting its small world structure
We propose here to enrich synonymy links of Wik-
tionary by taking into account that lexical net-
works have a high clustering coefficient. Our hy-
pothesis is that missing links in Wiktionary should
be within clusters.
A high clustering coefficient means that two
words which are connected to a third one are likely
to be connected together. In other words neigh-
bours of my neighbours should also be in my
neighbourhood. We propose to reverse this prop-
erty to the following hypothesis: "neighbour of my
neighbours which are not in my neighbourhood
should be a good neighbour candidate". Thus the
first method we test consist simply in connecting
every vertex to neighbours of its neighbours. One
can repeat this operation until the expected num-
ber of edges is obtained.
11
Secondly we used the PROX approach pro-
posed by (Gaume et al, 2009). It is a stochastic
method designed for studying ?Hierarchical Small
Worlds?. Briefly put, for a given vertex u, one
computes for all other vertices v the probability
that a randomly wandering particle starting from
u stands in v after a fixed number of steps. Let
P (u, v) be this value. We propose to connect u
to the k first vertices ranked in descending order
with respect of P (u, v). We always choose k pro-
portionally to the original degree of u (number of
neighbours of u).
For a small number of steps (3 in our case) ran-
dom wanderings tend to be trapped into local clus-
ter structures. So a vertex v with a high P (u, v) is
likely to belong to the same cluster as u, which
means that a link u?v might be relevant.
Figure 2 shows precision, recall and f-score
evolution for French verbs graph when edges are
added using ?neighourhood? method (neigh), and
using ?Prox? method. Dashed line correspond to
the value theoretically obtained by choosing edges
at random. First, both methods are clearly more
efficient than a random addition, which is not sur-
prising but it seems to confirm our hypothesis that
missing edges are within clusters. Adding sharply
11
We repeat it only two times, otherwise the number of
added edges is too large.
0 2000 4000 6000 8000 10000 12000 140000.0
0.10.2
0.30.4
0.50.6
0.70.8
P
prox3neighrandom
0 2000 4000 6000 8000 10000 12000 140000.03
0.040.05
0.060.07
0.080.09
R
0 2000 4000 6000 8000 10000 12000 140000.05
0.060.07
0.080.09
0.100.11
0.120.13
F
fr.V
Figure 2: Precision, recall and F-score of French verbs
graph enlarged using only existing synonymy links
neighbours of neighbours seems to be as good as
adding edges ranked by Prox, anyway the rank
provided by Prox permits to add a given number
of edges. This ranking can also be useful to order
potential links if one think about a user validation
system. Synonyms added by Prox and absent from
gold standards are not necessarily false.
For example Prox proposes a relevant link ab-
solve/forgive, not included in PWN. Moreover,
many false positive are still interesting to consider
for improving the resource. For example, Prox
adds relations such as hypernyms (to uncover/to
peel) or inter-domain ?synonyms? (to skin/to peel).
This is due to high clustering (see ?3.1) and to
the fact that clusters in synonymy networks corre-
lates with language concepts (Gaume et al, 2008;
Duvignau and Gaume, 2008; Gaume et al, 2009;
Fellbaum, 1999).
Finally note that results are similar for other
parts of speech and other languages.
5.2 Using Wiktionary?s translation links to
improve its synonymy network
Assuming that two words sharing many transla-
tions in different languages are likely to be syn-
onymous, we propose to use Wiktionary?s transla-
tion links to enhance the synonymy network of a
given language.
In order to rank links to be potentially added,
we use a simple Jaccard measure: let T
w
be the set
of a word w?s translations, then for every couple
of words (w,w
?
) we have:
Jaccard(w,w
?
) =
|T
w
? T
w
?
|
|T
w
? T
w
?
|
We compute this measure for every possible pair
of words and then, starting from Wiktionary?s syn-
onymy graph, we incrementally add links accord-
ing to their Jaccard rank.
25
We notice first that most of synonymy links
added by this method were not initially included
in Wiktionary?s synonymy network. For exam-
ple, regarding English verbs, 95% of 2000 best
ranked proposed links are new. Hence this method
may be efficient to improve graph density. How-
ever one can wonder about the quality of the new
added links, so we discuss precision in the next
paragraph.
In figure 3 is depicted the evolution of precision,
recall and F-score for French verbs in the enlarged
graph in regard of the total number of edges. We
use Dicosyn graph as a gold standard. The dashed
line corresponds to theoretical scores one can ex-
pect by adding randomly chosen links.
First we notice that both precision and recall
are significantly higher than we can expect from
random addition. This confirms that words shar-
ing the same translations are good synonym candi-
dates. Added links seem to be particularly relevant
at the beginning for higher Jaccard scores. From
the first dot to the second one we add about 1000
edges (whereas the original graph contains 1792
edges) and the precision only decreases from 0.71
to 0.69.
The methods we proposed in this section are
quite simple and there is room for improvement.
First, both methods can be combined in order
to improve the resource using translation links
and then using clusters structure. One can also
think to the corollary task that would consists in
adding translation links between two languages
using synonymy links of others languages.
0 2000 4000 6000 8000 10000 120000.0
0.10.2
0.30.4
0.50.6
0.70.8
P
random
0 2000 4000 6000 8000 10000 120000.02
0.040.06
0.080.10
0.120.14
0.16
R
0 2000 4000 6000 8000 10000 120000.04
0.060.08
0.100.12
0.140.16
0.180.20
0.22
F
fr.V
Figure 3: Precision, recall and F-score of French verbs
graph enlarged using translation links
6 Conclusion and future work
This paper gave us the opportunity to share some
Wiktionary experience related lexical resources
building. We presented in addition two approaches
for improving these resources and their evaluation.
The first approach relies on the small world struc-
ture of synonymy networks. We postulated that
many missing links in Wiktionary should be added
among members of the same cluster. The second
approach assumes that two words sharing many
translations in different languages are likely to be
synonymous. The comparison with traditional re-
sources shows that our hypotheses are confirmed.
We now plan to combine both approaches.
The work presented in this paper combines a
NLP contribution involving data extraction and
rough processing of the data and a mathematical
contribution concerning graph-like resource. In
our viewpoint the second aspect of our work is
therefore complementary of other NLP contribu-
tions, like (Zesch et al, 2008b), involving more
sophisticated NLP processing of the resource.
Support for collaborative editing Our results
should be useful for setting up a more efficient
framework for Wiktionary collaborative editing.
We should be able to always propose a set of syn-
onymy relations that are likely to be. For exam-
ple, when a contributor creates or edits an arti-
cle, he may think about adding very few links but
might not bother providing an exhaustive list of
synonyms. Our tool can propose a list of potential
synonyms, ordered by relevancy. Each item of this
list would only need to be validated (or not).
Diachronic study An interesting topic for future
work is a "diachronic" study of the resource. It
is possible to access Wiktionary at several stages,
this can be used for studying how such resources
evolve. Grounded on this kind of study, one may
predict the evolution of newer wiktionaries and
foresee contributors? NLP needs. We would like
to set up a framework for everyone to test out new
methodologies for enriching and using Wiktionary
resources. Such observatory, would allow to fol-
low not only the evolution of Wiktionary but also
of Wiktionary-grounded resources, that will only
improve thanks to steady collaborative develop-
ment.
Invariants and variabality Wiktionary as a
massively mutiligual synonymy networks is an
extremely promising resource for studying the
(in)variability of semantic pairings such as
house/family, child/fruit, feel/know... (Sweetser,
1991; Gaume et al, 2009). A systematic study
within the semantic approximation framework
presented in the paper on Wiktionary data will be
carried on in the future.
26
References
A-L. Barabasi, R. Albert, H. Jeong, and G. Bianconi.
2000. Power-Law Distribution of the World Wide
Web. Science, 287. (in Technical Comments).
M. Baroni, F. Chantree, A. Kilgarriff, and S. Sharoff.
2008. Cleaneval: a Competition for Cleaning
Web Pages. In Proceedings of the Conference on
Language Resources and Evaluation (LREC), Mar-
rakech.
Encyclopaedia Britannica. 2006. Fatally flawed: re-
futing the recent study on encyclopedic accuracy by
the journal Nature.
K. Duvignau and B. Gaume. 2008. Between words
and world: Verbal "metaphor" as semantic or prag-
matic approximation? In Proceedings of Interna-
tional Conference "Language, Communication and
Cognition", Brighton.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
C. Fellbaum. 1999. La repr?sentation des verbes
dans le r?seau s?mantique Wordnet. Langages,
33(136):27?40.
B. Gaume, K. Duvignau, L. Pr?vot, and Y. Desalle.
2008. Toward a cognitive organization for electronic
dictionaries, the case for semantic proxemy. In Col-
ing 2008: Proceedings of the Workshop on Cogni-
tive Aspects of the Lexicon (COGALEX 2008), pages
86?93, Manchester.
B. Gaume, K. Duvignau, and M. Vanhove. 2009. Se-
mantic associations and confluences in paradigmatic
networks. In M. Vanhove, editor, From Polysemy to
Semantic Change: Towards a Typology of Lexical
Semantic Associations, pages 233?264. John Ben-
jamins Publishing.
J. Giles. 2005. Internet encyclopaedias go head to
head. Nature, 438:900?901.
C. Jacquin, E. Desmontils, and L. Monceaux. 2002.
French EuroWordNet Lexical Database Improve-
ments. In Proceedings of the Third International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLING 2002), Mexico
City.
F. Keller and M. Lapata. 2002. Using the web to over-
come data sparseness. In Proceedings of EMNLP-
02, pages 230?237.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computa-
tional Linguistics, 29:333?347.
A. Kilgarriff. 1997. I don?t believe in word senses.
Computers and the humanities, 31(2):91?113.
C. Leacock and M. Chodorow. 1998. Combining local
context and wordnet similarity for word sense iden-
tification. In C. Fellbaum, editor, WordNet: An elec-
tronic lexical database, pages 265?283. MIT Press.
M. Newman. 2003. The structure and function of com-
plex networks.
B. Sagot and D. Fi?er. 2008. Building a Free French
Wordnet from Multilingual Resources. In Proceed-
ings of OntoLex 2008, Marrackech.
M. Steyvers and J. B. Tenenbaum. 2005. The large-
scale structure of semantic networks: Statistical
analyses and a model of semantic growth. Cogni-
tive Science, 29:41?78.
E. Sweetser. 1991. From etymology to pragmatics.
Cambridge University Press.
D. Tufis. 2000. Balkanet design and development of a
multilingual balkan wordnet. Romanian Journal of
Information Science and Technology, 7(1-2).
B. Victorri and C. Fuchs. 1996. La polys?mie, con-
struction dynamique du sens. Herm?s.
D.J. Watts and S.H. Strogatz. 1998. Collective dynam-
ics of small-world networks. Nature, 393:440?442.
T. Zesch, C. M?ller, and I. Gurevych. 2008a. Extract-
ing Lexical Semantic Knowledge from Wikipedia
and Wiktionary. In Proceedings of the Conference
on Language Resources and Evaluation (LREC),
Marrakech.
T. Zesch, C. Muller, and I. Gurevych. 2008b. Using
wiktionary for computing semantic relatedness. In
Proceedings of 23rd AAAI Conference on Artificial
Intelligence.
27
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 383?387,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Unsupervized Word Segmentation:
the case for Mandarin Chinese
Pierre Magistry
Alpage, INRIA & Univ. Paris 7,
175 rue du Chevaleret,
75013 Paris, France
pierre.magistry@inria.fr
Beno?t Sagot
Alpage, INRIA & Univ. Paris 7,
175 rue du Chevaleret,
75013 Paris, France
benoit.sagot@inria.fr
Abstract
In this paper, we present an unsupervized seg-
mentation system tested on Mandarin Chi-
nese. Following Harris's Hypothesis in Kempe
(1999) and Tanaka-Ishii's (2005) reformulation,
we base our work on the Variation of Branching
Entropy. We improve on (Jin and Tanaka-Ishii,
2006) by adding normalization and viterbi-
decoding. This enable us to remove most of
the thresholds and parameters from their model
and to reach near state-of-the-art results (Wang
et al, 2011) with a simpler system. We provide
evaluation on different corpora available from
the Segmentation bake-off II (Emerson, 2005)
and define a more precise topline for the task
using cross-trained supervized system available
off-the-shelf (Zhang and Clark, 2010; Zhao and
Kit, 2008; Huang and Zhao, 2007)
1 Introduction
The Chinese script has no explicit ?word? bound-
aries. Therefore, tokenization itself, although the
very first step of many text processing systems, is
a challenging task. Supervized segmentation sys-
tems exist but rely on manually segmented corpora,
which are often specific to a genre or a domain and
use many different segmentation guidelines. In order
to deal with a larger variety of genres and domains,
or to tackle more theoretic questions about linguistic
units, unsupervized segmentation is still an impor-
tant issue. After a short review of the corresponding
literature in Section 2, we discuss the challenging is-
sue of evaluating unsupervized word segmentation
systems in Section 3. Section 4 and Section 5 present
the core of our system. Finally, in Section 6, we de-
tail and discuss our results.
2 State of the Art
Unsupervized word segmentation systems tend to
make use of three different types of information: the
cohesion of the resulting units (e.g., Mutual Infor-
mation, as in (Sproat and Shih, 1990)), the degree of
separation between the resulting units (e.g., Acces-
sor Variety, see (Feng et al, 2004)) and the proba-
bility of a segmentation given a string (Goldwater et
al., 2006; Mochihashi et al, 2009).
A recently published work by Wang et al (2011)
introduce ESA: ?Evaluation, Selection, Adjust-
ment.? This method combines cohesion and separa-
tion measures in a ?goodness? metric that is maxi-
mized during an iterative process. This work is the
current state-of-the-art in unsupervized segmenta-
tion of Mandarin Chinese data.
The main drawbacks of ESA are the need to iterate
the process on the corpus around 10 times to reach
good performance levels and the need to set a param-
eter that balances the impact of the cohesion measure
w.r.t. the separation measure. Empirically, a corre-
lation is found between the parameter and the size of
the corpus but this correlation depends on the script
used in the corpus (it changes if Latin letters and
Arabic numbers are taken into account during pre-
processing or not). Moreover, computing this cor-
relation and finding the best value for the parameter
(i.e., what the authors call the proper exponent) re-
quires a manually segmented training corpus. There-
fore, this proper exponent may not be easily available
in all situations. However, if we only consider their
experiments using settings similar to ours, their re-
sults consistently lie around an f-score of 0.80.
An older approach, introduced by Jin and Tanaka-
Ishii (2006), solely relies on a separation measure
383
that is directly inspired by a linguistic hypothesis for-
mulated by Harris (1955). In Tanaka-Ishii (2005)
(following Kempe (1999)) who use Branching En-
tropy (BE), this hypothesis goes as follows: if se-
quences produced by human language were random,
we would expect the Branching Entropy of a se-
quence (estimated from the n-grams in a corpus)
to decrease as we increase the length of the se-
quence. Therefore the variation of the branching en-
tropy (VBE) should be negative. When we observe
that it is not the case, Harris hypothesizes that we
are at a linguistic boundary. Following this hypoth-
esis, (Jin and Tanaka-Ishii, 2006) propose a system
that segments when BE is rising or when it reach a
certain maximum.
The main drawback of Jin and Tanaka-Ishii (2006)
model is that segmentation decisions are taken very
locally1 and do not depend on neighboring cuts.
Moreover, this system also also relies on parameters,
namely the threshold on the VBE above which the
system decides to segment (in their system, this is
when VBE? 0). In theory, we could expect a de-
creasing BE and look for a less decreasing value (or
on the contrary, rising at least to some extent). A
threshold of 0 can be seen as a default value. Fi-
nally, Jin and Tanaka-Ishii do not take in account that
VBE of n-gram may not be directly comparable to
the VBE of m-grams if m ?= n. A normalization is
needed (as in (Cohen et al, 2002)).
Due to space constraints, we shall not describe
here other systems than those by Wang et al (2011)
and Jin and Tanaka-Ishii (2006). A more compre-
hensive state of the art can be found in (Zhao and
Kit, 2008) and (Wang et al, 2011).
In this paper we will show that we can correct the
drawbacks of Jin and Tanaka-Ishii (2006) model and
reach performances comparable to those of Wang et
al. (2011) with as simpler system.
3 Evaluation
In this paper, in order to be comparable with
Wang et al (2011), we evaluate our system against
the corpora from the Second International Chi-
nese Word Segmentation Bakeoff (Emerson, 2005).
These corpora cover 4 different segmentation guide-
lines from various origins: Academia Sinica (AS),
City-University of Hong-Kong (CITYU), Microsoft
Research (MSR) and Peking University (PKU).
1Jin (2007) uses self-training with MDL to address this issue.
Evaluating unsupervized systems is a challenge by
itself. As an agreement on the exact definition of
what a word is remains hard to reach, various seg-
mentation guidelines have been proposed and fol-
lowed for the annotation of different corpora. The
evaluation of supervized systems can be achieved on
any corpus using any guidelines: when trained on
data that follows particular guidelines, the resulting
system will follow as well as possible these guide-
lines, and can be evaluated on data annotated accord-
ingly. However, for unsupervized systems, there is
no reason why a system should be closer to one ref-
erence than another or even not to lie somewhere
in between the different existing guidelines. Huang
and Zhao (2007) propose to use cross-training of a
supervized segmentation system in order to have an
estimation of the consistency between different seg-
mentation guidelines, and therefore an upper bound
of what can be expected from an unsupervized sys-
tem (Zhao and Kit, 2008). The average consistency
is found to be as low as 0.85 (f-score). Therefore
this figure can be considered as a sensible topline for
unsupervized systems. The standard baseline which
consists in segmenting each character leads to a base-
line around 0.35 (f-score) ? almost half of the to-
kens in a manually segmented corpus are unigrams.
Per word-length evaluation is also important as
units of various lengths tend to have different distri-
butions. We used ZPAR (Zhang and Clark, 2010) on
the four corpora from the Second Bakeoff to repro-
duce Huang and Zhao's (2007) experiments, but also
to measure cross-corpus consistency at a per-word-
length level. Our overall results are comparable to
what Huang and Zhao (2007) report. However, the
consistency is quickly falling for longer words: on
unigrams, f-scores range from 0.81 to 0.90 (the same
as the overall results). We get slightly higher figures
on bigrams (0.85?0.92) but much lower on trigrams
with only 0.59?0.79. In a segmented Chinese text,
most of the tokens are uni- and bigrams but most of
the types are bi- and trigrams (as unigrams are often
high frequency grammatical words and trigrams the
result of more or less productive affixations). There-
fore the results of evaluations only based on tokens
do not suffer much from poor performances on tri-
grams even if a large part of the lexicon may be in-
correctly processed.
Another issue about the evaluation and compari-
son of unsupervized systems is to try and remain fair
384
in terms of preprocessing and prior knowledge given
to the systems. For example, Wang et al (2011)
used different levels of preprocessing (which they
call ?settings?). In their settings 1 and 2, Wang et
al. (2011) try not to rely on punctuation and char-
acter encoding information (such as distinguishing
Latin and Chinese characters). However, they opti-
mize their parameter for each setting. We therefore
consider that their system does take into account the
level of processing which is performed on Latin char-
acters and Arabic numbers, and therefore ?knows?
whether to expect such characters or not. In set-
ting 3 they add the knowledge of punctuation as clear
boundaries and in setting 4 they preprocess Arabic
and Latin and obtain better, more consistent and less
questionable results.
As we are more interested in reducing the amount
of human labor needed than in achieving by all
means fully unsupervized learning, we do not re-
frain from performing basic and straightforward pre-
processing such as detection of punctuation marks,
Latin characters and Arabic numbers.2 Therefore,
our experiments rely on settings similar to their set-
tings 3 and 4, and are evaluated against the same
corpora.
4 Normalized Variation of Branching
Entropy (nVBE)
Our system builds upon Harris's (1955) hypothesis
and its reformulation by Kempe (1999) and Tanaka-
Ishii (2005). Let us now define formally the notions
underlying our system.
Given an n-gram x0..n = x0..1 x1..2 . . . xn?1..n
with a left context ??, we define its Right Branching
Entropy (RBE) as:
h?(x0..n) = H(?? | x0..n)
= ?
?
x???
P (x | x0..n) logP (x | x0..n).
The Left Branching Entropy (LBE) is defined in a
symmetric way: if we note ?? the right context of
x0..n, its LBE is defined as:
h?(x0..n) = H(?? | x0..n).
The RBE (resp. LBE) can be considered as x0..n's
Branching Entropy (BE) when reading from left to
right (resp. right to left).
2Simple regular expressions could also be considered to deal
with unambiguous cases of numbers and dates in Chinese script.
From h?(x0..n) and h?(x0..n?1) on the one hand,
and from h?(x0..n) and h?(x1..n) we estimate the
Variation of Branching Entropy (VBE) in both direc-
tions, defined as follows:
?h?(x0..n) = h?(x0..n) ? h?(x0..n?1)
?h?(x0..n) = h?(x0..n) ? h?(x1..n).
The VBEs are not directly comparable for strings
of different lengths and need to be normalized. In
this work, we recenter them around 0 with respect to
the length of the string by substracting the mean of
the VBEs of the strings of the same length. Writing
??h?(x) and ??h?(x). The normalized VBEs for the
string x, or nVBEs, are then defined as follow (we
only defined ??h?(x) for clarity reasons): for each
length k and each k-gram x such that len(x) = k,
??h?(x) = ?h?(x)???,k, where ??,k is the mean
of the values of ?h?(x) of all k-grams x.
Note that we use and normalize the variation of
branching entropy and not the branching entropy it-
self. Doing so would break the Harris's hypothesis as
we would not expect h?(x0..n) < h?(x0..n?1) in non-
boundary situation anymore. Many studies use di-
rectly the branching entropy (normalized or not) and
report results that are below state-of-the-art systems
(Cohen et al, 2002).
5 Decoding algorithm
If we follow Harris's hypothesis and consider com-
plex morphological word structures, we expect a
large VBE at the boundaries of interesting units and
more unstable variations inside ?words.? This expec-
tation was confirmed by empirical data visualization.
For different lengths of n-grams, we compared the
distributions of the VBEs at different positions inside
the n-gram and at its boundaries. By plotting density
distributions for words vs. non-words, we observed
that the VBE at both boundaries were the most dis-
criminative value. Therefore, we decided to take in
account the VBE only at the word-candidate bound-
aries (left and right) and not to consider the inner val-
ues. Two interesting consequences of this decision
are: first, all ??h(x) can be precomputed as they do
not depend on the context. Second, best segmenta-
tion can be computed using dynamic programming.
Since we consider the VBE only at words bound-
ary, we can define for any n-gram w its autonomy as
a(x) = ???h(x) + ??h?(x). The more an n-gram is
autonomous, the more likely it is to be a word.
385
With this measure, we can redefine the sentence
segmentation problem as the maximization of the au-
tonomy measure of its words. For a character se-
quence s, if we call Seg(s) the set of all the possible
segmentations, then we are looking for:
arg max
W?Seg(s)
?
wi?W
a(wi) ? len(wi),
where W is the segmentation corresponding to the
sequence of words w0w1 . . . wm, and len(wi) is the
length of a word wi used here to be able to com-
pare segmentations resulting in a different number
of words. This best segmentation can be computed
easily using dynamic programming.
6 Results and discussion
We tested our system against the data from the 4 cor-
pora of the Second Bakeoff, in both settings 3 and 4,
as described in Section 3. Overall results are given
in Table 1 and per-word-length results in Table 2.
Our results (nVBE) show significant improve-
ments over Jin's (2006) strategy (VBE > 0) and
are closely competing with ESA. But contrarily to
ESA (Wang et al, 2011), it does not require multi-
ple iterations on the corpus and it does not rely on
any parameters. This shows that we can rely solely
on a separation measure and get high segmentation
scores. When maximized over a sentence, this mea-
sure captures at least in part what can be modeled by
a cohesion measure without the need for fine-tuning
the balance between the two.
The evolution of the results w.r.t. word length is
consistent with the supervized cross-evaluation re-
sults of the various segmentation guidelines as per-
formed in Section 3.
Due to space constraints, we cannot detail here a
qualitative analysis of the results. We can simply
mention that the errors we observed are consistent
with previous systems based on Harris's hypothesis
(see (Magistry and Sagot, 2011) and Jin (2007) for a
longer discussion). Many errors are related to dates
and Chinese numbers. This could and should be
dealt with during preprocessing. Other errors often
involve frequent grammatical morphemes or produc-
tive affixes. These errors are often interesting for lin-
guists and could be studied as such and/or corrected
in a post-processing stage that would introduce lin-
guistic knowledge. Indeed, unlike content words,
grammatical morphemes belongs to closed classes,
System AS CITYU PKU MSR
Setting 3
ESA worst 0.729 0.795 0.781 0.768
ESA best 0.782 0.816 0.795 0.802
nVBE 0.758 0.775 0.781 0.798
Setting 4
VBE > 0 0.63 0.640 0.703 0.713
ESA worst 0.732 0.809 0.784 0.784
ESA best 0.786 0.829 0.800 0.818
nVBE 0.766 0.767 0.800 0.813
Table 1: Evaluation on the Second Bakeoff data with
Wang et al's (2011) settings. ?Worst? and ?best? give the
range of the reported results with differents values of the
parameter in Wang et al's system. VBE > 0 correspond
to a cut whenever BE is raising. nVBE corresponds to our
proposal, based on normalized VBE with maximization at
word boundaries. Recall that the topline is around 0.85
Corpus overall unigrams bigrams trigrams
AS 0.766 0.741 0.828 0.494
CITYU 0.767 0.739 0.834 0.555
PKU 0.800 0.789 0.855 0.451
MSR 0.813 0.823 0.856 0.482
Table 2: Per word-length details of our results with our
nVBE algorithm and setting 4. Recall that the toplines
are respectively 0.85, 0.81, 0.85 and 0.59 (see Section 3)
therefore introducing this linguistic knowledge into
the system may be of great help without requiring
to much human effort. A sensible way to go in that
direction would be to let unsupervized system deal
with open classes and process closed classes with a
symbolic or supervized module.
One can also observe that our system performs bet-
ter on PKU and MSR corpora. As PKU is the small-
est corpus and AS the biggest, size alone cannot ex-
plain this result. However, PKU is more consistent
in genre as it contains only articles from the Peo-
ple's Daily. On the other end, AS is a balanced cor-
pus with a greater variety in many aspects. CITYU
Corpus is almost as small as PKU but contains arti-
cles from newspapers of various Mandarin Chinese
speaking communities where great variation is to be
expected. This suggest that consistency of the input
data is as important as the amount of data. This hy-
pothesis has to be confirmed in futur studies. If it is,
automatic clustering of the input data may be an im-
portant pre-processing step for this kind of systems.
386
References
Paul Cohen, Brent Heeringa, and Niall Adams. 2002.
An unsupervised algorithm for segmenting categorical
timeseries into episodes. Pattern Detection and Dis-
covery, page 117?133.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of the
Fourth SIGHAN Workshop on Chinese Language Pro-
cessing, volume 133.
Haodi Feng, Kang Chen, Xiaotie Deng, and Weiming
Zheng. 2004. Accessor variety criteria for Chi-
nese word extraction. Computational Linguistics,
30(1):75?93.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics, page 673?680.
Zellig S. Harris. 1955. From phoneme to morpheme.
Language, 31(2):190?222.
Changning. Huang and Hai Zhao. 2007. ??????
?? (Chinese word segmentation: A decade review).
Journal of Chinese Information Processing, 21(3):8?
20.
Zhihui Jin and Kumiko Tanaka-Ishii. 2006. Unsuper-
vised segmentation of Chinese text by use of branching
entropy. In Proceedings of the COLING/ACL on Main
conference poster sessions, page 428?435.
Zhihui Jin. 2007. A Study On Unsupervised Segmenta-
tion Of Text Using Contextual Complexity. Ph.D. the-
sis, University of Tokyo.
Andr? Kempe. 1999. Experiments in unsupervised
entropy-based corpus segmentation. In Workshop of
EACL in Computational Natural Language Learning,
page 7?13.
Pierre Magistry and Beno?t Sagot. 2011. Segmentation
et induction de lexique non-supervis?es du mandarin.
In TALN'2011 - Traitement Automatique des Langues
Naturelles, Montpellier, France, June. ATALA.
Daichi Mochihashi, Takeshi. Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1-Volume 1, page 100?108.
Richard W. Sproat and Chilin Shih. 1990. A statis-
tical method for finding word boundaries in Chinese
text. Computer Processing of Chinese and Oriental
Languages, 4(4):336?351.
Kumiko Tanaka-Ishii. 2005. Entropy as an indicator of
context boundaries: An experiment using a web search
engine. In IJCNLP, page 93?105.
Hanshi Wang, Jian Zhu, Shiping Tang, and Xiaozhong
Fan. 2011. A new unsupervised approach to word
segmentation. Computational Linguistics, 37(3):421?
454.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using a
single discriminative model. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, page 843?852.
Hai Zhao and Chunyu Kit. 2008. An empirical compar-
ison of goodness measures for unsupervised Chinese
word segmentation with a unified framework. In The
Third International Joint Conference on Natural Lan-
guage Processing (IJCNLP2008), Hyderabad, India.
387
