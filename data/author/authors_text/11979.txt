Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 99?107,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
High-Performance High-Volume Layered Corpora Annotation
Tiago Lu??s and David Martins de Matos
L2F - INESC-ID
R. Alves Redol 9, Lisboa, Portugal
{tiago.luis,david.matos}@l2f.inesc-id.pt
Abstract
NLP systems that deal with large collec-
tions of text require significant computa-
tional resources, both in terms of space
and processing time. Moreover, these sys-
tems typically add new layers of linguis-
tic information with references to another
layer. The spreading of these layered an-
notations across different files makes them
more difficult to process and access the
data. As the amount of input increases, so
does the difficulty to process it. One ap-
proach is to use distributed parallel com-
puting for solving these larger problems
and save time.
We propose a framework that simplifies
the integration of independently existing
NLP tools to build language-independent
NLP systems capable of creating layered
annotations. Moreover, it allows the devel-
opment of scalable NLP systems, that exe-
cutes NLP tools in parallel, while offering
an easy-to-use programming environment
and a transparent handling of distributed
computing problems. With this framework
the execution time was decreased to 40
times less than the original one on a cluster
with 80 cores.
1 Introduction
Linguistic information can be automatically cre-
ated by NLP systems. These systems are com-
posed by several NLP tools that are typically ex-
ecuted in a pipeline, where each tool performs a
processing step. Therefore, each tool uses the re-
sults produced by the previous processing steps
and produces new linguistic information that can
be later used by other tools. The addition of new
layers of linguistic information (layered annota-
tions) by NLP tools makes the processing and ac-
cess to data difficult due to the spreading of the
layered annotations across different files. More-
over, whenever these tools are integrated, several
problems related with information flow between
them may arise. A given tool may need an annota-
tion previously produced by another tool but some
of the information in annotation can be lost in con-
versions between the different tool data formats,
because the expressiveness of each format may be
different and not completely convertible into other
formats.
Besides tool integration problems, there is also
another problem related with the data-intensive
nature of NLP and the computation power needed
to produce the linguistic information. The wealth
of annotations has increased the amount of data to
process. Therefore, the processing of this linguis-
tic information is a computation-heavy process
and some algorithms continue to take a long time
(hours or days) to produce their results. This kind
of processing can benefit from distributed parallel
computing but it may create other problems, such
as fault tolerance to machine failures. Because
some NLP algorithms can take long time to pro-
duce their results, it is important to automatically
recover from these failures, in order not to lose the
results of computations already performed. Task
scheduling is also a problem due to data-intensive
nature of NLP. Data-driven scheduling (based on
data location) improves performance because it re-
duces bandwidth usage.
Our framework aims to simplify the integration
of independently developed NLP tools, while pro-
viding an easy-to-use programming environment,
and transparent handling of distributed computing
problems, such as fault tolerance and task schedul-
ing, when executing the NLP tools in parallel.
Moreover, NLP systems built on top of the frame-
work are language-independent and produce lay-
ered annotations. We also measured the gains that
can be achieved with the parallel execution of NLP
99
tools and the merging of the layered annotations.
Section 2 discusses related work, Section 3
present the framework?s architecture and a de-
tailed description of its components, Section 4
shows the integrated tools, Section 5 explains how
the information produced by tools is merged, and
Section 6 presents the achieved results. Finally,
Section 7 presents concluding remarks.
2 Related Work
GATE (Cunningham et al, 2002) is one of the
most used framework for building NLP systems.
However, it does not provide a controller for paral-
lel execution, it only supports the execution of ap-
plications on different machines over data shared
on the server (Bontcheva et al, 2004). However,
this solution cannot be applied in a large-scale dis-
tributed environment because the shared reposi-
tory becomes a bottleneck in computation due to
the accesses from all the machines making com-
putations.
UIMA (Ferrucci and Lally, 2004) is also used
to build NLP systems, and this framework sup-
ports replication of pipeline components to im-
prove throughput on multi-processor or multi-
machine platforms. However, we did not find
any published results regarding the parallel execu-
tion. The UIMA framework has been successfully
leveraged (Egner et al, 2007) with Condor1, a
manager of loosely coupled compute resources, al-
lowing the parallel execution of multiple instances
of the NLP system built with UIMA. The Condor
scheduler allows to solve problems where there is
no communication between tasks and complicates
the development of parallel applications when this
interaction is needed, like in our case, where it
is necessary to merge multiple layers of annota-
tions. Also, the Condor does not move computa-
tions closer to their input data, like the MapReduce
approach.
The MapReduce paradigm has already been
successfully adopted by the Ontea semantic anno-
tator (Laclav??k et al, 2008). We think that GATE
and UIMA frameworks could also benefit with
the MapReduce. For example, the NLTK (Loper
and Bird, 2002) adopted this paradigm, and al-
ready have implementations of some algorithms
like term frequency-inverse document frequency
(tf-idf) or expectation-maximization (EM).
There are already tools for merging of layered
1http://www.cs.wisc.edu/condor/


	





 


	










	








Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 33?40
Manchester, August 2008
Mixed-Source Multi-Document Speech-to-Text Summarization
Ricardo Ribeiro
INESC ID Lisboa/ISCTE/IST
Spoken Language Systems Lab
Rua Alves Redol, 9
1000-029 Lisboa, Portugal
rdmr@l2f.inesc-id.pt
David Martins de Matos
INESC ID Lisboa/IST
Spoken Language Systems Lab
Rua Alves Redol, 9
1000-029 Lisboa, Portugal
david@l2f.inesc-id.pt
Abstract
Speech-to-text summarization systems
usually take as input the output of an
automatic speech recognition (ASR)
system that is affected by issues like
speech recognition errors, disfluencies, or
difficulties in the accurate identification
of sentence boundaries. We propose the
inclusion of related, solid background
information to cope with the difficulties
of summarizing spoken language and the
use of multi-document summarization
techniques in single document speech-
to-text summarization. In this work, we
explore the possibilities offered by pho-
netic information to select the background
information and conduct a perceptual
evaluation to better assess the relevance of
the inclusion of that information. Results
show that summaries generated using
this approach are considerably better than
those produced by an up-to-date latent
semantic analysis (LSA) summarization
method and suggest that humans prefer
summaries restricted to the information
conveyed in the input source.
1 Introduction
News have been the subject of summarization
for a long time, demonstrating the importance
of both the subject and the process. Systems
like NewsInEssence (Radev et al, 2005), News-
blaster (McKeown et al, 2002), or even Google
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
News substantiate this relevance that is also sup-
ported by the spoken language scenario, where
most speech summarization systems concentrate
on broadcast news (McKeown et al, 2005). Nev-
ertheless, although the pioneering efforts on sum-
marization go back to the work of Luhn (1958)
and Edmundson (1969), it is only after the re-
naissance of summarization as a research area of
great activity?following up on the Dagstuhl Sem-
inar (Endres-Niggemeyer et al, 1995)?that the
first multi-document news summarization system,
SUMMONS (McKeown and Radev, 1995), makes
its breakthrough (Radev et al, 2005; Sp?arck Jones,
2007). In what concerns speech summarization,
the state of affairs is more problematic: news sum-
marization systems appeared later and still focus
only on single document summarization (McKe-
own et al, 2005). In fact, while text summarization
has attained some degree of success (Hovy, 2003;
McKeown et al, 2005; Sp?arck Jones, 2007) due to
the considerable body of work, speech summariza-
tion still requires further research, both in speech
and text analysis, in order to overcome the specific
challenges of the task (McKeown et al, 2005; Fu-
rui, 2007). Issues like speech recognition errors,
disfluencies, and difficulties in accurately identi-
fying sentence boundaries must be taken into ac-
count when summarizing spoken language. How-
ever, if on the one hand, recognition errors seem
not to have a considerable impact on the summa-
rization task (Murray et al, 2006; Murray et al,
2005), on the other hand, spoken language summa-
rization systems often explore ways of minimizing
that impact (Zechner and Waibel, 2000; Hori et al,
2003; Kikuchi et al, 2003).
We argue that by including related solid back-
ground information from a different source less
prone to this kind of errors (e.g., a textual source)
33
in the summarization process, we are able to re-
duce the influence of recognition errors on the re-
sulting summary. To support this argument, we de-
veloped a new approach to speech-to-text summa-
rization that combines information from multiple
information sources to produce a summary driven
by the spoken language document to be summa-
rized. The idea mimics the natural human behav-
ior, in which information acquired from different
sources is used to build a better understanding of
a given topic (Wan et al, 2007). Furthermore, we
build on the conjecture that this background infor-
mation is often used by humans to overcome per-
ception difficulties. In that sense, one of our goals
is also to understand what is expected in a sum-
mary: a comprehensive, shorter, text that addresses
the same subject of the input source to be summa-
rized (possibly introducing new information); or a
text restricted to the information conveyed in the
input source.
This work explores the use of phonetic domain
information to overcome speech recognition errors
and disfluencies. Instead of using the traditional
output of the ASR module, we use the phonetic
transliteration of the output and compare it to the
phonetic transliteration of solid background infor-
mation. This enables the use of text, related to the
input source, free from the common speech recog-
nition issues, in further processing.
We use broadcast news as a case study and
news stories from online newspapers provide the
background information. Media monitoring sys-
tems, used to transcribe and disseminate news,
provide an adequate framework to test the pro-
posed method.
This document is organized as follows: sec-
tion 2 briefly introduces the related work; section
3 presents a characterization of the speech-to-text
summarization problem and how we propose to
address it; section 4 explicits our use of phonetic
domain information, given the previously defined
context; the next section describes the case study,
including the experimental set up and results; con-
clusions close the document.
2 Related Work
McKeown et al (2005) depict spoken language
summarization as a much harder task than text
summarization. In fact, the previously enumerated
problems that make speech summarization such
a difficult task constrain the applicability of text
summarization techniques to speech summariza-
tion (although in the presence of planned speech,
as it partly happens in the broadcast news domain,
that portability is more feasible (Christensen et al,
2003)). On the other hand, speech offers possibili-
ties like the use of prosody and speaker identifica-
tion to ascertain relevant content.
Furui (2007) identifies three main approaches
to speech summarization: sentence extraction-
based methods, sentence compaction-based meth-
ods, and combinations of both.
Sentence extractive methods comprehend, es-
sentially, methods like LSA (Gong and Liu,
2001), Maximal Marginal Relevance (Carbonell
and Goldstein, 1998), and feature-based meth-
ods (Edmundson, 1969). Feature-based methods
combine several types of features: current work
uses lexical, acoustic/prosodic, structural, and dis-
course features to summarize documents from do-
mains like broadcast news or meetings (Maskey
and Hirschberg, 2005; Murray et al, 2006; Ribeiro
and de Matos, 2007). Even so, spoken language
summarization is still quite distant from text sum-
marization in what concerns the use of discourse
features, and shallow approaches is what can be
found in state-of-the-art work such as the one pre-
sented by Maskey and Hirschberg (2005) or Mur-
ray et al (2006). Sentence compaction methods
are based on word removal from the transcription,
with recognition confidence scores playing a ma-
jor role (Hori et al, 2003). A combination of these
two types of methods was developed by Kikuchi
et al (2003), where summarization is performed
in two steps: first, sentence extraction is done
through feature combination; second, compaction
is done by scoring the words in each sentence and
then a dynamic programming technique is applied
to select the words that will remain in the sentence
to be included in the summary.
3 Problem Characterization
Summarization can be seen as a reductive transfor-
mation ? that, given an input source I , produces a
summary S:
S = ?(I),
where len(S) < len(I) and inf (S) is as close
as possible of inf (I); len() is the length of the
given input and inf () is the information conveyed
by its argument.
The problem is that in order to compute S, we
are not using I , but
?
I , a noisy representation of I .
34
Thus, we are computing
?
S, which is a summary
affected by the noise present in
?
I:
?
S = ?(
?
I).
This means that
inf (
?
S) ? inf (S) ? inf (I), whereas
len(
?
S) ? len(S) < len(I).
Our argument is that using a similar reductive
transformation ?, where solid background infor-
mation B is also given as input, it is possible to
compute a summary
?
S:
?
S = ?(
?
I,B), such that
inf (
?
S) ? (inf (
?
S) ? inf (S)) ? inf (I), with
len(
?
S) ? len(
?
S) ? len(S) < len(I).
As seen in section 2, the most common method
to perform these transformations is by selecting
sentences (or extracts) from the corresponding in-
put sources.
Thus, let the input source representation
?
I be
composed by a sequence of extracts e
i
,
?
I = e
1
, e
2
, . . . , e
n
and the background information be defined as a
sequence of sentences
B = s
1
, s
2
, . . . , s
m
.
The proposed method consists of selecting sen-
tences s
i
form the background information B such
that
sim(s
i
, e
j
) < ? ? 0 ? i ? m ? 0 ? j ? n,
with sim() being a similarity function and ? an
adequate threshold. The difficulty lies in defining
the function and the threshold.
4 Working in the phonetic domain
The approach we introduce minimizes the effects
of recognition errors through the selection, from
previously determined background knowledge, of
sentence-like units close to the ones of the news
story transcription. In order to select sentence-like
units, while diminishing recognition problems, we
compute the similarity between them at the pho-
netic level. The estimation of the threshold is
based on the distance, measured in the phonetic
Feature Values
Type vowel, consonant
Vowel length short, long, diphthong,
schwa
Vowel height high, mid, low
Vowel frontness front mid back
Lip rounding yes, no
Consonant type stop, fricative, affricative,
nasal, liquid
Place of articulation labial, alveolar, palatal,
labio-dental, dental, velar
Consonant voicing yes, no
Table 1: Phone features.
domain, between the output of the ASR and its
hand-corrected version.
The selection of sentences from the background
information is based on the alignment cost of the
phonetic transcriptions of sentences from the input
source and sentence from the background informa-
tion. Sentences from the background information
with alignment costs below the estimated threshold
are selected to be used in summary generation.
4.1 Similarity Between Segments
There are several ways to compute phonetic simi-
larity. Kessler (2005) states that phonetic distance
can be seen as, among other things, differences
between acoustic properties of the speech stream,
differences in the articulatory positions during pro-
duction, or as the perceptual distance between iso-
lated sounds. Choosing a way to calculate phonetic
distance is a complex process.
The phone similarity function used in this pro-
cess is based on a model of phone production,
where the phone features correspond to the articu-
latory positions during production: the greater the
matching between phone features, the smaller the
distance between phones. The phone features used
are described in table 1.
The computation of the similarity between
sentence-like units is based on the alignment of
the phonetic transcriptions of the given segments.
The generation of the possible alignments and the
selection of the best alignment is done through
the use of Weighted Finite-State Transducers (WF-
STs) (Mohri, 1997; Paulo and Oliveira, 2002).
35
4.2 Threshold Estimation Process
To estimate the threshold to be used in the sentence
selection process, we use the algorithm presented
in figure 1. The procedure consists of comparing
automatic transcriptions and their hand-corrected
versions: the output is the average difference be-
tween the submitted inputs.
Phonetic 
transliteration
Phonetic 
transliteration
Sentence segmented 
ASR output
Manual transcription
Projection of the 
sentences of 
the ASR ouput 
over the manual 
transcription
Sentence segmented 
Manual transcription
Sentence-by-
sentence 
distance 
calculation
Figure 1: Threshold estimation process.
The idea is that the phonetic distance between
the automatic transcription and its hand-corrected
version would be similar to the phonetic distance
between the automatic transcription and the back-
ground information. Even though this heuristic
may appear naif, we believe it is adequate as a
rough approach, considering the target material
(broadcast news).
5 A Case Study Using Broadcast News
5.1 Media Monitoring System
SSNT (Amaral et al, 2007) is a system for selec-
tive dissemination of multimedia contents, work-
ing primarily with Portuguese broadcast news ser-
vices. The system is based on an ASR mod-
ule, that generates the transcriptions used by
the topic segmentation, topic indexing, and ti-
tle&summarization modules. User profiles enable
the system to deliver e-mails containing relevant
news stories. These messages contain the name
of the news service, a generated title, a summary,
a link to the corresponding video segment, and a
classification according to a thesaurus used by the
broadcasting company.
Preceding the speech recognition module, an au-
dio preprocessing module, based on Multi-layer
Perceptrons, classifies the audio in accordance to
several criteria: speech/non-speech, speaker seg-
mentation and clustering, gender, and background
conditions.
The ASR module, based on a hybrid speech
recognition system that combines Hidden Markov
Models with Multi-layer Perceptrons, with an av-
erage word error rate of 24% (Amaral et al, 2007),
greatly influences the performance of the subse-
quent modules.
The topic segmentation and topic indexing
modules were developed by Amaral and Tran-
coso (2004). Topic segmentation is based on clus-
tering and groups transcribed segments into sto-
ries. The algorithm relies on a heuristic derived
from the structure of the news services: each story
starts with a segment spoken by the anchor. This
module achieved an F -measure of 68% (Amaral
et al, 2007). The main problem identified by the
authors was boundary deletion: a problem which
impacts the summarization task. Topic indexing is
based on a hierarchically organized thematic the-
saurus provided by the broadcasting company. The
hierarchy has 22 thematic areas on the first level,
for which the module achieved a correctness of
91.4% (Amaral et al, 2006; Amaral et al, 2007).
Batista et al (2007) inserted a module for re-
covering punctuation marks, based on maximum
entropy models, after the ASR module. The punc-
tuation marks addressed were the ?full stop? and
?comma?, which provide the sentence units nec-
essary for use in the title&summarization mod-
ule. This module achieved an F -measure of 56%
and SER (Slot Error Rate, the measure commonly
used to evaluate this kind of task) of 0.74.
Currently, the title&summarization module pro-
duces a summary composed by the first n sen-
tences, as detected by the previous module, of each
news story and a title (the first sentence).
5.2 Corpora
Two corpora were used in this experiment: a
broadcast news corpus, the subject of our summa-
rization efforts; and a written newspaper corpus,
used to select the background information.
36
Corpus Stories SUs Tokens Duration
train 184 2661 57063 5h
test 26 627 7360 1h
Table 2: Broadcast news corpus composition.
The broadcast news corpus is composed by 6
Portuguese news programs, and exists in two ver-
sions: an automatically processed one, and a hand-
corrected one. Its composition (number of stories,
number of sentence-like units (SUs), number of to-
kens, and duration) is detailed in table 2. To es-
timate the threshold used for the selection of the
background information, 5 news programs were
used. The last one was used for evaluation.
The written newspaper corpus consists of the
online version a Portuguese newspaper, down-
loaded daily from the Internet. In this experiment,
three editions of the newspaper were used, corre-
sponding to the day and the two previous days of
the news program to be summarized. The corpus
is composed by 135 articles, 1418 sentence-like
units, and 43102 tokens.
5.3 The Summarization Process
The summarization process we implemented is
characterized by the use of LSA to compute the
relevance of the extracts (sentence-like units) of
the given input source.
LSA is based on the singular vector decomposi-
tion (SVD) of the term-sentence frequency m? n
matrix, M . U is an m ? n matrix of left singular
vectors; ? is the n? n diagonal matrix of singular
values; and, V is the n?n matrix of right singular
vectors (only possible if m ? n):
M = U?V
T
The idea behind the method is that the decom-
position captures the underlying topics of the doc-
ument by means of co-occurrence of terms (the la-
tent semantic analysis), and identifies the best rep-
resentative sentence-like units of each topic. Sum-
mary creation can be done by picking the best rep-
resentatives of the most relevant topics according
to a defined strategy.
For this summarization process, we imple-
mented a module following the original ideas of
Gong and Liu (2001) and the ones of Murray, Re-
nals, and Carletta (2005) for solving dimensional-
ity problems, and using, for matrix operations, the
GNU Scientific Library
1
.
5.4 Experimental Results
Our main objective was to understand if it is pos-
sible to select relevant information from back-
ground information that could improve the quality
of speech-to-text summaries. To assess the valid-
ity of this hypothesis, five different processes of
generating a summary were considered. To bet-
ter analyze the influence of the background in-
formation, all automatic summarization methods
are based on the up-to-date LSA method previ-
ously described: one taking as input only the news
story to be summarized (Simple) and used as base-
line; other taking as input only the selected back-
ground information (Background only); and, the
last one, using both the news story and the back-
ground information (Background + News). The
other two processes were human: extractive (using
only the news story) and abstractive (understand-
ing the news story and condensing it by means
of paraphrase). Since the abstractive summaries
had already been created, summary size was de-
termined by their size (which means creating sum-
maries using a compression rate of around 10% of
the original size).
As mentioned before, the whole summariza-
tion process begins with the selection of the back-
ground information. Using the threshold estimated
as described in section 4.2 and the method de-
scribed in section 4.1 to compute similarity be-
tween sentence-like units, no background informa-
tion was selected for 11 of the 26 news stories of
the test corpus. For the remaining 15 news sto-
ries, summaries were generated using the three au-
tomatic summarization strategies described before.
In what concerns the evaluation process, al-
though ROUGE (Lin, 2004) is the most common
evaluation metric for the automatic evaluation of
summarization, since our approach might intro-
duce in the summary information that it is not
present in the original input source, we found that a
human evaluation was more adequate to assess the
relevance of that additional information. A percep-
tual evaluation is also adequate to assess the per-
ceive quality of the summaries and a better indica-
tor of the what is expected to be in a summary.
We asked an heterogeneous group of sixteen
people to evaluate the summaries created for the
15 news stories for which background information
1
http://www.gnu.org/software/gsl/
37
0 20 40 60 80 100 120 
Simple (News only) 
Background only 
Background + News 
Human Extractive 
Human Abstractive ns00 
ns01 
ns02 
ns03 
ns04 
ns05 
ns06 
ns07 
ns08 
ns09 
ns10 
ns11 
ns12 
ns13 
ns14 
Figure 2: Overall results for each summary cre-
ation method (nsnn identifies a news story).
was selected. Each evaluator was given, for each
story, the news story itself (without background in-
formation) and five summaries, corresponding to
the five different methods presented before. The
evaluation procedure consisted in identifying the
best summary and in the classification of each
summary (1?5, 5 is better) according to its content
and readability (which covers issues like grammat-
icality, existence of redundant information, or en-
tity references (Nenkova, 2006)).
0% 
10% 
20% 
30% 
40% 
50% 
60% 
70% 
80% 
90% 
100% 
ns00 ns01 ns02 ns03 ns04 ns05 ns06 ns07 ns08 ns09 ns10 ns11 ns12 ns13 ns14 
Simple (News only) 
Background only 
Background + News 
Human Extractive 
Human Abstractive 
Figure 3: Relative results for each news story
(nsnn identifies a news story; stack order is inverse
of legend order).
Surprisingly enough (see figures 2 and 3), in
general, the extractive human summaries were pre-
ferred over the abstractive ones. Moreover, the
summaries generated automatically using back-
ground information (exclusively or not) were also
selected as best summary (over the human created
ones) a non-negligible number of times. The poor-
est performance was attained, as expected, by the
simple LSA summarizer, only preferred on two
news stories for which all summaries were very
similar. The results of the two approaches using
background information were very close, a result
that can be explained by the fact the summaries
generated by these two approaches were equal for
11 of the 15 news stories (in the remaining 4, the
average distribution was 31.25% from the news
story versus 68.75% from the background infor-
mation).
Figure 4 further discriminates the results in
terms of content and readability.
0.00 
0.50 
1.00 
1.50 
2.00 
2.50 
3.00 
3.50 
4.00 
4.50 
5.00 
Simple (News 
only) 
Background 
only 
Background + 
News 
Human 
Extractive 
Human 
Abstractive 
content readability 
Figure 4: Average of the content and readability
scores for each summary creation method.
Regarding content, the results suggest that the
choice of the best summary is highly correlated
with its content, as the average content scores
mimic the overall ones of figure 2. In what con-
cerns readability, the summaries generated using
background information achieved the best results.
The reasons underlying these results are that the
newspaper writing is naturally better planned than
speech and that speech transcriptions are affected
by the several problems described before (and the
original motivation for the work), hence the idea
of using them as background information. How-
ever, what is odd is that the result obtained by
the human abstractive summary creation method
is worse than the ones obtained by automatic
generation using background information, which
could suffer from coherence and cohesion prob-
lems. One possible explanation is that the human
abstractive summaries tend to mix both informa-
38
tive and indicative styles of summary.
0.00 
0.20 
0.40 
0.60 
0.80 
1.00 
1.20 
Simple (News 
only) 
Background 
only 
Background + 
News 
Human 
Extractive 
Human 
Abstractive 
content readability 
Figure 5: Standard deviation of the content and
readability scores.
Figure 5 presents the standard deviation for con-
tent and readability scores: concerning content,
automatically generated summaries using back-
ground information achieved the highest standard
deviation scores (see also figure 6 for a sample
story). That is in part supported by some commen-
taries made by the human evaluators on whether
a summary should contain information that is not
present in the input source. This aspect and the ob-
tained results, suggest that this issue should be fur-
ther analyzed, possibly using an extrinsic evalua-
tion setup. On the other hand, readability standard
deviation scores show that there is a considerable
agreement in what concerns this criterion.
0.00 
0.50 
1.00 
1.50 
2.00 
2.50 
3.00 
3.50 
4.00 
4.50 
5.00 
Simple (News 
only) 
Background 
only 
Background + 
News 
Human 
Extractive 
Human 
Abstractive 
Content (avg) Readability (avg) 
Content (stdev) Readability (stdev) 
Figure 6: Average and standard deviation of the
content and readability scores for one news story.
6 Conclusions
We present a new approach to speech summariza-
tion that goes in the direction of the integration of
text and speech analysis, as suggested by McKe-
own et al (2005). The main idea is the inclusion
of related, solid background information to cope
with the difficulties of summarizing spoken lan-
guage and the use of multi-document summariza-
tion techniques in single document speech-to-text
summarization. In this work, we explore the pos-
sibilities offered by phonetic information to select
the background information and conducted a per-
ceptual evaluation to assess the relevance of the in-
clusion of that information.
The results obtained show that the human eval-
uators preferred human extractive summaries over
human abstractive summaries. Moreover, simple
LSA summaries attained the poorest results both in
terms of content and readability, while human ex-
tractive summaries achieved the best performance
in what concerns content, and a considerably bet-
ter performance than simple LSA in what concerns
readability. This suggests that it is sill relevant to
pursue new methods for relevance estimation. On
the other hand, automatically generated summaries
using background information were significantly
better than simple LSA. This indicates that back-
ground information is a viable way to increase the
quality of automatic summarization systems.
References
Amaral, R. and I. Trancoso. 2004. Improving the Topic
Indexation and Segmentation Modules of a Media
Watch System. In Proceedings of INTERSPEECH
2004 - ICSLP, pages 1609?1612. ISCA.
Amaral, R., H. Meinedo, D. Caseiro, I. Trancoso, and
J. P. Neto. 2006. Automatic vs. Manual Topic Seg-
mentation and Indexation in Broadcast News. In
Proc. of the IV Jornadas en Tecnologia del Habla.
Amaral, R., H. Meinedo, D. Caseiro, I. Trancoso, and
J. P. Neto. 2007. A Prototype System for Selective
Dissemination of Broadcast News in European Por-
tuguese. EURASIP Journal on Advances in Signal
Processing, 2007.
Batista, F., D. Caseiro, N. J. Mamede, and I. Tran-
coso. 2007. Recovering Punctuation Marks for Au-
tomatic Speech Recognition. In Proceedings of IN-
TERSPEECH 2007, pages 2153?2156. ISCA.
Carbonell, J. and J. Goldstein. 1998. The Use of MMR,
Diversity-Based Reranking for Reordering Docu-
ments and Producing Summaries. In SIGIR 1998:
Proceedings of the 21
st
Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 335?336. ACM.
Christensen, H., Y. Gotoh, B. Kolluru, and S. Renals.
2003. Are Extractive Text Summarisation Tech-
niques Portable To Broadcast News? In Proceedings
39
of the IEEE Workshop on Automatic Speech Recog-
nition and Understanding, pages 489?494. IEEE.
Edmundson, H. P. 1969. New methods in automatic
abstracting. Journal of the Association for Comput-
ing Machinery, 16(2):264?285.
Endres-Niggemeyer, B., J. R. Hobbs, and K. Sp?arck
Jones, editors. 1995. Summarizing Text
for Intelligent Communication?Dagstuhl-Seminar-
Report 79. IBFI.
Furui, S. 2007. Recent Advances in Automatic Speech
Summarization. In Proceedings of the 8
th
Confer-
ence on Recherche d?Information Assist?ee par Or-
dinateur (RIAO). Centre des Hautes
?
Etudes Interna-
tionales d?Informatique Documentaire.
Gong, Y. and X. Liu. 2001. Generic Text Summariza-
tion Using Relevance Measure and Latent Semantic
Analysis. In SIGIR 2001: Proceedings of the 24
st
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 19?25. ACM.
Hori, T., C. Hori, and Y. Minami. 2003. Speech Sum-
marization using Weighted Finite-State Transducers.
In Proceedings of the 8
th
EUROSPEECH - INTER-
SPEECH 2003, pages 2817?2820. ISCA.
Hovy, E., 2003. The Oxford Handbook of Compu-
tational Linguistics, chapter Text Summarization,
pages 583?598. Oxford University Press.
Kessler, B. 2005. Phonetic comparison algo-
rithms. Transactions of the Philological Society,
103(2):243?260.
Kikuchi, T., S. Furui, and C. Hori. 2003. Two-
stage Automatic Speech Summarization by Sen-
tence Extraction and Compaction. In Proceedings
of the ISCA & IEEE Workshop on Spontaneous
Speech Processing and Recognition (SSPR-2003),
pages 207?210. ISCA.
Lin, C. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81. ACL.
Luhn, H. P. 1958. The Automatic Creation of Litera-
ture Abstracts. IBM Journal of Research and Devel-
opment, 2(2):159?165.
Maskey, S. and J. Hirschberg. 2005. Comparing Lexi-
cal, Acoustic/Prosodic, Strucural and Discourse Fea-
tures for Speech Summarization. In Proceedings
of the 9
th
EUROSPEECH - INTERSPEECH 2005,
pages 621?624. ISCA.
McKeown, K. R. and D. Radev. 1995. Generating
Summaries of Multiple News Articles. In SIGIR
1995: Proceedings of the 18
th
Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 74?82. ACM.
McKeown, K. R., R. Barzilay, D. Evans, V. Hatzi-
vassiloglou, J. L. Klavans, A. Nenkova, C. Sable,
B. Schiffman, and S. Sigelman. 2002. Track-
ing and Summarizing News on a Daily Basis with
Columbia?s Newsblaster. In Proc. of the 2
nd
Inter-
national Conference on Human Language Technol-
ogy Research, pages 280?285. Morgan Kaufmann.
McKeown, K. R., J. Hirschberg, M. Galley, and
S. Maskey. 2005. From Text to Speech Summa-
rization. In 2005 IEEE International Conference on
Acoustics, Speech, and Signal Processing. Proceed-
ings, volume V, pages 997?1000. IEEE.
Mohri, M. 1997. Finite-State Transducers in Language
and Speech Processing. Computational Linguistics,
23(2):269?311.
Murray, G., S. Renals, and J. Carletta. 2005. Extractive
Summarization of Meeting Records. In Proceedings
of the 9
th
EUROSPEECH - INTERSPEECH 2005,
pages 593?596. ISCA.
Murray, G., S. Renals, J. Carletta, and J. Moore.
2006. Incorporating Speaker and Discourse Features
into Speech Summarization. In Proceedings of the
HLT/NAACL, pages 367?374. ACL.
Nenkova, A. 2006. Summarization Evaluation for Text
and Speech: Issues and Approaches. In Proceedings
of INTERSPEECH 2006 - ICSLP, pages 1527?1530.
ISCA.
Paulo, S. and L. C. Oliveira. 2002. Multilevel Annota-
tion Of Speech Signals Using Weighted Finite State
Transducers. In Proc. of the 2002 IEEE Workshop
on Speech Synthesis, pages 111?114. IEEE.
Radev, D., J. Otterbacher, A. Winkel, and S. Blair-
Goldensohn. 2005. NewsInEssence: Summarizing
Online News Topics. Communications of the ACM,
48(10):95?98.
Ribeiro, R. and D. M. de Matos. 2007. Extractive Sum-
marization of Broadcast News: Comparing Strate-
gies for European Portuguese. In Text, Speech and
Dialogue ? 10
th
International Conference. Proceed-
ings, volume 4629 of Lecture Notes in Computer Sci-
ence (Subseries LNAI), pages 115?122. Springer.
Sp?arck Jones, K. 2007. Automatic summarising: The
state of the art. Information Processing and Man-
agement, 43:1449?1481.
Wan, X., J. Yang, and J. Xiao. 2007. CollabSum: Ex-
ploiting Multiple Document Clustering for Collabo-
rative Single Document Summarizations. In SIGIR
2007: Proc. of the 30
th
Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 143?150. ACM.
Zechner, K. and A. Waibel. 2000. Minimizing Word
Error Rate in Textual Summaries of Spoken Lan-
guage. In Proceedings of the 1
st
conference of the
North American chapter of the ACL, pages 186?193.
Morgan Kaufmann.
40
