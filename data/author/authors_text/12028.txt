Proceedings of the EACL 2009 Workshop on Language Technologies for African Languages ? AfLaT 2009, pages 112?120,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
An Ontology for Accessing Transcription Systems (OATS)
Steven Moran
University of Washington
Seattle, WA, USA
stiv@u.washington.edu
Abstract
This paper presents the Ontology for Ac-
cessing Transcription Systems (OATS), a
knowledge base that supports interopera-
tion over disparate transcription systems
and practical orthographies. The knowl-
edge base includes an ontological descrip-
tion of writing systems and relations for
mapping transcription system segments
to an interlingua pivot, the IPA. It in-
cludes orthographic and phonemic inven-
tories from 203 African languages. OATS
is motivated by the desire to query data in
the knowledge base via IPA or native or-
thography, and for error checking of dig-
itized data and conversion between tran-
scription systems. The model in this paper
implements these goals.
1 Introduction
The World Wide Web has emerged as the pre-
dominate source for obtaining linguistic field data
and language documentation in textual, audio and
video formats. A simple keyword search on the
nearly extinct language Livonian [liv]1 returns nu-
merous results that include text, audio and video
files. As data on the Web continue to increase, in-
cluding material posted by native language com-
munities, researchers are presented with an ideal
medium for the automated discovery and analysis
of linguistic data, e.g. (Lewis, 2006). However,
resources on the Web are not always accessible to
users or software agents. The data often exist in
legacy or proprietary software and data formats.
This makes them difficult to locate and access.
Interoperability of linguistic resources has the
ability to make disparate linguistic data accessible
to researchers. It is also beneficial for data aggre-
gation. Through the use of ontologies, applica-
1ISO 639-3 language codes are in [].
tions can be written to perform intelligent search
(deriving implicit knowledge from explicit infor-
mation). They can also interoperate between re-
sources, thus allowing data to be shared across ap-
plications and between research communities with
different terminologies, annotations, and notations
for marking up data.
OATS is a knowledge base, i.e. a data source
that uses an ontology to specify the structure of
entities and their relations. It includes general
knowledge of writing systems and transcription
systems that are core to the General Ontology of
Linguistic Description (GOLD)2 (Farrar and Lan-
gendoen, 2003). Other portions of OATS, in-
cluding the relationships encoded for relating seg-
ments of transcription systems, or the computa-
tional representations of these elements, extend
GOLD as a Community of Practice Extension
(COPE) (Farrar and Lewis, 2006). OATS provides
interoperability for transcription systems and prac-
tical orthographies that map phones and phonemes
in unique relationships to their graphemic repre-
sentations. These systematic mappings thus pro-
vide a computationally tractable starting point for
interoperating over linguistic texts. The resources
that are targeted also encompass a wide array of
data on lesser-studied languages of the world, as
well as low density languages, i.e. those with few
electronic resources (Baldwin et al, 2006).
This paper is structured as follows: in section
2, linguistic and technological definitions and ter-
minology are provided. In section 3, the theoreti-
cal and technological challenges of interoperating
over heterogeneous transcriptions systems are de-
scribed. The technologies used in OATS and its
design are presented in section 4. In section 5,
OATS? implementation is illustrated with linguis-
tic data that was mined from the Web, therefore
motivating the general design objectives taken into
2http://linguistics-ontology.org/
112
account in its development. Section 6 concludes
with future research goals.
2 Conventions and Terminology
2.1 Conventions
Standard conventions are used for distinguishing
between graphemic < >, phonemic / / and pho-
netic representations [ ].3 For character data infor-
mation, I follow the Unicode Standard?s notational
conventions (The Unicode Consortium, 2007).
Character names are represented in small capi-
tal letters (e.g. LATIN SMALL LETTER SCHWA)
and code points are expressed as ?U+n? where n
is a four to six digit hexadecimal number (e.g.
U+0256), which is rendered as <@>.
2.2 Linguistic definitions
In the context of this paper, a transcription sys-
tem is a system of symbols and rules for graphi-
cally transcribing the sounds of a language variety.
A practical orthography is a phonemic writing
system designed for practical use by speakers al-
ready competent in the language. The mapping re-
lation between phonemes and graphemes in prac-
tical orthographies is purposely shallow, i.e. there
is a faithful mapping from a unique sound to a
unique symbol.4 The IPA is often used by field
linguists in the development of practical orthogra-
phies for languages without writing systems. An
orthography specifies the symbols, punctuation,
and the rules in which a language is correctly writ-
ten in a standardized way. All orthographies are
language specific.
Practical orthographies and transcription sys-
tems are both kinds of writing systems. A writing
system is a symbolic system that uses visible or
tactile signs to represent a language in a systematic
way. Differences in the encoding of meaning and
sound form a continuum for representing writing
systems in a typology whose categories are com-
monly referred to as either logographic, syllabic,
phonetic or featural. A logographic system de-
notes symbols that visually represent morphemes
(and sometimes morphemes and syllables). A
syllabic system uses symbols to denote syllables.
A phonetic system represents sound segments as
3Phonemic and phonetic representations are given in the
International Phonetic Alphabet (IPA).
4Practical orthographies are intended to jump-start written
materials development by correlating a writing system with
its sound units, making it easier for speakers to master and
acquire literacy.
symbols. Featural systems are less common and
encode phonological features within the shapes of
the symbols represented in the script.
The term script refers to a collection of sym-
bols (or distinct marks) as employed by a writ-
ing system. The term script is confused with and
often used interchangeably with ?writing system?.
A writing system may be written with different
scripts, e.g. the alphabet writing system can be
written in Roman and Cyrillic scripts (Coulmas,
1999). A grapheme is the unit of writing that
represents a particular abstract representation of a
symbol employed by a writing system. Like the
phoneme is an abstract representation of a distinct
sound in a language, a grapheme is a contrastive
graphical unit in a writing system. A grapheme
is the basic, minimally distinctive symbol of a
writing system. A script may employ multiple
graphemes to represent a single phoneme, e.g. the
graphemes <c> and <h> when conjoined in En-
glish represent one phoneme in English, <ch>
pronounced /?/ (or /k/). The opposite is also found
in writing systems, where a single grapheme rep-
resents two or more phonemes, e.g. <x> in En-
glish is a combination of the phonemes /ks/.
A graph is the smallest unit of written language
(Coulmas, 1999). The electronic counterpart of
the graph is the glyph. Glyphs represent the varia-
tion of graphemes as they appear when rendered or
displayed. In typography glyphs are created using
different illustration techniques. These may result
in homoglyphs, pairs of characters with shapes
that are either identical or are beyond differenti-
ation by swift visual inspection. When rendered
by hand, a writer may use different styles of hand-
writing to produce glyphs in standard handwriting,
cursive, or calligraphy. When rendered computa-
tionally, a repertoire of glyphs makes up a font.
A final distinction is needed for interoperating
over transcription systems. The term scripteme
is used for the use of a grapheme within a writ-
ing system with the particular semantics (i.e., pro-
nunciation) it is assigned within that writing sys-
tem. The notion scripteme is needed because
graphemes may be homoglyphic across scripts and
languages, and the semantics of a grapheme is de-
pendent on the writing system using it. For ex-
ample, the grapheme <p> in Russian represents a
dental or alveolar trill; /r/ in IPA. However, <p> is
realized by English speakers as a voiceless bilabial
stop /p/. The defining of scripteme is necessary
113
for interoperability because it provides a level for
mapping a writing system specific grapheme to the
phonological level, allowing the same grapheme
to represent different sounds across different tran-
scription and writing systems.
2.3 Technological definitions
A document refers to an electronic document that
contains language data. Each document is associ-
ated with metadata and one or more transcription
systems or practical orthographies. A document?s
content is comprised of a set scriptemes from its
transcription system. A mapping relation is an
unordered pair of a scripteme in a transcription
system and its representation in IPA.
OATS first maps scriptemes to their grapheme
equivalent(s). Graphemes are then mapped to
their character equivalents. A character in OATS
is a computational representation of a grapheme.
Character encodings represent a range of inte-
gers known as the code space. A code point is
a unique integer, or point, within this code space.
An abstract character is then mapped to a unique
code point and rendered as an encoded charac-
ter and typographically defined by the font used
to render it. A set of encoded characters is a char-
acter set and different character encodings en-
code characters as numbers via different encoding
schemes.
3 Interoperating Over Transcription
Systems
Section 3.1 uses the Sisaala languages to illus-
trate interoperability challenges posed by linguis-
tic data. Section 3.2 addresses technological is-
sues including encoding and ambiguity.
3.1 Linguistic challenges
Three genetically related languages spoken in
Northern Ghana, Sisaala Pasaale [sig], Sisaala
Tumulung [sil] and Sisaala Western [ssl], differ
slightly in their orthographies for two reasons:
they have slightly divergent phonemic inventories
and their orthographies may differ graphemically
when representing the same phoneme. See Table
1.
The voiceless labial-velar phoneme /kp/ appears
in both Sisaala Tumulung and Sisaala Pasaale, but
has been lost in Sisaala Western. There is a con-
vergence of the allophones [d] and [r] into one
Table 1: Phoneme-to-grapheme relations
/kp/ d /?/ /I/ /U/ Tone
sig kp d, r ky ? V not marked
sil kp d ch i u accents
ssl - d ky I U accents
phoneme /d/ in Sisaala Pasaale (Toupin, 1995).5
These three orthographies also differ because of
their authors? choices in assigning graphemes to
phonemes. In Sisaala Pasaale and Sisaala West-
ern, the phonemes /?/ and /?/ are written as <ky>
and <gy>. In Sisaala Tumulung, however, these
sounds are written <ch> and <j>. Orthography
developers may have made these choices for prac-
tical reasons, such as ease of learnability or tech-
nological limitations (Bodomo, 1997). During the
development of practical orthographies for Sisaala
Pasaale and Sisaala Western, the digraphs <ky>
and<gy>were chosen because children learn Da-
gaare [dga] in schools, so they are already famil-
iar with their sounds in the Dagaare orthography
(Mcgill et al, 1999) (Moran, 2008).
Another difference lies in the representation of
vowels. Both Sisaala Pasaale and Sisaala West-
ern represent their full sets of vowels orthograph-
ically. These orthographies were developed rela-
tively recently, when computers, character encod-
ings, and font support, have become less problem-
atic. In Sisaala Tumulung, however, the phonemes
/i/ and /I/ are collapsed to <i>, and /u/ and /U/ to
<u> (Blass, 1975). Sisaala Tumulung?s orthog-
raphy was developed in the 1970s and technologi-
cal limitations may have led its developers to col-
lapse these phonemes in the writing system. For
example, the Ghana Alphabet Committee?s 1990
Report lacks an individual grapheme <N> for the
phoneme /N/ for Dagaare. This difficulty of render-
ing unconventional symbols on typewriters once
posed a challenge for orthography development
(Bodomo, 1997).
Tone is both lexically and grammatically con-
trastive in Sisaala languages. In Sisaala Pasaale?s
official orthography tone is not marked and is not
used in native speaker materials. On the other
hand, in linguistic descriptions that use this or-
thography, tone is marked to disambiguate tonal
5The phoneme /d/ has morphologically conditioned al-
lographs <d> (word initial) or <r> (elsewhere) (McGill,
2004).
114
minimal pairs in lexical items and grammatical
constructions (McGill, 2004). In the Sisaala
(Tumulung)-English dictionary, tone is marked
only to disambiguate lexical items (Blass, 1975).
In linguistic descriptions of Sisaala Western, non-
contrastive tone is marked. When tone is marked,
it appears as acute (high tone) and grave (low tone)
accents over vowels or nasals.
Language researchers would quickly pick up on
these minute differences in orthographies. How-
ever, what first seem to be trivial differences, illus-
trate one issue of resource discovery on the Web ?
without methods for interoperability, even slightly
divergent resources are more difficult to discover,
query and compare. How would someone re-
searching a comparative analysis of /?/ sounds of
languages in Northern Ghana discover that it is
represented as <ky> and <ch> without first lo-
cating the extremely sparse grammatical informa-
tion available on these languages? Furthermore,
automatic phonetic research is possible on lan-
guages with shallow orthographies (Zuraw, 2006),
but crosslinguistic versions of such work require
interoperation over writing systems.
3.2 Technological challenges
The main technological challenges in interoperat-
ing over textual electronic resources are: encod-
ing multilingual language text in an interoperable
format and resolving ambiguity between mapping
relations. These are addressed below.
Hundreds of character encoding sets for writ-
ing systems have been developed, e.g. ASCII,
GB 180306 and Unicode. Historically, different
standards were formalized differently and for dif-
ferent purposes by different standards commit-
tees. A lack of interoperability between char-
acter encodings ensued. Linguists, restricted to
standard character sets that lacked IPA support
and other language-specific graphemes that they
needed, made their own solutions (Bird and Si-
mons, 2003). Some chose to represent unavailable
graphemes with substitutes, e.g. the combination
of <ng> to represent <N>. Others redefined se-
lected characters from a character encoding to map
their own fonts to. One linguist?s redefined char-
acter set, however, would not render properly on
another linguist?s computer if they did not share
the same font. If two character encodings defined
6Guo?jia? Bia?ozhu?, the national standard character set for
the People?s Republic of China
two character sets differently, then data could not
be reliably and correctly displayed.
To circumvent these problems, OATS uses the
Unicode Standard7 for multilingual character en-
coding of electronic textual data. Unicode en-
codes 76 scripts and includes the IPA.8 In principle
this allows OATS to interoperate over IPA and all
scripts currently encoded in Unicode. However,
writing systems, scripts and transcriptions are of-
ten themselves encoded ambiguously.
Unicode encodes characters, not glyphs, in
scripts and sometimes unifies duplicate characters
across scripts. For example, IPA characters of
Greek and Latin origin, such as <B> and <k>
are not given a distinct position within Unicode?s
IPA character block. The Unicode code space
is subdivided into character blocks, which gener-
ally encode characters from a single script, but as
is illustrated by the IPA, characters may be dis-
persed across several different character blocks.
This poses a challenge for interoperation, particu-
larly with regard to homographs. Why shouldn?t a
speaker of Russian use the<a> CYRILLIC SMALL
LETTER A at code point U+0430 for IPA transcrip-
tion, instead of <a> LATIN SMALL LETTER A at
code point U+0061, when visually they are indis-
tinguishable?
Homoglyphs come in two flavors: linguistic and
non-linguistic. Linguists are unlikely to distin-
guish between the <@> LATIN SMALL LETTER
SCHWA at code point U+0259 and <@> LATIN
SMALL LETTER TURNED E at U+01DD. And non-
linguists are unlikely to differentiate any seman-
tic difference between an open back unrounded
vowel <A>, the LATIN SMALL LETTER ALPHA
at U+0251, and the open front unrounded vowel
<a>, LATIN SMALL LETTER A at U+0061.
Another challenge is how to handle ambigu-
ity in transcription systems and orthographies. In
Serbo-Croatian, for example, the digraphs <lj>,
<nj> and <dz> represent distinct phonemes and
each are comprised of two graphemes, which
themselves represent distinct phonemes. Words
like <nadzivjeti> ?to outlive? are composed of
the morphemes <nad>, a prefix, and the verb
<zivjeti>. In this instance the combination of
<d> and <z> does not represent a single digraph
<dz>; they represent two neighboring phonemes
across a morpheme boundary. Likewise in En-
7ISO/IEC 1064
8http://www.unicode.org/Public/UNIDATA/Scripts.txt
115
glish, the grapheme sequence <sh> can be both
a digraph as well as a sequence of graphemes,
as in <mishmash> and <mishap>. When pars-
ing words like <mishit> and <mishear> both
disambiguations are theoretically available. An-
other example is illustrated by <h>, <t>, and
<th>. How should <t> be interpreted be-
fore <h> when English gives us both /tOm@s/
?Thomas? and /Tioudor/ ?Theodore?? The Sisaala
Western word <niikyuru> ?waterfall? could be
parsed as /niik.yuru/ instead of /nii.?uru/ to speak-
ers unfamiliar with the <ky> digraph of orthogra-
phies of Northwestern Ghana.
These ambiguities are due to mapping relations
between phonemes and graphemes. Transcrip-
tion systems and orthographies often have com-
plex grapheme-to-phoneme relationships and they
vary in levels of phonological abstraction. The
transparency of the relation between spelling and
phonology differ between languages like English
and French, and say Serbo-Croatian. The former
represent deep orthographic systems where the
same grapheme can represent different phonemes
in different contexts. The latter, a shallow or-
thography, is less polyvalent in its grapheme-to-
phoneme relations. Challenges of ambiguity reso-
lution are particularly apparent in data conversion.
4 Ontological Structure and Design
4.1 Technologies
In Philosophy, Ontology is the study of existence
and the meaning of being. In the Computer and
Information Sciences, ontology has been co-opted
to represent a data model that represents concepts
within a certain domain and the relationships be-
tween those concepts. At a low level an ontol-
ogy is a taxonomy and a set of inference rules.
At a higher-level, ontologies are collections of in-
formation that have formalized relationships that
hold between entities in a given domain. This pro-
vides the basis for automated reasoning by com-
puter software, where content is given meaning
in the sense of interpreting data and disambiguat-
ing entities. This is the vision of the Semantic
Web,9 a common framework for integrating and
correlating linked data from disparate resources
for interoperability (Beckett, 2004). The Gen-
eral Ontology for Linguistic Description (GOLD)
is grounded in the Semantic Web and provides
a foundation for the interoperability of linguistic
9http://www.w3.org/2001/sw/
annotation to enable intelligent search across lin-
guistic resources (Farrar and Langendoen, 2003).
Several technologies are integral to the architec-
ture of the Semantic Web, including Unicode,
XML,10 and the Resource Description Framework
(RDF).11 OATS has been developed with these
technologies and uses SPARQL12 to query the
knowledge base of linked data.
The Unicode Standard is the standard text
encoding for the Web, the recommended best-
practice for encoding linguistic resources, and the
underlying encoding for OATS. XML is a gen-
eral purpose specification for markup languages
and provides a structured language for data ex-
change (Yergeau, 2006). It is the most widely
used implementation for descriptive markup, and
is in fact so extensible that its structure does not
provide functionality for encoding explicit rela-
tionships across documents. Therefore RDF is
needed as the syntax for representing informa-
tion about resources on the Web and it is itself
written in XML and is serializable. RDF de-
scribes resources in the form subject-predicate-
object (or entity-relationship-entity) and identi-
fies unique resources through Uniform Resource
Identifiers (URIs). In this manner, RDF encodes
meaning in sets of triples that resemble subject-
verb-object constructions. These triples form a
graph data structure of nodes and arcs that are
non-hierarchical and can be complexly connected.
Numerous algorithms have been written to access
and manipulate graph structures. Since all URIs
are unique, each subject, object and predicate are
uniquely defined resources that can be referred
to and reused by anyone. URIs give users flex-
ibility in giving concepts a semantic representa-
tion. However, if two individuals are using differ-
ent URIs for the same concept, then a procedure
is needed to know that these two objects are in-
deed equivalent. A common example in linguis-
tic annotation is the synonymous use of genitive
and possessive. By incorporating domain specific
knowledge into an ontology in RDF, disambigua-
tion and interoperation over data becomes pos-
sible. GOLD addresses the challenge of inter-
operability of disparate linguistic annotation and
termsets in morphosyntax by functioning as an in-
terlingua between them. In OATS, the interlingua
10http://www.w3.org/XML/
11http://www.w3.org/RDF/
12http://www.w3.org/TR/rdf-sparql-query/
116
between systems of transcription is the IPA.
4.2 IPA as interlingua
OATS uses the IPA as an interlingua (or pivot)
to which elements of systems of transcription are
mapped. The IPA was chosen for its broad cov-
erage of the sounds of the world?s languages, its
mainstream adoption as a system for transcription
by linguists, and because it is encoded (at least
mostly) in Unicode. The pivot component resides
at the Character ID entity, which is in a one-to-one
relationship with a Unicode Character. The Char-
acter ID entity is provided for mapping characters
to multiple character encodings. This is useful for
mapping IPA characters to legacy character encod-
ing sets like IPA Kiel and SIL IPA93, allowing
for data conversion between character encodings.
The IPA also encodes phonetic segments as small
feature bundles. Phonological theories extend the
idea and interpretation of proposed feature sets,
an area of debate within Linguistics. These issues
should be taken into consideration when encoding
interoperability via an interlingua, and should be
leveraged to expand current theoretical questions
that can be asked of the knowledge base. Charac-
ter semantics also require consideration (Gibbon
et al, 2005). Glyph semantics provide implicit in-
formation such as a resource?s language, its lan-
guage family assignment, its use by a specific so-
cial or scientific group, or corporate identity (Trip-
pel et al, 2007). Documents with IPA characters
or in legacy IPA character encodings provide se-
mantic knowledge regarding the document?s con-
tent, namely, that it contains transcribed linguistic
data.
4.3 Ontological design
OATS consists of the following ontological
classes: Character, Grapheme, Document, Map-
ping, MappingSystem, WritingSystem, and
Scripteme. WritingSystem is further subdivided
into OrthographicSystem and TranscriptionSys-
tem. Each Document is associated with the
OLAC Metadata Set,13 an extension of the Dublin
Core Type Vocabulary14 for linguistic resources.
This includes uniquely identifying the language
represented in the document with its ISO 639-3
three letter language code. Each Document is also
associated with an instance of WritingSystem.
13http://www.language-archives.org/OLAC/metadata.html
14http://dublincore.org/usage/terms/dcmitype/
Each TranscriptionSystem is a set of instances
of Scripteme. Every Scripteme instance is in a
Mapping relation with its IPA counterpart. The
MappingSystem contains a list of Transcription-
System instances that have Scripteme instances
mapped to IPA. The Grapheme class provides
the mapping between Scripteme and Character.
The Character class is the set of Unicode char-
acters and contains the Unicode version number,
character name, HTML entity and code point.
5 Implementation
5.1 Data
The African language data used in OATS were
mined from Syste`mes alphabe?tiques des langues
africanies,15 an online database of Alphabets des
langues africaines (Hartell, 1993). Additional
languages were added by hand. Currently, OATS
includes 203 languages from 23 language families.
Each language contains its phonemic and ortho-
graphic inventories.
5.2 Query
Linguists gain unprecedented access to linguistic
resources when they are able to query across dis-
parate data in standardized notations regardless of
how the data in those resources is encoded. Cur-
rently OATS contains two phonetic notations for
querying: IPA and X-SAMPA. To illustrate the
querying functionality currently in place, the IPA
is used to query the knowledge base of African
language data16 for the occurrence of two seg-
ments. The first is the voiced palatal nasal /?/. The
results are captured in table 2.
Table 2: Occurrences of voiced palatal nasal /?/
Grapheme Languages % of Data
<ny> 114 84%
<n?> 11 8%
<?> 8 6%
<N> 2 1%
<ni> 1 .05%
The voiced palatal nasal /?/ is accounted for
in 136 languages, or roughly 67% of the 203
languages queried. Orthographically the voiced
palatal nasal /?/ is represented as <ny>, <n?>,
15http://sumale.vjf.cnrs.fr/phono/
16For a list of these languages, see http://phoible.org
117
<?>, <ni>, and interestingly as <N>. The two
languages containing <N>, Koonzime [ozm] and
Akoose [bss] of Cameroon, both lack a phonemic
/N/. In these languages? orthographies, both <ny>
and <N> are used to represent the phoneme /?/.
With further investigation, one can determine if
they are contextually determined allographs like
the <d> and <r> in Sisaala Pasaale.
The second simple query retrieves the occur-
rence of the voiced alveo-palatal affricate /?/. Ta-
ble 3 displays the results from the same sample of
languages.
Table 3: Occurrences of voiced alveo-palatal af-
fricate /?/
Grapheme Languages % of Data
<j> 84 92%
<dz> 2 2%
<gy> 2 2%
<dj> 1 1%
<?> 1 1%
<g?> 1 1%
The voiced alveo-palatal affricate /?/ is ac-
counted for in 92 languages, or 45%, of the 203
languages sampled. The majority, over 92%, use
the same grapheme <j> to represent /?/. Other
graphemes found in the language sample include
<dz>, <gy>, <dj>, <?>, and <g?>. The <g?>
stands out in this data sample. Interestingly, it
comes from Sudanese Arabic, which uses Latin-
based characters in its orthography. It contains the
phonemes /g/, /G/, and /?/, which are gramphemi-
cally represented as <g>, <gh> and <g?>.
These are rather simplistic examples, but the
graph data structure of RDF, and the power of
SPARQL provides an increasingly complex sys-
tem for querying any data stored in the knowledge
base and relationships as encoded by its ontologi-
cal structure. For example, by combining queries
such as ?which languages have the phoneme /gb/?
and ?of those languages which lack its voiceless
counterpart /kp/?, 11 results are found from this
sample of African languages, as outlined in Table
4.
5.3 Querying for phonetic data via
orthography
The ability to query the knowledge base via a
language-specific orthography is ultimately the
Table 4: Occurrence of /gb/ and lack of /kp/
Code Language Name Genetic Affiliation
emk Maninkakan Mande
kza Karaboro Gur
lia Limba Atlantic
mif Mofu-Gudur Chadic
sld Sissala Gur
ssl Sisaala Gur
sus Susu Mande
ted Krumen Kru
tem Themne Atlantic
tsp Toussian Gur
same task as querying the knowledge base via
the pivot. In this case, however, a mapping rela-
tion from the language-specific grapheme to IPA
is first established. Since all transcription systems?
graphemes must have an IPA counterpart, this re-
lationship is always available. A query is then
made across all relevant mapping relations from
IPA to languages within the knowledge base.
For example, a user familiar with the Sisaala
Western orthography queries the knowledge base
for languages with <ky>. Initially, the OATS
system establishes the relationship between <ky>
and its IPA counterpart. In this case, <ky> repre-
sents the voiceless alveo-palatal affricate /?/. Hav-
ing retrieved the IPA counterpart, the query next
retrieves all languages that have /?/ in their phone-
mic inventories. In the present data sample, this
query retrieves 99 languages with the phonemic
voiceless alveo-palatal affricate. If the user then
wishes to compare the graphemic distributions of
/?/ and /?/, which was predominately <j>, these
results are easily provided. They are displayed in
Table 5.
The 97 occurrences of /?/ account for five more
than the 92 languages sampled in section 5.2 that
had its voiced alveo-palatal affricate counterpart.
Such information provides statistics for phoneme
distribution across languages in the knowledge
base. OATS is a powerful tool for gathering such
knowledge about the world?s languages.
5.4 Code
There were two main steps in the implementation
of OATS. The first was the design and creation of
the OATS RDF model. This task was undertaken
118
Table 5: Occurrences of voiceless alveo-palatal af-
fricate /?/
Grapheme Languages % of Data
<c> 60 62%
<ch> 28 29%
<ts> 3 3%
<ky> 2 2%
<?> 1 1%
<tS > 1 1%
<j> 1 1%
<c?> 1 1%
using Protege,17 an open source ontology editor
developed by Stanford Center for Biomedical In-
formatics Research. The use of Protege was pri-
marily to jump start the design and implementa-
tion of the ontology. The software provides a user
interface for ontology modeling and development,
and exports the results into RDF. After the archi-
tecture was in place, the second step was the de-
velopment of a code base in Python18 for gather-
ing data and working with RDF. This code base
includes two major pieces. The first was the de-
velopment of a scraper, which was used to gather
phonemic inventories off of the Web by download-
ing Web pages and scraping them for relevant con-
tents. Each language was collected with its ISO
639-3 code, and its orthographic inventory and
the mapping relation between these symbols and
their IPA phonemic symbols. The second chunk of
the code base provides functionality for working
with the RDF graph and uses RDFLib,19 an RDF
Python module. The code includes scripts that add
all relevant language data that was scraped from
the Web to the OATS RDF graph, it fills the graph
with the Unicode database character tables, and
provides SPARQL queries for querying the graph
as illustrated above. There is also Python code for
using OATS to convert between two character sets,
and for error checking of characters within a doc-
ument that are not in the target set.
6 Conclusion and Future Work
OATS is a knowledge base that supports interop-
eration over disparate transcription systems. By
leveraging technologies for ontology description,
17http://protege.stanford.edu/
18http://python.org
19http://rdflib.net/
query, and multilingual character encoding, OATS
is designed to facilitate resource discovery and
intelligent search over linguistic data. The cur-
rent knowledge base includes an ontological de-
scription of writing systems and specifies rela-
tions for mapping segments of transcription sys-
tems to their IPA equivalents. IPA is used as the
interlingua pivot that provides the ability to query
across all resources in the knowledge base. OATS?
data source includes 203 African languages? or-
thographic and phonemic inventories.
The case studies proposed and implemented in
this paper present functionality to use OATS to
query all data in the knowledge base via stan-
dards like the IPA. OATS also supports query via
any transcription system or practical orthography
in the knowledge base. Another outcome of the
OATS project is the ability to check for incon-
sistencies in digitized lexical data. The system
could also test linguist-proposed phonotactic con-
straints and look for exceptions in data. Data
from grapheme-to-phoneme mappings, phonotac-
tics and character encodings can provide an ortho-
graphic profile/model of a transcription or writing
system. This could help to bootstrap software and
resource development for low-density languages.
OATS also provides prospective uses for docu-
ment conversion and development of probabilistic
models of orthography-to-phoneme mappings.
Acknowledgements
This work was supported in part by the Max-
Planck-Institut fu?r evolutiona?re Anthropologie
and thanks go to Bernard Comrie, Jeff Good and
Michael Cysouw. For useful comments and re-
views, I thank Emily Bender, Scott Farrar, Sharon
Hargus, Will Lewis, Richard Wright, and three
anonymous reviewers.
References
Timothy Baldwin, Steven Bird, and Baden Hughes.
2006. Collecting Low-Density Language Materials
on the Web. In Proceedings of the 12th Australasian
World Wide Web Conference (AusWeb06).
David Beckett. 2004. RDF/XML Syntax Specification
(Revised). Technical report, W3C.
Steven Bird and Gary F. Simons. 2003. Seven Di-
mensions of Portability for Language Documenta-
tion and Description. Language, 79(3):557?582.
Regina Blass. 1975. Sisaala-English, English-Sisaala
Dictionary. Institute of Linguistics, Tamale, Ghana.
119
Adams Bodomo. 1997. The Structure of Dagaare.
Stanford Monographs in African Languages. CSLI
Publications.
Florian Coulmas. 1999. The Blackwell Encyclopedia
of Writing Systems. Blackwell Publishers.
Scott Farrar and Terry Langendoen. 2003. A Linguis-
tic Ontology for the Semantic Web. GLOT, 7(3):97?
100.
Scott Farrar and William D. Lewis. 2006. The GOLD
Community of Practice: An Infrastructure for Lin-
guistic Data on the Web. In Language Resources
and Evaluation.
Dafydd Gibbon, Baden Hughes, and Thorsten Trip-
pel. 2005. Semantic Decomposition of Charac-
ter Encodings for Linguistic Knowledge Discovery.
In Proceedings of Jahrestagung der Gesellschaft fu?r
Klassifikation 2005.
Rhonda L. Hartell. 1993. Alphabets des langues
africaines. UNESCO and Socie?te? Internationale de
Linguistique.
William D. Lewis. 2006. ODIN: A Model for Adapt-
ing and Enriching Legacy Infrastructure. In Pro-
ceedings of the e-Humanities Workshop, held in co-
operation with e-Science 2006: 2nd IEEE Interna-
tional Conference on e-Science and Grid Comput-
ing.
Stuart Mcgill, Samuel Fembeti, and Mike Toupin.
1999. A Grammar of Sisaala-Pasaale, volume 4 of
Language Monographs. Institute of African Studies,
University of Ghana, Legon, Ghana.
Stuart McGill. 2004. Focus and Activation in Paasaal:
the particle rE. Master?s thesis, University of Read-
ing.
Steven Moran. 2008. A Grammatical Sketch of Isaalo
(Western Sisaala). VDM.
The Unicode Consortium. 2007. The Unicode Stan-
dard, Version 5.0. Boston, MA, Addison-Wesley.
Mike Toupin. 1995. The Phonology of Sisaale Pasaale.
Collected Language Notes, 22.
Thorsten Trippel, Dafydd Gibbon, and Baden Hughes.
2007. The Computational Semantics of Characters.
In Proceedings of the Seventh International Work-
shop on Computational Semantics (IWCS-7), pages
324?329.
Francois Yergeau. 2006. Extensible Markup Language
(XML) 1.0 (Fourth Edition). W3C Recommenda-
tion 16 August 2006, edited in place 29 September
2006.
Kie Zuraw. 2006. Using the Web as a Phonological
Corpus: a case study from Tagalog. In Proceedings
of the 2nd International Workshop on Web as Cor-
pus.
120
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 13?18,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Open Source Toolkit for Quantitative Historical Linguistics
Johann-Mattis List
Research Center Deutscher Sprachatlas
Philipps-University Marburg
mattis.list@uni-marburg.de
Steven Moran
Department of General Linguistics
University of Zurich
steven.moran@uzh.ch
Abstract
Given the increasing interest and devel-
opment of computational and quantitative
methods in historical linguistics, it is im-
portant that scholars have a basis for doc-
umenting, testing, evaluating, and shar-
ing complex workflows. We present a
novel open-source toolkit for quantitative
tasks in historical linguistics that offers
these features. This toolkit also serves
as an interface between existing software
packages and frequently used data for-
mats, and it provides implementations of
new and existing algorithms within a ho-
mogeneous framework. We illustrate the
toolkit?s functionality with an exemplary
workflow that starts with raw language
data and ends with automatically calcu-
lated phonetic alignments, cognates and
borrowings. We then illustrate evaluation
metrics on gold standard datasets that are
provided with the toolkit.
1 Introduction
Since the turn of the 21st century, there has been an
increasing amount of research that applies compu-
tational and quantitative approaches to historical-
comparative linguistic processes. Among these
are: phonetic alignment algorithms (Kondrak,
2000; Proki? et al, 2009), statistical tests for ge-
nealogical relatedness (Kessler, 2001), methods
for phylogenetic reconstruction (Holman et al,
2011; Bouckaert et al, 2012), and automatic de-
tection of cognates (Turchin et al, 2010; Steiner et
al., 2011), borrowings (Nelson-Sathi et al, 2011),
and proto-forms (Bouchard-C?t? et al, 2013).
In contrast to traditional approaches to language
comparison, quantitative methods are often em-
phasized as advantageous with regard to objectiv-
ity, transparency and replicability of results. It
is striking then that given the multitude of new
approaches, very few are publicly available as
executable code. Thus in order to replicate a
study, researchers have to rebuild workflows from
published descriptions and reimplement their ap-
proaches and algorithms. These challenges make
the replication of results difficult, or even impos-
sible, and they hinder not only the evaluation and
comparison of existing algorithms, but also the de-
velopment of new approaches that build on them.
Another problem is that quantitative approaches
that have been released as software are largely in-
compatible with each other and they show great
differences in regard to their input and out for-
mats, application range and flexibility.1 Given the
breadth of research questions involved in deter-
mining language relatedness, this is not surprising.
Furthermore, the linguistic datasets upon which
many analyses and tools are based are only ? if at
all ? available in disparate formats that need man-
ual or semi-automatic re-editing before they can
be used as input elsewhere. Scholars who want
to analyze a dataset with different approaches of-
ten have to (time-consumingly) convert it into var-
ious input formats and they have to familiarize
themselves with many different kinds of software.
As a result, errors may occur during data conver-
sion processes and the output from different tools
must also be converted into a comparable format.
For the comparison of different output formats or
1There is the STARLING database program for lexicosta-
tistical and glottochronological analyses (Starostin, 2000).
TheRug/L04 software aligns sound sequences and calculates
phonetic distances using the Levensthein distance (Kleiweg,
2009; Levenshtein, 1966). The ASJP-Software also com-
putes the Levenshtein distance (Holman et al, 2011), but its
results are based on previously executed phonetic analyses.
The ALINE software carries out pairwise alignment analy-
ses (Kondrak, 2000). There are also software packages from
evolutionary biology, which are adapted for linguistic pur-
poses, such as MrBayes (Ronquist and Huelsenbeck, 2003),
PHYLIP (Felsenstein, 2005), and SplitsTree (Huson, 1998).
13
for the evaluation of competing quantitative ap-
proaches, gold standard datasets are desirable.
Towards a solution to these problems, we have
developed a toolkit that (a) serves as an interface
between existing software packages and data for-
mats frequently used in quantitative approaches,
(b) provides high-quality implementations of new
and existing approaches within a homogeneous
framework, and (c) offers a solid basis for test-
ing, documenting, evaluating, and sharing com-
plex workflows in quantitative historical linguis-
tics. We call this open source toolkit LingPy.
2 Lingpy
LingPy is written in Python3 and is freely avail-
able online.2 The Lingpy website contains an API,
documentation, tutorials, example scripts, work-
flows, and datasets that can be used for training,
testing, and comparing results from different algo-
rithms. We use Python because it is flexible and
object-oriented, it is easy to write C extensions
for scientific computing, and it is approachable
to non-programmers (Knight et al, 2007). Apart
from a large number of different functions for com-
mon automatic tasks, LingPy offers specific mod-
ules for implementing general workflows that are
used in historical linguistics and which partially
mimic the basic aspects of the traditional compar-
ative method (Trask, 2000, 64-67). Figure 1 il-
lustrates the interaction between different modules
along with the data they produce. In the following
subsections, these modules will be introduced in
the order of a typical workflow to illustrate the ba-
sic capabilities of the LingPy toolkit in more detail.
2.1 Input Formats
The basic input format read by LingPy is a tab-
delimited text file in which the first line (the
header) indicates the values of the columns and all
words are listed in the following rows. The for-
mat is very flexible. No specific order of columns
or rows is required. Any additional data can be
specified by the user, as long as it is in a separate
column. Each row represents a word that has to be
characterized by a minimum of four values that are
given in separate columns: (1) ID, an integer that
is used to uniquely identify the word during calcu-
lations, (2) CONCEPT, a gloss which indicates the
meaning of the word and which is used to align the
words semantically, (3) WORD, the orthographic
2http://lingpy.org
Raw data
Tokenized
data
Orthographic parsing
Cognate
sets
Alignments
Cognate
detection
Phonetic
alignment (PA)
Output 
formats
PA
Patchy 
cognate 
sets
borrowing
detection
PA
Figure 1: Basic Workflow in LingPy
representation of the word,3 and (4) TAXON, the
name of the language (or dialect) inwhich theword
occurs. Basic output formats are essentially the
same, the difference being that the results of cal-
culations are added as separate columns. Table 1
illustrates the basic structure of the input format
for a dataset covering 325 concepts translated into
18 Dogon language varieties taken from the Do-
gon comparative lexical spreadsheet (Heath et al,
2013).4
2.2 Parsing and Unicode Handling
Given a dataset in the basic LingPy input for-
mat, the first step towards sound-based normal-
ization for automatically identifying cognates and
sound changes with quantitative methods is to
parse words into tokens. Orthographic tokeniza-
tion is a non-trivial task, but it is needed to at-
3By this we mean a textual representation of the word,
whether in a document or language-specific orthography or
in some form of broad or narrow transcription, etc.
4This tokenized dataset and analyses that are discussed in this
work are available for download from the LingPy website.
14
ID CONCEPT WORD TAXON
... ... ... ...
1239 file (tool) ki?:ra? Toro_Tegu
1240 file (tool) di?:s?: Ben_Tey
1241 file (tool) ki?r?l Bankan_Tey
1242 file (tool) di?:ju? Jamsay
... ... ... ...
1249 file (tool) bi?mbu? Tommo_So
1250 file (tool) bi?mbu? Dogul_Dom
1251 file (tool) di?:zu? Yanda_Dom
1252 file (tool) bi?:mbye? Mombo
... ... ... ...
Table 1: Basic Input Format of LingPy
tain interoperability across different orthographies
or transcription systems and to enable the com-
parative analysis of languages. LingPy includes
a parser that takes as input a dataset and an op-
tional orthography profile, i.e. a description of
the Unicode code points, characters, graphemes
and orthographic rules that are needed to ade-
quately model a writing system for a language va-
riety as described in a particular document (Moran,
2012, 331). The LingPy parser first normalizes all
strings into UnicodeNormalization FormD,which
decomposes all character sequences and reorders
them into one canonical order. This step is nec-
essary because sequences of Unicode characters
may differ in their visual and logical orders. Next,
if no orthography profile is specified, the parser
will use a regular expression match \X for Uni-
code grapheme clusters, i.e. combining character
sequences typified by a base character followed by
one or more Combing Diacritical Marks. How-
ever, another layer of tokenization is usually re-
quired to match linguistic graphemes, or what Uni-
code calls ?tailored grapheme clusters?. Table 2 il-
lustrates the different technological and linguistic
levels involved in orthographic parsing.5
code points t s h o ? ? ? ? ? ? s h i
?characters? t s h o??? s h i
graphemes tsh o??? sh i
Table 2: Tokens for the string <tsh???shi>
So, given the dataset illustrated in Table 1 and
an orthography profile that defines the phone-
mic units in the Dogon comparative lexicon, the
5Note that even when a linguist transcribes a word with the
International Phonetic Alphabet (IPA; a transcription system
with one-to-one symbol-to-sound correspondences), explicit
definitions for phonemes are needed because some IPA dia-
critics are encoded as Unicode Spacing Modifier Letters, i.e.
characters that are not specified as how they combine with a
base character, such as aspiration.
LingPy parser produces the IPA tokenized output
shown in Table 3.
ID ... WORD TOKENS ...
... ... ... ... ...
1239 ... ki?:ra? # k i?: r a? # ...
1240 ... di?:s?: # d i?: s ?: # ...
1241 ... ki?r?l # k i? r ? l # ...
1242 ... di?:ju? # d i?: ? u? # ...
... ... ... ... ...
1249 ... bi?mbu? # b i? m b u? # ...
1250 ... bi?mbu? # b i? m b u? # ...
1251 ... di?:zu? # d i?: z u? # ...
1252 ... bi?:mbye? # b i?: m b j e? # ...
... ... ... ... ...
Table 3: Orthographic Parsing in LingPy
2.3 Phonetic Alignments
Although less common in traditional historical lin-
guistics, phonetic alignment plays a crucial role
in automatic approaches, with alignment analyses
being currently used in many different subfields,
such as dialectology (Proki? et al, 2009), phyloge-
netic reconstruction (Holman et al, 2011) and cog-
nate detection (List, 2012a). Furthermore, align-
ment analyses are very useful for data visualiza-
tion, since they directly show which sound seg-
ments correspond in cognate words.
LingPy offers implementations for many dif-
ferent approaches to pairwise and multiple pho-
netic alignment. Among these, there are stan-
dard approaches that are directly taken from evo-
lutionary biology and can be applied to linguistic
data with only slight modifications, such as the
Needleman-Wunsch algorithm (Needleman and
Wunsch, 1970) and the Smith-Waterman algo-
rithm (Smith and Waterman, 1981). Furthermore,
there are novel approaches that use more com-
plex sequence models in order to meet linguistic-
specific requirements, such as the Sound-Class-
based phonetic Alignment (SCA) method (List,
2012b). Figure 2 shows a plot of the multi-
ple alignment of the counterparts of the concept
?stool? in eight Dogon languages. The color
scheme for the sound segments follows the sound
class distinction of Dolgopolsky (1964).
2.4 Automatic Cognate Detection
The identification of cognates plays an impor-
tant role in both traditional and quantitative ap-
proaches in historical linguistics. Most quantita-
tive approaches dealing with phylogenetic recon-
struction are based on previously identified cog-
nate sets distributed over the languages being in-
15
Taxon AlignmentBen_Tey t u? ? g u? r - u? mBankan_Tey t u? ? g u? r - u? -Jamsay t u? ? - u? r? - u? -Perge_Tegu t u? ? - u? r? - u? mGourou t u? m - u? r - u? -Yorno_So t ?? ? - ?? - - - -Tommo_So t u? ? g u? r - u? -Tebul_Ure t u? ? g u? r g ?? -XXX XXX XXX XXX XXX XXX XXX XXX XXX
Figure 2: Multiple Phonetic Alignment in LingPy
vestigated (Bouckaert et al, 2012; Bouchard-C?t?
et al, 2013). Since the traditional approach to cog-
nate detection within the framework of the com-
parative method is very time-consuming and diffi-
cult to evaluate for the non-expert, automatic ap-
proaches to cognate detection can play an impor-
tant role in objectifying phylogenetic reconstruc-
tions.
Currently, LingPy offers four alternative ap-
proaches to cognate detection in multilingual
wordlists. Themethod by Turchin et al (2010) em-
ploys sound classes as proposed by Dolgopolsky
(1964) and assigns words that match in their first
two consonant classes to the same cognate set. The
NED method calculates the normalized edit dis-
tance between words and groups them into cognate
sets using a flat cluster algorithm.6 The SCA and
the LexStat methods (List, 2012a) use the same
strategy for clustering, but the distances for the
SCA method are calculated with help of the SCA
alignment method (List, 2012b), and the distances
for the LexStat method are derived from previ-
ously identified regular sound correspondences.
Table 4 shows a small section of the results from
the LexStat analysis of the Dogon data. As shown,
LingPy follows the STARLING approach in dis-
playing cognate judgments by assigning cognate
words the same cognate ID (COGID). In Table
4, the words judged to be cognate are shaded in
the same color. The full results are posted on the
LingPy website.
2.5 Automatic Borrowing Detection
Automatic approaches for borrowing detection
are still in their infancy in historical linguistics.
LingPy provides a full reimplementation (along
with specifically linguistic modifications) of the
minimal lateral network (MLN) approach (Nelson-
Sathi et al, 2011). This approach searches for cog-
nate sets which are not compatible with a given ref-
6The normalized edit distance is calculated by dividing the
edit distance (Levenshtein, 1966) by the length of the smaller
sequence, see Holman et al (2011) for details.
ID CONCEPT WORD TAXON COGID
... ... ... ... ...
1239 file (tool) ki?:ra? Toro_Tegu 68
1240 file (tool) di?:s?: Ben_Tey 69
1241 file (tool) ki?r?l Bankan_Tey 68
1242 file (tool) di?:ju? Jamsay 69
... ... ... ... ...
1249 file (tool) bi?mbu? Tommo_So 70
1250 file (tool) bi?mbu? Dogul_Dom 70
1251 file (tool) di?:zu? Yanda_Dom 69
1252 file (tool) bi?:mbye? Mombo 70
... ... ... ... ...
Table 4: Cognate Detection in LingPy
erence tree topology. Incompatible (patchy) cog-
nate sets often point to either borrowings or wrong
cognate assessments in the data. The results can
be visualized by connecting all taxa of the refer-
ence tree for which patchy cognate sets can be in-
ferred with lateral links. In Figure 3, the method
has been applied again to the Dogon dataset. Cog-
nate judgments for this analysis were carried out
with help of LingPy?s LexStat method. The tree
topology was calculated using MrBayes.
2.6 Output Formats
The output formats supported by LingPy can be di-
vided into three different classes. The first class
consists of text-based formats that can be used
for manual correction and inspection by import-
ing the data into spreadsheet programs, or sim-
ply editing and reviewing the results in a text
editor. The second class consists of specific
formats for third-party toolkits, such as PHY-
LIP, SplitsTree, MrBayes, or STARLING. LingPy
currently offers support for PHYLIP?s distance
calculations (DST-format), for tree-representation
(Newick-format), for complex representations of
character data (Nexus-format), and for the im-
port into STARLING databases (CSV with STAR-
LING markup). The third class consists of new
approaches to the visualization of phonetic align-
ments, cognate sets, and phylogenetic networks.
In fact, all plots in this paper were created with
LingPy?s output formats.
3 Evaluation
In order to improve the performance of quantita-
tive approaches, it is of crucial importance to test
and evaluate them. Evaluation is usually done by
comparing how well a given approach performs
on a reference dataset, i.e. a gold standard, where
the results of the analysis are known in advance.
LingPy comes with a module for the evaluation of
16
Ben Tey
Tomo Kan Diangassagou
Toro Tegu
Tebul Ure
Jamsay Mondoro
Yanda DomNanga
Tiranige
Bankan Tey
Jamsay
Perge Tegu
Gourou
Bunoge
Dogul Dom
Mombo
Yorno So
Tommo SoTogo Kan
1
9
19
Inf
err
ed
Lin
ks
Figure 3: Borrowing Detection in LingPy
basic tasks in historical linguistics, such as pho-
netic alignment and cognate detection. This mod-
ule offers both common evaluation measures that
are used to assess the accuracy of the respective
methods and gold standard datasets encoded in the
LingPy input format.
In Figure 4, the performance of the four above-
mentioned approaches to automatic cognate de-
tection are compared with the gold standard cog-
nate judgments of a dataset covering 207 con-
cepts translated into 20 Indo-European languages
taken from the Indo-European Lexical Cognacy
(IELex) database (Bouckaert et al, 2012).7 The
pair scores, implemented in LingPy after the de-
scription in Bouchard-C?t? et al (2013), were used
as an evaluation measure. For all approaches we
chose the respective thresholds that tend to yield
the best results on all of the gold standards. As
shown in Figure, both the SCA and LexStat meth-
ods show a higher accuracy than the Turchin and
NED methods, with LexStat slightly outperform-
ing SCA. However, the generally bad performance
7Gold standard here means that the cognate judgments were
carried out manually by the compilers of the IELex database.
of all approaches on this dataset shows that there is
a clear need for improving automatic cognate de-
tection approaches, especially in cases of remote
relationship, such as Indo-European.
Precision Recall F-Score0.2
0.3
0.4
0.5
0.6
0.7
0.8 TurchinNEDSCALexStat
Figure 4: Evaluating Cognate Detection Methods
4 Conclusion
Quantitative approaches in historical linguistics
are still in their infancy, far away from being able
to compete with the intuition of trained historical
17
linguists. The toolkit we presented is a first at-
tempt to close the gap between quantitative and
traditional methods by providing a homogeneous
framework that serves as an interface between ex-
isting packages and at the same time provides high-
quality implementations of new approaches.
References
A. Bouchard-C?t?, D. Hall, T. L. Griffiths, and
D. Klein. 2013. Automated reconstruction of an-
cient languages using probabilistic models of sound
change. PNAS, 110(11):4224?4229.
R. Bouckaert, P. Lemey, M. Dunn, S. J. Greenhill,
A. V. Alekseyenko, A. J. Drummond, R. D. Gray,
M. A. Suchard, and Q. D. Atkinson. 2012. Map-
ping the origins and expansion of the Indo-European
language family. Science, 337(6097):957?960, Aug.
A. B. Dolgopolsky. 1964. Gipoteza drevnej?ego rod-
stva jazykovych semej Severnoj Evrazii s verojatnos-
tej to?ky zrenija [A probabilistic hypothesis concern-
ing the oldest relationships among the language fam-
ilies of Northern Eurasia]. Voprosy Jazykoznanija,
2:53?63.
J. Felsenstein. 2005. Phylip (phylogeny inference
package) version 3.6. Distributed by the author. De-
partment of Genome Sciences, University of Wash-
ington, Seattle.
J. Heath, S Moran, K. Prokhorov, L. McPherson, and
B. Canslter. 2013. Dogon comparative lexicon.
URL: http://www.dogonlanguages.org.
E. W. Holman, C. H. Brown, S. Wichmann, A. M?ller,
V. Velupillai, H. Hammarstr?m, S. Sauppe, H. Jung,
D. Bakker, P. Brown, O. Belyaev, M. Urban,
R. Mailhammer, J.-M. List, and D. Egorov. 2011.
Automated dating of the world?s language families
based on lexical similarity. Current Anthropology,
52(6):841?875.
D. H. Huson. 1998. SplitsTree. Analyzing and visu-
alizing evolutionary data. Bioinformatics, 14(1):68?
73.
B. Kessler. 2001. The significance of word lists. Sta-
tistical tests for investigating historical connections
between languages. CSLI Publications, Stanford.
P. Kleiweg. 2009. RuG/L04. Software for dialecto-
metrics and cartography. Distributed by the Author.
Rijksuniversiteit Groningen. Faculteit der Letteren,
September.
R. Knight, P. Maxwell, A. Birmingham, J. Carnes,
J. G. Caporaso, B. Easton, M. Eaton, M. Hamady,
H. Lindsay, Z. Liu, C. Lozupone, D. McDonald,
M. Robeson, R. Sammut, S. Smit, M. Wakefield,
J. Widmann, S. Wikman, S. Wilson, H. Ying, and
G. Huttley. 2007. PyCogent. A toolkit for making
sense from sequence. Genome Biology, 8(8):R171.
G. Kondrak. 2000. A new algorithm for the align-
ment of phonetic sequences. In Proceedings of
the 1st North American chapter of the Association
for Computational Linguistics conference, NAACL
2000, pages 288?295, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
V. I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
J.-M. List. 2012a. LexStat. Automatic detection of
cognates in multilingual wordlists. InProceedings of
the EACL 2012 Joint Workshop of LINGVIS & UN-
CLH, pages 117?125. Association for Computational
Linguistics.
J.-M. List. 2012b. SCA. Phonetic alignment based
on sound classes. In M. Slavkovik and D. Las-
siter, editors, New directions in logic, language, and
computation, number 7415 in LNCS, pages 32?51.
Springer, Berlin and Heidelberg.
S. Moran. 2012. Phonetics information base and lexi-
con. Ph.D. thesis, University of Washington.
S. B. Needleman and C. D. Wunsch. 1970. A gene
method applicable to the search for similarities in
the amino acid sequence of two proteins. Journal
of Molecular Biology, 48:443?453, July.
S. Nelson-Sathi, J.-M. List, H. Geisler, H. Fangerau,
R. D. Gray, W. Martin, and T. Dagan. 2011. Net-
works uncover hidden lexical borrowing in Indo-
European language evolution. Proceedings of the
Royal Society B, 278(1713):1794?1803.
J. Proki?, M. Wieling, and J. Nerbonne. 2009. Multi-
ple sequence alignments in linguistics. In Proceed-
ings of the EACL 2009 Workshop on Language Tech-
nology and Resources for Cultural Heritage, Social
Sciences, Humanities, and Education, pages 18?25.
Association for Computational Linguistics.
F. Ronquist and J. P. Huelsenbeck. 2003. MrBayes 3.
Bayesian phylogenetic inference under mixed mod-
els. Bioinformatics, 19(12):1572?1574.
T. F. Smith and M. S. Waterman. 1981. Identifica-
tion of common molecular subsequences. Journal of
Molecular Biology, 1:195?197.
S. A. Starostin. 2000. The STARLING database pro-
gram. URL: http://starling.rinet.ru.
L. Steiner, P. F. Stadler, and M. Cysouw. 2011.
A pipeline for computational historical linguistics.
Language Dynamics and Change, 1(1):89?127.
R. L. Trask. 2000. The dictionary of historical
and comparative linguistics. Edinburgh University
Press, Edinburgh.
P. Turchin, I. Peiros, and M. Gell-Mann. 2010. An-
alyzing genetic connections between languages by
matching consonant classes. Journal of Language
Relationship, 3:117?126.
18
