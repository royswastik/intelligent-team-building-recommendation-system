Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 276?285,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Comparison of Classification and Ranking Approaches
to Pronominal Anaphora Resolution in Czech?
Ngu. y Giang Linh, Va?clav Nova?k, Zdene?k Z?abokrtsky?
Charles University in Prague
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, CZ-11800
{linh,novak,zabokrtsky}.ufal.mff.cuni.cz
Abstract
In this paper we compare two Ma-
chine Learning approaches to the task
of pronominal anaphora resolution: a
conventional classification system based
on C5.0 decision trees, and a novel
perceptron-based ranker. We use coref-
erence links annotated in the Prague De-
pendency Treebank 2.0 for training and
evaluation purposes. The perceptron sys-
tem achieves f-score 79.43% on recogniz-
ing coreference of personal and possessive
pronouns, which clearly outperforms the
classifier and which is the best result re-
ported on this data set so far.
1 Introduction
Anaphora Resolution (AR) is a well established
task in Natural Language Processing (Mitkov,
2002). Classification techniques (e.g., single can-
didate model aimed at answering: ?Is there a
coreference link between the anaphor and this
antecedent candidate, or not??) are very often
used for the task, e.g. in Mccarthy and Lehnert
(1995) and Soon et al (2001). However, as ar-
gued already in Yang et al (2003), better results
are achieved when the candidates can compete in
a pairwise fashion. It can be explained by the
fact that in this approach (called twin-candidate
model), more information is available for the de-
cision making. If we proceed further along this
direction, we come to the ranking approach de-
scribed in Denis and Baldridge (2007), in which
the entire candidate set is considered at once and
?The work on this project was supported by the
grants MSM 0021620838, GAAV C?R 1ET101120503 and
1ET201120505, MS?MT C?R LC536, and GAUK 4383/2009
which leads to further significant shift in perfor-
mance, more recently documented in Denis and
Baldridge (2008).
In this paper we deal with supervised ap-
proaches to pronominal anaphora in Czech.1 For
training and evaluation purposes, we use corefer-
ences links annotated in the Prague Dependency
Treebank, (Jan Hajic?, et al, 2006). We limit our-
selves only to textual coreference (see Section 2)
and to personal and possessive pronouns. We
make use of a rich set of features available thanks
to the complex annotation scenario of the tree-
bank.
We experiment with two of the above men-
tioned techniques for AR: a classifier and a ranker.
The former is based on a top-down induction of
decision trees (Quinlan, 1993). The latter uses
a simple scoring function whose optimal weight
vector is estimated using perceptron learning in-
spired by Collins (2002). We try to provide both
implementations with as similar input information
as possible in order to be able to compare their
performance for the given task.
Performance of the presented systems can be
compared with several already published works,
namely with a rule-based system described in
Kuc?ova? and Z?abokrtsky? (2005), some of the ?clas-
sical? algorithms implemented in Ne?mc???k (2006),
a system based on decision trees (Ngu. y, 2006),
and a rule-based system evaluated in Ngu. y and
Z?abokrtsky? (2007). To illustrate the real complex-
ity of the task, we also provide performance eval-
uation of a baseline solution.
1Currently one can see a growing interest in unsupervised
techniques, e.g. Charniak and Elsner (2009) and Ng (2008).
However, we make only a very tiny step in this direction:
we use a probabilistic feature based on collocation counts in
large unannotated data (namely in the Czech National Cor-
pus).
276
The most important result claimed in this pa-
per is that, to the best of our knowledge, the pre-
sented ranker system outperforms all the previ-
ously published systems evaluated on the PDT
data. Moreover, the performance of our ranker (f-
score 79.43%) for Czech data is not far from the
performance of the state-of-the-art system for En-
glish described in Denis and Baldridge (2008) (f-
score for 3rd person pronouns 82.2 %).2
A side product of this work lies in bringing
empirical evidence ? for a different language and
different data set ? for the claim of Denis and
Baldridge (2007) that the ranking approach is
more appropriate for the task of AR than the clas-
sification approach.
The paper is structured as follows. The data
with manually annotated links we use are de-
scribed in Section 2. Section 3 outlines prepro-
cessing the data for training and evaluation pur-
poses. The classifier-based and ranker-based sys-
tems are described in Section 4 and Section 5 re-
spectively. Section 6 summarizes the achieved re-
sults by evaluating both approaches using the test
data. Conclusions and final remarks follow in Sec-
tion 7.
2 Coreference links in the Prague
Dependency Treebank 2.0
The Prague Dependency Treebank 2.03 (PDT 2.0,
Jan Hajic?, et al (2006)) is a large collection of
linguistically annotated data and documentation,
based on the theoretical framework of Functional
Generative Description (FGD; introduced by Sgall
(1967) and later elaborated, e.g. in by Sgall et al
(1986)). The PDT 2.0 data are Czech newspaper
texts selected from the Czech National Corpus4
(CNC).
The PDT 2.0 has a three-level structure. On the
lowest morphological level, a lemma and a posi-
tional morphological tag are added to each token.
The middle analytical level represents each sen-
tence as a surface-syntactic dependency tree. On
the highest tectogrammatical level, each sentence
is represented as a complex deep-syntactic depen-
2However, it should be noted that exact comparison is not
possible here, since the tasks are slightly different for the
two languages, especially because of typological differences
between Czech and English (frequent pro-drop in Czech)
and different information available in the underlying data re-
source on the other hand (manually annotated morphological
and syntactical information available for Czech).
3http://ufal.mff.cuni.cz/pdt2.0/
4http://ucnk.ff.cuni.cz/
dency tree, see Mikulova? and others (2005) for de-
tails. This level includes also annotation of coref-
erential links.
The PDT 2.0 contains 3,168 newspaper texts
(49,431 sentences) annotated on the tectogram-
matical level. Coreference has been annotated
manually in all this data. Following the FGD,
there are two types of coreference distinguished:
grammatical coreference and textual coreference
(Panevova?, 1991). The main difference between
the two coreference types is that the antecedent in
grammatical coreference can be identified using
grammatical rules and sentence syntactic struc-
ture, whereas the antecedent in textual coreference
can not.
The further division of grammatical and textual
coreference is based on types of anaphors:
Grammatical anaphors: relative pronouns, re-
flexive pronouns, reciprocity pronouns, re-
stored (surface-unexpressed) ?subjects? of
infinitive verbs below verbs of control,
Textual anaphors: personal and possessive pro-
nouns, demonstrative pronouns.
The data in the PDT 2.0 are divided into three
groups: training set (80%), development test set
(10%), and evaluation test set (10%). The training
and development test set can be freely exploited,
while the evaluation test data should serve only for
the very final evaluation of developed tools.
Table 1 shows the distribution of each anaphor
type. The total number of coreference links in the
PDT 2.0 data is 45,174.5 Personal pronouns in-
cluding those zero ones and possessive pronouns
form 37.4% of all anaphors in the entire corpus
(16,888 links).
An example tectogrammatical tree with de-
picted coreference links (arrows) is presented in
Figure 1. For the sake of simplicity, only three
node attributes are displayed below the nodes: tec-
togrammatical lemma, functor, and semantic part
of speech (tectogrammatical nodes themselves are
complex data structures and around twenty at-
tributes might be stored with them).
Tectogrammatical lemma is a canonical word
form or an artificial value of a newly created node
5In terms of the number of coreference links, PDT 2.0
is one of the largest existing manually annotated resources.
Another comparably large resource is BBN Pronoun Coref-
erence and Entity Type Corpus (Weischedel and Brunstein,
2005), which contains a stand-off annotation of coreference
links in the Penn Treebank texts.
277
Type/Count train dtest etest
Personal pron. 12,913 1,945 2,030
Relative pron. 6,957 948 1,034
Under-control pron. 6,598 874 907
Reflexive pron. 3,381 452 571
Demonstrative pron. 2,582 332 344
Reciprocity pron. 882 110 122
Other 320 35 42
Total 34,983 4,909 5,282
Table 1: Distribution of the different anaphor
types in the PDT 2.0.
on the tectogrammatical level. E.g. the (artifi-
cial) tectogrammatical lemma #PersPron stands
for personal (and possessive) pronouns, be they
expressed on the surface (i.e., present in the orig-
inal sentence) or restored during the annotation
of the tectogrammatical tree structure (zero pro-
nouns).
Functor captures the deep-syntactic dependency
relation between a node and its governor in the
tectogrammatical tree. According to FGD, func-
tors are divided into actants (ACT ? actor, PAT ?
patient, ADDR ? addressee, etc.) and free modi-
fiers (LOC ? location, BEN ? benefactor, RHEM
? rhematizer, TWHEN ? temporal modifier, APP
? appurtenance, etc.).
Semantic parts of speech correspond to ba-
sic onomasiological categories (substance, fea-
ture, factor, event). The main semantic POS dis-
tinguished in PDT 2.0 are: semantic nouns, se-
mantic adjectives, semantic adverbs and semantic
verbs (for example, personal and possessive pro-
nouns belong to semantic nouns).
3 Training data preparation
The training phase of both presented AR systems
can be outlined as follows:
1. detect nodes which are anaphors (Sec-
tion 3.1),
2. for each anaphor ai, collect the set of an-
tecedent candidates Cand(ai) (Section 3.2),
3. for each anaphor ai, divide the set of
candidates into positive instances (true an-
tecedents) and negative instances (Sec-
tion 3.3),
4. for each pair of an anaphor ai and an an-
tecedent candidate cj ? Cand(ai), compute
the feature vector ?(c, ai) (Section 3.4),
5. given the anaphors, their sets of antecedent
candidates (with related feature vectors), and
the division into positive and negative candi-
dates, train the system for identifying the true
antecedents among the candidates.
Steps 1-4 can be seen as training data prepro-
cessing, and are very similar for both systems.
System-specific details are described in Section 4
and Section 5 respectively.
3.1 Anaphor selection
In the presented work, only third person per-
sonal (and possessive) pronouns are considered,6
be they expressed on the surface or reconstructed.
We treat as anaphors all tectogrammatical nodes
with lemma #PersPron and third person stored in
the gram/person grammateme. More than 98 %
of such nodes have their antecedents (in the sense
of textual coreference) marked in the training data.
Therefore we decided to rely only on this highly
precise rule when detecting anaphors.7
In our example tree, the node #PersPron rep-
resenting his on the surface and the node #Per-
sPron representing the zero personal pronoun he
will be recognized as anaphors.
3.2 Candidate selection
In both systems, the predicted antecedent of a
given anaphor ai is selected from an easy-to-
compute set of antecedent candidates denoted as
Cand(ai). We limit the set of candidates to se-
mantic nouns which are located either in the same
sentence before the anaphor, or in the preced-
ing sentence. Table 2 shows that if we disregard
cataphoric and longer anaphoric links, we loose
a chance for correct answer with only 6 % of
anaphors.
6The reason is that antecedents of most other types of
anaphors annotated in PDT 2.0 can be detected ? given
the tree topology and basic node attributes ? with precision
higher than 90 %, as it was shown already in Kuc?ova? and
Z?abokrtsky? (2005). For instance, antecedents of reflexive
pronouns are tree-nearest clause subjects in most cases, while
antecedents of relative pronouns are typically parents of the
relative clause heads.
7It is not surprising that no discourse status model (as used
e.g. in Denis and Baldridge (2008)) is practically needed
here, since we limit ourselves to personal pronouns, which
are almost always ?discourse-old?.
278
Antecedent location Percnt.
Previous sentence 37 %
Same sentence, preceding the anaphor 57 %
Same sentence, following the anaphor 5 %
Other 1 %
Table 2: Location of antecedents with respect to
anaphors in the training section of PDT 2.0.
3.3 Generating positive and negative
instances
If the true antecedent of ai is not present in
Cand(ai), no training instance is generated. If it is
present, the sets of negative and positive instances
are generated based on the anaphor. This prepro-
cessing step differs for the two systems, because
the classifier can be easily provided with more
than one positive instance per anaphor, whereas
the ranker can not.
In the classification-based system, all candi-
dates belonging to the coreferential chain are
marked as positive instances in the training data.
The remaining candidates are marked as negative
instances.
In the ranking-based system, the coreferential
chain is followed from the anaphor to the nearest
antecedent which itself is not an anaphor in gram-
matical coreference.8 The first such node is put on
the top of the training rank list, as it should be pre-
dicted as the winner (E.g., the nearest antecedent
of the zero personal pronoun he in the example
tree is the relative pronoun who, however, it is a
grammatical anaphor, so its antecedent Brien is
chosen as the winner instead). All remaining (neg-
ative) candidates are added to the list, without any
special ordering.
3.4 Feature extraction
Our model makes use of a wide range of features
that are obtained not only from all three levels of
the PDT 2.0 but also from the Czech National Cor-
pus and the EuroWordNet. Each training or test-
ing instance is represented by a feature vector. The
features describe the anaphor, its antecedent can-
didate and their relationship, as well as their con-
8Grammatical anaphors are skipped because they usually
do not provide sufficient information (e.g., reflexive pronouns
provide almost no cues at all). The classification approach
does not require such adaptation ? it is more robust against
such lack of information as it treats the whole chain as posi-
tive instances.
texts. All features are listed in Table 4 in the Ap-
pendix.
When designing the feature set on personal pro-
nouns, we take into account the fact that Czech
personal pronouns stand for persons, animals and
things, therefore they agree with their antecedents
in many attributes and functions. Further we use
the knowledge from the Lappin and Leass?s al-
gorithm (Lappin and Leass, 1994), the Mitkov?s
robust, knowledge-poor approach (Mitkov, 2002),
and the theory of topic-focus articulation (Kuc?ova?
et al, 2005). We want to take utmost advantage of
information from the antecedent?s and anaphor?s
node on all three levels as well.
Distance: Numeric features capturing the dis-
tance between the anaphor and the candidate, mea-
sured by the number of sentences, clauses, tree
nodes and candidates between them.
Morphological agreement: Categorial features
created from the values of tectogrammatical gen-
der and number9 and from selected morphological
categories from the positional tag10 of the anaphor
and of the candidate. In addition, there are features
indicating the strict agreement between these pairs
and features formed by concatenating the pair of
values of the given attribute in the two nodes (e.g.,
masc neut).
Agreement in dependency functions: Catego-
rial features created from the values of tec-
togrammatical functor and analytical functor (with
surface-syntactic values such as Sb, Pred, Obj) of
the anaphor and of the candidate, their agreement
and joint feature. There are two more features in-
dicating whether the candidate/anaphor is an ac-
tant and whether the candidate/anaphor is a sub-
ject on the tectogrammatical level.11
Context: Categorial features describing the con-
text of the anaphor and of the candidate:
? parent ? tectogrammatical functor and the se-
mantic POS of the effective parent12 of the
9Sometimes gender and number are unknown, but we can
identify the gender and number of e.g. relative or reflexive
pronouns on the tectogrammatical level thanks to their an-
tecedent.
10A positional tag from the morphological level is a string
of 15 characters. Every positions encodes one morphological
category using one character.
11A subject on the tectogrammatical level can be a node
with the analytical functor Sb or with the tectogrammatical
functor Actor in a clause without a subject.
12The ?true governor? in terms of dependency relations.
279
anaphor and the candidate, their agreement
and joint feature; a feature indicating the
agreement of both parents? tectogrammatical
lemma and their joint feature; a joint feature
of the pair of the tectogrammatical lemma
of the candidate and the effective parent?s
lemma of the anaphor; and a feature indicat-
ing whether the candidate and the anaphor are
siblings.13
? coordination ? a feature that indicates
whether the candidate is a member of a coor-
dination and a feature indicating whether the
anaphor is a possessive pronoun and is in the
coordination with the candidate
? collocation ? a feature indicating whether the
candidate has appeared in the same colloca-
tion as the anaphor within the text14 and a
feature that indicates the collocation assumed
from the Czech National Corpus.15
? boundness ? features assigned on the ba-
sis of contextual boundness (available in the
tectogrammatical trees) {contextually bound,
contrastively contextually bound, or contex-
tually non-bound}16 for the anaphor and the
candidate; their agreement and joint feature.
? frequency ? 1 if the candidate is a denotative
semantic noun and occurs more than once
within the text; otherwise 0.
Semantics: Semantically oriented feature that
indicates whether the candidate is a person name
for the present and a set of 63 binary ontologi-
cal attributes obtained from the EuroWordNet.17
These attributes determine the positive or negative
13Both have the same effective parent.
14If the anaphor?s effective parent is a verb and the can-
didate is a denotative semantic noun and has appeared as a
child of the same verb and has had the same functor as the
anaphor.
15The probability of the candidate being a subject preced-
ing the verb, which is the effective parent of the anaphor.
16Contextual boundness is a property of an expression (be
it expressed or absent in the surface structure of the sentence)
which determines whether the speaker (author) uses the ex-
pression as given (for the recipient), i.e. uniquely determined
by the context.
17The Top Ontology used in EuroWordNet (EWN) con-
tains the (structured) set of 63 basic semantic concepts like
Place, Time, Human, Group, Living, etc. For the majority of
English synsets (set of synonyms, the basic unit of EWN), the
appropriate subset of these concepts are listed. Using the In-
ter Lingual Index that links the synsets of different languages,
the set of relevant concepts can be found also for Czech lem-
mas.
relation between the candidate?s lemma and the se-
mantic concepts.
4 Classifier-based system
Our classification approach uses C5.0, a succes-
sor of C4.5 (Quinlan, 1993), which is probably the
most widely used program for inducing decision
trees. Decision trees are used in many AR sys-
tems such as Aone and Bennett (1995), Mccarthy
and Lehnert (1995), Soon et al (2001), and Ng
and Cardie (2002).18
Our classifier-based system takes as input a set
of feature vectors as described in Section 3.4 and
their classifications (1 ? true antecedent, 0 ? non-
antecedent) and produces a decision tree that is
further used for classifying new pairs of candidate
and anaphor.
The classifier antecedent selection algorithm
works as follows. For each anaphor ai, feature
vectors ?(c, ai) are computed for all candidates
c ? Cand(ai) and passed to the trained decision
tree. The candidate classified as positive is re-
turned as the predicted antecedent. If there are
more candidates classified as positive, the nearest
one is chosen.
If no candidate is classified as positive, a sys-
tem of handwritten fallback rules can be used. The
fallback rules are the same rules as those used in
the baseline system in Section 6.2.
5 Ranker-based system
In the ranker-based AR system, every training ex-
ample is a pair (ai, yi), where ai is the anaphoric
expression and yi is the true antecedent. Using
the candidate extraction function Cand, we aim
to rank the candidates so that the true antecedent
would always be the first candidate on the list. The
ranking is modeled by a linear model of the fea-
tures described in Section 3.4. According to the
model, the antecedent y?i for an anaphoric expres-
sion ai is found as:
y?i = argmax
c?Cand(ai)
?(c, ai) ?
??w
The weights ??w of the linear model are trained
using a modification of the averaged perceptron al-
18Besides C5.0, we plan to use also other classifiers in the
future (especially Support Vector Machine, which is often
employed in AR experiments, e.g. by Ng (2005) and Yang
et al (2006)) in order to study how the classifier choice in-
fluences the AR system performance on our data and feature
sets.
280
gorithm (Collins, 2002). This is averaged percep-
tron learning with a modified loss function adapted
to the ranking scenario. The loss function is tai-
lored to the task of correctly ranking the true an-
tecedent, the ranking of other candidates is irrel-
evant. The algorithm (without averaging the pa-
rameters) is listed as Algorithm 1. Note that the
training instances where yi /? Cand(ai) were ex-
cluded from the training.
input : N training examples (ai, yi),
number of iterations T
init : ??w ?
??
0 ;
for t? 1 to T , i? 1 to N do
y?i ? argmaxc?Cand(ai) ?(c, ai) ?
??w ;
if y?i 6= yi then
??w = ??w + ?(yi, ai)? ?(y?i, ai);
end
end
output: weights ??w
Algorithm 1: Modified perceptron algorithm
for ranking. ? is the feature extraction func-
tion, ai is the anaphoric expression, yi is the
true antecedent.
Antecedent selection algorithm using a ranker:
For each third person pronoun create a feature vec-
tor from the pronoun and the semantic noun pre-
ceding the pronoun and is in the same sentence or
in the previous sentence. Use the trained ranking
features weight model to get out the candidate?s
total weight. The candidate with the highest fea-
tures weight is identified as the antecedent.
6 Experiments and evaluation
6.1 Evaluation metrics
For the evaluation we use the standard metrics:19
Precision = number of correctly predicted anaphoric third person pronounsnumber of all predicted third person pronouns
Recall = number of correctly predicted anaphoric third person pronounsnumber of all anaphoric third person pronouns
F-measure = 2?Precision?RecallPrecision+Recall
We consider an anaphoric third person pronoun
to be correctly predicted when we can success-
19Using simple accuracy would not be adequate, as there
can be no link (or more than one) leading from an anaphor
in the annotated data. In other words, finding whether a pro-
noun has an antecedent or not is a part of the task. A deeper
discussion about coreference resolution metrics can be found
in Luo (2005).
fully indicate its antecedent, which can be any an-
tecedent from the same coreferential chain as the
anaphor.
Both the AR systems were developed and tested
on PDT 2.0 training and development test data. Fi-
nally they were tested on evaluation test data for
the final scoring, summarized in Section 6.3.
6.2 Baseline system
We have made some baseline rules for the task of
AR and tested them on the PDT 2.0 evaluation test
data. Their results are reported in Table 3. Base-
line rules are following: For each third person pro-
noun, consider all semantic nouns which precede
the pronoun and are not further than the previous
sentence, and:
? select the nearest one as its antecedent
(BASE 1),
? select the nearest one which is a clause sub-
ject (BASE 2),
? select the nearest one which agrees in gender
and number (BASE 3),
? select the nearest one which agrees in gen-
der and number; if there is no such noun,
choose the nearest clause subject; if no clause
subject was found, choose the nearest noun
(BASE 3+2+1).
6.3 Experimental results and discussion
Scores for all three systems (baseline, clasifier
with and without fallback, ranker) are given in Ta-
ble 3. Our baseline system based on the combina-
tion of three rules (BASE 3+2+1) reports results
superior to the ones of the rule-based system de-
scribed in Kuc?ova? and Z?abokrtsky? (2005). Kuc?ova?
and Z?abokrtsky? proposed a set of filters for per-
sonal pronominal anaphora resolution. The list of
candidates was built from the preceding and the
same sentence as the personal pronoun. After ap-
plying each filter, improbable candidates were cut
off. If there was more than one candidate left at
the end, the nearest one to the anaphor was cho-
sen as its antecedent. The reported final success
rate was 60.4 % (counted simply as the number of
correctly predicted links divided by the number of
pronoun anaphors in the test data section).
An interesting point of the classifier-based sys-
tem lies in the comparison with the rule-based
281
Rule P R F
BASE 1 17.82% 18.00% 17.90%
BASE 2 41.69% 42.06% 41.88%
BASE 3 59.00% 59.50% 59.24%
BASE 3+2+1 62.55% 63.03% 62.79%
CLASS 69.9% 70.44% 70.17%
CLASS+3+2+1 76.02% 76.60% 76.30%
RANK 79.13% 79.74% 79.43%
Table 3: Precision (P), Recall (R) and F-measure
(F) results for the presented AR systems.
system of Ngu. y and Z?abokrtsky? (2007). With-
out the rule-based fallback (CLASS), the clas-
sifier falls behind the Ngu. y and Z?abokrtsky??s
system (74.2%), while including the fallback
(CLASS+3+2+1) it gives better results.
Overall, the ranker-based system (RANK) sig-
nificantly outperforms all other AR systems for
Czech with the f-score of 79.43%. Comparing
with the model for third person pronouns of Denis
and Baldridge (2008), which reports the f-score of
82.2%, our ranker is not so far behind. It is im-
portant to say that our system relies on manually
annotated information20 and we solve the task of
anaphora resolution for third person pronouns on
the tectogrammatical level of the PDT 2.0. That
means these pronouns are not only those expressed
on the surface, but also artificially added (recon-
structed) into the structure according to the princi-
ples of FGD.
7 Conclusions
In this paper we report two systems for AR in
Czech: the classifier-based system and the ranker-
based system. The latter system reaches f-score
79.43% on the Prague Dependency Treebank test
data and significantly outperforms all previously
published results. Our results support the hypoth-
esis that ranking approaches are more appropriate
for the AR task than classification approaches.
References
Chinatsu Aone and Scott William Bennett. 1995.
Evaluating automated and manual acquisition of
20In the near future, we plan to re-run the experiments us-
ing sentence analyses created by automatic tools (all needed
tools are available in the TectoMT software framework
(Z?abokrtsky? et al, 2008)) instead of manually created analy-
ses, in order to examine the sensitivity of the AR system to
annotation quality.
anaphora resolution strategies. In Proceedings of the
33rd annual meeting on Association for Computa-
tional Linguistics, pages 122?129, Morristown, NJ,
USA. Association for Computational Linguistics.
Anto?nio Branco, Tony McEnery, Ruslan Mitkov, and
Fa?tima Silva, editors. 2007. Proceedings of the 6th
Discourse Anaphora and Anaphor Resolution Col-
loquium (DAARC 2007), Lagos (Algarve), Portugal.
CLUP-Center for Linguistics of the University of
Oporto.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 148?156, Athens, Greece,
March. Association for Computational Linguistics.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of EMNLP, volume 10, pages 1?8.
Pascal Denis and Jason Baldridge. 2007. A ranking
approach to pronoun resolution. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence (IJCAI2007), pages 1588?1593, Hyder-
abad, India, January 6?12.
Pascal Denis and Jason Baldridge. 2008. Special-
ized models and ranking for coreference resolu-
tion. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP2008), pages 660?669, Honolulu, Hawaii,
USA, October 25?27.
Jan Hajic?, et al 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T01, Philadelphia.
Lucie Kuc?ova? and Zdene?k Z?abokrtsky?. 2005.
Anaphora in Czech: Large Data and Experiments
with Automatic Anaphora. LNCS/Lecture Notes in
Artificial Intelligence/Proceedings of Text, Speech
and Dialogue, 3658:93?98.
Lucie Kuc?ova?, Kater?ina Vesela?, Eva Hajic?ova?, and
Jir??? Havelka. 2005. Topic-focus articulation and
anaphoric relations: A corpus based probe. In Klaus
Heusinger and Carla Umbach, editors, Proceedings
of Discourse Domains and Information Structure
workshop, pages 37?46, Edinburgh, Scotland, UK,
Aug. 8-12.
Shalom Lappin and Herbert J. Leass. 1994. ?an algo-
rithm for pronominal anaphora resolution?. Compu-
tational Linguistics, 20(4):535?561.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In HLT ?05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 25?32, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
282
J Mccarthy and Wendy G. Lehnert. 1995. Using de-
cision trees for coreference resolution. In In Pro-
ceedings of the Fourteenth International Joint Con-
ference on Artificial Intelligence, pages 1050?1055.
Marie Mikulova? et al 2005. Anotace na tektogra-
maticke? rovine? Praz?ske?ho za?vislostn??ho korpusu.
Anota?torska? pr???ruc?ka (t-layer annotation guide-
lines). Technical Report TR-2005-28, U?FAL MFF
UK, Prague, Prague.
Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man, London.
Va?clav Ne?mc???k. 2006. Anaphora Resolution. Mas-
ter?s thesis, Faculty of Informatics, Masaryk Univer-
sity.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In ACL ?02: Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 104?111, Morristown, NJ, USA. Association
for Computational Linguistics.
Vincent Ng. 2005. Supervised ranking for pro-
noun resolution: Some recent improvements. In
Manuela M. Veloso and Subbarao Kambhampati,
editors, AAAI, pages 1081?1086. AAAI Press / The
MIT Press.
Vincent Ng. 2008. Unsupervised models for corefer-
ence resolution. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP2008), pages 640?649, Hon-
olulu, Hawaii, USA.
Giang Linh Ngu. y and Zdene?k Z?abokrtsky?. 2007.
Rule-based approach to pronominal anaphora reso-
lution applied on the prague dependency treebank
2.0 data. In Branco et al (Branco et al, 2007), pages
77?81.
Giang Linh Ngu. y. 2006. Proposal of a Set of Rules
for Anaphora Resolution in Czech. Master?s thesis,
Faculty of Mathematics and Physics, Charles Uni-
versity.
Jarmila Panevova?. 1991. Koreference gramaticka? nebo
textova?? In Etudes de linguistique romane et slave.
Krakow.
J. Ross Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and
Pragmatic Aspects. D. Reidel Publishing Company,
Dordrecht.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska?
deklinace. Academia, Prague, Czech Republic.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Comput. Linguist., 27(4):521?544.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceed-
ings of the 3rd Workshop on Statistical Machine
Translation, ACL.
Ralph Weischedel and Ada Brunstein. 2005. BBN
Pronoun Coreference and Entity Type Corpus. CD-
ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2005T33, Philadelphia.
Xiaofeng Yang, Guodong Zhou, Jian Su, and
Chew Lim Tan. 2003. Coreference resolution us-
ing competition learning approach. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 176?
183, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured
syntactic knowledge. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING-
ACL2006), pages 41?48, Sydney, Australia, July
17?21.
283
A Appendix
t-ln95049-047-p3s1
root
O - O
RSTR
n.denot
Brien - BRIEN
ACT
n.denot
kter? - WHO
ACT
n.pron.indef
Louganis - LOUGANIS
PAT
n.denot
tr novat - TO TRAIN
RSTR
v
rok - YEAR
THL
n.denot
deset - TEN
RSTR
adj.quant.def
#PersPron - HIS
ACT
n.pron.def.pers
onemocn?n - INJURY
PAT
n.denot.neg
vdt - TO KNOW
PRED
v
ale - BUT enunc
ADVS
coap
zavzat_se - TO TIE SOMEONE'S SELF
PRED
v
#PersPron - (HE)
ACT
n.pron.def.pers
ml?en - SECRECY
PAT
n.denot.neg
.
Figure 1: Simplified tectogrammatical tree representing the sentence O?Brien, ktery? Louganise tre?noval
deset let, o jeho onemocne?n?? ve?de?l, ale zava?zal se mlc?en??m. (Lit.: O?Brien, who Louganis trained for
ten years, about his injury knew, but (he) tied himself to secrecy.) Note two coreferential chains {Brien,
who, (he)} and {Louganis, his}.
284
Distance
sent dist sentence distance between c and ai
clause dist clause distance between c and ai
node dist tree node distance between c and ai
cand ord mention distance between c and ai
Morphological Agreement
gender t-gender of c and ai, agreement, joint
number t-number of c and ai, agreement, joint
apos m-POS of c and ai, agreement, joint
asubpos detailed POS of c and ai, agreement, joint
agen m-gender of c and ai, agreement, joint
anum m-number of c and ai, agreement, joint
acase m-case of c and ai, agreement, joint
apossgen m-possessor?s gender of c and ai, agreement, joint
apossnum m-possessor?s number of c and ai, agreement, joint
apers m-person of c and ai, agreement, joint
Functional Agreement
afun a-functor of c and ai, agreement, joint
fun t-functor of c and ai, agreement, joint
act c/ai is an actant, agreement
subj c/ai is a subject, agreement
Context
par fun t-functor of the parent of c and ai, agreement, joint
par pos t-POS of the parent of c and ai, agreement, joint
par lemma agreement between the parent?s lemma of c and ai, joint
clem aparlem joint between the lemma of c and the parent?s lemma of ai
c coord c is a member of a coordination
app coord c and ai are in coordination & ai is a possessive pronoun
sibl c and ai are siblings
coll c and ai have the same collocation
cnk coll c and ai have the same CNC collocation
tfa contextual boundness of c and ai, agreement, joint
c freq c is a frequent word
Semantics
cand pers c is a person name
cand ewn semantic position of c?s lemma within the EuroWordNet Top Ontology
Table 4: Features used by the perceptron-based model
285
