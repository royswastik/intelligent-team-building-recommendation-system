Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 620?628,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Unsupervised Approaches for Automatic Keyword Extraction Using
Meeting Transcripts
Feifan Liu, Deana Pennell, Fei Liu and Yang Liu
Computer Science Department
The University of Texas at Dallas
Richardson, TX 75080, USA
{ffliu,deana,feiliu,yangl}@hlt.utdallas.edu
Abstract
This paper explores several unsupervised ap-
proaches to automatic keyword extraction
using meeting transcripts. In the TFIDF
(term frequency, inverse document frequency)
weighting framework, we incorporated part-
of-speech (POS) information, word clustering,
and sentence salience score. We also evalu-
ated a graph-based approach that measures the
importance of a word based on its connection
with other sentences or words. The system
performance is evaluated in different ways, in-
cluding comparison to human annotated key-
words using F-measure and a weighted score
relative to the oracle system performance, as
well as a novel alternative human evaluation.
Our results have shown that the simple un-
supervised TFIDF approach performs reason-
ably well, and the additional information from
POS and sentence score helps keyword ex-
traction. However, the graph method is less
effective for this domain. Experiments were
also performed using speech recognition out-
put and we observed degradation and different
patterns compared to human transcripts.
1 Introduction
Keywords in a document provide important infor-
mation about the content of the document. They
can help users search through information more effi-
ciently or decide whether to read a document. They
can also be used for a variety of language process-
ing tasks such as text categorization and informa-
tion retrieval. However, most documents do not
provide keywords. This is especially true for spo-
ken documents. Current speech recognition system
performance has improved significantly, but there
is no rich structural information such as topics and
keywords in the transcriptions. Therefore, there is
a need to automatically generate keywords for the
large amount of written or spoken documents avail-
able now.
There have been many efforts toward keyword ex-
traction for text domain. In contrast, there is less
work on speech transcripts. In this paper we fo-
cus on one speech genre ? the multiparty meeting
domain. Meeting speech is significantly different
from written text and most other speech data. For
example, there are typically multiple participants
in a meeting, the discussion is not well organized,
and the speech is spontaneous and contains disflu-
encies and ill-formed sentences. It is thus ques-
tionable whether we can adopt approaches that have
been shown before to perform well in written text
for automatic keyword extraction in meeting tran-
scripts. In this paper, we evaluate several differ-
ent keyword extraction algorithms using the tran-
scripts of the ICSI meeting corpus. Starting from
the simple TFIDF baseline, we introduce knowl-
edge sources based on POS filtering, word cluster-
ing, and sentence salience score. In addition, we
also investigate a graph-based algorithm in order to
leverage more global information and reinforcement
from summary sentences. We used different per-
formance measurements: comparing to human an-
notated keywords using individual F-measures and
a weighted score relative to the oracle system per-
formance, and conducting novel human evaluation.
Experiments were conducted using both the human
transcripts and the speech recognition (ASR) out-
620
put. Overall the TFIDF based framework seems to
work well for this domain, and the additional knowl-
edge sources help improve system performance. The
graph-based approach yielded worse results, espe-
cially for the ASR condition, suggesting further in-
vestigation for this task.
2 Related Work
TFIDF weighting has been widely used for keyword
or key phrase extraction. The idea is to identify
words that appear frequently in a document, but do
not occur frequently in the entire document collec-
tion. Much work has shown that TFIDF is very ef-
fective in extracting keywords for scientific journals,
e.g., (Frank et al, 1999; Hulth, 2003; Kerner et al,
2005). However, we may not have a big background
collection that matches the test domain for a reli-
able IDF estimate. (Matsuo and Ishizuka, 2004) pro-
posed a co-occurrence distribution based method us-
ing a clustering strategy for extracting keywords for
a single document without relying on a large corpus,
and reported promising results.
Web information has also been used as an ad-
ditional knowledge source for keyword extraction.
(Turney, 2002) selected a set of keywords first and
then determined whether to add another keyword hy-
pothesis based on its PMI (point-wise mutual infor-
mation) score to the current selected keywords. The
preselected keywords can be generated using basic
extraction algorithms such as TFIDF. It is impor-
tant to ensure the quality of the first selection for the
subsequent addition of keywords. Other researchers
also used PMI scores between each pair of candidate
keywords to select the top k% of words that have
the highest average PMI scores as the final keywords
(Inkpen and Desilets, 2004).
Keyword extraction has also been treated as a
classification task and solved using supervised ma-
chine learning approaches (Frank et al, 1999; Tur-
ney, 2000; Kerner et al, 2005; Turney, 2002; Tur-
ney, 2003). In these approaches, the learning al-
gorithm needs to learn to classify candidate words
in the documents into positive or negative examples
using a set of features. Useful features for this ap-
proach include TFIDF and its variations, position of
a phrase, POS information, and relative length of a
phrase (Turney, 2000). Some of these features may
not work well for meeting transcripts. For exam-
ple, the position of a phrase (measured by the num-
ber of words before its first appearance divided by
the document length) is very useful for news article
text, since keywords often appear early in the doc-
ument (e.g., in the first paragraph). However, for
the less well structured meeting domain (lack of ti-
tle and paragraph), these kinds of features may not
be indicative. A supervised approach to keyword ex-
traction was used in (Liu et al, 2008). Even though
the data set in that study is not very big, it seems that
a supervised learning approach can achieve reason-
able performance for this task.
Another line of research for keyword extrac-
tion has adopted graph-based methods similar to
Google?s PageRank algorithm (Brin and Page,
1998). In particular, (Wan et al, 2007) attempted
to use a reinforcement approach to do keyword ex-
traction and summarization simultaneously, on the
assumption that important sentences usually contain
keywords and keywords are usually seen in impor-
tant sentences. We also find that this assumption also
holds using statistics obtained from the meeting cor-
pus used in this study. Graph-based methods have
not been used in a genre like the meeting domain;
therefore, it remains to be seen whether these ap-
proaches can be applied to meetings.
Not many studies have been performed on speech
transcripts for keyword extraction. The most rel-
evant work to our study is (Plas et al, 2004),
where the task is keyword extraction in the mul-
tiparty meeting corpus. They showed that lever-
aging semantic resources can yield significant per-
formance improvement compared to the approach
based on the relative frequency ratio (similar to
IDF). There is also some work using keywords for
other speech processing tasks, e.g., (Munteanu et
al., 2007; Bulyko et al, 2007; Wu et al, 2007; De-
silets et al, 2002; Rogina, 2002). (Wu et al, 2007)
showed that keyword extraction combined with se-
mantic verification can be used to improve speech
retrieval performance on broadcast news data. In
(Rogina, 2002), keywords were extracted from lec-
ture slides, and then used as queries to retrieve rel-
evant web documents, resulting in an improved lan-
guage model and better speech recognition perfor-
mance of lectures. There are many differences be-
tween written text and speech ? meetings in par-
ticular. Thus our goal in this paper is to investi-
621
gate whether we can successfully apply some exist-
ing techniques, as well as propose new approaches
to extract keywords for the meeting domain. The
aim of this study is to set up some starting points for
research in this area.
3 Data
We used the meetings from the ICSI meeting data
(Janin et al, 2003), which are recordings of naturally
occurring meetings. All the meetings have been
transcribed and annotated with dialog acts (DA)
(Shriberg et al, 2004), topics, and extractive sum-
maries (Murray et al, 2005). The ASR output for
this corpus is obtained from a state-of-the-art SRI
conversational telephone speech system (Zhu et al,
2005), with a word error rate of about 38.2% on
the entire corpus. We align the human transcripts
and ASR output, then map the human annotated DA
boundaries and topic boundaries to the ASR words,
such that we have human annotation of these infor-
mation for the ASR output.
We recruited three Computer Science undergradu-
ate students to annotate keywords for each topic seg-
ment, using 27 selected ICSI meetings.1 Up to five
indicative key words or phrases were annotated for
each topic. In total, we have 208 topics annotated
with keywords. The average length of the topics
(measured using the number of dialog acts) among
all the meetings is 172.5, with a high standard devi-
ation of 236.8. We used six meetings as our devel-
opment set (the same six meetings as the test set in
(Murray et al, 2005)) to optimize our keyword ex-
traction methods, and the remaining 21 meetings for
final testing in Section 5.
One example of the annotated keywords for a
topic segment is:
? Annotator I: analysis, constraints, template
matcher;
? Annotator II: syntactic analysis, parser, pattern
matcher, finite-state transducers;
? Annotator III: lexicon, set processing, chunk
parser.
Note that these meetings are research discussions,
and that the annotators may not be very familiar with
1We selected these 27 meetings because they have been used
in previous work for topic segmentation and summarization
(Galley et al, 2003; Murray et al, 2005).
the topics discussed and often had trouble deciding
the important sentences or keywords. In addition,
limiting the number of keywords that an annotator
can select for a topic also created some difficulty.
Sometimes there are more possible keywords and
the annotators felt it is hard to decide which five are
the most topic indicative. Among the three annota-
tors, we notice that in general the quality of anno-
tator I is the poorest. This is based on the authors?
judgment, and is also confirmed later by an indepen-
dent human evaluation (in Section 6).
For a better understanding of the gold standard
used in this study and the task itself, we thoroughly
analyzed the human annotation consistency. We re-
moved the topics labeled with ?chitchat? by at least
one annotator, and also the digit recording part in
the ICSI data, and used the remaining 140 topic seg-
ments. We calculated the percentage of keywords
agreed upon by different annotators for each topic,
as well as the average for all the meetings. All of the
consistency analysis is performed based on words.
Figure 1 illustrates the annotation consistency over
different meetings and topics. The average consis-
tency rate across topics is 22.76% and 5.97% among
any two and all three annotators respectively. This
suggests that people do not have a high agreement
on keywords for a given document. We also notice
that the two person agreement is up to 40% for sev-
eral meetings and 80% for several individual top-
ics, and the agreement among all three annotators
reaches 20% and 40% for some meetings or topics.
This implies that the consistency depends on topics
(e.g., the difficulty or ambiguity of a topic itself, the
annotators? knowledge of that topic). Further studies
are needed for the possible factors affecting human
agreement. We are currently creating more annota-
tions for this data set for better agreement measure
and also high quality annotation.
4 Methods
Our task is to extract keywords for each of the topic
segments in each meeting transcript. Therefore, by
?document?, we mean a topic segment in the re-
mainder of this paper. Note that our task is different
from keyword spotting, where a keyword is provided
and the task is to spot it in the audio (along with its
transcript).
The core part of keyword extraction is for the sys-
622
0
0.2
0.4
0.6
0.8
1
0 30 60 90 120
3 agree
2 agree
0
0.1
0.2
0.3
0.4
0.5
1 3 5 7 9 11 13 15 17 19 21 23 25 27
3 agree
2 agree
Figure 1: Human annotation consistency across differ-
ent topics (upper graph) and meetings (lower graph). Y-
axis is the percent of the keywords agreed upon by two or
three annotators.
tem to assign an importance score to a word, and
then pick the top ranked words as keywords. We
compare different methods for weight calculation in
this study, broadly divided into the following two
categories: the TFIDF framework and the graph-
based model. Both are unsupervised learning meth-
ods.2 In all of the following approaches, when se-
lecting the final keywords, we filter out any words
appearing on the stopword list. These stopwords are
generated based on the IDF values of the words us-
ing all the meeting data by treating each topic seg-
ment as a document. The top 250 words from this
list (with the lowest IDF values) were used as stop-
words. We generated two different stopword lists for
human transcripts and ASR output respectively. In
addition, in this paper we focus on performing key-
word extraction at the single word level, therefore
no key phrases are generated.
2Note that by unsupervised methods, we mean that no data
annotated with keywords is needed. These methods do require
the use of some data to generate information such as IDF, or
possibly a development set to optimize some parameters or
heuristic rules.
4.1 TFIDF Framework
(A) Basic TFIDF weighting
The term frequency (TF) for a word wi in a doc-
ument is the number of times the word occurs in the
document. The IDF value is:
IDFi = log(N/Ni)
whereNi denotes the number of the documents con-
taining word wi, and N is the total number of the
documents in the collection. We also performed L2
normalization for the IDF values when combining
them with other scores.
(B) Part of Speech (POS) filtering
In addition to using a stopword list to remove
words from consideration, we also leverage POS in-
formation to filter unlikely keywords. Our hypothe-
sis is that verb, noun and adjective words are more
likely to be keywords, so we restrict our selection to
words with these POS tags only. We used the TnT
POS tagger (Brants, 2000) trained from the Switch-
board data to tag the meeting transcripts.
(C) Integrating word clustering
One weakness of the baseline TFIDF is that it
counts the frequency for a particular word, without
considering any words that are similar to it in terms
of semantic meaning. In addition, when the docu-
ment is short, the TF may not be a reliable indicator
of the importance of the word. Our idea is therefore
to account for the frequency of other similar words
when calculating the TF of a word in the document.
For this, we group all the words into clusters in an
unsupervised fashion. If the total term frequency
of all the words in one cluster is high, it is likely
that this cluster contributes more to the current topic
from a thematic point of view. Thus we want to as-
sign higher weights to the words in this cluster.
We used the SRILM toolkit (Stolcke, 2002) for
automatic word clustering over the entire docu-
ment collection. It minimizes the perplexity of the
induced class-based n-gram language model com-
pared to the original word-based model. Using the
clusters, we then adjust the TF weighting by inte-
grating with the cluster term frequency (CTF):
TF CTF (wi) = TF (wi)??(
P
wl?Ci,wl 6=wi freq(wl))
where the last summation component means the to-
tal term frequency of all the other words in this docu-
ment that belong to the same clusterCi as the current
623
word wi. We set parameter ? to be slightly larger
than 1. We did not include stopwords when adding
the term frequencies for the words in a cluster.
(D) Combining with sentence salience score
Intuitively, the words in an important sentence
should be assigned a high weight for keyword ex-
traction. In order to leverage the sentence infor-
mation, we adjust a word?s weight by the salience
scores of the sentences containing that word. The
sentence score is calculated based on its cosine sim-
ilarity to the entire meeting. This score is often used
in extractive summarization to select summary sen-
tences (Radev et al, 2001). The cosine similarity
between two vectors, D1 and D2, is defined as:
sim(D1, D2) =
?
i t1it2i??
i t21i ?
??
i t22i
where ti is the term weight for a word wi, for which
we use the TFIDF value.
4.2 Graph-based Methods
For the graph-based approach, we adopt the itera-
tive reinforcement approach from (Wan et al, 2007)
in the hope of leveraging sentence information for
keyword extraction. This algorithm is based on the
assumption that important sentences/words are con-
nected to other important sentences/words.
Four graphs are created: one graph in which sen-
tences are connected to other sentences (S-S graph),
one in which words are connected to other words
(W-W graph), and two graphs connecting words to
sentences with uni-directional edges (W-S and S-W
graphs). Stopwords are removed before the creation
of the graphs so they will be ineligible to be key-
words.
The final weight for a word node depends on its
connection to other words (W-W graph) and other
sentences (W-S graph); similarly, the weight for
a sentence node is dependent on its connection to
other sentences (S-S graph) and other words (S-W
graph). That is,
u = ?UTu+ ?W? T v
v = ?V T v + ?W Tu
where u and v are the weight vectors for sentence
and word nodes respectively, U, V,W, W? represent
the S-S, W-W, S-W, and W-S connections. ? and ?
specify the contributions from the homogeneous and
the heterogeneous nodes. The initial weight is a uni-
form one for the word and sentence vector. Then
the iterative reinforcement algorithm is used until
the node weight values converge (the difference be-
tween scores at two iterations is below 0.0001 for all
nodes) or 5,000 iterations are reached.
We have explored various ways to assign weights
to the edges in the graphs. Based on the results on
the development set, we use the following setup in
this paper:
? W-W Graph: We used a diagonal matrix for
the graph connection, i.e., there is no connec-
tion among words. The self-loop values are
the TFIDF values of the words. This is also
equivalent to using an identity matrix for the
word-word connection and TFIDF as the initial
weight for each vertex in the graph. We investi-
gated other strategies to assign a weight for the
edge between two word nodes; however, so far
the best result we obtained is using this diago-
nal matrix.
? S-W and W-S Graphs: The weight for an
edge between a sentence and a word is the TF
of the word in the sentence multiplied by the
word?s IDF value. These weights are initially
added only to the S-W graph, as in (Wan et al,
2007); then that graph is normalized and trans-
posed to create the W-S graph.
? S-S Graph: The sentence node uses a vector
space model and is composed of the weights of
those words connected to this sentence in the
S-W graph. We then use cosine similarity be-
tween two sentence vectors.
Similar to the above TFIDF framework, we also
use POS filtering for the graph-based approach. Af-
ter the weights for all the words are determined, we
select the top ranked words with the POS restriction.
5 Experimental Results: Automatic
Evaluation
Using the approaches described above, we com-
puted weights for the words and then picked the top
five words as the keywords for a topic. We chose five
keywords since this is the number of keywords that
624
human annotators used as a guideline, and it also
yielded good performance in the development set.
To evaluate system performance, in this section we
use human annotated keywords as references, and
compare the system output to them. The first metric
we use is F-measure, which has been widely used
for this task and other detection tasks. We compare
the system output with respect to each human anno-
tation, and calculate the maximum and the average
F-scores. Note that our keyword evaluation is word-
based. When human annotators choose key phrases
(containing more than one word), we split them into
words and measure the matching words. Therefore,
when the system only generates five keywords, the
upper bound of the recall rate may not be 100%. In
(Liu et al, 2008), a lenient metric is used which ac-
counts for some inflection of words. Since that is
highly correlated with the results using exact word
match, we report results based on strict matching in
the following experiments.
The second metric we use is similar to Pyramid
(Nenkova and Passonneau, 2004), which has been
used for summarization evaluation. Instead of com-
paring the system output with each individual hu-
man annotation, the method creates a ?pyramid?
using all the human annotated keywords, and then
compares system output to this pyramid. The pyra-
mid consists of all the annotated keywords at dif-
ferent levels. Each keyword has a score based on
how many annotators have selected this one. The
higher the score, the higher up the keyword will be in
the pyramid. Then we calculate an oracle score that
a system can obtain when generating k keywords.
This is done by selecting keywords in the decreas-
ing order in terms of the pyramid levels until we
obtain k keywords. Finally for the system hypoth-
esized k keywords, we compute its score by adding
the scores of the keywords that match those in the
pyramid. The system?s performance is measured us-
ing the relative performance of the system?s pyramid
scores divided by the oracle score.
Table 1 shows the results using human transcripts
for different methods on the 21 test meetings (139
topic segments in total). For comparison, we also
show results using the supervised approach as in
(Liu et al, 2008), which is the average of the 21-
fold cross validation. We only show the maximum
F-measure with respect to individual annotations,
since the average scores show similar trend. In ad-
dition, the weighted relative scores already accounts
for the different annotation and human agreement.
Methods F-measure weighted relative score
TFIDF 0.267 0.368
+ POS 0.275 0.370
+ Clustering 0.277 0.367
+ Sent weight 0.290 0.404
Graph 0.258 0.364
Graph+POS 0.277 0.380
Supervised 0.312 0.401
Table 1: Keyword extraction results using human tran-
scripts compared to human annotations.
We notice that for the TFIDF framework, adding
POS information slightly helps the basic TFIDF
method. In all the meetings, our statistics show that
adding POS filtering removed 2.3% of human anno-
tated keywords from the word candidates; therefore,
this does not have a significant negative impact on
the upper bound recall rate, but helps eliminate un-
likely keyword candidates. Using word clustering
does not yield a performance gain, most likely be-
cause of the clustering technique we used ? it does
clustering simply based on word co-occurrence and
does not capture semantic similarity properly.
Combining the term weight with the sentence
salience score improves performance, supporting the
hypothesis that summary sentences and keywords
can reinforce each other. In fact we performed an
analysis of keywords and summaries using the fol-
lowing two statistics:
(1) k = Psummary(wi)Ptopic(wi)
where Psummary(wi) and Ptopic(wi) represent the
the normalized frequency of a keyword wi in the
summary and the entire topic respectively; and
(2) s = PSsummaryPStopic
where PSsummary represents the percentage of the
sentences containing at least one keyword among all
the sentences in the summary, and similarly PStopic
is measured using the entire topic segment. We
found that the average k and s are around 3.42 and
6.33 respectively. This means that keywords are
625
more likely to occur in the summary compared to the
rest of the topic, and the chance for a summary sen-
tence to contain at least one keyword is much higher
than for the other sentences in the topic.
For the graph-based methods, we notice that
adding POS filtering also improves performance,
similar to the TFIDF framework. However, the
graph method does not perform as well as the TFIDF
approach. Comparing with using TFIDF alone, the
graph method (without using POS) yielded worse re-
sults. In addition to using the TFIDF for the word
nodes, information from the sentences is used in the
graph method since a word is linked to sentences
containing this word. The global information in the
S-S graph (connecting a sentence to other sentences
in the document) is propagated to the word nodes.
Unlike the study in (Wan et al, 2007), this infor-
mation does not yield any gain. We did find that the
graph approach performed better in the development
set, but it seems that it does not generalize to this test
set.
Compared to the supervised results, the TFIDF
approach is worse in terms of the individual maxi-
mum F-measure, but achieves similar performance
when using the weighted relative score. However,
the unsupervised TFIDF approach is much simpler
and does not require any annotated data for train-
ing. Therefore it may be easily applied to a new
domain. Again note that these results used word-
based selection. (Liu et al, 2008) investigated
adding bigram key phrases, which we expect to
be independent of these unigram-based approaches
and adding bigram phrases will yield further per-
formance gain for the unsupervised approach. Fi-
nally, we analyzed if the system?s keyword ex-
traction performance is correlated with human an-
notation disagreement using the unsupervised ap-
proach (TFIDF+POS+Sent weight). The correla-
tion (Spearman?s ? value) between the system?s
F-measure and the three-annotator consistency on
the 27 meetings is 0.5049 (p=0.0072). This indi-
cates that for the meetings with a high disagreement
among human annotators, it is also challenging for
the automatic systems.
Table 2 shows the results using ASR output for
various approaches. The performance measure is
the same as used in Table 1. We find that in gen-
eral, there is a performance degradation compared
to using human transcripts, which is as expected.
We found that only 59.74% of the human annotated
keywords appear in ASR output, that is, the upper
bound of recall is very low. The TFIDF approach
still outperforms the graph method. Unlike on hu-
man transcripts, the addition of information sources
in the TFIDF approach did not yield significant per-
formance gain. A big difference from the human
transcript condition is the use of sentence weight-
ing ? adding it degrades performance in ASR, in
contrast to the improvement in human transcripts.
This is possibly because the weighting of the sen-
tences is poor when there are many recognition er-
rors from content words. In addition, compared to
the supervised results, the TFIDF method has sim-
ilar maximum F-measure, but is slightly worse us-
ing the weighted score. Further research is needed
for the ASR condition to investigate better modeling
approaches.
Methods F-measure weighted relative score
TFIDF 0.191 0.257
+ POS 0.196 0.259
+ Clustering 0.196 0.259
+ Sent weigh 0.178 0.241
Graph 0.173 0.223
Graph+POS 0.183 0.233
Supervised 0.197 0.269
Table 2: Keyword extraction results using ASR output.
6 Experimental Results: Human
Evaluation
Given the disagreement among human annotators,
one question we need to answer is whether F-
measure or even the weighted relative scores com-
pared with human annotations are appropriate met-
rics to evaluate system-generated keywords. For
example, precision measures among the system-
generated keywords how many are correct. How-
ever, this does not measure if the unmatched system-
generated keywords are bad or acceptable. We
therefore performed a small scale human evaluation.
We selected four topic segments from four differ-
ent meetings, and gave output from different sys-
tems to five human subjects. The subjects ranged
in age from 22 to 63, and all but one had only basic
knowledge of computers. We first asked the eval-
626
uators to read the entire topic transcript, and then
presented them with the system-generated keywords
(randomly ordered by different systems). For com-
parison, the keywords annotated by our three hu-
man annotators were also included without reveal-
ing which sets of keywords were generated by a
human and which by a computer. Because there
was such disagreement between annotators regard-
ing what made good keywords, we instead asked our
evaluators to mark any words that were definitely
not keywords. Systems that produced more of these
rejected words (such as ?basically? or ?mmm-hm?)
are assumed to be worse than those containing fewer
rejected words. We then measured the percentage of
rejected keywords for each system/annotator. The
results are shown in Table 3. Not surprisingly, the
human annotations rank at the top. Overall, we find
human evaluation results to be consistent with the
automatic evaluation metrics in terms of the ranking
of different systems.
Systems Rejection rate
Annotator 2 8%
Annotator 3 19%
Annotator 1 25%
TFIDF + POS 28%
TFIDF 30%
Table 3: Human evaluation results: percentage of the re-
jected keywords by human evaluators for different sys-
tems/annotators.
Note this rejection rate is highly related to the re-
call/precision measure in the sense that it measures
how many keywords are acceptable (or rejected)
among the system generated ones. However, instead
of comparing to a fixed set of human annotated key-
words (e.g., five) and using that as a gold standard
to compute recall/precision, in this evaluation, the
human evaluator may have a larger set of accept-
able keywords in their mind. We also measured the
human evaluator agreement regarding the accepted
or bad keywords. We found that the agreement on
a bad keyword among five, four, and three human
evaluator is 10.1%, 14.8%, and 10.1% respectively.
This suggests that humans are more likely to agree
on a bad keyword selection compared to agreement
on the selected keywords, as discussed in Section 3
(even though the data sets in these two analysis are
not the same). Another observation from the human
evaluation is that sometimes a person rejects a key-
word from one system output, but accepts that on
the list from another system. We are not sure yet
whether this is the inconsistency from human evalu-
ators or whether the judgment is based on a word?s
occurrence with other provided keywords and thus
some kind of semantic coherence. Further investi-
gation on human evaluation is still needed.
7 Conclusions and Future Work
In this paper, we evaluated unsupervised keyword
extraction performance for the meeting domain, a
genre that is significantly different from most pre-
vious work. We compared several different ap-
proaches using the transcripts of the ICSI meeting
corpus. Our results on the human transcripts show
that the simple TFIDF based method is very compet-
itive. Adding additional knowledge such as POS and
sentence salience score helps improve performance.
The graph-based approach performs less well in this
task, possibly because of the lack of structure in
this domain. We use different performance measure-
ments, including F-measure with respect to individ-
ual human annotations and a weighted metric rela-
tive to the oracle system performance. We also per-
formed a new human evaluation for this task and our
results show consistency with the automatic mea-
surement. In addition, experiments on the ASR out-
put show performance degradation, but more impor-
tantly, different patterns in terms of the contributions
of information sources compared to using human
transcripts. Overall the unsupervised approaches are
simple but effective; however, system performance
compared to the human performance is still low,
suggesting more work is needed for this domain.
For the future work, we plan to investigate dif-
ferent weighting algorithms for the graph-based ap-
proach. We also need a better way to decide the
number of keywords to generate instead of using a
fixed number. Furthermore, since there are multiple
speakers in the meeting domain, we plan to incor-
porate speaker information in various approaches.
More importantly, we will perform a more rigorous
human evaluation, and also use extrinsic evaluation
to see whether automatically generated keywords fa-
cilitate tasks such as information retrieval or meeting
browsing.
627
Acknowledgments
This work is supported by NSF award IIS-0714132.
Any opinions expressed in this work are those of the
authors and do not necessarily reflect the views of
NSF.
References
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In Proceedings of the 6th Applied NLP Conference.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. Computer Networks
and ISDN Systems, 30.
I. Bulyko, M. Ostendorf, M. Siu, T. Ng, A. Stolcke, and
O. Cetin. 2007. Web resources for language modeling
in conversational speech recognition. ACM Transac-
tions on Speech and Language Processing, 5:1?25.
A. Desilets, B.D. Bruijn, and J. Martin. 2002. Extracting
keyphrases from spoken audio documents. In Infor-
mation Retrieval Techniques for Speech Applications,
pages 339?342.
E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and C.G.
Nevill-Manning. 1999. Domain-specific keyphrase
extraction. In Proceedings of IJCAI, pages 688?673.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proceedings of ACL.
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of
EMNLP, pages 216?223.
D. Inkpen and A. Desilets. 2004. Extracting
semantically-coherent keyphrases from speech. Cana-
dian Acoustics Association, 32:130?131.
A. Janin, D. Baron, J. Edwards, D. Ellis, G . Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,
and C. Wooters. 2003. The ICSI meeting corpus. In
Proceedings of ICASSP.
Y.H. Kerner, Z. Gross, and A. Masa. 2005. Automatic
extraction and learning of keyphrases from scientific
articles. In Computational Linguistics and Intelligent
Text Processing, pages 657?669.
F. Liu, F. Liu, and Y. Liu. 2008. Automatic keyword
extraction for the meeting corpus using supervised ap-
proach and bigram expansion. In Proceedings of IEEE
SLT.
Y. Matsuo and M. Ishizuka. 2004. Keyword extraction
from a single document using word co-occurrence sta-
tistical information. International Journal on Artifi-
cial Intelligence, 13(1):157?169.
C. Munteanu, G. Penn, and R. Baecker. 2007. Web-
based language modeling for automatic lecture tran-
scription. In Proceedings of Interspeech.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005.
Evaluating automatic summaries of meeting record-
ings. In Proceedings of ACL 2005 MTSE Workshop,
pages 33?40.
A. Nenkova and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: the pyramid method.
In Proceedings of HLT/NAACL.
L. Plas, V. Pallotta, M. Rajman, and H. Ghorbel. 2004.
Automatic keyword extraction from spoken text. a
comparison of two lexical resources: the EDR and
WordNet. In Proceedings of the LREC.
D. Radev, S. Blair-Goldensohn, and Z. Zhang. 2001. Ex-
periments in single and multi-document summariza-
tion using MEAD. In Proceedings of The First Docu-
ment Understanding Conference.
I. Rogina. 2002. Lecture and presentation tracking in an
intelligent meeting room. In Proceedings of ICMI.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.
2004. The ICSI meeting recorder dialog act (MRDA)
corpus. In Proceedings of SIGDial Workshop, pages
97?100.
A. Stolcke. 2002. SRILM ? An extensible language
modeling toolkit. In Proceedings of ICSLP, pages
901?904.
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2:303?336.
P.D. Turney. 2002. Mining the web for lexical knowl-
edge to improve keyphrase extraction: Learning from
labeled and unlabeled data. In National Research
Council, Institute for Information Technology, Techni-
cal Report ERB-1096.
P.D. Turney. 2003. Coherent keyphrase extraction via
web mining. In Proceedings of IJCAI, pages 434?439.
X. Wan, J. Yang, and J. Xiao. 2007. Towards an iter-
ative reinforcement approach for simultaneous docu-
ment summarization and keyword extraction. In Pro-
ceedings of ACL, pages 552?559.
C.H. Wu, C.L. Huang, C.S. Hsu, and K.M. Lee. 2007.
Speech retrieval using spoken keyword extraction and
semantic verification. In Proceedings of IEEE Region
10 Conference, pages 1?4.
Q. Zhu, A. Stolcke, B. Chen, and N. Morgan. 2005.
Using MLP features in SRI?s conversational speech
recognition system. In Proceedings of Interspeech.
628
