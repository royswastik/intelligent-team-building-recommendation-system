Proceedings of the Workshop on BioNLP: Shared Task, pages 115?118,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Biomedical Event Detection using Rules,  
Conditional Random Fields and Parse Tree Distances 
 
Farzaneh Sarafraz*, James Eales*, Reza Mohammadi?, Jonathan Dickerson?,  
David Robertson?, Goran Nenadic* 
*School of Computer Science, University of Manchester 
?Faulty of Life Sciences, University of Manchester 
?Dept. of Mathematics and Computer Science, Sharif University of Technology 
sarafraf@cs.man.ac.uk, g.nenadic@manchester.ac.uk 
 
Abstract 
This paper reports on a system developed for 
the BioNLP'09 shared task on detection and 
characterisation of biomedical events. Event 
triggers and types were recognised using a 
conditional random field classifier and a set of 
rules, while event participants were identified 
using a rule-based system that relied on rela-
tive distances between candidate entities and 
the trigger in the associated parse tree. The re-
sults on previously unseen test data were en-
couraging: for non-regulatory events, the F-
score was almost 50% (with precision above 
60%), with the overall F-score of around 30% 
(49% precision). The performance on more 
complex regulatory events was poor  
(F-measure of 7%). Among the 24 teams 
submitting the test results, our results were 
ranked 12th for the overall F-score and 8th for 
the F-score of non-regulation events. 
1 Introduction 
The aim of the BioNLP'09 shared task 1 was to 
characterise molecular events being reported in a 
Medline abstract by identifying the textual trigger, 
event type and participating entities (Kim et al 
2009). Nine event types were considered: gene 
expression, transcription, protein catabolism, lo-
calisation, phosporylation, binding, regulation, 
positive regulation, and negative regulation. De-
pending on the event type, the task included the 
identification of either one (for the first five event 
types mentioned above) or more (e.g. for binding) 
participating proteins. Information requested for 
regulatory events was more complex: in addition to 
one theme (a protein or another event), these 
events could also have a cause (a protein or an-
other event) that needed to be identified. 
 
The organisers have distributed a training 
dataset of 800 abstracts, with gene and gene prod-
uct mentions pre-annotated in text. In addition, a 
development set (150 abstracts) was provided to 
assess the quality of the extractions during the 
training and development phases. 
2 Methods 
The system developed for the challenge consists of 
three main modules: (1) event trigger and type de-
tection, (2) event participant detection, and  
(3) post-processing of the results. 
2.1 Event Trigger and Type Detection 
Our view of the event trigger and type detection 
subtask was that each token in a sentence needed 
to be tagged either as a trigger for one of the nine 
event types, or as a non-trigger/event token. We 
therefore decided to identify event types and trig-
gers in a single step by training a conditional ran-
dom field (CRF) classifier that assigned one of ten 
(nine types plus non-trigger) tags to each token. 
CRFs have been shown to be particularly suitable 
for tagging sequential data such as natural lan-
guage text, because they take into account features 
and tags of neighbouring tokens when evaluating 
the probability of a tag for a given token.  
Tokens and their part-of-speech (POS) tags 
were recognised using the Genia Tagger (Tsuruoka 
et al 2005). Each stemmed token was represented 
using a feature vector consisting of the following 
features:  
? A binary feature indicating whether the to-
ken is a protein; 
? A binary feature indicating whether the to-
ken is a known protein-protein interaction 
word (we used a pre-complied dictionary of 
115
such words collected from previous studies 
(Fu et al 2008; Yang et al 2008); 
? The token's POS tag; 
? The log-frequencies of the token being a 
trigger for each event type in the training 
data (nine features); 
? The number of proteins in the given sen-
tence. 
Other features (e.g. separating the known inter-
action words according to the nine event types) 
were explored during the development phase, but 
were not included in the final feature list since they 
increased the sparseness of the data and did not 
improve the overall results. The CRF parameters 
were adjusted for maximum performance, includ-
ing the choice of training algorithms, the number 
of training steps, the size of the window within 
which the tokens can affect any certain token, and 
the number of training abstracts used in each train-
ing step. It was interesting to notice that there were 
no significant improvements in the performance 
after training on 100, 400 or 800 abstracts from the 
training set (data not shown).  
2.2 Locating Event Themes 
After detecting potential triggers and associated 
event types, the next task was to locate possible 
participants (i.e. ?themes? and ?causes?) for each 
event. It was obvious that participants did not have 
to be the nearest to the trigger on the surface level, 
so our approach was based on distances within the 
parse trees associated with the sentences contain-
ing candidate events. Parse tree distances have 
been studied previously in clustering and automatic 
translation tasks (Emms 2008), so we hypothesised 
that we could use them to identify the most likely 
participants. The training data was analysed for the 
proximities between the triggers and the (correct) 
event participants in the parse tree of the sentence.1 
Figure 1 gives a detailed density function of these 
distances (ignoring non-protein nodes). The analy-
sis showed that a theme was usually amongst the 
nearest proteins to the trigger in terms of parse tree 
distances: for example, in 60% of all single theme 
events (e.g. localisation, phosphorylation) the cor-
rect protein participant was the trigger?s nearest or 
second nearest protein in the parse tree. A further 
                                                           
1
 The parse trees were produced by the GDep parser (Sagae 
and Tsujii, 2007) and supplied by the challenge organisers. 
analysis demonstrated that it was more likely for a 
theme to appear in the sub-tree of the correspond-
ing trigger, with 70% of all single theme events 
having a theme which appeared in the sub-tree of 
the trigger. Furthermore, specific analyses of the 
parse trees associated to the binding events (which 
can have more than one theme) suggested a linear 
relationship between the parse tree distance and 
binding event participant number (participant1 is 
the nearest, participant2 is the second nearest, etc.). 
 
 
Figure 1: Probability density function of the distance 
between the trigger and the theme in the parse tree  
(ignoring the tokens that are not proteins) 
 
We used this distributional analysis (derived 
from the training data) to design a rule-based 
method for the identification of participating 
themes. The rules were manually derived for each 
of the nine event classes, by defining:  
? a threshold for the maximum distance  to the 
trigger in the sub-tree for the given event 
type; 
? a threshold for the difference between the 
maximum distance in the whole tree and the 
given sub-tree for the given event type; 
? the number of nearest proteins to be re-
ported for each trigger. 
 
All entities that satisfied a distance-based rule 
for a given trigger were selected as the correspond-
ing theme(s). For example, if the event type is 
binding, then up to the second closest protein in the 
sub-tree, and the first closest protein in the rest of 
the tree are reported as themes.  
116
Figure 2 provides an example of the method ap-
plied to a sentence with multiple events. Regulates 
and secretion are correctly identified as triggers for 
a regulation and a localization event in the first 
phase. Using the rules for localization, the themes 
for two localization events are correctly recognised 
as proteins T2 and T3, whereas T1 was ignored 
since it did not appear in the trigger's sub-tree.  
Engineering and applying rules for non-
regulatory events was relatively straightforward. 
However, regulatory events can have different 
kinds of participants (a protein or an event). In the 
case of an event, we were trying to locate the near-
est trigger for the event (being regulated) in the 
parse tree. For example, in Figure 2, the nearest 
option to the regulation trigger (secretion) was the 
trigger of the two localization events, and both 
events should be (correctly) reported as the themes 
of two regulation events. Therefore, we require a 
number of recursions in the application of the rules 
to represent higher-order regulatory dependences. 
For the purposes of this challenge, only regulations 
up to the second ?order? were detected, allowing 
other events to act as themes and causes as well as 
proteins. Attempts to find more complicated regu-
latory events using this method resulted in a de-
creased precision and/or F-score.  
 
2.3 Post-processing Event Profiles 
The performance of the first two phases was 
studied on the development dataset: we noted a 
number of false-positive and false-negative results 
that were mostly due to a set of recurring triggers. 
We therefore decided to perform a post-processing 
step to improve the identification of event triggers 
and associated types. In the first step (improving 
the event trigger and type detection), the output of 
the CRF was overridden in cases where the triggers 
appeared in a list of negatively discriminated trig-
ger words which was collected after the manual 
analysis of the false positive results on the training 
and development data. Similarly, in cases where 
the CRF missed a highly indicative trigger (from a 
manually collected set) for a given event type, the 
trigger was added as part of post-processing. In the 
latter case, the sentence was then processed for the 
event theme detection (as described in 2.2).  
In the second step of the pre-processing phase, 
we forced highly indicative regulation triggers (if 
not previously identified) to be associated with an 
 
Figure 2: The parse tree of sentence Monocyte tethering 
by P-selectin regulates monocyte chemotactic protein-1 
and tumor necrosis factor-alpha secretion. The triggers 
are shown in boxes, and the entities are numbered.  
 
event by assigning proteins appearing in the sen-
tence to them, even when no protein in the sen-
tence satisfied the theme or cause criteria described 
in Section 2.2. This was aimed at improving the 
extremely low recall for regulatory events. 
Finally, since triggers could consist of more than 
one consecutive token, a set of simple rules were 
applied to remove typical false-negative constitu-
ents identified by the CRF as part of triggers (e.g. 
sometimes linking words appeared within triggers). 
3 Results and discussion 
The task 1 assessment was based on the output of 
the system when applied to the test dataset of 260 
previously unseen abstracts. An event was counted 
as a true positive if its type, trigger and all partici-
pants had been correctly identified. The overall F-
score for our system was 30.35% with 48.61% pre-
cision (approximate span matching, see Table 1). 
The best performing event types were phosphory-
lation (the best F-score and the best recall) and 
gene expression (the best precision with a reasona-
bly good F-measure). While the results for non-
regulatory events were encouraging, they were low 
for regulatory events. Among the 24 teams submit-
ting the test results, our results were ranked 12th for 
the overall F-score and 8th for the F-score of non-
regulation events. 
A preliminary analysis of the results was per-
formed on the development data (as the test data is 
not available), which had around 5% higher overall 
F-score than the test data (9% for non-regulation 
events, see Table 2 for details). 
117
Event Class #Gold R P F-score 
Localisation 174 44.83 53.06 48.60 
Binding 347 12.68 40.37 19.30 
Gene expression 722 52.63 69.34 59.84 
Transcription 137 15.33 67.74 25.00 
Protein catabolism 14 42.86 50.00 46.15 
Phosphorylation 135 78.52 53.81 63.86 
Non-reg total 1529 41.53 60.82 49.36 
Regulation 291 3.09 19.15  5.33 
Positive regulation 983 1.12 8.87 1.99 
Neg. regulation 379 12.4 20.52 15.46 
Regulatory total 1653 4.05 16.75 6.53 
All total 3182 22.06 48.61 30.35 
 
Table 1: Evaluation of the test data (260 abstracts), 
(approximate span matching; #Gold = the number of  
examples in the gold standard) 
 
 
In order to assess the effects of different steps 
in our approach, we evaluated the performance of 
the event trigger and event participant detection 
steps separately. The results presented in Table 3 
indicated that the performance of the CRF module 
was not much better than the overall performance 
of the system (an F-score of 43% vs. 35%), sug-
gesting that the CRF part was mostly responsible 
for the errors, by both missing triggers and falsely 
reporting them. This was particularly the case with 
non-regulatory events (even for binding). Con-
versely, when considering only those events whose 
triggers were correctly identified, their participants 
were also correctly recognised in most cases. 
Overall, the analysis suggested that the parse tree 
distance method performed reasonable well, de-
spite a reduction in recall of approximately 12%.  
There are a number of possibilities for im-
provements. We believe applying the CRF model 
in two stages would be a better approach to detect 
 
Event Class #Gold R P F-score 
Localisation 40 77.50 47.69 59.05 
Binding 180 33.33 54.55 41.38 
Gene expression 282 76.60 58.54 66.36 
Transcription 68 58.82 18.60 28.27 
Protein catabolism 19 84.21 88.89 86.49 
Phosphorylation 40 97.50 81.25 88.64 
Non-reg total 629 63.91 48.73 55.30 
Regulation 138 13.04 62.07 21.56 
Positive regulation 462 13.85 54.24 22.07 
Neg. regulation 153 29.41 45.92 35.86 
All total 1382 38.28 49.44 43.15 
 
Table 3: Trigger-only evaluation of the development data  
Event Class #Gold R P F-score 
Localisation 53 67.92 46.75 55.38 
Binding 312 21.47 63.81 32.13 
Gene expression 356 64.61 76.33 69.98 
Transcription 82 53.66 89.80 67.18 
Protein catabolism 21 90.48 67.86 77.55 
Phosphorylation 47 91.49 53.09 67.19 
Non-reg total 871 50.4 68.44 58.05 
Regulation 172 5.23 33.33 9.05 
Positive regulation 632 3.48 21.36 5.99 
Neg. regulation 201 9.45 15.08 11.62 
Regulatory total 1005 4.98 19.53 7.93 
All total 1876 26.07 54.46 35.26 
 
Table 2: Evaluation of the development data (150 abstracts) 
(approximate span matching; #Gold as in Table 1) 
 
events: first identify triggers and then link them to 
event classes. In addition, the rules employed for 
determining themes need to be more specific to 
reflect both event type and grammatical structure.  
In the case of regulatory events, however, signifi-
cantly better results were noticed in the trigger de-
tection part when compared to the overall scores, 
indicating that it was difficult to identify regulatory 
participants, as any of those participants could be 
either a protein or another event.  
Overall, the results achieved by our system 
suggest that combining parse tree results, rules and 
CRFs is a promising approach for the identification 
of non-regulatory events in the literature, while 
more work would be needed for regulatory events. 
References  
Emms M. 2008. Tree-distance and some other Variants 
of evalb. Proc. of LREC 2008, pp 1373-1379. 
Fu W. et al 2008. Human Immunodeficiency Virus type 
1, Human protein interaction database at NCBI, Nu-
cleic Acid Research 2008, D417-D422 
Kim JD et al 2009. Overview of BioNLP'09 Shared 
Task on Event Extraction, Proc. of BioNLP NAACL 
2009 Workshop (to appear) 
Sagae K, Tsujii J. 2007. Dependency Parsing and Do-
main Adaptation with LR Models and Parser Ensem-
bles. Proc. of the CoNLL 2007 Shared Task, 1044-50 
Tsuruoka Y. et al 2005. Developing a Robust Part of-
Speech Tagger for Biomedical Text. Advances in In-
formatics, 382?392. 
Yang H. et al 2008. Identification of Transcription Fac-
tor Contexts in Literature using Machine Learning 
Approaches. BMC Bioinformatics, Vol. 9(3):S11. 
118
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 78?85,
Uppsala, July 2010.
Using SVMs with the Command Relation Features to  
Identify Negated Events in Biomedical Literature  
Farzaneh Sarafraz 
School of Computer Science  
University of Manchester 
Manchester, United Kingdom 
sarafraf@cs.man.ac.uk 
Goran Nenadic 
School of Computer Science  
University of Manchester 
Manchester, United Kingdom 
g.nenadic@manchester.ac.uk 
  
 
 
Abstract 
In this paper we explore the identification of 
negated molecular events (e.g. protein binding, 
gene expressions, regulation, etc.) in biomedi-
cal research abstracts. We construe the prob-
lem as a classification task and apply a ma-
chine learning (ML) approach that uses lexi-
cal, syntactic, and semantic features associated 
with sentences that represent events. Lexical 
features include negation cues, whereas syn-
tactic features are engineered from constitu-
ency parse trees and the command relation be-
tween constituents. Semantic features include 
event type and participants. We also consider a 
rule-based approach that uses only the com-
mand relation. On a test dataset, the ML ap-
proach showed significantly better results 
(51% F-measure) compared to the command-
based rules (35-42% F-measure). Training a 
separate classifier for each event class proved 
to be useful, as the micro-averaged F-score 
improved to 63% (with 88% precision), dem-
onstrating the potential of task-specific ML 
approaches to negation detection. 
1 Introduction 
With almost 2000 new papers published every 
day, biomedical knowledge is mainly communi-
cated through a growing body of research papers. 
As the amount of textual information increases, 
the need for sophisticated information extraction 
(IE) methods are becoming more than evident. IE 
methods rely on a range of language processing 
methods such as named entity recognition and 
parsing to extract the required information in a 
more structured form which can be used for 
knowledge exploration and hypothesis genera-
tion (Donaldson et al 2003; Natarajan et al 
2006). 
Given the large number of publications, the 
identification of conflicting or contradicting facts 
is critical for systematic mining of biomedical 
literature and knowledge consolidation. Detec-
tion of negations is of particular importance for 
IE methods, as it often can hugely affect the 
quality of the extracted information. For exam-
ple, when mining molecular events, a key piece 
of information is whether the text states that the 
two proteins are or are not interacting, or that a 
given gene is or is not expressed. In recent years, 
several challenges and shared tasks have in-
cluded the extraction of negations, typically as 
part of other tasks (e.g. the BioNLP?09 Shared 
Task 3 (Kim et al 2009)). 
Several systems and methods have aimed to 
handle negation detection in order to improve the 
quality of extracted information (Hakenberg et 
al. 2009; Morante and Daelemans 2009). Prior 
research on this topic has primarily focused on 
finding negated concepts by negation cues and 
scopes. These concepts are usually represented 
by a set of predefined terms, and negation detec-
tion typically aims to determine whether a term 
falls within the scope of a negation cue. 
In this paper we address the task of identifi-
cation of negated events. We present a machine 
learning (ML) method that combines a set of fea-
tures mainly engineered from a sentence parse 
tree with lexical cues. More specifically, parse-
based features use the notion of the command 
relation that models the scope affected by an 
element (Langacker, 1969). We use molecular 
events as a case study and experiment on the 
BioNLP?09 data, which comprises a gold-
standard corpus of research abstracts manually 
annotated for events and negations (Kim et al 
2009). The evaluation shows that, by using the 
proposed approach, negated events can be identi-
fied with precision of 88% and recall of 49% 
(63% F-measure). We compare these results with 
two rule-based approaches that achieved the 
maximum F-measure of 42%. 
78
The rest of this paper is organised as follows. 
Section 2 summarises and reviews previous re-
search on negation extraction. Section 3 defines 
the problem and introduces the data used for the 
case study. Section 4 focuses on the ML-based 
methodology for extracting negated events. The 
final sections contain the results and discussions. 
2 Related Work 
There have been numerous contemplations of the 
concept of negation (Lawler, 2010), but no gen-
eral agreement so far exists on its definition, 
form, and function. We adopt here a definition of 
negation as given by Cambridge Encyclopedia of 
Language Sciences: ?Negation is a comparison 
between a ?real? situation lacking some element 
and an ?imaginal? situation that does not lack it?. 
The imaginal situation is affirmative compared 
with the negative real situation. The element 
whose polarity differs between the two situations 
is the negation target. 
Negations in natural language can be ex-
pressed by syntactically negative expressions, i.e. 
with the use of negating words such as no, not, 
never, etc. The word or phrase that makes the 
sentence wholly or partially negative is the nega-
tion cue and the part of the sentence that is af-
fected by the negation cue and has become nega-
tive is the negation scope.  
We briefly review two classes of approaches 
to detect negations: those aiming at negated con-
cepts and those targeting negated events.  
2.1 Detecting Negated Concepts and 
Phrases 
There have been a number of approaches sug-
gested for detection of negated targets and 
scopes. Most of them rely on task-specific, hand-
crafted rules of various complexities. They differ 
in the size and composition of the list of negation 
cues, and the way to utilise such a list. Some 
methods use parse trees, whilst others use results 
of shallow parsing. 
Rule-based methods range from simple co-
occurrence based approaches to patterns that rely 
on shallow parsing. The ?bag-of-words? ap-
proach, looking for proximate co-occurrences of 
negation cues and terms in the same sentence, is 
probably the simplest method for finding nega-
tions, and is used by many as a baseline method.  
Many approaches have targeted the clinical 
and biomedical domains. NegEx (Chapman et al 
2001), for example, uses two generic regular ex-
pressions that are triggered by negation phrases 
such as: 
 
<negation cue> * <target term> 
<target term> * <negation cue>  
 
where the asterisk (*) represents a string of up to 
five tokens. Target terms represent domain con-
cepts that are terms from the Unified Medical 
Language System (UMLS1). The cue set com-
prises 272 clinically-specific negation cues, in-
cluding those such as denial of or absence of. 
Although simple, the proposed approach showed 
good results on clinical data (78% sensitivity 
(recall), 84% precision, and 94% specificity). 
In addition to concepts that are explicitly ne-
gated by negation phrases, Patrick et al (2006) 
further consider so-called pre-coordinated nega-
tive terms (e.g. headache) that have been col-
lected from SNOMED CT2 medical terminology. 
Similarly, NegFinder uses hand-crafted rules to 
detect negated UMLS terms, including simple 
conjunctive and disjunctive statements (Mutalik 
et al 2001). They used a list of 60 negation cues. 
Tolentino et al (2006), however, show that using 
rules on a small set of only five negation cues 
(no, neither/nor, ruled out, denies, without) can 
still be reasonably successful in detecting nega-
tions in medical reports (F-score 91%). 
Huang and Lowe (2007) introduced a negation 
grammar that used regular expressions and de-
pendency parse trees to identify negation cues 
and their scope in the sentence. They applied the 
rules to a set of radiology reports and reported a 
precision of 99% and a recall of 92%. 
Not many efforts have been reported on using 
machine learning to detect patterns in sentences 
that contain negative expressions. Still, Morante 
and Daelemans (2009), for example, used vari-
ous classifiers (Memory-based Learners, Support 
Vector Machines, and Conditional Random 
Fields) to detect negation cues and their scope. 
An extensive list of features included the token?s 
stem and part-of-speech, as well as those of the 
neighbouring tokens. Separate classifiers were 
used for detecting negation cues and negation 
scopes. The method was applied to clinical text, 
biomedical abstracts, and biomedical papers with 
F-scores of 80%, 77%, and 68% respectively. 
2.2 Detecting Negated Events 
Several approaches have recently been suggested 
for the extraction of negated events, particularly 
                                                 
1
   http://www.nlm.nih.gov/research/umls/ 
2
   http://www.snomed.org 
79
in the biomedical domain. Events are typically 
represented via participants (biomedical entities 
that take part in an event) and event triggers (to-
kens that indicate presence of the event). Van 
Landeghem et al (2008) used a rule-based ap-
proach based on token distances in sentence and 
lexical information in event triggers to detect 
negated molecular events. Kilicoglu and Bergler 
(2009), Hakenberg et al (2009), and Sanchez 
(2007) used a number of heuristic rules concern-
ing the type of the negation cue and the type of 
the dependency relation to detect negated mo-
lecular events described in text. For example, a 
rule can state that if the negation cue is ?lack? or 
?absence?, then the trigger has to be in the 
prepositional phrase of the cue; or that if the cue 
is ?unable? or ?fail?, then the trigger has to be in 
the clausal complement of the cue (Kilicoglu and 
Bergler 2009). As expected, such approaches 
suffer from lower recall. 
MacKinlay et al (2009), on the other hand, 
use ML, assigning a vector of complex deep 
parse features (including syntactic predicates to 
capture negation scopes, conjunctions and se-
mantically negated verbs) to every event trigger. 
The system achieved an F-score of 36% on the 
same dataset as used in this paper. 
We note that the methods mentioned above 
mainly focus on finding negated triggers in order 
to detect negated events. In this paper we explore 
not only negation of triggers but also phrases in 
which participants are negated (consider, for ex-
ample, ?SLP-76? in the sentence ?In contrast, 
Grb2 can be coimmunoprecipitated with Sos1 
and Sos2 but not with SLP-76.?) 
3 Molecular Events  
As a case study, we look at identification of ne-
gated molecular events. In general, molecular 
events include various types of reactions that 
affect genes and protein molecules. Each event is 
of a particular type (e.g. binding, phosphoryla-
tion, regulation, etc.). Depending on the type, 
each event may have one or more participating 
proteins (sometimes referred to as themes). 
Regulatory events are particularly complex, as 
they can have a cause (a protein or another 
event) in addition to a theme, which can be either 
a protein or another event. Table 1 shows exam-
ples of five events, where participants are bio-
medical entities (events 1-3) or other events 
(events 4 and 5). Note that a sentence can ex-
press more than one molecular event. 
Identification of molecular events in the litera-
ture is a challenging IE task (Kim et al 2009; 
Sarafraz et al 2009). For the task of identifying 
negated events, we assume that events have al-
ready been identified in text. Each event is repre-
sented by its type, a textual trigger, and one or 
more participants or causes (see Table 1). Since 
the participants of different event types can vary 
in both their number and type, we consider three 
classes of events to support our analysis (see 
Section 5):  
? Class I comprises events with exactly one 
entity theme (e.g. transcription, protein ca-
tabolism, localization, gene expression, 
phosphorylation). 
? Class II events include binding events only, 
which have one or more entity participants. 
? Class III contains regulation events, which 
have exactly one theme and possibly one 
cause. However, the theme and the cause can 
be entities or events of any type.  
 
The corpus used in this study is provided by 
the BioNLP?09 challenge (Kim et al 2009). It 
contains two sets of biomedical abstracts: a 
?training? set (containing 800 abstracts used for 
training and analysis purposes) and a ?develop-
ment? set (containing 150 abstracts used for test-
ing purposes only). Both document sets are 
manually annotated with information about en-
tity mentions (e.g. genes and proteins). Sentences 
that report molecular events are further annotated 
with the corresponding event type, textual trigger 
and participants. In total, nine event types are 
?The effect of this synergism was perceptible at the level of induction of the IL-2 gene.? 
Event Trigger Type Participant (theme) Cause 
Event 1 ?induction? Gene expression IL-2  
     
?Overexpression of full-length ALG-4 induced transcription of FasL and, consequently, apoptosis.? 
Event Trigger Type Participant (theme) Cause 
Event 2 ?transcription? Transcription FasL  
Event 3 ?Overexpression? Gene expression ALG-4  
Event 4 ?Overexpression? Positive regulation Event 3  
Event 5 ?induced? Positive regulation Event 2 Event 4 
Table 1: Examples of how molecular events described in text are characterised. 
 
80
considered (gene expression, transcription, pro-
tein catabolism, localization, phosphorylation, 
binding, regulation, positive regulation, and 
negative regulation). In addition, every event has 
been tagged as either affirmative (reporting a 
specific interaction) or negative (reporting that a 
specific interaction has not been observed).   
Table 2 provides an overview of the two 
BioNLP?09 datasets. We note that only around 
6% of events are negated. 
 
Training  
data 
Development 
data Event  
class total negated total  negated 
Class I 2,858 131 559 26
Class II 887 44 249 15
Class III 4,870 440 987 66
Total 9,685 615 1,795 107
 
Table 2: Overview of the total number of events and 
negated event annotations in the two datasets. 
4 Methodology  
We consider two approaches to extract negated 
events. We first discuss a rule-based approach 
that uses constituency parse trees and the com-
mand relation to identify negated events. Then, 
we introduce a ML method that combines lexi-
cal, syntactic and semantic features to identify 
negated events. Note that in all cases, input sen-
tences have been pre-annotated for entity men-
tions, event triggers, types, and participants. 
4.1 Negation Detection Using the Command 
Relation Rules 
The question of which parts of a syntactic struc-
ture affect the other parts has been extensively 
investigated. Langacker (1969) introduced the 
concept of command to determine the scope 
within a sentence affected by an element. More 
precisely, if a and b are nodes in the constituency 
parse tree of a sentence, then a X-commands b 
iff the lowest ancestor of a with label X is also 
an ancestor of b. Note that the command relation 
is not symmetrical. Langacker observed that 
when a S-commands b, then a affects the scope 
containing b. For simplicity, we say ?command? 
when we mean S-command.  
To determine whether token a commands to-
ken b, given the parse tree of a sentence, we use 
a simple algorithm introduced by McCawley 
(1993): trace up the branches of the constituency 
parse tree from a until you hit a node that is la-
belled X. If b is reachable by tracing down the 
branches of the tree from that node, then a X-
commands b; otherwise, it does not. 
We hypothesise that if a negation cue com-
mands an event trigger or participant, then the 
associated event is negated. 
4.2 Negation Detection Using Machine 
Learning on Parse Tree Features 
Given a sentence that describes an event, we fur-
ther construe the negation detection problem as a 
classification task: the aim is to classify the event 
as affirmative or negative. We explore both a 
single SVM (support vector machine) classifier 
for all events and three separate SVMs for each 
of the event classes. The following features have 
been engineered from an event-representing sen-
tence: 
 
1. Event type (one of the nine types as defined 
in BioNLP?09); 
2. Whether the sentence contains a negation 
cue from the cue list; 
3. The negation cue itself (if present); 
4. The part-of-speech (POS) tag of the negation 
cue; 
5. The POS tag of the event trigger; 
6. The POS tag of the participants of the event. 
If the participant is another event, the POS 
tag of the trigger of that event is used; 
7. The parse node type of the lowest common 
ancestor of the trigger and the cue (i.e. the 
type of the smallest phrase that contains both 
the trigger and the cue, e.g. S, VP, PP, etc.); 
8. Whether or not the negation cue commands 
any of the participants; nested events (for 
Class III) are treated as above (i.e. as being 
represented by their triggers); 
9. Whether or not the negation cue commands 
the trigger; 
10. The parse-tree distance between the event 
trigger and the negation cue. 
 
We use a default value (null) where none of 
the other values apply (e.g. when there is no cue 
in feature 3, 4, 7). These features have been used 
to train four SVMs on the training dataset: one 
modelled all events together, and the others 
modelled the three event classes separately. 
5 Results 
All the results refer to the methods applied on the 
development dataset (see Table 2). If the nega-
tion detection task is regarded as an information 
extraction task of finding positive instances (i.e. 
81
negated events), then precision, recall, and F-
score would be appropriate measures. If we con-
sider the classification aspect of the task, speci-
ficity is more appropriate if true negative hits are 
considered as valuable as true positive ones. We 
therefore use the following metrics to evaluate 
the two methods: 
 
Precision= TPTP+FP   
 
Recall=Sensitivity= TPTP+FN  
 
F1= 2? Precision? Recall
Precision+Recall  
 
Specificity= TNTN+FP  
where TP denotes the number of true positives 
(the number of correctly identified negated 
events), FN is the number of false negatives (the 
number of negated events that have been re-
ported as affirmative), with TN and FP defined 
accordingly. 
Two sets of negation cues were used in order 
to compare their influence. A smaller set was 
derived from related work, whereas additional 
cues were semi-automatically extracted by ex-
ploring the training data. The small negation cue 
set contains 14 words3, whereas the larger nega-
tion cue set contains 32 words4. As expected, the 
larger set resulted in increased recall, but de-
creased precision. However, the effects on the F-
score were typically not significant. The results 
are only shown using the larger cue set. 
The texts were processed using the GENIA 
tagger (Tsuruoka and Tsujii 2005).We used con-
stituency parse trees automatically produced by 
two different constituency parsers reported in 
(McClosky et al 2006) and (Bikel 2004). No 
major differences were observed in the results 
using the two parsers. The data shown in the re-
sults are produced by the former.  
5.1 Baseline Results 
Our baseline method relies on an implementation 
of the NegEx algorithm as explained in Section 
2.1. Event triggers were used as negation targets 
for the algorithm. An event is then considered to 
be negated if the trigger is negated; otherwise it 
                                                 
3
 Negation cues in this set include: no, not, none, 
negative, without, absence, fail, fails, failed, fail-
ure, cannot, lack, lacking, lacked. 
4
 Negation cues in this set include the smaller set and 
18 task-specific words: inactive, neither, nor, in-
hibit, unable, blocks, blocking, preventing, pre-
vents, absent, never, unaffected, unchanged, im-
paired, little, independent, except, and exception. 
is affirmative. The results (see Table 3) are sub-
stantially lower than those reported for NegEx on 
clinical data (specificity of 94% and sensitivity 
of 78%). For comparison, the table also provides 
an even simpler baseline approach that tags as 
negated any event whose associated sentence 
contains any negation cue word. 
 
Approach P R F1 Spec. 
any negation cue present 20% 78% 32% 81% 
NegEx 36% 37% 36% 93% 
 
Table 3: Baseline results. 
(NegEx and a ?bag-of-words? approach) 
5.2 Rules Based on the Command Relation 
Table 4 shows the results of applying the S-
command relation rule for negation detection. 
We experimented with three possible ap-
proaches: an event is considered negated if  
- the negation cue commands any event 
participant in the parse tree;  
- the negation cue commands the event 
trigger in the tree; 
- the negation cue commands both. 
 
Approach P R F1 Spec. 
negation cue commands 
any participant 23% 76% 35% 84% 
negation cue  
commands trigger 23% 68% 34% 85% 
negation cue  
commands both 23% 68% 35% 86% 
 
Table 4: Performance when only the S-command  
relation is used. 
 
Compared with the baseline methods, the rules 
based on the command relation did not improve 
the performance. While precision was low 
(23%), recall was high (around 70%), indicating 
that in the majority of cases there is an S-
command relation in particular with the partici-
pants (the highest recall). We also note a signifi-
cant drop in specificity, as many affirmative 
events have triggers/participants S-commanded 
by a negation cue (not ?linked? to a given event). 
5.3 Machine Learning Results 
All SVM classifiers have been trained on the 
training dataset using a Python implementation 
of SVM Light using the linear kernel and the 
default parameters (Joachims 1999). Table 5 
shows the results of the single SVM classifier 
that has been trained for all three event classes 
together (applied on the development data). 
82
Compared to previous methods, there was sig-
nificant improvement in precision, while recall 
was relatively low. Still, the overall F-measure 
was significantly better compared with the rule-
based methods (51% vs. 35%). 
 
Feature set P R F1 Spec. 
Features 1-7 43% 8% 14% 99.2% 
Features 1-8 73% 19% 30% 99.3% 
Features 1-9 71% 38% 49% 99.2% 
Features 1-10 76% 38% 51% 99.2% 
 
Table 5: The results of the single SVM classifier. Fea-
tures 1-7 are lexical and POS tag-based features. Fea-
ture 8 models whether the cue S-commands any of the 
participants. Feature 9 is related to the cue S-
commanding the trigger. Feature 10 is the parse-tree 
distance between the cue and trigger.  
 
We first experimented with the effect of differ-
ent types of feature on the quality of the negation 
prediction. Table 5 shows the results of the first 
classifier with an incremental addition of lexical 
features, parse tree-related features, and finally a 
combination of those with the command relation 
between the negation cue and event trigger and 
participants. It is worth noting that both precision 
and recall improved as more features are added. 
We also separately trained classifiers on the 
three classes of events (see Table 6). This further 
increased the performance: compared with the 
results of the single classifier, the F1 micro-
average improved from 51% to 63%, with simi-
lar gains for both precision and recall. 
 
Event class P R F1 Spec. 
Class I 
(559 events) 94% 65% 77% 99.8% 
Class II 
(249 events) 100% 33% 50% 100% 
Class III 
(987 events) 81% 44% 57% 99.2% 
Micro Average 
(1,795 events) 88% 49% 63% 99.4% 
Macro Average 
(3 classes) 92% 47% 62% 99.7% 
 
Table 6: The results of the separate classifiers on dif-
ferent classes using common features. 
6 Discussion 
As expected, approaches that focus only on event 
triggers and their surface distances from negation 
cues proved inadequate for biomedical scientific 
articles. Low recall was mainly caused by many 
event triggers being too far from the negation cue 
to be detected as within the scope. 
Furthermore, compared to clinical notes, for 
example, sentences that describe molecular 
events are significantly more complex. For ex-
ample, the event-describing sentences in the 
training data have on average 2.6 event triggers. 
The number of events per sentence is even 
higher, as the same trigger can indicate multiple 
events, sometimes with opposite polarities. Con-
sider for example the sentence 
 
?We also demonstrate that the IKK complex, 
but not p90 (rsk), is responsible for the in vivo 
phosphorylation of I-kappa-B-alpha mediated 
by the co-activation of PKC and calcineurin.? 
 
Here, the trigger (phosphorylation) is linked with 
one affirmative and one negative regulatory 
event by two different molecules, hence trigger-
ing two events of opposite polarities. 
These findings, together with previous work, 
suggested that for any method to effectively de-
tect negations, it should be able to link the nega-
tion cue to the specific token, event trigger or 
entity name in question. Therefore, more com-
plex models are needed to capture the specific 
structure of the sentence as well as the composi-
tion of the interaction and the arrangement of its 
trigger and participants. 
By combining several feature types (lexical, 
syntactic and semantic), the machine learning 
approach proved to provide significantly better 
results. In the incremental feature addition explo-
ration process, adding the cue-commands-
participant feature had the greatest effect on the 
F-score, suggesting the significance of treating 
event participants. We note, however, that many 
of the previous attempts focus on event triggers 
only, although participants do play an important 
role in the detection of negations in biomedical 
events and thus should be used as negation tar-
gets instead of or in addition to triggers. It is in-
teresting that adding parse-tree distance between 
the trigger and negation cue improves precision 
by 5%. 
Differences in event classes (in the number 
and type of participants) proved to be important. 
Significant improvement in performance was 
observed when individual classifiers were trained 
for the three event classes, suggesting that events 
with different numbers or types of participants 
are expressed differently in text, at least when 
negations are considered. Class I events are the 
simplest (one participant only), so it was ex-
pected that negated events in this class would be 
83
the easiest to detect (F-score of 77%). Class II 
negated events (which can have multiple partici-
pants), demonstrated the lowest recall (33%). A 
likely reason is that the feature set used is not 
suitable for multi-participant events: for exam-
ple, feature 8 focuses on the negation cue com-
manding any of the participants, and not all of 
them. It is surprising that negated regulation 
events (Class III) were not the most difficult to 
identify, given their complexity. 
We applied the negation detection on the 
type, trigger and participants of pre-identified 
events in order to explore the complexity of ne-
gations, unaffected by automatic named entity 
recognition, event trigger detection, participant 
identification, etc. As these steps are typically 
performed before further characterisation of 
events, this assumption is not superficial and 
such information can be used as input to the ne-
gation detection module. MacKinlay et al (2009) 
also used gold annotations as input for negation 
detection, and reported precision, recall, and F-
score of 68%, 24%, and 36% respectively on the 
same dataset (compared to 88%, 49% and 63% 
in our case). The best performing negation detec-
tion approach in the BioNLP?09 shared task re-
ported recall of up to 15%, but with overall event 
detection sensitivity of 33% (Kilicoglu and Ber-
gler 2009) on a ?test? dataset (different from that 
used in this study). This makes it difficult to di-
rectly compare their results to our work, but we 
can still provide some rough estimates: had all 
events been correctly identified, their negation 
detection approach could have reached 45% re-
call (compared to 49% in our case). With preci-
sion of around 50%, their projected F-score, 
again assuming perfect event identification, 
could have been in the region of 50% (compared 
to 63% in our case). 
The experiments with rules that were based 
on the command relations have proven to be ge-
neric, providing very high recall (~70%) but with 
poor precision. Although only the results with S-
command relations have been reported here (see 
Table 4), we examined other types of command 
relation, namely NP-, PP-, SBAR-, and VP-
command. The only variation able to improve 
prediction accuracy was whether the cue VP-
commands any of the participants, with an F-
score of 42%, which is higher than the results 
achieved by the S-command (F-score of 35%). 
The S-command relation was used in the SVM 
modules as VP-command did not make the re-
sults significantly better. 
One of the issues we faced was the manage-
ment of multi-token and sub-token entities and 
triggers (e.g. alpha B1 and alpha B2 in ?alpha 
B1/alpha B2 ratio?, which will be typically to-
kenised as ?alpha?, ?B1/alpha?, and ?B2?). In 
our approach, we considered all the entities that 
are either multi-token or sub-token. However, if 
we assign participants that are both multi-token 
and sub-token simultaneously to events and ex-
tract similar features for the classifier from them 
as from simple entities, the F-score is reduced by 
about 2%. It would be probably better to assign a 
new category to those participants and add a new 
value for them specifically in every feature. 
7 Conclusions 
Given the number of published articles, detection 
of negations is of particular importance for bio-
medical IE. Here we explored the identification 
of negated molecular events, given their triggers 
(to characterise event type) and participants. We 
considered two approaches: 5  a rule-based ap-
proach using constituency parse trees and the 
command relation to identify negation cues and 
scopes, and a machine learning method that 
combines a set of lexical, syntactic and semantic 
features engineered from the associated sentence. 
When compared with a regular-expression-based 
baseline method (NegEx-like), the proposed ML 
method achieved significantly better results: 63% 
F-score with 88% precision. The best results 
were obtained when separate classifiers were 
trained for each of the three event classes, as dif-
ferences between them (in the number and type 
of participants) proved to be important. 
The results presented here were obtained by 
using the ?gold? event annotations as the input. It 
would be interesting to explore the impact of 
typically noisy automatic event extraction on 
negation identification. Furthermore, an immedi-
ate future step would be to explore class-specific 
features (e.g. type of theme and cause for Class 
III events, and whether the cue S-commands all 
participants for Class II events). In addition, in 
the current approach we used constituency parse 
trees. Our previous attempts to identify molecu-
lar events (Sarafraz et al 2009) as well as those 
discussed in Section 2 use dependency parse 
trees. A topic open for future research will be to 
combine information from both dependency and 
constituency parse trees as features for detecting 
negated events. 
                                                 
5
 Available at http://bit.ly/bzBaUX 
84
Acknowledgments 
We are grateful to the organisers of BioNLP?09 
for providing the annotated data. 
References 
Daniel Bikel. 2004. A Distributional Analysis of a 
Lexicalized Statistical Parsing. Proc. Conference 
on Empirical Methods in Natural Language. 
Wendy Chapman. 2001. A Simple Algorithm for 
Identifying Negated Findings and Diseases in Dis-
charge Summaries. Journal of Biomedical Infor-
matics, 34(5):301-310. 
Ian Donaldson, Martin, J., de Bruijn, B., Wolting, C., 
Lay, V., Tuekam, B., Zhang, S., Baskin, B., Bader, 
G. D., Michalickova, K., Pawson, T. and Hogue, C. 
W. 2003. PreBIND and Textomy--mining the bio-
medical literature for protein-protein interactions 
using a support vector machine. BMC Bioinf. 4: 11. 
J?rg Hakenberg, Ill?s Solt, Domonkos Tikk, Luis 
Tari, Astrid Rheinl?nder, Quang L. Ngyuen, 
Graciela Gonzalez and Ulf Leser. 2009. Molecular 
event extraction from link grammar parse trees. 
BioNLP?09: Proceedings of the Workshop on 
BioNLP. 86-94. 
Yang Huang and Henry J. Lowe. 2007. A Novel Hy-
brid Approach to Automated Negation Detection in 
Clinical Radiology Reports. Journal of the Ameri-
can Medical Informatics Association, 14(3):304-
311. 
Thorsten Joachims. 1999. Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods - 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed.) MIT-Press, MA. 
Halil Kilicoglu, and Sabine Bergler. 2009. Syntactic 
dependency based heuristics for biological event 
extraction. BioNLP?09: Proceedings of the Work-
shop on BioNLP. 119-127. 
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yo-
shinobu Kano, Jun?ichi Tsujii. 2009. Overview of 
BioNLP?09 shared task on event extraction. 
BioNLP?09: Proceedings of the Workshop on 
BioNLP. 1-9. 
Sofie Van Landeghem, Yvan Saeys, Bernard De 
Baets and Yves Van de Peer. 2008. Extracting Pro-
tein-Protein Interactions from Text using Rich Fea-
ture Vectors and Feature Selection. Proceedings of 
the Third International Symposium on Semantic 
Mining in Biomedicine. 77-84. 
Ronald Langacker. 1969. On Pronominalization and 
the Chain of Command. In D. Reibel and S. Schane 
(eds.), Modern Studies in English, Prentice-Hall, 
Englewood Cliffs, NJ. 160?186. 
John Lawler. 2010. Negation and Negative Polarity. 
The Cambridge Encyclopedia of the Language Sci-
ences. Patrick Colm Hogan (ed.) Cambridge Uni-
versity Press. Cambridge, UK. 
Andrew MacKinlay, David Martinez and Timothy 
Baldwin. 2009. Biomedical Event Annotation with 
CRFs and Precision Grammars. BioNLP?09: Pro-
ceedings of the Workshop on BioNLP. 77-85. 
James McCawley. 1993. Everything that Linguists 
have Always Wanted to Know about Logic But 
Were Ashamed to Ask. 2nd edition. The University 
of Chicago Press. Chicago, IL. 
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective Self-Training for Parsing. 
Proceedings of HLT/NAACL 2006. 152-159. 
Roser Morante and Walter Daelemans. 2009. A 
Metalearning Approach to Processing the Scope of 
Negation. CoNLL ?09: Proceedings of the 13th 
Conference on Computational Natural Language 
Learning. 21-29. 
Pradeep Mutalik, Aniruddha Deshpande, and Prakash 
M. Nadkarni. 2001. Use of General-purpose Nega-
tion Detection to Augment Concept Indexing of 
Medical Documents: A Quantitative Study using 
the UMLS. Journal of the American Medical In-
formatics Association : JAMIA. 8(6):598-609. 
Jeyakumar Natarajan, Berrar, D., Dubitzky, W., Hack, 
C., Zhang, Y., DeSesa, C., Van Brocklyn, J. R. and 
Bremer, E.G. 2006. Text mining of full-text journal 
articles combined with gene expression analysis 
reveals a relationship between sphingosine-1-
phosphate and invasiveness of a glioblastoma cell 
line. BMC Bioinformatics. 7: 373. 
Jon Patrick, Yefeng Wang, and Peter Budd. 2006. 
Automatic Mapping Clinical Notes to Medical 
Terminologies. Proc. Of the 2006 Australian Lan-
guage Technology Workshop. 75-82. 
Olivia Sanchez. 2007. Text mining applied to biologi-
cal texts: beyond the extraction of protein-protein 
interactions. PhD Thesis.  
Farzaneh Sarafraz, James Eales, Reza Mohammadi, 
Jonathan Dickerson, David Robertson and Goran 
Nenadic. 2009. Biomedical Event Detection using 
Rules, Conditional Random Fields and Parse Tree 
Distances. BioNLP?09: Proceedings of the Work-
shop on BioNLP. 
Herman Tolentino, Michael Matters, Wikke Walop, 
Barbara Law, Wesley Tong, Fang Liu, Paul Fon-
telo, Katrin Kohl, and Daniel Payne. 2006. Concept 
Negation in Free Text Components of Vaccine 
Safety Reports. AMIA Annual Symposium proc. 
Yoshimasa Tsuruoka, and Jun?ichi Tsujii. 2005. Bidi-
rectional Inference with the Easiest-First Strategy 
for Tagging Sequence Data. Proceedings of 
HLT/EMNLP 2005, 467-474. 
85
