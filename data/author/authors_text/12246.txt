Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 88?91,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
A Noisy Channel Model for Grapheme-based Machine Transliteration 
 
 
Yuxiang Jia, Danqing Zhu, Shiwen Yu 
Institute of Computational Linguistics, Peking University, Beijing, China 
Key Laboratory of Computational Linguistics, Ministry of Education, China 
{yxjia,zhudanqing,yusw}@pku.edu.cn 
 
  
 
Abstract 
Machine transliteration is an important Natu-
ral Language Processing task. This paper 
proposes a Noisy Channel Model for Graph-
eme-based machine transliteration. Moses, a 
phrase-based Statistical Machine Translation 
tool, is employed for the implementation of 
the system. Experiments are carried out on 
the NEWS 2009 Machine Transliteration 
Shared Task English-Chinese track. English-
Chinese back transliteration is studied as well. 
1 Introduction 
Transliteration is defined as phonetic translation 
of names across languages. Transliteration of 
Named Entities is necessary in many applications, 
such as machine translation, corpus alignment, 
cross-language information retrieval, information 
extraction and automatic lexicon acquisition. 
The transliteration modeling approaches can 
be classified into phoneme-based, grapheme-
based and hybrid approach of phoneme and 
grapheme. 
Many previous studies are devoted to the pho-
neme-based approach (Knight and Graehl, 1998; 
Virga and Khudanpur, 2003). Suppose that E is 
an English name and C is its Chinese translitera-
tion. The phoneme-based approach first converts 
E into an intermediate phonemic representation p, 
and then converts p into its Chinese counterpart 
C. The idea is to transform both source and target 
names into comparable phonemes so that the 
phonetic similarity between two names can be 
measured easily. 
The grapheme-based approach has also at-
tracted much attention (Li et al, 2004). It treats 
the transliteration as a statistical machine transla-
tion problem under monotonic constraint. The 
idea is to obtain the bilingual orthographical cor-
respondence directly to reduce the possible errors 
introduced in multiple conversions. 
The hybrid approach attempts to utilize both 
phoneme and grapheme information for translit-
eration. (Oh and Choi, 2006) proposed a way to 
fuse both phoneme and grapheme features into a 
single learning process. 
The rest of this paper is organized as follows. 
Section 2 briefly describes the noisy channel 
model for machine transliteration. Section 3 in-
troduces the model?s implementation details. Ex-
periments and analysis are given in section 4. 
Conclusions and future work are discussed in 
section 5. 
2 Noisy Channel Model 
Machine transliteration can be regarded as a 
noisy channel problem. Take the English-
Chinese transliteration as an example. An Eng-
lish name E is considered as the output of the 
noisy channel with its Chinese transliteration C 
as the input. The transliteration process is as fol-
lows. The language model generates a Chinese 
name C, and the transliteration model converts C 
into its back-transliteration E. The channel de-
coder is used to find ? that is the most likely to 
the word C that gives rise to E. ? is the result 
transliteration of E. 
The process can be formulated with equation 1. 
)(
)|()(
maxarg)|(maxarg EP
CEPCPECPC
CC
==
)
(1) 
Since P(E) is constant for the given E, we can 
rewrite equation 1 as follows: 
)|()(maxarg CEPCPC
C
=
)
                               (2) 
The language model P(C) is simplified as n-
gram model of Chinese characters and is trained 
with a Chinese name corpus. The transliteration 
model P(E|C) is estimated from a parallel corpus 
of English names and their Chinese translitera-
tions. The channel decoder combines the lan-
88
guage model and transliteration model to gener-
ate Chinese transliterations for given English 
names. 
3 Implementation 
Moses (Koehn et al, 2007), a phrase-based sta-
tistical machine translation tool, is leveraged to 
implement the noisy channel model for graph-
eme-based machine transliteration without reor-
dering process (Matthews, 2007). Figure 1 is an 
illustration of the phrase alignment result in ma-
chine transliteration of the name pairs ?Clinton? 
and ?????, where characters are as words and 
combinations of characters are as phrases. 
 
 
 
Figure 1. Example phrase alignment 
 
A collection of tools are used by Moses. 
SRILM is used to build statistical language mod-
els. GIZA++ is used to perform word alignments 
over parallel corpora. Mert is used for weight 
optimization. It includes several improvements to 
the basic training method including randomized 
initial conditions and permuted model order and 
dynamic parameter range expansion or restric-
tion. Bleu, an automatic machine translation 
evaluation metric, is used during Mert optimiza-
tion. Moses? beam-search decoding algorithm is 
an efficient search algorithm that quickly finds 
the highest probability translation among the ex-
ponential number of choices. 
Moses automatically trains translation models 
for any language pairs with only a collection of 
parallel corpora. The parallel transliteration cor-
pora need to be preprocessed at first. English 
names need to be lowercased. Both English 
names and Chinese transliterations are space de-
limited. Samples of preprocessed input are 
shown in figure 2. 
 
a a b y e  ? ?  
a a g a a r d  ? ? ?  
a a l l i b o n e  ? ? ?  
a a l t o  ? ? ?  
a a m o d t  ? ? ?  
  
Figure 2. Sample preprocessed name pairs 
4 Experiments 
This section describes the data sets, experimental 
setup, experiment results and analysis. 
4.1 Data Sets 
The training set contains 31961 paired names 
between English and Chinese. The development 
set has 2896 pairs. 2896 English names are given 
to test the English-Chinese transliteration per-
formance. 
Some statistics on the training data are shown 
in table 1. All the English-Chinese transliteration 
pairs are distinct. English names are unique 
while some English names may share the same 
Chinese transliteration. So the total number of 
unique Chinese names is less than that of English 
names. The Chinese characters composing the 
Chinese transliterations are limited, where there 
are only 370 unique characters in the 25033 Chi-
nese names. Supposing that the name length is 
computed as the number of characters it contains, 
the average length of English names is about 
twice that of Chinese names. Name length is use-
ful when considering the order of the character n-
gram language model. 
 
#unique transliteration pairs  31961
#unique English names 31961
#unique Chinese names 25033
#unique Chinese characters 370 
Average number of English characters 
per name 
6.8231
Average number of Chinese characters 
per name 
3.1665
Maximum number of English charac-
ters per name 
15 
Maximum number of Chinese charac-
ters per name 
7 
Table 1. Training data statistics 
4.2 Experimental setup 
Both English-Chinese forward transliteration and 
back transliteration are studied. The process can 
be divided into four steps: language model build-
ing, transliteration model training, weight tuning, 
and decoding. When building language model, 
data smoothing techniques Kneser-Ney and in-
terpolate are employed. In transliteration model 
training step, the alignment heuristic is grow-
diag-final, while other parameters are default 
settings. Tuning parameters are all defaults. 
When decoding, the parameter distortion-limit is 
set to 0, meaning that no reordering operation is 
c lin ton 
? ? ? 
89
needed. The system outputs the 10-best distinct 
transliterations. 
The whole training set is used for language 
model building and transliteration model training. 
The development set is used for weight tuning 
and system testing. 
4.3 Evaluation Metrics 
The following 6 metrics are used to measure the 
quality of the transliteration results (Li et al, 
2009a): Word Accuracy in Top-1 (ACC), Fuzzi-
ness in Top-1 (Mean F-score), Mean Reciprocal 
Rank (MRR), MAPref, MAP10, and MAPsys. 
In the data of English-Chinese transliteration 
track, each source name only has one reference 
transliteration. Systems are required to output the 
10-best unique transliterations for every source 
name. Thus, MAPref equals ACC, and MAPsys is 
the same or very close to MAP10. So we only 
choose ACC, Mean F-score, MRR, and MAP10 
to show the system performance. 
4.4 Results 
The language model n-gram order is an impor-
tant factor impacting transliteration performance, 
so we experiment on both forward and back 
transliteration tasks with increasing n-gram order, 
trying to find the order giving the best perform-
ance. Here the development set is used for test-
ing. 
Figure 3 and 4 show the results of forward and 
back transliteration respectively, where the per-
formances become steady when the order reaches 
6 and 11. The orders with the best performance 
in all metrics for forward and back transliteration 
are 2 and 5, which may relate to the average 
length of Chinese and English names. 
 
Language Model N-Gram (Order)
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6
ACC Mean F-score
MRR MAP10
Figure 3. E2C language model n-gram (forward) 
 
Language Model N-Gram (Order)
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7 8 9 10 11
ACC Mean F-score
MRR MAP10
 
Figure 4. E2C language model n-gram (back) 
 
Weights generated in the training step can be 
optimized through the tuning process. The de-
velopment set, 2896 name pairs, is divided into 4 
equal parts, 1 for testing and other 3 for tuning. 
We take the best settings as the baseline, and in-
crease tuning size by 1 part at one time. Table 2 
and 3 show the tuning results of forward and 
back transliteration, where the best results are 
boldfaced. Tuning set size of 0 refers to the best 
settings before tuning. Performances get im-
proved after tuning, among which the ACC of 
forward transliteration gets improved by over 
11%. The forward transliteration performance 
gets improved steadily with the increase of tun-
ing set size, while the back transliteration per-
formance peaks at tuning set size of 2.  
 
Tuning 
size 
ACC Mean F-score MRR MAP10
0 0.543 0.797 0.669 0.209 
1 0.645 0.851 0.752 0.231 
2 0.645 0.850 0.749 0.230 
3 0.655 0.854 0.758 0.233 
Table 2. E2C tuning performance (forward) 
 
Tuning 
size 
ACC Mean F-score MRR MAP10
0 0.166 0.790 0.278 0.092 
1 0.181 0.801 0.306 0.102 
2 0.190 0.806 0.314 0.104 
3 0.187 0.801 0.312 0.104 
Table 3. E2C tuning performance (back) 
 
Table 2 shows that forward transliteration per-
formance gets improved with the increase of tun-
ing set size, so we use the whole development set 
as the tuning set to tune the final system and the 
final official results from the shared task report 
(Li et al, 2009b) are shown in table 4. 
90
 
ACC Mean 
F-score 
MRR MAPref MAP10 MAPsys
0.652 0.858 0.755 0.652 0.232 0.232 
Table 4. The final official results of E2C forward 
 
Experiments show that forward transliteration 
has better performance than back transliteration. 
One reason may be that on average English name 
is longer than Chinese name, thus need more 
data to train a good character level language 
model. Another reason is that some information 
is lost during transliteration which can not be 
recovered in back transliteration. One more very 
important reason is as follows. Typically in back 
transliteration, you have only one correct refer-
ence transliteration, and therefore, a wide cover-
age word level language model is very useful. 
Without it, back transliteration may have a poor 
performance. 
5 Conclusions and future work 
This paper proposes a Noisy Channel Model for 
grapheme-based machine transliteration. The 
phrase-based statistical machine translation tool, 
Moses, is leveraged for system implementation. 
We participate in the NEWS 2009 Machine 
Transliteration Shared Task English-Chinese 
track. English-Chinese back transliteration is also 
studied. This model is language independent and 
can be applied to transliteration of any language 
pairs. 
To improve system performance, extensive er-
ror analyses will be made in the future and meth-
ods will be proposed according to different error 
types. We will pay much attention to back trans-
literation for its seemingly greater difficulty and 
explore relations between forward and back 
transliteration to seek a strategy solving the two 
simultaneously. 
Acknowledgements 
The authors are grateful to the organizers of the 
NEWS 2009 Machine Transliteration Shared 
Task for their hard work to provide such a good 
research platform. The work in this paper is sup-
ported by a grant from the National Basic Re-
search Program of China (No.2004CB318102) 
and a grant from the National Natural Science 
Foundation of China (No.60773173). 
References 
K. Knight and J. Graehl. 1998. Machine Translitera-
tion. Computational Linguistics, Vol. 24, No. 4, pp. 
599-612. 
P. Virga and S. Khudanpur. 2003. Transliteration of 
Proper Names in Cross-lingual Information Re-
trieval. In Proceedings of the ACL Workshop on 
Multi-lingual Named Entity Recognition 2003. 
H.Z. Li, M. Zhang and J. Su. 2004. A Joint Source 
Channel Model for Machine Transliteration. In 
Proceedings of the 42nd ACL, pp. 159-166. 
J.H. Oh and K.S. Choi. 2006. An Ensemble of Trans-
literation Models for Information Retrieval. In In-
formation Processing and Management, Vol. 42, 
pp. 980-1002. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. 
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin 
and E. Herbst. 2007. Moses: Open Source Toolkit 
for Statistical Machine Translation. In Proceedings 
of the 45th ACL Companion Volume of the Demo 
and Poster Sessions, pp. 177-180. 
D. Matthews. 2007. Machine Transliteration of Proper 
Names. Master thesis. University of Edinburgh. 
H.Z. Li, A. Kumaran, M. Zhang and V. Pervouchine. 
2009a. Whitepaper of NEWS 2009 Machine Trans-
literation Shared Task. In Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS 
2009), Singapore.  
H.Z. Li, A. Kumaran, V. Pervouchine and M. Zhang. 
2009b. Report on NEWS 2009 Machine Translit-
eration Shared Task. In Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS 
2009), Singapore. 
91
