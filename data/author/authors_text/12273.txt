Proceedings of the Workshop on BioNLP, pages 193?200,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Semantic Annotation of Papers: Interface & Enrichment Tool (SAPIENT)
Maria Liakata?, Claire Q??, Larisa N. Soldatova???
Department of Computer Science
University of Wales, Aberystwyth
SY23 3DB UK
?mal@aber.ac.uk, ??ceq08@aber.ac.uk, ???lss@aber.ac.uk
Abstract
In this paper we introduce a web application
(SAPIENT) for sentence based annotation of
full papers with semantic information. SAPI-
ENT enables experts to annotate scientific pa-
pers sentence by sentence and also to link re-
lated sentences together, thus forming spans
of interesting regions, which can facilitate text
mining applications. As part of the system,
we developed an XML-aware sentence split-
ter (SSSplit) which preserves XML markup
and identifies sentences through the addition
of in-line markup. SAPIENT has been used
in a systematic study for the annotation of
scientific papers with concepts representing
the Core Information about Scientific Papers
(CISP) to create a corpus of 225 annotated pa-
pers.
1 Introduction
Given the rapid growth in the quantity of scientific
literature, particularly in the Biosciences, there is
an increasing need to work with full papers rather
than abstracts, both to identify their key contribu-
tions and to provide some automated assistance to
researchers (Karamanis et al, 2008; Medlock and
Briscoe, 2007). Initiatives like OTMI1, which aim
to make full papers available to researchers for text
mining purposes is further evidence that relying
solely on abstracts presents important limitations for
such tasks. A recent study on whether information
retrieval from full text is more effective than search-
ing abstracts alone (Lin Jimmy, 2009) showed that
1http://opentextmining.org/wiki/Main Page
the former is indeed the case. Their experimental re-
sults suggested that span-level analysis is a promis-
ing strategy for taking advantage of the full papers,
where spans are defined as paragraphs of text as-
sessed by humans and deemed to be relevant to one
of 36 pre-defined topics. Therefore, when working
with full papers, it is important to be able to iden-
tify and annotate spans of text. In previous research,
sentence based annotation has been used to identify
text regions with scientific content of interest to the
user (Wilbur et al, 2006; Shatkay et al, 2008) or
zones of different rhetorical status (AZ) (Teufel and
Moens, 2002). Sentences are the structural units of
paragraphs and can be more flexible than paragraphs
for text mining purposes other than information re-
trieval.
Current general purpose systems for linguistic an-
notation such as Callisto2 allow the creation of a
simple annotation schema that is a tag set augmented
with simple (e.g. string) attributes for each tag.
Knowtator (Ogren, 2006) is a plug-in of the knowl-
edge representation tool Prote?ge?3, which works as
a general purpose text annotation tool and has the
advantage that it can work with complex ontology-
derived schemas. However, these systems are not
particularly suited to sentence by sentence annota-
tion of full papers, as one would need to highlight
entire sentences manually. Also these systems work
mainly with plain text, so they do not necessarily
interpret the structural information already available
in the paper, which can be crucial to annotation deci-
sions for the type of high level annotation mentioned
2http://callisto.mitre.org/manual/use.html
3http://protege.stanford.edu/
193
above. The OSCAR3 (Corbett et al, 2007) tool for
the recognition and annotation of chemical named
entities fully displays underlying paper information
in XML but is not suited to sentence by sentence an-
notation.
To address the above issues, we present a sys-
tem (SAPIENT) for sentence by sentence annota-
tion of scientific papers which supports ontology-
motivated concepts representing the core informa-
tion about scientific papers (CISP) (Soldatova and
Liakata, 2007). An important aspect of the system is
that although annotation is sentence based, the sys-
tem caters for identifiers, which link together sen-
tences pertaining to the same concept. This way
spans of interest or key regions are formed. SAPI-
ENT also incorporates OSCAR3 capability for the
automatic recognition of chemical named entities
and runs within a browser, which makes it platform
independent. SAPIENT takes as input full scien-
tific papers in XML, splits them into individual sen-
tences, displays them and allows the user to anno-
tate each sentence with one of 11 CISP concepts as
well as link the sentence to other sentences refer-
ring to the same instance of the concept selected.
The system is especially suitable for so called multi-
dimensional annotation (Shatkay et al, 2008) or
ontology-motivated annotation, where a label origi-
nates from a class with properties. SAPIENT is cur-
rently being employed by 16 Chemistry experts to
develop a corpus of scientific papers (ART Corpus)
annotated with Core Information about Scientific
Papers (CISP) covering topics in Physical Chemistry
and Biochemistry.
2 SAPIENT System Description
We chose to implement SAPIENT as a web appli-
cation, so as to make it platform independent and
easier to incorporate as part of an online workflow.
We have used state of the art web technologies to
develop SAPIENT, namely Java, Javascript (with
Asynchronous JavaScript and XML (AJAX) func-
tionality), XSLT, CSS and XML. The system has a
client-server architecture (see Figure 1), with pa-
pers being uploaded and stored on the server but
functionality for annotation contained in Javascript,
which runs client-side in the browser. This is in-
spired by but in contrast with OSCAR3 (Corbett
et al, 2007), which also allows manual annota-
tion alongside the automated annotation of chemical
named entities, but where each minor edit is saved
to the server, writing to a file. We chose to make
more of the functionality client-side in order to re-
duce the number of server requests, which could be-
come problematic if the system became widely dis-
tributed.SAPIENT ArchitectureUser Input Browser XML HttprequestresponseClick on paperPaper in.xml Page for paper upload &links to uploadedpapersPaper displayedin dynamic htmlJavascript basedannotation with CISP   Processing    with .xslClick on Save
Server
Annotations savedIn mode2.xml
Paper saved as source.xml1) Paper is splitinto sentenceswith SSSplit2) Paper savedas mode2.xml
OSCAR annotations
Figure 1: Architecture of the SAPIENT System
SAPIENT has been designed to take as input full
papers in XML, conforming to the SciXML schema
(Rupp et al, 2006)(see Section 3).
To view or annotate a paper, a user must first up-
load it. The index page of SAPIENT shows a list
of papers already uploaded (available as links) and
an interface for uploading more papers (See Figure
2). Once the user selects a link to a paper, the pa-
per is split into sentences using the XML-aware sen-
tence splitter SSSplit which we have developed (See
section 4) and is included in the server-side Java.
The resultant XML file is stored alongside the origi-
nal upload. Sentence splitting involves detecting the
boundaries of sentences and, in this context, mark-
ing the latter by inline <s></s> tags added to the
original XML. The <s></s> tags contain an id at-
tribute enumerating the sentence.
After sentence splitting, the new XML file
containing sentence boundaries marked by <s
id=#NUM>< /s> tags is parsed by XSLT into
HTML, so that it displays in the browser. In the
HTML interface dynamically generated in this way,
Javascript annotation drop-downs are available for
194
Figure 2: Index page of the SAPIENT System
each sentence. The user can perform annotations
by selecting items from the drop-downs and all the
corresponding annotation information is stored in
Javascript until a request to save is made by the user.
The Javascript drop-downs allow annotation at
two levels (Figure 3), enabling a sentence to have a
semantic label (type) with properties (subtypes) and
an identifier (conceptID).
In the current implementation of SAPIENT, The
type drop-down value corresponds to the selection
of one out of 11 general scientific concepts (Li-
akata and Soldatova, 2008), namely (?Background?,
?Conclusion?, ?Experiment?, ?Goal of the Investi-
gation?, ?Hypothesis?,?Method?, ?Model?, ?Motiva-
tion?, ?Object of the Investigation?, ?Observation?,
?Result?). These labels originate from a set of
meta-data (The Core Information about Scientific
Concepts (CISP) (Soldatova and Liakata, 2007)
which were constructed using an ontology method-
ology, based on an ontology of experiments EXPO
(Soldatova and King, 2006). Because these labels
map to ontology classes, they can also have prop-
erties. For example, ?Method? has the property
?New?/?Old?,?Advantage?/?Disadvantage?. These
properties are dependent on the type selected and
are expressed in terms of the subtype drop-down.
The third drop-down, concept ID allows a user to
provide a concept identifier. The latter is an entity
formed by the name of a concept and a number (e.g.
?Res2?). Concept identifiers uniquely identify an in-
stance of a concept (e.g. the second Result), but not
a sentence. That is, concept identifiers designate and
link together instances of the same semantic con-
cept, spread across different sentences, which can
be in different parts of the paper. For example, the
second result (?Res2?) can be referred to by 1 sen-
tence in the abstract, 5 sentences in the Discussion
and 2 sentences in the Conclusion sections.
The distinction between sentence identifiers and
concept identifiers is an important characteristic of
the system. It means that the system does not neces-
sarily assume a ?1-1? correspondence between a sen-
tence and a concept, but rather that concepts can be
represented by spans of often disjoint text. There-
fore, SAPIENT indirectly allows the annotation of
discourse segments beyond the sentence level and
also keeps track of co-referring sentences.
2.1 SAPIENT Usability
Even though SAPIENT has been primarily designed
to work with CISP concepts, it can be used to an-
notate papers according to any sentence based anno-
tation scheme. Changes required can be easily per-
formed by modifying the XSL sheet which dynami-
cally generates HTML from XML and organises the
structure of drop-down menus. Automated noun-
phrase based annotation from existing ontologies
is available to SAPIENT users through OSCAR3
(Corbett et al, 2007), since SAPIENT incorporates
OSCAR3 functionality for chemical named entity
recognition. The latter is implemented as a link
which when selected calls the OSCAR3 workflow
(integrated in the system) to automatically recognise
chemical named entities (NEs) (See Figure 5).
When all annotations (both sentence based and
chemical NEs) are saved to the server, a new ver-
sion of the XML file is produced, which contains
in-line annotation for sentences as well as extra in-
line annotation for the semantic concepts and NEs
embedded within <s></s> tags. These annotation
tags are compliant with the SciXML schema (Rupp
et al, 2006) and in the case of sentence-based anno-
tations are of the form:
<annotationART atype=??GSC??
type=#TYPE
conceptID=#CONCEPTID
novelty=??Yes/No??
advantage=??Yes/No??
</annotationART>
(See Figure 4). The attribute type, stands for the
CISP concept selected for the sentence in question.
The conceptID attribute is an enumerator of the par-
ticular concept, which the sentence refers to. For
195
example, two different sentences will have differ-
ent sentence ids but if they refer to the same con-
cept (e.g. the same ?Conclusion?) , they will be
assigned the same concept ID (e.g. ?Con3?). The
attributes novelty and advantage, are properties of
the concepts assigned to a sentence and depend
on the concept selection. They take boolean val-
ues or the dummy value ?None? if the properties
are not defined for a particular concept. For ex-
ample, these attributes are relevant when the con-
cept selected is a ?Method?, in which case the
method can be ?New/Old? and/or have an ?Advan-
tage/Disadvantage?. The novelty and advantage at-
tributes co-exist in the annotation (as can be seen in
Figure 4) but they are not set by the system at the
same time. For instance, if a sentence refers to a new
method, it will be given the type ?Method? and the
subtype ?New?; this sets the novelty attribute in the
underlying XML to ?Yes? and leaves the advantage
attribute set to the default ?None?. The sentence will
also be given a conceptID, e.g. ?Met1?. If another
sentence refers to an advantage of this method, then
the new sentence will be assigned the type ?Method?,
the subtype ?Advantage? (which sets the underlying
advantage attribute to ?Yes?) and the same concep-
tID ?Met1?. The novelty attribute value is then in-
herited from the novelty attribute value of the first
coreferring sentence, which in this case is ?New?.
3 Input: Paper in XML
SAPIENT currently accepts as input papers in XML,
especially ones compliant with the SciXML schema
(Rupp et al, 2006). SciXML is ideally suited for
this purpose as it was developed for representing the
logical structure of scientific research papers. Tags
used in the schema serve the purpose of paper iden-
tification (e.g. <TITLE>,<AUTHOR>), defining
sections of the paper (e.g. <DIV>,<HEADER>),
text sections with specific function and formatting
(e.g. <ABSTRACT>, <EQUATION>), paragraph
tags <P>, references, tables, figures and footnotes,
lists, bibliography. SAPIENT operates only on the
<TITLE>, <ABSTRACT> ,<BODY> and <P>
tags, leaving out any list elements following the
body, such as acknowledgements, figures or refer-
ences at the end of the paper. This is because we
make the assumption that only the abstract and the
body contain sentences with semantic content of any
importance to the research carried out in the paper.
This would have been different if SAPIENT anno-
tated figures as well, but such provision is not cur-
rently made. Tags such as <REF>, citations in the
text, are included within the sentence boundaries.
Even though SAPIENT was developed with the
SciXML schema in mind, it will work with any
well formed XML document that has <PAPER>
as the root node and which also contains an
<ABSTRACT> and <BODY> node. Therefore, it
is relatively easy to adapt SAPIENT to other XML
schemas.
4 SSSplit: Sapient Sentence Splitting
4.1 Sentence Matching
The reason for developing our own sentence split-
ter was that sentence splitters widely available could
not handle XML properly. The XML markup con-
tains useful information about the document struc-
ture and formatting in the form of inline tags,
which is important for determining the logical struc-
ture of the paper. The latter is worth preserv-
ing for our purposes, since it can influence the
annotation of individual sentences. XML markup
(e.g. <ABSTRACT>,<REF>,<EQUATION>)
needs to be combined carefully with tags designat-
ing sentence boundaries (<s></s>), so that the
resulting document is in well formed XML. Cur-
rent sentence splitters ignore XML markup, which
means that any document formatting/information
would have to be removed in order to use them.
RASP (Briscoe et al, 2006), the sentence splitter
used in the Sciborg project4 at the University of
Cambridge, can deal with XML but has to be com-
piled for different operating systems, which would
result in compromising the platform independence
of SAPIENT. A recent MPhil thesis (Owusu, 2008)
has also developed an XML-aware sentence splitter
but the code is in Microsoft C#.Net and therefore not
platform independent.
We have written the XML-aware sentence split-
ter SSSplit in the platform-independent Java lan-
guage (version 1.6), based on and extending open
source Perl code5 for handling plain text. In or-
4http://www.cl.cam.ac.uk/research/nl/sciborg/www/
5http://search.cpan.org/ tgrose/HTML-Summary-0.017/
196
Figure 3: Example of SAPIENT annotation through selection from drop-down menu.
Figure 4: Behind the scenes: Example XML fragment of a paper annotated using SAPIENT.
Figure 5: Incorporation of OSCAR3 annotations in SAPIENT, after selecting the link ?Auto Annotate?
197
der to make our sentence splitter XML aware, we
translated the Perl regular expression rules into Java
and modifed them to make them compatible with the
SciXML(Rupp et al, 2006) schema. We then fur-
ther improved the rules, by training on a set of 14
papers in SciXML. This involved displaying the pa-
pers, checking whether the XML was well formed
and making corrections accordingly. We would ob-
serve cases of oversplit and undersplit sentences and
amend the rules while keeping them as general as
possible. The rules in SSSplit were evaluated by
comparing the system output against a gold standard
of 41 papers, where sentence boundaries had been
provided by human experts (See section 4.2). The
sentence splitter is integrated within the SAPIENT
system but is also available as a separate package
(?SSSplit?). This should enable any future work to
easily incorporate or extend it. It is currently trained
for splitting papers in SciXML, but can be easily
ported to any other kind of XML, as discussed in
section 3.
4.2 SSSplit Evaluation
SAPIENT and SSSplit have been have been em-
ployed by more than 20 different users to success-
fully display 270 full papers. For a more accurate
evaluation of the quality of the sentences produced
by SSSplit, we used a Perl script which compared
the sentence boundaries (start and end) generated
by SSSplit, to sentence tags in a set of 41 papers
(SciXML files) annotated manually by human ex-
perts. If both the start and end of a sentence matched
up in the generated and manual versions, we consid-
ered this a true positive result. In the case where a
sentence did not match in the two versions, we first
searched for a matching end in our generated set of
sentences and then in the hand annotated version. If
the ?true? end of the sentence (as defined by the man-
ual annotation) was found in later sentences in the
SSSplit version, this meant that the system had split
a sentence too early, or ?oversplit?. This we consid-
ered to be a false positive, since we had detected a
sentence boundary where in reality there was none.
This would result in the following sentence being
matched at the end only, which also counts as a false
positive. In the case where the end of the SSSplit
sentence was found in a later sentence, within the
set of ?true? sentences, it meant that our sentence
RASP Owusu SSSplit
Precision 0.994 0.996 0.964
Recall 0.983 0.990 0.994
F-measure 0.988 0.992 0.978
Table 1: Comparison of sentence splitters in RASP,
Owusu and SSSplit.
spanned too wide, or that the system had ?under-
split?. These cases we considered to be false nega-
tives, as we had failed to detect a sentence boundary
where there was one.
Our training consisted of 14 papers in the fields of
physical chemistry and biochemistry. A different set
of 41 papers distinct from the training set but from
the same thematic domain was used as a test set. Out
of these 41 papers, 36 feature as a test set (with n-
fold validation) also for the sentence splitters RASP
(Briscoe et al, 2006) and the XML-aware sentence
splitter developed by (Owusu, 2008). The results for
all three systems, obtained as medians of Precision,
Recall and F-measure for the 36 papers are shown in
Table 1.
Precision is the proportion of true positives over
all end and start tags returned, giving a measure of
the number of boundaries identified correctly. Re-
call is the proportion of true positives over all the
relevant start and end tags in the hand-annotated pa-
pers, giving a measure of the number of boundaries
actually found. F-Measure combines Precision and
Recall to give a more balanced view on the system
performance.
In comparison with RASP and the XML-Aware
splitter of (Owusu, 2008), SSSplit performed well,
though it did not outperform these systems. Their
highest result for precision was 0.996 (vs 0.964 for
SSSplit) and for recall 0.990 (vs 0.994 for SSSplit).
We can explain their higher results somewhat by
their use of n-fold cross-validation on 36 out of the
same 41 papers that we used, which can allow in-
formation from the test set to leak into the training
data. We did not perform n-fold cross-validation, as
this would have involved going through each of the
papers and removing any potential influence on our
regular expression rules of the sentences included
within, which is a non-trivial process. Our test data
was completely unseen, which meant that our eval-
198
Training Testing
(1979 sentences) (5002 sentences)
Precision 0.961 0.964
Recall 0.995 0.994
F-measure 0.96875 0.978
Table 2: Comparison of SSSplit on the training and test-
ing papers. The training set consisted of 14 papers (1979
sentences) and the testing set of 41 papers (5002 sen-
tences).
uation is stricter, avoiding any influence from the
training data.
In addition to the comparison between SSSplit
and the other two XML-aware sentence splitters, we
also performed a comparison between our training
and testing sets, depicted in Table 2.
As can be seen in Table 2, recall was only slightly
better on the training set than the test set, but preci-
sion was worse on the training set, presumably be-
cause of lack of attention being paid to the oversplit-
ting in a particular paper (?b103844n?). This shows
that we have not overfitted to the training set in de-
veloping our splitter. Our recall is particularly high,
indicating that our splitter makes very few false neg-
ative errors. We can attribute many of the false pos-
itive errors to our somewhat small set of abbrevi-
ations considered, resulting in oversplit sentences.
We would like to incorporate a more sophisticated
approach to abbreviations in the future.
5 Performing CISP Annotations
Within the context of the ART project (Soldatova et
al., 2007), SAPIENT has been used by 16 Chem-
istry experts to annotate 265 papers from RSC Pub-
lishing journals, covering topics in Physical Chem-
istry and Biochemistry. Experts have been anno-
tating the papers sentence by sentence, assigning
each sentence one of 11 core scientific concepts and
linking together sentences across a paper which re-
fer to the same instance of a concept. The aim
is to create a corpus of annotated papers (ART-
corpus) with regions of scientific interest identified
by CISP concepts (?Result?,?Conclusion?, ?Obser-
vation?,?Method? and so on).
A preliminary evaluation of the experts? agree-
ment on the ART Corpus, based on a sample of
41 papers, annotated by the 16 experts in non-
overlapping groups of 3, shows significant agree-
ment between annotators, given the difficulty of
the task (an average kappa co-efficient of 0.55 per
group). The details of this work are beyond the
scope of the current paper, but the preliminary re-
sults underline the usability of both the CISP meta-
data and SAPIENT. In the future, we plan to further
evaluate the ART Corpus by incorporating existing
machine learning algorithms into SAPIENT and au-
tomating the generation of CISP meta-data. This
would make SAPIENT a very useful tool and would
indeed add a lot more value to the meta-data, since
training and paying annotators is a costly process
and manually annotating papers is incredibly time
consuming.
6 Conclusion and Future Work
We have presented SAPIENT, a web-based tool for
the annotation of full papers, sentence by sentence,
with semantic information. We have also discussed
how these annotations result in the indirect defini-
tion of regions of interest within the paper. The sys-
tem has been already tested in a systematic study
and has been employed for the creation of a corpus
of papers annotated with CISP concepts (ART Cor-
pus). In the future we plan to extend SAPIENT so
that the system can itself suggest annotation labels
to users. We also plan to target the needs of partic-
ular users such as authors of papers, reviewers and
editors.
SAPIENT, SSSplit and their documenta-
tion are both available for download from
http://www.aber.ac.uk/compsci/Research/bio/art/sapient/.
Acknowledgments
We would like to thank Peter Corbett, Amanda
Clare, Jem Rowland and Andrew Sparkes for
reading and commenting on earlier versions
of this paper. We would also like to thank
the anonymous reviewers for their useful com-
ments. This work was part of the ART Project
(http://www.aber.ac.uk/compsci/Research/bio/art/),
funded by the U.K. Higher Education Joint
Information Services Committee (JISC).
199
References
E. Briscoe, J. Carroll and R. Watson 2006. The Sec-
ond Release of the RASP System. Proceedings of the
COLING/ACL 2006 Interactive Presentation Sessions,
Sydney, Australia.
P. Corbett, P. Batchelor and S. Teufel. 2007. Annotation
of Chemical Named Entities. Proc. BioNLP.
Nikiforos Karamanis, Ruth Seal, Ian Lewin, Peter Mc-
Quilton, Andreas Vlachos, Caroline Gasperin, Rachel
Drysdale and Ted Briscoe. 2008. Natural Language
Processing in aid of FlyBase curators. BMC Bioinfor-
matics, 9:193.
Maria Liakata and Larisa N. Soldatova. 2008. Guide-
lines for the annotation of General Scientific Concepts
JISC Project Report, http://ie-repository.jisc.ac.uk/.
Jimmy Lin 2009. Is Searching Full Text More Effec-
tive Than Searching Abstracts? BMC Bioinformatics,
10:46.
Ben Medlock and Ted Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific literature.
45th Annual Meeting of the Association for Compu-
tational Linguistics, 23-30 Jun 2007, Prague, Czech
Republic.
P. Ogren. 2006. Knowtator: a Prote?ge? plug-in for an-
notated corpus construction. Proceedings of the 2006
Conference of the North American Chapter of the As-
sociation For Computational Linguistics on Human
Language Technology: Companion Volume: Demon-
strations, New York Press, New York, June 04 - 09,
2006.
Lawrence Owusu. 2008. XML-Aware Sentence Splitter.
MPhil thesis, Cambridge, UK.
CJ Rupp, Ann Copestake, Simone Teufel and Ben Wal-
dron. 2006. Flexible Interfaces in the Application of
Language Technology to an eScience Corpus. Pro-
ceedings of the UK e-Science Programme All Hands
Meeting 2006 (AHM2006), Nottingham, UK
Hagit Shatkay, Fengxia Pan, Andrey Rzhetsky and W.
John Wilbur. 2008. Multi-dimensional classification
of biomedical text: Toward automated, practical pro-
vision of high-utility text to diverse users. Bioinfor-
matics, 24(18):2086?2093.
Larisa N. Soldatova and Maria Liakata. 2007. An ontol-
ogy methodology and CISP - the proposed Core Infor-
mation about Scientific Papers. JISC Project Report,
http://ie-repository.jisc.ac.uk/137/.
L. Soldatova, C. Batchelor, M. Liakata, H. Fielding, S.
Lewis and R. King 2007. ART: An ontology based
tool for the translation of papers into Semantic Web
format. Proceedings of the SIG/ISMB07 ontology
workshop., p.33?36.
Larisa N. Soldatova and Ross D. King. 2006. An On-
tology of Scientific Experiments. Journal of the Royal
Society Interface, 3:795?803.
S. Teufel and M. Moens. 2002. Summarizing Scientific
Articles ? Experiments with Relevance and Rhetorical
Status. Computational Linguistics, 28(4). (preprint)
W. Wilbur, A. Rzhetsky and H. Shatkay. 2006. New Di-
rections in Biomedical Text Annotations: Deifinitions,
Guidelines and Corpus Construction. BMC Bioinfor-
matics, 7:356.
200
