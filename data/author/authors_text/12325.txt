Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 25?28,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Capturing Errors in Written Chinese Words 
Chao-Lin Liu1 Kan-Wen Tien2 Min-Hua Lai3 Yi-Hsuan Chuang4 Shih-Hung Wu5
1-4National Chengchi University, 5Chaoyang University of Technology, Taiwan 
{1chaolin, 296753027, 395753023, 494703036}@nccu.edu.tw, 5shwu@cyut.edu.tw 
 
Abstract 
A collection of 3208 reported errors of Chinese 
words were analyzed. Among which, 7.2% in-
volved rarely used character, and 98.4% were 
assigned common classifications of their causes 
by human subjects. In particular, 80% of the er-
rors observed in writings of middle school stu-
dents were related to the pronunciations and 
30% were related to the compositions of words. 
Experimental results show that using intuitive 
Web-based statistics helped us capture only 
about 75% of these errors. In a related task, the 
Web-based statistics are useful for recommend-
ing incorrect characters for composing test items 
for "incorrect character identification" tests 
about 93% of the time. 
1 Introduction 
Incorrect writings in Chinese are related to our under-
standing of the cognitive process of reading Chinese 
(e.g., Leck et al, 1995), to our understanding of why 
people produce incorrect characters and our offering 
corresponding remedies (e.g., Law et al, 2005), and 
to building an environment for assisting the prepara-
tion of test items for assessing students? knowledge of 
Chinese characters (e.g., Liu and Lin, 2008). 
Chinese characters are composed of smaller parts 
that can carry phonological and/or semantic informa-
tion. A Chinese word is formed by Chinese characters. 
For example, ??? (Singapore) is a word that con-
tains three Chinese characters. The left (?) and the 
right (?) part of ?, respectively, carry semantic and 
phonological information. Evidences show that pro-
duction of incorrect characters are related to either 
phonological or the semantic aspect of the characters. 
In this study, we investigate several issues that are 
related to incorrect characters in Chinese words. In 
Section 2, we present the sources of the reported er-
rors. In Section 3, we analyze the causes of the ob-
served errors. In Section 4, we explore the effective-
ness of relying on Web-based statistics to correct the 
errors. The current results are encouraging but de-
mand further improvements. In Section 5, we employ 
Web-based statistics in the process of assisting teach-
ers to prepare test items for assessing students? 
knowledge of Chinese characters. Experimental re-
sults showed that our method outperformed the one 
reported in (Liu and Lin, 2008), and captured the best 
candidates for incorrect characters 93% of the time. 
2 Data Sources 
We obtained data from three major sources. A list that 
contains 5401 characters that have been believed to be 
sufficient for everyday lives was obtained from the 
Ministry of Education (MOE) of Taiwan, and we call 
the first list the Clist, henceforth. We have two lists of 
words, and each word is accompanied by an incorrect 
way to write certain words. The first list is from a 
book published by MOE (MOE, 1996). The MOE 
provided the correct words and specified the incorrect 
characters which were mistakenly used to replace the 
correct characters in the correct words. The second 
list was collected, in 2008, from the written essays of 
students of the seventh and the eighth grades in a 
middle school in Taipei. The incorrect words were 
entered into computers based on students? writings, 
ignoring those characters that did not actually exist 
and could not be entered.  
We will call the first list of incorrect words the 
Elist, and the second the Jlist from now on. Elist and 
Jlist contain, respectively, 1490 and 1718 entries. 
Each of these entries contains a correct word and the 
incorrect character. Hence, we can reconstruct the 
incorrect words easily. Two or more different ways to 
incorrectly write the same words were listed in differ-
ent entries and considered as two entries for simplic-
ity of presentation. 
3 Error Analysis of Written Words 
Two subjects, who are native speakers of Chinese and 
are graduate students in Computer Science, examined 
Elist and Jlist and categorized the causes of errors. 
They compared the incorrect characters with the cor-
rect characters to determine whether the errors were 
pronunciation-related or semantic-related. Referring 
to an error as being ?semantic-related? is ambiguous. 
Two characters might not contain the same semantic 
part, but are still semantically related. In this study, 
we have not considered this factor. For this reason we 
refer to the errors that are related to the sharing of 
semantic parts in characters as composition-related. 
It is interesting to learn that native speakers had a 
high consensus about the causes for the observed er-
rors, but they did not always agree. Hence, we studied 
the errors that the two subjects had agreed categoriza-
tions. Among the 1490 and 1718 words in Elist and 
Jlist, respectively, the two human subjects had con-
sensus over causes of 1441 and 1583 errors.  
The statistics changed when we disregarded errors 
that involved characters not included in Clist. An er-
ror would be ignored if either the correct or the incor-
rect character did not belong to the Clist. It is possible 
for students to write such rarely used characters in an 
incorrect word just by coincidence. 
After ignoring the rare characters, there were 1333 
and 1645 words in Elist and Jlist, respectively. The 
subjects had consensus over the categories for 1285 
25
and 1515 errors in Elist and Jlist, respectively.  
Table 1 shows the percentages of five categories of 
errors: C for the composition-related errors, P for the 
pronunciation-related errors, C&P for the intersection 
of C and P, NE for those errors that belonged to nei-
ther C nor P, and D for those errors that the subjects 
disagreed on the error categories. There were, respec-
tively, 505 composition-related and 1314 pronuncia-
tion-related errors in Jlist, so we see 30.70% 
(=505/1645) and 79.88% (=1314/1645) in the table. 
Notice that C&P represents the intersection of C and 
P, so we have to deduct C&P from the sum of C, P, 
NE, and D to find the total probability, namely 1. 
It is worthwhile to discuss the implication of the 
statistics in Table 1. For the Jlist, similarity between 
pronunciations accounted for nearly 80% of the errors, 
and the ratio for the errors that are related to composi-
tions and pronunciations is 1:2.6. In contrast, for the 
Elist, the corresponding ratio is almost 1:1. The Jlist 
and Elist differed significantly in the ratios of the er-
ror types. It was assumed that the dominance of pro-
nunciation-related errors in electronic documents was 
a result of the popularity of entering Chinese with 
pronunciation-based methods. The ratio for the Jlist 
challenges this popular belief, and indicates that even 
though the errors occurred during a writing process, 
rather than typing on computers, students still pro-
duced more pronunciation-related errors than compo-
sition-related errors. Distribution over error types is 
not as related to input method as one may have be-
lieved. Nevertheless, the observation might still be a 
result of students being so used to entering Chinese 
text with pronunciation-based method that the organi-
zation of their mental lexicons is also pronunciation 
related. The ratio for the Elist suggests that editors of 
the MOE book may have chosen the examples with a 
special viewpoint in their minds ? balancing the errors 
due to pronunciation and composition. 
4 Reliability of Web-based Statistics  
In this section, we examine the effectiveness of using 
Web-based statistics to differentiate correct and incor-
rect characters. The abundant text material on the 
Internet gives people to treat the Web as a corpus (e.g., 
webascorpus.org). When we send a query to Google, 
we will be informed of the number of pages (NOPs) 
that possibly contain relevant information. If we put 
the query terms in quotation marks, we should find 
the web pages that literally contain the query terms. 
Hence, it is possible for us to compare the NOPs for 
two competing phrases for guessing the correct way 
of writing. At the time of this writing, Google found 
107000 and 3220 pages, respectively, for ?strong tea? 
and ?powerful tea?. (When conducting such advanced 
searches with Google, the quotation marks are needed 
to ensure the adjacency of individual words.) Hence, 
?strong? appears to be a better choice to go with ?tea?. 
How does this strategy serve for learners of Chinese? 
We verified this strategy by sending the words in 
both the Elist and the Jlist to Google to find the NOPs. 
We can retrieve the NOPs from the documents re-
turned by Google, and compare the NOPs for the cor-
rect and the incorrect words to evaluate the strategy. 
Again, we focused on those in the 5401 words that the 
human subjects had consensus about their error types. 
Recall that we have 1285 and 1515 such words in 
Elist and Jlist, respectively. As the information avail-
able on the Web changes all the time, we also have to 
note that our experiments were conducted during the 
first half of March 2009. The queries were submitted 
at reasonable time intervals to avoid Google?s treating 
our programs as malicious attackers. 
Table 2 shows the results of our investigation. We 
considered that we had a correct result when we found 
that the NOP for the correct word larger than the NOP 
for the incorrect word. If the NOPs were equal, we 
recorded an ambiguous result; and when the NOP for 
the incorrect word is larger, we recorded an incorrect 
event. We use ?C?, ?A?, and ?I? to denote ?correct?, 
?ambiguous?, and ?incorrect? events in Table 2.  
The column headings of Table 2 show the setting 
of the searches with Google and the set of words that 
were used in the experiments. We asked Google to 
look for information from web pages that were en-
coded in traditional Chinese (denoted Trad). We 
could add another restriction on the source of infor-
mation by asking Google to inspect web pages from 
machines in Taiwan (denoted Twn+Trad). We were 
not sure how Google determined the languages and 
locations of the information sources, but chose to trust 
Google. The headings ?Comp? and ?Pron? indicate 
whether the words whose error types were composi-
tion and pronunciation-related, respectively.  
Table 2 shows eight distributions, providing ex-
perimental results that we observed under different 
settings. The distribution printed in bold face showed 
that, when we gathered information from sources that 
were encoded in traditional Chinese, we found the 
correct words 73.12% of the time for words whose 
error types were related to composition in Elist. Under 
the same experimental setting, we could not judge the 
correct word 4.58% of the time, and would have cho-
sen an incorrect word 22.30% of the time. 
Statistics in Table 2 indicate that web statistics is 
not a very reliable factor to judge the correct words. 
The average of the eight numbers in the ?C? rows is 
only 71.54% and the best sample is 76.59%, suggest-
Table 2. Reliability of Web-based statistics 
Trad Twn+Trad  
Comp Pron Comp Pron 
C 73.12% 73.80% 69.92% 68.72%
A 4.58% 3.76% 3.83% 3.76%
E
list 
I 22.30% 22.44% 26.25% 27.52%
C 76.59% 74.98% 69.34% 65.87%
A 2.26% 3.97% 2.47% 5.01%
Jlist 
I 21.15% 21.05% 28.19% 29.12%
Table 1. Error analysis for Elist and Jlist 
 C P C&P NE D 
Elist 66.09% 67.21% 37.13% 0.23% 3.60%
Jlist 30.70% 79.88% 20.91% 2.43% 7.90%
26
ing that we did not find the correct words frequently. 
We would made incorrect judgments 24.75% of the 
time. The statistics also show that it is almost equally 
difficult to find correct words for errors that are com-
position and pronunciation related. In addition, the 
statistics reveal that choosing more features in the 
advanced search affected the final results. Using 
?Trad? offered better results in our experiments than 
using ?Twn+Trad?. This observation may arouse a 
perhaps controversial argument. Although Taiwan has 
proclaimed to be the major region to use traditional 
Chinese, their web pages might not have used as ac-
curate Chinese as web pages located in other regions. 
We have analyzed the reasons for why using Web-
based statistics did not find the correct words. Fre-
quencies might not have been a good factor to deter-
mine the correctness of Chinese. However, the myriad 
amount of data on the Web should have provided a 
better performance. Google?s rephrasing our submit-
ted queries is an important factor, and, in other cases, 
incorrect words were more commonly used. 
5 Facilitating Test Item Authoring 
Incorrect character correction is a very popular type of 
test in Taiwan. There are simple test items for young 
children, and there are very challenging test items for 
the competitions among adults. Finding an attractive 
incorrect character to replace a correct character to 
form a test item is a key step in authoring test items.  
We have been trying to build a software environ-
ment for assisting the authoring of test items for in-
correct character correction (Liu and Lin, 2008, Liu et 
al., 2009). It should be easy to find a lexicon that con-
tains pronunciation information about Chinese charac-
ters. In contrast, it might not be easy to find visually 
similar Chinese characters with computational meth-
ods. We expanded the original Cangjie codes (OCC), 
and employed the expanded Cangjie codes (ECC) to 
find visually similar characters (Liu and Lin, 2008).  
With a lexicon, we can find characters that can be 
pronounced in a particular way. However, this is not 
enough for our goal. We observed that there were 
different symptoms when people used incorrect char-
acters that are related to their pronunciations. They 
may use characters that could be pronounced exactly 
the same as the correct characters. They may also use 
characters that have the same pronunciation and dif-
ferent tones with the correct character. Although rela-
tively infrequently, people may use characters whose 
pronunciations are similar to but different from the 
pronunciation of the correct character.  
As Liu and Lin (2008) reported, replacing OCC 
with ECC to find visually similar characters could 
increase the chances to find similar characters. Yet, it 
was not clear as to which components of a character 
should use ECC. 
5.1 Formalizing the Extended Cangjie Codes 
We analyzed the OCCs for all the words in Clist to 
determine the list of basic components. We treated a 
Cangjie basic symbol as if it was a word, and com-
puted the number of occurrences of n-grams based on 
the OCCs of the words in Clist. Since the OCC for a 
character contains at most five symbols, the longest n-
grams are 5-grams. Because the reason to use ECC 
was to find common components in characters, we 
disregarded n-grams that repeated no more than three 
times. In addition, the n-grams that appeared more 
than three times might not represent an actual compo-
nent in Chinese characters. Hence, we also removed 
such n-grams from the list of our basic components. 
This process naturally made our list include radicals 
that are used to categorize Chinese characters in typi-
cal printed dictionaries. The current list contains 794 
components, and it is possible to revise the list of ba-
sic components in our work whenever necessary. 
After selecting the list of basic components with 
the above procedure, we encoded the words in Elist 
with our list of basic components. We adopted the 12 
ways that Liu and Lin (2008) employed to decompose 
Chinese characters. There are other methods for de-
composing Chinese characters into components. 
Juang et al (2005) and the research team at the Sinica 
Academia propose 13 different ways for decomposing 
characters. 
5.2 Recommending Incorrect Alternatives 
With a dictionary that provides the pronunciation of 
Chinese characters and the improved ECC encodings 
for words in the Elist, we can create lists of candidate 
characters for replacing a specific correct character in 
a given word to create a test item for incorrect charac-
ter correction.  
There are multiple strategies to create the candidate 
lists. We may propose the candidate characters be-
cause their pronunciations have the same sound and 
the same tone with those of the correct character (de-
noted SSST). Characters that have same sounds and 
different tones (SSDT), characters that have similar 
sounds and same tones (MSST), and characters that 
have similar sounds and different tones (MSDT) can 
be considered as candidates as well. It is easy to judge 
whether two Chinese characters have the same tone. 
In contrast, it is not trivial to define ?similar? sound. 
We adopted the list of similar sounds that was pro-
vided by a psycholinguistic researcher (Dr. Chia-Ying 
Lee) at the Sinica Academia. 
In addition, we may propose characters that look 
similar to the correct character. Two characters may 
look similar for two reasons. They may contain the 
same components, or they contain the same radical 
and have the same total number of strokes (RS). 
When two characters contain the same component, the 
shared component might or might not locate at the 
same position within the bounding boxes of characters.  
In an authoring tool, we could recommend a lim-
ited number of candidate characters for replacing the 
correct character. We tried two strategies to compare 
and choose the visually similar characters. The first 
strategy (denoted SC1) gave a higher score to the 
shared component that located at the same location in 
the two characters being compared. The second strat-
27
egy (SC2) gave the same score to any shared compo-
nent even if the component did not reside at the same 
location in the characters. When there were more than 
20 characters that receive nonzero scores, we chose to 
select at most 20 characters that had leading scores as 
the list of recommended characters. 
5.3 Evaluating the Recommendations 
We examined the usefulness of these seven categories 
of candidates with errors in Elist and Jlist. The first 
set of evaluation (the inclusion tests) checked only 
whether the lists of recommended characters con-
tained the incorrect character in our records. The sec-
ond set of evaluation (the ranking tests) was designed 
for practical application in computer assisted item 
generation. Only for those words whose actual incor-
rect characters were included in the recommended list, 
we replaced the correct characters in the words with 
the candidate incorrect characters, submitted the in-
correct words to Google, and ordered the candidate 
characters based on their NOPs. We then recorded the 
ranks of the incorrect characters among all recom-
mended characters.  
Since the same character may appear simultane-
ously in SC1, SC2, and RS, we computed the union of 
these three sets, and checked whether the incorrect 
characters were in the union. The inclusion rate is 
listed under Comp. Similarly, we computed the union 
for SSST, SSDT, MSST, and MSDT, checked whether 
the incorrect characters were in the union, and re-
corded the inclusion rate under Pron. Finally, we 
computed the union of the lists created by the seven 
strategies, and recorded the inclusion rate under Both. 
The second and the third rows of Table 3 show the 
results of the inclusion tests. The data show the per-
centage of the incorrect characters being included in 
the lists that were recommended by the seven strate-
gies. Notice that the percentages were calculated with 
different denominators. The number of composition-
related errors was used for SC1, SC2, RS, and Comp 
(e.g. 505 that we mentioned in Section 3 for the Jlist); 
the number of pronunciation-related errors for SSST, 
SSDT, MSST, MSDT, and Pron (e.g., 1314 mentioned 
in Section 3 for the Jlist); the number of either of 
these two errors for Both (e.g., 1475 for Jlist).  
The results recorded in Table 3 show that we were 
able to find the incorrect character quite effectively, 
achieving better than 93% for both Elist and Jlist. The 
statistics also show that it is easier to find incorrect 
characters that were used for pronunciation-related 
problems. Most of the pronunciation-related problems 
were misuses of characters that had exactly the same 
pronunciations with the correct characters. Unex-
pected confusions, e.g., those related to pronuncia-
tions in Chinese dialects, were the main for the failure 
to capture the pronunciation-related errors. SSDT is a 
crucial complement to SSST. There is still room to 
improve our methods to find confusing characters 
based on their compositions. We inspected the list 
generated by SC1 and SC2, and found that, although 
SC2 outperformed SC1 on the inclusion rate, SC1 and 
SC2 actually generated complementary lists and 
should be used together. The inclusion rate achieved 
by the RS strategy was surprisingly high.  
The fourth and the fifth rows of Table 3 show the 
effectiveness of relying on Google to rank the candi-
date characters for recommending an incorrect charac-
ter. The rows show the average ranks of the included 
cases. The statistics show that, with the help of 
Google, we were able to put the incorrect character on 
top of the recommended list when the incorrect char-
acter was included.  This allows us to build an envi-
ronment for assisting human teachers to efficiently 
prepare test items for incorrect character identification. 
6 Summary  
The analysis of the 1718 errors produced by real stu-
dents show that similarity between pronunciations of 
competing characters contributed most to the ob-
served errors. Evidences show that the Web statistics 
are not very reliable for differentiating correct and 
incorrect characters. In contrast, the Web statistics are 
good for comparing the attractiveness of incorrect 
characters for computer assisted item authoring.  
Acknowledgements 
This research has been funded in part by the National 
Science Council of Taiwan under the grant NSC-97-
2221-E-004-007-MY2. We thank the anonymous re-
viewers for invaluable comments, and more responses 
to the comments are available in (Liu et al 2009). 
References  
D. Juang, J.-H. Wang, C.-Y. Lai, C.-C. Hsieh, L.-F. Chien, 
J.-M. Ho. 2005. Resolving the unencoded character 
problem for Chinese digital libraries, Proc. of the 5th 
ACM/IEEE Joint Conf. on Digital Libraries, 311?319. 
S.-P. Law, W. Wong, K. M. Y. Chiu. 2005. Whole-word 
phonological representations of disyllabic words in the 
Chinese lexicon: Data from acquired dyslexia, Behav-
ioural Neurology, 16, 169?177. 
K. J. Leck, B. S. Weekes, M. J. Chen. 1995. Visual and 
phonological pathways to the lexicon: Evidence from 
Chinese readers, Memory & Cognition, 23(4), 468?476. 
C.-L. Liu et al 2009. Phonological and logographic influ-
ences on errors in written Chinese words, Proc. of the 7th 
Workshop on Asian Language Resources, 47th ACL. 
C.-L. Liu, J.-H. Lin. 2008. Using structural information for 
identifying similar Chinese characters, Proc. of the 46th 
ACL, short papers, 93?96. 
MOE. 1996. Common Errors in Chinese Writings (???
???), Ministry of Education, Taiwan. 
Table 3. Incorrect characters were contained and ranked high in the recommended lists 
 SC1 SC2 RS SSST SSDT MSST MSDT Comp Pron Both 
Elist 73.92% 76.08% 4.08% 91.64% 18.39% 3.01% 1.67% 81.97% 99.00% 93.37% 
Jlist 67.52% 74.65% 6.14% 92.16% 20.24% 4.19% 3.58% 77.62% 99.32% 97.29% 
Elist 3.25 2.91 1.89 2.30 1.85 2.00 1.58 
Jlist 2.82 2.64 2.19 3.72 2.24 2.77 1.16 
28
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 84?91,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Phonological and Logographic Influences on Errors in Written 
Chinese Words 
Chao-Lin Liu1 Kan-Wen Tien2 Min-Hua Lai3 Yi-Hsuan Chuang4 Shih-Hung Wu5
1-4National Chengchi University, 5Chaoyang University of Technology, Taiwan 
{1chaolin, 296753027, 395753023, 494703036}@nccu.edu.tw, 5shwu@cyut.edu.tw 
 
Abstract 
We analyze a collection of 3208 reported errors 
of Chinese words. Among these errors, 7.2% in-
volved rarely used character, and 98.4% were 
assigned common classifications of their causes 
by human subjects. In particular, 80% of the er-
rors observed in the writings of middle school 
students were related to the pronunciations and 
30% were related to the logographs of the words. 
We conducted experiments that shed light on us-
ing the Web-based statistics to correct the errors, 
and we designed a software environment for pre-
paring test items whose authors intentionally re-
place correct characters with wrong ones. Ex-
perimental results show that using Web-based 
statistics can help us correct only about 75% of 
these errors. In contrast, Web-based statistics are 
useful for recommending incorrect characters for 
composing test items for ?incorrect character 
identification? tests about 93% of the time. 
1 Introduction 
Incorrect writings in Chinese are related to our under-
standing of the cognitive process of reading Chinese 
(e.g., Leck et al, 1995), to our understanding of why 
people produce incorrect characters and our offering 
corresponding remedies (e.g., Law et al, 2005), and 
to building an environment for assisting the prepara-
tion of test items for assessing students? knowledge of 
Chinese characters (e.g., Liu and Lin, 2008). 
Chinese characters are composed of smaller parts 
that can carry phonological and/or semantic informa-
tion. A Chinese word is formed by Chinese characters. 
For example, ??? (Singapore) is a word that con-
tains three Chinese characters. The left (?) and the 
right (?) part of ?, respectively, carry semantic and 
phonological information. The semantic information, 
in turn, is often related to the logographs that form the 
Chinese characters. Evidences show that production 
of incorrect characters are related to phonological, 
logographic, or the semantic aspect of the characters. 
Although the logographs of Chinese characters can be 
related to the lexical semantics, not all errors that are 
related to semantics were caused by the similarity in 
logographs. Some were due to the context of the 
words and/or permissible interpretations of different 
words.  
In this study, we investigate issues that are related 
to the phonological and logographical influences on 
the occurrences of incorrect characters in Chinese 
words. In Section 2, we present the details about the 
sources of the reported errors. We have collected er-
rors from a published book and from a group of mid-
dle school students. In Section 3, we analyze the 
causes of the observed errors. Native speakers of Chi-
nese were asked to label whether the observed errors 
were related to the phonological or the logographic 
reasons. In Section 4, we explore the effectiveness of 
relying on Web-based statistics to correct the errors. 
We submitted an incorrect word and a correct word 
separately to Google to find the number of web pages 
that contained these words. The correct and incorrect 
words differed in just the incorrect character. We ex-
amine whether the number of web pages that con-
tained the words can help us find the correct way of 
writing. In Section 5, we employ Web-based statistics 
in the process of assisting teachers to prepare test 
items for assessing students? knowledge of Chinese 
characters. Experimental results showed that our 
method outperformed the one reported in (Liu and Lin, 
2008), and captured the incorrect characters better 
than 93% of the time. 
2 Data Sources 
We obtained data from three major sources. A list that 
contains 5401 characters that have been believed to be 
sufficient for everyday lives was obtained from the 
Ministry of Education (MOE) of Taiwan, and we call 
the first list the Clist, henceforth. The 5401 characters 
form the core basis for the BIG-5 code, and an official 
introduction of these 5401 characters is available at 
http://www.cns11643.gov.tw/AIDB/encodings.do#encode4.  
We have two lists of words, and each word is ac-
companied by an incorrect way to write the word. The 
first list is from a book published by MOE (1996). 
The MOE provided the correct words and specified 
the incorrect characters which were mistakenly used 
to replace the correct characters in the correct words. 
The second list was collected, in 2008, from the writ-
ten essays of students of the seventh and the eighth 
grades in a middle school in Taipei. The incorrect 
characters were entered into computers based on stu-
dents? writings, ignoring those characters that did not 
actually exist and could not be entered.  
We will call the first list of word the Elist, and the 
second the Jlist from now on. Elist and Jlist contain, 
respectively, 1490 and 1718 entries. Each of these 
entries contains a correct word and the incorrect char-
acter. Hence, we can reconstruct the incorrect words 
84
easily. Two or more different ways to incorrectly 
write the same words were listed in different entries 
and considered as two entries for simplicity of presen-
tation. 
3 Error Analysis of Written Words 
Two human subjects, who are native speakers of Chi-
nese and are graduate students in Computer Science, 
examined Elist and Jlist and categorized the causes of 
errors. They compared the incorrect characters with 
the correct characters to determine whether the errors 
were pronunciation-related or logographs-related. 
Referring to an error as being ?semantics-related? is 
ambiguous. Two characters might not contain the 
same semantic part, but are still semantically related, 
e.g., misusing ???(tou1) for ???(tou2) in ????
??. In this study, we have not considered this factor. 
For this reason we refer to the errors that are related to 
the sharing of logographic parts in characters as com-
position-related. 
Among the 1490 and 1718 words in Elist and Jlist, 
respectively, the two human subjects had consensus 
over causes of 1441 and 1583 errors. It is interesting 
to learn that native speakers had a high consensus 
about the causes for the observed errors, but they did 
not always agree. To have a common standard in 
comparison, we studied the errors that the two sub-
jects had agreed categorizations.  
The statistics changed when we disregarded errors 
that involved characters not included in Clist. An er-
ror would be ignored if either the correct or the incor-
rect character did not belong to the Clist. It is possible 
for students to write such rarely used characters in an 
incorrect word just by coincidence.  
After ignoring the rare characters, there were 1333 
and 1645 words in Elist and Jlist, respectively. The 
subjects had consensus over the causes of errors for 
1285 and 1515 errors in Elist and Jlist, respectively.  
Table 1 shows the percentages of five categories of 
errors: C for the composition-related errors, P for the 
pronunciation-related errors, C&P for the intersection 
of C and P, NE for those errors that belonged to nei-
ther C nor P, and D for those errors that the subjects 
disagreed on the error categories. There were, respec-
tively, 505 composition-related and 1314 pronuncia-
tion-related errors in Jlist, so we see 
505/1645=30.70% and 1314/1645=79.88% in the 
table. Notice that C&P represents the intersection of 
C and P, so we have to deduct C&P from the sum of 
C, P, NE, and D to find the total probability, namely 1. 
It is worthwhile to discuss the implication of the 
statistics in Table 1. For the Jlist, similarity between 
pronunciations accounted for nearly 80% of the errors, 
and the ratio for the errors that are related to composi-
tions and pronunciations is 1:2.6. In contrast, for the 
Elist, the corresponding ratio is almost 1:1. The Jlist 
and Elist differed significantly in the ratios of the er-
ror types. It was assumed that the dominance of pro-
nunciation-related errors in electronic documents was 
a result of the popularity of entering Chinese with 
pronunciation-based methods. The ratio for the Jlist 
challenges this popular belief, and indicates that even 
though the errors occurred during a writing process, 
rather than typing on computers, students still pro-
duced more pronunciation-related errors than compo-
sition-related errors. Distribution over error types is 
not as related to input method as one may have be-
lieved. Nevertheless, the observation might still be a 
result of students being so used to entering Chinese 
text with pronunciation-based method that the organi-
zation of their mental lexicons is also pronunciation 
related. The ratio for the Elist suggests that editors of 
the MOE book may have chosen the examples with a 
special viewpoint in their minds ? balancing pronun-
ciation and composition related errors. 
4 Reliability of Web-based Statistics  
In this section, we examine the effectiveness of using 
Web-based statistics to differentiate correct and incor-
rect characters. The abundant text material on the 
Internet gives people to treat the Web as a corpus (e.g., 
webascorpus.org). When we send a query to Google, 
we will be informed of the number of pages (NOPs) 
that possibly contain relevant information. If we put 
the query terms in quotation marks, we should find 
the web pages that literally contain the query terms. 
Hence, it is possible for us to compare the NOPs for 
two competing phrases for guessing the correct way 
of writing. At the time of this writing, Google found 
107000 and 3220 pages, respectively, for ?strong tea? 
and ?powerful tea?. (When conducting such advanced 
searches with Google, the quotation marks are needed 
to ensure the adjacency of individual words.) Hence, 
?strong? appears to be a better choice to go with ?tea?. 
This is an idea similar to the approach that compute 
collocations based on word frequencies (cf. Manning 
and Sch?tze, 1999). Although the idea may not work 
very well for small database, the size of the current 
Web should be considered large enough. 
Using the quotation marks for the query terms en-
forced the influences of the surrounding characters in 
Chinese words, and provides a better clue for judging 
correct usage of Chinese characters. For instance,  
without the context, ??? and ??? might be used 
incorrectly to replace each other because they have 
the same pronunciation, i.e., Mei3. It is relatively 
unlikely for one to replace ??? with ??? when we 
write ???? (every one), but these two characters can 
become admissible candidates when we write ???? 
(USA) and ???? (every country).  
Table 1. Error analysis for Elist and Jlist 
 C P C&P NE D 
Elist 66.09% 67.21% 37.13% 0.23% 3.60%
Jlist 30.70% 79.88% 20.91% 2.43% 7.90%
85
4.1 Field Tests 
We test this strategy by sending the words in Elist and 
Jlist to Google to find the NOPs. We can retrieve the 
NOPs from the documents returned by Google, and 
compare the NOPs for the correct and the incorrect 
words to evaluate the strategy. Again, we focused on 
those in the 5401 words that the human subjects had 
consensus about their error types. Recall that we have 
1285 and 1515 such words in Elist and Jlist, respec-
tively. As the information available on the Web 
changes all the time, we also have to note that our 
experiments were conducted during the first half of 
March 2009. The queries were submitted at reason-
able time intervals to avoid Google?s treating our pro-
grams as malicious attackers. 
Table 2 shows the results of our investigation. We 
considered that we had a correct result when we found 
that the NOP for the correct word was larger than the 
NOP for the incorrect word. If the NOPs were equal, 
we recorded an ambiguous result; and when the NOP 
for the incorrect word was larger, we recorded an in-
correct event. We use ?C?, ?A?, and ?I? to denote ?cor-
rect?, ?ambiguous?, and ?incorrect? events in Table 2.  
The column headings of Table 2 show the setting 
of the searches with Google and the set of words that 
were used in the experiments. We asked Google to 
look for information from web pages that were en-
coded in traditional Chinese (denoted Trad). We 
could add another restriction on the source of infor-
mation by asking Google to inspect web pages from 
machines in Taiwan (denoted Twn+Trad). We were 
not sure how Google determined the languages and 
locations of the information sources, but chose to trust 
Google. The headings ?Comp? and ?Pron? indicate 
whether the words whose error types were composi-
tion and pronunciation-related, respectively.  
Table 2 shows eight distributions, providing ex-
perimental results that we observed under different 
settings. The distribution printed in bold face showed 
that, when we gathered information from sources that 
were encoded in traditional Chinese, we found the 
correct words 73.12% of the time for words whose 
error types were related to composition in Elist. Under 
the same experimental setting, we could not judge the 
correct word 4.58% of the time, and would have cho-
sen an incorrect word 22.30% of the time. 
Statistics in Table 2 indicate that web statistics is 
not a very reliable factor to judge the correct words. 
The average of the eight numbers in the ?C? rows is 
only 71.54% and the best sample is 76.59%, suggest-
ing that we did not find the correct words frequently. 
We would made incorrect judgments 24.75% of the 
time. The statistics also show that it is almost equally 
difficult to find correct words for errors that are com-
position and pronunciation related. In addition, the 
statistics reveal that choosing more features in the 
advanced search affected the final results. Using 
?Trad? offered better results in our experiments than 
using ?Twn+Trad?. This observation may arouse a 
perhaps controversial argument. Although Taiwan is 
the main area to use traditional Chinese, their web 
pages might not have used as accurate Chinese as web 
pages located in other regions. 
4.2 An Error Analysis for the Field Tests 
We have analyzed the reasons for why using Web-
based statistics did not always find the correct words. 
Frequencies might not have been a good factor to de-
termine the correctness of Chinese. However, the 
myriad amount of data on the Web should have pro-
vided a better performance.  
The most common reason for errors is that some of 
the words are really confusing such that the majority 
of the Web pages actually used the incorrect words. 
Some of errors were so popular that even one of the 
Chinese input methods on Windows XP offered 
wrong words as possible choices, e.g., ????? (the 
correct one) vs. ?????. It is interesting to note that 
people may intentionally use incorrect words in some 
occasions; for instance, people may choose to write 
homophones in advertisements.  
Another popular reason is that whether a word is 
correct depends on a larger context. For instance, ??
?? is more popular than ???? because the former 
is a popular nickname. Unless we had provided more 
contextual information about the queried words, 
checking only the NOPs of ???? and ???? led us 
to choose ????, which happened to be an incorrect 
word when we meant to find the right way to write 
????. Another difficult pair of words to distinguish 
is ???? and ????. 
Yet another reason for having a large NOP of the 
incorrect words was due to errors in segmenting Chi-
nese character strings. Consider a correct character 
string ?WXYZ?, where ?WX? and ?YZ? are two cor-
rect words. It is possible that ?XY? happens to be an 
incorrect way to write a correct word. This is the case 
for having the counts for ?????? to contribute to 
the count for ???? which is an incorrect form of 
????. 
5 Facilitating Test Item Authoring 
Incorrect character correction is a very popular type of 
test in Taiwan. There are simple test items for young 
children, and there are very challenging test items for 
the competitions among adults. Finding an attractive 
incorrect character to replace a correct character to 
form a test item is a key step in authoring test items.  
Table 2. Reliability of Web-based statistics 
Trad Twn+Trad  
Comp Pron Comp Pron 
C 73.12% 73.80% 69.92% 68.72%
A 4.58% 3.76% 3.83% 3.76%
E
list 
I 22.30% 22.44% 26.25% 27.52%
C 76.59% 74.98% 69.34% 65.87%
A 2.26% 3.97% 2.47% 5.01%
Jlist 
I 21.15% 21.05% 28.19% 29.12%
86
We have been trying to build a software environ-
ment for assisting the authoring of test items for in-
correct character correction (Liu and Lin, 2008, Liu et 
al., 2009). It should be easy to find a lexicon that con-
tains pronunciation information about Chinese charac-
ters. In contrast, it might not be easy to find visually 
similar Chinese characters with computational meth-
ods. We expanded the original Cangjie codes (OCC), 
and employed the expanded Cangjie codes (ECC) to 
find visually similar characters (Liu and Lin, 2008). 
Cangjie encoding (Chu, 2009) is a special system 
for representing the formation of Chinese characters 
with a sequence of at most five basic symbols. For 
instance, ??? and ??? are represented by ????
?? and ??????, respectively. It is evident that 
the Cangjie codes are useful for finding visually simi-
lar characters. 
With a lexicon, we can find characters that can be 
pronounced in a particular way. However, this is not 
enough for our goal. We observed that there were 
different symptoms when people used incorrect char-
acters that are related to their pronunciations. They 
may use characters that could be pronounced exactly 
the same as the correct characters. They may also use 
characters that have the same pronunciation and dif-
ferent tones with the correct character. Although rela-
tively infrequently, people may use characters whose 
pronunciations are similar to but different from the 
pronunciation of the correct character.  
We reported that replacing OCCs with ECCs to 
find visually similar characters could increase the 
chances to find similar characters. Instead of saving 
?????? for ??? directly, we divide a Chinese 
character into subareas systematically, and save the 
Cangjie codes for each of the subareas. A Chinese 
character is stored with the information about how it 
is divided into subareas and the Cangjie sequences for 
each of its subareas.  The internal code for how we 
divide ??? is 2, and the ECC for ??? has two parts: 
??? and ?????. Yet, it was not clear as to which 
components of a character should use ECCs (Liu and 
Lin, 2008; Liu et al, 2009). 
5.1 Formalizing the Extended Cangjie Codes 
We analyzed the OCCs for all the characters in Clist 
to determine the list of basic components, with com-
puter programs. We treated a basic Cangjie symbol as 
if it was a word, and computed the number of occur-
rences of n-grams based on the OCCs of the charac-
ters in Clist. Since the OCC for a character contains at 
most five symbols, the longest n-grams are 5-grams. 
Because the reason to use ECCs was to find common 
components in characters, we saved n-grams that re-
peated no less than three times in a list. After obtain-
ing this initial list of n-grams, we removed those n-
grams that were substrings of longer n-grams in the 
list.  
In addition, the n-grams that appeared no less than 
three times might not represent an actual part in any 
Chinese characters. This may happen by chance be-
cause we considered only frequencies of n-grams 
when we generated the initial list at the previous step. 
For instance, the OCC codes for ??? (shai4), ??? 
(wu4), and ??? (chen2) are ??????, ??????, 
and ??????, respectively. Although the substring 
????? appears three times, it does represent an 
actual part of Chinese characters. Hence, we manually 
examined all of the n-grams in the initial list, and re-
moved such n-grams from the list.  
In addition to considering the frequencies of n-
grams formed by the basic Cangjie codes to determine 
the list of components, we also took advantage of 
radicals that are used to categorize Chinese characters 
in typical printed dictionaries. Radicals that are stand-
alone Chinese words were included in the list of com-
ponents.  
After selecting the list of basic components with 
the above procedure, we encoded the words in Elist 
with these basic components. We inherited the 12 
ways reported in a previous work (Liu and Lin, 2008) 
to decompose Chinese characters. There are other 
methods for decomposing Chinese characters into 
components. Juang et al (2005) and their team at the 
Sinica Academia propose 13 different ways for de-
composing characters. 
At the same time when we annotated individual 
characters with their ECCs, we may revise the list of 
basic components. If a character that actually con-
tained an intuitively ?common? part and that part had 
not been included in the list of basic component, we 
would add this part into the list to make it a basic 
component and revised the ECC for all characters 
accordingly.  The judgment of being ?common? is 
subjective, but we still maintained the rule that such 
common parts must appear in more than three charac-
ters. When defining the basic components, not all 
judgments are completely objectively yet, and this is 
also the case of defining the original Cangjie codes. 
We tried to be as systematic as possible, but intuition 
sometimes stepped in. 
We repeated the procedure described in the preced-
ing paragraph five times to make sure that we were 
satisfied with the ECCs for all of the 5401 characters. 
The current list contains 794 components, and we can 
revise the list of basic components in our work when-
ever necessary. 
5.2 Recommending Incorrect Alternatives 
With the pronunciation of Chinese characters in a 
dictionary and with our ECC encodings for words in 
the Elist, we can create lists of candidate characters 
for replacing a specific correct character in a given 
word to create a test item for incorrect character cor-
rection.  
There are multiple strategies to create the candidate 
lists. We may propose the candidate characters be-
cause their pronunciations have the same sound and 
the same tone with those of the correct character (de-
noted SSST). Characters that have same sounds and 
87
different tones (SSDT), characters that have similar 
sounds and same tones (MSST), and characters that 
have similar sounds and different tones (MSDT) can 
be considered as candidates as well. It is easy to judge 
whether two Chinese characters have the same tone. 
In contrast, it is not trivial to define ?similar? sound. 
We adopted the list of similar sounds that was pro-
vided by a psycholinguistic researcher (Dr. Chia-Ying 
Lee) at the Sinica Academia. ??? (po) and ??? (bo) 
and ???(fan4) and ???(huan4) are pairs that have 
similar sounds. It was observed that these are four 
possible reasons that people used incorrect characters 
in writing. 
Because a Chinese character might be pronounced 
in multiple ways, character lists generated based on 
these strategies may include the same characters. 
More specifically, the lists SSST and SSDT may over-
lap when a character that can be pronounced in multi-
ple ways, and these pronunciations share the same 
sound and have different tones. The characters ??? 
and ??? are such examples. ??? can be pronounced 
as ?dai1? or ?dai4?, and ??? can be pronounced as 
?hao3? or ?hao4?. Hence, characters that can be pro-
nounced as ?hao3? will be listed in both SSST and 
SSDT for ???.  
In addition, we may propose characters that look 
similar to the correct character. Two characters may 
look similar for many reasons (Liu et al, 2009). The 
most common reason is that they contain the same 
components, and the other is that they belong to the 
same radical category and have the same total number 
of strokes (RS), e.g., the pairs ??? and ???, ??? 
and ???, and ??? and ???. When two characters 
contain the same component, the shared component 
might or might not locate at the same position, e.g., 
??? and ???.  
In an authoring tool, we could recommend a se-
lected number of candidate characters for replacing 
the correct character. We tried two different strategies 
to compare and choose the visually similar characters. 
The similarity is computed based on the number and 
the locations of shared Cangjie symbols in the ECCs 
of the characters. The first strategy (denoted SC1) 
gave a higher score to the shared component that lo-
cated at the same location in the two characters being 
compared. The second strategy (SC2) gave the same 
score to any shared component even if the component 
did not reside at the same location in the characters. 
The characters ???, ???, and ??? share the same 
component ???. When computing the similarity be-
tween these characters with SC1, the contribution of 
??? will be the same for any pair. When computing 
with SC2, the contribution of ??? will be larger for 
the pair ??? and ??? than for the pair ??? and ???. 
In the former case, ??? appears at the same location 
in the characters. 
 When there were more than 20 characters that re-
ceive nonzero scores in the SC1 and SC2 categories, 
we chose to select at most 20 characters that had lead-
ing scores as the list of recommended characters. 
We had to set a bound on the number of candidate 
characters, i.e., 20, for strategies SC1 and SC2.  The 
number of candidates generated from these two 
strategies can be large and artificial, depending on our 
scoring functions for determining similarities between 
characters. We did not limit the sizes of candidate 
lists that were generated by other strategies because 
those lists were created based on more objective 
methods. The rules for determining ?similar? sounds 
were given by the domain experts, so we considered 
the rules objective in this research. 
For the experiments that we reported in the follow-
ing subsection, we submitted more than 300 thousand 
of queries to Google. As we mentioned in Section 4.1, 
a frequent continual submission of queries to Google 
will make Google treat our programs as malicious 
processes. (We are studying the Google API for a 
more civilized solution.) Without the bound, it is pos-
sible to offer a very long list of candidates. On the 
other hand, it is also possible that our program does 
not find any visually similar characters for some spe-
cial characters, and this is considered a possible phe-
nomenon.  
5.3 Evaluating the Recommendations 
We examined the usefulness of these seven categories 
of candidates with errors in Elist and Jlist. The first 
set of evaluation (the inclusion tests) checked whether 
the lists of recommended characters contained the 
incorrect character in our records. The second set of 
evaluation (the ranking tests) was designed for practi-
cal application in computer assisted item generation. 
Only for those words whose actual incorrect charac-
ters were included in the recommended list, we re-
placed the correct characters in the words with the 
candidate incorrect characters, submitted the incorrect 
words to Google, and ordered the candidate characters 
based on their NOPs. We then recorded the ranks of 
the incorrect characters among all recommended 
characters.  
Since the same character may appear simultane-
ously in SC1, SC2, and RS, we computed the union of 
these three sets, and checked whether the incorrect 
characters were in the union. The inclusion rate is 
listed under Comp, representing the inclusion rate 
when we consider only logographic influences. Simi-
larly, we computed the union for SSST, SSDT, MSST, 
and MSDT, checked whether the incorrect characters 
were in the union, and recorded the inclusion rate 
under Pron, representing the inclusion rate when we 
consider only phonological influences. Finally, we 
computed the union of the lists created by the seven 
strategies, and recorded the inclusion rate under Both. 
The second and the third rows of Table 3 show the 
results of the inclusion tests when we recommended 
candidate characters with the methods indicated in the 
column headings. The data show the percentage of the 
incorrect characters being included in the lists that 
88
were recommended by the seven strategies. Notice 
that the percentages were calculated with different 
denominators. The number of composition-related 
errors was used for SC1, SC2, RS, and Comp (e.g., 
505 that we mentioned in Section 3 for Jlist); the 
number of pronunciation-related errors for SSST, 
SSDT, MSST, MSDT, and Pron (e.g., 1314 mentioned 
in Section 3 for the Jlist); the number of either of 
these two  types of errors for Both (e.g., 1475 for Jlist).  
The results recorded in Table 3 show that we were 
able to find the incorrect character quite effectively, 
achieving better than 93% for both Elist and Jlist. The 
statistics also show that it is easier to find incorrect 
characters that were used for pronunciation-related 
problems. Most of the pronunciation-related problems 
were misuses of homophones. Unexpected confusions, 
e.g., those related to pronunciations in Chinese dia-
lects, were the main reason for the failure to capture 
the pronunciation-related errors. (Namely, few pro-
nunciation-related errors were not considered in the 
information that the psycholinguist provided.) SSDT 
is a crucial complement to SSST.  
There is still room to improve our methods to find 
confusing characters based on their compositions. We 
inspected the list generated by SC1 and SC2, and 
found that, although SC2 outperformed SC1 on the 
inclusion rate, SC1 and SC2 actually generated com-
plementary lists in many cases, and should be used 
together. The inclusion rate achieved by the RS strat-
egy was surprisingly high. We found that many of the 
errors that were captured by the RS strategy were also 
captured by the SSST strategy. 
The fourth and the fifth rows of Table 3 show the 
effectiveness of relying on Google to rank the candi-
date characters for recommending an incorrect charac-
ter. The rows show the average ranks of the included 
cases. The statistics show that, with the help of 
Google, we were able to put the incorrect character on 
top of the recommended list when the incorrect char-
acter was included.  This allows us to build an envi-
ronment for assisting human teachers to efficiently 
prepare test items for incorrect character identification. 
Note that we did not provide data for all columns 
in the fourth and the firth rows. Unlike that we show 
the inclusion rates in the second and the third rows, 
the fourth and the fifth rows show how the actual in-
correct characters were ranked in the recommended 
lists. Hence, we need to have a policy to order the 
characters of different lists to find the ranks of the 
incorrect characters in the integrated list.  
However, integrating the lists is not necessary and 
can be considered confusing to the teachers. The se-
lection of incorrect characters from different lists is 
related to the goals of the assessment, and it is better 
to leave the lists separated for the teachers to choose. 
The same phenomenon and explanation apply to the 
sixth and the seventh rows as well. 
The sixth and the seventh rows show the average 
numbers of candidate characters proposed by different 
methods. Statistics shown between the second and the 
fifth rows are related to the recall rates (cf. Manning 
and Sch?tz, 1999) achieved by our system. For these 
four rows, we calculated how well the recommended 
lists contained the reported errors and how the actual 
incorrect characters ranked in the recommended lists. 
The sixth and the seventh rows showed the costs for 
these achievements, measured by the number of rec-
ommended characters. The sum of the sixth and the 
seventh rows, i.e., 103.59 and 108.75, are, respec-
tively, the average numbers of candidate characters 
that our system recommended as possible errors re-
corded in Elist and Jlist. (Note that some of these 
characters were repeated.) 
There are two ways to interpret the statistics shown 
in the sixth and the seventh rows. Comparing the cor-
responding numbers on the fourth and the sixth rows, 
e.g., 3.25 and 19.27, show the effectiveness of using 
the NOPs to rank the candidate characters. The ranks 
of the actual errors were placed at very high places, 
considering the number of the originally recom-
mended lists. The other way to use the statistics in the 
sixth and the seventh rows is to compute the average 
precision. For instance, we recommended an average 
19.13 characters in SSST to achieve the 91.64 inclu-
sion rate. The recall rate is very high, but the averaged 
precision is very low. This, however, is not a very 
convincing interpretation of the results. Having as-
sumed that there was only one best candidate as in our 
experiments, it was hard to achieve high precision 
rates. The recall rates are more important than the 
precision rates, particularly when we have proved that 
the actual errors were ranked among the top five al-
ternatives. 
When designing a system for assisting the author-
ing of test items, it is not really necessary to propose 
all of the characters in the categories. In the reported 
experiments, choosing the top 5 or top 10 candidates 
will contain the most of the actual incorrect characters 
based on the statistics shown in the fourth and the 
fifth rows. Hence the precision rates can be signifi-
cantly increased practically. We do not have to merge 
the candidate characters among different categories 
Table 3. Incorrect characters were contained and ranked high in the recommended lists 
 SC1 SC2 RS SSST SSDT MSST MSDT Comp Pron Both 
Elist 73.92% 76.08% 4.08% 91.64% 18.39% 3.01% 1.67% 81.97% 99.00% 93.37% 
Jlist 67.52% 74.65% 6.14% 92.16% 20.24% 4.19% 3.58% 77.62% 99.32% 97.29% 
Elist 3.25 2.91 1.89 2.30 1.85 2.00 1.58 
Jlist 2.82 2.64 2.19 3.72 2.24 2.77 1.16 
Elist 19.27 17.39 11.34 19.13 8.29 19.02 9.15 
Jlist 17.58 16.24 12.52 22.85 9.75 22.11 7.68 
89
because choosing the categories of incorrect charac-
ters depends on the purpose of the assessment. Reduc-
ing the length of the candidate list increases the 
chances of reducing the recall rates. Achieving the 
best trade off between precision and recall rates relies 
on a more complete set of experiments that involve 
human subjects. 
Furthermore, in a more realistic situation, there can 
be more than one ?good? incorrect character, not just 
one and only gold standard as in the reported experi-
ments. It is therefore more reasonable the compute the 
precision rates based the percentage of ?acceptable? 
incorrect characters. Hence, the precision rates are 
likely to increase and become less disconcerting.  
We reported experimental results in which we 
asked 20 human subjects to choose an incorrect char-
acter for 20 test items (Liu et al, 2009). The best so-
lutions were provided by a book. The recommenda-
tions provided by our previous system and chosen by 
the human subjects achieved comparable qualities.  
Notice that the numbers do not directly show the 
actual number of queries that we had to submit to 
Google to receive the NOPs for ranking the characters. 
Because the lists might contain the same characters, 
the sum of the rows showed just the maximum num-
ber of queries that we submitted. Nevertheless, they 
still served as good estimations, and we actually sub-
mitted 103.59?1441(=149273) and 108.75?1583 
(=172151) queries to Google for Elist and Jlist in ex-
periments from which we obtained the data shown in 
the fourth and the fifth rows. These quantities ex-
plained why we had to be cautious about how we 
submitted queries to Google. When we run our pro-
gram for just a limited number of characters, the prob-
lems caused by intensive queries should not be very 
serious. 
5.4 Discussions 
Dividing characters into subareas proved to be crucial 
in our experiments (Liu and Lin, 2008; Liu et al, 
2009), but this strategy is not perfect, and could not 
solve all of the problems. The way we divided Chi-
nese characters into subareas like (Juang et al, 2005; 
Liu and Lin, 2008) sometimes contributed to the fail-
ure of our current implementation to capture all of the 
errors that were related to the composition of the 
words. The most eminent reason is that how we di-
vide characters into areas. Liu and Lin (2008) fol-
lowed the division of Cangjie (Chu, 2009), and Juang 
et al (2005) proposed an addition way to split the 
characters.   
The best divisions of characters appear to depend 
on the purpose of the applications. Recall that each 
part of the character is represented by a string of 
Cangjie codes in ECCs. The separation of Cangjie 
codes in ECCs was instrumental to find the similarity 
of ??? and ??? because ??? is a standalone subpart 
in both ??? and ???. The Cangjie system has a set 
of special rules to divide Chinese characters (Chu, 
2009; Lee, 2008). Take ??? and ??? for example. 
The component ??? is recorded as an standalone part 
in ???, but is divided into two parts in ???. Hence, 
??? is stored as one string, ?????, in ??? and as 
two strings, ???? and ???, in ???. The different 
ways of saving ??? in two different words made it 
harder to find the similarity between ??? and ???. 
An operation of concatenation is in need, but the 
problems are that it is not obvious to tell when the 
concatenation operations are useful and which of the 
parts should be rejoined. Hence, using the current 
methods to divide Chinese characters, it is easy to 
find the similar between ??? and ??? but difficult to 
find the similar between ??? and ???. In contrast, if 
we enforce a rule to save ??? as one string of Cang-
jie code, it will turn the situations around. Determin-
ing the similarity between ??? and ??? will be more 
difficult than finding the similarity between ??? and 
???. 
Due to this observation, we have come to believe 
that it is better to save the Chinese characters with 
more detailed ECCs. By saving all detailed informa-
tion about a character, our system can offer candidate 
characters based on users? preferences which can be 
provided via a good user interface. This flexibility can 
be very helpful when we are preparing text materials 
for experiments for psycholinguistics or cognitive 
sciences (e.g., Leck et al 1995; Yeh and Li, 2002).  
6 Summary  
The analysis of the 1718 errors produced by real stu-
dents show that similarity between pronunciations of 
competing characters contributed most to the ob-
served errors. Evidences show that the Web statistics 
are not very reliable for differentiating correct and 
incorrect characters. In contrast, the Web statistics are 
good for comparing the attractiveness of incorrect 
characters for computer assisted item authoring.  
Acknowledgments 
This research was supported in part by the National 
Science Council of Taiwan under grant NSC-97-
2221-E-004-007-MY2. We thank anonymous review-
ers for their invaluable comments. 
References  
B.-F. Chu. 2009. Handbook of the Fifth Generation of 
the Cangjie Input Method, available at 
http://www.cbflabs.com/book/ocj5/ocj5/index.html. 
Last visited on 30 April 2009. 
D. Juang, J.-H. Wang, C.-Y. Lai, C.-C. Hsieh, L.-F. 
Chien, J.-M. Ho. 2005. Resolving the unencoded 
character problem for Chinese digital libraries, 
Proc. of the 5th ACM/IEEE Joint Conf. on Digital 
Libraries, 311?319. 
S.-P. Law, W. Wong, K. M. Y. Chiu. 2005. Whole-
word phonological representations of disyllabic 
90
words in the Chinese lexicon: Data from acquired 
dyslexia, Behavioural Neurology, 16, 169?177. 
K. J. Leck, B. S. Weekes, M. J. Chen. 1995. Visual 
and phonological pathways to the lexicon: Evi-
dence from Chinese readers, Memory & Cognition, 
23(4), 468?476. 
H. Lee. 2008. Cangjie Input Methods in 30 Days, 
http://input.foruto.com/cjdict/Search_1.php, Foruto 
Company, Hong Kong. Last visited on 30 April 
2009. 
C.-L. Liu, K.-W. Tien, Y.-H. Chuang, C.-B. Huang, 
J.-Y. Weng. 2009. Two applications of lexical in-
formation to computer-assisted item authoring for 
elementary Chinese, Proc. of the 22nd Int?l Conf. 
on Industrial Engineering & Other Applications of 
Applied Intelligent Systems, 470?480. 
C.-L. Liu, J.-H. Lin. 2008. Using structural informa-
tion for identifying similar Chinese characters, 
Proc. of the 46th ACL, short papers, 93?96. 
C. D. Manning, H. Sch?tze. Foundations of Statistical 
Natural Language Processing. The MIT Press. 
1999. 
MOE. 1996. Common Errors in Chinese Writings (?
?????), Ministry of Education, Taiwan. 
S.-L. Yeh, J.-L. Li. 2002. Role of structure and com-
ponent in judgments of visual similarity of Chinese 
characters, Journal of Experimental Psychology: 
Human Perception and Performance, 28(4), 933?
947.
 
91
