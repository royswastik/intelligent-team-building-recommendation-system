Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 28?31,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
Using the Wiktionary Graph Structure for Synonym Detection
Timothy Weale, Chris Brew, Eric Fosler-Lussier
Department of Computer Science and Engineering
The Ohio State University
{weale,cbrew,fosler}@cse.ohio-state.edu
Abstract
This paper presents our work on using
the graph structure of Wiktionary for syn-
onym detection. We implement seman-
tic relatedness metrics using both a direct
measure of information flow on the graph
and a comparison of the list of vertices
found to be ?close? to a given vertex. Our
algorithms, evaluated on ESL 50, TOEFL
80 and RDWP 300 data sets, perform bet-
ter than or comparable to existing seman-
tic relatedness measures.
1 Introduction
The recent creation of large-scale, collabora-
tively constructed semantic resources provides re-
searchers with cheap, easily accessible informa-
tion. Previous metrics used for synonym detec-
tion had to be built using co-occurrence statistics
of collected corpora (Higgins, 2004) or expensive,
expert-created resources such as WordNet or Ro-
get?s Thesaurus (Jarmasz and Szpakowicz, 2003).
Here, we evaluate the effectiveness of Wiktionary,
a collaboratively constructed resource, as a source
of semantic relatedness information for the syn-
onym detection problem.
Researching these metrics is important because
they have been empirically shown to improve per-
formance in a variety of NLP applications, includ-
ing word sense disambiguation (Turdakov and Ve-
likhov, 2008), real-world spelling errors (Budan-
itsky and Hirst, 2006) and coreference resolution
(Strube and Ponzetto, 2006).
Synonym detection is a recognized testbed
for comparing semantic relatedness metrics (e.g
(Zesch et al, 2008)). In this task, a target word
or phrase is presented to the system, which is then
presented with four alternative words or phrases.
The goal of the system is to pick the alternative
most related to the target. Example questions can
be found in Figure 1.
Through the Wikimedia Foundation,
1
volun-
teers have created two large-scale, collaborative
resources that have been used in previous related-
ness research ? Wikipedia (an encyclopedia) and
Wiktionary (a dictionary). These sources have
been used for synonym detection and replicating
human relatedness evaluations using the category
structure (Strube and Ponzetto, 2006), local link
structure (Milne and Witten, 2008) and (Turdakov
and Velikhov, 2008) and global features (Zesch et
al., 2008). They contain related information but
focus on different information needs; which infor-
mation source provides better results depends on
the needs of the task. We use Wiktionary which,
due to its role as a dictionary, focuses on common
words and definitions ? the type of information
found in our synonym detection problems.
Both Wikipedia and Wiktionary are organized
around a basic ?page? unit, containing informa-
tion about an individual word, phrase or entity
in the world ? definitions, thesaurus entries, pro-
nunciation guides and translations in Wiktionary
and general biographical, organizational or philo-
sophical information in Wikipedia. In both data
sets, pages are linked to each each other and to
a user-created category structure ? a graph struc-
ture where pages are vertices of the graph and page
links are the graph edges. We will leverage this
graph for determining relatedness.
1
http://www.wikimedia.org/
Source Word Alternative Words
make earn, print, trade, borrow
flawed imperfect, tiny, lustrous, crude
solitary alone, alert, restless, fearless
Figure 1: Example TOEFL Questions
28
2 Extracting Relatedness Measures
We define relatedness based on information flow
through the entire Wiktionary graph, rather than
by any local in-bound or out-bound link structure.
This provides a global measurement of vertex im-
portance, as we do not limit the approach to com-
paring immediate neighbors.
To do this, we first run the PageRank algorithm
(Brin and Page, 1998) iteratively over the graph
until convergence to measure the a priori impor-
tance of each vertex in graph:
~
PR
t+1
= ??
(
~
PR
t
? E
)
+ (1 ? ?) ?
~
J (1)
In this, E contains the edge transition probabilities,
set to a uniform out-bound probability.
~
PR holds
the PageRank value for each vertex and
~
J is uni-
form vector used to randomly transition between
vertices. Traditionally, ? = 0.85 and is used to
tradeoff between a strict transition model and the
random-walk model.
We then adopt the extensions proposed in (Ol-
livier and Senellart, 2007) (OS) to determine re-
latedness given a source vertex:
~
R
t+1
= ??
(
~
R
t
? E + (
~
S ?
~
PR)
)
+(1??)?
~
J
(2)
~
S is a vector that contains zeros except for a one
at our source vertex, and
~
PR removes an overall
value of 1 based on the a priori PageRank value of
the vertex. In this way, vertices close to the source
are rewarded with weight and vertices that have a
high a priori importance are penalized. When
~
R
converges, it contains measures of importance for
vertices based on the source vertex.
Final relatedness values are then calculated
from the vector generated by Equation 2 and the
a priori importance of the vector based on the
PageRank from Equation 1:
rel
OS
(w, a) =
~
R
w
[a] ? log
(
1
PR[a]
)
(3)
w is the vertex for the source word and a is the
alternative word vertex. ThePR[a] penalty is used
to further ensure that our alternative vertex is not
highly valued simply because it is well-connected.
Applying Equation 3 provides comparable se-
mantic relatedness performance (see Tables 1 and
2). However, cases exist where a single data value
is insufficient to make an adequate determination
of word relatedness because of small differences
for candidate words. We can incorporate addi-
tional relatedness information about our vertices
by leveraging information about the set of vertices
deemed ?most related? to our current vertex.
2.1 Integrating N-Best Neighbors
We add information by looking at the similarity
between the n-best related words for each vertex.
Intuitively, given a source word w and candidate
alternatives a
1
and a
2
,
2
we look at the set of words
that are semantically related to each of the can-
didates (represented as vectors W , A
1
and A
2
).
If the overlap between elements of W and A
1
is
greater thanW andA
2
, A
1
is more likely to be the
synonym of W .
Highly-ranked shared elements are good indi-
cators of relatedness and should contribute more
than low-ranked related words. Lists with many
low-ranked words could be an artifact of the data
set and should not be ranked higher than ones con-
taining a few high-ranked words.
Our ranked-list comparison metric (NB) is a se-
lective mean reciprocal ranking function:
rel
NB
(
~
W,
~
A, n) =
n
?
r=1
1
r
? ?(W
r
?
~
A) (4)
~
W is the n-best list based on the source vertex
and
~
A is the n-best list based on the alternative
vertex. Values are added to our relatedness metric
based on the position of a vertex in the target list
and the traditional Dirac ?-function, which has a
value of one if the target vertex appears anywhere
in our candidate list and a zero in all other cases.
Each metric (OS and NB) will have different
ranges. We therefore normalize the reported value
by scaling each based on the maximum value for
that portion in order to achieve a uniform scale.
Our final metric (OS+NB) is created by aver-
aging the two normalized scores. In this work,
both scores are given equal weighting. Deriving
weightings for combining the two scores will be
part of our future work.
rel
OS+NB
(w
i
,
j
) =
OS(c
i
, c
j
) + NB(c
i
, c
j
, n)
2
(5)
In this, OS() returns the normalized rel
OS
()
value and NB() returns the normalized rel
NB
value. The maximum rel
P+N
() value of 1.0 is
achieved if c
j
has the highest PageRank-based
value and the highest N-Best value.
2
See Figure 1
29
Source
ESL TOEFL
Acc. (%) Acc. (%)
JPL 82 78.8
LC-IR 78 81.3
OS 86 88.8
NB 80 88.8
OS+NB 88 93.8
Table 1: ESL and TOEFL Performance
3 Evaluation
We present performance results on three data sets.
The first, ESL, uses 50 questions from the English
as a Second Language test (Turney, 2001). Next,
an 80 question data set from the Test of English
as a Foreign Language (TOEFL) is used (Lan-
dauer and Dumais, 1997). Finally, we evaluate
on the Reader?s Digest WordPower (RDWP) data
set (Jarmasz and Szpakowicz, 2003). This is a set
of 300 synonym detection problems gathered from
the Word Power game of the Canadian edition of
Reader?s Digest Word from 2000 ? 2001.
We use the Feb. 03, 2009 version of the English
Wiktionary data set
3
for extracting graph structure
and relatedness information.
Table 1 presents the performance of our algo-
rithm on the ESL and TOEFL test sets. Our results
are compared to Jarmasz and Szpakowicz (2003),
which uses a path-based cost on the structure
of Roget?s Thesaurus (JPL) and a cooccurence-
based metric, LC-IR (Higgins, 2004), which con-
strained context to only consider adjacent words in
structured web queries.
Information about our algorithm?s performance
on the RDWP test set is found in Table 2. Our re-
sults are compared to the previously mentioned al-
gorithms and also the work of Zesch et al (2008).
Their first metric (ZPL) uses the path length be-
tween two graph vertices for relatedness determi-
nation. The second, (ZCV), creates concept vec-
tors based on a distribution of pages that contain a
particular word.
RDWP is not only larger then the previous two,
but also more complictated. TOEFL and ESL
average 1.0 and 1.008 number of words in each
source and alternative, respectively. For RDWP
each entry averages 1.4 words.
We map words and phrases to graph vertices by
first matching against the page title. If there is no
3
http://download.wikimedia.org
match, we follow the approach outlined in (Zesch
et al, 2008). Common words are removed from
the phrase
4
and for every remaining word in the
phrase, we determine the page mapping for that
individual word. The relatedness of the phrase
is then set to be the maximum relatedness value
attributed to any of the individual words in the
phrase.
Random guessing by an algorithm could in-
crease algorithm performance through random
chance. Therefore, we present both a overall
percentage and also a precision-based percentage.
The first (Raw) is defined as the correct number of
guesses over all questions. The second (Prec) is
defined as the correct number of guesses divided
by only those questions that were attempted.
3.1 Discussion
For NB and OS+NB, we set n = 3000 based on
TOEFL data set training.
5
Testing was then per-
formed on the ESL and RDWP data set.
As shown in Table 1, the OS algorithm per-
forms better on the task than the comparison sys-
tems. On its own, NB relatedness performs well ?
at or slightly worse than OS. Combining the two
measures increases performance on both data sets.
While our TOEFL results are below the reported
performance of (Turney et al, 2003) (97.5%), we
do not use any task-dependent learning for our re-
sults and our algorithms have better performance
than any individual module in their system.
Combining OS with NB mitigates the influence
of OS when it is not confident. OS correctly picks
?pinnacle? as a synonym of ?zenith? with a relat-
edness value 126,000 times larger than its next
competitor. For ?consumed?, OS is wrong, giving
?bred? a higher score than ?eaten? ? but only by
a value 1.2 times that of ?eaten?. The latter case
is overcome by the addition of n-best information
while the former is unaffected.
Table 2 demonstrates that we have results com-
parable to existing state-of-the-art measures. Our
choice of n resulted in reduced scores on this task
when compared to using the OS metric by itself.
But, our algorithm still outperforms both the ZPL
and ZCV metrics for our data set in raw scores and
in three out of the four precision measures. Fur-
ther refinement of the RDWP data set mapping or
changing our metric score to a weighted sum of
4
Defined here as: {and, or, to, be, the, a, an, of, on, in, for,
with, by, into, is, no}
5
Out of 1.1 million vertices
30
Metric Source Attempted Score # Ties Raw Prec
JPL Roget?s 300 223 0 .74 .74
LC-IR Web 300 224.33 - .75 .75
ZPL
Wikipedia
226 88.33 96 .29 .39
ZCV 288 165.83 2 .55 .58
ZPL
Wiktionary
201 103.7 55 .35 .52
ZCV 174 147.3 3 .49 .85
OS
Wiktionary
300 234 0 .78 .78
NB 300 212 0 .71 .71
OS+NB 300 227 0 .76 .76
Table 2: Reader?s Digest WordPower 300 Overall Performance
sorts (rather than a raw maximum) could result in
increased performance.
Wiktionary?s coverage enables all words in the
first two tasks to be found (with the exception of
?bipartisanly?). Enough of the words in the RDWP
task are found to enable the algorithm to attempt
all synonym detection questions.
4 Conclusion and Future Work
In this paper, we have demonstrated the effective-
ness of Wiktionary as a source of relatedness in-
formation when coupled with metrics based on
information flow using synonym detection as our
evaluation testbed.
Our immediate work will be in learning weights
for the combination measure, using (Turney et al,
2003) as our guideline. Additional work will be in
automatically determining an effective value for n
across all data sets.
Long-term work will be in modifying the page
transition values to achieve non-uniform transition
values. Links are of differing quality, and the tran-
sition probabilities should reflect that.
References
Sergey Brin and Lawrence Page. 1998. The Anatomy
of a Large-Scale Hypertextual Web Search En-
gine. Computer Networks and ISDN Systems, 30(1?
7):107?117.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based Measures of Lexical Se-
mantic Relatedness. Computational Linguistics,
32(1):13?47.
Derrick Higgins. 2004. Which Statistics Reflect Se-
mantics? Rethinking Synonymy and Word Similar-
ity. In Proceedings of the International Conference
on Linguistic Evidence.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
Thesaurus and Semantic Similarity. In Proceedings
of Conference on Recent Advances in Natural Lan-
guage Processing (RANLP 2003).
Thomas K. Landauer and Susan T. Dumais. 1997. A
Solution to Plato?s Problem: The Latent Semantic
Analysis Theory of Acquisition, Induction, and Rep-
resentation of Knowledge. Psychological Review.
David Milne and Ian H. Witten. 2008. An Effec-
tive, Low-Cost Measure of Semantic Relatedness
Obtained from Wikipedia Links. In Proceedings of
AAAI 2008.
Yann Ollivier and Pierre Senellart. 2007. Finding Re-
lated Pages Using Green Measures: An Illustration
with Wikipedia. In Proceedings of AAAI 2007.
Michael Strube and Simone Paolo Ponzetto. 2006.
WikiRelate! Computing Semantic Relatedness Us-
ing Wikipedia. In AAAI.
Denis Turdakov and Pavel Velikhov. 2008. Semantic
relatedness metric for Wikipedia concepts based on
link analysis and its application to word sense dis-
ambiguation. In Proceedings of CEUR.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining Indepen-
dent Modules in Lexical Multiple-Choice Problems.
In Recent Advances in Natural Language Processing
III: Selected Papers from RANLP 2003.
Peter D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning (ECML-2001), pages 491?502, Freidburg,
Germany.
Torsten Zesch, Christof Muller, and Iryna Gurevych.
2008. Using Wiktionary for Computing Semantic
Relatedness. In Proceedings of AAAI 2008.
31
