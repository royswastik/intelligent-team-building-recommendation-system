Proceedings of the Workshop on BioNLP: Shared Task, pages 50?58,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
High-precision biological event extraction with a concept recognizer
K. Bretonnel Cohen?, Karin Verspoor?, Helen L. Johnson, Chris Roeder,
Philip V. Ogren, William A. Baumgartner Jr., Elizabeth White, Hannah Tipney, and Lawrence Hunter
Center for Computational Pharmacology
University of Colorado Denver School of Medicine
PO Box 6511, MS 8303, Aurora, CO 80045 USA
kevin.cohen@gmail.com, karin.verspoor@ucdenver.edu, helen.linguist@gmail.com,
chris.roeder@ucdenver.edu, philip@ogren.info, william.baumgartner@ucdenver.edu,
elizabeth.white@colorado.edu, hannah.tipney@ucdenver.edu, larry.hunter@ucdenver.edu
Abstract
We approached the problems of event detec-
tion, argument identification, and negation and
speculation detection as one of concept recog-
nition and analysis. Our methodology in-
volved using the OpenDMAP semantic parser
with manually-written rules. We achieved
state-of-the-art precision for two of the three
tasks, scoring the highest of 24 teams at pre-
cision of 71.81 on Task 1 and the highest of 6
teams at precision of 70.97 on Task 2.
The OpenDMAP system and the rule set are
available at bionlp.sourceforge.net.
*These two authors contributed equally to the
paper.
1 Introduction
We approached the problem of biomedical event
recognition as one of concept recognition and anal-
ysis. Concept analysis is the process of taking a
textual input and building from it an abstract rep-
resentation of the concepts that are reflected in it.
Concept recognition can be equivalent to the named
entity recognition task when it is limited to locat-
ing mentions of particular semantic types in text, or
it can be more abstract when it is focused on recog-
nizing predicative relationships, e.g. events and their
participants.
2 BioNLP?09 Shared Task
Our system was entered into all three of the
BioNLP?09 (Kim et al, 2009) shared tasks:
? Event detection and characterization This
task requires recognition of 9 basic biological
events: gene expression, transcription, protein
catabolism, protein localization, binding, phos-
phorylation, regulation, positive regulation and
negative regulation. It requires identification
of the core THEME and/or CAUSE participants
in the event, i.e. the protein(s) being produced,
broken down, bound, regulated, etc.
? Event argument recognition This task builds
on the previous task, adding in additional argu-
ments of the events, such as the site (protein or
DNA region) of a binding event, or the location
of a protein in a localization event.
? Recognition of negations and speculations
This task requires identification of negations of
events (e.g. event X did not occur), and specu-
lation about events (e.g. We claim that event X
should occur).
3 Our approach
We used the OpenDMAP system developed at the
University of Colorado School of Medicine (Hunter
et al, 2008) for our submission to the BioNLP
?09 Shared Task on Event Extraction. OpenDMAP
is an ontology-driven, integrated concept analysis
system that supports information extraction from
text through the use of patterns represented in a
classic form of ?semantic grammar,? freely mixing
text literals, semantically typed basal syntactic con-
stituents, and semantically defined classes of enti-
ties. Our approach is to take advantage of the high
50
quality ontologies available in the biomedical do-
main to formally define entities, events, and con-
straints on slots within events and to develop pat-
terns for how concepts can be expressed in text that
take advantage of both semantic and linguistic char-
acteristics of the text. We manually built patterns for
each event type by examining the training data and
by using native speaker intuitions about likely ways
of expressing relationships, similar to the technique
described in (Cohen et al, 2004). The patterns char-
acterize the linguistic expression of that event and
identify the arguments (participants) of the events
according to (a) occurrence in a relevant linguistic
context and (b) satisfaction of appropriate semantic
constraints, as defined by our ontology. Our solution
results in very high precision information extraction,
although the current rule set has limited recall.
3.1 The reference ontology
The central organizing structure of an OpenDMAP
project is an ontology. We built the ontology
for this project by combining elements of several
community-consensus ontologies?the Gene Ontol-
ogy (GO), Cell Type Ontology (CTO), BRENDA
Tissue Ontology (BTO), Foundational Model of
Anatomy (FMA), Cell Cycle Ontology (CCO), and
Sequence Ontology (SO)?and a small number of
additional concepts to represent task-specific aspects
of the system, such as event trigger words. Combin-
ing the ontologies was done with the Prompt plug-in
for Prote?ge?.
The ontology included concepts representing each
event type. These were represented as frames, with
slots for the various things that needed to be re-
turned by the system?the trigger word and the var-
ious slot fillers. All slot fillers were constrained to
be concepts in some community-consensus ontol-
ogy. The core event arguments were constrained in
the ontology to be of type protein from the Sequence
Ontology (except in the case of regulation events,
where biological events themselves could satisfy the
THEME role), while the type of the other event argu-
ments varied. For instance, the ATLOC argument
of a gene expression event was constrained to be
one of tissue (from BTO), cell type (from CTO), or
cellular component (from GO-Cellular Component),
while the BINDING argument of a binding event was
constrained to be one of binding site, DNA, domain,
or chromosome (all from the SO and all tagged by
LingPipe). Table 1 lists the various types.
3.2 Named entity recognition
For proteins, we used the gold standard annota-
tions provided by the organizers. For other seman-
tic classes, we constructed a compound named en-
tity recognition system which consists of a LingPipe
GENIA tagging module (LingPipe, (Alias-i, 2008)),
and several dictionary look-up modules. The dictio-
nary lookup was done using a component from the
UIMA (IBM, 2009; Ferrucci and Lally, 2004) sand-
box called the ConceptMapper.
We loaded the ConceptMapper with dictionar-
ies derived from several ontologies, including the
Gene Ontology Cellular Component branch, Cell
Type Ontology, BRENDA Tissue Ontology, and
the Sequence Ontology. The dictionaries contained
the names and name variants for each concept in
each ontology, and matches in the input documents
were annotated with the relevant concept ID for the
match. The only modifications that we made to
these community-consensus ontologies were to re-
move the single concept cell from the Cell Type On-
tology and to add the synonym nuclear to the Gene
Ontology Cell Component concept nucleus.
The protein annotations were used to constrain the
text entities that could satisfy the THEME role in the
events of interest. The other named entities were
added for the identification of non-core event partic-
ipants for Task 2.
3.3 Pattern development strategies
3.3.1 Corpus analysis
Using a tool that we developed for visualizing the
training data (described below), a subset of the gold-
standard annotations were grouped by event type
and by trigger word type (nominalization, passive
verb, active verb, or multiword phrase). This orga-
nization helped to suggest the argument structures of
the event predicates and also highlighted the varia-
tion within argument structures. It also showed the
nature of more extensive intervening text that would
need to be handled for the patterns to achieve higher
recall.
Based on this corpus analysis, patterns were de-
veloped manually using an iterative process in which
individual patterns or groups of patterns were tested
51
Table 1: Semantic restrictions on Task 2 event arguments. CCO = Cell Cycle Ontology, FMA = Foundational Model
of Anatomy, other ontologies identified in the text.
Event Type Site AtLoc ToLoc
binding protein domain (SO),
binding site (SO), DNA
(SO), chromosome (SO)
gene expression gene (SO), biological
entity (CCO)
tissue (BTO), cell type
(CTO), cellular compo-
nent (GO)
localization cellular component
(GO)
cellular component
(GO)
phosphorylation amino acid (FMA),
polypeptide region (SO)
protein catabolism cellular component
(GO)
transcription gene (SO), biological
entity (CCO)
on the training data to determine their impact on per-
formance. Pattern writers started with the most fre-
quent trigger words and argument structures.
3.3.2 Trigger words
In the training data, we were provided annotations
of all relevant event types occurring in the training
documents. These annotations included a trigger
word specifying the specific word in the input text
which indicated the occurrence of each event. We
utilized the trigger words in the training set as an-
chors for our linguistic patterns. We built patterns
around the generic concept of, e.g. an expression
trigger word and then varied the actual strings that
were allowed to satisfy that concept. We then ran ex-
periments with our patterns and these varying sets of
trigger words for each event type, discarding those
that degraded system performance when evaluated
with respect to the gold standard annotations.
Most often a trigger word was removed from an
event type trigger list because it was also a trig-
ger word for another event type and therefore re-
duced performance by increasing the false positive
rate. For example, the trigger words ?level? and
?levels? appear in the training data trigger word lists
of gene expression, transcription, and all three regu-
lation event types.
The selection of trigger words was guided by a
frequency analysis of the trigger words provided in
the task training data. In a post-hoc analysis, we find
that a different proportion of the set of trigger words
was finally chosen for each different event type. Be-
tween 10-20% of the top frequency-ranked trigger
words were used for simple event types, with the
exception that phosphorylation trigger words were
chosen from the top 30%. For instance, for gene ex-
pression all of the top 15 most frequent trigger words
were used (corresponding to the top 16%). For com-
plex event types (the regulations) better performance
was achieved by limiting the list to between 5-10%
of the most frequent trigger words.
In addition, variants of frequent trigger words
were included. For instance, the nominalization ?ex-
pression? is the most frequent gene expression trig-
ger word and the verbal inflections ?expressed? and
?express? are also in the top 20%. The verbal inflec-
tion ?expresses? is ranked lower than the top 30%,
but was nonetheless included as a trigger word in the
gene expression patterns.
3.3.3 Patterns
As in our previous publications on OpenDMAP,
we refer to our semantic rules as patterns. For
this task, each pattern has at a minimum an event
argument THEME and an event-specific trigger
word. For example, {phosphorylation} :=
52
[phosphorylation nominalization][Theme],
where [phosphorylization nominalization]
represents a trigger word. Both elements are defined
semantically. Event THEMEs are constrained by
restrictions placed on them in the ontology, as
described above.
The methodology for creating complex event pat-
terns such as regulation was the same as for sim-
ple events, with the exception that the THEMEs
were defined in the ontology to also include bio-
logical processes. Iterative pattern writing and test-
ing was a little more arduous because these pat-
terns relied on the success of the simple event pat-
terns, and hence more in-depth analysis was re-
quired to perform performance-increasing pattern
adjustments. For further details on the pattern lan-
guage, the reader is referred to (Hunter et al, 2008).
3.3.4 Nominalizations
Nominalizations were very frequent in the train-
ing data; for seven out of nine event types, the most
common trigger word was a nominalization. In writ-
ing our grammars, we focused on these nominaliza-
tions. To write grammars for nominalizations, we
capitalized on some of the insights from (Cohen et
al., 2008). Non-ellided (or otherwise absent) argu-
ments of nominalizations can occur in three basic
positions:
? Within the noun phrase, after the nominaliza-
tion, typically in a prepositional phrase
? Within the noun phrase, immediately preceding
the nominalization
? External to the noun phrase
The first of these is the most straightforward to
handle in a rule-based approach. This is particu-
larly true in the case of a task definition like that
of BioNLP ?09, which focused on themes, since an
examination of the training data showed that when
themes were post-nominal in a prepositional phrase,
then that phrase was most commonly headed by of.
The second of these is somewhat more challeng-
ing. This is because both agents and themes can
occur immediately before the nominalization, e.g.
phenobarbital induction (induction by phenobarbi-
tal) and trkA expression (expression of trkA). To de-
cide how to handle pre-nominal arguments, we made
use of the data on semantic roles and syntactic posi-
tion found in (Cohen et al, 2008). That study found
that themes outnumbered agents in the prenominal
position by a ratio of 2.5 to 1. Based on this obser-
vation, we assigned pre-nominal arguments to the
theme role.
Noun-phrase-external arguments are the most
challenging, both for automatic processing and for
human interpreters; one of the major problems is
to differentiate between situations where they are
present but outside of the noun phrase, and situations
where they are entirely absent. Since the current im-
plementation of OpenDMAP does not have robust
access to syntactic structure, our only recourse for
handling these arguments was through wildcards,
and since they mostly decreased precision without a
corresponding increase in recall, we did not attempt
to capture them.
3.3.5 Negation and speculation
Corpus analysis of the training set revealed two
broad categories each for negation and speculation
modifications, all of which can be described in terms
of the scope of modification.
Negation
Broadly speaking, an event itself can be negated
or some aspect of an event can be negated. In other
words, the scope of a negation modification can be
over the existence of an event (first example below),
or over an argument of an existing event (second ex-
ample).
? This failure to degrade IkappaBalpha ...
(PMID 10087185)
? AP-1 but not NF-IL-6 DNA binding activity ...
(PMID 10233875)
Patterns were written to handle both types of
negation. The negation phrases ?but not? and ?but
neither? were appended to event patterns to catch
those events that were negated as a result of a
negated argument. For event negation, a more ex-
tensive list of trigger words was used that included
verbal phrases such as ?failure to? and ?absence of.?
The search for negated events was conducted in
two passes. Events for which negation cues fall out-
side the span of text that stretches from argument to
53
event trigger word were handled concurrently with
the search for events. A second search was con-
ducted on extracted events for negation cues that fell
within the argument to event trigger word span, such
as
. . . IL-2 does not induce I kappa B alpha degrada-
tion (PMID 10092783)
This second pass allowed us to capture one addi-
tional negation (6 rather than 5) on the test data.
Speculation
The two types of speculation in the training data
can be described by the distinction between ?de re?
and ?de dicto? assertions. The ?de dicto? assertions
of speculation in the training data are modifications
that call into question the degree of known truth of
an event, as in
. . . CTLA-4 ligation did not appear to affect the
CD28 - mediated stabilization (PMID 10029815)
The ?de re? speculation address the potential ex-
istence of an event rather that its degree of truth. In
these cases, the event is often being introduced in
text by a statement of intention to study the event, as
in
. . . we investigated CTCF expression
. . . [10037138]
To address these distinct types of speculation, two
sets of trigger words were developed. One set con-
sisted largely of verbs denoting research activities,
e.g. research, study, examine investigate, etc. The
other set consisted of verbs and adverbs that denote
uncertainty, and included trigger words such as sug-
gests, unknown, and seems.
3.4 Handling of coordination
Coordination was handled using the OpenNLP con-
stituent parser along with the UIMA wrappers that
they provide via their code repository. We chose
OpenNLP because it is easy to train a model, it in-
tegrates easily into a UIMA pipeline, and because
of competitive parsing results as reported by Buyko
(Buyko et al, 2006). The parser was trained using
500 abstracts from the beta version of the GENIA
treebank and 10 full-text articles from the CRAFT
corpus (Verspoor et al, In press). From the con-
stituent parse we extracted coordination structures
into a simplified data structure that captures each
conjunction along with its conjuncts. These were
provided to downstream components. The coordi-
nation component achieves an F-score of 74.6% at
the token level and an F-score of 57.5% at the con-
junct level when evaluated against GENIA. For both
measures the recall was higher than the precision by
4% and 8%, respectively.
We utilized the coordination analysis to identify
events in which the THEME argument was expressed
as a conjoined noun phrase. These were assumed to
have a distributed reading and were post-processed
to create an individual event involving each con-
junct, and further filtered to only include given (A1)
protein references. So, for instance, analysis of the
sentence in the example below should result in the
detection of three separate gene expression events,
involving the proteins HLA-DR, CD86, and CD40,
respectively.
NAC was shown to down-regulate the
production of cytokines by DC as well
as their surface expression of HLA-
DR, CD86 (B7-2), and CD40 molecules
. . . (PMID 10072497)
3.5 Software infrastructure
We took advantage of our existing infrastructure
based on UIMA (The Unstructured Information
Management Architecture) (IBM, 2009; Ferrucci
and Lally, 2004) to support text processing and data
analysis.
3.5.1 Development tools
We developed a visualization tool to enable the
linguistic pattern writers to better analyze the train-
ing data. This tool shows the source text one sen-
tence at a time with the annotated words highlighted.
A list following each sentence shows details of the
annotations.
3.6 Errors in the training data
In some cases, there were discrepancies between the
training data and the official problem definitions.
This was a source of problems in the pattern devel-
opment phase. For example, phosphorylation events
are defined in the task definition as having only a
THEME and a SITE. However, there were instances
in the training data that included both a THEME and
a CAUSE argument. When those events were identi-
fied by our system and the CAUSE was labelled, they
54
were rejected during a syntactic error check by the
test server.
4 Results
4.1 Official Results
We are listed as Team 13. Table 2 shows our re-
sults on the official metrics. Our precision was the
highest achieved by any group for Task 1 and Task
2, at 71.81 for Task 1 and 70.97 for task 2. Our re-
calls were much lower and adversely impacted our
F-measure; ranked by F-measure, we ranked 19th
out of 24 groups.
We noted that our results for the exact match met-
ric and for the approximate match metric were very
close, suggesting that our techniques for named en-
tity recognition and for recognizing trigger words
are doing a good job of capturing the appropriate
spans.
4.2 Other analysis: Bug fixes and coordination
handling
In addition to our official results, we also report in
Table 3 (see last page) the results of a run in which
we fixed a number of bugs. This represents our cur-
rent best estimate of our performance. The precision
drops from 71.81 for Task 1 to 67.19, and from 70.97
for Task 2 to 65.74, but these precisions are still
well above the second-highest precisions of 62.21
for Task 1 and 56.87 for Task 2. As the table shows,
we had corresponding small increases in our recall
to 17.38 and in our F-measure to 27.62 for Task 1,
and in our recall to 17.07 and F-measure to 27.10 for
Task 2.
We evaluated the effects of coordination handling
by doing separate runs with and without this ele-
ment of the processing pipeline. Compared to our
unofficial results, which had an overall F-measure
for Task 1 of 27.62 and for Task 2 of 27.10, a ver-
sion of the system without handling of coordination
had an overall F-measure for Task 1 of 24.72 and for
Task 2 of 24.21.
4.3 Error Analysis
4.3.1 False negatives
To better understand the causes of our low recall,
we performed a detailed error analysis of false neg-
atives using the devtest data. (Note that this section
includes a very small number of examples from the
devtest data.) We found five major causes of false
negatives:
? Intervening material between trigger words and
arguments
? Coordination that was not handled by our coor-
dination component
? Low coverage of trigger words
? Anaphora and coreference
? Appositive gene names and symbols
Intervening material For reasons that we detail
in the Discussion section, we avoided the use of
wildcards. This, and the lack of syntactic analy-
sis in the version of the system that we used (note
that syntactic analyses can be incorporated into an
OpenDMAP workflow), meant that if there was text
intervening between a trigger word and an argument,
e.g. in to efficiently [express] in developing thymo-
cytes a mutant form of the [NF-kappa B inhibitor]
(PMID 10092801), where the bracketed text is the
trigger word and the argument, our pattern would
not match.
Unhandled coordination Our coordination system
only handled coordinated protein names. Thus, in
cases where other important elements of the utter-
ance, such as the trigger word transcription in tran-
scription and subsequent synthesis and secretion
of galectin-3 (PMID 8623933) were in coordinated
structures, we missed the relevant event arguments.
Low coverage of trigger words As we discuss in
the Methods section, we did not attempt to cover
all trigger words, in part because some less-frequent
trigger words were involved in multiple event types,
in part because some of them were extremely low-
frequency and we did not want to overfit to the train-
ing data, and in part due to the time constraints of the
shared task.
Anaphora and coreference Recognition of some
events in the data would require the ability to do
anaphora and coreference resolution. For example,
in Although 2 early lytic transcripts, [BZLF1] and
[BHRF1], were also detected in 13 and 10 cases,
respectively, the lack of ZEBRA staining in any case
indicates that these lytic transcripts are most likely
55
Tasks 1 and 3 Task 2
Event class GS answer R P F R P F
Localization 174 (18) 18 (18) 10.34 100.00 18.75 9.77 94.44 17.71
Binding 347 (44) 110 (44) 12.68 40.00 19.26 12.32 39.09 18.74
Gene expression 722 (263) 306 (263) 36.43 85.95 51.17 36.43 85.95 51.17
Transcription 137 (18) 20 (18) 13.14 90.00 22.93 13.14 90.00 22.93
Protein catabolism 14 (4) 6 (4) 28.57 66.67 40.00 28.57 66.67 40.00
Phosphorylation 135 (30) 30 (30) 22.22 100.00 36.36 20.14 93.33 33.14
EVENT TOTAL 1529 (377) 490 (377) 24.66 76.94 37.35 24.30 76.12 36.84
Regulation 291 (9) 19 (9) 3.09 47.37 5.81 3.08 47.37 5.79
Positive regulation 983 (32) 65 (32) 3.26 49.23 6.11 3.24 49.23 6.08
Negative regulation 379 (10) 22 (10) 2.64 45.45 4.99 2.37 40.91 4.49
REGULATION TOTAL 1653 (51) 106 (51) 3.09 48.11 5.80 3.02 47.17 5.67
Negation 227 (4) 76 (4) 1.76 5.26 2.64
Speculation 208 (14) 105 (14) 6.73 13.33 8.95
MODIFICATION TOTAL 435 (18) 181 (18) 4.14 9.94 5.84
ALL TOTAL 3182 (428) 596 (428) 13.45 71.81 22.66 13.25 70.97 22.33
Table 2: Official scores for Tasks 1 and 2, and modification scores only for Task 3, from the approximate span
matching/approximate recursive matching table. GS = gold standard (true positives) (given for Tasks 1/3 only), answer
= all responses (true positives) (given for tasks 1/3 only), R = recall, P = precision, F = F-measure. All results are as
calculated by the official scoring application.
[expressed] by rare cells in the biopsies entering
lytic cycle (PMID 8903467), where the bracketed
text is the arguments and the trigger word, the syn-
tactic object of the verb is the anaphoric noun phrase
these lytic transcripts, so even with the addition of
a syntactic component to our system, we still would
not have recognized the appropriate arguments with-
out the ability to do anaphora resolution.
Appositives The annotation guidelines for proteins
apparently specified that when a gene name was
present in an appositive with its symbol, the symbol
was selected as the gold-standard argument. For this
reason, in examples like [expression] of Fas ligand
[FasL] (PMID 10092076), where the bracketed text
is the trigger word and the argument, the gene name
constituted intervening material from the perspec-
tive of our patterns, which therefore did not match.
We return to a discussion of recall and its implica-
tions for systems like ours in the Discussion section.
4.3.2 False positives
Although our overall rate of false positives was
low, we sampled 45 false positive events distributed
across the nine event types and reviewed them with
a biologist.
We noted two main causes of error. The most
common was that we misidentified a slot filler or
were missing a slot filler completely for an actual
event. The other main reason for false positives was
when we erroneously identified a (non)event. For
example, in coexpression of NF-kappa B/Rel and
Sp1 transcription factors (PMID 7479915), we mis-
takenly identified Sp1 transcription as an event.
5 Discussion
Our results demonstrate that it is possible to achieve
state-of-the art precision over a broad range of tasks
and event types using our approach of manually
constructed, ontologically typed rules?our preci-
sion of 71.81 on Task 1 was ten points higher than
the second-highest precision (62.21), and our preci-
sion of 70.97 on Task 2 was 14 points higher than
the second-highest precision (56.87). It remains the
case that our recall was low enough to drop our F-
measure considerably. Will it be the case that a sys-
tem like ours can scale to practical performance lev-
els nonetheless? Four factors suggest that it can.
The first is that there is considerable redundancy
in the data; although we have not quantified it for
this data set, we note that the same event is often
56
Tasks 1 and 3 Task 2
Event class GS answer R P F R P F
Localization 174 (33) 41 (33) 18.97 80.49 30.70 16.67 69.05 26.85
Binding 347 (62) 152 (62) 17.87 40.79 24.85 17.48 40.13 24.35
Gene expression 722 (290) 344 (290) 40.17 84.30 54.41 40.17 84.30 54.41
Transcription 137 (28) 31 (28) 20.44 90.32 33.33 20.44 90.32 33.33
Protein catabolism 14 (4) 6 (4) 28.57 66.67 40.00 28.57 66.67 40.00
Phosphorylation 135 (47) 48 (47) 34.81 97.92 51.37 32.37 84.91 46.88
EVENT TOTAL 1529 (464) 622 (464) 30.35 74.60 43.14 29.77 72.77 42.26
Regulation 291 (11) 31 (11) 3.78 35.48 6.83 3.77 35.48 6.81
Positive regulation 983 (60) 129 (60) 6.10 46.51 10.79 6.08 46.51 10.75
Negative regulation 379 (18) 41 (18) 4.75 43.90 8.57 4.49 41.46 8.10
REGULATION TOTAL 1653 (89) 201 (89) 5.38 44.28 9.60 5.31 43.78 9.47
Negation 227 (6) 129 (6) 2.64 4.65 3.37
Speculation 208 (25) 165 (25) 12.02 15.15 13.40
MODIFICATION TOTAL 435 (31) 294 (31) 7.13 10.54 8.50
ALL TOTAL 3182 (553) 823 (553) 17.38 67.19 27.62 17.07 65.74 27.10
Table 3: Updated results on test data for Tasks 1-3, with important bug fixes in the code base. See key above.
mentioned repeatedly, but for knowledge base build-
ing and other uses of the extracted information, it is
only strictly necessary to recognize an event once
(although multiple recognition of the same assertion
may increase our confidence in its correctness).
The second is that there is often redundancy
across the literature; the best-supported assertions
will be reported as initial findings and then repeated
as background information.
The third is that these recall results reflect an ap-
proach that made no use of syntactic analysis be-
yond handling coordination. There is often text
present in the input that cannot be disregarded with-
out either using wildcards, which generally de-
creased precision in our experiments and which
we generally eschewed, or making use of syntac-
tic information to isolate phrasal heads. Syntactic
analysis, particularly when combined with analysis
of predicate-argument structure, has recently been
shown to be an effective tool in biomedical infor-
mation extraction (Miyao et al, 2009). There is
broad need for this?for example, of the thirty lo-
calization events in the training data whose trigger
word was translocation, a full eighteen had inter-
vening textual material that made it impossible for
simple patterns like translocationof [Theme] or
[ToLoc]translocation to match.
Finally, our recall numbers reflect a very short de-
velopment cycle, with as few as four patterns writ-
ten for many event types. A less time-constrained
pattern-writing effort would almost certainly result
in increased recall.
Acknowledgments
We gratefully acknowledge Mike Bada?s help in
loading the Sequence Ontology into Prote?ge?.
This work was supported by NIH
grants R01LM009254, R01GM083649, and
R01LM008111 to Lawrence Hunter and
T15LM009451 to Philip Ogren.
References
Alias-i. 2008. LingPipe 3.1.2.
Ekaterina Buyko, Joachim Wermter, Michael Poprat, and
Udo Hahn. 2006. Automatically mapping an NLP
core engine to the biology domain. In Proceedings
of the ISMB 2006 joint BioLINK/Bio-Ontologies meet-
ing.
K. B. Cohen, L. Tanabe, S. Kinoshita, and L. Hunter.
2004. A resource for constructing customized test
suites for molecular biology entity identification sys-
tems. BioLINK 2004, pages 1?8.
K. Bretonnel Cohen, Martha Palmer, and Lawrence
Hunter. 2008. Nominalization and alternations in
biomedical language. PLoS ONE, 3(9).
57
D. Ferrucci and A. Lally. 2004. Building an example
application with the unstructured information manage-
ment architecture. IBM Systems Journal, 43(3):455?
475, July.
Lawrence Hunter, Zhiyong Lu, James Firby, William
A. Baumgartner Jr., Helen L. Johnson, Philip V. Ogren,
and K. Bretonnel Cohen. 2008. OpenDMAP: An
open-source, ontology-driven concept analysis engine,
with applications to capturing knowledge regarding
protein transport, protein interactions and cell-specific
gene expression. BMC Bioinformatics, 9(78).
IBM. 2009. UIMA Java framework. http://uima-
framework.sourceforge.net/.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 shared task on event extraction. In
Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop. To
appear.
Yusuke Miyao, Kenji Sagae, Rune Saetre, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2009. Evaluating contri-
butions of natural language parsers to protein-protein
interaction extraction. Bioinformatics, 25(3):394?400.
Karin Verspoor, K. Bretonnel Cohen, and Lawrence
Hunter. In press. The textual characteristics of tradi-
tional and Open Access scientific journals are similar.
BMC Bioinformatics.
58
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 134?135,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
The CISP Annotation Schema Uncovers Hypotheses and Explanations in
Full-Text Scientific Journal Articles
Elizabeth White, K. Bretonnel Cohen, and Lawrence Hunter
Department of Pharmacology, Computational Bioscience Program,
University of Colorado School of Medicine, Aurora, Colorado, USA
elizabeth.white@ucdenver.edu,
kevin.cohen@gmail.com,
larry.hunter@ucdenver.edu
Abstract
Increasingly, as full-text scientific papers are
becoming available, scientific queries have
shifted from looking for facts to looking for
arguments. Researchers want to know when
their colleagues are proposing theories, out-
lining evidentiary relations, or explaining dis-
crepancies. We show here that sentence-level
annotation with the CISP schema adapts well
to a corpus of biomedical articles, and we
present preliminary results arguing that the
CISP schema is uniquely suited to recovering
common types of scientific arguments about
hypotheses, explanations, and evidence.
1 Introduction
In the scientific domain, the deluge of full-text
publications is driving researchers to find better
techniques for extracting or summarizing the main
claims and findings in a paper. Many researchers
have noted that the sentences of a paper play a small
set of different rhetorical roles (Teufel and Moens,
1999; Blais et al, 2007; Agarwal and Yu, 2009). We
are investigating the rhetorical roles of sentences in
the CRAFT corpus, a set of 97 full-text papers that
we have annotated using the CISP schema. Hand
alignment of the resulting annotations suggests that
patterns in these CISP-annotated sentences corre-
spond to common argumentative gambits in scien-
tific writing.
2 Methods
The CRAFT corpus is a set of 97 full-text papers de-
scribing the function of genes in the Mouse Genome
Informatics database (Blake et al, 2011). These
documents have already been annotated with syn-
tactic information (parse trees and part-of-speech
tags), linguistic phenomena (coreference), and se-
mantic entities (genes, chemicals, cell lines, biolog-
ical functions and molecular processes), making the
corpus a rich resource for extracting or inferring in-
formation from full scientific papers.
The CISP schema (Soldatova and Liakata, 2007;
Liakata et al, 2009) contains 11 categories, and sev-
eral of the categories describe the intentions of the
authors, making it well suited for markup of argu-
mentation. We chose to narrow these down to 9 cat-
egories (excluding Model and Object) during anno-
tation training; our guidelines are shown in Figure
1. We expect this schema to describe the pragmat-
ics in the text well, while still offering the poten-
tial for high interannotator agreement due to a man-
ageable number of categories. The process of mark-
ing the sentences in the CRAFT corpus according to
the CISP guidelines took one annotator about four
months.
3 Results and Discussion
Six of the 97 CRAFT papers do not follow the stan-
dard IMRaD paper structure (one was a review ar-
ticle, and five combined Results and Discussion);
these documents were eliminated from this analy-
sis. Annotation of the 91 remaining CRAFT papers
resulted in 20676 sentences. The distribution of the
annotated classes is shown in Table 1.
Our use of the CISP schema exposes an approach
for recovering two types of explanatory arguments.
The first sets the context with a sequence of Back-
134
Figure 1: Flow chart for CISP annotation of the CRAFT corpus.
CISP Type Count Percentage
Hypothesis 1050 5.08
Goal 992 4.80
Motivation 928 4.49
Background 2838 13.73
Method 637 3.08
Experiment 5270 25.49
Result 5471 26.46
Observation 1168 5.65
Conclusion 2322 11.23
Total 20676 100.0
Table 1: Distribution of CISP sentence types annotated in
91 CRAFT articles.
ground sentences, followed by a Hypothesis, Moti-
vation, or Goal; this echoes a motif found by Swales
(1990) and Teufel and Moens (1999). We also find
another pattern that consists of a combination of Re-
sults and Observations, either preceded or followed
by a Conclusion; Teufel and Moens (1999) also find
exemplars of this maneuver, and note that it paral-
lels Swales? notion of occupying a niche in the re-
search world. Hand alignment of CISP annotations
in Introduction and Result sections suggests that a
finite state machine may be capable of modeling the
transitions between CISP sentence types in these ar-
guments, and machine learning approaches to rep-
resent these and other patterns with hidden Markov
models or conditional random fields are underway.
References
Shashank Agarwal and Hong Yu. 2009. Automatically
classifying sentences in full-text biomedical articles
into Introduction, Methods, Results, and Discussion.
Bioinformatics, 25(23): 3174?3180.
Antoine Blais, Iana Atanassova, Jean-Pierre Descle?s,
Mimi Zhang, and Leila Zighem. 2007. Discourse
automatic annotation of texts: an application to sum-
marization. In Proceedings of the Twentieth Interna-
tional Florida Artificial Intelligence Research Society
Conference, May 7-9, 2007, Key West, Florida, USA,
350?355. AAAI Press.
Judith A. Blake, Carol J. Bult, James A. Kadin, Joel E.
Richardson, Janan T. Eppig, and the Mouse Genome
Database Group 2011. The Mouse Genome Database
(MGD): premier model organism resource for mam-
malian genomics and genetics. Nucleic Acids Res.,
39(Suppl. 1): D842?D848.
Maria Liakata, Claire Q, and Larisa N. Soldatova. Se-
mantic Annotation of Papers: Interface & Enrichment
Tool (SAPIENT). 2009. In Proceedings of BioNLP
2009, Boulder, Colorado,193?200.
Larisa Soldatova and Maria Liakata. 2007. An ontology
methodology and CISP - the proposed Core Informa-
tion about Scientific Papers. JISC intermediate project
report.
John M. Swales. 1990. Genre Analysis: English in
academic and research settings, 137?166. Cambridge
University Press, Cambridge.
Simone Teufel and Marc Moens. 1999. Argumentative
classification of extracted sentences as a first step to-
wards flexible abstracting. In Advances in Automatic
Text Summarization, I. Mani and D. Maybury, eds.
MIT Press, Cambridge, MA.
135
