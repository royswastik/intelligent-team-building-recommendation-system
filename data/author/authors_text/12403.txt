Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 173?181,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Hierarchical Dirichlet Trees for Information Retrieval
Gholamreza Haffari
School of Computing Sciences
Simon Fraser University
ghaffar1@cs.sfu.ca
Yee Whye Teh
Gatsby Computational Neuroscience
University College London
ywteh@gatsby.ucl.ac.uk
Abstract
We propose a principled probabilisitc frame-
work which uses trees over the vocabulary to
capture similarities among terms in an infor-
mation retrieval setting. This allows the re-
trieval of documents based not just on occur-
rences of specific query terms, but also on sim-
ilarities between terms (an effect similar to
query expansion). Additionally our principled
generative model exhibits an effect similar to
inverse document frequency. We give encour-
aging experimental evidence of the superiority
of the hierarchical Dirichlet tree compared to
standard baselines.
1 Introduction
Information retrieval (IR) is the task of retrieving,
given a query, the documents relevant to the user
from a large quantity of documents (Salton and
McGill, 1983). IR has become very important in
recent years, with the proliferation of large quanti-
ties of documents on the world wide web. Many IR
systems are based on some relevance score function
R(j, q) which returns the relevance of document j to
query q. Examples of such relevance score functions
include term frequency-inverse document frequency
(tf-idf) and Okapi BM25 (Robertson et al, 1992).
Besides the effect that documents containing
more query terms should be more relevant (term fre-
quency), the main effect that many relevance scores
try to capture is that of inverse document frequency:
the importance of a term is inversely related to the
number of documents that it appears in, i.e. the
popularity of the term. This is because popular
terms, e.g. common and stop words, are often un-
informative, while rare terms are often very infor-
mative. Another important effect is that related or
co-occurring terms are often useful in determining
the relevance of documents. Because most relevance
scores do not capture this effect, IR systems resort to
techniques like query expansion which includes syn-
onyms and other morphological forms of the origi-
nal query terms in order to improve retrieval results;
e.g. (Riezler et al, 2007; Metzler and Croft, 2007).
In this paper we explore a probabilistic model for
IR that simultaneously handles both effects in a prin-
cipled manner. It builds upon the work of (Cow-
ans, 2004) who proposed a hierarchical Dirichlet
document model. In this model, each document is
modeled using a multinomial distribution (making
the bag-of-words assumption) whose parameters are
given Dirichlet priors. The common mean of the
Dirichlet priors is itself assumed random and given
a Dirichlet hyperprior. (Cowans, 2004) showed that
the shared mean parameter induces sharing of infor-
mation across documents in the corpus, and leads to
an inverse document frequency effect.
We generalize the model of (Cowans, 2004) by re-
placing the Dirichlet distributions with Dirichlet tree
distributions (Minka, 2003), thus we call our model
the hierarchical Dirichlet tree. Related terms are
placed close by in the vocabulary tree, allowing the
model to take this knowledge into account when de-
termining document relevance. This makes it unnec-
essary to use ad-hoc query expansion methods, as re-
lated words such as synonyms will be taken into ac-
count by the retrieval rule. The structure of the tree
is learned from data in an unsupervised fashion, us-
173
ing a variety of agglomerative clustering techniques.
We review the hierarchical Dirichlet document
(HDD) model in section 2, and present our proposed
hierarchical Dirichlet tree (HDT) document model
in section 3. We describe three algorithms for con-
structing the vocabulary tree in section 4, and give
encouraging experimental evidence of the superi-
ority of the hierarchical Dirichlet tree compared to
standard baselines in section 5. We conclude the pa-
per in section 6.
2 Hierarchical Dirichlet Document Model
The probabilistic approach to IR assumes that each
document in a collection can be modeled probabilis-
tically. Given a query q, it is further assumed that
relevant documents j are those with highest gener-
ative probability p(q|j) for the query. Thus given q
the relevance score is R(j, q) = p(q|j) and the doc-
uments with highest relevance are returned.
Assume that each document is a bag of words,
with document j modeled as a multinomial distri-
bution over the words in j. Let V be the terms in
the vocabulary, njw be the number of occurrences
of term w ? V in document j, and ?flatjw be the proba-
bility of w occurring in document j (the superscript
?flat? denotes a flat Dirichlet as opposed to our pro-
posed Dirichlet tree). (Cowans, 2004) assumes the
following hierarchical Bayesian model for the docu-
ment collection:
?flat0 = (?flat0w)w?V ? Dirichlet(?u) (1)
?flatj = (?flatjw)w?V ? Dirichlet(??flat0 )
nj = (njw)w?V ? Multinomial(?flatj )
In the above, bold face a = (aw)w?V means that a
is a vector with |V | entries indexed by w ? V , and
u is a uniform distribution over V . The generative
process is as follows (Figure 1(a)). First a vector
?flat0 is drawn from a symmetric Dirichlet distribution
with concentration parameter ?. Then we draw the
parameters ?flatj for each document j from a common
Dirichlet distribution with mean ?flat0 and concentra-
tion parameter ?. Finally, the term frequencies of
the document are drawn from a multinomial distri-
bution with parameters ?flatj .
The insight of (Cowans, 2004) is that because
the common mean parameter ?flat0 is random, it in-
duces dependencies across the document models in
u
njw
nj
J
?flatj
?flat0 ?
?
?k0
?kj
J
uk
?k
?k
(a) (b)
?flatkb
Figure 1: (a) The graphical model representation of the
hierarchical Dirichlet document model. (b) The global
tree and local trees in hierarchical Dirichlet tree docu-
ment model. Triangles stand for trees with the same
structure, but different parameters at each node. The gen-
eration of words in each document is not shown.
the collection, and this in turn is the mechanism for
information sharing among documents. (Cowans,
2004) proposed a good estimate of ?flat0 :
?flat0w = ?/|V |+ n0w? +?w?V n0w
(2)
where n0w is simply the number of documents con-
taining term w, i.e. the document frequency. Inte-
grating out the document parameters ?flatj , we see that
the probability of query q being generated from doc-
ument j is:
p(q|j) =?
x?q
??flat0x + njx
? +?w?V njw
(3)
= Const ??
x?q
Const + njx?/|V |+n0x
? +?w?V njw
Where Const are terms not depending on j. We see
that njx is term frequency, its denominator ?/|V |+
n0x is an inverse document frequency factor, and
? + ?w?V njw normalizes for document length.
The inverse document frequency factor is directly
related to the shared mean parameter, in that popular
terms x will have high ?flat0x value, causing all docu-
ments to assign higher probability to x, and down
weighting the term frequency. This effect will be
inherited by our model in the next section.
174
3 Hierarchical Dirichlet Trees
Apart from the constraint that the parameters should
sum to one, the Dirichlet priors in the HDD model
do not impose any dependency among the param-
eters of the resulting multinomial. In other words,
the document models cannot capture the notion that
related terms tend to co-occur together. For exam-
ple, this model cannot incorporate the knowledge
that if the word ?computer? is seen in a document, it
is likely to observe the word ?software? in the same
document. We relax the independence assump-
tion of the Dirichlet distribution by using Dirichlet
tree distributions (Minka, 2003), which can capture
some dependencies among the resulting parameters.
This allows relationships among terms to be mod-
eled, and we will see that it improves retrieval per-
formance.
3.1 Model
Let us assume that we have a tree over the vocab-
ulary whose leaves correspond to vocabulary terms.
Each internal node k of the tree has a multinomial
distribution over its children C(k). Words are drawn
by starting at the root of the tree, recursively picking
a child l ? C(k) whenever we are in an internal node
k, until we reach a leaf of the tree which corresponds
to a vocabulary term (see Figure 2(b)). The Dirich-
let tree distribution is the product of Dirichlet dis-
tributions placed over the child probabilities of each
internal node, and serves as a (dependent) prior over
the parameters of multinomial distributions over the
vocabulary (the leaves).
Our model generalizes the HDD model by replac-
ing the Dirichlet distributions in (1) by Dirichlet tree
distributions. At each internal node k, define a hier-
archical Dirichlet prior over the choice of the chil-
dren:
?0k = (?0l)l?C(k) ? Dirichlet(?kuk) (4)
?jk = (?jl)l?C(k) ? Dirichlet(?k?0k)
where uk is a uniform distribution over the children
of node k, and each internal node has its own hy-
perparameters ?k and ?k. ?jl is the probability of
choosing child l if we are at internal node k. If the
tree is degenerate with just one internal node (the
root) and all leaves are direct children of the root we
recover the ?flat? HDD model in the previous sec-
tion. We call our model the hierarchical Dirichlet
tree (HDT).
3.2 Inference and Learning
Given a term, the path from the root to the corre-
sponding leaf is unique. Thus given the term fre-
quencies nj of document j as defined in (1), the
number of times njl child l ? C(k) was picked at
node k is known and fixed. The probability of all
words in document j, given the parameters, is then
a product of multinomials probabilities over internal
nodes k:
p(nj |{?jk}) =
?
k
njk!
Q
l?C(k) njl!
?
l?C(k)
?njljl (5)
The probability of the documents, integrating out the
?jk?s, is:
p({nj}|{?0k}) = (6)?
j
?
k
njk !
Q
l?C(k) njl!
?(?k)
?(?k+njk)
?
l?C(k)
?(?k?0l+njl)
?(?k?0l)
The probability of a query q under document j, i.e.
the relevance score, follows from (3):
p(q|j) =?
x?q
?
(kl)
?k?0l+njl
?k+njk (7)
where the second product is over pairs (kl) where k
is a parent of l on the path from the root to x.
The hierarchical Dirichlet tree model we pro-
posed has a large number of parameters and hy-
perparameters (even after integrating out the ?jk?s),
since the vocabulary trees we will consider later typ-
ically have large numbers of internal nodes. This
over flexibility might lead to overfitting or to param-
eter regimes that do not aid in the actual task of IR.
To avoid both issues, we constrain the hierarchical
Dirichlet tree to be centered over the flat hierarchi-
cal Dirichlet document model, and allow it to learn
only the ?k hyperparameters, integrating out the ?jk
parameters.
We set {?0k}, the hyperparameters of the global
tree, so that it induces the same distribution over vo-
cabulary terms as ?flat0 :
?0l = ?flat0l ?0k =
?
l?C(k)
?0l (8)
175
The hyperparameters of the local trees ?k?s are es-
timated using maximum a posteriori learning with
likelihood given by (6), and a gamma prior with
informative parameters. The density function of a
Gamma(a, b) distribution is
g(x; a, b) = x
a?1bae?bx
?(a)
where the mode happens at x = a?1b . We set the
mode of the prior such that the hierarchical Dirichlet
tree reduces to the hierarchical Dirichlet document
model at these values:
?flatl = ??flat0l ?flatk =
?
l?C(k)
?flatl (9)
?k ? Gamma(b?flatk + 1, b)
and b > 0 is an inverse scale hyperparameter to be
tuned, with large values giving a sharp peak around
?flatk . We tried a few values1 of b and have found that
the results we report in the next section are not sen-
sitive to b. This prior is constructed such that if there
is insufficient information in (6) the MAP value will
simply default back to the hierarchical Dirichlet doc-
ument model.
We used LBFGS2 which is a gradient based opti-
mization method to find the MAP values, where the
gradient of the likelihood part of the objective func-
tion (6) is:
? log p({nj}|{?0j})
??k
=?
j
?(?k)??(?k + njk)
+ ?
l?C(k)
?0l
(
?(?k?0l + njl)??(?k?0l)
)
where ?(x) := ? log ?(x)/?x is the digamma func-
tion. Because each ?k can be optimized separately,
the optimization is very fast (approximately 15-30
minutes in the experiments to follow on a Linux ma-
chine with 1.8 GH CPU speed).
4 Vocabulary Tree Structure Learning
The structure of the vocabulary tree plays an impor-
tant role in the quality of the HDT document model,
1Of the form 10i for i ? {?2,?1, 0, 1}.
2We used a C++ re-implementation of Jorge Nocedal?s
LBFGS library (Nocedal, 1980) from the ALGLIB website:
http://www.alglib.net.
Algorithm 1 Greedy Agglomerative Clustering
1: Place m words into m singleton clusters
2: repeat
3: Merge the two clusters with highest similarity, re-
sulting in one less cluster
4: If there still are unincluded words, pick one and
place it in a singleton cluster, resulting in one more
cluster
5: until all words have been included and there is only
one cluster left
since it encapsulates the similarities among words
captured by the model. In this paper we explored
using trees learned in an unsupervised fashion from
the training corpus.
The three methods are all agglomerative cluster-
ing algorithms (Duda et al, 2000) with different
similarity functions. Initially each vocabulary word
is placed in its own cluster; each iteration of the al-
gorithm finds the pair of clusters with highest sim-
ilarity and merges them, continuing until only one
cluster is left. The sequence of merges determines a
binary tree with vocabulary words as its leaves.
Using a heap data structure, this basic agglom-
erative clustering algorithm requires O(n2 log(n) +
sn2) computations where n is the size of the vocab-
ulary and s is the amount of computation needed to
compute the similarity between two clusters. Typi-
cally the vocabulary size n is large; to speed up the
algorithm, we use a greedy version described in Al-
gorithm 1 which restricts the number of cluster can-
didates to at most m ? n. This greedy version is
faster with complexity O(nm(logm + s)). In the
experiments we used m = 500.
Distributional clustering (Dcluster) (Pereira et
al., 1993) measures similarity among words in terms
of the similarity among their local contexts. Each
word is represented by the frequencies of various
words in a window around each occurrence of the
word. The similarity between two words is com-
puted to be a symmetrized KL divergence between
the distributions over neighboring words associated
with the two words. For a cluster of words the neigh-
boring words are the union of those associated with
each word in the cluster. Dcluster has been used
extensively in text classification (Baker and McCal-
lum, 1998).
Probabilistic hierarchical clustering (Pcluster)
176
(Friedman, 2003). Dcluster associates each word
with its local context, as a result it captures both
semantic and syntactic relationships among words.
Pcluster captures more relevant semantic relation-
ships by instead associating each word with the doc-
uments in which it appears. Specifically, each word
is associated with a binary vector indexed by doc-
uments in the corpus, where a 1 means the word
appears in the corresponding document. Pcluster
models a cluster of words probabilistically, with the
binary vectors being iid draws from a product of
Bernoulli distributions. The similarity of two clus-
ters c1 and c2 of words is P (c1 ? c2)/P (c1)P (c2),
i.e. two clusters of words are similar if their union
can be effectively modeled using one cluster, rela-
tive to modeling each separately. Conjugate beta pri-
ors are placed over the parameters of the Bernoulli
distributions and integrated out so that the similarity
scores are comparable.
Brown?s algorithm (Bcluster) (Brown et al,
1990) was originally proposed to build class-based
language models. In the 2-gram case, words are
clustered such that the class of the previous word
is most predictive of the class of the current word.
Thus the similarity between two clusters of words
is defined to be the resulting mutual information be-
tween adjacent classes corrresponding to a sequence
of words.
4.1 Operations to Simplify Trees
Trees constructed using the agglomerative hierarchi-
cal clustering algorithms described in this section
suffer from a few drawbacks. Firstly, because they
are binary trees they have large numbers of internal
nodes. Secondly, many internal nodes are simply not
informative in that the two clusters of words below
a node are indistinguishable. Thirdly, Pcluster and
Dcluster tend to produce long chain-like branches
which significantly slows down the computation of
the relevance score.
To address these issues, we considered operations
to simplify trees by contracting internal edges of the
tree while preserving as much of the word relation-
ship information as possible. Let L be the set of tree
leaves and ?(a) be the distance from node or edge a
to the leaves:
?(a) := min
l?L
#{edges between a and l} (10)
a
b
Figure 2: ?(root) = 2, while ?(v) = 1 for shaded ver-
tices v. Contracting a and b results in both child of b
being direct children of a while b is removed.
In the experiments we considered either contracting
edges3 close to the leaves ?(a) = 1 (thus remov-
ing many of the long branches described above), or
edges further up the tree ?(a) ? 2 (preserving the
informative subtrees closer to the leaves while re-
moving many internal nodes). See Figure 2.
(Miller et al, 2004) cut the BCluster tree at a cer-
tain depth k to simplify the tree, meaning every leaf
descending from a particular internal node at level
k is made an immediate child of that node. They
use the tree to get extra features for a discrimina-
tive model to tackle the problem of sparsity?the
features obtained from the new tree do not suffer
from sparsity since each node has several words as
its leaves. This technique did not work well for our
application so we will not report results using it in
our experiments.
5 Experiments
In this section we present experimental results on
two IR datasets: Cranfield and Medline4. The Cran-
field dataset consists of 1,400 documents and 225
queries; its vocabulary size after stemming and re-
moving stop words is 4,227. The Medline dataset
contains 1,033 documents and 30 queries with the
vocabulary size of 8,800 after stemming and remov-
ing stop words. We compare HDT with the flat
HDD model and Okapi BM25 (Robertson et al,
1992). Since one of our motivations has been to
3Contracting an edge means removing the edge and the adja-
cent child node and connecting the grandchildren to the parent.
4Both datasets can be downloaded from
http://www.dcs.gla.ac.uk/idom/ir resources/test collections.
177
Depth Statistics Performance
Tree Cranfield Medline Cranfield Medline
avg / max total avg / max total avg-pr top10-pr avg-pr top10-pr
BCluster 16.7 / 24 4226 16.4 / 22 8799 0.2675 0.3218 0.2131 0.6433
BC contract ? ? 2 6.2 / 16 3711 5.3 / 14 7473 0.2685 0.3147 0.2079 0.6533
BC contract ? = 1 16.1 / 23 3702 15.8 / 22 7672 0.2685 0.3204 0.1975 0.6400
DCluster 41.2 / 194 4226 38.1 / 176 8799 0.2552 0.3120 0.1906 0.6300
DC contract ? ? 2 2.3 / 8 2469 3.3 / 9 5091 0.2555 0.3156 0.1906 0.6167
DC contract ? = 1 40.9 / 194 3648 38.1 / 176 8799 0.2597 0.3129 0.1848 0.6300
PCluster 50.2 / 345 4226 37.1 / 561 8799 0.2613 0.3231 0.1681 0.6633
PC contract ? ? 2 35.2 / 318 3741 20.4 / 514 7280 0.2624 0.3213 0.1792 0.6767
PC contract ? = 1 33.6 / 345 2246 34.1 / 561 4209 0.2588 0.3240 0.1880 0.6633
flat model 1 / 1 1 1 / 1 1 0.2506 0.3089 0.1381 0.6133
BM25 ? ? ? ? 0.2566 0.3124 0.1804 0.6567
BM25QueryExp ? ? ? ? 0.2097 0.3191 0.2121 0.7366
Table 1: Average precision and Top-10 precision scores of HDT with different trees versus flat model and BM25. The
statistics for each tree shows its average/maximum depth of its leaf nodes as well as the number of its total internal
nodes. The bold numbers highlight the best results in the corresponding columns.
get away from query expansion, we also compare
against Okapi BM25 with query expansion. The
new terms to expand each query are chosen based
on Robertson-Sparck Jones weights (Robertson and
Sparck Jones, 1976) from the pseudo relevant docu-
ments. The comparison criteria are (i) top-10 preci-
sion, and (ii) average precision.
5.1 HDT vs Baselines
All the hierarchical clustering algorithms mentioned
in section 4 are used to generate trees, each of which
is further post-processed by tree simplification op-
erators described in section 4.1. We consider (i)
contracting nodes at higher levels of the hierarchy
(? ? 2), and (ii) contracting nodes right above the
leaves (? = 1).
The statistics of the trees before and after post-
processing are shown in Table 1. Roughly, the
Dcluster and BCluster trees do not have long chains
with leaves hanging directly off them, which is why
their average depths are reduced significantly by the
? ? 2 simplification, but not by the ? = 1 sim-
plification. The converse is true for Pcluster: the
trees have many chains with leaves hanging directly
off them, which is why average depth is not reduced
as much as the previous trees based on the ? ? 2
simplification. However the average depth is still re-
duced significantly compared to the original trees.
Table 1 presents the performance of HDT with
different trees against the baselines in terms of the
top-10 and average precision (we have bold faced
the performance values which are the maximum
of each column). HDT with every tree outper-
forms significantly the flat model in both datasets.
More specifically, HDT with (original) BCluster and
PCluster trees significantly outperforms the three
baselines in terms of both performance measure for
the Cranfield. Similar trends are observed on the
Medline except here the baseline Okapi BM25 with
query expansion is pretty strong5, which is still out-
performed by HDT with BCluster tree.
To further highlight the differences among the
methods, we have shown the precision at particular
recall points on Medline dataset in Figure 4 for HDT
with PCluster tree vs the baselines. As the recall
increases, the precision of the PCluster tree signifi-
cantly outperforms the flat model and BM25. We at-
tribute this to the ability of PCluster tree to give high
scores to documents which have words relevant to a
query word (an effect similar to query expansion).
5.2 Analysis
It is interesting to contrast the learned ?k?s for each
of the clustering methods. These ?k?s impose cor-
5Note that we tuned the parameters of the baselines BM25
with/without query expansion with respect to their performance
on the actual retrieval task, which in a sense makes them appear
better than they should.
178
10?1 100 101 102 103
10?1
100
101
102
103
BCluster
?0k ?parent(k)
?
k
10?2 10?1 100 101 102 103
10?2
10?1
100
101
102
103
DCluster
?0k ?parent(k)
?
k
10?2 10?1 100 101 102 103
10?2
10?1
100
101
102
103
PCluster
?0k ?parent(k)
?
k
Figure 3: The plots showing the contribution of internal nodes in trees constructed by the three clustering algorithms
for the Cranfield dataset. In each plot, a point represent an internal node showing a positive exponent in the node?s
contribution (i.e. positive correlation among its children) if the point is below x = y line. From left to the right plots,
the fraction of nodes below the line is 0.9044, 0.7977, and 0.3344 for a total of 4,226 internal nodes.
0 .1 .2 .3 .4 .5 .6 .7 .8 .9
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Precision at particular recall points
 
 
PCluster
Flat model
BM25
BM25 Query Expansion
Figure 4: The precision of all methods at particular recall
points for the Medline dataset.
relations on the probabilities of the children under k
in an interesting fashion. In particular, if we com-
pare ?k to ?0k?parent(k), then a larger value of ?k
implies that the probabilities of picking one of the
children of k (from among all nodes) are positively
correlated, while a smaller value of ?k implies neg-
ative correlation. Roughly speaking, this is because
drawn values of ?jl for l ? C(k) are more likely
to be closer to uniform (relative to the flat Dirichlet)
thus if we had picked one child of k we will likely
pick another child of k.
Figure 3 shows scatter plots of ?k values ver-
sus ?0k?parent(k) for the internal nodes of the trees.
Firstly, smaller values for both tend to be associ-
ated with lower levels of the trees, while large val-
ues are with higher levels of the trees. Thus we
see that PCluster tend to have subtrees of vocabu-
lary terms that are positively correlated with each
other?i.e. they tend to co-occur in the same docu-
ments. The converse is true of DCluster and BClus-
ter because they tend to put words with the same
meaning together, thus to express a particular con-
cept it is enough to select one of the words and not
to choose the rest. Figure 5 show some fragments
of the actual trees including the words they placed
together and ?k parameters learned by HDT model
for their internal nodes. Moreover, visual inspection
of the trees shows that DCluster can easily misplace
words in the tree, which explains its lower perfor-
mance compared to the other tree construction meth-
ods.
Secondly, we observed that for higher nodes of
the tree (corresponding generally to larger values of
?k and ?0k?parent(k)) PCluster ?k?s are smaller, thus
higher levels of the tree exhibit negative correlation.
This is reasonable, since if the subtrees capture pos-
itively correlated words, then higher up the tree the
different subtrees correspond to clusters of words
that do not co-occur together, i.e. negatively corre-
lated.
6 Conclusion and Future Work
We presented a hierarchical Dirichlet tree model for
information retrieval which can inject (semantical or
syntactical) word relationships as the domain knowl-
edge into a probabilistic model for information re-
trieval. Using trees to capture word relationships,
the model is highly efficient while making use of
both prior information about words and their occur-
rence statistics in the corpus. Furthermore, we inves-
tigated the effect of different tree construction algo-
rithms on the model performance.
On the Cranfield dataset, HDT achieves 26.85%
for average-precision and 32.40% for top-10 preci-
179
Figure 5: Small parts of the trees learned by clustering algorithms for the Cranfield dataset where the learned ?k for
each internal node is written close to it.
sion, and outperforms all baselines including BM25
which gets 25.66% and 31.24% for these two mea-
sures. On the Medline dataset, HDT is competi-
tive with BM25 with Query Expansion and outper-
forms all other baselines. These encouraging results
show the benefits of HDT as a principled probabilis-
tic model for information retrieval.
An interesting avenue of research is to construct
the vocabulary tree based on WordNet, as a way to
inject independent prior knowledge into the model.
However WordNet has a low coverage problem, i.e.
there are some words in the data which do not ex-
ist in it. One solution to this low coverage problem
is to combine trees generated by the clustering algo-
rithms mentioned in this paper and WordNet, which
we leave as a future work.
References
L. Douglas Baker and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classification. In SIGIR ?98: Proceedings of the
21st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 96?103.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1990. Class-based n-gram models
of natural language. Computational Linguistics.
P. J. Cowans. 2004. Information retrieval using hierar-
chical dirichlet processes. In Proceedings of the 27th
Annual International Conference on Research and De-
velopment in Information Retrieval (SIGIR).
R. O. Duda, P. E. Hart, and D. G. Stork. 2000. Pattern
Classification. Wiley-Interscience Publication.
N. Friedman. 2003. Pcluster: Probabilistic agglomera-
tive clustering of gene expression profiles. Available
from http://citeseer.ist.psu.edu/668029.html.
Donald Metzler and W. Bruce Croft. 2007. Latent con-
cept expansion using markov random fields. In Pro-
ceedings of the 30th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative training.
In Proceedings of North American Chapter of the As-
sociation for Computational Linguistics - Human Lan-
guage Technologies conference (NAACL HLT).
180
T. Minka. 2003. The dirichlet-tree distribu-
tion. Available from http://research.microsoft.com/
minka/papers/dirichlet/minka-dirtree.pdf.
J. Nocedal. 1980. Updating quasi-newton matrices with
limited storage. Mathematics of Computation, 35.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of english words. In 31st
Annual Meeting of the Association for Computational
Linguistics, pages 183?190.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics.
S. E. Robertson and K. Sparck Jones. 1976. Relevance
weighting of search terms. Journal of the American
Society for Information Science, 27(3):129?146.
S. E. Robertson, S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at trec. In Text
REtrieval Conference, pages 21?30.
G. Salton and M.J. McGill. 1983. An Introduction to
Modern Information Retrieval. McGraw-Hill, New
York.
181
