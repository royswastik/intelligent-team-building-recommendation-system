Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1030?1038,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Phrase Clustering for Discriminative Learning 
Dekang Lin and Xiaoyun Wu 
Google, Inc. 
1600 Amphitheater Parkway, Mountain View, CA 
{lindek,xiaoyunwu}@google.com 
 
Abstract 
We present a simple and scalable algorithm for 
clustering tens of millions of phrases and use 
the resulting clusters as features in 
discriminative classifiers. To demonstrate the 
power and generality of this approach, we 
apply the method in two very different 
applications: named entity recognition and 
query classification. Our results show that 
phrase clusters offer significant improvements 
over word clusters. Our NER system achieves 
the best current result on the widely used 
CoNLL benchmark. Our query classifier is on 
par with the best system in KDDCUP 2005 
without resorting to labor intensive knowledge 
engineering efforts. 
1 Introduction 
Over the past decade, supervised learning 
algorithms have gained widespread acceptance in 
natural language processing (NLP). They have 
become the workhorse in almost all sub-areas 
and components of NLP, including part-of-
speech tagging, chunking, named entity 
recognition and parsing. To apply supervised 
learning to an NLP problem, one first represents 
the problem as a vector of features. The learning 
algorithm then optimizes a regularized, convex 
objective function that is expressed in terms of 
these features.  The performance of such 
learning-based solutions thus crucially depends 
on the informativeness of the features. The 
majority of the features in these supervised 
classifiers are predicated on lexical information, 
such as word identities. The long-tailed 
distribution of natural language words implies 
that most of the word types will be either unseen 
or seen very few times in the labeled training 
data, even if the data set is a relatively large one 
(e.g., the Penn Treebank). 
While the labeled data is generally very costly 
to obtain, there is a vast amount of unlabeled 
textual data freely available on the web. One way 
to alleviate the sparsity problem is to adopt a 
two-stage strategy: first create word clusters with 
unlabeled data and then use the clusters as 
features in supervised training. Under this 
approach, even if a word is not found in the 
training data, it may still fire cluster-based 
features as long as it shares cluster assignments 
with some words in the labeled data.  
Since the clusters are obtained without any 
labeled data, they may not correspond directly to 
concepts that are useful for decision making in 
the problem domain. However, the supervised 
learning algorithms can typically identify useful 
clusters and assign proper weights to them, 
effectively adapting the clusters to the domain. 
This method has been shown to be quite 
successful in named entity recognition (Miller et 
al. 2004) and dependency parsing (Koo et al, 
2008).  
In this paper, we present a semi-supervised 
learning algorithm that goes a step further. In 
addition to word-clusters, we also use phrase-
clusters as features. Out of context, natural 
language words are often ambiguous. Phrases are 
much less so because the words in a phrase 
provide contexts for one another.  
Consider the phrase ?Land of Odds?. One 
would never have guessed that it is a company 
name based on the clusters containing Odds and 
Land. With phrase-based clustering, ?Land of 
Odds? is grouped with many names that are 
labeled as company names, which is a strong 
indication that it is a company name as well. The 
disambiguation power of phrases is also 
evidenced by the improvements of phrase-based 
machine translation systems (Koehn et. al., 
2003) over word-based ones. 
Previous approaches, e.g., (Miller et al 2004) 
and (Koo et al 2008), have all used the Brown 
algorithm for clustering (Brown et al 1992). The 
main idea of the algorithm is to minimize the 
bigram language-model perplexity of a text 
corpus. The algorithm is quadratic in the number 
of elements to be clustered. It is able to cluster 
tens of thousands of words, but is not scalable 
enough to deal with tens of millions of phrases. 
Uszkoreit and Brants (2008) proposed a 
1030
distributed clustering algorithm with a similar 
objective function as the Brown algorithm. It 
substantially increases the number of elements 
that can be clustered. However, since it still 
needs to load the current clustering of all 
elements into each of the workers in the 
distributed system, the memory requirement 
becomes a bottleneck. 
We present a distributed version of a much 
simpler K-Means clustering that allows us to 
cluster tens of millions of elements. We 
demonstrate the advantages of phrase-based 
clusters over word-based ones with experimental 
results from two distinct application domains: 
named entity recognition and query 
classification. Our named entity recognition 
system achieves an F1-score of 90.90 on the 
CoNLL 2003 English data set, which is about 1 
point higher than the previous best result. Our 
query classifier reaches the same level of 
performance as the KDDCUP 2005 winning 
systems, which were built with a great deal of 
knowledge engineering. 
2 Distributed K-Means clustering 
K-Means clustering (MacQueen 1967) is one of 
the simplest and most well-known clustering 
algorithms. Given a set of elements represented 
as feature vectors and a number, k, of desired 
clusters, the K-Means algorithm consists of the 
following steps: 
Step Operation 
i. Select k elements as the initial centroids 
for k clusters. 
ii. Assign each element to the cluster with 
the closest centroid according to a 
distance (or similarity) function. 
iii. Recompute each cluster?s centroid by 
averaging the vectors of its elements 
iv. Repeat Steps ii and iii until 
convergence 
Before describing our parallel implementation of 
the K-Means algorithm, we first describe the 
phrases to be clusters and how their feature 
vectors are constructed. 
2.1 Phrases 
To obtain a list of phrases to be clustered, we 
followed the approach in (Lin et al, 2008) by 
collecting 20 million unique queries from an 
anonymized query log that are found in a 700 
billion token web corpus with a minimum 
frequency count of 100. Note that many of these 
queries are not phrases in the linguistic sense. 
However, this does not seem to cause any real 
problem because non-linguistic phrases may 
form their own clusters. For example, one cluster 
contains {?Cory does?, ?Ben saw?, ?I can?t 
lose?, ?..}.  
To reduce the memory requirement for storing 
a large number of phrases, we used Bloom Filter 
(Bloom 1970) to decide whether a sequence of 
tokens is a phrase. The Bloom filter allows a 
small percentage of false positives to pass 
through. We did not remove them with post 
processing since our notion of phrases is quite 
loose to begin with. 
2.2 Context representation 
Distributional word clustering is based on the 
assumption that words that appear in similar 
contexts tend to have similar meanings. The 
same assumption holds for phrases as well. 
Following previous approaches to distributional 
clustering of words, we represent the contexts of 
a phrase as a feature vector. There are many 
possible definitions for what constitutes the 
contexts. In the literature, contexts have been 
defined as subject and object relations involving 
the word (Hindle, 1990), as the documents 
containing the word (Deerwester et al 1990), or 
as search engine snippets for the word as a query 
(Sahami and Heilman, 2006). We define the 
contexts of a phrase to be small, fixed-sized 
windows centered on occurrences of the phrase 
in a large corpus. The features are the words 
(tokens) in the window. The context feature 
vector of a phrase is constructed by first 
aggregating the frequency counts of the words in 
the context windows of different instances of the 
Table 1 Cluster of ?English lessons? 
Window Cluster members (partial list) 
size=1 environmental courses, summer school 
courses, professional development 
classes, professional training programs, 
further education courses, leadership 
courses, accelerated courses, vocational 
classes, technical courses, technical 
classes, special education courses, ?.. 
size=3 learn english spanish, grammar learn, 
language learning spanish, translation 
spanish language, learning spanish 
language, english spanish language, 
learn foreign language, free english 
learning, language study english, 
spanish immersion course, how to 
speak french, spanish learning games, 
?.. 
1031
phrase. The frequency counts are then converted 
into point-wise mutual information (PMI) values: 
2/+:LDN? B; L ???F 2:LDN? B;2:LDN;2:B;G 
where phr is a phrase and  f  is a feature of 
phr. PMI effectively discounts the prior 
probability of the features and measures how 
much beyond random a feature tends to occur in 
a phrase?s context window.  Given two feature 
vectors, we compute the similarity between two 
vectors as the cosine function of the angle 
between the vectors. Note that even though a 
phrase phr can have multiple tokens, its feature f 
is always a single-word token.  
We impose an upper limit on the number of 
instances of each phrase when constructing its 
feature vector. The idea is that if we have already 
seen 300K instances of a phrase, we should have 
already collected enough data for the phrase. 
More data for the same phrase will not 
necessarily tell us anything more about it. There 
are two benefits for such an upper limit. First, it 
drastically reduces the computational cost. 
Second, it reduces the variance in the sizes of the 
feature vectors of the phrases. 
2.3 K-Means by MapReduce  
K-Means is an embarrassingly parallelizable 
algorithm. Since the centroids of clusters are 
assumed to be constant within each iteration, the 
assignment of elements to clusters (Step ii) can 
be done totally independently. 
The algorithm fits nicely into the MapReduce 
paradigm for parallel programming (Dean and 
Ghemawat, 2004). The most straightforward 
MapReduce implementation of K-Means would 
be to have mappers perform Step ii and reducers 
perform Step iii. The keys of intermediate pairs 
are cluster ids and the values are feature vectors 
of elements assigned to the corresponding 
cluster. When the number of elements to be 
clustered is very large, sorting the intermediate 
pairs in the shuffling stage can be costly. 
Furthermore, when summing up a large number 
of features vectors, numerical underflow 
becomes a potential problem.  
A more efficient and numerically more stable 
method is to compute, for each input partition, 
the partial vector sums of the elements belonging 
to each cluster. When the whole partition is done, 
the mapper emits the cluster ids as keys and the 
partial vector sums as values. The reducers then 
aggregate the partial sums to compute the 
centroids. 
2.4 Indexing centroid vectors 
In a na?ve implementation of Step ii of K-Means, 
one would compute the similarities between a 
feature vector and all the centroids in order to 
find the closest one. The kd-tree algorithm 
(Bentley 1980) aims at speeding up nearest 
neighbor search. However, it only works when 
the vectors are low-dimensional, which is not the 
case here. Fortunately, the high-dimensional and 
sparse nature of our feature vectors can also be 
exploited.  
Since the cosine measure of two unit length 
vectors is simply their dot product, when 
searching for the closest centroid to an element, 
we only care about features in the centroids that 
are in common with the element. We therefore 
create an inverted index that maps a feature to 
the list of centroids having that feature. Given an 
input feature vector, we can iterate through all of 
its components and compute its dot product with 
all the centroids at the same time. 
2.5 Sizes of context window 
In our experiments, we use either 1 or 3 as the 
size of the context windows. Window size has an 
interesting effect on the types of clusters. With 
larger windows, the clusters tend to be more 
topical, whereas smaller windows result in 
categorical clusters.  
For example, Table 1 contains the cluster that 
the phrase ?English lessons? belongs to. With 3-
word context windows, the cluster is about 
language learning and translation. With 1-word 
context windows, the cluster contains different 
types of lessons. 
The ability to produce both kinds of clusters 
turns out to be very useful. In different 
applications we need different types of clusters. 
For example, in the named entity recognition 
task, categorical clusters are more successful, 
whereas in query categorization, the topical 
clusters are much more beneficial.  
The Brown algorithm uses essentially the 
same information as our 1-word window 
clusters. We therefore expect it to produce 
mostly categorical clusters.  
2.6 Soft clustering 
Although K-Means is generally described as a 
hard clustering algorithm (each element belongs 
to at most one cluster), it can produce soft 
clustering simply by assigning an element to all 
clusters whose similarity to the element is greater 
than a threshold. For natural language words and 
1032
phrases, the soft cluster assignments often reveal 
different senses of a word. For example, the 
word Whistler may refer to a town in British 
Columbia, Canada, which is also a ski resort, or 
to a painter. These meanings are reflected in the 
top clusters assignments for Whistler in Table 2 
(window size = 3). 
2.7 Clustering data sets 
We experimented with two corpora (Table 3). 
One contains web documents with 700 billion 
tokens. The second consists of various news texts 
from LDC: English Gigaword, the Tipster corpus 
and Reuters RCV1. The last column lists the 
numbers of phrases we used when running the 
clustering with that corpus.  
Even though our cloud computing 
infrastructure made phrase clustering possible, 
there is no question that it is still very time 
consuming. To create 3000 clusters among 20 
million phrases using 3-word windows, each K-
Means iteration takes about 20 minutes on 1000 
CPUs. Without using the indexing technique in 
Section 2.4, each iteration takes about 4 times as 
long. In all our experiments, we set the 
maximum number of iterations to be 50. 
3 Named Entity Recognition 
Named entity recognition (NER) is one of the 
first steps in many applications of information 
extraction, information retrieval, question 
answering and other applications of NLP. 
Conditional Random Fields (CRF) (Lafferty et. 
al. 2001) is one of the most competitive NER 
algorithms. We employed a linear chain CRF 
with L2 regularization as the baseline algorithm 
to which we added phrase cluster features. 
The CoNLL 2003 Shared Task (Tjong Kim 
Sang and Meulder 2003) offered a standard 
experimental platform for NER. The CoNLL 
data set consists of news articles from Reuters1. 
The training set has 203,621 tokens and the 
development and test set have 51,362 and 46,435 
tokens, respectively. We adopted the same 
evaluation criteria as the CoNLL 2003 Shared 
Task. 
To make the clusters more relevant to this 
domain, we adopted the following strategy: 
1. Construct the feature vectors for 20 
million phrases using the web data. 
2. Run K-Means clustering on the phrases 
that appeared in the CoNLL training data 
to obtain K centroids. 
3. Assign each of the 20 million phrases to 
the nearest centroid in the previous step. 
3.1 Baseline features 
The features in our baseline CRF classifier are a 
subset of the conventional features. They are 
defined with the following templates: >U??,>U??5???,<>U?? S??=?@??5?>5 ? <>U??5??? S??=?@??5?>5 ,  <>U?? OBTu??=?@??5?>5 ,   <>U??5??? OBTu??=?@??5?>5 ,  <<>U?? SPL?? ?=?@??5?>5 =?@68 ,<<>U??5??? SPL?? ?=?@??5?>5 =?@68 ?  <>U?? S??5???=?@??>5 ,<>U??5??? S??5???=?@??>5 ,       <<>U?? SPL??5??? ?=?@??>5=?@57 ,<<>U??5??? SPL??5??? ?=?@??>5=?@57  
Here, s denotes a position in the input sequence; 
ys is a label that indicates whether the token at 
position s is a named entity as well as its type; wu 
is the word at position u; sfx3 is a word?s three-
letter suffix; <SPL?=?@58  are indicators of 
                                                          
1
 http://www.reuters.com/researchandstandards/ 
Table 2 Soft clusters for Whistler 
cluster1: sim=0.17, members=104048 
bc vancouver, british columbia accommodations, 
coquitlam vancouver, squamish vancouver, 
langley vancouver, vancouver surrey,  ? 
cluster2: sim=0. 16, members= 182692 
vail skiing, skiing colorado, tahoe ski vacation, 
snowbird skiing, lake tahoe skiing, breckenridge 
skiing, snow ski packages, ski resort whistler, ? 
cluster3: sim=0.12, members= 91895 
ski chalets france, ski chalet holidays, france ski, 
catered chalets, luxury ski chalets, france skiing, 
france skiing, ski chalet holidays, ?? 
cluster4: sim=0.11, members=237262 
ocean kayaking, mountain hiking, horse trekking, 
river kayaking, mountain bike riding, white water 
canoeing, mountain trekking, sea kayaking, ?? 
cluster5: sim=0.10, members=540775 
rent cabin, pet friendly cabin, cabins rental, cabin 
vacation, cabins colorado, cabin lake tahoe, maine 
cabin, tennessee mountain cabin,  ? 
cluster6: sim=0.09, members=117365 
mary cassatt, oil painting reproductions, henri 
matisse, pierre bonnard, edouard manet, auguste 
renoir, paintings famous, picasso paintings, ?? 
?? 
 
Table 3 Corpora used in experiments 
Corpus Description tokens phrases 
Web web documents 700B 20M 
LDC News text from LDC 3.4B 700K 
1033
different word types: wtp1 is true when a word is 
punctuation; wtp2 indicates whether a word is in 
lower case, upper case, or all-caps; wtp3 is true 
when a token is a number; wtp4 is true when a 
token is a hyphenated word with different 
capitalization before and after the hyphen. 
NER systems often have global features to 
capture discourse-level regularities (Chieu and 
Ng 2003). For example, documents often have a 
full mention of an entity at the beginning and 
then refer to the entity in partial or abbreviated 
forms. To help in recognizing the shorter 
versions of the entities, we maintain a history of 
unigram word features. If a token is encountered 
again, the word unigram features of the previous 
instances are added as features for the current 
instance as well. We have a total of 48 feature 
templates. In comparison, there are 79 templates 
in (Suzuki and Isozaki, 2008).  
Part-of-speech tags were used in the top-
ranked systems in CoNLL 2003, as well as in 
many follow up studies that used the data set 
(Ando and Zhang 2005; Suzuki and Isozaki 
2008).  Our system does not need this 
information to achieve its peak performance. An 
important advantage of not needing a POS tagger 
as a preprocessor is that the system is much 
easier to adapt to other languages, since training 
a tagger often requires a larger amount of more 
extensively annotated data than the training data 
for NER. 
3.2 Phrase cluster features 
We used hard clustering with 1-word context 
windows for NER. For each input token 
sequence, we identify all sequences of tokens 
that are found in the phrase clusters. The phrases 
are allowed to overlap with or be nested in one 
another. If a phrase belonging to cluster c is 
found at positions b to e (inclusive), we add the 
following features to the CRF classifier: >U??5? $??? >U?>5? #??? >U??6???5? $??? >U???>5? #?? >U?? 5??? <>U?? /??=?@?>5??5 ? >U? ? '?? >U??5??? 5??? <>U??5??? /??=?@?>5??5 ? >U??5?? ? '?? 
where B (before), A (after), S (start), M (middle), 
and E (end) denote a position in the input 
sequence relative to the phrase belonging to 
cluster c. We treat the cluster membership as 
binary. The similarity between an element and its 
cluster centroid is ignored. For example, suppose 
the input sentence is ?? guitar legend Jimi 
Hendrix was ?? and ?Jimi Hendrix? belongs to 
cluster 183. Figure 1 shows the attributes at 
different input positions. The cluster features are 
the cross product of the unigram/bigram labels 
and the attributes. 
 
Figure 1 Phrase cluster features 
 
The phrasal cluster features not only help in 
resolving the ambiguities of words within a 
phrase, the B and A features also allow words 
adjacent to a phrase to consider longer contexts 
than a single word. Although one may argue 
longer n-grams can also capture this information, 
the sparseness of n-grams means that long n-
gram features are rarely useful in practice.  
We can easily use multiple clusterings in 
feature extraction. This allows us to side-step the 
matter of choosing the optimal value k in the K-
Means clustering algorithm.  
Even though the phrases include single token 
words, we create word clusters with the same 
clustering algorithm as well. The reason is that 
the phrase list, which comes from query logs, 
does not necessarily contain all the single token 
words in the documents. Furthermore, due to 
tokenization differences between the query logs 
and the documents, we systematically missed 
some words, such as hyphenated words. When 
creating the word clusters, we do not rely on a 
predefined list. Instead, any word above a 
minimum frequency threshold is included.  
In their dependency parser with cluster-based 
features, Koo et al (2008) found it helpful to 
restrict lexicalized features to only relatively 
frequent words. We did not observe a similar 
phenomenon with our CRF. We include all 
words as features and rely on the regularized 
CRF to select from them.  
3.3 Evaluation results 
Table 4 summarizes the evaluation results for 
our NER system and compares it with the two 
best results on the data set in the literature, as 
well the top-3 systems in CoNLL 2003. In this 
table, W and P refer to word and phrase clusters 
created with the web corpus. The superscripts are 
the numbers of clusters. LDC refers to the 
clusters created with the smaller LDC corpus and 
+pos indicates the use of part-of-speech tags as 
features.  
The performance of our baseline system is 
rather mediocre because it has far fewer feature 
functions than the more competitive systems. 
1034
The Top CoNLL 2003 systems all employed 
gazetteers or other types of specialized resources 
(e.g., lists of words that tend to co-occur with 
certain named entity types) in addition to part-of-
speech tags. 
Introducing the word clusters immediately 
brings the performance up to a very competitive 
level. Phrasal clusters obtained from the LDC 
corpus give the same level of improvement as 
word clusters from the web corpus that is 20 
times larger. The best F-score of 90.90, which is 
about 1 point higher than the previous best result, 
is obtained with a combination of clusters. 
Adding POS tags to this configuration caused a 
small drop in F1. 
4 Query Classification 
We now look at the use of phrasal clusters in a 
very different application: query classification. 
The goal of query classification is to determine 
to which ones of a predefined set of classes a 
query belongs. Compared with documents, 
queries are much shorter and their categories are 
much more ambiguous.  
4.1 KDDCUP 2005 data set 
The task in the KDDCUP 2005 competition2 is to 
classify 800,000 internet user search queries into 
67 predefined topical categories. The training set 
consists of 111 example queries, each of which 
belongs to up to 5 of the 67 categories. Table 5 
shows three example queries and their classes.  
Three independent human labelers classified 
800 queries that were randomly selected from the 
                                                          
2
 http://www.acm.org/sigs/sigkdd/kdd2005/kddcup.html 
complete set of 800,000. The participating 
systems were evaluated by their average F-scores 
(F1) and average precision (P) over these three 
sets of answer keys for the 800 selected queries.  
 L ? S?????????????????????????gg ? S????????????????gg  
 L ? S?????????????????????????gg? S?????????????????????gg  
	s L t H  H E   
Here, ?tagged as? refer to systems outputs and 
?labeled as? refer to human judgments. The 
subscript i ranges over all the query classes. 
Table 6 shows the scores of each of the three 
human labelers when each of them is evaluated 
against the other two. It can be seen that the 
consistency among the labelers is quite low, 
indicating that the query classification task is 
very difficult even for humans.  
To maximize the little information we have 
about the query classes, we treat the words in 
query class names as additional example queries. 
For example, we added three queries: living, 
tools, and hardware to the class Living\Tools & 
Hardware. 
4.2 Baseline classifier 
Since the query classes are not mutually 
exclusive, we treat the query classification task 
as 67 binary classification problems. For each 
query class, we train a logistic regression 
classifier (Vapnik 1999) with L2 regularization. 
Table 4 CoNLL NER test set results 
System Test F1  Improv. 
Baseline CRF (Sec. 3.1) 83.78  
W500 88.34 +4.56 
P64 89.73 +5.94 
P125 89.80 +6.02 
W500 + P125 90.62 +6.84 
W500 + P64 90.63 +6.85 
W500 + P125 + P64 90.90 +7.12 
W500 + P125 + P64+pos 90.62 +6.84 
LDC64 87.24 +3.46 
LDC125 88.33 +4.55 
LDC64 +LDC125 88.44 +4.66 
(Suzuki and Isozaki, 2008) 89.92  
(Ando and Zhang, 2005) 89.31  
(Florian et al, 2003) 88.76  
(Chieu and Ng, 2003) 88.31  
(Klein et al, 2003) 86.31  
Table 5 Example queries and their classes 
ford field 
   Sports/American Football 
   Information/Local & Regional 
   Sports/Schedules & Tickets 
john deere gator 
   Living/Landscaping & Gardening 
   Living/Tools & Hardware 
   Information/Companies & Industries 
   Shopping/Stores & Products 
   Shopping/Buying Guides & Researching 
justin timberlake lyrics 
   Entertainment/Music 
   Information/Arts & Humanities 
   Entertainment/Celebrities 
Table 6 Labeler Consistency 
 
L1  L2 L3 Average
F1 0.538 0.477 0.512 0.509
P 0.501 0.613 0.463 0.526
1035
Given an input x, represented as a vector of m 
features: (x1, x2, ....., xm), a logistic regression 
classifier with parameter vector ? L(w1, w2, ....., 
wm) computes the posterior probability of the 
output y, which is either 1 or -1, as 
L:U?; L s
sE A????	? 
We tag a query as belonging to a class if the 
probability of the class is among the highest 5 
and is greater than 0.5. 
The baseline system uses only the words in the 
queries as features (the bag-of-words 
representation), treating the query classification 
problem as a typical text categorization problem. 
We found the prior distribution of the query 
classes to be extremely important. In fact, a 
system that always returns the top-5 most 
frequent classes has an F1 score of 26.55, which 
would have outperformed 2/3 of the 37 systems 
in the KDDCUP and ranked 13th. 
We made a small modification to the objective 
function for logistic regression to take into 
account the prior distribution and to use 50% as a 
uniform decision boundary for all the classes. 
Normally, training a logistic regression classifier 
amounts to solving: 
???IEJ? ]???? E sJ? ???@sE A?????	??A
?
?@5
a 
where n is the number of training examples and ?  
is the regularization constant. In this formula, 1/n 
can be viewed as the weight of an example in the 
training corpus. When training the classifier for a 
class with p positive examples out of a total of n 
examples, we change the objective function to: 
???IEJ? P????E ? ???@sE A?????	??A??@5J E U?:tL F J; Q 
With this modification, the total weight of the 
positive and negative examples become equal. 
4.3 Phrasal clusters in query classification 
Since topical information is much more relevant 
to query classification than categorical 
information, we use clusters created with 3-word 
context windows. Moreover, we use soft 
clustering instead of hard clustering. A phrase 
belongs to a cluster if the cluster?s centroid is 
among the top-50 most similar centroids to the 
phrase (by cosine similarity), and the similarity is 
greater than 0.04.  
Given a query, we first retrieve all its phrases 
(allowing overlap) and the clusters they belong 
to. For each of these clusters, we sum the 
cluster?s similarity to all the phrases in the query 
and select the top-N as features for the logistic 
regression classifier (N=150 in our experiments). 
When we extract features from multiple 
clusterings, the selection of the top-N clusters is 
done separately for each clustering. Once a 
cluster is selected, its similarity values are 
ignored. Using the numerical feature values in 
our experiments always led to worse results. We 
suspect that such features make the optimization 
of the objective function much more difficult. 
 
Figure 2 Comparison with KDDCUP systems 
4.4 Evaluation results 
Table 7 contains the evaluation results of various 
configurations of our system. Here, bow 
indicates the use of bag-of-words features; WN 
refers to word clusters of size N; and PN refers to 
phrase clusters of size N. All the clusters are soft 
clusters created with the web corpus using 3-
word context windows. 
The bag-of-words features alone have dismal 
performance. This is obviously due to the 
extreme paucity of training examples. In fact, 
only 12% of the words in the 800 test queries are 
found in the training examples. Using word 
clusters as features resulted in a big increase in 
F-score. The phrasal cluster features offer 
another big improvement. The best result is 
achieved with multiple phrasal clusterings.  
Figure 2 compares the performance of our 
system (the dark bar at 2) with the top tercile 
systems in KDDCUP 2005. The best two 
systems in the competition (Shen et al, 2005) 
and (Vogel et al, 2005) resorted to knowledge 
engineering techniques to bridge the gap between 
0
0.1
0.2
0.3
0.4
0.5
1 2 3 4 5 6 7 8 9 10 11 12 13
Table 7 Query Classification results 
System F1  
bow 11.58 
bow+W3K 34.71 
bow+P500 39.84 
bow+P3K 40.80 
bow+P500+P1K +P2K +P3K+P5K 43.80 
1036
the small set of examples and the new queries. 
They manually constructed a mapping from the 
query classes to hierarchical directories such as 
Google Directory3 or Open Directory Project4. 
They then sent training and testing queries to 
internet search engines to retrieve the top pages 
in these directories. The positions of the result 
pages in the directory hierarchies as well as the 
words in the pages are used to classify the 
queries. With phrasal clusters, we can achieve 
top-level performance without manually 
constructed resources, or having to rely on 
internet search results.  
5 Discussion and Related Work  
In earlier work on semi-supervised learning, e.g., 
(Blum and Mitchell 1998), the classifiers learned 
from unlabeled data were used directly. Recent 
research shows that it is better to use whatever is 
learned from the unlabeled data as features in a 
discriminative classifier. This approach is taken 
by (Miller et. al. 2004), (Wong and Ng 2007), 
(Suzuki and Isozaki 2008), and (Koo et. al., 
2008), as well as this paper.  
Wong and Ng (2007) and Suzuki and Isozaki 
(2008) are similar in that they run a baseline 
discriminative classifier on unlabeled data to 
generate pseudo examples, which are then used  
to train a different type of classifier for the same 
problem. Wong and Ng (2007) made the 
assumption that each proper named belongs to 
one class (they observed that this is true about 
85% of the time for English). Suzuki and Isozaki 
(2008), on the other hand, used the automatically 
labeled corpus to train HMMs. 
Ando and Zhang (2005) defined an objective 
function that combines the original problem on 
the labeled data with a set of auxiliary problems 
on unlabeled data. The definition of an auxiliary 
problem can be quite flexible as long as it can be 
automatically labeled and shares some structural 
properties with the original problem. The 
combined objective function is then alternatingly 
optimized with the labeled and unlabeled data. 
This training regime puts pressure on the 
discriminative learner to exploit the structures 
uncovered from the unlabeled data. 
In the two-stage cluster-based approaches such 
as ours, clustering is mostly decoupled from the 
supervised learning problem. However, one can 
rely on a discriminative classifier to establish the 
connection by assigning proper weights to the 
                                                          
3
 http://directory.google.com 
4
 http://www.dmoz.org 
cluster features. One advantage of the two-stage 
approach is that the same clusterings may be 
used for different problems or different 
components of the same system. Another 
advantage is that it can be applied to a wider 
range of domains and problems. Although the 
method in (Suzuki and Isozaki 2008) is quite 
general, it is hard to see how it can be applied to 
the query classification problem. 
Compared with Brown clustering, our 
algorithm for distributional clustering with 
distributed K-Means offers several benefits: (1) it 
is more scalable and parallelizable; (2) it has the 
ability to generate topical as well as categorical 
clusters for use in different applications; (3) it 
can create soft clustering as well as hard ones. 
There are two main scenarios that motivate 
semi-supervised learning. One is to leverage a 
large amount of unsupervised data to train an 
adequate classifier with a small amount of 
labeled data. Another is to further boost the 
performance of a supervised classifier that is 
already trained with a large amount of supervised 
data. The named entity problem in Section 3 and 
the query classification problem in Section 4 
exemplify the two scenarios. 
One nagging issue with K-Means clustering is 
how to set k. We show that this question may not 
need to be answered because we can use 
clusterings with different k?s at the same time 
and let the discriminative classifier cherry-pick 
the clusters at different granularities according to 
the supervised data. This technique has also been 
used with Brown clustering (Miller et. al. 2004, 
Koo, et. al. 2008). However, they require clusters 
to be strictly hierarchical, whereas we do not. 
6 Conclusions 
We presented a simple and scalable algorithm to 
cluster tens of millions of phrases and we used 
the resulting clusters as features in discriminative 
classifiers. We demonstrated the power and 
generality of this approach on two very different 
applications: named entity recognition and query 
classification. Our system achieved the best 
current result on the CoNLL NER data set. Our 
query categorization system is on par with the 
best system in KDDCUP 2005, which, unlike 
ours, involved a great deal of knowledge 
engineering effort. 
 
Acknowledgments 
The authors wish to thank the anonymous 
reviewers for their comments. 
1037
References  
R. Ando and T. Zhang A Framework for Learning 
Predictive Structures from Multiple Tasks and 
Unlabeled Data. Journal of Machine Learning 
Research, Vol 6:1817-1853, 2005. 
B.H. Bloom. 1970, Space/time trade-offs in hash 
coding with allowable errors, Communications of 
the ACM 13 (7): 422?426 
A. Blum and T. Mitchell. 1998. Combining labeled 
and unlabeled data with co-training. Proceedings of 
the Eleventh Annual Conference on Computational 
Learning Theory pp. 92?100. 
P.F. Brown, V.J. Della Pietra, P.V. de Souza, J.C. Lai, 
and R.L. Mercer. 1992. Class-based n-gram models 
of natural language. Computational Linguistics, 
18(4):467?479.  
H. L. Chieu and H. T. Ng. Named entity recognition 
with a maximum entropy approach. In Proceedings 
CoNLL-2003, pages 160?163, 2003. 
J. Dean and S. Ghemawat. 2004. MapReduce: 
Simplified data processing on large clusters. In 
Proceedings of the Sixth Symposium on Operating 
System Design and Implementation (OSDI-04), 
San Francisco, CA, USA 
S Deerwester, S. T. Dumais, G. W. Furnas, T. K. 
Landauer, and R. A. Harshman. 1990. Indexing by 
latent semantic analysis, Journal of the American 
Society for Information Science, 1990, 41(6), 391-
407 
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 
Named entity recognition through classifier 
combination. In Proceedings CoNLL-2003, pages 
168?171, 2003. 
D. Klein, J. Smarr, H. Nguyen, and C. D. Manning. 
Named entity recognition with character-level 
models. In Proceedings CoNLL-2003, pages 188?
191, 2003. 
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical 
phrase-based translation. In Proceedings of HLT-
NAACL 2003, pp. 127?133. 
T. Koo, X. Carreras, and M. Collins. Simple Semi-
supervised Dependency Parsing. Proceedings of 
ACL, 2008. 
J. Lafferty, A. McCallum, F. Pereira. Conditional 
random fields: Probabilistic models for segmenting 
and labeling sequence data. In: Proc. 18th 
International Conf. on Machine Learning, Morgan 
Kaufmann, San Francisco, CA (2001) 282?289 
Y. Li, Z. Zheng, and H.K. Dai, KDD Cup-2005 
Report: Facing a Great Challenge. SIGKDD 
Explorations, 7 (2), 2005, 91-99. 
D. Lin, S. Zhao, and B. Van Durme, and M. Pasca. 
2008. Mining Parenthetical Translations from the 
Web by Word Alignment. Proc. of ACL-08. 
Columbus, OH. 
J.  Lin. Scalable Language Processing Algorithms for 
the Masses: A Case Study in Computing Word Co-
occurrence Matrices with MapReduce. Proceedings 
of  EMNLP 2008, pp. 419-428, Honolulu, Hawaii. 
J. B. MacQueen (1967): Some Methods for 
classification and Analysis of Multivariate 
Observations, Proc. of 5-th Berkeley Symposium 
on Mathematical Statistics and Probability", 
Berkeley, University of California Press, 1:281-
297 
S. Miller, J. Guinness, and A. Zamanian. 2004. Name 
Tagging with Word Clusters and Discriminative 
Training. In Proceedings of HLT-NAACL, pages 
337?342. 
M. Sahami and T.D. Heilman. 2006. A web-based 
kernel function for measuring the similarity of 
short text snippets. Proceedings of the 15th 
international conference on World Wide Web, pp. 
377?386. 
D. Shen, R. Pan, J.T. Sun, J.J. Pan, K. Wu, J. Yin, Q. 
Yang. Q2C@UST: our winning solution to query 
classification in KDDCUP 2005. SIGKDD 
Explorations, 2005: 100~110. 
J. Suzuki, and H. Isozaki. 2008. Semi-Supervised 
Sequential Labeling and Segmentation using Giga-
word Scale Unlabeled Data. In Proc. of ACL/HLT-
08. Columbus, Ohio. pp. 665-673. 
E. T. Tjong Kim Sang and F. De Meulder. 2003. 
Introduction to the CoNLL-2003 Shared Task: 
Language-Independent Named Entity Recognition. 
In Proc. of CoNLL-2003, pages 142?147. 
Y. Wong and H. T. Ng, 2007. One Class per Named 
Entity: Exploiting Unlabeled Text for Named 
Entity Recognition. In Proc. of IJCAI-07,  
Hyderabad, India. 
J. Uszkoreit and T. Brants. 2008. Distributed Word 
Clustering for Large Scale Class-Based Language 
Modeling in Machine Translation. Proceedings of 
ACL-08: HLT, pp. 755-762. 
V. Vapnik, 1999. The Nature of Statistical Learning 
Theory, 2nd edition. Springer Verlag. 
D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. 
Siemen, S. Bridges, T. Scheffer. Classifying 
Search Engine Queries Using the Web as 
Background Knowledge. SIGKDD Explorations 
7(2): 117-122. 2005. 
 
1038
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 835?845,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Binarized Forest to String Translation
Hao Zhang
Google Research
haozhang@google.com
Licheng Fang
Computer Science Department
University of Rochester
lfang@cs.rochester.edu
Peng Xu
Google Research
xp@google.com
Xiaoyun Wu
Google Research
xiaoyunwu@google.com
Abstract
Tree-to-string translation is syntax-aware and
efficient but sensitive to parsing errors. Forest-
to-string translation approaches mitigate the
risk of propagating parser errors into transla-
tion errors by considering a forest of alterna-
tive trees, as generated by a source language
parser. We propose an alternative approach to
generating forests that is based on combining
sub-trees within the first best parse through
binarization. Provably, our binarization for-
est can cover any non-consitituent phrases in
a sentence but maintains the desirable prop-
erty that for each span there is at most one
nonterminal so that the grammar constant for
decoding is relatively small. For the purpose
of reducing search errors, we apply the syn-
chronous binarization technique to forest-to-
string decoding. Combining the two tech-
niques, we show that using a fast shift-reduce
parser we can achieve significant quality gains
in NIST 2008 English-to-Chinese track (1.3
BLEU points over a phrase-based system, 0.8
BLEU points over a hierarchical phrase-based
system). Consistent and significant gains are
also shown in WMT 2010 in the English to
German, French, Spanish and Czech tracks.
1 Introduction
In recent years, researchers have explored a wide
spectrum of approaches to incorporate syntax and
structure into machine translation models. The uni-
fying framework for these models is synchronous
grammars (Chiang, 2005) or tree transducers
(Graehl and Knight, 2004). Depending on whether
or not monolingual parsing is carried out on the
source side or the target side for inference, there are
four general categories within the framework:
? string-to-string (Chiang, 2005; Zollmann and
Venugopal, 2006)
? string-to-tree (Galley et al, 2006; Shen et al,
2008)
? tree-to-string (Lin, 2004; Quirk et al, 2005;
Liu et al, 2006; Huang et al, 2006; Mi et al,
2008)
? tree-to-tree (Eisner, 2003; Zhang et al, 2008)
In terms of search, the string-to-x models explore all
possible source parses and map them to the target
side, while the tree-to-x models search over the sub-
space of structures of the source side constrained
by an input tree or trees. Hence, tree-to-x mod-
els are more constrained but more efficient. Mod-
els such as Huang et al (2006) can match multi-
level tree fragments on the source side which means
larger contexts are taken into account for transla-
tion (Poutsma, 2000), which is a modeling advan-
tage. To balance efficiency and accuracy, forest-to-
string models (Mi et al, 2008; Mi and Huang, 2008)
use a compact representation of exponentially many
trees to improve tree-to-string models. Tradition-
ally, such forests are obtained through hyper-edge
pruning in the k-best search space of a monolin-
gual parser (Huang, 2008). The pruning parameters
that control the size of forests are normally hand-
tuned. Such forests encode both syntactic variants
and structural variants. By syntactic variants, we re-
fer to the fact that a parser can parse a substring into
either a noun phrase or verb phrase in certain cases.
835
We believe that structural variants which allow more
source spans to be explored during translation are
more important (DeNeefe et al, 2007), while syn-
tactic variants might improve word sense disam-
biguation but also introduce more spurious ambi-
guities (Chiang, 2005) during decoding. To focus
on structural variants, we propose a family of bina-
rization algorithms to expand one single constituent
tree into a packed forest of binary trees containing
combinations of adjacent tree nodes. We control the
freedom of tree node binary combination by restrict-
ing the distance to the lowest common ancestor of
two tree nodes. We show that the best results are
achieved when the distance is two, i.e., when com-
bining tree nodes sharing a common grand-parent.
In contrast to conventional parser-produced-forest-
to-string models, in our model:
? Forests are not generated by a parser but by
combining sub-structures using a tree binarizer.
? Instead of using arbitary pruning parameters,
we control forest size by an integer number that
defines the degree of tree structure violation.
? There is at most one nonterminal per span so
that the grammar constant is small.
Since GHKM rules (Galley et al, 2004) can cover
multi-level tree fragments, a synchronous grammar
extracted using the GHKM algorithm can have syn-
chronous translation rules with more than two non-
terminals regardless of the branching factor of the
source trees. For the first time, we show that simi-
lar to string-to-tree decoding, synchronous binariza-
tion significantly reduces search errors and improves
translation quality for forest-to-string decoding.
To summarize, the whole pipeline is as follows.
First, a parser produces the highest-scored tree for
an input sentence. Second, the parse tree is re-
structured using our binarization algorithm, result-
ing in a binary packed forest. Third, we apply the
forest-based variant of the GHKM algorithm (Mi
and Huang, 2008) on the new forest for rule extrac-
tion. Fourth, on the translation forest generated by
all applicable translation rules, which is not neces-
sarily binary, we apply the synchronous binarization
algorithm (Zhang et al, 2006) to generate a binary
translation forest. Finally, we use a bottom-up de-
coding algorithm with intergrated LM intersection
using the cube pruning technique (Chiang, 2005).
The rest of the paper is organized as follows. In
Section 2, we give an overview of the forest-to-
string models. In Section 2.1, we introduce a more
efficient and flexible algorithm for extracting com-
posed GHKM rules based on the same principle as
cube pruning (Chiang, 2007). In Section 3, we in-
troduce our source tree binarization algorithm for
producing binarized forests. In Section 4, we ex-
plain how to do synchronous rule factorization in a
forest-to-string decoder. Experimental results are in
Section 5.
2 Forest-to-string Translation
Forest-to-string models can be described as
e = Y( arg max
d?D(T ), T?F (f)
P (d|T ) ) (1)
where f stands for a source string, e stands for a tar-
get string, F stands for a forest, D stands for a set
of synchronous derivations on a given tree T , and
Y stands for the target side yield of a derivation.
The search problem is finding the derivation with
the highest probability in the space of all deriva-
tions for all parse trees for an input sentence. The
log probability of a derivation is normally a lin-
ear combination of local features which enables dy-
namic programming to find the optimal combination
efficiently. In this paper, we focus on the models
based on the Synchronous Tree Substitution Gram-
mars (STSG) defined by Galley et al (2004). In con-
trast to a tree-to-string model, the introduction of F
augments the search space systematically. When the
first-best parse is wrong or no good translation rules
are applicable to the first-best parse, the model can
recover good translations from alternative parses.
In STSG, local features are defined on tree-to-
string rules, which are synchronous grammar rules
defining how a sequence of terminals and nontermi-
nals on the source side translates to a sequence of
target terminals and nonterminals. One-to-one map-
ping of nonterminals is assumed. But terminals do
not necessarily need to be aligned. Figure 1 shows a
typical English-Chinese tree-to-string rule with a re-
ordering pattern consisting of two nonterminals and
different numbers of terminals on the two sides.
836
VP
VBD
was
VP-C
.x1:VBN PP
P
by
.x2:NP-C
? bei? x2 x1
Figure 1: An example tree-to-string rule.
Forest-to-string translation has two stages. The
first stage is rule extraction on word-aligned parallel
texts with source forests. The second stage is rule
enumeration and DP decoding on forests of input
strings. In both stages, at each tree node, the task on
the source side is to generate a list of tree fragments
by composing the tree fragments of its children. We
propose a cube-pruning style algorithm that is suit-
able for both rule extraction during training and rule
enumeration during decoding.
At the highest level, our algorithm involves three
steps. In the first step, we label each node in the in-
put forest by a boolean variable indicating whether it
is a site of interest for tree fragment generation. If it
is marked true, it is an admissible node. In the case
of rule extraction, a node is admissible if and only if
it corresponds to a phrase pair according to the un-
derlying word alignment. In the case of decoding,
every node is admissible for the sake of complete-
ness of search. An initial one-node tree fragment is
placed at each admissible node for seeding the tree
fragment generation process. In the second step,
we do cube-pruning style bottom-up combinations
to enumerate a pruned list of tree fragments at each
tree node. In the third step, we extract or enumerate-
and-match tree-to-string rules for the tree fragments
at the admissible nodes.
2.1 A Cube-pruning-inspired Algorithm for
Tree Fragment Composition
Galley et al (2004) defined minimal tree-to-string
rules. Galley et al (2006) showed that tree-to-string
rules made by composing smaller ones are impor-
tant to translation. It can be understood by the anal-
ogy of going from word-based models to phrase-
based models. We relate composed rule extraction
to cube-pruning (Chiang, 2007). In cube-pruning,
the process is to keep track of the k-best sorted lan-
guage model states at each node and combine them
bottom-up with the help of a priority queue. We
can imagine substituting k-best LM states with k
composed rules at each node and composing them
bottom-up. We can also borrow the cube pruning
trick to compose multiple lists of rules using a pri-
ority queue to lazily explore the space of combina-
tions starting from the top-most element in the cube
formed by the lists.
We need to define a ranking function for com-
posed rules. To simulate the breadth-first expansion
heuristics of Galley et al (2006), we define the fig-
ure of merit of a tree-to-string rule as a tuple m =
(h, s, t), where h is the height of a tree fragment,
s is the number of frontier nodes, i.e., bottom-level
nodes including both terminals and non-terminals,
and t is the number of terminals in the set of frontier
nodes. We define an additive operator +:
m1 + m2
= ( max{h1, h2} + 1, s1 + s2, t1 + t2 )
and a min operator based on the order <:
m1 < m2 ??
?
?
?
h1 < h2 ?
h1 = h2 ? s1 < s2 ?
h1 = h2 ? s1 = s2 ? t1 < t2
The + operator corresponds to rule compositions.
The < operator corresponds to ranking rules by their
sizes. A concrete example is shown in Figure 2,
in which case the monotonicity property of (+, <)
holds: if ma < mb, ma +mc < mb +mc. However,
this is not true in general for the operators in our def-
inition, which implies that our algorithm is indeed
like cube-pruning: an approximate k-shortest-path
algorithm.
3 Source Tree Binarization
The motivation of tree binarization is to factorize
large and rare structures into smaller but frequent
ones to improve generalization. For example, Penn
Treebank annotations are often flat at the phrase
level. Translation rules involving flat phrases are un-
likely to generalize. If long sequences are binarized,
837
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VBD (1, 1, 0)
VBD
was
(2, 1, 1)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VP-C (1, 1, 0)
VP-C
VPB PP
(2, 2, 0)
VP-C
VPB PP
P NP-C
(3, 3, 1)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
=
(1, 1, 0) (2, 2, 0) (3, 3, 1)
(1, 1, 0) VP
VBD VP-C
(2, 2, 0) VP
VBD VP-C
VPB PP
(3, 3, 0) VP
VBD VP-C
VPB PP
P NP-C
(4, 4, 1)
(2, 1, 1) VP
VBD
was
VP-C
(3, 2, 1) VP
VBD
was
VP-C
VPB PP
(3, 3, 1) VP
VBD
was
VP-C
VPB PP
P NP-C
(4, 4, 2)
Figure 2: Tree-to-string rule composition as cube-pruning. The left shows two lists of composed rules sorted by their
geometric measures (height, # frontiers,# frontier terminals), under the gluing rule of VP ? VBD VP?C.
The right part shows a cube view of the combination space. We explore the space from the top-left corner to the
neighbors.
the commonality of subsequences can be discov-
ered. For example, the simplest binarization meth-
ods left-to-right, right-to-left, and head-out explore
sharing of prefixes or suffixes. Among exponentially
many binarization choices, these algorithms pick a
single bracketing structure for a sequence of sibling
nodes. To explore all possible binarizations, we use
a CYK algorithm to produce a packed forest of bi-
nary trees for a given sibling sequence.
With CYK binarization, we can explore any span
that is nested within the original tree structure, but
still miss all cross-bracket spans. For example,
translating from English to Chinese, The phrase
?There is? should often be translated into one verb
in Chinese. In a correct English parse tree, however,
the subject-verb boundary is between ?There? and
?is?. As a result, tree-to-string translation based on
constituent phrases misses the good translation rule.
The CYK-n binarization algorithm shown in Al-
gorithm 1 is a parameterization of the basic CYK
binarization algorithm we just outlined. The idea is
that binarization can go beyond the scope of parent
nodes to more distant ancestors. The CYK-n algo-
rithm first annotates each node with its n nearest
ancestors in the source tree, then generates a bina-
rization forest that allows combining any two nodes
with common ancestors. The ancestor chain labeled
at each node licenses the node to only combine with
nodes having common ancestors in the past n gener-
ations.
The algorithm creates new tree nodes on the fly.
New tree nodes need to have their own states in-
dicated by a node label representing what is cov-
ered internally by the node and an ancestor chain
representing which nodes the node attaches to ex-
ternally. Line 22 and Line 23 of Algorithm 1 up-
date the label and ancestor annotations of new tree
nodes. Using the parsing semiring notations (Good-
man, 1999), the ancestor computation can be sum-
marized by the (?,?) pair. ? produces the ances-
tor chain of a hyper-edge. ? produces the ancestor
chain of a hyper-node. The node label computation
can be summarized by the (concatenate, min) pair.
concatenate produces a concatenation of node la-
bels. min yields the label with the shortest length.
A tree-sequence (Liu et al, 2007) is a sequence of
sub-trees covering adjacent spans. It can be proved
that the final label of each new node in the forest
corresponds to the tree sequence which has the min-
imum length among all sequences covered by the
node span. The ancestor chain of a new node is the
common ancestors of the nodes in its minimum tree
sequence.
For clarity, we do full CYK loops over all O(|w|2)
spans and O(|w|3) potential hyper-edges, where |w|
is the length of a source string. In reality, only de-
scendants under a shared ancestor can combine. If
we assume trees have a bounded branching factor
b, the number of descendants after n generations is
still bounded by a constant c = bn. The algorithm is
O(c3 ? |w|), which is still linear to the size of input
sentence when the parameter n is a constant.
838
VP
VBD+VBN
VBD
was
VBN
PP
P
by
NP-C
VP
VBD
was
VP-C
VBN+P
VBN P
by
NP-C
(a) (b)
VP
VBD+VBN+P
VBD+VBN
VBD
was
VBN
P
by
NP-C
VP
VBD+VBN+P
VBD
was
VBN+P
VBN P
by
NP-C
(c) (d)
1 2 3 4
0 VBD VBD+VBN VBD+VBN+P VP
1 VBN VBN+P VP-C
2 P PP
3 NP-C
Figure 3: Alternative binary parses created for the origi-
nal tree fragment in Figure 1 through CYK-2 binarization
(a and b) and CYK-3 binarization (c and d). In the chart
representation at the bottom, cells with labels containing
the concatenation symbol + hold nodes created through
binarization.
Figure 3 shows some examples of alternative trees
generated by the CYK-n algorithm. In this example,
standard CYK binarization will not create any new
trees since the input is already binary. The CYK-2
and CYK-3 algorithms discover new trees with an
increasing degree of freedom.
4 Synchronous Binarization for
Forest-to-string Decoding
In this section, we deal with binarization of transla-
tion forests, also known as translation hypergraphs
(Mi et al, 2008). A translation forest is a packed
forest representation of all synchronous derivations
composed of tree-to-string rules that match the
source forest. Tree-to-string decoding algorithms
work on a translation forest, rather than a source for-
est. A binary source forest does not necessarily al-
ways result in a binary translation forest. In the tree-
to-string rule in Figure 4, the source tree is already
ADJP
RB+JJ
x0:RB JJ
responsible
PP
IN
for
NP-C
NPB
DT
the
x1:NN
x2:PP
? x0
fuze
?? x2
de
? x1
ADJP
RB+JJ
x0:RB JJ
responsible
x1:PP
? x0
fuze
?? x1
PP
IN
for
NP-C
NPB
DT
the
x0:NN
x1:PP
? x1
de
? x0
Figure 4: Synchronous binarization for a tree-to-string
rule. The top rule can be binarized into two smaller rules.
binary with the help of source tree binarization, but
the translation rule involves three variables in the set
of frontier nodes. If we apply synchronous binariza-
tion (Zhang et al, 2006), we can factorize it into
two smaller translation rules each having two vari-
ables. Obviously, the second rule, which is a com-
mon pattern, is likely to be shared by many transla-
tion rules in the derivation forest. When beams are
fixed, search goes deeper in a factorized translation
forest.
The challenge of synchronous binarization for a
forest-to-string system is that we need to first match
large tree fragments in the input forest as the first
step of decoding. Our solution is to do the matching
using the original rules and then run synchronous
binarization to break matching rules down to factor
rules which can be shared in the derivation forest.
This is different from the offline binarization scheme
described in (Zhang et al, 2006), although the core
algorithm stays the same.
5 Experiments
We ran experiments on public data sets for English
to Chinese, Czech, French, German, and Spanish
839
Algorithm 1 The CYK-n Binarization Algorithm
1: function CYKBINARIZER(T,n)
2: for each tree node ? T in bottom-up topological order do
3: Make a copy of node in the forest output F
4: Ancestors[node] = the nearest n ancestors of node
5: Label [node] = the label of node in T
6: L? the length of the yield of T
7: for k = 2...L do
8: for i = 0, ..., L? k do
9: for j = i + 1, ..., i + k ? 1 do
10: lnode ? Node[i, j]; rnode ? Node[j, i + k]
11: if Ancestors[lnode] ? Ancestors[rnode] 6= ? then
12: pnode ? GETNODE(i, i + k)
13: ADDEDGE(pnode, lnode, rnode)
return F
14: function GETNODE(begin, end)
15: if Node[begin, end] /? F then
16: Create a new node for the span (begin, end)
17: Ancestors[node] = ?
18: Label [node] = the sequence of terminals in the span (begin, end) in T
19:
return Node[begin, end]
20: function ADDEDGE(pnode, lnode, rnode)
21: Add a hyper-edge from lnode and rnode to pnode
22: Ancestors[pnode] = Ancestors[pnode] ? (Ancestors[lnode] ?Ancestors[rnode])
23: Label [pnode] = min{Label[pnode], CONCATENATE(Label[lnode], Label[rnode])}
translation to evaluate our methods.
5.1 Setup
For English-to-Chinese translation, we used all the
allowed training sets in the NIST 2008 constrained
track. For English to the European languages, we
used the training data sets for WMT 2010 (Callison-
Burch et al, 2010). For NIST, we filtered out sen-
tences exceeding 80 words in the parallel texts. For
WMT, the filtering limit is 60. There is no filtering
on the test data set. Table 1 shows the corpus statis-
tics of our bilingual training data sets.
Source Words Target Words
English-Chinese 287M 254M
English-Czech 66M 57M
English-French 857M 996M
English-German 45M 43M
English-Spanish 216M 238M
Table 1: The Sizes of Parallel Texts.
At the word alignment step, we did 6 iterations
of IBM Model-1 and 6 iterations of HMM. For
English-Chinese, we ran 2 iterations of IBM Model-
4 in addition to Model-1 and HMM. The word align-
ments are symmetrized using the ?union? heuris-
tics. Then, the standard phrase extraction heuristics
(Koehn et al, 2003) were applied to extract phrase
pairs with a length limit of 6. We ran the hierar-
chical phrase extraction algorithm with the standard
heuristics of Chiang (2005). The phrase-length limit
is interpreted as the maximum number of symbols
on either the source side or the target side of a given
rule. On the same aligned data sets, we also ran the
tree-to-string rule extraction algorithm described in
Section 2.1 with a limit of 16 rules per tree node.
The default parser in the experiments is a shift-
reduce dependency parser (Nivre and Scholz, 2004).
It achieves 87.8% labelled attachment score and
88.8% unlabeled attachment score on the standard
Penn Treebank test set. We convert dependency
parses to constituent trees by propagating the part-
of-speech tags of the head words to the correspond-
ing phrase structures.
We compare three systems: a phrase-based sys-
tem (Och and Ney, 2004), a hierarchical phrase-
based system (Chiang, 2005), and our forest-to-
string systemwith different binarization schemes. In
the phrase-based decoder, jump width is set to 8. In
the hierarchical decoder, only the glue rule is applied
840
to spans longer than 10. For the forest-to-string sys-
tem, we do not have such length-based reordering
constraints.
We trained two 5-gram language models with
Kneser-Ney smoothing for each of the target lan-
guages. One is trained on the target side of the par-
allel text, the other is on a corpus provided by the
evaluation: the Gigaword corpus for Chinese and
news corpora for the others. Besides standard fea-
tures (Och and Ney, 2004), the phrase-based decoder
also uses a Maximum Entropy phrasal reordering
model (Zens and Ney, 2006). Both the hierarchi-
cal decoder and the forest-to-string decoder only use
the standard features. For feature weight tuning, we
do Minimum Error Rate Training (Och, 2003). To
explore a larger n-best list more efficiently in train-
ing, we adopt the hypergraph-based MERT (Kumar
et al, 2009).
To evaluate the translation results, we use BLEU
(Papineni et al, 2002).
5.2 Translation Results
Table 2 shows the scores of our system with the
best binarization scheme compared to the phrase-
based system and the hierarchical phrase-based sys-
tem. Our system is consistently better than the other
two systems in all data sets. On the English-Chinese
data set, the improvement over the phrase-based sys-
tem is 1.3 BLEU points, and 0.8 over the hierarchi-
cal phrase-based system. In the tasks of translat-
ing to European languages, the improvements over
the phrase-based baseline are in the range of 0.5 to
1.0 BLEU points, and 0.3 to 0.5 over the hierar-
chical phrase-based system. All improvements ex-
cept the bf2s and hier difference in English-Czech
are significant with confidence level above 99% us-
ing the bootstrap method (Koehn, 2004). To demon-
strate the strength of our systems including the two
baseline systems, we also show the reported best re-
sults on these data sets from the 2010 WMT work-
shop. Our forest-to-string system (bf2s) outperforms
or ties with the best ones in three out of four lan-
guage pairs.
5.3 Different Binarization Methods
The translation results for the bf2s system in Ta-
ble 2 are based on the cyk binarization algorithm
with bracket violation degree 2. In this section, we
BLEU
dev test
English-Chinese pb 29.7 39.4
hier 31.7 38.9
bf2s 31.9 40.7??
English-Czech wmt best - 15.4
pb 14.3 15.5
hier 14.7 16.0
bf2s 14.8 16.3?
English-French wmt best - 27.6
pb 24.1 26.1
hier 23.9 26.1
bf2s 24.5 26.6??
English-German wmt best - 16.3
pb 14.5 15.5
hier 14.9 15.9
bf2s 15.2 16.3??
English-Spanish wmt best - 28.4
pb 24.1 27.9
hier 24.2 28.4
bf2s 24.9 28.9??
Table 2: Translation results comparing bf2s, the
binarized-forest-to-string system, pb, the phrase-based
system, and hier, the hierarchical phrase-based system.
For comparison, the best scores from WMT 2010 are also
shown. ?? indicates the result is significantly better than
both pb and hier. ? indicates the result is significantly
better than pb only.
vary the degree to generate forests that are incremen-
tally augmented from a single tree. Table 3 shows
the scores of different tree binarization methods for
the English-Chinese task.
It is clear from reading the table that cyk-2 is the
optimal binarization parameter. We have verified
this is true for other language pairs on non-standard
data sets. We can explain it from two angles. At
degree 2, we allow phrases crossing at most one
bracket in the original tree. If the parser is reason-
ably good, crossing just one bracket is likely to cover
most interesting phrases that can be translation units.
From another point of view, enlarging the forests
entails more parameters in the resulting translation
model, making over-fitting likely to happen.
5.4 Binarizer or Parser?
A natural question is how the binarizer-generated
forests compare with parser-generated forests in
translation. To answer this question, we need a
841
BLEU
rules dev test
no binarization 378M 28.0 36.3
head-out 408M 30.0 38.2
cyk-1 527M 31.6 40.5
cyk-2 803M 31.9 40.7
cyk-3 1053M 32.0 40.6
cyk-? 1441M 32.0 40.3
Table 3: Comparing different source tree binarization
schemes for English-Chinese translation, showing both
BLEU scores and model sizes. The rule counts include
normal phrases which are used at the leaf level during
decoding.
parser that can generate a packed forest. Our fast
deterministic dependency parser does not generate
a packed forest. Instead, we use a CRF constituent
parser (Finkel et al, 2008) with state-of-the-art ac-
curacy. On the standard Penn Treebank test set, it
achieves an F-score of 89.5%. It uses a CYK algo-
rithm to do full dynamic programming inference, so
is much slower. We modified the parser to do hyper-
edge pruning based on posterior probabilities. The
parser preprocesses the Penn Treebank training data
through binarization. So the packed forest it pro-
duces is also a binarized forest. We compare two
systems: one is using the cyk-2 binarizer to generate
forests; the other is using the CRF parser with prun-
ing threshold e?p, where p = 2 to generate forests.1
Although the parser outputs binary trees, we found
cross-bracket cyk-2 binarization is still helpful.
BLEU
dev test
cyk-2 14.9 16.0
parser 14.7 15.7
Table 4: Binarized forests versus parser-generated forests
for forest-to-string English-German translation.
Table 4 shows the comparison of binarization for-
est and parser forest on English-German translation.
The results show that cyk-2 forest performs slightly
1All hyper-edges with negative log posterior probability
larger than p are pruned. In Mi and Huang (2008), the thresh-
old is p = 10. The difference is that they do the forest pruning
on a forest generated by a k-best algorithm, while we do the
forest-pruning on the full CYK chart. As a result, we need more
aggressive pruning to control forest size.
better than the parser forest. We have not done full
exploration of forest pruning parameters to fine-tune
the parser-forest. The speed of the constituent parser
is the efficiency bottleneck. This actually demon-
strates the advantage of the binarizer plus forest-to-
string scheme. It is flexible, and works with any
parser that generates projective parses. It does not
require hand-tuning of forest pruning parameters for
training.
5.5 Synchronous Binarization
In this section, we demonstrate the effect of syn-
chronous binarization for both tree-to-string and
forest-to-string translation. The experiments are on
the English-Chinese data set. The baseline systems
use k-way cube pruning, where k is the branching
factor, i.e., the maximum number of nonterminals on
the right-hand side of any synchronous translation
rule in an input grammar. The competing system
does online synchronous binarization as described in
Section 4 to transform the grammar intersected with
the input sentence to the minimum branching factor
k? (k? < k), and then applies k?-way cube pruning.
Typically, k? is 2.
BLEU
dev test
head-out cube pruning 29.2 37.0
+ synch. binarization 30.0 38.2
cyk-2 cube pruning 31.7 40.5
+ synch. binarization 31.9 40.7
Table 5: The effect of synchronous binarization for tree-
to-string and forest-to-string systems, on the English-
Chinese task.
Table 5 shows that synchronous binarization does
help reduce search errors and find better translations
consistently in all settings.
6 Related Work
The idea of concatenating adjacent syntactic cate-
gories has been explored in various syntax-based
models. Zollmann and Venugopal (2006) aug-
mented hierarchial phrase based systems with joint
syntactic categories. Liu et al (2007) proposed tree-
sequence-to-string translation rules but did not pro-
vide a good solution to place joint subtrees into con-
nection with the rest of the tree structure. Zhang et
842
al. (2009) is the closest to our work. But their goal
was to augment a k-best forest. They did not bina-
rize the tree sequences. They also did not put con-
straint on the tree-sequence nodes according to how
many brackets are crossed.
Wang et al (2007) used target tree binarization to
improve rule extraction for their string-to-tree sys-
tem. Their binarization forest is equivalent to our
cyk-1 forest. In contrast to theirs, our binarization
scheme affects decoding directly because we match
tree-to-string rules on a binarized forest.
Different methods of translation rule binarization
have been discussed in Huang (2007). Their argu-
ment is that for tree-to-string decoding target side
binarization is simpler than synchronous binariza-
tion and works well because creating discontinous
source spans does not explode the state space. The
forest-to-string senario is more similar to string-to-
tree decoding in which state-sharing is important.
Our experiments show that synchronous binariza-
tion helps significantly in the forest-to-string case.
7 Conclusion
We have presented a new approach to tree-to-string
translation. It involves a source tree binarization
step and a standard forest-to-string translation step.
The method renders it unnecessary to have a k-best
parser to generate a packed forest. We have demon-
strated state-of-the-art results using a fast parser and
a simple tree binarizer that allows crossing at most
one bracket in each binarized node. We have also
shown that reducing search errors is important for
forest-to-string translation. We adapted the syn-
chronous binarization technqiue to improve search
and have shown significant gains. In addition, we
also presented a new cube-pruning-style algorithm
for rule extraction. In the new algorithm, it is easy to
adjust the figure-of-merit of rules for extraction. In
the future, we plan to improve the learning of trans-
lation rules with binarized forests.
Acknowledgments
We would like to thank the members of the MT team
at Google, especially Ashish Venugopal, Zhifei Li,
John DeNero, and Franz Och, for their help and dis-
cussions. We would also like to thank Daniel Gildea
for his suggestions on improving the paper.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and Metrics(MATR),
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics. Revised August 2010.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Conference of the Association for
Computational Linguistics (ACL-05), pages 263?270,
Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 755?763,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, companion volume, pages 205?208, Sap-
poro, Japan.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08:
HLT, pages 959?967, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the 2004 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-04), pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06), pages 961?968, July.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proceedings of the 2004 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (NAACL-04).
843
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas (AMTA), Boston, MA.
Liang Huang. 2007. Binarization, synchronous bina-
rization, and target-side binarization. In Proceedings
of the NAACL/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation (SSST), pages 33?40,
Rochester, NY.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
46th Annual Conference of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL-08:HLT), Columbus, OH. ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03), Edmonton, Alberta.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 388?395, Barcelona, Spain, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 163?171, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Dekang Lin. 2004. A path-based transfer model for
machine translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING-04), pages 625?630, Geneva, Switzerland.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the International Conference
on Computational Linguistics/Association for Compu-
tational Linguistics (COLING/ACL-06), Sydney, Aus-
tralia, July.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of the 45th Annual Conference of the Associ-
ation for Computational Linguistics (ACL-07), Prague.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 206?214, Honolulu, Hawaii, Octo-
ber. Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of the 46th An-
nual Conference of the Association for Computational
Linguistics: Human Language Technologies (ACL-
08:HLT), pages 192?199.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings of
Coling 2004, pages 64?70, Geneva, Switzerland, Aug
23?Aug 27. COLING.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Com-
putational Linguistics (ACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics (ACL-02).
Arjen Poutsma. 2000. Data-oriented translation. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING-00).
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual Con-
ference of the Association for Computational Linguis-
tics (ACL-05), pages 271?279, Ann Arbor, Michigan.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Conference of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08:HLT), Columbus, OH.
ACL.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 746?
754, Prague, Czech Republic, June. Association for
Computational Linguistics.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
Association for Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the 2006 Meeting of the
844
North American chapter of the Association for Compu-
tational Linguistics (NAACL-06), pages 256?263, New
York, NY.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-08: HLT, pages 559?567, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence to
string translation model. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 172?180,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138?141, New York City, June. As-
sociation for Computational Linguistics.
845
