Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1571?1580,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Latent Document Re-Ranking 
 
 
Dong Zhou1,2                       Vincent Wade1 
1. University of Dublin, Trinity College, Dublin 2, Ireland 
2. School of Computer and Communication, Hunan University, Changsha, 
Hunan, China 
dongzhou1979@hotmail.com      Vincent.Wade@cs.tcd.ie 
 
 
  
 
Abstract 
 
The problem of re-ranking initial retrieval re-
sults exploring the intrinsic structure of docu-
ments is widely researched in information re-
trieval (IR) and has attracted a considerable 
amount of time and study. However, one of 
the drawbacks is that those algorithms treat 
queries and documents separately.  Further-
more, most of the approaches are predomi-
nantly built upon graph-based methods, which 
may ignore some hidden information among 
the retrieval set.   
This paper proposes a novel document re-
ranking method based on Latent Dirichlet Al-
location (LDA) which exploits the implicit 
structure of the documents with respect to 
original queries. Rather than relying on graph-
based techniques to identify the internal struc-
ture, the approach tries to find the latent struc-
ture of ?topics? or ?concepts? in the initial re-
trieval set. Then we compute the distance be-
tween queries and initial retrieval results based 
on latent semantic information deduced. Em-
pirical results demonstrate that the method can 
comfortably achieve significant improvement 
over various baseline systems.  
1 Introduction 
Consider a traditional IR problem, where there 
exists a set of documents ? in the collection. In 
response to an information need (as expressed in 
a query ?), the system determines a best fit be-
tween the query and the documents and returns a 
list of retrieval results, sorted in a decreasing or-
der of their relevancy. In practice, high precision 
at the top rankings of the returned results is of 
particular interest. Generally, there are two ways 
to automatically assist in achieving this ultimate 
goal after an initial retrieval process (Baeza-
Yates and Ribeiro-Neto, 1999): document re-
ranking and query expansion/re-weighting. Since 
the latter normally need a second round of re-
trieval process, our method focuses on the docu-
ment re-ranking approach.  We will focus on ad-
justing the ranking positions directly over initial 
retrieval results set ????? .  
Recently, there is a trend of exploring the hid-
den structure of documents to re-rank results. 
Some of the approaches represent the document 
entities as a connected graph ?. It is usually con-
structed by links inferred from the content in-
formation as a nearest-neighbor graph. For ex-
ample, Zhang et al (2005) proposed an affinity 
ranking graph to re-rank search results by opti-
mizing diversity and information richness. Kur-
land and Lee (2005) introduced a structural re-
ranking approach by exploiting asymmetric rela-
tionships between documents induced by lan-
guage models. Diaz (2005); Deng et al (2009) 
use a family of semi-supervised machine learn-
ing methods among documents graph con-
structed by incorporating different evidences.  
However in this work we are more interested in 
adopting an automatic approach.  
There are two important factors that should be 
taken into account when designing any re-
ranking algorithms: the original queries and ini-
tial retrieval scores. One of issues is that pre-
vious structural re-ranking algorithms treat the 
query and the content individually when compu-
ting re-ranking scores. Each document is as-
signed a score independent of other documents 
without considering of queries. The problem we 
want to address in this paper is how we can leve-
rage the interconnections between query and 
documents for the re-ranking purpose.    
Another problem with such approaches con-
cerns the fundamental re-ranking strategy they 
adopted. HITS (Kleinberg, 1999) and PageRank 
1571
(Brin and Page, 1998) style algorithms were 
widely used in the past. However, approaches 
depend only on the structure of the global graph 
or sub-graph may ignore important information 
content of a document entity. As pointed out by 
Deng et al (2009), re-ranking algorithms that 
rely only on the structure of the global graph are 
likely lead to the problem of topic drift.  
Instead, we introduce a new document re-
ranking method based on Latent Dirichlet Allo-
cation (LDA) (Blei et al, 2003) which exploits 
implicit structure of the documents with respect 
to original queries. Rather than relying on graph-
based techniques to identify the internal struc-
ture, the approach tries to directly model the la-
tent structure of ?topics? or ?concepts? in the 
initial retrieval set. Then we can compute the 
distance between queries and initial retrieval re-
sults based on latent semantic information in-
ferred. To prevent the problem of topic drift, the 
generative probability of a document is summed 
over all topics induced. By combining the initial 
retrieval scores calculated by language models, 
we are able to gather important information for 
re-ranking purposes. The intuition behind this 
method is the hidden structural information 
among the documents: similar documents are 
likely to have the same hidden information with 
respect to a query. In other words, if a group of 
documents are talking about the same topic 
which shares a strong similarity with a query, in 
our method they will get alocated similar rank-
ing as they are more likely to be relevant to the 
query. In addition, the refined ranking scores 
should be relevant to the initial ranking scores, 
which, in our method, are combined together 
with the re-ranking score either using a linear 
fashion or multiplication process.  
To illustrate the effectiveness of the proposed 
methodology, we apply the framework to ad-hoc 
document retrieval and compare it with the initial 
language model-based method and other three 
PageRank style re-ranking methods. Experimen-
tal results show that the improvement brought by 
our method is consistent and promising. 
The rest of the paper is organized as follows. 
Related work on re-ranking algorithms and LDA 
based methods is briefly summarized in Section 
2. Section 3 describes the re-ranking framework 
based on latent information induced together 
with details of how to build generative model. In 
Section 4 we report on a series of experiments 
performed over three different test collections in 
English and French as well as results obtained. 
Finally, Section 5 concludes the paper and specu-
lates on future work.  
2 Related Work 
There exist several groups of related work in the 
areas of document retrieval and re-ranking.  
The first category performs re-ranking by us-
ing inter-document relationship (Lee et al, 
2001), evidences obtained from external re-
sources (Kamps, 2004), or through local context 
analysis (Xu and Croft, 2000). In the past, docu-
ment distances (Balinski and Daniowicz, 2005), 
manually built external thesaurus (Qu et al, 
2001), and structural information (such as docu-
ment title) (Luk and Wong, 2004), etc have been 
used extensively for this very purpose.   
A second category of work is related to recent 
advances in structural re-ranking paradigm over 
graphs. Kurland and Lee performed re-ranking 
based on measures of centrality in the graph 
formed by generation links induced by language 
model scores, through a weighted version of Pa-
geRank algorithm (Kurland and Lee, 2005) and 
HITS-style cluster-based approach (Kurland and 
Lee, 2006). Zhang et al (2005) proposed a simi-
lar method to improve web search based on a 
linear combination of results from text search 
and authority ranking. The graph, which they 
named affinity graph, shares strong similarities 
with Kurland and Lee?s work with the links in-
duced by a modified version of cosine similarity 
using the vector space model. Diaz (2005) used 
score regularization to adjust document retrieval 
rankings from an initial retrieval by a semi-
supervised learning method. Deng et al (2009) 
further developed this method. They built a latent 
space graph based on content and explicit links 
information. Unlike their approach we are trying 
to model the latent information directly.   
This work is also related to a family of me-
thods so called latent semantic analysis (LSA) 
(Landauer et al, 1998), especially topic models 
used for document representation. Latent Dirich-
let Allocation (LDA), after it was first introduced 
by Blei et al (2003), has quickly become one of 
the most popular probabilistic text modeling 
techniques and has inspired research ranging 
from text classification and clustering (Phan et 
al., 2008), information discovery (Mei et al, 
2007; Titov and McDonald, 2008) to information 
retrieval (Wei and Croft, 2006). In this model, 
each topic is represented by a set of words and 
each word corresponds with a weight to measure 
its contribution to the topic. Wei and Croft 
1572
(2006) described large-scale information retriev-
al experiments by using LDA. In their work, 
LDA-based document model and language mod-
el-based document model were linearly com-
bined to rank the entire corpus. However, unlike 
this approach we only apply LDA to a small set 
of documents. There are two reasons by doing 
so. One is the concern of computational cost. 
LDA is a very complex model and the complexi-
ty will grow linearly with the number of topics 
and the number of documents. Only running it 
through a document set significantly smaller than 
the whole corpus has obvious advantages. Se-
condly, it is well known that LSA-based method 
suffers from an incremental build problem. Nor-
mally adding new documents to the corpus needs 
to ?be folded in? to the latent representation.  
Such incremental addition fails to capture the co-
occurrences of the newly added documents (and 
even ignores all new terms they contain). As 
such, the quality of the LSA representation will 
degrade as more documents are added and will 
eventually require a re-computation of the LSA 
representation. Because our method only requires 
running LDA once for a small number of docu-
ments, this problems could be easily avoided.  In 
addition, we also introduce two new measures to 
calculate the distance between a query and a 
document. 
 
3 Latent Re-Ranking Framework 
In this section, we describe a novel document re-
ranking method based on extracting the latent 
structure among the initial retrieval set and mea-
suring the distance between queries and docu-
ments.  
3.1 Problem Definition 
Let ? = {?1,?2,? ,??} denote the set of docu-
ments to be retrieved. Given a query ?, a set of 
initial results ????? ? ?  of top documents are 
returned by a standard information retrieval 
model (initial ranker). However, the initial ranker 
tends to be imperfect. The purpose of our re-
ranking method is to re-order a set of documents  
?????
?  so as to improve retrieval accuracy at the 
very top ranks of the final results.  
3.2 Latent Dirichlet Allocation 
We will first introduce Latent Dirichlet Alloca-
tion model which forms the basis of the re-
ranking framework that will be detailed in the 
next subsection. It was previously shown that co-
occurrence structure of terms in text documents 
can be used to recover some latent topic struc-
tures without any usage of background informa-
tion (Landauer et al, 1998). This means that la-
tent-topic representations of text allow modeling 
of linguistic phenomena such as synonymy and 
polysemy. By doing so, information retrieval 
systems can match the information needs with 
content items on a meaning level rather than by 
just lexical congruence. 
The basic generative process of LDA closely 
resembles PLSA (Hofmann, 1999). LDA extends 
PLSA method by defining a complete generative 
model of text. The topic mixture is drawn from a 
conjugate Dirichlet prior that remains the same 
for all documents. The process of generating a 
document corpus is as follows: 
1) Pick a multinomial distribution ?  ?  for 
each topic ? from a Dirichlet distribu-
tion with hyperparameter ? . 
2) For each document ? , pick a multi-
nomial distribution ? ? , from a Dirich-
let distribution with hyperparameter 
? . 
3) For each word token ? in document 
?, pick a topic ? ? {1??} from the 
multinomial distribution ? ? . 
4) Pick word ?  from the multinomial 
distribution ?  ? . 
Thus, the likelihood of generating a corpus is: 
 
? ?1,? ,?? |? ,?  
=  ?(? ? |? )
?
?=1
? ?(
?
?=1
?  ?|? )
?  ? ??  ? ? ?(?? |?,?  ?)
?
??=1
??
?=1
?? ???  ?  
Unlike PLSA model, LDA possesses fully 
consistent generative semantics by treating the 
topic mixture distribution as a ?-parameter hid-
den random variable.  LDA offers a new and in-
teresting framework to model a set of documents.  
The documents and new text sequences (for ex-
ample, queries) could be easily connected by 
?mapping? them to the topics in the corpus. In 
the next subsection we will introduce how to 
achieve this goal and apply it to document re-
ranking. 
LDA is a complex model and cannot be solved 
by exact inference. There are a few approximate 
inference techniques available in the literature: 
variational methods (Blei et al, 2003), expecta-
tion propagation (Griffiths and Steyvers, 2004) 
1573
and Gibbs sampling (Griffiths and Steyvers, 
2004). Gibbs sampling is a special case of Mar-
kov-Chain Monte Carlo (MCMC) simulation and 
often yields relatively simple algorithms. For this 
reason, we choose to use Gibbs sampling to es-
timate LDA. 
According to Gibbs sampling, we need to 
compute the conditional probability ?(?? |? ? ? ,?   ), 
where ?    denotes the vector of all words and ? ? ?  
denotes the vector of topic assignment except the 
considered word at position ? . This probability 
distribution can be derived as: 
? ?? ? ? ? ,?    =
?? ,? ?
? ? + ?? ?
( ??
? + ??)? 1
?
?=1
?
??? ,? ?
? + ??
( ???
? + ??)? 1
?
?=1
 
where ?? ,? ?
?  indicates the number of instances of 
word ??  assigned to topic ? = ? , not including 
the current token and ??? ,? ?
?  denotes the number 
of words in document ??  assigned to topic ? = ?, 
not including the current token.  
Then we can obtain the multinomial parameter 
sets: 
??? ,? =
???
? + ??
 ???
? +??=1 ??
 
?? ,? ? =
??
? ? + ?? ?
 ??
? +??=1 ??
 
The Gibbs sampling algorithm runs over three 
periods: initialization, burn-in and sampling. We 
do not tune to optimize these parameters because 
in our experiments the markov chain turns out to 
converge very quickly.  
3.3 LDA-based Re-Ranking 
Armed with this LDA methodology, we now 
describe the main idea of our re-ranking method. 
Given a set of initial results ????? , we are trying 
to re-measure the distance between the query and 
a document. In the vector space model, this dis-
tance is normally the cosine or inner product 
measure between two vectors. Under the proba-
bilistic model framework, this distance can be 
obtained from a non-commutative measure of the 
difference between two probability distributions.  
The distance used in our approach is the Kull-
back-Leibler (KL) divergence (Kullback and 
Leibler, 1951). Given two probability mass func-
tion  ? ?  and ?(?), the KL divergence (or rela-
tive entropy) between ? and ? is defined as: 
?(?| ? = ? ? ???
?
?(?)
?(?)
 
In terms of text sequences (either queries or 
documents), the probability distribution can be 
regarded as a probabilistic language model ??   
or ??  from each document ? or each query ?. In 
other words, it assumes that there is an underly-
ing language model which ?generates? a term 
(sequence) (Ponte and Croft, 1998). The unigram 
language model is utilized here. There are sever-
al ways to estimate the probabilities. Let 
?(? ? ?) denotes the number of times the term 
?  occurs in a document ?  (same idea can be 
used on a query). The Maximum-likelihood es-
timation (MLE) of ? with respect to ? is defined 
as: 
????? ?
?(? ? ?)
 ?(?? ? ?)? ?
 
Previous work in language-model-based in-
formation retrieval (Zhai and Lafferty, 2004) 
advocates the use of a Dirichlet-smoothed esti-
mation: 
????? ?
? ? ? ? + ? ? ?????
 ?(?? ? ?)? ? + ?
 
where smoothing parameter ?  controls the de-
gree of reliance on relative frequencies in the 
document corpus rather than on the counts in ?. 
The initial ranker that we choose to use later in 
the experiment computes the KL divergence be-
tween the ?????  and a modified version of 
????? (Zhai and Lafferty, 2001).  
Both estimations can be easily extended to dis-
tributions over text sequences by assuming that 
the terms are independent: 
????(?1?2 ???) ? ????(?? )
?
?=1
 
 
????(?1?2 ???) ? ???? (?? )
?
?=1
 
In the re-ranking setting, we estimate that the 
probability of a document ? generates ?, using a 
mixture model LDA. It uses a convex combina-
tion of a set of component distributions to model 
observations. In this model, a word  ? is gener-
ated from a convex combination of some hidden 
topics ?: 
???? ? = ? ? ? ?(?|?)
?
?=1
 
where each mixture model ?(?|?)  is a multi-
nomial distribution over terms that correspond to 
one of the latent topics ?. Similar to MLE and 
DIR estimations, this could be generated to give 
a distribution on a sequence of text: 
1574
????(?1?2 ???) ?  ???? (?? )
?
?=1  
   Then the distance between a query and a doc-
ument based on this model can be obtained. The 
first method we propose here adopts the KL di-
vergence between the query terms and document 
terms to compute a Re-Rank score ?????
??1: 
?????
??1 = ??(????(?)||???? ? ) 
This method also has the property of length-
normalization to ameliorate long document bias 
problems (Kurland and Lee, 2005).  
The second method also measures a KL diver-
gence between a query and a document, however, 
in a different way. As in the original LDA model, 
the multinomial parameter  ? ?  indicates the topic 
distribution of a document ? . Query ?  can be 
considered as topic estimation of a unknown 
document ?    . Thus by first randomly assigning 
topics to words and then performing a number of 
loops through the Gibbs sampling update, we 
have: 
? ?? ?  ? ? ,?    ; ? ? ? ,?    
=
?? ,? ?
? ? + ? ? ,? ?
? ? + ?? ?
( ??
? + ? ?
? + ??)? 1
?
?=1
?
?? ? ,? ?
? + ??
( ?? ?
? + ??)? 1
?
?=1
 
where ? ? ,? ?
? ?  counts the observations of word ??  
and topic ? in unseen document. Then the topic 
distribution for the query (just the unseen docu-
ment ? ?) is:  
? ? ? ,? =
?? ?
? + ??
 ?? ?
? +??=1 ??
 
so that the distance between a query ? and a doc-
ument ?  is defined as the KL divergence be-
tween the topic distributions of ?  and ? . Then 
the re-ranking score is calculated as: 
?????
??2 = ??(? ? ||? ?) 
   Thus we can re-rank the initial retrieved docu-
ments according to the scores acquired. However, 
as in other topic models, a topic in the LDA 
model represents a combination of words, and it 
may not be as precise a representation as words 
in language model. Hence we need to further 
consider how to combine initial retrieval scores 
with the re-ranking scores calculated. Two com-
bination methods will be presented in the next 
subsection. 
3.4 Combining Initial Retrieval Scores 
Motivated by the significant improvement ob-
tained by (Wei and Croft, 2006) and (Zhang et 
al., 2005), we formulate our method through a 
linear combination of the re-ranking scores based 
on initial ranker and the latent document re-
ranker, shown as follow: 
??1 = (1? ?) ? ?? + ? ? ?????
???  
where ?? denotes original scores returned by the 
initial ranker and ?  is a parameter that can be 
tuned with ? = 0 meaning no re-ranking is per-
formed. 
Another scheme considers a multiplication 
combination to incorporate the original score. It 
does not need to tune any parameters:  
??2 = ?? ? ?????
???  
This concludes our overview of the proposed 
latent re-ranking method. 
4 Evaluation 
   In this section, we will empirically study the 
effectiveness of the latent document re-ranking 
method over three different data collections.  
4.1 Experimental Setup 
Data The text corpus used in our experiment 
was made up from elements of the CLEF-2007 
and CLEF-2008 the European Library (TEL) 
collections1 written in English and French. These 
collections are described in greater detail in Ta-
ble 1. All of the documents in the experiment 
were indexed using the Lemur toolkit2. Prior to 
                                               
1 http://www.clef-campaign.org 
2 http://www.lemurproject.org 
Collection Contents Language Num of docs Size Queries 
BL 
(CLEF2008) 
British Library 
Data 
English 
(Main) 
1,000,100 1.2 GB 50 
BNF 
(CLEF2008) 
Biblioth?que Na-
tionale de France 
French (Main) 1,000,100 1.3 GB 50 
LAT 
(CLEF2007) 
Los Angeles 
Times 2002  
English 135,153 434 MB 50 
Table 1. Statistics of test collections 
1575
indexing, Porter's stemmer and a stopword list3 
were used for the English documents. We use a 
French analyzer4 to analyze French documents.  
   It is worth noting that the CLEF-2008 TEL 
data is actually multilingual: all collections to a 
greater or lesser extent contain records pointing 
to documents in other languages. However this is 
not a major problem because the majority of 
documents in the test collection are written in 
main languages of those test collections (BL-
English, BNF-French). Furthermore, documents 
written in different languages tend not to match 
the queries in main languages. Also the data is 
very different from the newspaper articles and 
news agency dispatches previously used in the 
CLEF as well as TREC5. The data tends to be 
very sparse. Many records contain only title, au-
thor and subject heading information; other 
records provide more detail. The average docu-
ment lengths are 14.66 for BL and 24.19 for 
BNF collections after pre-processing, respective-
ly. Please refer to (Agirre et al, 2008) for a more 
detailed discussion about this data. The reason 
we choose these data collections is that we 
wanted to test the scalability of the proposed me-
thod in different settings and over different 
guages. In addition we also select a more 
tional collection (LAT from CLEF2007) as a test 
base. 
   We also used the CLEF-2007 and CLEF-
2008 query sets. The query sets consist of 50 
topics in English for LAT, BL and in French for 
BNF, all of which were used in the experiment. 
Each topic is composed of several parts such as: 
Title, Description, Narrative. We chose to 
conduct Title+Description runs as queries. The 
queries are processed similarly to the treatment 
in the test collections. The relevance judgments 
are taken from the judged pool of top retrieved 
documents by various participating retrieval 
systems from previous CLEF workshops.  
We compare the proposed latent re-ranking 
method with four other approaches: the initial 
ranker, mentioned above, is a KL-divergence 
retrieval function using the language models. 
Three other baseline systems are: Kurland and 
Lee?s structural re-ranking approach (Recursive 
Weighted Influx + Language Model), chosen as 
it demonstrates the best performance in their pa-
per (Kurland and Lee, 2005), Zhang et al?s af-
finity graph-based approach (Zhang et al, 2005) 
                                               
3 ftp://ftp.cs.cornell.edu/pub/smart/ 
4 http://lucene.apache.org/ 
5 http://trec.nist.gov/ 
and a variant of Kurland and Lee?s work with 
links in the graph calculated by the vector-space 
model (cosine similarity as mentioned in (Kur-
land and Lee, 2005)). We denote these four sys-
tems as InR, RWILM, AFF, and VEC respective-
ly.  Furthermore, we denote the permutations of 
our methods as follows: LDA1: ??2 ???? ?????
??1 , 
LDA2: ??1 ???? ?????
??1  , LDA3: 
??2 ???? ?????
??2 , LDA4: ??1 ???? ?????
??2 . 
Because the inconsistency of the evaluation 
metrics employed in the past work, we choose to 
employ all of them to measure the effectiveness 
of various approaches. These include: mean av-
erage precision (MAP), the precision of the top 5 
documents (Prec@5), the precision of the top 10 
documents (Prec@10), normalized discounted 
cumulative gain (NDCG) (Jarvelin and Kekalai-
nen, 2002) and Bpref (Buckley and Voorhees, 
2004). Statistical-significant differences in per-
formance were determined using a paired t-test at 
a confidence level of 95%. 
It is worth pointing out that the above meas-
urements are not directly comparable with those 
of the CLEF participants because we restricted 
our initial pool to a smaller number of documents 
and the main purpose in the paper is to compare 
the proposed method with different baseline sys-
tems. 
 
Parameter Two primary parameters need to 
be determined in our experiments. For the re-
ranking experiments, the combination parameter 
? must be defined. For the LDA estimation, the 
number of topics ? must be specified. We opti-
mized settings for these parameters with respect 
to MAP, not with all other metrics over the BL 
collection and apply them to all three collections 
directly. 
   The search ranges for these two parameters 
were: 
? :     0.1, 0.2, ?, 0.9 
k :     5, 10, 15, ?, 45 
   As it turned out, for many instances, the optim-
al value of ? with respect to MAP was either 0.1 
or 0.2, suggesting the initial retrieval scores have 
valuable information inside them. In contrast, the 
optimal value of ? was between 20 and 40. Al-
though this demonstrates a relatively large va-
riance, the differences in terms of MAP have 
remained small and statistically insignificant. We 
set ?????  to 50 in all results reported, as in Kur-
land and Lee?s paper (Kurland and Lee, 2005) 
and we later show that the performance turns out 
to be very stable when this set enlarged.  
1576
 
Table 2. Experimental Results. For each evaluation setting, improvements over the RWILM baseline 
are given in italics (because it has highest performance); statistically significant differences between our 
methods and InR, RWILM, AFF, VEC are indicated by o, l, a, v, respectively. Bold highlights the best 
results over all algorithms. 
MAP Prec@5 Prec@10 NDCG Bpref
InR 0.1913 0.52 0.452 0.3489 0.2287
RWILM
0.2152
0.532 0.468 0.3663 0.2242
AFF 0.1737 0.444 0.434 0.3273 0.22
VEC 0.1756 0.448 0.434 0.3258 0.2216
LDA1 0.21 o, a, v 0.544 a, v 0.47 0.3679 o, a, v 0.2429 a, v
LDA2 0.2148 o, a, v 0.58 o, a, v 0.5 o, a, v 0.3726 o, a, v 0.2491 o, l, a, v
LDA3 0.1673 0.452 0.402 0.3297 0.2
LDA4 0.2035 o, a, v 0.548 a, v 0.468 a, v 0.3626 o, a, v 0.2326 a
MAP Prec@5 Prec@10 NDCG bpref
InR 0.1266 0.268 0.216 0.2456 0.1482
RWILM 0.1274 0.264 0.218 0.2495 0.1498
AFF 0.108 0.248 0.21 0.2221 0.1404
VEC 0.1126 0.252 0.214 0.2262 0.1463
LDA1 0.1374 a, v 0.292 a 0.242 0.2544 a, v 0.1617
LDA2 0.1452 o, a, v 0.292 a, v 0.244 a 0.2608 o, a, v 0.1697 o, l, a, v
LDA3 0.1062 0.232 0.202 0.2226 0.1439
LDA4 0.1377 a,v 0.28 a 0.246 o, a, v 0.2507 a, v 0.1672 o, a, v
MAP Prec@5 Prec@10 NDCG bpref
InR 0.3119 0.568 0.48 0.5093 0.3105
RWILM 0.3097 0.556 0.478 0.5096 0.3064
AFF 0.3065 0.572 0.492 0.5037 0.312
VEC 0.301 0.536 0.474 0.4975 0.3087
LDA1 0.3253 v 0.584 v 0.502 v 0.5158 v 0.3339 o, l, v
LDA2 0.3271 a, v 0.584 o, v 0.496 0.518 o, v 0.3351 o, l, a, v
LDA3 0.2848 0.444 0.398 0.486 0.2879
LDA4 0.3274 o 0.552 0.478 0.5202 o, v 0.3396 o, l, v
BL
BNF
LAT02
   Lastly, the parameters in the baseline systems 
are set according to the tuning procedures in their 
original papers6. 
                                               
6 More specifically, the combination parameter was 
set to 0.5 for AFF, the number of links was set to 4 for 
RWILM. 
4.2 Results 
Primary Evaluation The main experimental 
results are presented in Table 2. The first four 
rows in each collection specify reference-
comparison data. The first question we are inter-
ested in is how our latent re-ranking methods 
1577
perform (taken as a whole). It is shown that our 
methods bring improvements upon the various 
baselines in 75% of the 48 relevant comparisons 
(4 latent re-ranking methods ? 4 corpora ? 4 
baselines). Only the algorithm permutation 
LDA3 performs less well. Furthermore, our me-
thods are able to achieve the highest performance 
across all the evaluation metrics over three test 
collections except in one case (MAP in BL col-
lection). An even more exciting observation is 
that in many cases, our methods, even though 
tuned for MAP, can outperform various baselines 
for all the evaluation metrics, with statistically 
significant improvements in many runs.  
A closer examination of the results in Table 2 
reveals some interesting properties. As expected, 
the RWILM method bought improvements in 
many cases in CLEF-2008 test collections. How-
ever, the performance over CLEF-2007 collec-
tion was somewhat disappointing. This seems to 
indicate that the language model induced graph 
method tends to perform better in sparse data 
rather than longer documents. Also Language 
Modeling requires large set training data to be 
effective, while the complexity of our method is 
only linear with number of topics and the number 
of documents for each iteration.  The affinity and 
vector graph based methods demonstrated poor 
performance across all the collections. This may 
be due to the fact that the approach Zhang et al 
(Zhang et al, 2005) developed focuses more on 
diversity and information richness and cares less 
about the precision of the retrieval results while 
asymmetric graph as constructed by the vector 
space model fails in capturing important relation-
ship between the documents. 
Another observation we can draw from Table 
2 is that the relative performance tends to be sta-
ble during test collections written in different 
languages. This shows a promising future for 
studying structure of the documents with respect 
to queries for re-ranking purpose.  At the same 
time, efficiency is always an issue in all re-
ranking methods. Although this is not a primary 
concern in the current work, it would definitely 
worth thinking in the future. 
We also conducted some experiments over 
queries constructed by using Title field only. 
This forms some more realistic short queries. 
The experiments showed very similar results 
compared to longer queries. This demonstrates 
that the query length is a trivial issue in our me-
thods (as in other graph-based structural re-
ranking). We examined the best and worse per-
formed queries, their performance are generally 
consistent across all the methods. This phenome-
non should be investigated further in the follow 
up evaluation. 
 
Comparison of Different Methods In com-
parison of performance between four permuta-
tions of our methods, LDA2 is the clear winner 
over CLEF-2008 test collections. The results ob-
tained by LDA2 and LDA4 over CLEF-2007 test 
collection were mixed. LDA2 performed better 
in precision at top ? documents while LDA4 
showed promising results in terms of more gen-
eral evaluation metrics. On the other hand, the 
linear combination approach performed much 
better than multiplication based combination. 
The situation is even worse when we adopted the 
?????
??2  method, which was inferior in several 
cases. Thus the linear combination should be 
highly recommended.  
 
Scalability We have shown that our latent 
document re-ranking method is successful at ac-
complishing the goal of improving the results 
returned by an initial retrieval engine. But one 
may raise a question of whether it is necessary to 
restrict our attention to an initial pool  ?????  at 
such a small size. As it happens, preliminary ex-
periments with LDA2 on larger size of the initial 
pool are presented in Figure 1. As we can see, 
our method can bring consistently stable im-
provements.  
 
 
 
Figure 1. Experiments with larger initial pools 
 
5 Conclusion and Future Work 
In this paper we proposed and evaluated a la-
tent document re-ranking method for re-ordering 
the initial retrieval results. The key to refine the 
results is finding the latent structure of ?topics? 
or ?concepts? in the document set, which leve-
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
50 100 200 300 400 500
Size of the initial retrieval set
MAP (InR) MAP (LDA2) Prec@5 (InR) Prec@5 (LDA2)
Prec@10 (InR) Prec@10 (LDA2) NDCG (InR) NDCG (LDA2)
B ref (InR) Bpref (LDA2)
1578
rages the latent Dirichlet alocation technique for 
the query-dependent ranking problem and results 
in state-of-art performance.  
There are many research directions we are 
planning to investigate. It has been shown that 
LDA-based retrieval is a promising method for 
ranking the whole corpus. There is a desire to 
call for a direct comparison between ranking and 
re-ranking using the proposed algorithmic varia-
tions. Future work will also include the compari-
son between our methods with other related ap-
proaches, such as Kurland and Lee?s cluster-
based approach (Kurland and Lee, 2006). 
There exist a sufficient number of latent se-
mantic techniques such as singular vector de-
composition, non-negative matrix factorization, 
PLSA, etc. We are planning to explore these me-
thods to compare their performance. Also direct 
re-ranking can be used to improve automatic 
query expansion since better ranking in top re-
trieved documents can be expected to improve 
the quality of the augmented query. We believe 
this is another fruitful line for future research. 
 
Acknowledgments 
  The authors would like to thank three anonym-
ous reviewers for many constructive comments. 
This research is supported by the Science Foun-
dation Ireland (Grant 07/CE/I1142) as part of the 
Centre for Next Generation Localisation 
(www.cngl.ie) at University of Dublin, Trinity 
College. 
References  
Eneko Agirre, Giorgio M. Di Nunzio, Nicola Ferro, 
Thomas Mandl and Carol Peters (2008). CLEF 
2008: Ad Hoc Track Overview. In Working notes of 
CLEF2008. 
Ricardo A. Baeza-Yates and Berthier Ribeiro-Neto 
(1999). Modern Information Retrieval, Addison-
Wesley Longman Publishing Co., Inc. 
Jaroslaw Balinski and Czeslaw Daniowicz (2005). 
"Re-ranking method based on inter-document dis-
tances." Inf. Process. Manage. 41(4): 759-775. 
David M. Blei, Andrew Y. Ng and Michael I. Jordan 
(2003). "Latent dirichlet alocation." J. Mach. 
Learn. Res. 3: 993-1022. 
Sergey Brin and Lawrence Page (1998). "The anato-
my of a large-scale hypertextual Web search en-
gine." Comput. Netw. ISDN Syst. 30(1-7): 107-117. 
Chris Buckley and Ellen M. Voorhees (2004). Re-
trieval evaluation with incomplete information. In 
Proceedings of the 27th annual international ACM 
SIGIR conference on Research and development in 
information retrieval, Sheffield, United Kingdom, 
ACM. p. 25-32. 
Hongbo Deng, Michael R. Lyu and Irwin King 
(2009). Effective latent space graph-based re-
ranking model with global consistency. In Proceed-
ings of the Second ACM International Conference 
on Web Search and Data Mining, Barcelona, Spain, 
ACM. p. 212-221. 
Fernando Diaz (2005). Regularizing ad hoc retrieval 
scores. In Proceedings of the 14th ACM interna-
tional conference on Information and knowledge 
management, Bremen, Germany, ACM. p. 672-679. 
Thomas L. Griffiths and Mark Steyvers (2004). Find-
ing scientific topics. In Proceeding of the National 
Academy of Sciences. p. 5228-5235. 
Thomas Hofmann (1999). Probabilistic latent seman-
tic indexing. In Proceedings of the 22nd annual in-
ternational ACM SIGIR conference on Research 
and development in information retrieval, Berkeley, 
California, United States, ACM. p. 50-57. 
Kalervo Jarvelin and Jaana Kekalainen (2002). "Cu-
mulated gain-based evaluation of IR techniques." 
ACM Trans. Inf. Syst. 20(4): 422-446. 
Jaap Kamps (2004). Improving Retrieval Effective-
ness by Reranking Documents Based on Controlled 
Vocabulary In Proceedings of 26th European Con-
ference on IR Research, ECIR 2004, Sunderland, 
UK. p. 283-295. 
Jon M. Kleinberg (1999). "Authoritative sources in a 
hyperlinked environment." J. ACM 46(5): 604-632. 
S. Kullback and R. A. Leibler (1951). "On Informa-
tion and Sufficiency." The Annals of Mathematical 
Statistics 22(1): 79-86. 
Oren Kurland and Lillian Lee (2005). PageRank 
without hyperlinks: structural re-ranking using links 
induced by language models. In Proceedings of the 
28th annual international ACM SIGIR conference 
on Research and development in information re-
trieval, Salvador, Brazil, ACM. p. 306-313. 
Oren Kurland and Lillian Lee (2006). Respect my 
authority!: HITS without hyperlinks, utilizing clus-
ter-based language models. In Proceedings of the 
29th annual international ACM SIGIR conference 
on Research and development in information re-
trieval, Seattle, Washington, USA, ACM. p. 83-90. 
Thomas  K. Landauer, Peter  W. Foltz and Darrell 
Laham (1998). "An Introduction to Latent Semantic 
Analysis." Discourse Processes 25: 259-284. 
Kyung-Soon Lee, Young-Chan Park and Key-Sun 
Choi (2001). "Re-ranking model based on document 
clusters." Inf. Process. Manage. 37(1): 1-14. 
Robert W. P. Luk and K.F. Wong (2004). Pseudo-
Relevance Feedback and Title Re-ranking for Chi-
nese Information Retrieval. In Working Notes of the 
Fourth NTCIR Workshop Meeting, Tokyo, Japan, 
National Institute of Informatics. 
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su 
and ChengXiang Zhai (2007). Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In 
Proceedings of the 16th international conference on 
World Wide Web, Banff, Alberta, Canada, ACM. p. 
171-180. 
1579
Xuan-Hieu Phan, Le-Minh Nguyen and Susumu Ho-
riguchi (2008). Learning to classify short and sparse 
text \& web with hidden topics from large-scale data 
collections. In Proceeding of the 17th international 
conference on World Wide Web, Beijing, China, 
ACM. p. 91-100. 
Jay M. Ponte and W. Bruce Croft (1998). A language 
modeling approach to information retrieval. In Pro-
ceedings of the 21st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, Melbourne, Australia, ACM. p. 
275-281. 
Youli Qu, Guowei Xu and Jun Wang (2001). Rerank 
Method Based on Individual Thesaurus. In Proceed-
ings of the Second NTCIR Workshop on Research in 
Chinese & Japanese Text Retrieval and Text Sum-
marization, Tokyo, Japan, National Institute of In-
formatics. 
Ivan Titov and Ryan McDonald (2008). Modeling 
online reviews with multi-grain topic models. In 
Proceeding of the 17th international conference on 
World Wide Web, Beijing, China, ACM. p. 111-120. 
Xing Wei and W. Bruce Croft (2006). LDA-based 
document models for ad-hoc retrieval. In Proceed-
ings of the 29th annual international ACM SIGIR 
conference on Research and development in infor-
mation retrieval, Seattle, Washington, USA, ACM. 
p. 178-185. 
Jinxi Xu and W. Bruce Croft (2000). "Improving the 
effectiveness of information retrieval with local con-
text analysis." ACM Trans. Inf. Syst. 18(1): 79-112. 
Chengxiang Zhai and John Lafferty (2001). Model-
based feedback in the language modeling approach 
to information retrieval. In Proceedings of the tenth 
international conference on Information and know-
ledge management, Atlanta, Georgia, USA, ACM. 
p. 403-410. 
Chengxiang Zhai and John Lafferty (2004). "A study 
of smoothing methods for language models applied 
to information retrieval." ACM Trans. Inf. Syst. 
22(2): 179-214. 
Benyu Zhang, Hua Li, Yi Liu, Lei Ji, Wensi Xi, Wei-
guo Fan, Zheng Chen and Wei-Ying Ma (2005). 
Improving web search results using affinity graph. 
In Proceedings of the 28th annual international 
ACM SIGIR conference on Research and develop-
ment in information retrieval, Salvador, Brazil, 
ACM. p. 504-511. 
 
 
1580
Coling 2010: Poster Volume, pages 1524?1532,
Beijing, August 2010
Dual-Space Re-ranking Model for Document Retrieval 
Dong Zhou1, Seamus Lawless1, Jinming Min2, Vincent Wade1 
 
1. Center for Next Generation Localisation, University of Dublin, Trinity College 
2. Center for Next Generation Localisation, Dublin City University 
 
dongzhou1979@hotmail.com, seamus.lawless@scss.tcd.ie,  
jinming.min@googlemail.com, Vincent.Wade@sccs.tcd.ie 
 
Abstract 
The field of information retrieval still 
strives to develop models which allow 
semantic information to be integrated in 
the ranking process to improve perform-
ance in comparison to standard bag-of-
words based models. A conceptual 
model has been adopted in general-
purpose retrieval which can comprise a 
range of concepts, including linguistic 
terms, latent concepts and explicit 
knowledge concepts. One of the draw-
backs of this model is that the computa-
tional cost is significant and often in-
tractable in modern test collections. 
Therefore, approaches utilising concept-
based models for re-ranking initial re-
trieval results have attracted a consider-
able amount of study. This method en-
joys the benefits of reduced document 
corpora for semantic space construction 
and improved ranking results. However, 
fitting such a model to a smaller collec-
tion is less meaningful than fitting it into 
the whole corpus. This paper proposes a 
dual-space model which incorporates 
external knowledge to enhance the space 
produced by the latent concept method. 
This model is intended to produce 
global consistency across the semantic 
space: similar entries are likely to have 
the same re-ranking scores with respect 
to the latent and manifest concepts. To 
illustrate the effectiveness of the pro-
posed method, experiments were con-
ducted using test collections across dif-
ferent languages. The results demon-
strate that the method can comfortably 
achieve improvements in retrieval per-
formance. 
1 Introduction 
Information retrieval often suffers from the so 
called ?vocabulary mismatch? problem. A 
document may be semantically relevant to a 
query despite the fact that the specific query 
terms used and the terms found in the document 
completely or partially differ (Furnas et al, 
1987). Consequently, overlap with respect to 
linguistic terms should not be a necessary con-
dition in query-document similarity and meth-
ods relying on the bag-of-words model can dis-
play poor performance as a result. In order to 
overcome the vocabulary mismatch problem, 
several solutions have been suggested which 
exploit semantic relations between text units. 
Among these methods, the latent model, the 
explicit model and the mixed model are com-
monly employed.  
The latent model (Landauer et al, 1998; Blei 
et al, 2003) tries to directly model the internal 
structure of ?topics? or ?concepts? in the text 
data, thus building meaningful groups beyond 
single words. Typically some form of dimen-
sion reduction (Fodor, 2002) is applied to the 
data matrix to find such latent dimensions 
which correspond to concepts. In contrast, the 
explicit model (Gabrilovich and Markovitch, 
2007) indexes texts according to an external 
knowledge base. Typically the meaning of a 
piece of text is represented as a weighted vector 
of knowledge-based concepts derived from ex-
1524
ternal resources such as ODP 1  or Wikipedia 2 
articles. The mixed model (Serban et al, 2005) 
extends the bag-of-words vector by adding ex-
ternal categories derived from WordNet or simi-
lar thesaurus. Based upon these definitions, the 
explicit model and the mixed model are similar 
in nature but differ in their use of external 
knowledge sources.  
Models such as those described above, how-
ever, have well documented drawbacks. Firstly, 
these methods are very computationally com-
plex. In the latent model, complexity grows 
linearly with the number of dimensions and the 
number of documents. For example, the compu-
tational cost of singular value decomposition 
(SVD) is significant; no successful experiment 
has been reported with over one million docu-
ments (Manning et al, 2008). This has been the 
biggest obstacle to the widespread adoption of 
this kind of method. For the explicit and mixed 
model, the dimensions of projecting documents 
into the external knowledge space are often lim-
ited to ten thousand (Potthast et al, 2008) in 
order to facilitate the large size of the test col-
lections used. Another problem with the explicit 
model is that the documents are often distrib-
uted over thousands of dimensions in which the 
semantic relatedness will degrade dramatically. 
For example, in (Sorg and Cimiano, 2008) when 
the whole Wikipedia collection is adopted to 
build the space, one document is mapped to ten 
thousand dimensions, in which it may only have 
very few truly semantically related dimensions. 
The means of identifying these dimensions is 
not reported and this may significantly influence 
the retrieval performance.  
Therefore, researchers started to consider in-
tegrating the aforementioned models into 
smaller, controlled document collections to 
overcome these shortcomings and assist the re-
trieval process. (Zhou and Wade, 2009b) pro-
posed a Latent Dirichlet Allocation (LDA)-
based method to model the latent structure of 
?topics? deduced from the initial retrieval re-
sults. The scores obtained from this process are 
then combined with initial ranking scores to 
produce a re-ranked list of results that are supe-
rior to original ordering.  The method also en-
joys the benefits of fast and tractable latent se-
                                                 
1 http://www.dmoz.org/ 
2 http://www.wikipedia.org/ 
mantic computation and successfully avoids the 
incremental build problem (Landauer et al, 
1998) which commonly exists in latent semantic 
analysis (LSA) techniques.  
There is an important factor, however, that 
needs to be taken into account when applying 
this method. Due to the smaller corpus size, fit-
ting a latent model into this corpus is less mean-
ingful than fitting the same model into a large, 
web-scale corpus. This means that some form of 
justification has to be applied to achieve better 
performance. A simple approach to address this 
problem is to directly apply the explicit or 
mixed model into a controlled corpus to im-
prove ranking performance. A similar problem 
will arise in the latent model in this single se-
mantic space, resulting in limited improvements.  
To address the challenges described above, 
this paper proposes a dual-space model which 
incorporates external knowledge to enhance the 
semantic space produced by the latent concept 
method. This model is intended to produce 
global consistency across the semantic space: 
similar entries are likely to have the same re-
ranking scores with respect to the latent and 
manifest concepts. In other words: in this model, 
if a group of documents deal with the same 
topic induced from a dual semantic space which 
shares a strong similarity with a query, the 
documents will get alocated similar ranking as 
they are more likely to be relevant to the query. 
In the experiments carried out in this paper, 
the dual-space model is applied to ad-hoc 
document retrieval and compared with the ini-
tial language model-based ranker and single-
space model exploiting latent and explicit fea-
tures. The results show that the explicit model 
could only bring minor improvements over the 
initial ranker. The latent model delivered more 
significant improvements than the explicit 
model. Both, however, are outperformed by the 
dual-space model.  
The main contribution of this paper is to pro-
pose a dual-space semantic model for the re-
ranking problem, which aims to improve preci-
sion, especially of the most highly ranked re-
sults. Other contributions of the paper include 
proposing a novel way of applying the explicit 
model to the re-ranking problem, and perform-
ing a systematic comparison between different 
models.  
1525
The rest of this paper is organised as follows. 
Related work on re-ranking and concept-based 
methods is briefly summarised in Section 2. 
Section 3 describes the latent space model and 
explicit space model used in the framework de-
veloped by this research, Section 4 presents de-
tails of how to build the dual-space model. In 
Section 5 a report is provided on a series of ex-
periments performed over three different test 
collections written in English, French and Ger-
man. This report includes details of the results 
obtained. Finally, Section 6 concludes the paper 
and speculates on future work. 
2 Related Work 
There exist several strands of related work in 
the areas of re-ranking and concept-based 
document retrieval. 
A family of work on the structural re-ranking 
paradigm over different sized document corpora 
was proposed to refine initial ranking scores. 
Kurland and Lee performed re-ranking based on 
measures of centrality in the graph formed by 
the generation of links induced by language 
model scores, through a weighted version of the 
PageRank algorithm (Kurland and Lee, 2005) 
and a HITS-style cluster-based approach 
(Kurland and Lee, 2006). Zhang et al (Zhang et 
al., 2005) proposed a similar method to improve 
web search based on a linear combination of 
results from text search and authority ranking. 
The graph, which they named an ?affinity 
graph?, shares strong similarities with Kurland 
and Lee?s work where the links are induced by a 
modified version of cosine similarity using the 
vector space model. Diaz (Diaz, 2005) used 
score regularisation to adjust document retrieval 
rankings from an initial retrieval by a semi-
supervised learning method. Deng et al (Deng 
et al, 2009) further developed this method by 
building a latent space graph based on content 
and explicit link information. Unlike their ap-
proach this research attempts to model the ex-
plicit information directly.  
The latent concept retrieval model has a long 
history in information retrieval. (Dumais, 1993; 
Dumais, 1995) conducted experiments with la-
tent semantic indexing (LSI) on TREC3 docu-
ments and tasks. These experiments achieved 
                                                 
3 http:// trec.nist.gov 
precision at, or above, that of the median TREC 
participant. On about 20% of TREC topics this 
system was the top scorer, and reportedly 
slightly better than average results in compari-
son to standard vector spaces for LSI at about 
350 dimensions. (Hofmann, 1999) provides an 
initial probabilistic extension of the basic latent 
semantic indexing technique. A more satisfac-
tory formal basis for a probabilistic latent vari-
able model for dimensionality reduction is the 
LDA model (Blei et al, 2003), which is genera-
tive and assigns probabilities to documents out-
side of the training set. Wei and Croft (Wei and 
Croft, 2006) presented the first large-scale 
evaluation of LDA, finding it to significantly 
outperform the query likelihood model. (Zhou 
and Wade, 2009b; Zhou and Wade, 2009a) suc-
cessfully applied this method to document re-
ranking and achieved significant improvement 
over language model-based ranking and various 
graph-based re-ranking methods.  
The explicit concept model has recently at-
tracted much attention in the information re-
trieval community. Notably, explicit semantic 
analysis (ESA) has been proposed as an ap-
proach to computing semantic relatedness be-
tween words and thus, has a natural application 
in this field (Gabrilovich and Markovitch, 2007). 
In essence, ESA indexes documents with re-
spect to the Wikipedia article space, indicating 
how strongly a given word in the document is 
associated to a specific Wikipedia article. In this 
model, each article is regarded as a concept, an 
analogical unit used in the latent model. As in 
the latent model, two words or texts can be se-
mantically related in spite of not having any 
words in common. Specifically, this method has 
been widely adopted in cross-language informa-
tion retrieval (CLIR) as an approach to resolv-
ing  an extreme case of the vocabulary mis-
match problem, where queries and documents 
are written in different languages (Potthast et al, 
2008). (Anderka et al, 2009) showed that this 
approach has comparable performance to lin-
guistic matching methods. (Cimiano et al, 2009) 
compared this method with a latent concept 
model based on LSI/LDA and concluded that it 
will outperform the latent model if trained on 
Wikipedia articles.  
1526
3 Latent and Explicit Models 
In this section, an overview of the problem ad-
dressed by this paper is presented and the latent 
and explicit document re-ranking models are 
described in more detail. This section also dem-
onstrates how these models can be used in a re-
ranking setting. 
3.1 Problem Definition 
Let ? = {?1 ,?2 ,? ,??}  denote the set of 
documents to be retrieved. Given a query ?, a 
set of initial results ????? ? ? of top documents 
are returned by a standard information retrieval 
model (initial ranker). However, typically the 
performance of the initial ranker can be im-
proved upon. The purpose of the re-ranking 
method developed by this research is to re-order 
a set of documents  ?????
?  so as to improve re-
trieval accuracy at the most highly ranked re-
sults.  
3.2 Latent Concept Model 
The specific method used here is borrowed from 
(Zhou and Wade, 2009b), which is based on the 
LDA model. The topic mixture is drawn from a 
conjugate Dirichlet prior that remains the same 
for all documents. The process of generating a 
document corpus is as follows: 
1) Pick a multinomial distribution ?  ?  for each 
topic ?  from a Dirichlet distribution with 
hyperparameter ? . 
2) For each document ? , pick a multinomial 
distribution ? ? , from a Dirichlet distribution 
with hyperparameter ? . 
3) For each word token ? in document ?, pick 
a topic ? ? {1??}  from the multinomial 
distribution ? ? . 
4) Pick word ? from the multinomial distribu-
tion ?  ? . 
    LDA possesses fully consistent generative 
semantics by treating the topic mixture distribu-
tion as a ?-parameter hidden random variable.  
LDA offers a new and interesting framework to 
model a set of documents. The documents and 
new text sequences (for example, queries) can 
easily be connected by ?mapping? them to the 
topics in the corpus.  
    In a re-ranking setting, the probability that a 
document ?  generates ?  is estimated using a 
mixture model LDA. It uses a convex combina-
tion of a set of component distributions to 
model observations. In this model, a word  ? is 
generated from a convex combination of some 
hidden topics ?: 
???? ? = ? ? ? ?(?|?)
?
?=1
 
where each mixture model ?(?|?)  is a multi-
nomial distribution over terms that correspond 
to one of the latent topics ?. This could be gen-
erated to give a distribution on a sequence of 
text: 
????(?1?2???) ? ????(?? )
?
?=1
 
    Then the distance between a query and a 
document based on this model can be obtained. 
The method used here adopts the KL divergence 
(Baeza-Yates and Ribeiro-Neto, 1999) between 
the query terms and document terms to compute 
a Re-Rank score ?????
?? : 
?????
?? = ??(????(?)||???? ? ) 
    The final score is then obtained through a 
linear combination of the re-ranking scores 
based on the initial ranker and the latent docu-
ment re-ranker, shown as follows: 
????????
??? = ? ? ?? + (1? ?) ? ?????
??  
where ??  denotes original scores returned by 
the initial ranker and ? is a parameter that can 
be tuned with ? = 1 meaning no re-ranking is 
performed.  
Another well-known approach to the latent 
model is the LSI method. It is based on SVD, a 
technique from linear algebra. This method has 
not been reported anywhere previously for re-
ranking purposes. It has been included here to 
compare the effectiveness of different latent 
approaches. As a full SVD is a loss-free decom-
position of a matrix ?, which is decomposed 
into two orthogonal matrices ? and ? and a di-
agonal matrix ?. Estimating less singular values 
and their corresponding singular vectors leads to 
reduced dimensions resembling latent concepts 
so that documents are no longer represented by 
terms but by concepts. New documents (que-
ries) are represented in terms of concepts by 
folding them into the LSI model. Next, cosine 
similarities may be used to compute the similar-
ity between a query and a document to obtain 
?????
???  and combine it with the original score to 
produce the final re-ranking score: 
1527
????????
??? = ?? ? ?? + (1? ??) ? ?????
???  
3.3 Explicit Concept Model 
As an example of explicit concept model 
(Gabrilovich and Markovitch, 2007), explicit 
semantic analysis attempts to index or classify a 
given text ? with respect to a set of explicitly 
given external categories. The basic idea is to 
take as input a document ? and map it to a high-
dimensional, real-valued vector space. This 
space is spanned by a Wikipedia database 
?? = {?1 ,? ,??}.This mapping is given by the 
following function: 
?? :? ? ?
|??| 
?? ? ?  ?1 ,? , ?|??|  
Where |?? |  is the number of articles in 
Wikipedia ??  corresponding to language ?. The 
value ??  in the vector ? expresses the strength of 
association between ? and the Wikipedia article 
??  and is defined as the cosine similarity: 
?????
??? =
 ?, ?? 
? ? ?? ?? ?
 
As pointed out in section 1, documents are 
often distributed over thousands of dimensions 
in which the semantic relatedness will degrade 
dramatically. The main purpose is to find the 
most relevant dimensions with respect to que-
ries. To apply this method to re-ranking, ??  is 
limited to the number of highly relevant docu-
ments for a given query. In other words, the en-
tire set of Wikipedia articles in language ?  is 
retrieved, and only return a specific number of 
documents as in ?? . This modification will also 
lead to fast computation of scores compared to 
scanning through the whole Wikipedia collec-
tion.  
 Similar to the latent model described above, 
the final ranking score is defined as: 
??????????
??? = ? ? ?? + (1? ?) ? ?????
???  
4 Dual space model 
Armed with the latent and explicit models, the 
dual-space model proposed by this paper is now 
described. In order to make a direct connection 
between the two models, the key point is to 
make the dimensions comparable across differ-
ent models. The detail presented on the latent 
and explicit concept models in the previous sec-
tion did not describe how to define a specific 
number of dimensions. A simple assumption is 
taken here in the dual-space model: the number 
of dimensions produced by the explicit model 
has to correspond to the number of dimensions 
induced by the latent model. As the same group 
of documents are being mapped into two differ-
ent semantic spaces, it is assumed that the con-
cepts induced by the latent model reflect the 
hidden structures in this document collection. 
Therefore, the same phenomenon should be ob-
served when applying the explicit model and 
vice-versa. Based on this assumption, the dual-
space model could be conducted so as to make a 
constraint: 
 ?? = ? 
and the final ranking score for this dual 
space is: 
??????
??? = ? ? ??+  1? ?? ? ? ?????
??
+ ? ? ?????
??? 
or 
??????
??? = ? ? ??+  1? ?? ? ? ?????
???
+ ? ? ?????
??? 
4 Experiments and Results 
In this section, an empirical study of the effec-
tiveness of the dual-space model over three data 
collections written in English, French and Ger-
man is presented. 
Collection Contents Language Num of docs Size Queries 
BL 
(CLEF2009) 
British Library 
Data 
English 
(Main) 
1,000,100 1.2 GB 50 
BNF 
(CLEF2009) 
Biblioth?que Na-
tionale de France 
French 
(Main) 
1,000,100 1.3 GB 50 
ONB 
(CLEF2009) 
Austrian National 
Library 
German 
(Main) 
869,353 1.3 GB 50 
Table 1. Statistics of test collections 
1528
4.1 Experimental Setup 
The text corpus used in the experiment de-
scribed below consisted of elements of the 
CLEF-20084 and CLEF-2009 European Library 
(TEL) collections 5  written in English, French 
and German. These collections are described in 
greater detail in Table 1. All of the documents 
in the experiment were indexed using the Ter-
rier toolkit6. Prior to indexing, Porter's stemmer 
and a stopword list7 were used for the English 
documents. A French and German analyser8 is 
used to analyse French and German documents.  
   It is worth noting that the CLEF TEL data is 
actually multilingual: all collections to a greater 
or lesser extent contain records pointing to 
documents in other languages. However this is 
not a major problem because the majority of 
documents in the test collection are written in 
the primary language of those test collections 
(BL-English, BNF-French, ONB-German). 
Please refer to (Ferro and Peters, 2009) for a 
more detailed discussion about this data. These 
collections were chosen to test the scalability of 
the proposed method in different settings and 
over different languages.  
   The CLEF-2008 and CLEF-2009 query sets 
were also used. Both query sets consist of 50 
topics in each language being tested. The 
CLEF-2008 queries written in English were 
used in training the parameters and all of the 
CLEF-2009 queries were used in the experiment 
for testing purposes. Each topic is composed of 
several parts, including: Title, Description and 
Narrative. Title+Description combinations 
were chosen as queries. The queries are proc-
essed similarly to the treatment of the test col-
lections. The relevance judgments are taken 
from the judged pool of top retrieved documents 
by various participating retrieval systems from 
previous CLEF workshops. The initial ranker 
used in this study is the classic vector space 
model. This was selected to facilitate the LSI 
and ESA models used and the main purpose of 
the experiments is to compare different models 
                                                 
4 The test collections used in CLEF-2008 and CLEF-
2009 are in fact identical. 
5 http://www.clef-campaign.org 
6 http://terrier.org 
7 ftp://ftp.cs.cornell.edu/pub/smart/ 
8 http://lucene.apache.org/ 
in addition to demonstrating the effectiveness of 
the dual-space model. 
A Wikipedia database in English, French and 
German was used as an explicit concept space. 
Only those articles that are connected via cross-
language links between all three Wikipedia da-
tabases were selected. A snapshot was obtained 
on the 29/11/2009, which contained an aligned 
collection of 220,086 articles in all three lan-
guages. 
The following evaluation metrics were cho-
sen to measure the effectiveness of the various 
approaches: mean average precision (MAP), the 
precision of the top 5 documents (Prec@5), the 
precision of the top 10 documents (Prec@10), 
normalised discounted cumulative gain (NDCG) 
and Bpref. Statistically-significant differences 
in performance were determined using a paired 
t-test at a confidence level of 95%. 
4.2 Parameter Tuning 
Three primary categories of parameter combina-
tions need to be determined in the experiments. 
For the latent re-ranking experiments, the pa-
rameters ?, ??  must be defined. For the explicit 
model the parameter ? must be chosen. For both 
models, the weights ?, ? have to be determined. 
In addition, the number of dimensions |?? | and 
? must be specified. Settings for these parame-
ters were optimised with respect to MAP over 
the BL collection using CLEF-2008 English 
queries and were applied to all three collections. 
This optimisation was not conducted for the 
other metrics used. 
    The search ranges for these two parameters 
were: 
?, ?? ,?, ?, ?:     0.1, 0.2, ?, 0.9 
  ??  ,?:     5, 10, 15, ?, 40 
Note that parameters ? and ? are the weights 
assigned to the latent model and the explicit 
model in the dual-space model. The choice of 
one will have direct influence over another. As 
it turned out, for many instances, the optimal 
value of ?, ??  with respect to MAP was either 
0.3 or 0.4, suggesting the initial retrieval scores 
still contain valuable information. In contrast, 
parameter ? shows no obvious difference in per-
formance when the value is above 0.1. With this 
observation, when setting the parameters ? and 
? more weight is assigned to the latent model 
rather than the explicit model. The optimal 
1529
 Dual space build upon LDA and ESA Dual space build upon LSI and ESA 
BL BL 
initial 
ranker 
latent 
space 
explicit 
space 
dual 
space 
initial 
ranker 
latent 
space 
explicit 
space 
dual 
space 
Precision@5 0.508 0.528 0.514 0.54* 0.508 0.54* 0.508 0.556* 
Precision@10 0.468 0.498* 0.47 0.508* 0.468 0.51* 0.48 0.512* 
Precision@20 0.408 0.424 0.41 0.435* 0.408 0.408 0.407 0.409 
NDCG 0.4053 0.4137* 0.4053 0.416* 0.4053 0.4145* 0.4055 0.4213* 
MAP 0.2355 0.2433* 0.2358 0.2499* 0.2355 0.2478* 0.236 0.2499* 
R-Precision 0.316 0.3243 0.3165 0.3248 0.316 0.3173 0.3202* 0.3232 
bpref 0.271 0.2746 0.2725 0.2812 0.271 0.2836* 0.2714 0.2879* 
 
BNF BNF 
initial 
ranker 
latent 
space 
explicit 
space 
dual 
space 
initial 
ranker 
latent 
space 
explicit 
space 
dual 
space 
Precision@5 0.376 0.368 0.372 0.376 0.376 0.376 0.376 0.384* 
Precision@10 0.346 0.352* 0.35 0.352 0.346 0.348 0.35 0.354* 
Precision@20 0.297 0.297 0.297 0.3* 0.297 0.303 0.299 0.3* 
NDCG 0.3162 0.3158 0.3156 0.3163 0.3162 0.317 0.3164 0.3178 
MAP 0.1621 0.1622 0.162 0.1634 0.1621 0.1629 0.1622 0.1624 
R-Precision 0.2274 0.2279 0.2211 0.2285 0.2274 0.2278 0.2264 0.2277 
bpref 0.1897 0.1899 0.1887 0.19 0.1897 0.1914 0.1892 0.1918 
 
ONB ONB 
initial 
ranker 
latent 
space 
explicit 
space 
dual 
space 
initial 
ranker 
latent 
space 
explicit 
space 
dual 
space 
Precision@5 0.38 0.388 0.36 0.404* 0.38 0.4 0.364 0.412* 
Precision@10 0.308 0.322 0.302 0.332* 0.308 0.324 0.302 0.324 
Precision@20 0.246 0.252 0.252 0.259* 0.246 0.247 0.251 0.252 
NDCG 0.3042 0.304 0.3059 0.3101 0.3042 0.3152* 0.3062 0.3154* 
MAP 0.1482 0.1524 0.1509 0.1567* 0.1482 0.1567* 0.1494 0.1578* 
R-Precision 0.2115 0.2152 0.2137 0.2175 0.2115 0.212 0.2106 0.2128 
bpref 0.1778 0.1871 0.1799 0.1896 0.1778 0.1833 0.1788 0.1832 
Table 2. Experimental Results. For each evaluation setting, statistically significant differences 
between different methods and the initial ranker are indicated by star. Bold highlights the best 
results over all algorithms. 
value of ? was between 25 and 35 for the LDA 
based model and between 5 and 15 for the LSI 
based model. Although this demonstrates a rela-
tively large variance, the differences in terms of 
MAP have remained small and statistically in-
significant. ?????  is set to 50 in all results re-
ported. 
4.3 Results 
Primary Evaluation The main experimental 
results, which describe the performance of the 
different re-ranking algorithms on the CLEF 
document collection, are shown in Table 2. The 
first four rows in each test collection specify the 
most important measurements because this re-
search is particularly interested in performance 
over the most highly ranked results. As illus-
1530
trated by the data, the initial ranker was always 
the lowest performer in terms of nearly all 
measurements. This indicates the need for re-
ranking. Using the method computed by the 
explicit space always led to an improvement in 
retrieval effectiveness. But this improvement is 
only minor in comparison to the other two mod-
els and the results are often statistically insig-
nificant. When the re-ranking score was calcu-
lated using the latent model, retrieval effective-
ness always exceeded initial ranker and the ex-
plicit model. There was a noticeable improve-
ment in retrieval effectiveness in the English 
collection (BL, statistically significant results 
were often observed), but a modest increase for 
the other two collections (BNF and ONB). 
The empirical results obtained using the dual 
space model are very promising. Pleasingly, 
both the LDA+ESA and LSI+ESA models out-
performed the basic latent and explicit space 
model in the majority of retrieval runs, with the 
best scores relating to the LSI-based models. An 
important phenomenon is that statistically sig-
nificant improvements are always recorded in 
the metrics which measure the most highly 
ranked results. An even more exciting observa-
tion is that in many cases, the dual-space model, 
even though tuned for MAP, can outperform 
various baselines and other models for all the 
evaluation metrics, with statistically significant 
improvements in many runs. 
Another observation that can be drawn from 
Table 2 is that the relative performance tends to 
be stable across test collections written in dif-
ferent languages. This indicates a promising 
future for studying document structure with re-
spect to latent and explicit semantic space for 
re-ranking purposes.   
 
The Comparison of Latent Methods  Table 2 
also shows a side-by-side comparison of the 
various performance measurements between the 
latent model used in this research on the CLEF-
2009 BL test collection. The LSI-based method 
appeared to outscore the LDA-based method in 
the latent model in the vast majority of cases, 
while the difference between the various scor-
ings was fairly marginal as both methods de-
liver statistically significant results. For the 
dual-space model, similar results were ob-
served. A possible reason is that the initial 
ranker used was based on the vector space 
model and LSI is also vector based. It shows 
that more research with respect to the latent 
model selection will be necessary in the future.  
 
Effectiveness of Explicit Methods As part of 
experimental objectives of this research, it was 
also necessary to test the newly developed ex-
plicit model for re-ranking. In the parameter 
tuning section, the explicit model displayed no 
obvious difference in terms of combination ef-
fectiveness. However, some variations could be 
observed when applying different dimensions 
where statistically significant results often ap-
pear in lower dimensions. This confirms the 
need to find more relevant dimensions, both for 
performance and efficiency purposes. 
5 Conclusion and Future Work 
This paper proposed and evaluated a dual-space 
document re-ranking method for re-ordering the 
initial retrieval results. The key to refining the 
results is the global consistency over the seman-
tic space, which leverages latent and explicit 
semantic information and results in state-of-art 
performance. This paper also proposed a novel 
way to apply the explicit model to the re-
ranking problem, and performed a systematic 
comparison between different models. 
Further investigation is planned in many re-
search directions. It has been shown that the 
latent model-based retrieval is a promising 
method for ranking the whole corpus. There is a 
desire to call for a direct comparison between 
ranking and re-ranking using the proposed algo-
rithmic variations. Future work will also include 
identifying improvements upon linear combina-
tion for engineering different models. At the 
same time, there exist a sufficient number of 
latent and explicit semantic techniques which 
will be explored to compare their performance.  
 
Acknowledgments 
The authors would like to thank the three 
anonymous reviewers for their many construc-
tive comments. This research is supported by 
the Science Foundation Ireland (Grant 
07/CE/I1142) as part of the Centre for Next 
Generation Localisation (www.cngl.ie) at Uni-
versity of Dublin, Trinity College and Dublin 
City University. 
1531
References 
Anderka, Maik, Nedim Lipka and Benno Stein. 2009. 
Evaluating Cross-Language Explicit Semantic 
Analysis and Cross Querying at TEL@CLEF 2009. 
In CLEF 2009 Workshop, Corfu, Greece. 
Baeza-Yates, Ricardo A. and Berthier Ribeiro-Neto. 
1999. Modern Information Retrieval, Addison-
Wesley Longman Publishing Co., Inc. 
Blei, David M., Andrew Y. Ng and Michael I. Jordan. 
2003. Latent dirichlet alocation. J. Mach. Learn. 
Res. 3: 993-1022. 
Cimiano, Philipp, Antje Schultz, Sergej Sizov, 
Philipp Sorg and Steffen Staab. 2009. Explicit 
versus latent concept models for cross-language 
information retrieval. In Proceedings of the 21st 
international jont conference on Artifical 
intelligence, Pasadena, California, USA, Morgan 
Kaufmann Publishers Inc. p. 1513-1518. 
Deng, Hongbo, Michael R. Lyu and Irwin King. 
2009. Effective latent space graph-based re-ranking 
model with global consistency. In Proceedings of 
the Second ACM WSDM conference, Barcelona, 
Spain, ACM. p. 212-221. 
Diaz, Fernando. 2005. Regularizing ad hoc retrieval 
scores. In Proceedings of the 14th ACM CIKM 
conference, Bremen, Germany, ACM. p. 672-679. 
Dumais, Susan T. 1993. Latent semantic indexing 
(LSI) and TREC-2. In Proceedings of TREC. p. 
105-115. 
Dumais, Susan T. 1995. Latent semantic indexing 
(LSI): TREC-3 report. In Proceedings of TREC. p. 
219-230. 
Ferro, Nicola and Carol Peters. 2009. CLEF 2009 Ad 
Hoc Track Overview: TEL & Persian Tasks. In 
Working notes of CLEF2008, Corfu, Greece. 
Fodor, Imola K. 2002. A Survey of Dimension 
Reduction Techniques.   
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=
10.1.1.8.5098. Accessed: 18th April 2010. 
Furnas, G. W. , T. K.  Landauer, L. M.  Gomez and S. 
T.  Dumais. 1987. The vocabulary problem in 
human-system communication. Commun. ACM 
30(11): 964-971. 
Gabrilovich, Evgeniy and Shaul Markovitch. 2007. 
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of 
the 20th international joint conference on Artifical 
intelligence, Hyderabad, India, Morgan Kaufmann 
Publishers Inc. p. 1606-1611. 
Hofmann, Thomas. 1999. Probabilistic latent 
semantic indexing. In Proceedings of the 22nd 
annual international ACM SIGIR conference, 
Berkeley, California, United States, ACM. p. 50-57. 
Kurland, Oren and Lillian Lee. 2005. PageRank 
without hyperlinks: structural re-ranking using 
links induced by language models. In Proceedings 
of the 28th annual international ACM SIGIR 
conference, Salvador, Brazil, ACM. p. 306-313. 
Kurland, Oren and Lillian Lee. 2006. Respect my 
authority!: HITS without hyperlinks, utilizing 
cluster-based language models. In Proceedings of 
the 29th annual international ACM SIGIR 
conference, Seattle, Washington, USA, ACM. p. 
83-90. 
Landauer, Thomas  K., Peter  W. Foltz and Darrell 
Laham. 1998. An Introduction to Latent Semantic 
Analysis. Discourse Processes 25: 259-284. 
Manning, Christopher D., Prabhakar Raghavan and 
Hinrich Schtze. 2008. Introduction to Information 
Retrieval, Cambridge University Press. 
Potthast, Martin, Benno Stein and Maik Anderka. 
2008. A Wikipedia-Based Multilingual Retrieval 
Model. In Proceedings of 30th European 
Conference on Information Retrieval, Glasgow, 
Scotland, Springer. p. 522-530. 
Serban, Radu, Annette ten Teije, Frank van 
Harmelen, Mar Marcos and Cristina Polo. 2005. 
Ontology-driven extraction of linguistic patterns for 
modelling clinical guidelines. Proceedings of the 
10th European Conference on Artificial 
Intelligence in Medicine (AIME-05). 
Wei, Xing and W. Bruce Croft. 2006. LDA-based 
document models for ad-hoc retrieval. In 
Proceedings of the 29th annual international ACM 
SIGIR conference, Seattle, Washington, USA, 
ACM. p. 178-185. 
Zhang, Benyu, Hua Li, Yi Liu, Lei Ji, Wensi Xi, 
Weiguo Fan, Zheng Chen and Wei-Ying Ma. 2005. 
Improving web search results using affinity graph. 
In Proceedings of the 28th annual international 
ACM SIGIR conference, Salvador, Brazil, ACM. p. 
504-511. 
Zhou, Dong and Vincent Wade. 2009a. Language 
Modeling and Document Re-Ranking: Trinity 
Experiments at TEL@CLEF-2009. In CLEF 2009 
Workshop, Corfu, Greece. 
Zhou, Dong and Vincent Wade. 2009b. Latent 
Document Re-Ranking. In Proceedings of the 2009 
Conference on Empirical Methods in Natural 
Language Processing, Singapore, ACL. p. 1571-
1580. 
1532
