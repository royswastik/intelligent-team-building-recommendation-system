CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 119?126
Manchester, August 2008
Easy as ABC? Facilitating Pictorial Communication
via Semantically Enhanced Layout
Andrew B. Goldberg, Xiaojin Zhu, Charles R. Dyer, Mohamed Eldawy, Lijie Heng
Department of Computer Sciences
University of Wisconsin, Madison, WI 53706, USA
{goldberg, jerryzhu, dyer, eldawy, ljheng}@cs.wisc.edu
Abstract
Pictorial communication systems convert
natural language text into pictures to as-
sist people with limited literacy. We define
a novel and challenging problem: picture
layout optimization. Given an input sen-
tence, we seek the optimal way to lay out
word icons such that the resulting picture
best conveys the meaning of the input sen-
tence. To this end, we propose a family
of intuitive ?ABC? layouts, which organize
icons in three groups. We formalize layout
optimization as a sequence labeling prob-
lem, employing conditional random fields
as our machine learning method. Enabled
by novel applications of semantic role la-
beling and syntactic parsing, our trained
model makes layout predictions that agree
well with human annotators. In addition,
we conduct a user study to compare our
ABC layout versus the standard linear lay-
out. The study shows that our semantically
enhanced layout is preferred by non-native
speakers, suggesting it has the potential to
be useful for people with other forms of
limited literacy, too.
1 Introduction
A picture is worth a thousand words?especially
when you are someone with communicative dis-
orders, a foreign language speaker, or a young
child. Pictorial communication systems aim to au-
tomatically convert general natural language text
into meaningful pictures. A perfect pictorial
c? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
communication system can turn signs and opera-
tion instructions into easy-to-understand graphical
forms; combined with optical character recogni-
tion input, a personal assistant device could create
such visual translations on-the-fly without the help
of a caretaker. Pictorial communication may also
facilitate literacy development and rapid browsing
of documents through pictorial summaries.
Pictorial communication research is in its in-
fancy with a spectrum of experimental systems,
which we review in Section 2. At one end of
the spectrum, some systems render highly realis-
tic 3D scenes but require specific scene-descriptive
language. At the other end, some systems per-
form dictionary-based iconic transliteration (turn-
ing words into icons1 one by one) on arbitrary text
but the pictures can be hard to understand. We are
interested in using pictorial communication as an
assistive communication tool. Thus, our system
needs to be able to handle general text yet produce
easy-to-understand pictures, which is in the middle
of the spectrum. To this end, our system adopts
a ?collage? approach (Zhu et al, 2007). Given a
piece of text (e.g., a sentence), it first identifies im-
portant and easy-to-depict words (or phrases) with
natural language processing (NLP) techniques. It
then finds one good icon per word, either from a
manually created picture-dictionary, or via image
analysis on image search results. Finally, it lays
out the icons to create the picture. Each step in-
volves several interesting research problems.
This paper focuses exclusively on the picture
layout component and addresses the following
question: Can we use machine learning and NLP
techniques to learn a good picture layout that im-
1In this paper, an icon refers to a small thumbnail image
corresponding to a word or phrase. A picture refers to the
overall large image corresponding to the whole text.
119
proves picture comprehension for our target audi-
ences of limited literacy? We first propose a sim-
ple yet novel picture layout scheme called ?ABC.?
Next, we design a Conditional Random Field-
based semantic tagger for predicting the ABC lay-
out. Finally, we conduct a user study contrasting
our ABC layout to the linear layout used in iconic
transliteration. The main contribution of this paper
is to introduce the novel task of layout prediction,
learned using linguistic features including Prop-
Bank role labels, part-of-speech tags, and lexical
features.
2 Prior Pictorial Communication Work
At one extreme, there has been significant prior
work on ?text-to-scene? type systems, which were
often intended to aid graphic designers in placing
objects in a 3D environment. Example systems in-
clude NALIG (Adorni et al, 1983), SPRINT (Ya-
mada et al, 1992), Put (Clay and Wilhelms,
1996), and others (Brown and Chandrasekaran,
1981). Perhaps the best known system of this type,
WordsEye (Coyne and Sproat, 2001), uses a large
manually tagged collection of 3D polyhedral mod-
els to create photo-realistic scenes. Similarly, Car-
Sim (Johansson et al, 2005) can create animated
scenes, but operates exclusively in the limited do-
main of reconstructing road accidents from traffic
reports. These systems cater to detailed descriptive
text with visual and spatial elements. They are not
intended as assistive tools to communicate general
text, which is our goal.
Several systems (Zhu et al, 2007; Mihalcea and
Leong, 2006; Joshi et al, 2006) attempt to bal-
ance language coverage versus picture sophistica-
tion. They perform some form of keyword selec-
tion, and select corresponding icons automatically
from a 2D image database. The result is a pictorial
summary representing the main idea of the origi-
nal text, but precisely determining the original text
by looking at the picture can be difficult.
At the other extreme, augmentative and alterna-
tive communication software allows users to in-
put arbitrary text. The words, and sometimes
common phrases, are semi-automatically translit-
erated into icons, and displayed in sequential or-
der. Users must learn special icons, which corre-
spond to function words, before the resulting pic-
tures can be fully understood. Examples include
SymWriter (Widgit Software, 2007) and Blissym-
bols (Hehner, 1980).
Other than explicit scene-descriptive languages,
pictorial communication systems have not suffi-
ciently addressed the issue of picture layout for
general text. We believe a good layout can better
communicate the text a picture is trying to convey.
The present work studies the use of a semantically
inspired layout to enhance pictorial communica-
tion. For simplicity, we restrict our attention to the
layout of a single sentence. We anticipate the use
of text simplification (Chandrasekar et al, 1996;
Vickrey and Koller, 2008) to convert complex text
into a set of appropriate inputs for our system.
3 The ABC Layout
A good picture layout scheme must be intuitive to
humans and easy to generate by computers. To
design such a layout, we conducted a pilot study.
Five human annotators produced free-hand pic-
tures of many sentences. Analyzing these pictures,
we found a large amount of agreement in the use
of arrows to mark actions and to provide structure
to what would otherwise be a jumble of icons.
Motivated by the pilot study, we propose a sim-
ple layout scheme called ABC. It features three
positions, referred to as A, B, and C. In addition,
an arrow points from A through B to C (Figure 1).
These positions are meant to denote certain seman-
tic roles: roughly speaking, A denotes ?who,? B
denotes ?what action,? and C denotes ?to whom,
for what.? Each position can contain any number
of icons, each representing a word or phrase in the
text. Words that do not play a significant role in
the text will be omitted from the ABC layout.
There are two main advantages of the ABC lay-
out:
1. The ABC positioning of icons allows users to
infer the semantic role of the corresponding con-
cepts. In particular, we found that verbs can be dif-
ficult to depict and understand without such hints.
The B position serves as an action indicator to dis-
ambiguate between multiple senses of the same
icon. For example, in Figure 1, the school bus icon
clearly represents the verb phrase ?rides the bus,?
rather than just the noun ?bus.?
2. Such a layout is particularly amenable to ma-
chine learning. Specifically, we can turn the prob-
lem of finding the optimal layout for an input sen-
tence into a sequence tagging problem, which is
well-studied in NLP.
120
The girl rides the bus to school in the morning
O A B B B O C O O B
Figure 1: Example ABC picture layout, original
text, and tag sequence.
3.1 ABC Layout as Sequence Tagging
Given an input sentence, one can assign each word
a tag from the set {A, B, C, O}. The bottom row in
Figure 1 shows an example tag sequence. The tag
specifies the ABC layout position of the icon cor-
responding to that word. Tag O means ?other? and
marks words not included in the picture. Within
each position, icons appear in the word order in the
input sentence. Therefore, a tag sequence uniquely
determines an ABC layout of the picture.
Finding the optimal ABC layout of the input
sentence is thus equivalent to computing the most
likely tag sequence given the input sentence. We
adopt a machine learning approach by training a
sequence tagger for this task. To do so, we need
to collect labeled training data in the form of sen-
tences with manually annotated tag sequences. We
discuss our annotation effort next, and present our
machine learning models in Section 4.
3.2 Human Annotated Training Data
We asked the five annotators to manually label 571
sentences compiled from several online sources,
including grade school texts about history and sci-
ence, children?s books, and recent news headlines.
Some sentences were written by the annotators and
describe daily activities. The annotators tagged
each sentence using a Web-based tool to drag-and-
drop icons into the desired positions in the layout2.
To gauge the quality of the manually labeled
data, and to understand the difficulty of the ABC
2The manual tagging actually employs a more detailed tag
set to denote phrase structure: Each A, B, or C tag is com-
bined with a modifier of b (begin phrase) or i (inside phrase).
For example, the phrase ?rides the bus? in Figure 1 is tagged
with Bb Bi Bi, and shares one icon. The icons were also
manually selected by the annotator from a list of Web image
search results.
layout, we computed inter annotator agreement
among three of the five annotators on a common
set of 48 sentences. Considering all pair-wise com-
parisons of the three annotators, the overall aver-
age tag agreement was 77%. This measures the to-
tal number of matching tags (across all sentences)
divided by the total number of tags. Matching
strictly requires both the correct tag and the correct
modifier. We also computed Fleiss? kappa, which
measures the degree of inter-annotator agreement
beyond the amount expected by chance (Fleiss,
1971). The values range from 0 to 1, with 1 indi-
cating perfect agreement. The kappa statistic was
0.71, which is often considered moderate to high
agreement.
Further inspection revealed that most disagree-
ment was due to annotators reversing A and C
tags. This could arise from interpreting passive
sentences in different ways or trying to represent
physical movement. For example, some annotators
found it more natural to depict eating by placing a
food item in A and the eater in C, treating the ar-
row as the transfer of food. It was also common for
annotators to disagree on whether certain adverbs
and time modifiers belong in B or in C. These dif-
ferences all suggest the highly subjective nature of
conceptualizing pictures from text.
4 A Conditional Random Field Model for
ABC Layout Prediction
We now introduce our approach to automatically
predicting the ABC layout of an input sentence.
While it was most natural for human annotators to
annotate text at the word level, early experiments
quickly revealed that predicting tags at this level is
quite challenging. Most of this stems from the fact
that human annotators tend to fragment the text
into many small segments based on the availability
of good icons. For example, the phrase ?the white
pygmy elephant? may be tagged as ?O A O A? be-
cause it is difficult for the annotator to find an icon
of this exact phrase or the word ?pygmy,? but easy
to find icons of ?white? and ?elephant? separately.
Essentially, human annotation combines two tasks
in one: deciding where each phrase goes in the lay-
out, and deciding which words within a phrase can
be depicted with icons.
To rectify this situation, we make layout predic-
tions at the level of chunks (phrases); that is, we
automatically break the text into chunks, then pre-
dict one A, B, C, or O tag for each chunk. Since the
121
tag choices made for different chunks may depend
on each other, we employ Conditional Random
Fields (CRF) (Lafferty et al, 2001), which are fre-
quently used in sequential labeling tasks like infor-
mation extraction. Our choice of chunking is de-
scribed in Section 4.1, and the CRF models and in-
put features are described in Section 4.2. The task
of deciding which words within a chunk should ap-
pear in the picture is addressed by a ?word pictura-
bility? model, and is discussed in a separate paper.
For training, we automatically map the word-
level tags in our annotated data to chunk-level tags
based on the majority ABC tag within a chunk.
4.1 Chunking by Semantic Role Labeling
Ideally, we would like semantically coherent text
chunks to be represented pictorially in the same
layout position. To obtain such chunks, we lever-
age existing semantic role labeling (SRL) tech-
nology (Palmer et al, 2005; Gildea and Jurafsky,
2002). SRL is an active NLP task in which words
or phrases in a sentence are assigned a label indi-
cating the role they play with respect to a particu-
lar verb (also known as the target predicate). SRL
systems like FrameNet (Baker et al, 1998) and
PropBank (Palmer et al, 2005) aim to provide a
rich representation for applications requiring some
degree of natural language understanding, and are
thus perfectly suited for our needs. We shall fo-
cus on PropBank labels because they are easier to
use for our task. To obtain semantic role labels,
we use the automatic statistical semantic role la-
beler ASSERT (Pradhan et al, 2004), trained to
identify PropBank arguments through the use of
support vector machines and full syntactic parses.
To understand how SRL can be useful for deriv-
ing pictorial layouts, consider the sentence ?The
boy gave the ball to the girl.? PropBank marks
the semantic role labels of the ?arguments? of
verbs. The target verb ?give? is part of the frameset
?transfer,? with core arguments ?Arg0: giver? (the
boy), ?Arg1: thing given? (the ball), and ?Arg2:
entity given to? (the girl). Verbs can also in-
volve non-core modifier arguments, such as ArgM-
TMP (time), ArgM-LOC (location), ArgM-CAU
(cause), etc. The entities playing semantic roles
are likely to be entities we want to portray in a
picture. For PropBank, Arg0 often represents an
Agent, and Arg1 the Patient or Theme. If we could
map the different semantic role labels to ABC tags
with simple rules, then we would be done.
Unfortunately, it is not this simple, as Prop-
Bank roles are verb-specific. As Palmer et al
pointed out, ?No consistent generalizations can be
made across verbs for the higher-numbered argu-
ments? (Palmer et al, 2005). In the above exam-
ple, we might expect a layout rule of [Arg0]?A,
[Target, Arg1]?B, [Arg2]?C. However, this rule
does not generalize to other verbs, such as ?drive,?
as in the sentence ?The boy drives his parents
crazy,? which also has three core arguments ?Arg0:
driver,? ?Arg1: thing in motion,? and ?Arg2: sec-
ondary predication on Arg1.? However, here the
action is figurative, and we would expect a lay-
out rule that puts Arg1 in position C: [Arg0]?A,
[Target]?B, [Arg1,Arg2]?C.
In addition, while modifier arguments have the
same meaning across verbs, their pictorial repre-
sentation may differ based on context. Consider
the sentences ?Polar bears live in the Arctic.? and
?Yesterday at the zoo, the students saw a polar
bear.? In the former, a human annotator is likely
to place an icon for the ArgM-LOC ?in the Arc-
tic? in position C (e.g., following a polar bear icon
in A and a house icon in B). However, the ArgM-
LOC in the second sentence, ?at the zoo,? seems
more appropriately placed in position B since it de-
scribes where this particular action occurred.
Finally, the situation is further complicated
when a sentence contains multiple verbs. SRL
treats each verb in isolation, producing multiple
sets of role labels, yet our goal is to produce a sin-
gle picture. Clearly, the mapping from semantic
roles to layout positions is non-trivial. We describe
our statistical machine learning approach next.
4.2 Our CRF Models and Features
We use a linear-chain CRF as our sequence tag-
ging model. A CRF is a discriminative model of
the conditional probability p(y|x), where y is the
sequence of layout tags in Y ={A,B,C,O}, and x
is the sequence of SRL chunks produced by the
process described in Section 4.1. Our CRF has the
general form
p(y|x) =
1
Z(x)
exp
?
?
|x|
?
t=1
K
?
k=1
?kfk(yt, yt?1, x, t)
?
?
where the model parameters are {?k}. We
use binary features fk(yt, yt?1, x, t) detailed be-
low. Finally, we use an isotropic Gaussian prior
N(0,?2I) on parameters as regularization.
122
We explored three versions of the above model
by specializing the weighted feature function
?kfk(). Model 1 ignores the pairwise label poten-
tials and treats each labeling prediction indepen-
dently: ?jk1{yt=j}fk(x, t), where 1{z} is an indi-
cator function on z. This is equivalent to a multi-
class logistic regression classifier. Model 2 resem-
bles a Hidden Markov Model (HMM) by factoring
pairwise label potentials and emission potentials:
?ij1{yt?1=i}1{yt=j}+?jk1{yt=j}fk(x, t). Finally,
Model 3 has the most general linear-chain poten-
tial: ?ijk1{yt?1=i}1{yt=j}fk(x, t). Model 3 is the
most flexible, but has the most weights to learn.
We use the following binary predicate features
fk(x, t) in all our models, evaluated on each chunk
produced by the semantic role labeler:
1. PropBank role label(s) of the chunk (e.g., Tar-
get, Arg0, Arg1, ArgM-LOC). A chunk can have
multiple role labels if the sentence contains multi-
ple verbs; in this case, we merge the multiple SRL
results by taking their union.
2. Part-of-speech tags of all the words in the
chunk. All syntactic parsing results are obtained
from the Stanford Parser (Klein and Manning,
2003), using the default PCFG model.
3. Phrase type (e.g., NP, VP, PP) of the deepest
syntactic parse tree node covering the entire chunk.
We also include a feature indicating whether the
phrase is nested within an ancestor VP.
4. Lexical features: individual word identities in
the top 5000 most frequent words in the Google 1T
5gram corpus (Brants and Franz, 2006). For other
words, we use their automatically predicted Word-
Net supersenses (Ciaramita and Altun, 2006). Su-
persenses are 41 broad semantic categories (e.g.,
noun.location, verb.communication). By dividing
lexical features in this way, we hope to learn spe-
cific qualities of common words, but generalize
across rarer words.
We also experimented with features derived
from typed dependency relations, but these did not
improve our models. We suspect the PropBank
role labels capture much of the same information.
In addition, the Google 5000-word list was the best
among several word lists that we explored for split-
ting up the lexical features.
4.3 CRF Experimental Results
We trained our CRF models using the MAL-
LET toolkit (McCallum, 2002). Our complete
dataset consists of the 571 manually annotated sen-
10?1 100 101
0.71
0.72
0.73
0.74
0.75
0.76
0.77
0.78
Variance
Ac
cu
ra
cy
 a
nd
 F
1
 
 
Accuracy
F1
Model 1
Model 2
Model 3
Figure 2: 5-fold cross validation results for dif-
ferent values of the regularization parameter (vari-
ance ?2) and three CRF models predicting A, B,
C, or O layout tags.
tences (tags mapped to chunk-level). The only
tuning parameter is the Gaussian prior variance,
?2. We performed 5-fold cross validation, vary-
ing ?2 and comparing performance across models.
Figure 2 demonstrates that peak per-chunk accu-
racy (77.6%) and macro-averaged F1 scores are
achieved using the most general sequence labeling
model. As a result, the user study in the next sec-
tion is based on layouts predicted by Model 3 with
?2 = 1.0, trained on all the data.
To understand which features contribute most
to performance, we experimented with removing
each of the four types (individually). Peak accu-
racy drops the most when lexical features are re-
moved (76.4%), followed by PropBank features
(76.5%), phrase features (76.9%), and POS fea-
tures (77.1%).
The features in the final learned model make in-
tuitive sense. It prefers tag transitions A?B and
B?C, but not A?C or C?A. The model likes the
word ?I? and noun phrases (not nested in a verb
phrase) to have tag A. Verbs and ArgM-NEGs are
frequently tagged B, while noun.object?s, Arg4s,
and ArgM-CAUs are typically C. The model dis-
courages Arg0s and conjunctions in B, and dislikes
adverbial phrases and noun.time?s in C.
While 77.6% cross validation accuracy may
seem low, it is in fact close to the 81% inter an-
notator agreement3, and thus close to optimal. The
confusion matrix (not shown) reveals that most er-
3The 81% agreement is on mapped chunk-level tags with-
out modifiers (Fleiss? kappa 0.74), while the 77% agreement
in Section 3.2 is on word-level tags with modifiers.
123
rors probably arise from disagreements in the in-
dividual annotators. The most common errors are
predicting B for chunks labeled O and confusing
tags B and C. Manually inspecting the pictures in
our training set shows that annotators often omit-
ted the verb (such as ?is? or ?has?) and left the B
position empty, since it could be inferred by the
presence of the arrow and the images in A and C.
Also, annotators tended to disagree on the location
of adverbial expressions, dividing them between
positions B and C. Finally, only 3.3% of chunks
were incorrectly omitted from the pictures. There-
fore, we conclude that our CRFmodels are capable
of predicting the ABC layouts.
5 User Study
We have proposed the ABC layout, and showed
that we can learn to predict it reasonably well. But
an important question remains: Can the proposed
ABC layout help a target audience of limited lit-
eracy understand pictures better, compared to the
linear layout used in state-of-the-art augmentative
and alternative communication software? We de-
scribe a user study as our first attempt to answer
this question. This line of work has two main chal-
lenges: one is the practical difficulty of working
with human subjects of limited literacy; the other is
the lack of a quantitative measure of picture com-
prehension.
[Subjects]: To partially overcome the first chal-
lenge, we recruited two groups of subjects with
medium and high literacy respectively, in hopes
of extrapolating our findings towards the low lit-
eracy group. Specifically, the medium group con-
sisted of seven non-native English speakers who
speak some degree of English??medium literacy?
refers to their English fluency; twelve native En-
glish speakers comprised the high literacy group.
All subjects were adults and did not include the
authors of this paper or the five annotators. The
subjects had no prior exposure to pictorial com-
munication systems.
[Material]: We randomly chose 90 test sen-
tences from three sources4 representing our
target application domains: short narratives
written by and for individuals with commu-
nicative disorders (symbolworld.org);
one-sentence news synopses written in simple
English targeting foreign language learners
(simpleenglishnews.com); and the child
4Distinct from the sources of the 571 training sentences.
writing sections of the LUCY corpus (Sampson,
2003). We created two pictures for each test
sentence: one using a linear layout and one
using an ABC layout. For the linear layout,
we used SymWriter. Typing text in SymWriter
automatically produces a left-to-right sequence
of icons, chosen from an icon database. In cases
where SymWriter suggests several possible icons
for a word, we manually selected the best one. For
words not in the database, we found appropriate
thumbnail images using Web image search. This
is how a typical user would use SymWriter. To
produce the ABC layout, we applied the trained
CRF tagger Model 3 to the test sentence. After
obtaining A, B, C, and O tags for text chunks, we
placed the corresponding icons (from SymWriter?s
linear layout) in the correct layout positions. Icons
for words tagged O did not appear in the ABC
version of the picture. Aside from this difference,
both pictures of each test sentence contained
exactly the same icons?the only difference was
the layout.
[Protocol]: All 19 subjects observed each of
the 90 test sentences exactly once: 45 with the
linear layout and 45 with the ABC layout. The
layouts and the order of sentences were both ran-
domized throughout the sequence, and the subjects
were counter-balanced so each sentence?s linear
and ABC layouts were viewed by roughly equal
numbers of subjects. At the start of the study,
each subject read a brief introduction describing
the task and saw an example of each layout style.
Then for each test sentence, we displayed a pic-
ture, and the subject typed a guess of the underly-
ing sentence. Finally, the subject provided a confi-
dence rating (2=?almost sure,? 1=?maybe correct,?
or 0=?no idea?). We measured response time as
the time from image display until sentence/rating
submission. Figure 3 shows a test sentence in both
layouts, together with several subjects? guesses.
[Evaluation metrics]: As noted above, the
second main challenge is measuring picture
comprehension?we need a way to compare the
original sentences with the subjects? guesses. In
many ways, this is like machine translation (via
pictures), so we turned to two automatic eval-
uation metrics: BLEU-1 (Papineni et al, 2002)
and METEOR (Lavie and Agarwal, 2007). BLEU-1
computes unigram precision (i.e., fraction of re-
sponse words that exactly match words in the orig-
inal), multiplied by a brevity penalty for omit-
124
?we sing a song about a farm.?
?i sing about the farm and animals?
?we sang for the farmer and he gave us animals.?
?Someone went to his grandfather?s farm
and played with the animals?
?i can?t sing in the choir because i have to tend
to the animals.?
?twins sing old macdonald has a farm?
?they sang about a farm?
?they sing old mcdonald had a farm.?
?we have a farm with a sheep, a pig and a cow.?
?two people sing old mcdonald had a farm?
?we sang old mcdonald on the farm.?
?they both sing ?old macdonald had a farm?.?
Figure 3: The linear and ABC layout pictures for the test sentence ?We sang Old MacDonald had a
farm.? and some subjects? guesses. Note the predicted ABC layout omits the ambiguous ?had? icon.
ting words. In contrast, METEOR finds a one-to-
one word alignment between the texts that allows
partial matches (after stemming and by consider-
ing WordNet-based synonyms) and optionally ig-
nores stop words. Based on this alignment, uni-
gram precision, recall, and weighted F measure are
computed, and the final METEOR score is obtained
by scaling F to account for word-order preserva-
tion. We computed METEOR using its default pa-
rameters and the stop word list from the Snowball
project (Porter, 2001).
[Results]: We report average METEOR and BLEU
scores, confidence ratings, and response time for
the 4 conditions (native vs. non-native, ABC vs.
linear) in Table 1. The most striking observation
is that native speakers perform better (in terms of
METEOR and BLEU) with the linear layout, while
non-native speakers do better with ABC. 5
To explain this finding, it is worth noting that
SymWriter pictures include function words, whose
icons are abstract but distinct. We speculate that
even though none of our subjects were trained to
recognize these function-word icons, the native
speakers are more accustomed to the English syn-
tactic structure, so they may be able to transliter-
ate those icons back to words. In an ABC lay-
5Using a Mann-Whitney rank sum test, the difference in
native speakers? METEOR scores is statistically significant
(p = 0.003), though the other differences are not (native
BLEU, p = 0.085; non-native METEOR, p = 0.172; non-
native BLEU, p = 0.170). Nevertheless, we observe some
evidence to support our hypothesis that non-native speak-
ers benefit from the ABC layout, and we intend to conduct
follow-up experiments to test the claim further.
Non-native Native
ABC Linear ABC Linear
METEOR 0.1975 0.1800 0.2955 0.3335
BLEU 0.1497 0.1456 0.2710 0.3011
Conf. 0.50 0.47 0.90 0.89
Time 47.4s 47.8s 38.1s 38.6s
Table 1: User study results.
out, the sentence order is mostly removed, and
some phrases might be omitted due to the O tag.
Thus native speakers do not get as many syntactic
hints. On the other hand, non-native speakers do
not have the same degree of built-in English syn-
tactic knowledge. As such, they do not gain much
from seeing the whole sentence sequence includ-
ing function-word icons. Instead, they may have
benefited from the ABC layout?s added organiza-
tion and potential exclusion of irrelevant icons.
If this reasoning holds, it has interesting impli-
cations for viewers who have lower English liter-
acy: they might take away more meaning from a
semantically structured layout like ABC. Verifying
this is a direction for future work.
Finally, it is interesting that all subjects feel
more confident in their responses to ABC layouts
than linear layouts, and, despite their added com-
plexity, ABC layouts do not require more response
time than linear layouts.
125
6 Conclusions
We proposed a semantically enhanced picture lay-
out for pictorial communication. We formulated
our ABC layout prediction problem as sequence
tagging, and trained CRF models with linguistic
features including semantic role labels. A user
study indicated that our ABC layout has the poten-
tial to facilitate picture comprehension for people
with limited literacy. Future work includes incor-
porating ABC layouts into our pictorial communi-
cation system, improving other components, and
verifying our findings with additional user studies.
Acknowledgments
This work is supported by NSF IIS-0711887, and
by the Wisconsin Alumni Research Foundation.
References
Adorni, G., M. Di Manzo, and G. Ferrari. 1983. Natu-
ral language input for scene generation. In ACL.
Baker, C. F., C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley FrameNet Project. In COLING.
Brants, T. and A. Franz. 2006. Web 1T 5-gram version
1.1. Linguistic Data Consortium, Philadelphia.
Brown, D. C. and B. Chandrasekaran. 1981. Design
considerations for picture production in a natural lan-
guage graphics system. SIGGRAPH, 15(2).
Chandrasekar, R., C. Doran, and B. Srinivas. 1996.
Motivations and methods for text simplification. In
COLING.
Ciaramita, M. and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction
with a supersense sequence tagger. In EMNLP.
Clay, S. R. and J. Wilhelms. 1996. Put: Language-
based interactive manipulation of objects. IEEE
Computer Graphics and Applications, 16(2).
Coyne, B. and R. Sproat. 2001. WordsEye: An au-
tomatic text-to-scene conversion system. In SIG-
GRAPH.
Fleiss, J. L. 1971. Measuring nominal scale agreement
among many raters. Psychological Bulletin, 76(5).
Gildea, D. and D. Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics, 28(3).
Hehner, B. 1980. Blissymbols for use. Blissymbolics
Communication Institute.
Johansson, R., A. Berglund, M. Danielsson, and
P. Nugues. 2005. Automatic text-to-scene conver-
sion in the traffic accident domain. In IJCAI.
Joshi, D., J. Z. Wang, and J. Li. 2006. The story pictur-
ing engine?a system for automatic text illustration.
ACM Transactions on Multimedia Computing, Com-
munications, and Applications, 2(1).
Klein, D. and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In ACL.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML.
Lavie, A. and A. Agarwal. 2007. METEOR: An au-
tomatic metric for MT evaluation with high levels of
correlation with human judgments. In Second Work-
shop on Statistical Machine Translation, June.
McCallum, A. K. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Mihalcea, R. and B. Leong. 2006. Toward commu-
nicating simple sentences using pictorial representa-
tions. In Association of Machine Translation in the
Americas.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
Papineni, K., S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
Porter, M. F. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/.
Pradhan, S., W. Ward, K. Hacioglu, J. Martin, and
D. Jurafsky. 2004. Shallow semantic parsing using
support vector machines. In HLT/NAACL.
Sampson, G. 2003. The structure of children?s writ-
ing: Moving from spoken to adult written norms. In
Granger, S. and S. Petch-Tyson, editors, Extending
the Scope of Corpus-Based Research. Rodopi.
Vickrey, D. and D. Koller. 2008. Sentence simplifica-
tion for semantic role labeling. In ACL. To appear.
Widgit Software. 2007. SymWriter.
http://www.mayer-johnson.com.
Yamada, A., T. Yamamoto, H. Ikeda, T. Nishida, and
S. Doshita. 1992. Reconstructing spatial image
from natural language texts. In COLING.
Zhu, X., A. B. Goldberg, M. Eldawy, C. Dyer, and
B. Strock. 2007. A Text-to-Picture synthesis system
for augmenting communication. In AAAI.
126
