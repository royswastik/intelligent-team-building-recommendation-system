Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 29?35
Manchester, August 2008
Parser Evaluation across Frameworks without Format Conversion
Wai Lok Tam
Interfaculty Initiative in
Information Studies
University of Tokyo
7-3-1 Hongo Bunkyo-ku
Tokyo 113-0033 Japan
Yo Sato
Dept of Computer Science
Queen Mary
University of London
Mile End Road
London E1 4NS, U.K.
Yusuke Miyao
Dept of Computer Science
University of Tokyo
7-3-1 Hongo Bunkyo-ku
Tokyo 113-0033 Japan
Jun-ichi Tsujii
Abstract
In the area of parser evaluation, formats
like GR and SD which are based on
dependencies, the simplest representation
of syntactic information, are proposed as
framework-independent metrics for parser
evaluation. The assumption behind these
proposals is that the simplicity of depen-
dencies would make conversion from syn-
tactic structures and semantic representa-
tions used in other formalisms to GR/SD a
easy job. But (Miyao et al, 2007) reports
that even conversion between these two
formats is not easy at all. Not to mention
that the 80% success rate of conversion
is not meaningful for parsers that boast
90% accuracy. In this paper, we make
an attempt at evaluation across frame-
works without format conversion. This
is achieved by generating a list of names
of phenomena with each parse. These
names of phenomena are matched against
the phenomena given in the gold stan-
dard. The number of matches found is used
for evaluating the parser that produces the
parses. The evaluation method is more ef-
fective than evaluation methods which in-
volve format conversion because the gen-
eration of names of phenomena from the
output of a parser loaded is done by a rec-
ognizer that has a 100% success rate of
recognizing a phenomenon illustrated by a
sentence. The success rate is made pos-
sible by the reuse of native codes: codes
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
used for writing the parser and rules of the
grammar loaded into the parser.
1 Introduction
The traditional evaluation method for a deep parser
is to test it against a list of sentences, each of which
is paired with a yes or no. The parser is evaluated
on the number of grammatical sentences it accepts
and that of ungrammatical sentences it rules out.
A problem with this approach to evaluation is that
it neither penalizes a parser for getting an analy-
sis wrong for a sentence nor rewards it for getting
it right. What prevents the NLP community from
working out a universally applicable reward and
penalty scheme is the absence of a gold standard
that can be used across frameworks. The correct-
ness of an analysis produced by a parser can only
be judged by matching it to the analysis produced
by linguists in syntactic structures and semantic
representations created specifically for the frame-
work on which the grammar is based. A match or
a mismatch between analyses produced by differ-
ent parsers based on different frameworks does not
lend itself for a meaningful comparison that leads
to a fair evaluation of the parsers. To evaluate two
parsers across frameworks, two kinds of methods
suggest themselves:
1. Converting an analysis given in a certain for-
mat native to one framework to another na-
tive to a differernt framework (e.g. converting
from a CCG (Steedman, 2000) derivation tree
to an HPSG (Pollard and Sag, 1994) phrase
structure tree with AVM)
2. Converting analyses given in different
framework-specific formats to some simpler
format proposed as a framework-independent
evaluation schema (e.g. converting from
29
HPSG phrase structure tree with AVM to GR
(Briscoe et al, 2006))
However, the feasibility of either solution is
questionable. Even conversion between two eval-
uation schemata which make use of the simplest
representation of syntactic information in the form
of dependencies is reported to be problematic by
(Miyao et al, 2007).
In this paper, therefore, we propose a different
method of parser evaluation that makes no attempt
at any conversion of syntactic structures and se-
mantic representations. We remove the need for
such conversion by abstracting away from com-
parison of syntactic structures and semantic rep-
resentations. The basic idea is to generate a list
of names of phenomena with each parse. These
names of phenomena are matched against the phe-
nomena given in the gold standard for the same
sentence. The number of matches found is used
for evaluating the parser that produces the parse.
2 Research Problem
Grammar formalisms differ in many aspects. In
syntax, they differ in POS label assignment, phrase
structure (if any), syntactic head assignment (if
any) and so on, while in semantics, they differ
from each other in semantic head assignment, role
assignment, number of arguments taken by pred-
icates, etc. Finding a common denominator be-
tween grammar formalisms in full and complex
representation of syntactic information and seman-
tic information has been generally considered by
the NLP community to be an unrealistic task, al-
though some serious attempts have been made re-
cently to offer simpler representation of syntactic
information (Briscoe et al, 2006; de Marneffe et
al., 2006).
Briscoe et al(2006)?s Grammatical Rela-
tion (GR) scheme is proposed as a framework-
independent metric for parsing accuracy. The
promise of GR lies actually in its dependence on
a framework that makes use of simple representa-
tion of syntactic information. The assumption be-
hind the usefulness of GR for evaluating the out-
put of parsers is that most conflicts between gram-
mar formalisms would be removed by discarding
less useful information carried by complex syn-
tactic or semantic representations used in gram-
mar formalisms during conversion to GRs. But
is this assumption true? The answer is not clear.
A GR represents syntactic information in the form
of a binary relation between a token assigned as
the head of the relation and other tokens assigned
as its dependents. Notice however that grammar
frameworks considerably disagree in the way they
assign heads and non-heads. This would raise the
doubt that, no matter how much information is re-
moved, there could still remain disagreements be-
tween grammar formalisms in what is left.
The simplicity of GR, or other dependency-
based metrics, may give the impression that con-
version from a more complex representation into
it is easier than conversion between two complex
representations. In other words, GRs or a sim-
ilar dependency relation looks like a promising
candidate for lingua franca of grammar frame-
works. However the experiment results given by
Miyao et al(2007) show that even conversion into
GRs of predicate-argument structures, which is not
much more complex than GRs, is not a trivial task.
Miyao et al(2007) manage to convert 80% of the
predicate-argument structures outputted by their
deep parser, ENJU, to GRs correctly. However the
parser, with an over 90% accuracy, is too good for
the 80% conversion rate. The lesson here is that
simplicity of a representation is a different thing
from simplicity in converting into that representa-
tion.
3 Outline of our Solution
The problem of finding a common denominator for
grammar formalisms and the problem of conver-
sion to a common denominator may be best ad-
dressed by evaluating parsers without making any
attempt to find a common denominator or conduct
any conversion. Let us describe briefly in this sec-
tion how such evaluation can be realised.
3.1 Creating the Gold Standard
The first step of our evaluation method is to con-
struct or find a number of sentences and get an an-
notator to mark each sentence for the phenomena
illustrated by each sentence. After annotating all
the sentences in a test suite, we get a list of pairs,
whose first element is a sentence ID and second is
again a list, one of the corresponding phenomena.
This list of pairs is our gold standard. To illustrate,
suppose we only get sentence 1 and sentence 2 in
our test suite.
(1) John gives a flower to Mary
(2) John gives Mary a flower
30
Sentence 1 is assigned the phenomena: proper
noun, unshifted ditransitive, preposition. Sentence
2 is assigned the phenomena: proper noun, dative-
shifted ditransitive. Our gold standard is thus the
following list of pairs:
?1, ?proper noun, unshifted ditransitive, preposition? ?,
?2, ?proper noun,dative-shifted ditransitive? ?
3.2 Phenomena Recognition
The second step of our evaluation method requires
a small program that recognises what phenomena
are illustrated by an input sentence taken from the
test suite based on the output resulted from pars-
ing the sentence. The recogniser provides a set
of conditions that assign names of phenomena to
an output, based on which the output is matched
with some framework-specific regular expressions.
It looks for hints like the rule being applied at a
node, the POS label being assigned to a node, the
phrase structure and the role assigned to a refer-
ence marker. The names of phenomena assigned
to a sentence are stored in a list. The list of phe-
nomena forms a pair with the ID of the sentence,
and running the recogniser on multiple outputs ob-
tained by batch parsing (with the parser to be eval-
uated) will produce a list of such pairs, in exactly
the same format as our gold standard. Let us illus-
trate this with a parser that:
1. assigns a monotransitive verb analysis to
?give? and an adjunct analysis to ?to Mary? in
1
2. assigns a ditransitive verb analysis to ?give? in
2
The list of pairs we obtain from running the
recogniser on the results produced by batch pars-
ing the test suite with the parser to be evaluated is
the following:
?1,?proper noun,monotransitive,preposition,adjunct??,
?2, ?proper noun,dative-shifted ditransitive? ?
3.3 Performance Measure Calculation
Comparing the two list of pairs generated from the
previous steps, we can calculate the precision and
recall of a parser using the following formulae:
Precision = (
n
?
i=1
| R
i
?A
i
|
| R
i
|
)? n (1)
Recall = (
n
?
i=1
| R
i
?A
i
|
| A
i
|
)? n (2)
where list R
i
is the list generated by the recogniser
for sentence i, list A
i
is the list produced by anno-
tators for sentence i, and n the number of sentences
in the test suite.
In our example, the parser that does a good job
with dative-shifted ditransitives but does a poor job
with unshifted ditranstives would have a precision
of:
(
2
4
+
2
2
)? 2 = 0.75
and a recall of:
(
2
3
+
2
2
)? 2 = 0.83
4 Refining our Solution
In order for the precision and recall given above to
be a fair measure, it is necessary for both the recog-
niser and the annotators to produce an exhaustive
list of the phenomena illustrated by a sentence.
But we foresee that annotation errors are likely
to be a problem of exhaustive annotation, as is re-
ported in Miyao et al(2007) for the gold standard
described in Briscoe et al(2006). Exhaustive an-
notation procedures require annotators to repeat-
edly parse a sentence in search for a number of
phenomena, which is not the way language is nor-
mally processed by humans. Forcing annotators to
do this, particularly for a long and complex sen-
tence, is a probable reason for the annotation er-
rors in the gold standard described in (Briscoe et
al., 2006).
To avoid the same problem in our creation of a
gold standard, we propose to allow non-exhaustive
annotation. In fact, our proposal is to limit the
number of phenomena assigned to a sentence to
one. This decision on which phenomenon to be as-
signed is made, when the test suite is constructed,
for each of the sentences contained in it. Follow-
ing the traditional approach, we include every sen-
tence in the test suite, along with the core phe-
nomenon we intend to test it on (Lehmann and
Oepen, 1996). Thus, Sentence 1 would be as-
signed the phenomenon of unshifted ditransitive.
Sentence 2 would be assigned the phenomenon of
31
dative-shifted ditransitive. This revision of anno-
tation policy removes the need for exhaustive an-
notation. Instead, annotators are given a new task.
They are asked to assign to each sentence the most
common error that a parser is likely to make. Thus
Sentence 1 would be assigned adjunct for such an
error. Sentence 2 would be assigned the error of
noun-noun compound. Note that these errors are
also names of phenomena.
This change in annotation policy calls for a
change in the calculation of precision and recall.
We leave the recogniser as it is, i.e. to produce an
exhaustive list of phenomena, since it is far beyond
our remit to render it intelligent enough to select a
single, intended, phenomenon. Therefore, an in-
correctly low precision would result from a mis-
match between the exhaustive list generated by the
recogniser and the singleton list produced by an-
notators for a sentence. For example, suppose we
only have sentence 2 in our test suite and the parser
correctly analyses the sentence. Our recogniser as-
signs two phenomena (proper noun, dative-shifted
ditransitive) to this sentence as before. This would
result in a precision of 0.5.
Thus we need to revise our definition of preci-
sion, but before we give our new definition, let us
define a truth function t:
t(A ? B) =
{
1 A ? B
0 A ?B = ?
t(A ?B = ?) =
{
0 A ?B 6= ?
1 A ?B = ?
Now, our new definition of precision and recall
is as follows:
Precision (3)
=
(
?
n
i=1
t(R
i
?AP
i
)+t(R
i
?AN
i
=?)
2
)
n
Recall (4)
=
(
?
n
i=1
|R
i
?AP
i
|
|AP
i
|
)
n
where list AP
i
is the list of phenomena produced
by annotators for sentence i, and list AN
i
is the list
of errors produced by annotators for sentence i.
While the change in the definition of recall is
trivial, the new definition of precision requires
some explanation. The exhaustive list of phenom-
ena generated by our recogniser for each sentence
is taken as a combination of two answers to two
questions on the two lists produced by annotators
for each sentence. The correct answer to the ques-
tion on the one-item-list of phenomenon produced
by annotators for a sentence is a superset-subset re-
lation between the list generated by our recogniser
and the one-item-list of phenomenon produced by
annotators. The correct answer to the question on
the one-item-list of error produced by annotators
for a sentence is the non-existence of any common
member between the list generated by our recog-
niser and the one-item-list of error produced by an-
notators.
To illustrate, let us try a parser that does a good
job with dative-shifted ditransitives but does a poor
job with unshifted ditranstives on both 2 and 1.
The precision of such a parser would be:
(
0
2
+
2
2
)? 2 = 0.5
and its recall would be:
(
0
1
+
1
1
)? 2 = 0.5
5 Experiment
For this abstract, we evaluate ENJU (Miyao,
2006), a released deep parser based on the HPSG
formalism and a parser based on the Dynamic Syn-
tax formalism (Kempson et al, 2001) under devel-
opment against the gold standard given in table 1.
The precision and recall of the two parsers
(ENJU and DSPD, which stands for ?Dynamic
Syntax Parser under Development?) are given in
table 3:
The experiment that we report here is intended
to be an experiment with the evaluation method de-
scribed in the last section, rather than a very seri-
ous attempt to evaluate the two parsers in question.
The sentences in table 1 are carefully selected to
include both sentences that illustrate core phenom-
ena and sentences that illustrate rarer but more in-
teresting (to linguists) phenomena. But there are
too few of them. In fact, the most important num-
ber that we have obtained from our experiment is
the 100% success rate in recognizing the phenom-
ena given in table 1.
32
ID Phenomenon Error
1 unshifted ditransi-
tive
adjunct
2 dative-shifted di-
transitive
noun-noun com-
pound
3 passive adjunct
4 nominal gerund verb that takes
verbal comple-
ment
5 verbal gerund imperative
6 preposition particle
7 particle preposition
8 adjective with ex-
trapolated senten-
tial complement
relative clause
9 inversion question
10 raising control
Figure 1: Gold Standard for Parser Evaluation
ID Sentence
1 John gives a flower to Mary
2 John give Mary a flower
3 John is dumped by Mary
4 Your walking me pleases me
5 Abandoning children increased
6 He talks to Mary
7 John makes up the story
8 It is obvious that John is a fool
9 Hardly does anyone know Mary
10 John continues to please Mary
Figure 2: Sentences Used in the Gold Standard
Measure ENJU DSPD
Precision 0.8 0.7
Recall 0.7 0.5
Figure 3: Performance of Two Parsers
6 Discussion
6.1 Recognition Rate
The 100% success rate is not as surprising as it
may look. We made use of two recognisers, one
for each parser. Each of them is written by the
one of us who is somehow involved in the devel-
opment of the parser whose output is being recog-
nised and familiar with the formalism on which the
output is based. This is a clear advantage to for-
mat conversion used in other evaluation methods,
which is usually done by someone familiar with ei-
ther the source or the target of conversion, but not
both, as such a recogniser only requires knowledge
of one formalism and one parser. For someone
who is involved in the development of the gram-
mar and of the parser that runs it, it is straight-
forward to write a recogniser that can make use
of the code built into the parser or rules included
in the grammar. We can imagine that the 100%
recognition rate would drop a little if we needed
to recognise a large number of sentences but were
not allowed sufficient time to write detailed regular
expressions. Even in such a situation, we are con-
fident that the success rate of recognition would be
higher than the conversion method.
Note that the effectiveness of our evaluation
method depends on the success rate of recognition
to the same extent that the conversion method em-
ployed in Briscoe et al (2006) and de Marneff et
al. (2006) depends on the conversion rate. Given
the high success rate of recognition, we argue that
our evaluation method is more effective than any
evaluation method which makes use of a format
claimed to be framework independent and involves
conversion of output based on a different formal-
ism to the proposed format.
6.2 Strictness of Recognition and Precision
There are some precautions regarding the use of
our evaluation method. The redefined precision 4
is affected by the strictness of the recogniser. To
illustrate, let us take Sentence 8 in Table 1 as an
example. ENJU provides the correct phrase struc-
ture analysis using the desired rules for this sen-
tence but makes some mistakes in assigning roles
to the adjective and the copular verb. The recog-
niser we write for ENJU is very strict and refuses
to assign the phenomenon ?adjective with extrap-
olated sentential complement? based on the output
given by ENJU. So ENJU gets 0 point for its an-
swer to the question on the singleton list of phe-
33
nomenon in the gold standard. But it gets 1 point
for its answer to the question on the singleton list
of error in the gold standard because it does not
go to the other extreme: a relative clause analysis,
yielding a 0.5 precision. In this case, this value is
fair for ENJU, which produces a partially correct
analysis. However, a parser that does not accept
the sentence at all, a parser that fails to produce
any output or one that erroneously produces an un-
expected phenomenon would get the same result:
for Sentence 8, such a parser would still get a pre-
cision of 0.5, simply because its output does not
show that it assigns a relative clause analysis.
We can however rectify this situation. For the
lack of parse output, we can add an exception
clause to make the parser automatically get a 0 pre-
cision (for that sentence). Parsers that make unex-
pected mistakes are more problematic. An obvi-
ous solution to deal with these parsers is to come
up with an exhaustive list of mistakes but this is an
unrealistic task. For the moment, a temporary but
realistic solution would be to expand the list of er-
rors assigned to each sentence in the gold standard
and ask annotators to make more intelligent guess
of the mistakes that can be made by parsers by con-
sidering factors such as similarities in phrase struc-
tures or the sharing of sub-trees.
6.3 Combining Evaluation Methods
For all measures, some distortion is unavoidable
when applied to exceptional cases. This is true for
the classical precision and recall, and our redefined
precision and recall is no exception. In the case of
the classical precision and recall, the distortion is
countered by the inverse relation between them so
that even if one is distorted, we can tell from the
other that how well (poorly) the object of evalua-
tion performs. Our redefined precision and recall
works pretty much the same way.
What motivates us to derive measures so closely
related to the classical precision and recall is the
ease to combine the redefined precision and recall
obtained from our evaluation method with the clas-
sical precision and recall obtained from other eval-
uation methods, so as to obtain a full picture of
the performance of the object of evaluation. For
example, our redefined precision and recall figures
given in Table 3 (or figures obtained from running
the same experiment on a larger test set) for ENJU
can be combined with the precision and recall fig-
ures given in Miyao et al (2006) for ENJU, which
is based on a evaluation method that compares its
predicate-argument structures those given in Penn
Treebank. Here the precision and recall figures are
calculated by assigning an equal weight to every
sentence in Section 23 of Penn Treebank. This
means that different weights are assigned to dif-
ferent phenomena depending on their frequency in
the Penn Treebank. Such assignment of weights
may not be desirable for linguists or developers
of NLP systems who are targeting a corpus with a
very different distribution of phenomena from this
particular section of the Penn Treebank. For exam-
ple, a linguist may wish to assign an equal weight
across phenomena or more weights to ?interesting?
phenomena. A developer of a question-answering
system may wish to give more weights to question-
related phenomena than other phenomena of less
interest which are nevertheless attested more fre-
quently in the Penn Treebank.
In sum, the classical precision and recall fig-
ures calculated by assigning equal weight to ev-
ery sentence could be considered skewed from the
perspective of phenomena, whereas our redefined
precision and recall figures may be seen as skewed
from the frequency perspective. Frequency is rela-
tive to domains: less common phenomena in some
domains could occur more often in others. Our re-
defined precision and recall are not only useful for
those who want a performance measure skewed the
way they want, but also useful for those who want
a performance measure as ?unskewed? as possible.
This may be obtained by combining our redefined
precision and recall with the classical precision
and recall yielded from other evaluation methods.
7 Conclusion
We have presented a parser evaluation method
that addresses the problem of conversion between
frameworks by totally removing the need for that
kind of conversion. We do some conversion but
it is a different sort. We convert the output of a
parser to a list of names of phenomena by drawing
only on the framework that the parser is based on.
It may be inevitable for some loss or inaccuracy
to occur during this kind of intra-framework con-
version if we try our method on a much larger test
set with a much larger variety of longer sentences.
But we are confident that the loss would still be
far less than any inter-framework conversion work
done in other proposals of cross-framework evalu-
ation methods. What we believe to be a more prob-
34
lematic area is the annotation methods we have
suggested. At the time we write this paper based
on a small-scale experiment, we get slightly bet-
ter result by asking our annotator to give one phe-
nomenon and one common mistake for each sen-
tence. This may be attributed to the fact that he
is a member of the NLP community and hence he
gets the knowledge to identify the core phenom-
ena we want to test and the common error that
parsers tend to make. If we expand our test set
and includes longer sentences, annotators would
make more mistakes whether they attempt exhaus-
tive annotation or non-exhaustive annotation. It
is difficult to tell whether exhaustive annotation
or non-exhaustive annotation would be better for
large scale experiments. As future work, we intend
to try our evaluation method on more test data to
determine which one is better and find ways to im-
prove the one we believe to be better for large scale
evaluation.
References
Briscoe, Ted, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of COLING/ACL 2006.
de Marneffe, Marie-Catherine, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC 2006.
Kempson, Ruth, Wilfried Meyer-Viol, and Dov Gab-
bay. 2001. Dynamic Syntax: The Flow of Language
Understanding. Blackwell.
Lehmann, Sabine and Stephan Oepen. 1996. TSNLP
test suites for natural language processing. In Pro-
ceedings of COLING 1996.
Miyao, Yusuke, Kenji Sagae, and Junichi Tsujii. 2007.
Towards framework-independent evaluation of deep
linguistic parsers. In Proceedings of GEAF 2007.
Miyao, Yusuke. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Devel-
opment and Feature Forest Model. Ph.D. thesis, Uni-
versity of Tokyo.
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press and CSLI Publications.
Steedman, Mark. 2000. Syntactic Process. MIT Press.
35
