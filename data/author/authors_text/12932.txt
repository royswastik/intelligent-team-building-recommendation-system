Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 10?18,
Beijing, August 2010
Identifying Multi-word Expressions by
Leveraging Morphological and Syntactic Idiosyncrasy
Hassan Al-Haj
Language Technologies Institute
Carnegie Mellon University
hhaj@cs.cmu.edu
Shuly Wintner
Department of Computer Science
University of Haifa
shuly@cs.haifa.ac.il
Abstract
Multi-word expressions constitute a sig-
nificant portion of the lexicon of every
natural language, and handling them cor-
rectly is mandatory for various NLP appli-
cations. Yet such entities are notoriously
hard to define, and are consequently miss-
ing from standard lexicons and dictionar-
ies. Multi-word expressions exhibit id-
iosyncratic behavior on various levels: or-
thographic, morphological, syntactic and
semantic. In this work we take advan-
tage of the morphological and syntactic
idiosyncrasy of Hebrew noun compounds
and employ it to extract such expressions
from text corpora. We show that relying
on linguistic information dramatically im-
proves the accuracy of compound extrac-
tion, reducing over one third of the errors
compared with the best baseline.
1 Introduction
Multi-word expressions (MWEs) are notoriously
hard to define. They span a range of constructions,
from completely frozen, semantically opaque id-
iomatic expressions, to frequent but morpholog-
ically productive and semantically compositional
collocations. Various linguistic processes (ortho-
graphic, morphological, syntactic, semantic, and
cognitive) apply to MWEs in idiosyncratic ways.
Notably, MWEs blur the distinction between the
lexicon and the grammar, since they often have
some properties of words and some of phrases.
In this work we define MWEs as expressions
whose linguistic properties (morphological, syn-
tactic or semantic) are not directly derived from
the properties of their word constituents. This is
a functional definition, driven by a practical mo-
tivation: any natural language processing (NLP)
application that cares about morphology, syntax
or semantics must consequently store MWEs in
the lexicon.
MWEs are numerous and constitute a signif-
icant portion of the lexicon of any natural lan-
guage. They are a heterogeneous class of con-
structions with diverse sets of characteristics.
Morphologically, some MWEs allow some of
their constituents to freely inflect while restricting
(or even preventing) the inflection of other con-
stituents. MWEs may allow constituents to un-
dergo non-standard morphological inflections that
they would not undergo in isolation. Some MWEs
contain words that never occur outside the context
of the MWE. Syntactically, some MWEs appear
in one rigid pattern (and a fixed order), while oth-
ers permit various syntactic transformations. Se-
mantically, the compositionality of MWEs (i.e.,
the degree to which the meaning of the whole ex-
pression results from combining the meanings of
its individual words when they occur in isolation)
is gradual.
These morphological, syntactic and semantic
idiosyncrasies make MWEs a challenge for NLP
applications (Sag et al, 2002). They are even
more challenging in languages with complex mor-
phology, because of the unique interaction of mor-
phological and orthographic processes with the
lexical specification of MWEs (Oflazer et al,
2004; Alegria et al, 2004).
Because the idiosyncratic features of MWEs
cannot be predicted on the basis of their com-
ponent words, they must be stored in the lexi-
con of NLP applications. Handling MWEs cor-
rectly is beneficial for a variety of applications,
including information retrieval, building ontolo-
gies, text alignment, and machine translation. Au-
tomatic identification and corpus-based extraction
of MWEs is thus crucial for such (and several
other) applications.
10
In this work we describe an approach that lever-
ages the morphological and syntactic idiosyncrasy
of a certain class of Hebrew1 MWEs, namely
noun compounds, to help identify such expres-
sions in texts. While the main contribution of
this work is a system that can distinguish be-
tween MWE and non-MWE instances of a partic-
ular construction in Hebrew, thereby facilitating
faster and more accurate integration of MWEs in
a large-coverage lexicon of the language, we be-
lieve that it carries added value to anyone inter-
ested in MWEs. The technique that we propose
here should be applicable in principle to any lan-
guage in which MWEs exhibit linguistically id-
iosyncratic behavior.
We describe the properties of Hebrew noun-
noun constructions in Section 2, and specify the
irregularities exhibited by compounds. Section 3
presents the experimental setup and the main re-
sults. Compared with the best (collocation-based)
baseline, our approach reduces over 30% of the
errors, yielding accuracy of over 80%. We dis-
cuss related work in Section 4 and conclude with
suggestions for future research.
2 Hebrew noun-noun constructions
We focus on Hebrew noun-noun constructions;
these are extremely frequent constructions, and
while many of them are fully compositional, oth-
ers, called noun compounds (or just compounds)
here, are clearly MWEs. We first discuss the gen-
eral construction and then describe the peculiar,
idiosyncratic properties of compounds.
2.1 The general case
Hebrew nouns inflect for number (singular and
plural) and, when the noun denotes an animate en-
tity, for gender (masculine and feminine). In ad-
dition, nouns come in three states: indefinite, def-
inite and a construct state that is used in genitive
constructions. Table 1 demonstrates the paradigm.
A noun-noun construction (henceforth NNC)
consists of a construct-state noun, called head
here, followed by a noun phrase, the modi-
fier (Borer, 1988; Borer, 1996; Glinert, 1989).
1To facilitate readability we use a transliteration of He-
brew using Roman characters; the letters used, in Hebrew
lexicographic order, are abgdhwzxTiklmns?pcqrs?t.
State M/Sg F/Sg M/Pl F/Pl
indefinite ild ildh ildim ildwt
definite hild hildh hildim hildwt
construct ild ildt ildi ildwt
Table 1: The noun paradigm, demonstrated on ild
?child?
The semantic relation between the two is usually,
but not always, related to possession (Levi, 1976).
Construct-state nouns only occur in the context of
NNC, and can never occur in isolation. When a
NNC is definite, the definite article is expressed
on its modifier (Wintner, 2000).
In the examples below, we explicitly indicate
construct-state nouns by the morpheme ?.CONST?
in the gloss; and definite nouns are indicated by
the morpheme ?the-?. We provide both a literal
and a non-literal meaning of the MWE examples.
Expressions that have a literal, but not the ex-
pected MWE meaning, are preceded by ?#?.
Example 1 (Noun-noun constructions)
hxlTt hw?dh
decision.CONST the-committee
?the committee decision?
?wrk h?itwn
editor.CONST the-journal
?the journal editor?
?wrk din
editor.CONST law
?law editor? =? lawyer
bti xwlim
houses.CONST patients
?patient houses? =? hospitals
2.2 Noun compounds: Linguistic properties
While many of the NNCs are free, compositional
combinations of words, some are not; we use the
term noun compounds for the latter group. Com-
pounds typically (but not necessarily) have non-
compositional meaning; presumably due to their
opaque, more lexical meaning, they also differ
from other NNCs in their morphological and syn-
tactic behavior. Some of these distinctive prop-
erties are listed below, to motivate the methodol-
ogy that we propose in Section 3 to distinguish
between compounds and non-MWE NNCs.
11
2.2.1 Limited inflection
When a NNC consists of two nouns, the sec-
ond can typically occur in either singular or plural
form. Compounds often limit the possibilities to
only one of those.
Example 2 (No plural form of the modifier)
?wrki h?itwnim
editors-.CONST the-journals
?the journals? editors?
?wrki hdin
editors.CONST the-law
?the law editors? =? the lawyers
#wrki hdinim
editors.CONST the-laws
Example 3 (No singular form of the modifier)
kiwwn hrwx
direction.CONST the-wind
?the wind?s direction?
kiwwn hrwxwt
direction.CONST the-winds
?the winds? direction?
s?ws?nt h-rwxwt
lily.CONST the-winds
?lily of the winds? =? compass rose
#s?ws?nt h-rwx
lily.CONST the-wind
2.2.2 Limited syntactic variation
Since NNCs typically denote genitive (posses-
sive) constructions, they can be paraphrased by a
construction that uses the genitive preposition s?l
?of? (or, in some cases, other prepositions). These
syntactic variants are often restricted in the case of
compounds.
Example 4 (Limited paraphrasing)
h?wrk s?l h?itwn
the-editor of the-journal
?the journal editor?
#h?wrk s?l hdin
the-editor of the-law
Example 5 (Limited paraphrasing)
m?il cmr
coat.CONST wool
?wool coat?
m?il mcmr
coat from-wool
?wool coat?
cmr pldh
wool.CONST steel
?steel wool? =? steel wool
#cmr mpldh
wool from-steel
2.2.3 Limited syntactic modification
NNCs typically allow adjectival modification
of either of their constituents. Since compounds
tend to be more semantically opaque, it is of-
ten only possible to modify the entire compound,
but not any of the constituents. In the follow-
ing example, note that ?wrkt ?editor? is feminine,
whereas ?itwn ?journal? is masculine; adjectives
must agree on gender with the noun they modify.
Example 6 (Limited adjectival modification)
?wrkt h?itwn
editor-f.CONST the-journal-m
?the journal editor?
?wrkt h?itwn hxds?h
editor-f.CONST the-journal-m the-new-f
?the new editor of the journal?
?wrkt h?itwn hxds?
editor-f.CONST the-journal-m the-new-m
?the editor of the new journal?
?wrkt hdin hxds?h
editor-f.CONST the-law-m the-new-f
?the new law editor? =? the new lawyer
#?wrkt hdin hxds?
editor-f.CONST the-law-m the-new-m
2.2.4 Limited coordination
Two NNCs that share a common head can be
conjoined using the coordinating conjunction w
?and?. This possibility is often blocked in the case
of compounds.
Example 7 (Limited coordination)
mwsdwt xinwk wbriawt
institutions.CONST education and-health
?education and health institutions?
bti spr
houses.CONST book
?book houses? =? schools
12
bti xwlim
houses.CONST patients
?patient houses? =? hospitals
#bti spr wxwlim
houses.CONST book and-patients
3 Identification of noun compounds
In this section we describe a system that identi-
fies noun compounds in Hebrew text, and extracts
them in order to extend the lexicon. We capitalize
on the morphological and syntactic irregularities
of noun compounds described in Section 2.2.
Given a large monolingual corpus, the text
is first morphologically analyzed and disam-
biguated. Then, all NNCs (candidate noun com-
pounds) are extracted from the morphologically
disambiguated text. For each candidate noun
compound we define a set of features (Section 3.3)
based on the idiosyncratic morphological and syn-
tactic properties defined in Section 2.2. These
features inform a support vector machine classi-
fier which is then used to identify the noun com-
pounds in the set of NNCs with high accuracy
(Section 3.5).
3.1 Resources
We use (a subset of) the Corpus of Contempo-
rary Hebrew (Itai and Wintner, 2008) which con-
sists of four sub-corpora: The Knesset corpus
contains the Israeli parliament proceedings from
2004-2005; the Haaretz corpus contains articles
from the Haaretz newspaper from 1991; The-
Marker corpus contains financial articles from the
TheMarker newspaper from 2002; and the Arutz
7 corpus contains newswire articles from 2001-
2006. Corpora sizes are listed in Table 2.
Corpus Number of tokens
Knesset 12,742,879
Harretz 463,085
The Marker 684,801
Arutz 7 7,714,309
Total 21,605,074
Table 2: Corpus data
The entire corpus was morphologically ana-
lyzed (Yona and Wintner, 2008; Itai and Wintner,
2008) and POS-tagged (Bar-haim et al, 2008);
note that no syntactic parser is available for He-
brew. From the morphologically disambiguated
corpus, we extract all bi-grams in which the first
token is a noun in the construct state and the sec-
ond token is a noun that is not in the construct
state, i.e., all two-word NNC candidates.
3.2 Annotation
For training and evaluation, we select the NNCs
that occur at least 100 times in the corpus, yield-
ing 1060 NNCs. These NNCs were annotated
by three annotators, who were asked to classify
them to the following four groups: compounds
(+); non-compounds (?); unsure (0); and errors of
the morphological processor (i.e., the candidate is
not a NNC at all). Table 3 lists the number of can-
didates in each class.
Annotator + ? 0 err
1 314 332 238 176
2 335 403 179 143
3 400 630 16 14
Table 3: NNC classification by annotator
We adopt a conservative approach in combin-
ing the three annotations. First, we eliminate 204
NNCs that were tagged as errors by at least one
annotator. For the remaining NNCs, a candidate is
considered a compound or a non-compound only
if all three annotators agree on its classification.
This reduces the annotated data to 463 instances,
of which 205 are compounds and 258 are clear
cases of non-compound NNCs.2
3.3 Linguistically-motivated features
We define a set of features based on the idiosyn-
cratic properties of noun compounds defined in
Section 2.2. For each candidate NNC, we com-
pute counts which reflect the likelihood of it ex-
hibiting one of the linguistic properties.
Refer back to Section 2.2. We focus on the
property of limited inflection (Section 2.2.1), and
define features 1?8 to reflect it. To reflect limited
syntactic variation (Section 2.2.2) we define fea-
tures 9?10. Feature 11 addresses the phenomenon
2This annotated corpus is freely available for download.
13
of limited coordination (Section 2.2.4). To reflect
limited syntactic modification (Section 2.2.3) we
define feature 12. .
For each NNC candidate N1 N2, the following
features are defined:
1. The number of occurrences of the NNC in
which both constituents are in singular.
2. The number of occurrences of the NNC in
which N1 is in singular and N2 is in plural.
3. The number of occurrences of the NNC in
which N1 is in plural and N2 is in singular.
4. The number of occurrences of the NNC in
which both constituents are in plural.
5. The number of occurrences of N1 in plural
outside the expression.
6. The number of occurrences of N1 in singular
outside the expression.
7. The number of occurrences of N2 in plural
outside the expression.
8. The number of occurrences of N2 in singular
outside the expression.
9. The number of occurrences of N1 s?l N2 ?N1
of N2? in the corpus.
10. The number of occurrences of N1 m N2 ?N1
from N2? in the corpus.
11. The number of occurrences of N1 N2 w N3
?N1 N2 and N3? in the corpus, where N3 is
an indefinite, non-construct-state noun.
12. The number of occurrences of N1 N2 Adj in
the corpus, where the adjective Adj agrees
with N2 on both gender and number, while
disagreeing with N1 on at least one of these
attributes.
We also define four features that represent known
collocation measures (Evert and Krenn, 2001):
Point-wise mutual information (PMI); T-Score;
log-likelihood; and the raw frequency of N1 N2
in the corpus.3
3A detailed description of these measures is given by
Manning and Schu?tze (1999, Chapter 5); see also http:
//www.collocations.de/, where several other asso-
ciation measures are discussed as well.
3.4 Training and evaluation
For each NNC in the annotated set of Section 3.2
we create a vector of the 16 features described in
Section 3.3 (12 linguistically-motivated features
plus four collocation measures). We obtain a list
of 463 instances, of which 205 are positive ex-
amples (noun compounds) and 258 are negative.
We use this set for training and evaluation of a
two class soft margin SVM classifier (Chang and
Lin, 2001) with a radial basis function kernel. We
experiment below with different combinations of
features, where for each combination we use 10-
fold cross-validation over the 463 NNcs to evalu-
ate the classifier. We report Precision, Recall, F-
score and Accuracy (averaged over the 10 folds).
3.5 Results
The results of the different classifiers that we
trained are given in Table 4. The first four rows
of the table show the performance of classifiers
trained using each of the four different colloca-
tion measure features alone. Both PMI and Log-
likelihood outperform the other collocation mea-
sures, with an F-score of 60, which we consider
our baseline. We also report the performance of
two combinations of collocation measures, which
yield small improvement. The best combinations
provide accuracy of about 70% and F-score of 63.
The remaining rows report results using the
linguistically-motivated features (LMF) of Sec-
tion 3.3. These features alone yield accuracy of
77.75% and an F-score of 76. Adding also Log-
likelihood improves F-score by 1.16 and accuracy
by 1.29%. Finally, using Log-likelihood with a
subset of the LMF consisting of features 1-2, 4-
6, 9-10 and 12 (see below) yields the best re-
sults, namely accuracy of over 80% and F-score
of 78.85, reflecting a reduction of over one third
in classification error rate compared with the base-
line.
3.6 Optimizing feature combination
We search for the combination of linguistically-
motivated features that would yield the best per-
formance. Training a classifier on all possible
feature combinations is clearly infeasible. In-
stead, we follow a more efficient greedy approach,
whereby we start with the best collocation mea-
14
Features Accuracy Precision Recall F-score
PMI 67.17 64.97 56.09 60.20
Frequency 60.47 60.00 32.19 41.90
T-Score 61.98 59.86 42.92 50.00
Log-likelihood 69.33 71.42 51.21 59.65
T-score+Log-likelihood 70.62 71.42 56.09 62.84
PMI+Log-likelihood 69.97 68.96 58.53 63.32
LMF 77.75 71.98 81.46 76.43
LMF+PMI 77.32 71.18 81.95 76.19
LMF+Log-likelihood 79.04 73.68 81.95 77.59
Log-likelihood+LMF[1-2,4-6,9-10,12] 80.77 76.85 80.97 78.85
Table 4: Results: 10-Fold accuracy, precision, recall, and F-score for classifiers trained using different
combinations of features. LMF stands for linguistically-motivated features
sure, Log-likelihood, and add other features one at
a time, in the order in which they are listed in Sec-
tion 3.3. After adding each feature the classifier is
retrained; the feature is retained in the feature set
only if adding it improves the 10-fold F-score of
the current feature set.
Table 5 lists the results of this experiment. For
each feature set the difference in the 10-fold F-
score compared to the previous feature set is listed
in parentheses. The results show that the best fea-
ture combination improves the F-score by 1.26,
compared with using all features. This experi-
ments shows that features 3, 7, 8 and 11 turn out
not to be useful, and the classifier is more accurate
without them. We also tried this approach with
PMI as the starting feature, with very similar re-
sults.
Feature set F-score
Log-likelihood 59.65
Log-likelihood,1 60.34 (+0.68)
Log-likelihood,1-2 65.42 (+5.08)
Log-likelihood,1-3 64.87 (-0.54)
Log-likelihood,1-2,4 66.66 (+1.78)
Log-likelihood,1-2,4-5 70.00 (+3.33)
Log-likelihood,1-2,4-6 74.37 (+4.37)
Log-likelihood,1-2,4-7 73.78 (?0.58)
Log-likelihood,1-2,4-6,8 73.58 (?0.79)
Log-likelihood,1-2,4-6,9 78.72 (+4.35)
Log-likelihood,1-2,4-6,9-10 78.83 (+0.10)
Log-likelihood,1-2,4-6,9-11 77.37 (?1.46)
Log-likelihood,1-2,4-6,9-10,12 78.85 (+0.02)
Table 5: Optimizing the set of linguistically-
motivated features
4 Related work
There has been a growing awareness in the re-
search community of the problems that MWEs
pose, both in linguistics and in NLP (Villavicencio
et al, 2005). Recent works address the definition,
lexical representation and computational process-
ing of MWEs, as well as algorithms for extracting
them from data.
Focusing on acquisition of MWEs, early ap-
proaches concentrated on their collocational be-
havior (Church and Hanks, 1989). Pecina (2008)
compares 55 different association measures in
ranking German Adj-N and PP-Verb colloca-
tion candidates. This work shows that combin-
ing different collocation measures using standard
statistical-classification methods (such as Linear
Logistic Regression and Neural Networks) gives
a significant improvement over using a single col-
location measure. Our results show that this is
indeed the case, but the contribution of colloca-
tion methods is limited, and more information is
needed in order to distinguish frequent colloca-
tions from bona fide MWEs.
Other works show that adding linguistic infor-
mation to collocation measures can improve iden-
tification accuracy. Several approaches rely on the
semantic opacity of MWEs; but very few seman-
tic resources are available for Hebrew (the He-
brew WordNet (Ordan and Wintner, 2007), the
only lexical semantic resource for this language,
is small and too limited). Instead, we capital-
15
ize on the morphological and syntactic irregular-
ities that MWEs exhibit, using computational re-
sources that are more readily-available.
Ramisch et al (2008) evaluate a number of
association measures on the task of identifying
English Verb-Particle Constructions and German
Adjective-Noun pairs. They show that adding
linguistic information (mostly POS and POS-
sequence patterns) to the association measure
yields a significant improvement in performance
over using pure frequency. We follow this line
of research by defining a number of syntactic pat-
terns as a source of linguistic information. In ad-
dition, our linguistic features are much more spe-
cific to the phenomenon we are interested in, and
the syntactic patterns are enriched by morpholog-
ical information pertaining to the idiosyncrasy of
MWEs; we believe that this explains the improved
performance compared to the baseline.
Several works address the lexical fixedness or
syntactic fixedness of (certain types of) MWEs in
order to extract them from texts. An expression
is considered lexically fixed if replacing any of its
constituents by a semantically (and syntactically)
similar word generally results in an invalid or lit-
eral expression. Syntactically fixed expressions
prohibit (or restrict) syntactic variation.
For example, Van de Cruys and Villada Moiro?n
(2007) use lexical fixedness to extract Dutch Verb-
Noun idiomatic combinations (VNICs). Bannard
(2007) uses syntactic fixedness to identify En-
glish VNICs. Another work uses both the syn-
tactic and the lexical fixedness of VNICs in or-
der to distinguish them from non-idiomatic ones,
and eventually to extract them from corpora (Fa-
zly and Stevenson, 2006). While these approaches
are in line with ours, they require lexical seman-
tic resources (e.g., a database that determines se-
mantic similarity among words) and syntactic re-
sources (parsers) that are unavailable for Hebrew
(and many other languages). Our approach only
requires morphological processing, which is more
readily-available for several languages.
Another unique feature of our work is that
it computationally addresses Hebrew (and, more
generally, Semitic) MWEs for the first time.
Berman and Ravid (1986) define the dictionary
degree of noun compounds in Hebrew as their
closeness to a single word from a grammatical
point of view, as judged by the manner in which
they are grasped by language speakers. A group
of 120 Hebrew speakers were asked to assign a
dictionary degree (from 1 to 5) to a list of 30
noun compounds. An analysis of the question-
naire results revealed that language speaker share
a common dictionary, where the highest degree of
agreement was achieved on the ends of the dictio-
nary degree spectrum. Another conclusion is that
both the pragmatic uses of the noun compound
and the semantic relation between its constituents
define the dictionary degree of the compound. Not
having access to semantic and pragmatic knowl-
edge, we are trying to approximate it using mor-
phology.
Attia (2005) proposes methods to process
fixed, semi-fixed, and syntactically-flexible Ara-
bic MWEs (adopting the classification and the ter-
minology of Sag et al (2002)). Fabri (2009) pro-
vides an overview of the different types of com-
pounds (14 in total) in present-day Maltese, fo-
cusing on one type of compounds consisting of an
adjective followed by a noun. He also provides
morphological, syntactic, and semantic properties
of this group which distinguishes them from other
non-compound constructions. Automatic identifi-
cation of MWEs is not addressed in either of these
works.
5 Conclusions and future work
We described a system that can identify Hebrew
noun compounds with high accuracy, distinguish-
ing them from non-idiomatic noun-noun construc-
tions. The methodology we advocate is based on
careful examination of the linguistic peculiarities
of the construction, followed by corpus-based ap-
proximation of these properties via a general ma-
chine learning algorithm that is fed with features
based on the linguistic properties. While our ap-
plication is limited to a particular construction in
a particular language, we are confident that it can
be equally well applied to other constructions and
other languages, as long as the targeted MWEs
exhibit a consistent set of irregular features (es-
pecially in the morphology).
This work can be extended in various direc-
tions. Addressing other constructions is relatively
16
easy, and requires only a theoretical linguistic in-
vestigation of the construction. We are currently
interested in extending the system to cope also
with Adjective-Noun, Noun-Adjective and Verb-
Preposition constructions in Hebrew.
The accuracy of MWE acquisition systems can
be further improved by combining our morpho-
logical and syntactic features with semantically
informed features such as translational entropy
computed from a parallel corpus (Villada Moiro?n
and Tiedemann, 2006), or features that can cap-
ture the local linguistic context of the expression
using latent semantic analysis (Katz and Gies-
brecht, 2006). We are currently working on the
former direction (Tsvetkov and Wintner, 2010b),
utilizing a small Hebrew-English parallel corpus
(Tsvetkov and Wintner, 2010a).
Finally, we are interested in evaluating the
methodology proposed in this paper to other lan-
guages with complex morphology, in particular to
Arabic. We leave this direction to future research.
Acknowledgments
This research was supported by THE IS-
RAEL SCIENCE FOUNDATION (grants
No. 137/06, 1269/07). We are grateful to Alon
Itai for his continuous help and advice throughout
the course of this project, and to Bracha Nir for
very useful comments. We also wish to thank
Yulia Tsvetkov and Gily Chen for their annotation
work.
References
Alegria, In?aki, Olatz Ansa, Xabier Artola, Nerea
Ezeiza, Koldo Gojenola, and Ruben Urizar. 2004.
Representation and treatment of multiword expres-
sions in Basque. In Tanaka, Takaaki, Aline Villavi-
cencio, Francis Bond, and Anna Korhonen, editors,
Second ACL Workshop on Multiword Expressions:
Integrating Processing, pages 48?55, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Attia, Mohammed A. 2005. Accommodat-
ing multiword expressions in an lfg grammar.
The ParGram Meeting, Japan September 2005,
September. Mohammed A. Attia The Univer-
sity of Manchester School of Informatics mo-
hammed.attia@postgrad.manchester.ac.uk.
Bannard, Colin. 2007. A measure of syntactic flexibil-
ity for automatically identifying multiword expres-
sions in corpora. In Proceedings of the Workshop on
A Broader Perspective on Multiword Expressions,
pages 1?8. Association for Computational Linguis-
tics.
Bar-haim, Roy, Khalil Sima?an, and Yoad Winter.
2008. Part-of-speech tagging of Modern Hebrew
text. Natural Language Engineering, 14(2):223?
251.
Berman, Ruth A. and Dorit Ravid. 1986. Lexicaliza-
tion of noun compounds. Hebrew Linguistics, 24:5?
22. In Hebrew.
Borer, Hagit. 1988. On the morphological parallelism
between compounds and constructs. In Booij, Geert
and Jaap van Marle, editors, Yearbook of Morphol-
ogy 1, pages 45?65. Foris publications, Dordrecht,
Holland.
Borer, Hagit. 1996. The construct in review.
In Lecarme, Jacqueline, Jean Lowenstamm, and
Ur Shlonsky, editors, Studies in Afroasiatic Gram-
mar, pages 30?61. Holland Academic Graphics,
The Hague.
Chang, Chih-Chung and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Church, Kenneth. W. and Patrick Hanks. 1989. Word
association norms, mutual information and lexicog-
raphy (rev). Computational Linguistics, 19(1):22?
29.
Evert, Stefan and Brigitte Krenn. 2001. Methods for
the qualitative evaluation of lexical association mea-
sures. In Proceedings of the 39th Annual Meeting
of the Association for Computational Linguistics,
pages 188?195, Morristown, NJ, USA. Association
for Computational Linguistics.
Fabri, Ray. 2009. Compounding and adjective-noun
compounds in Maltese. In Comrie, Bernard, Ray
Fabri, Elizabeth Hume, Manwel Mifsud, Thomas
Stolz, and Martine Vanhove, editors, Introducing
Maltese Linguistics, volume 113 of Studies in Lan-
guage Companion Series. John Benjamins.
Fazly, Afsaneh and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations. In Proceedings of the 11th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL), pages
337?344.
Glinert, Lewis. 1989. The Grammar of Modern He-
brew. Cambridge University Press, Cambridge.
17
Itai, Alon and Shuly Wintner. 2008. Language re-
sources for Hebrew. Language Resources and Eval-
uation, 42:75?98, March.
Katz, Graham and Eugenie Giesbrecht. 2006. Au-
tomatic identification of non-compositional multi-
word expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties, pages 12?19, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Levi, Judith N. 1976. A semantic analysis of Hebrew
compound nominals. In Cole, Peter, editor, Stud-
ies in Modern Hebrew Syntax and Semantics, num-
ber 32 in North-Holland Linguistic Series, pages 9?
55. North-Holland, Amsterdam.
Manning, Christopher D. and Hinrich Schu?tze. 1999.
Foundations of statistical natural language process-
ing. The MIT Press, Cambridge, Mass.
Oflazer, Kemal, O?zlem C?etinog?lu, and Bilge Say.
2004. Integrating morphology with multi-word ex-
pression processing in Turkish. In Tanaka, Takaaki,
Aline Villavicencio, Francis Bond, and Anna Ko-
rhonen, editors, Second ACL Workshop on Multi-
word Expressions: Integrating Processing, pages
64?71, Barcelona, Spain, July. Association for
Computational Linguistics.
Ordan, Noam and Shuly Wintner. 2007. Hebrew
WordNet: a test case of aligning lexical databases
across languages. International Journal of Transla-
tion, special issue on Lexical Resources for Machine
Translation, 19(1).
Pecina, Pavel. 2008. A machine learning approach
to multiword expression extraction. In Proceedings
of the LREC Workshop Towards a Shared Task for
Multiword Expressions.
Ramisch, Carlos, Paulo Schreiner, Marco Idiart, and
Alline Villavicencio. 2008. An evaluation of meth-
ods for the extraction of multiword expressions.
In Proceedings of the LREC Workshop Towards a
Shared Task for Multiword Expressions.
Sag, Ivan, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the Third International Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLING 2002), pages 1?15, Mexico City,
Mexico.
Tsvetkov, Yulia and Shuly Wintner. 2010a. Automatic
acquisition of parallel corpora from websites with
dynamic content. In Proceedings of the Seventh
conference on International Language Resources
and Evaluation (LREC?10), pages 3389?3392. Eu-
ropean Language Resources Association (ELRA),
May.
Tsvetkov, Yulia and Shuly Wintner. 2010b. Ex-
traction of multi-word expressions from small par-
allel corpora. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING 2010), August.
Van de Cruys, Tim and Begon?a Villada Moiro?n. 2007.
Semantics-based multiword expression extraction.
In Proceedings of the Workshop on A Broader
Perspective on Multiword Expressions, pages 25?
32, Prague, Czech Republic, June. Association for
Computational Linguistics.
Villada Moiro?n, Begon?a and Jo?rg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word alignment. In Proceedings of the EACL 2006
Workshop on Multi-word-expressions in a multilin-
gual context. Association for Computational Lin-
guistics.
Villavicencio, Aline, Francis Bond, Anna Korhonen,
and Diana McCarthy. 2005. Introduction to the
special issue on multiword expressions: Having a
crack at a hard nut. Computer Speech & Language,
19(4):365?377.
Wintner, Shuly. 2000. Definiteness in the Hebrew
noun phrase. Journal of Linguistics, 36:319?363.
Yona, Shlomo and Shuly Wintner. 2008. A finite-state
morphological grammar of Hebrew. Natural Lan-
guage Engineering, 14(2):173?190, April.
18
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 66?70,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Turker-Assisted Paraphrasing for English-Arabic Machine Translation
Michael Denkowski and Hassan Al-Haj and Alon Lavie
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA
{mdenkows,hhaj,alavie}@cs.cmu.edu
Abstract
This paper describes a semi-automatic para-
phrasing task for English-Arabic machine
translation conducted using Amazon Me-
chanical Turk. The method for automatically
extracting paraphrases is described, as are
several human judgment tasks completed by
Turkers. An ideal task type, revised specif-
ically to address feedback from Turkers, is
shown to be sophisticated enough to identify
and filter problem Turkers while remaining
simple enough for non-experts to complete.
The results of this task are discussed along
with the viability of using this data to combat
data sparsity in MT.
1 Introduction
Many language pairs have large amounts of paral-
lel text that can be used to build statistical machine
translation (MT) systems. For such language pairs,
resources for system tuning and evaluation tend to
be disproportionately abundant in the language typ-
ically used as the target. For example, the NIST
Open Machine Translation Evaluation (OpenMT)
2009 (Garofolo, 2009) constrained Arabic-English
development and evaluation data includes four En-
glish translations for each Arabic source sentence,
as English is the usual target language. However,
when considering this data to tune and evaluate
an English-to-Arabic system, each English sentence
has a single Arabic translation and such translations
are often identical. With at most one reference trans-
lation for each source sentence, standard minimum
error rate training (Och, 2003) to the BLEU met-
ric (Papineni et al, 2002) becomes problematic, as
BLEU relies on the availability of multiple refer-
ences.
We describe a semi-automatic paraphrasing
technique that addresses this problem by identifying
paraphrases that can be used to create new reference
translations based on valid phrase substitutions on
existing references. Paraphrases are automatically
extracted from a large parallel corpus and filtered by
quality judgments collected from human annotators
using Amazon Mechanical Turk. As Turkers are
not trained to complete natural language processing
(NLP) tasks and can dishonestly submit random
judgments, we develop a task type that is able to
catch problem Turkers while remaining simple
enough for untrained annotators to understand.
2 Data Set
The parallel corpus used for paraphrasing con-
sists of all Arabic-English sentence pairs in the
NIST OpenMT Evaluation 2009 (Garofolo, 2009)
constrained training data. The target corpus to be
paraphrased consists of the 728 Arabic sentences
from the OpenMT 2002 (Garofolo, 2002) develop-
ment data.
2.1 Paraphrase Extraction
We conduct word alignment and phrase extraction
on the parallel data to produce a phrase table con-
taining Arabic-English phrase pairs (a, e) with trans-
lation probabilities P (a|e) and P (e|a). Follow-
66
ing Bannard and Callison-Burch (2005), we iden-
tify Arabic phrases (a1) in the target corpus that are
translated by at least one English phrase (e). We
identify paraphrase candidates as alternate Arabic
phrases (a2) that translate e. The probability of a2
being a paraphrase of a1 given foreign phrases e is
defined:
P (a2|a1) =
?
e
P (e|a1)P (a2|e)
A language model trained on the Arabic side of the
parallel corpus is used to further score the possi-
ble paraphrases. As each original phrase (a1) oc-
curs in some sentence (s1) in the target corpus, a
paraphrased sentence (s2) can be created by replac-
ing a1 with one of its paraphrases (a2). The final
paraphrase score considers context, scaling the para-
phrase probability proportionally to the change in
log-probability of the sentence:
F (a2, s2|a1, s1) = P (a2|a1)
logP (s1)
logP (s2)
These scores can be combined for each pair (a1, a2)
to obtain overall paraphrase scores, however we
use the F scores directly as our task considers the
sentences in which paraphrases occur.
3 Turker Paraphrase Assessment
To determine which paraphrases to use to trans-
form the development set references, we elicit bi-
nary judgments of quality from human annotators.
While collecting this data from experts would be ex-
pensive and time consuming, Amazon?s Mechani-
cal Turk (MTurk) service facilitates the rapid collec-
tion of large amounts of inexpensive data from users
around the world. As these users are not trained
to work on natural language processing tasks, any
work posted on MTurk must be designed such that
it can be understood and completed successfully by
untrained annotators. Further, some Turkers attempt
to dishonestly profit from entering random answers,
creating a need for tasks to have built-in measures
for identifying and filtering out problem Turkers.
Our original evaluation task consists of eliciting
two yes/no judgments for each paraphrase and cor-
responding sentence. Shown the original phrase
(a1) and the paraphrase (a2), annotators are asked
whether or not these two phrases could have the
same meaning in some possible context. Annotators
are then shown the original sentence (s1) and the
paraphrased sentence (s2) and asked whether these
two sentences have the same meaning. This task has
the attractive property that if s1 and s2 have the same
meaning, a1 and a2 can have the same meaning. An-
notators assigning ?yes? to the sentence pair should
always assign ?yes? to the phrase pair.
To collect these judgments from MTurk, we de-
sign a human intelligence task (HIT) that presents
Turkers with two instances of the above task along
with a text area for optional feedback. The task
description asks skilled Arabic speakers to evalu-
ate paraphrases of Arabic text. For each HIT, we
pay Turkers $0.01 and Amazon fees of $0.005 for
a total label cost of $0.015. For our initial test,
we ask Turkers to evaluate the 400 highest-scoring
paraphrases, collecting 3 unique judgments for each
paraphrase in and out of context. These HITs were
completed at a rate of 200 per day.
Examining the results, we notice that most
Turkers assign ?yes? to the sentence pairs more
often than to the phrase pairs, which should not be
possible. To determine whether quality of Turkers
might be an issue, we run another test for the same
400 paraphrases, this time paying Turkers $0.02 per
HIT and requiring a worker approval rate of 98% to
work on this task. These HITs, completed by high
quality Turkers at a rate of 100 per day, resulted
in similarly impossible data. However, we also
received valuable feedback from one of the Turkers.
3.1 Turker Feedback
We received a comment from one Turker that
our evaluation task was causing confusion. The
Turker would select ?no? for some paraphrase in
isolation due to missing information. However, the
Turker would then select ?yes? for the paraphrased
sentence, as the context surrounding the phrase
rendered the missing information unnecessary.
This illustrates the point that untrained annotators
understand the idea of ?possible context? differently
from experts and allows us to restructure our HITs
to be ideal for untrained Turkers.
67
3.2 Revised Main Task
We simplify our task to eliminate as many sources
of ambiguity as possible. Our revised task simply
presents annotators with the original sentence la-
beled ?sentence 1? and the paraphrased sentence la-
beled ?sentence 2?, and asks whether or not the two
sentences have the same meaning. Each HIT, titled
?Evaluate Arabic Sentences?, presents Turkers with
2 such tasks, pays $0.02, and costs $0.005 in Ama-
zon fees.
Without additional consideration, this task re-
mains highly susceptible to random answers from
dishonest or unreliable Turkers. To ensure that such
Turkers are identified and removed, we intersperse
absolute positive and negative examples with the
sentence pairs from our data set. Absolute posi-
tives consist of the same original sentence s1 re-
peated twice and should always receive a ?yes? judg-
ment. Absolute negatives consist of some origi-
nal s1 and a different, randomly selected original
sentence s?1 with several words dropped to obscure
meaning. Absolute negatives should always receive
a ?no? judgment. Positive and negative control cases
can be inserted with a frequency based either on de-
sired confidence that enough cases are encountered
for normalization or on the availability of funds.
Inserting either a positive or negative control
case every 5th task increases the per-label cost to
$0.0156. We use this task type to collect 3 unique
judgments for each of the 1280 highest-scoring
paraphrases at a total cost of $60.00 for 2400 HITs.
These HITs were completed substantially faster at a
rate of 500-1000 per day. The results of this task are
discussed in section 4.
3.3 Editing Task
We conduct an additional experiment to see if Turk-
ers will fix paraphrases judged to be incorrect. The
task extends the sentence evaluation task described
in the previous section by asking Turkers who select
?no? to edit the paraphrase text in the second sen-
tence such that the sentences have the same mean-
ing. While the binary judgment task is used for fil-
tering only, this editing task ensures a usable data
point for every HIT completed. As such, fewer total
HITs are required and high quality Turkers can be
0 0.25 0.5 0.75 1
0
2
4
6
8
10
12
14
16
18
20
Accuracy of judgments of control cases
Nu
mb
er 
of T
urk
ers
Figure 1: Turker accuracy classifying control cases
paid more for each HIT. We run 3 sequential tests
for this task, offering $0.02, $0.04, and $0.10 per
paraphrase approved or edited.
Examining the results, we found that regardless
of price, very few paraphrases were actually edited,
even when Turkers selected ?no? for sentence
equality. While this allows us to easily identify and
remove problem Turkers, it does not solve the issue
that honest Turkers either cannot or will not provide
usable paraphrase edits for this price range. A brief
examination by an expert indicates that the $0.02
per HIT edits are actually better than the $0.10 per
HIT edits.
4 Results
Our main task of 2400 HITs was completed through
the combined effort of 47 unique Turkers. As shown
Figure 1, these Turkers have varying degrees of ac-
curacy classifying the control cases. The two most
common classes of Turkers include (1) those spend-
ing 15 or more seconds per judgment and scoring
above 0.9 accuracy on the control cases and (2) those
spending 5-10 seconds per judgment and scoring be-
tween 0.4 and 0.6 accuracy as would be expected by
chance. As such, we accept but do not consider the
judgments of Turkers scoring between 0.7 and 0.9
accuracy on the control set, and reject all HITs for
Turkers scoring below 0.7, republishing them to be
completed by other workers.
68
Decision Confirm Reject Undec.
Paraphrases 726 423 131
Table 1: Turker judgments of top 1280 paraphrases
Figure 2: Paraphrases confirmed by Turkers
After removing judgments from below-threshold
annotators, all remaining judgments are used to
confirm or reject the covered paraphrases. If a
paraphrase has at least 2 remaining judgments, it is
confirmed if at least 2 annotators judge it positively
and rejected otherwise. Paraphrases with fewer than
2 remaining judgments are considered undecidable.
Table 1 shows the distribution of results for the 1280
top-scoring paraphrases. As shown in the table,
726 paraphrases are confirmed as legitimate phrase
substitutions on reference translations, providing
an average of almost one paraphrase per reference.
Figures 2 and 3 show example Arabic paraphrases
filtered by Turkers.
5 Conclusions
We have presented a semi-automatic paraphrasing
technique for creating additional reference transla-
tions. The paraphrase extraction technique provides
a ranked list of paraphrases and their contexts which
can be incrementally filtered by human judgments.
Our judgment task is designed to address specific
Turker feedback, remaining simple enough for
non-experts while successfully catching problem
users. The $60.00 worth of judgments collected
produces enough paraphrases to apply an average
Figure 3: Paraphrases rejected by Turkers
of one phrase substitution to each reference. Our
future work includes collecting sufficient data to
substitute multiple paraphrases into each Arabic
reference in our development set, producing a full
additional set of reference translations for use tuning
our English-to-Arabic MT system. The resulting
individual paraphrases can also be used for other
tasks in MT and NLP.
Acknowledgements
This work was supported by a $100 credit from
Amazon.com, Inc. as part of a shared task for the
NAACL 2010 workshop ?Creating Speech and Lan-
guage Data With Amazon?s Mechanical Turk?.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proc. of
ACL.
John Garofolo. 2002. NIST OpenMT Eval. 2002.
http://www.itl.nist.gov/iad/mig/tests/mt/2002/.
John Garofolo. 2009. NIST OpenMT Eval. 2009.
http://www.itl.nist.gov/iad/mig/tests/mt/2009/.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of ACL.
69
Fi
gu
re
4:
E
xa
m
pl
e
H
IT
as
se
en
by
T
ur
ke
rs
70
