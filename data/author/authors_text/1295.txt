Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 810?818, Prague, June 2007. c?2007 Association for Computational Linguistics
A Statistical Language Modeling Approach to
Lattice-Based Spoken Document Retrieval
Tee Kiah Chia? Haizhou Li? Hwee Tou Ng?
?Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
{chiateek,nght}@comp.nus.edu.sg
?Institute for Infocomm Research
21 Heng Mui Keng Terrace
Singapore 119613
hli@i2r.a-star.edu.sg
Abstract
Speech recognition transcripts are far from
perfect; they are not of sufficient quality to
be useful on their own for spoken document
retrieval. This is especially the case for con-
versational speech. Recent efforts have tried
to overcome this issue by using statistics
from speech lattices instead of only the 1-
best transcripts; however, these efforts have
invariably used the classical vector space re-
trieval model. This paper presents a novel
approach to lattice-based spoken document
retrieval using statistical language models: a
statistical model is estimated for each doc-
ument, and probabilities derived from the
document models are directly used to mea-
sure relevance. Experimental results show
that the lattice-based language modeling
method outperforms both the language mod-
eling retrieval method using only the 1-best
transcripts, as well as a recently proposed
lattice-based vector space retrieval method.
1 Introduction
Information retrieval (IR) is the task of ranking a
collection of documents according to an estimate of
their relevance to a query. With the recent growth
in the amount of speech recordings in the form of
voice mails, news broadcasts, and so forth, the task
of spoken document retrieval (SDR) ? information
retrieval in which the document collection is in the
form of speech recordings ? is becoming increas-
ingly important.
SDR on broadcast news corpora has been
?deemed to be a solved problem?, due to the fact that
the performance of retrieval engines working on 1-
best automatic speech recognition (ASR) transcripts
was found to be ?virtually the same as their perfor-
mance on the human reference transcripts? (NIST,
2000). However, this is still not the case for SDR
on data which are more challenging, such as conver-
sational speech in noisy environments, as the 1-best
transcripts of these data contain too many recogni-
tion errors to be useful for retrieval. One way to
ameliorate this problem is to work with not just one
ASR hypothesis for each utterance, but multiple hy-
potheses presented in a lattice data structure. A lat-
tice is a connected directed acyclic graph in which
each edge is labeled with a term hypothesis and a
likelihood value (James, 1995); each path through a
lattice gives a hypothesis of the sequence of terms
spoken in the utterance.
Each lattice can be viewed as a statistical model
of the possible transcripts of an utterance (given the
speech recognizer?s state of knowledge); thus, an
IR model based on statistical inference will seem
to be a more natural and more principled approach
to lattice-based SDR. This paper thus proposes a
lattice-based SDR method based on the statistical
language modeling approach of Song and Croft
(1999). In this method, the expected word count ?
the mean number of occurrences of a word given
a lattice?s statistical model ? is computed for each
word in each lattice. Using these expected counts,
a statistical language model is estimated for each
spoken document, and a document?s relevance to a
query is computed as a probability under this model.
810
The rest of this paper is organized as follows. In
Section 2 we review related work in the areas of
speech processing and IR. Section 3 describes our
proposed method as well as the baseline methods.
Details of the experimental setup are given in Sec-
tion 4, and experimental results are in Section 5. Fi-
nally, Section 6 concludes our discussions and out-
lines our future work.
2 Related Work
2.1 Lattices for Spoken Document Retrieval
James and Young (1994) first introduced the lattice
as a representation for indexing spoken documents,
as part of a method for vocabulary-independent key-
word spotting. The lattice representation was later
applied to the task of spoken document retrieval
by James (1995): James counted how many times
each query word occurred in each phone lattice with
a sufficiently high normalized log likelihood, and
these counts were then used in retrieval under a vec-
tor space model with tf ? idf weighting. Jones et al
(1996) combined retrieval from phone lattices using
variations of James? method with retrieval from 1-
best word transcripts to achieve better results.
Since then, a number of different methods for
SDR using lattices have been proposed. For in-
stance, Siegler (1999) used word lattices instead of
phone lattices as the basis of retrieval, and gener-
alized the tf ? idf formalism to allow uncertainty
in word counts. Chelba and Acero (2005) prepro-
cessed lattices into more compact Position Specific
Posterior Lattices (PSPL), and computed an aggre-
gate score for each document based on the poste-
rior probability of edges and the proximity of search
terms in the document. Mamou et al (2006) con-
verted each lattice into a word confusion network
(Mangu et al, 2000), and estimated the inverse doc-
ument frequency (idf ) of each word t as the ratio of
the total number of words in the document collection
to the total number of occurrences of t.
Despite the differences in the details, the above
lattice-based SDR methods have all been based on
the classical vector space retrieval model with tf ?idf
weighting.
2.2 Expected Counts from Lattices
A speech recognizer generates a 1-best transcript
of a spoken document by considering possible tran-
scripts of the document, and then selecting the tran-
script with the highest probability. However, unlike
a text document, such a 1-best transcript is likely to
be inexact due to speech recognition errors. To rep-
resent the uncertainty in speech recognition, and to
incorporate information from multiple transcription
hypotheses rather than only the 1-best, it is desirable
to use expected word counts from lattices output by
a speech recognizer.
In the context of spoken document search, Siegler
(1999) described expected word counts and for-
mulated a way to estimate expected word counts
from lattices based on the relative ranks of word
hypothesis probabilities; Chelba and Acero (2005)
used a more explicit formula for computing word
counts based on summing edge posterior probabili-
ties in lattices; Saraclar and Sproat (2004) performed
word-spotting in speech lattices by looking for word
occurrences whose expected counts were above a
certain threshold; and Yu et al (2005) searched for
phrases in spoken documents using a similar mea-
sure, the expected word relevance.
Expected counts have also been used to sum-
marize the phonotactics of a speech recording rep-
resented in a lattice: Hatch et al (2005) per-
formed speaker recognition by computing the ex-
pected counts of phone bigrams in a phone lattice,
and estimating an unsmoothed probability distribu-
tion of phone bigrams.
Although many uses of expected counts have been
studied, the use of statistical language models built
from expected word counts has not been well ex-
plored.
2.3 Retrieval via Statistical Language
Modeling
Finally, the statistical language modeling approach
to retrieval was used by Ponte and Croft (1998) for
IR with text documents, and it was shown to outper-
form the tf ? idf approach for this task; this method
was further improved on in Song and Croft (1999).
Chen et al (2004) applied Song and Croft?s method
to Mandarin spoken document retrieval using 1-best
ASR transcripts. In this task, it was also shown to
811
outperform tf ? idf . Thus, the statistical language
modeling approach to retrieval has been shown to be
superior to the vector space approach for both these
IR tasks.
2.4 Contributions of Our Work
The main contributions of our work include
? extending the language modeling IR approach
from text-based retrieval to lattice-based spo-
ken document retrieval; and
? formulating a method for building a statistical
language model based on expected word counts
derived from lattices.
Our method is motivated by the success of the sta-
tistical retrieval framework over the vector space ap-
proach with tf ? idf for text-based IR, as well as
for spoken document retrieval via 1-best transcripts.
Our use of expected counts differs from Saraclar and
Sproat (2004) in that we estimate probability mod-
els from the expected counts. Conceptually, our
method is close to that of Hatch et al (2005), as
both methods build a language model to summa-
rize the content of a spoken document represented
in a lattice. In practice, our method differs from
Hatch et al (2005)?s in many ways: first, we derive
word statistics for representing semantics, instead of
phone bigram statistics for representing phonotac-
tics; second, we introduce a smoothing mechanism
(Zhai and Lafferty, 2004) to the language model that
is specific for information retrieval.
3 Methods
We now describe the formulation of three different
SDR methods: a baseline statistical retrieval method
which works on 1-best transcripts, our proposed sta-
tistical lattice-based SDR method, as well as a pre-
viously published vector space lattice-based SDR
method.
3.1 Baseline Statistical Retrieval Method
Our baseline retrieval method is motivated by Song
and Croft (1999), and uses the language model
smoothing methods of Zhai and Lafferty (2004).
This method is used to perform retrieval on the docu-
ments? 1-best ASR transcripts and reference human
transcripts.
Let C be the collection of documents to retrieve
from. For each document d contained in C, and each
query q, the relevance of d to q can be defined as
Pr(d | q). This probability cannot be computed di-
rectly, but under the assumption that the prior Pr(d)
is uniform over all documents in C, we see that
Pr(d | q) = Pr(q | d) Pr(d)Pr(q) ? Pr(q | d);
This means that ranking documents by Pr(d | q) is
equivalent to ranking them by Pr(q | d), and thus
Pr(q | d) can be used to measure relevance (Berger
and Lafferty, 1999).
Now express q as a series of words drawn from
a vocabulary V = {w1, w2, ? ? ?wV }; that is, q =
q1q2 ? ? ? qK , where K is the number of words in the
query, and qi ? V for 1 ? i ? K. Then given
a unigram model derived from d which assigns a
probability Pr(w | d) to each word w in V , we can
compute Pr(q | d) as follows:
Pr(q | d) = Pr(q1q2 ? ? ? qK | d)
=
K
?
i=1
Pr(qi | d)
=
?
w?V ,
C(w|q)>0
Pr(w|d)C(w|q) (1)
where C(w | q) is the word count of w in q.
Before using Equation 1, we must estimate a uni-
gram model from d: that is, an assignment of proba-
bilities Pr(w | d) for all w ? V . One way to do this
is to use a maximum likelihood estimate (MLE) ? an
assignment of Pr(w | d) for all w which maximizes
the probability of generating d. The MLE is given
by the equation
Pr mle(w | d) =
C(w | d)
|d|
where C(w | d) is the number of occurrences of
w in d, and |d| is the total number of words in d.
However, using this formula means we will get a
value of zero for Pr(q | d) if even a single query
word qi is not found in d. To overcome this problem,
we smooth the model by assigning some probability
mass to such unseen words. Specifically, we adopt
812
a two-stage smoothing method (Zhai and Lafferty,
2004):
Pr(w | d) = (1 ? ?)C(w | d) + ?Pr(w | C)|d| + ?
+?Pr(w | U) (2)
Here, U denotes a background language model, and
? > 0 and ? ? (0, 1) are parameters to the smooth-
ing procedure. This is a combination of Bayesian
smoothing using Dirichlet priors (MacKay and Peto,
1984) and Jelinek-Mercer smoothing (Jelinek and
Mercer, 1980).
The parameter ? can be set empirically according
to the nature of the queries. For the parameter ?, we
adopt the estimation procedure of Zhai and Lafferty
(2004): we maximize the leave-one-out log likeli-
hood of the document collection, namely
??1(? | C) =
?
d?C
?
w?V
C(w | d)
log
(
C(w | d) ? 1 + ?Pr(w | C)
|d| ? 1 + ?
)
(3)
by using Newton?s method to solve the equation
???1(? | C) = 0
3.2 Our Proposed Statistical Lattice-Based
Retrieval Method
We now propose our lattice-based retrieval method.
In contrast to the above baseline method, our pro-
posed method works on the lattice representation of
spoken documents, as generated by a speech recog-
nizer.
First, each spoken document is divided into M
short speech segments. A speech recognizer then
generates a lattice for each speech segment. As
previously stated, a lattice is a connected directed
acyclic graph with edges labeled with word hypothe-
ses and likelihoods. Thus, each path through the lat-
tice contains a hypothesis of the series of words spo-
ken in this speech segment, t = t1t2 ? ? ? tN , along
with acoustic probabilities Pr(o1 | t1), Pr(o2 | t2),
? ? ? Pr(oN | tN ), where oi denotes the acoustic
observations for the time interval of the word ti
hypothesized by the speech recognizer. Let o =
o1o2 ? ? ? oN denote the acoustic observations for the
entire speech segment; then
Pr(o | t) =
N
?
i=1
Pr(oi | ti)
We then rescore each lattice with an n-gram lan-
guage model. Effectively, this means multiplying
the acoustic probabilities with n-gram probabilities:
Pr(t,o) = Pr(o | t) Pr(t)
=
N
?
i=1
Pr(oi | ti) Pr(ti | ti?n+1 ? ? ? ti?1)
This produces an expanded lattice in which paths
(hypotheses) are weighted by their posterior proba-
bilities rather than their acoustic likelihoods: specif-
ically, by Pr(t,o) ? Pr(t | o) rather than Pr(o | t)
(Odell, 1995). The lattice is then pruned, by remov-
ing those paths in the lattice whose log posterior
probabilities ? to be precise, whose ? ln Pr(t | o)
? are not within a threshold ? of the best path?s log
posterior probability (in our implementation, ? =
10000.5).
Next, we compute the expected count of each
word in each document. For each word w and each
document d comprised of M speech segments rep-
resented by M acoustic observations o(1), o(2), ? ? ?
o(M), the expected count of w in d is
E[C(w | d)] =
M
?
j=1
?
t
C(w | t) Pr(t | o(j))
where C(w | t) is the word count of w in the hy-
pothesized transcript t. We can also analogously
compute the expected document length:
E[|d|] =
M
?
j=1
?
t
|t|Pr(t | o(j))
where |t| denotes the number of words in t.
We now replace C(w | d) and |d| in Equation 2
with E[C(w | d)] and E[|d|]; thus
Pr(w | d) = (1 ? ?)E[C(w | d)] + ?Pr(w | C)E[|d|] + ?
+?Pr(w | U) (4)
In addition, we also modify the procedure for
estimating ?, by replacing C(w | d) and
813
? 0.528
? 0.472
?
 0.580
3 0.404
? 0.016
?
 0.764
? 0.236
?
 0.764
?q 0.099
?- 0.071
?? 0.066
?
? 0.673
? 0.327
Figure 1: Example of a word confusion network
|d| in Equation 3 with
?
E[C(w | d)] + 12
?
and
?
w?V
?
E[C(w | d)] + 12
?
respectively. The prob-
ability estimates from Equation 4 can then be sub-
stituted into Equation 1 to yield relevance scores.
3.3 Baseline tf ? idf Lattice-Based Retrieval
Method
As a further comparison, we also implemented
Mamou et al (2006)?s vector space retrieval method
(without query refinement via lexical affinities). In
this method, each document d is represented as
a word confusion network (WCN) (Mangu et al,
2000) ? a simplified lattice which can be viewed as
a sequence of confusion sets c1, c2, c3, ? ? ? . Each ci
corresponds approximately to a time interval in the
spoken document and contains a group of word hy-
potheses, and each word w in this group of hypothe-
ses is labeled with the probability Pr(w | ci,d) ? the
probability that w was spoken in the time interval of
ci. A confusion set may also give a probability for
Pr(? | ci,d), the probability that no word was spo-
ken in the time of ci. Figure 1 gives an example of a
WCN.
Mamou et al?s retrieval method proceeds as fol-
lows. First, the documents are divided into speech
segments, lattices are generated from the speech seg-
ments, and the lattices are pruned according to the
path probability threshold ?, as described in Sec-
tion 3.2. The lattice for each speech segment is then
converted into a WCN according to the algorithm
of Mangu et al (2000). The WCNs for the speech
segments in each document are then concatenated to
form a single WCN per document.
Now, to retrieve documents in response to a query
q, the method computes, for each document d ? C
and each word w ? V ,
? the ?document length? |d|, computed as the
number of confusion sets in the WCN of d;
? the ?average document length? avdl, computed
as
avdl = 1|C|
?
d??C
?
?d?
?
? ;
? the ?document term frequency? C?(w | d),
computed as
C?(w|d) =
?
c?occ(w,d)
(brank(w|c,d)?Pr(w|c,d))
where occ(w,d) is the set of confusion sets
in d?s WCN which contain w as a hypothe-
sis, rank(w | c,d) is the rank of w in terms
of probability within the confusion set c, and
(b1, b2, b3, ? ? ? ) = (10, 9, 8, 7, 6, 5, 4, 3, 2, 1,
0, 0, 0, ? ? ? ) is a boosting vector which serves
to discard all but the top 10 hypotheses, and
gives more weight to higher-ranked word hy-
potheses;
? the query term frequency C(w | q), which is
simply the word count of w in q; and
? the ?inverse document frequency? idf(w),
computed as
idf(w) = log OOw
where
Ow =
?
d?C
?
c?occ(w,d)
Pr(w | c,d)
O =
?
w??V
Ow?
With these, the relevance of d to q is computed as
(Carmel et al, 2001)
rel(d,q) =
P
w?V C
?(w | d) ? C(w | q) ? idf(w)
p
0.8 ? avdl + 0.2 ? |d|
4 Experiments
4.1 Document Collection
To evaluate our proposed retrieval method, we per-
formed experiments using the Hub5 Mandarin train-
ing corpus released by the Linguistic Data Consor-
tium (LDC98T26). This is a conversational tele-
phone speech corpus which is 17 hours long, and
814
contains recordings of 42 telephone calls corre-
sponding to approximately 600Kb of transcribed
Mandarin text. Each conversation has been broken
up into speech segments of less than 8 seconds each.
As the telephone calls in LDC98T26 have not
been divided neatly into ?documents?, we had to
choose a suitable unit of retrieval which could serve
as a ?document?. An entire conversation would be
too long for such a purpose, while a speech segment
or speaker turn would be too short. We decided to
use 12 -minute time windows with 50% overlap as re-
trieval units, following Abberley et al (1999) and
Tuerk et al (2001). The 42 telephone conversations
were thus divided into 4,312 retrieval units (?doc-
uments?). Each document comprises multiple con-
secutive speech segments.
4.2 Queries and Ground Truth Relevance
Judgements
We then formulated 18 queries (14 test queries, 4
development queries) to issue on the document col-
lection. Each query was comprised of one or more
written Chinese keywords. We then obtained ground
truth relevance judgements by manually examining
each of the 4,312 documents to see if it is relevant
to the topic of each query. The number of retrieval
units relevant to each query was found to range from
4 to 990. The complete list of queries and the num-
ber of documents relevant to each query are given in
Table 1.
4.3 Preprocessing of Documents and Queries
Next, we processed the document collection with a
speech recognizer. For this task we used the Abacus
system (Hon et al, 1994), a large vocabulary contin-
uous speech recognizer which contains a triphone-
based acoustic system and a frame-synchronized
search algorithm for effective word decoding. Each
Mandarin syllable was modeled by one to four tri-
phone models. Acoustic models were trained from
a corpus of 200 hours of telephony speech from
500 speakers sampled at 8kHz. For each speech
frame, we extracted a 39-dimensional feature vec-
tor consisting of 12 MFCCs and normalized en-
ergy, and their first and second order derivatives.
Sentence-based cepstral mean subtraction was ap-
plied for acoustic normalization both in the training
and testing. Each triphone was modeled by a left-
Test queries
Topic Keywords # relevant
documents
Contact information ??,Rh,??,??,v 103
Chicago z? 15
The weather ?,?,y,FZ,Z,?O,?,8?, 117
?,?,?,??
Housing matters 2,,?,2,?,?,?2, 354
??,y?,2?,?
Studies, academia ?,?,A,,?,Wt,'V, 990
1,I,?,D,3
Litigation F,F,K?,?? 31
Raising children B/,/,	?,?,???,m, 334
m?,E?
Christian churches s?, ,?,??,s?,??,?, 78
L?
Floods vy,?,?,y 4
Clothing q,?,?,F,?:g,g, 28
:F,?q,
Eating out ?,j,iq,?j,>0,,? 57
Playing sports KE,??,?|E,\E 24
Dealings with banks Uq,|,,?,TQ 54
Computers and ?,??,?G 175
software
Development queries
Topic Keywords # relevant
documents
Passport and visa ?L,?y,??,C?,I,#? 143
matters
Washington D. C. ?? 15
Working life ??,,K?,{,?*,??,??, 509
??,l,??,??,3/,?,?
1996 Olympics ???,?}L 8
Table 1: List of test and development queries
to-right 3-state hidden Markov model (HMM), each
state having 16 Gaussian mixture components. In
total, we built 1,923 untied within-syllable triphone
models for 43 Mandarin phonemes, as well as 3 si-
lence models. The search algorithm was supported
by a loop grammar of over 80,000 words.
We processed the speech segments in our collec-
tion corpus, to generate lattices incorporating acous-
tic likelihoods but not n-gram model probabilities.
We then rescored the lattices using a backoff tri-
815
gram language model interpolated in equal propor-
tions from two trigram models:
? a model built from the TDT-2, TDT-3, and
TDT-4 Mandarin news broadcast transcripts
(about 58Mb of text)
? a model built from corpora of transcripts of
conversations, comprised of a 320Kb subset of
the Callhome Mandarin corpus (LDC96T16)
and the CSTSC-Flight corpus from the Chinese
Corpus Consortium (950Kb)
The unigram counts from this model were also used
as the background language model U in Equations 2
and 4.
The reference transcripts, queries, and trigram
model training data were all segmented into words
using Low et al (2005)?s Chinese word segmenter,
trained on the Microsoft Research (MSR) corpus,
with the speech recognizer?s vocabulary used as an
external dictionary. The 1-best ASR transcripts were
decoded from the rescored lattices.
Lattice rescoring, trigram model building, WCN
generation, and computation of expected word
counts were done using the SRILM toolkit (Stolcke,
2002), while lattice pruning was done with the help
of the AT&T FSM Library (Mohri et al, 1998).
We also computed the character error rate (CER)
and syllable error rate (SER) of the 1-best tran-
scripts, and the lattice oracle CER, for one of
the telephone conversations in the speech corpus
(ma_4160). The CER was found to be 69%, the
SER 63%, and the oracle CER 29%.
4.4 Retrieval and Evaluation
We then performed retrieval on the document col-
lection using the algorithms in Section 3, using the
reference transcripts, the 1-best ASR transcripts, lat-
tices, and WCNs. We set ? = 0.1, which was sug-
gested by Zhai and Lafferty (2004) to give good re-
trieval performance for keyword queries.
The results of retrieval were checked against the
ground truth relevance judgements, and evaluated in
terms of the non-interpolated mean average preci-
sion (MAP):
MAP = 1L
L
?
i=1
?
?
1
Ri
Ri
?
j=1
j
ri,j
?
?
Retrieval Retrieval MAP for MAP for
method source development test
queries queries
Statistical Reference 0.5052 0.4798
transcripts
Statistical 1-best 0.1251 0.1364
transcripts
Vector space Lattices, 0.1685 0.1599
tf ? idf ? = 27, 500
Statistical Lattices, 0.2180 0.2154
? = 65, 000
Table 2: Summary of experimental results
where L denotes the total number of queries, Ri the
total number of documents relevant to the ith query,
and ri,j the position of the jth relevant document
in the ranked list output by the retrieval method for
query i.
For the lattice-based retrieval methods, we per-
formed retrieval with the development queries using
several values of ? between 0 and 100,000, and then
used the value of ? with the best MAP to do retrieval
with the test queries.
5 Experimental Results
The results of our experiments are summarized
in Table 2; the MAP of the two lattice-based
retrieval methods, Mamou et al (2006)?s vector
space method and our proposed statistical retrieval
method, are shown in Figure 2 and Figure 3 respec-
tively.
The results show that, for the vector space re-
trieval method, the MAP of the development queries
is highest at ? = 27, 500, at which point the MAP
for the test queries is 0.1599; and for our proposed
method, the MAP for the development queries is
highest at ? = 65, 000, and at this point the MAP
for the test queries reaches 0.2154.
As can be seen, the performance of our statistical
lattice-based method shows a marked improvement
over the MAP of 0.1364 achieved using only the 1-
best ASR transcripts, and indeed a one-tailed Stu-
dent?s t-test shows that this improvement is statisti-
cally significant at the 99.5% confidence level. The
statistical method also yields better performance
than Mamou et al?s vector space method ? a t-test
816
For 4 development queries
 0.12
 0.13
 0.14
 0.15
 0.16
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 0  20000  40000  60000  80000  100000
M
AP
? (max. log probability difference of paths)
Retrieval using word probabilities from word confusion networks
? = 27,500
For 14 test queries
 0.13
 0.14
 0.15
 0.16
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 0  20000  40000  60000  80000  100000
M
AP
? (max. log probability difference of paths)
Retrieval using word probabilities from word confusion networks
? = 27,500
Figure 2: MAP of Mamou et al (2006)?s vector
space method for lattice-based retrieval, at various
pruning thresholds ?
shows the performance difference to be statistically
significant at the 97.5% confidence level.
6 Conclusions and Future Work
We have presented a method for performing spo-
ken document retrieval using lattices which is based
on a statistical language modeling retrieval frame-
work. Results show that our new method can sig-
nificantly improve the retrieval MAP compared to
using only the 1-best ASR transcripts. Also, our
proposed retrieval method has been shown to out-
perform Mamou et al (2006)?s vector space lattice-
based retrieval method.
Besides the better empirical performance, our
method also has other advantages over Mamou et
al.?s vector space method. For one, our method com-
putes expected word counts directly from rescored
lattices, and does not require an additional step to
For 4 development queries
 0.12
 0.13
 0.14
 0.15
 0.16
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 0  20000  40000  60000  80000  100000
M
AP
? (max. log probability difference of paths)
Retrieval using expected counts from lattices
Retrieval using 1?best transcripts
? = 65,000
For 14 test queries
 0.13
 0.14
 0.15
 0.16
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 0  20000  40000  60000  80000  100000
M
AP
? (max. log probability difference of paths)
Retrieval using expected counts from lattices
Retrieval using 1?best transcripts
? = 65,000
Figure 3: MAP of our proposed statistical method
for lattice-based retrieval, at various pruning thresh-
olds ?
convert lattices lossily to WCNs. Furthermore, our
method uses all the hypotheses in each lattice, rather
than just the top 10 word hypotheses at each time
interval. Most importantly, our method provides
a more natural and more principled approach to
lattice-based spoken document retrieval based on a
sound statistical foundation, by harnessing the fact
that lattices are themselves statistical models; the
statistical approach also means that our method can
be more easily augmented with additional statistical
knowledge sources in a principled way.
For future work, we plan to test our proposed
method on English speech corpora, and with larger-
scale retrieval tasks involving more queries and
more documents. We would like to extend our
method to other speech processing tasks, such as
spoken document classification and example-based
spoken document retrieval as well.
817
References
Dave Abberley, David Kirby, Steve Renals, and Tony
Robinson. 1999. The THISL broadcast news retrieval
system. In Proceedings of ESCA ETRW Workshop on
Accessing Information in Spoken Audio, pages 14?19.
Adam Berger and John Lafferty. 1999. Information re-
trieval as statistical translation. In Proceedings of SI-
GIR 1999, pages 222?229.
David Carmel, Einat Amitay, Miki Herscovici, Yoelle
Maarek, Yael Petruschka, and Aya Soffer. 2001. Juru
at TREC 10 ? Experiments with index pruning. In
Proceedings of the Tenth Text Retrieval Conference
(TREC-10), pages 228?236.
Ciprian Chelba and Alex Acero. 2005. Position specific
posterior lattices for indexing speech. In Proceedings
of ACL 2005, pages 443?450.
Berlin Chen, Hsin-min Wang, and Lin-shan Lee. 2004. A
discriminative HMM/n-gram-based retrieval approach
for Mandarin spoken documents. ACM Transactions
on Asian Language Information Processing, 3(2):128?
145.
Andrew O. Hatch, Barbara Peskin, and Andreas Stol-
cke. 2005. Improved phonetic speaker recognition us-
ing lattice decoding. In Proceedings of IEEE ICASSP
2005, 1:169?172.
Hsiao-Wuen Hon, Baosheng Yuan, Yen-Lu Chow, S.
Narayan, and Kai-Fu Lee. 1994. Towards large vocab-
ulary Mandarin Chinese speech recognition. In Pro-
ceedings of IEEE ICASSP 1994, 1:545?548.
David Anthony James and Steve J. Young. 1994. A
fast lattice-based approach to vocabulary independent
wordspotting. In Proceedings of ICASSP 1994, 1:377?
380.
David Anthony James. 1995. The Application of Classi-
cal Information Retrieval Techniques to Spoken Docu-
ments. Ph. D. thesis, University of Cambridge.
Frederick Jelinek and Robert L. Mercer. 1980. Interpo-
lated estimation of Markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
tern Recognition in Practice, pages 381?397.
Gareth J. F. Jones, Jonathan T. Foote, Karen Sp?rck
Jones, and Steve J. Young. 1996. Retrieving spoken
documents by combining multiple index sources. In
Proceedings of SIGIR 1996, pages 30?38.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A
maximum entropy approach to Chinese word segmen-
tation. In Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing, pages 161?
164.
David J. C. MacKay and Linda C. Bauman Peto. 1994,
A hierarchical Dirichlet language model. Natural Lan-
guage Engineering, 1(3):1?19.
Jonathan Mamou, David Carmel, and Ron Hoory. 2006.
Spoken document retrieval from call-center conversa-
tions. In Proceedings of SIGIR 2006, pages 51?58.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Ri-
ley. 1998. A rational design for a weighted finite-state
transducer library. Lecture Notes in Computer Science,
1436:144?158.
National Institute of Standards and Technol-
ogy. 2000. TREC-9 SDR Track web site.
www.nist.gov/speech/tests/sdr/sdr2000/sdr2000.htm.
Julian James Odell. 1995. The Use of Context in Large
Vocabulary Speech Recognition. Ph. D. thesis, Cam-
bridge University Engineering Department.
Jay M. Ponte and W. Bruce Croft. 1998. A language mod-
eling approach to information retrieval. In Proceedings
of SIGIR 1998, pages 275?281.
Murat Saraclar and Richard Sproat. 2004. Lattice-based
search for spoken utterance retrieval. In Proceedings
of HLT-NAACL 2004, pages 129?136.
Matthew A. Siegler. 1999. Integration of Continuous
Speech Recognition and Information Retrieval for Mu-
tually Optimal Performance. Ph. D. thesis, Carnegie
Mellon University.
Fei Song and W. Bruce Croft. 1999. A general lan-
guage model for information retrieval. In Proceedings
of CIKM 1999, pages 316?321.
Andreas Stolcke. 2002. SRILM ? An extensible language
modeling toolkit. In Proceedings of ICSLP, 2:901?
904.
Andy Tuerk, Sue E. Johnson, Pierre Jourlin, Karen
Sp?rck Jones, and Philip C. Woodland. 2001. The
Cambridge University multimedia document retrieval
demo system. International Journal of Speech Tech-
nology, 4(3?4):241?250.
Peng Yu, Kaijiang Chen, Lie Lu, and Frank Seide.
2005. Searching the audio notebook: keyword
search in recorded conversations. In Proceedings of
HLT/EMNLP 2005, pages 947?954.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to in-
formation retrieval. ACM Transactions on Information
Systems, 22(2):179?214.
818
Supervised Word Sense Disambiguation with
Support Vector Machines and Multiple Knowledge Sources
Yoong Keok Lee and Hwee Tou Ng and Tee Kiah Chia
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
y.k.lee@alumni.nus.edu.sg
nght@comp.nus.edu.sg
chiateek@comp.nus.edu.sg
Abstract
We participated in the SENSEVAL-3 English lexi-
cal sample task and multilingual lexical sample task.
We adopted a supervised learning approach with
Support Vector Machines, using only the official
training data provided. No other external resources
were used. The knowledge sources used were part-
of-speech of neighboring words, single words in the
surrounding context, local collocations, and syntac-
tic relations. For the translation and sense subtask
of the multilingual lexical sample task, the English
sense given for the target word was also used as
an additional knowledge source. For the English
lexical sample task, we obtained fine-grained and
coarse-grained score (for both recall and precision)
of 0.724 and 0.788 respectively. For the multilin-
gual lexical sample task, we obtained recall (and
precision) of 0.634 for the translation subtask, and
0.673 for the translation and sense subtask.
1 Introduction
This paper describes the approach adopted by our
systems which participated in the English lexical
sample task and the multilingual lexical sample task
of SENSEVAL-3. The goal of the English lexical
sample task is to predict the correct sense of an am-
biguous English word   , while that of the multi-
lingual lexical sample task is to predict the correct
Hindi (target language) translation of an ambiguous
English (source language) word   .
The multilingual lexical sample task is further
subdivided into two subtasks: the translation sub-
task, as well as the translation and sense subtask.
The distinction is that for the translation and sense
subtask, the English sense of the target ambiguous
word   is also provided (for both training and test
data).
In all, we submitted 3 systems: system nusels
for the English lexical sample task, system nusmlst
for the translation subtask, and system nusmlsts for
the translation and sense subtask.
All systems were based on the supervised word
sense disambiguation (WSD) system of Lee and Ng
(2002), and used Support Vector Machines (SVM)
learning. Only the training examples provided in the
official training corpus were used to train the sys-
tems, and no other external resources were used. In
particular, we did not use any external dictionary or
the sample sentences in the provided dictionary.
The knowledge sources used included part-of-
speech (POS) of neighboring words, single words in
the surrounding context, local collocations, and syn-
tactic relations, as described in Lee and Ng (2002).
For the translation and sense subtask of the multi-
lingual lexical sample task, the English sense given
for the target word was also used as an additional
knowledge source. All features encoding these
knowledge sources were used, without any feature
selection.
We next describe SVM learning and the com-
bined knowledge sources adopted. Much of the de-
scription follows that of Lee and Ng (2002).
2 Support Vector Machines (SVM)
The SVM (Vapnik, 1995) performs optimization to
find a hyperplane with the largest margin that sepa-
rates training examples into two classes. A test ex-
ample is classified depending on the side of the hy-
perplane it lies in. Input features can be mapped into
high dimensional space before performing the opti-
mization and classification. A kernel function can
be used to reduce the computational cost of training
and testing in high dimensional space. If the train-
ing examples are nonseparable, a regularization pa-
rameter
 (  by default) can be used to control
the trade-off between achieving a large margin and
a low training error. We used the implementation
of SVM in WEKA (Witten and Frank, 2000), where
each nominal feature with  possible values is con-
verted into  binary (0 or 1) features. If a nominal
feature takes the  th value, then the  th binary fea-
ture is set to 1 and all the other binary features are
set to 0. The default linear kernel is used. Since
SVM only handles binary (2-class) classification,
we built one binary classifier for each sense class.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Note that our supervised learning approach made
use of a single learning algorithm, without combin-
ing multiple learning algorithms as adopted in other
research (such as (Florian et al, 2002)).
3 Multiple Knowledge Sources
To disambiguate a word occurrence   , systems
nusels and nusmlst used the first four knowledge
sources listed below. System nusmlsts used the
English sense given for the target ambiguous word
  as an additional knowledge source. Previous re-
search (Ng and Lee, 1996; Stevenson and Wilks,
2001; Florian et al, 2002; Lee and Ng, 2002) has
shown that a combination of knowledge sources im-
proves WSD accuracy.
Our experiments on the provided training data
of the SENSEVAL-3 translation and sense subtask
also indicated that the additional knowledge source
of the English sense of the target word further im-
proved accuracy (See Section 4.3 for details).
We did not attempt feature selection since our
previous research (Lee and Ng, 2002) indicated that
SVM performs better without feature selection.
3.1 Part-of-Speech (POS) of Neighboring
Words
We use 7 features to encode this knowledge source:
  
	    	 
, where    (    ) is
the POS of the  th token to the left (right) of   , and
  is the POS of   . A token can be a word or a
punctuation symbol, and each of these neighboring
tokens must be in the same sentence as   . We use a
sentence segmentation program (Reynar and Ratna-
parkhi, 1997) and a POS tagger (Ratnaparkhi, 1996)
to segment the tokens surrounding   into sentences
and assign POS tags to these tokens.
For example, to disambiguate the word
bars in the POS-tagged sentence ?Reid/NNP
saw/VBD me/PRP looking/VBG at/IN the/DT
iron/NN bars/NNS ./.?, the POS feature vector is
