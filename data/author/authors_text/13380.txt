Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 277?285,
Beijing, August 2010
Entity Disambiguation for Knowledge Base Population
?Mark Dredze and ?Paul McNamee and ?Delip Rao and ?Adam Gerber and ?Tim Finin
?Human Language Technology Center of Excellence, Center for Language and Speech Processing
Johns Hopkins University
?University of Maryland ? Baltimore County
mdredze,mcnamee,delip,adam.gerber@jhu.edu, finin@umbc.edu
Abstract
The integration of facts derived from information extraction
systems into existing knowledge bases requires a system to
disambiguate entity mentions in the text. This is challeng-
ing due to issues such as non-uniform variations in entity
names, mention ambiguity, and entities absent from a knowl-
edge base. We present a state of the art system for entity dis-
ambiguation that not only addresses these challenges but also
scales to knowledge bases with several million entries using
very little resources. Further, our approach achieves perfor-
mance of up to 95% on entities mentioned from newswire
and 80% on a public test set that was designed to include
challenging queries.
1 Introduction
The ability to identify entities like people, orga-
nizations and geographic locations (Tjong Kim
Sang and De Meulder, 2003), extract their at-
tributes (Pasca, 2008), and identify entity rela-
tions (Banko and Etzioni, 2008) is useful for sev-
eral applications in natural language processing
and knowledge acquisition tasks like populating
structured knowledge bases (KB).
However, inserting extracted knowledge into a
KB is fraught with challenges arising from nat-
ural language ambiguity, textual inconsistencies,
and lack of world knowledge. To the discern-
ing human eye, the ?Bush? in ?Mr. Bush left
for the Zurich environment summit in Air Force
One.? is clearly the US president. Further con-
text may reveal it to be the 43rd president, George
W. Bush, and not the 41st president, George H.
W. Bush. The ability to disambiguate a polyse-
mous entity mention or infer that two orthograph-
ically different mentions are the same entity is
crucial in updating an entity?s KB record. This
task has been variously called entity disambigua-
tion, record linkage, or entity linking. When per-
formed without a KB, entity disambiguation is
called coreference resolution: entity mentions ei-
ther within the same document or across multi-
ple documents are clustered together, where each
cluster corresponds to a single real world entity.
The emergence of large scale publicly avail-
able KBs like Wikipedia and DBPedia has spurred
an interest in linking textual entity references to
their entries in these public KBs. Bunescu and
Pasca (2006) and Cucerzan (2007) presented im-
portant pioneering work in this area, but suffer
from several limitations including Wikipedia spe-
cific dependencies, scale, and the assumption of
a KB entry for each entity. In this work we in-
troduce an entity disambiguation system for link-
ing entities to corresponding Wikipedia pages de-
signed for open domains, where a large percent-
age of entities will not be linkable. Further, our
method and some of our features readily general-
ize to other curated KB. We adopt a supervised
approach, where each of the possible entities con-
tained within Wikipedia are scored for a match to
the query entity. We also describe techniques to
deal with large knowledge bases, like Wikipedia,
which contain millions of entries. Furthermore,
our system learns when to withhold a link when
an entity has no matching KB entry, a task that
has largely been neglected in prior research in
cross-document entity coreference. Our system
produces high quality predictions compared with
recent work on this task.
2 Related Work
The information extraction oeuvre has a gamut of
relation extraction methods for entities like per-
sons, organizations, and locations, which can be
classified as open- or closed-domain depending
on the restrictions on extractable relations (Banko
and Etzioni, 2008). Closed domain systems ex-
tract a fixed set of relations while in open-domain
systems, the number and type of relations are un-
bounded. Extracted relations still require process-
ing before they can populate a KB with facts:
namely, entity linking and disambiguation.
277
Motivated by ambiguity in personal name
search, Mann and Yarowsky (2003) disambiguate
person names using biographic facts, like birth
year, occupation and affiliation. When present
in text, biographic facts extracted using regular
expressions help disambiguation. More recently,
the Web People Search Task (Artiles et al, 2008)
clustered web pages for entity disambiguation.
The related task of cross document corefer-
ence resolution has been addressed by several
researchers starting from Bagga and Baldwin
(1998). Poesio et al (2008) built a cross document
coreference system using features from encyclo-
pedic sources like Wikipedia. However, success-
ful coreference resolution is insufficient for cor-
rect entity linking, as the coreference chain must
still be correctly mapped to the proper KB entry.
Previous work by Bunescu and Pasca (2006)
and Cucerzan (2007) aims to link entity men-
tions to their corresponding topic pages in
Wikipedia but the authors differ in their ap-
proaches. Cucerzan uses heuristic rules and
Wikipedia disambiguation markup to derive map-
pings from surface forms of entities to their
Wikipedia entries. For each entity in Wikipedia,
a context vector is derived as a prototype for the
entity and these vectors are compared (via dot-
product) with the context vectors of unknown en-
tity mentions. His work assumes that all entities
have a corresponding Wikipedia entry, but this as-
sumption fails for a significant number of entities
in news articles and even more for other genres,
like blogs. Bunescu and Pasca on the other hand
suggest a simple method to handle entities not in
Wikipedia by learning a threshold to decide if the
entity is not in Wikipedia. Both works mentioned
rely on Wikipedia-specific annotations, such as
category hierarchies and disambiguation links.
We just recently became aware of a system
fielded by Li et al at the TAC-KBP 2009 eval-
uation (2009). Their approach bears a number
of similarities to ours; both systems create candi-
date sets and then rank possibilities using differing
learning methods, but the principal difference is in
our approach to NIL prediction. Where we simply
consider absence (i.e., the NIL candidate) as an-
other entry to rank, and select the top-ranked op-
tion, they use a separate binary classifier to decide
whether their top prediction is correct, or whether
NIL should be output. We believe relying on fea-
tures that are designed to inform whether absence
is correct is the better alternative.
3 Entity Linking
We define entity linking as matching a textual en-
tity mention, possibly identified by a named en-
tity recognizer, to a KB entry, such as a Wikipedia
page that is a canonical entry for that entity. An
entity linking query is a request to link a textual
entity mention in a given document to an entry in
a KB. The system can either return a matching en-
try or NIL to indicate there is no matching entry.
In this work we focus on linking organizations,
geo-political entities and persons to a Wikipedia
derived KB.
3.1 Key Issues
There are 3 challenges to entity linking:
Name Variations. An entity often has multiple
mention forms, including abbreviations (Boston
Symphony Orchestra vs. BSO), shortened forms
(Osama Bin Laden vs. Bin Laden), alternate
spellings (Osama vs. Ussamah vs. Oussama),
and aliases (Osama Bin Laden vs. Sheikh Al-
Mujahid). Entity linking must find an entry de-
spite changes in the mention string.
Entity Ambiguity. A single mention, like
Springfield, can match multiple KB entries, as
many entity names, like people and organizations,
tend to be polysemous.
Absence. Processing large text collections vir-
tually guarantees that many entities will not ap-
pear in the KB (NIL), even for large KBs.
The combination of these challenges makes
entity linking especially challenging. Consider
an example of ?William Clinton.? Most read-
ers will immediately think of the 42nd US pres-
ident. However, the only two William Clintons in
Wikipedia are ?William de Clinton? the 1st Earl
of Huntingdon, and ?William Henry Clinton? the
British general. The page for the 42nd US pres-
ident is actually ?Bill Clinton?. An entity link-
ing system must decide if either of the William
Clintons are correct, even though neither are ex-
act matches. If the system determines neither
278
matches, should it return NIL or the variant ?Bill
Clinton?? If variants are acceptable, then perhaps
?Clinton, Iowa? or ?DeWitt Clinton? should be
acceptable answers?
3.2 Contributions
We address these entity linking challenges.
Robust Candidate Selection. Our system is
flexible enough to find name variants but suffi-
ciently restrictive to produce a manageable can-
didate list despite a large-scale KB.
Features for Entity Disambiguation. We de-
veloped a rich and extensible set of features based
on the entity mention, the source document, and
the KB entry. We use a machine learning ranker
to score each candidate.
Learning NILs. We modify the ranker to learn
NIL predictions, which obviates hand tuning and
importantly, admits use of additional features that
are indicative of NIL.
Our contributions differ from previous efforts
(Bunescu and Pasca, 2006; Cucerzan, 2007) in
several important ways. First, previous efforts de-
pend on Wikipedia markup for significant perfor-
mance gains. We make no such assumptions, al-
though we show that optional Wikipedia features
lead to a slight improvement. Second, Cucerzan
does not handle NILs while Bunescu and Pasca
address them by learning a threshold. Our ap-
proach learns to predict NIL in a more general
and direct way. Third, we develop a rich fea-
ture set for entity linking that can work with any
KB. Finally, we apply a novel finite state machine
method for learning name variations. 1
The remaining sections describe the candidate
selection system, features and ranking, and our
novel approach learning NILs, followed by an
empirical evaluation.
4 Candidate Selection for Name Variants
The first system component addresses the chal-
lenge of name variants. As the KB contains a large
number of entries (818,000 entities, of which 35%
are PER, ORG or GPE), we require an efficient se-
lection of the relevant candidates for a query.
Previous approaches used Wikipedia markup
for filtering ? only using the top-k page categories
1http://www.clsp.jhu.edu/ markus/fstrain
(Bunescu and Pasca, 2006) ? which is limited to
Wikipedia and does not work for general KBs.
We consider a KB independent approach to selec-
tion that also allows for tuning candidate set size.
This involves a linear pass over KB entry names
(Wikipedia page titles): a naive implementation
took two minutes per query. The following sec-
tion reduces this to under two seconds per query.
For a given query, the system selects KB entries
using the following approach:
? Titles that are exact matches for the mention.
? Titles that are wholly contained in or contain
the mention (e.g., Nationwide and Nationwide In-
surance).
? The first letters of the entity mention match the
KB entry title (e.g., OA and Olympic Airlines).
? The title matches a known alias for the entity
(aliases described in Section 5.2).
? The title has a strong string similarity score
with the entity mention. We include several mea-
sures of string similarity, including: character
Dice score > 0.9, skip bigram Dice score > 0.6,
and Hamming distance <= 2.
We did not optimize the thresholds for string
similarity, but these could obviously be tuned to
minimize the candidate sets and maximize recall.
All of the above features are general for any
KB. However, since our evaluation used a KB
derived from Wikipedia, we included a few
Wikipedia specific features. We added an entry if
its Wikipedia page appeared in the top 20 Google
results for a query.
On the training dataset (Section 7) the selection
system attained a recall of 98.8% and produced
candidate lists that were three to four orders of
magnitude smaller than the KB. Some recall er-
rors were due to inexact acronyms: ABC (Arab
Banking; ?Corporation? is missing), ASG (Abu
Sayyaf; ?Group? is missing), and PCF (French
Communist Party; French reverses the order of the
pre-nominal adjectives). We also missed Interna-
tional Police (Interpol) and Becks (David Beck-
ham; Mr. Beckham and his wife are collectively
referred to as ?Posh and Becks?).
279
4.1 Scaling Candidate Selection
Our previously described candidate selection re-
lied on a linear pass over the KB, but we seek
more efficient methods. We observed that the
above non-string similarity filters can be pre-
computed and stored in an index, and that the skip
bigram Dice score can be computed by indexing
the skip bigrams for each KB title. We omitted
the other string similarity scores, and collectively
these changes enable us to avoid a linear pass over
the KB. Finally we obtained speedups by serving
the KB concurrently2. Recall was nearly identical
to the full system described above: only two more
queries failed. Additionally, more than 95% of
the processing time was consumed by Dice score
computation, which was only required to cor-
rectly retrieve less than 4% of the training queries.
Omitting the Dice computation yielded results in
a few milliseconds. A related approach is that of
canopies for scaling clustering for large amounts
of bibliographic citations (McCallum et al, 2000).
In contrast, our setting focuses on alignment vs.
clustering mentions, for which overlapping parti-
tioning approaches like canopies are applicable.
5 Entity Linking as Ranking
We select a single correct candidate for a query
using a supervised machine learning ranker. We
represent each query by a D dimensional vector
x, where x ? RD, and we aim to select a sin-
gle KB entry y, where y ? Y , a set of possible
KB entries for this query produced by the selec-
tion system above, which ensures that Y is small.
The ith query is given by the pair {xi, yi}, where
we assume at most one correct KB entry.
To evaluate each candidate KB entry in Y we
create feature functions of the form f(x, y), de-
pendent on both the example x (document and en-
tity mention) and the KB entry y. The features
address name variants and entity disambiguation.
We take a maximum margin approach to learn-
ing: the correct KB entry y should receive a
higher score than all other possible KB entries
y? ? Y, y? 6= y plus some margin ?. This learning
2Our Python implementation with indexing features and
four threads achieved up to 80? speedup compared to naive
implementation.
constraint is equivalent to the ranking SVM algo-
rithm of Joachims (2002), where we define an or-
dered pair constraint for each of the incorrect KB
entries y? and the correct entry y. Training sets pa-
rameters such that score(y) ? score(y?) + ?. We
used the library SVMrank to solve this optimiza-
tion problem.3 We used a linear kernel, set the
slack parameter C as 0.01 times the number of
training examples, and take the loss function as
the total number of swapped pairs summed over
all training examples. While previous work used
a custom kernel, we found a linear kernel just as
effective with our features. This has the advan-
tage of efficiency in both training and prediction 4
? important considerations in a system meant to
scale to millions of KB entries.
5.1 Features for Entity Disambiguation
200 atomic features represent x based on each
candidate query/KB pair. Since we used a lin-
ear kernel, we explicitly combined certain fea-
tures (e.g., acroynym-match AND known-alias) to
model correlations. This included combining each
feature with the predicted type of the entity, al-
lowing the algorithm to learn prediction functions
specific to each entity type. With feature combina-
tions, the total number of features grew to 26,569.
The next sections provide an overview; for a de-
tailed list see McNamee et al (2009).
5.2 Features for Name Variants
Variation in entity name has long been recog-
nized as a bane for information extraction sys-
tems. Poor handling of entity name variants re-
sults in low recall. We describe several features
ranging from simple string match to finite state
transducer matching.
String Equality. If the query name and KB en-
try name are identical, this is a strong indication of
a match, and in our KB entry names are distinct.
However, similar or identical entry names that
refer to distinct entities are often qualified with
parenthetical expressions or short clauses. As
an example, ?London, Kentucky? is distinguished
3www.cs.cornell.edu/people/tj/svm_light/svm_rank.html
4Bunescu and Pasca (2006) report learning tens of thou-
sands of support vectors with their ?taxonomy? kernel while
a linear kernel represents all support vectors with a single
weight vector, enabling faster training and prediction.
280
from ?London, Ontario?, ?London, Arkansas?,
?London (novel)?, and ?London?. Therefore,
other string equality features were used, such as
whether names are equivalent after some transfor-
mation. For example, ?Baltimore? and ?Baltimore
City? are exact matches after removing a common
GPE word like city; ?University of Vermont? and
?University of VT? match if VT is expanded.
Approximate String Matching. Many entity
mentions will not match full names exactly. We
added features for character Dice, skip bigram
Dice, and left and right Hamming distance scores.
Features were set based on quantized scores.
These were useful for detecting minor spelling
variations or mistakes. Features were also added if
the query was wholly contained in the entry name,
or vice-versa, which was useful for handling ellip-
sis (e.g., ?United States Department of Agricul-
ture? vs. ?Department of Agriculture?). We also
included the ratio of the recursive longest com-
mon subsequence (Christen, 2006) to the shorter
of the mention or entry name, which is effective at
handling some deletions or word reorderings (e.g.,
?Li Gong? and ?Gong Li?). Finally, we checked
whether all of the letters of the query are found in
the same order in the entry name (e.g., ?Univ Wis-
consin? would match ?University of Wisconsin?).
Acronyms. Features for acronyms, using dic-
tionaries and partial character matches, enable
matches between ?MIT? and ?Madras Institute of
Technology? or ?Ministry of Industry and Trade.?
Aliases. Many aliases or nicknames are non-
trivial to guess. For example JAVA is the
stock symbol for Sun Microsystems, and ?Gin-
ger Spice? is a stage name of Geri Halliwell. A
reasonable way to do this is to employ a dictio-
nary and alias lists that are commonly available
for many domains5.
FST Name Matching. Another measure of sur-
face similarity between a query and a candidate
was computed by training finite-state transducers
similar to those described in Dreyer et al (2008).
These transducers assign a score to any string pair
by summing over all alignments and scoring all
5We used multiple lists, including class-specific lists (i.e.,
for PER, ORG, and GPE) lists extracted from Freebase (Bol-
lacker et al, 2008) and Wikipedia redirects. PER, ORG, and
GPE are the commonly used terms for entity types for peo-
ple, organizations and geo-political regions respectively.
contained character n-grams; we used n-grams of
length 3 and less. The scores are combined using a
global log-linear model. Since different spellings
of a name may vary considerably in length (e.g.,
J Miller vs. Jennifer Miller) we eliminated the
limit on consecutive insertions used in previous
applications.6
5.3 Wikipedia Features
Most of our features do not depend on Wikipedia
markup, but it is reasonable to include features
from KB properties. Our feature ablation study
shows that dropping these features causes a small
but statistically significant performance drop.
WikiGraph statistics. We added features de-
rived from the Wikipedia graph structure for an
entry, like indegree of a node, outdegree of a node,
and Wikipedia page length in bytes. These statis-
tics favor common entity mentions over rare ones.
Wikitology. KB entries can be indexed with hu-
man or machine generated metadata consisting of
keywords or categories in a domain-appropriate
taxonomy. Using a system called Wikitology,
Syed et al (2008) investigated use of ontology
terms obtained from the explicit category system
in Wikipedia as well as relationships induced from
the hyperlink graph between related Wikipedia
pages. Following this approach we computed top-
ranked categories for the query documents and
used this information as features. If none of the
candidate KB entries had corresponding highly-
ranked Wikitology pages, we used this as a NIL
feature (Section 6.1).
5.4 Popularity
Although it may be an unsafe bias to give prefer-
ence to common entities, we find it helpful to pro-
vide estimates of entity popularity to our ranker
as others have done (Fader et al, 2009). Apart
from the graph-theoretic features derived from the
Wikipedia graph, we used Google?s PageRank to
by adding features indicating the rank of the KB
entry?s corresponding Wikipedia page in a Google
query for the target entity mention.
6Without such a limit, the objective function may diverge
for certain parameters of the model; we detect such cases and
learn to avoid them during training.
281
5.5 Document Features
The mention document and text associated with a
KB entry contain context for resolving ambiguity.
Entity Mentions. Some features were based on
presence of names in the text: whether the query
appeared in the KB text and the entry name in the
document. Additionally, we used a named-entity
tagger and relation finder, SERIF (Boschee et al,
2005), identified name and nominal mentions that
were deemed co-referent with the entity mention
in the document, and tested whether these nouns
were present in the KB text. Without the NE anal-
ysis, accuracy on non-NIL entities dropped 4.5%.
KB Facts. KB nodes contain infobox attributes
(or facts); we tested whether the fact text was
present in the query document, both locally to a
mention, or anywhere in the text. Although these
facts were derived from Wikipedia infoboxes,
they could be obtained from other sources as well.
Document Similarity We measured similarity
between the query document and the KB text in
two ways: cosine similarity with TF/IDF weight-
ing (Salton and McGill, 1983); and using the Dice
coefficient over bags of words. IDF values were
approximated using counts from the Google 5-
gram dataset as by Klein and Nelson (2008).
Entity Types. Since the KB contained types
for entries, we used these as features as well as
the predicted NE type for the entity mention in
the document text. Additionally, since only a
small number of KB entries had PER, ORG, or
GPE types, we also inferred types from Infobox
class information to attain 87% coverage in the
KB. This was helpful for discouraging selection
of eponymous entries named after famous enti-
ties (e.g., the former U.S. president vs. ?John F.
Kennedy International Airport?).
5.6 Feature Combinations
To take into account feature dependencies we cre-
ated combination features by taking the cross-
product of a small set of diverse features. The
attributes used as combination features included
entity type; a popularity based on Google?s rank-
ings; document comparison using TF/IDF; cov-
erage of co-referential nouns in the KB node
text; and name similarity. The combinations were
cascaded to allow arbitrary feature conjunctions.
Thus it is possible to end up with a feature kbtype-
is-ORG AND high-TFIDF-score AND low-name-
similarity. The combined features increased the
number of features from roughly 200 to 26,000.
6 Predicting NIL Mentions
So far we have assumed that each example has a
correct KB entry; however, when run over a large
corpus, such as news articles, we expect a signifi-
cant number of entities will not appear in the KB.
Hence it will be useful to predict NILs.
We learn when to predict NIL using the SVM
ranker by augmenting Y to include NIL, which
then has a single feature unique to NIL answers.
It can be shown that (modulo slack variables) this
is equivalent to learning a single threshold ? for
NIL predictions as in Bunescu and Pasca (2006).
Incorporating NIL into the ranker has several
advantages. First, the ranker can set the thresh-
old optimally without hand tuning. Second, since
the SVM scores are relative within a single exam-
ple and cannot be compared across examples, set-
ting a single threshold is difficult. Third, a thresh-
old sets a uniform standard across all examples,
whereas in practice we may have reasons to favor
a NIL prediction in a given example. We design
features for NIL prediction that cannot be cap-
tured in a single parameter.
6.1 NIL Features
Integrating NIL prediction into learning means
we can define arbitrary features indicative of NIL
predictions in the feature vector corresponding to
NIL. For example, if many candidates have good
name matches, it is likely that one of them is cor-
rect. Conversely, if no candidate has high entry-
text/article similarity, or overlap between facts
and the article text, it is likely that the entity is
absent from the KB. We included several features,
such as a) the max, mean, and difference between
max and mean for 7 atomic features for all KB
candidates considered, b) whether any of the can-
didate entries have matching names (exact and
fuzzy string matching), c) whether any KB en-
try was a top Wikitology match, and d) if the top
Google match was not a candidate.
282
Micro-Averaged Macro-Averaged
Best Median All Features Best Features Best Median All Features Best Features
All 0.8217 0.7108 0.7984 0.7941 0.7704 0.6861 0.7695 0.7704
non-NIL 0.7725 0.6352 0.7063 0.6639 0.6696 0.5335 0.6097 0.5593
NIL 0.8919 0.7891 0.8677 0.8919 0.8789 0.7446 0.8464 0.8721
Table 1: Micro and macro-averaged accuracy for TAC-KBP data compared to best and median reported performance.
Results are shown for all features as well as removing a small number of features using feature selection on development data.
7 Evaluation
We evaluated our system on two datasets: the
Text Analysis Conference (TAC) track on Knowl-
edge Base Population (TAC-KBP) (McNamee and
Dang, 2009) and the newswire data used by
Cucerzan (2007) (Microsoft News Data).
Since our approach relies on supervised learn-
ing, we begin by constructing our own training
corpus.7 We highlighted 1496 named entity men-
tions in news documents (from the TAC-KBP doc-
ument collection) and linked these to entries in
a KB derived from Wikipedia infoboxes. 8 We
added to this collection 119 sample queries from
the TAC-KBP data. The total of 1615 training ex-
amples included 539 (33.4%) PER, 618 (38.3%)
ORG, and 458 (28.4%) GPE entity mentions. Of
the training examples, 80.5% were found in the
KB, matching 300 unique entities. This set has a
higher number of NIL entities than did Bunescu
and Pasca (2006) (10%) but lower than the TAC-
KBP test set (43%).
All system development was done using a train
(908 examples) and development (707 examples)
split. The TAC-KBP and Microsoft News data
sets were held out for final tests. A model trained
on all 1615 examples was used for experiments.
7.1 TAC-KBP 2009 Experiments
The KB is derived from English Wikipedia pages
that contained an infobox. Entries contain basic
descriptions (article text) and attributes. The TAC-
KBP query set contains 3904 entity mentions for
560 distinct entities; entity type was only provided
for evaluation. The majority of queries were for
organizations (69%). Most queries were missing
from the KB (57%). 77% of the distinct GPEs
in the queries were present in the KB, but for
7Data available from www.dredze.com
8http://en.wikipedia.org/wiki/Help:Infobox
PERs and ORGs these percentages were signifi-
cantly lower, 19% and 30% respectively.
Table 1 shows results on TAC-KBP data us-
ing all of our features as well a subset of features
based on feature selection experiments on devel-
opment data. We include scores for both micro-
averaged accuracy ? averaged over all queries
? and macro-averaged accuracy ? averaged over
each unique entity ? as well as the best and me-
dian reported results for these data (McNamee
and Dang, 2009). We obtained the best reported
results for macro-averaged accuracy, as well as
the best results for NIL detection with micro-
averaged accuracy, which shows the advantage of
our approach to learning NIL. See McNamee et
al. (2009) for additional experiments.
The candidate selection phase obtained a re-
call of 98.6%, similar to that of development data.
Missed candidates included Iron Lady, which
refers metaphorically to Yulia Tymoshenko, PCC,
the Spanish-origin acronym for the Cuban Com-
munist Party, and Queen City, a former nickname
for the city of Seattle, Washington. The system re-
turned a mean of 76 candidates per query, but the
median was 15 and the maximum 2772 (Texas). In
about 10% of cases there were four or fewer can-
didates and in 10% of cases there were more than
100 candidate KB nodes. We observed that ORGs
were more difficult, due to the greater variation
and complexity in their naming, and that they can
be named after persons or locations.
7.2 Feature Effectiveness
We performed two feature analyses on the TAC-
KBP data: an additive study ? starting from a
small baseline feature set used in candidate selec-
tion we add feature groups and measure perfor-
mance changes (omitting feature combinations),
and an ablative study ? starting from all features,
remove a feature group and measure performance.
283
Class All non-NIL NIL
Baseline 0.7264 0.4621 0.9251
Acronyms 0.7316 0.4860 0.9161
NE Analysis 0.7661 0.7181 0.8022
Google 0.7597 0.7421 0.7730
Doc/KB Text Similarity 0.7313 0.6699 0.7775
Wikitology 0.7318 0.4549 0.9399
All 0.7984 0.7063 0.8677
Table 2: Additive analysis: micro-averaged accuracy.
Table 2 shows the most significant features in
the feature addition experiments. The baseline
includes only features based on string similarity
or aliases and is not effective at finding correct
entries and strongly favors NIL predictions. In-
clusion of features based on analysis of named-
entities, popularity measures (e.g., Google rank-
ings), and text comparisons provided the largest
gains. The overall changes are fairly small,
roughly ?1%; however changes in non-NIL pre-
cision are larger.
The ablation study showed considerable redun-
dancy across feature groupings. In several cases,
performance could have been slightly improved
by removing features. Removing all feature com-
binations would have improved overall perfor-
mance to 81.05% by gaining on non-NIL for a
small decline on NIL detection.
7.3 Experiments on Microsoft News Data
We downloaded the evaluation data used in
Cucerzan (2007)9: 20 news stories from MSNBC
with 642 entity mentions manually linked to
Wikipedia and another 113 mentions not having
any corresponding link to Wikipedia.10 A sig-
nificant percentage of queries were not of type
PER, ORG, or GPE (e.g., ?Christmas?). SERIF
assigned entity types and we removed 297 queries
not recognized as entities (counts in Table 3).
We learned a new model on the training data
above using a reduced feature set to increase
speed.11 Using our fast candidate selection sys-
tem, we resolved each query in 1.98 seconds (me-
dian). Query processing time was proportional to
9http://research.microsoft.com/en-us/um/people/silviu/WebAssistant/TestData/
10One of the MSNBC news articles is no longer available
so we used 759 total entities.
11We removed Google, FST and conjunction features
which reduced system accuracy but increased performance.
Num. Queries Accuracy
Total Nil All non-NIL NIL
NIL 452 187 0.4137 0.0 1.0
GPE 132 20 0.9696 1.00 0.8000
ORG 115 45 0.8348 0.7286 1.00
PER 205 122 0.9951 0.9880 1.00
All 452 187 0.9469 0.9245 0.9786
Cucerzan (2007) 0.914 - -
Table 3: Micro-average results for Microsoft data.
the number of candidates considered. We selected
a median of 13 candidates for PER, 12 for ORG
and 102 for GPE. Accuracy results are in Table
3. The high results reported for this dataset over
TAC-KBP is primarily because we perform very
well in predicting popular and rare entries ? both
of which are common in newswire text.
One issue with our KB was that it was derived
from infoboxes in Wikipedia?s Oct 2008 version
which has both new entities, 12 and is missing en-
tities.13 Therefore, we manually confirmed NIL
answers and new answers for queries marked as
NIL in the data. While an exact comparison is not
possible (as described above), our results (94.7%)
appear to be at least on par with Cucerzan?s sys-
tem (91.4% overall accuracy).With the strong re-
sults on TAC-KBP, we believe that this is strong
confirmation of the effectiveness of our approach.
8 Conclusion
We presented a state of the art system to disam-
biguate entity mentions in text and link them to
a knowledge base. Unlike previous approaches,
our approach readily ports to KBs other than
Wikipedia. We described several important chal-
lenges in the entity linking task including han-
dling variations in entity names, ambiguity in en-
tity mentions, and missing entities in the KB, and
we showed how to each of these can be addressed.
We described a comprehensive feature set to ac-
complish this task in a supervised setting. Impor-
tantly, our method discriminately learns when not
to link with high accuracy. To spur further re-
search in these areas we are releasing our entity
linking system.
122008 vs. 2006 version used in Cucerzan (2007) We
could not get the 2006 version from the author or the Internet.
13Since our KB was derived from infoboxes, entities not
having an infobox were left out.
284
References
Javier Artiles, Satoshi Sekine, and Julio Gonzalo.
2008. Web people search: results of the first evalu-
ation and the plan for the second. In WWW.
Amit Bagga and Breck Baldwin. 1998. Entity-
based cross-document coreferencing using the vec-
tor space model. In Conference on Computational
Linguistics (COLING).
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Association for Computational Linguistics.
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and
J. Taylor. 2008. Freebase: a collaboratively cre-
ated graph database for structuring human knowl-
edge. In SIGMOD Management of Data.
E. Boschee, R. Weischedel, and A. Zamanian. 2005.
Automatic information extraction. In Conference
on Intelligence Analysis.
Razvan C. Bunescu and Marius Pasca. 2006. Using
encyclopedic knowledge for named entity disam-
biguation. In European Chapter of the Assocation
for Computational Linguistics (EACL).
Peter Christen. 2006. A comparison of personal name
matching: Techniques and practical issues. Techni-
cal Report TR-CS-06-02, Australian National Uni-
versity.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on wikipedia data. In Em-
pirical Methods in Natural Language Processing
(EMNLP).
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In Empirical Methods in
Natural Language Processing (EMNLP).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling Wikipedia-based named entity dis-
ambiguation to arbitrary web text. In WikiAI09
Workshop at IJCAI 2009.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Knowledge Discovery
and Data Mining (KDD).
Martin Klein and Michael L. Nelson. 2008. A com-
parison of techniques for estimating IDF values to
generate lexical signatures for the web. In Work-
shop on Web Information and Data Management
(WIDM).
Fangtao Li, Zhicheng Zhang, Fan Bu, Yang Tang,
Xiaoyan Zhu, and Minlie Huang. 2009. THU
QUANTA at TAC 2009 KBP and RTE track. In Text
Analysis Conference (TAC).
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Conference
on Natural Language Learning (CONLL).
Andrew McCallum, Kamal Nigam, and Lyle Ungar.
2000. Efficient clustering of high-dimensional data
sets with application to reference matching. In
Knowledge Discovery and Data Mining (KDD).
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track.
In Text Analysis Conference (TAC).
Paul McNamee, Mark Dredze, Adam Gerber, Nikesh
Garera, Tim Finin, James Mayfield, Christine Pi-
atko, Delip Rao, David Yarowsky, and Markus
Dreyer. 2009. HLTCOE approaches to knowledge
base population at TAC 2009. In Text Analysis Con-
ference (TAC).
Marius Pasca. 2008. Turning web text and search
queries into factual knowledge: hierarchical class
attribute extraction. In National Conference on Ar-
tificial Intelligence (AAAI).
Massimo Poesio, David Day, Ron Artstein, Jason Dun-
can, Vladimir Eidelman, Claudio Giuliano, Rob
Hall, Janet Hitzeman, Alan Jern, Mijail Kabadjov,
Stanley Yong, Wai Keong, Gideon Mann, Alessan-
dro Moschitti, Simone Ponzetto, Jason Smith, Josef
Steinberger, Michael Strube, Jian Su, Yannick Ver-
sley, Xiaofeng Yang, and Michael Wick. 2008. Ex-
ploiting lexical and encyclopedic resources for en-
tity disambiguation: Final report. Technical report,
JHU CLSP 2007 Summer Workshop.
Gerard Salton and Michael McGill. 1983. Introduc-
tion to Modern Information Retrieval. McGraw-
Hill Book Company.
Erik Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Confer-
ence on Natural Language Learning (CONLL).
Zareen Syed, Tim Finin, and Anupam Joshi. 2008.
Wikipedia as an ontology for describing documents.
In Proceedings of the Second International Confer-
ence on Weblogs and Social Media. AAAI Press.
285
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 204?207,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Non-Expert Correction of Automatically Generated Relation Annotations
Matthew R. Gormley?? and Adam Gerber?? and Mary Harper?? and Mark Dredze ??
?Human Language Technology Center of Excellence
?Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21211, USA
?Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park, MD 20742 USA
mrg@cs.jhu.edu,adam.gerber@jhu.edu,mharper@umd.edu,mdredze@cs.jhu.edu
Abstract
We explore a new way to collect human an-
notated relations in text using Amazon Me-
chanical Turk. Given a knowledge base of
relations and a corpus, we identify sentences
which mention both an entity and an attribute
that have some relation in the knowledge base.
Each noisy sentence/relation pair is presented
to multiple turkers, who are asked whether the
sentence expresses the relation. We describe
a design which encourages user efficiency and
aids discovery of cheating. We also present
results on inter-annotator agreement.
1 Introduction
Relation extraction (RE) is the task of determining
the existence and type of relation between two tex-
tual entity mentions. Slot filling, a general form of
relation extraction, includes relations between non-
entities, such as a person and an occupation, age, or
cause of death (McNamee and Dang, 2009).
RE annotated data, such as ACE (2008), is expen-
sive to produce so systems take different approaches
to minimizing data needs. For example, tree kernels
can reduce feature sparsity and generalize across
many examples (GuoDong et al, 2007; Zhou et
al., 2009). Distant supervision automatically gen-
erates noisy training examples from a knowledge
base (KB) without needing annotations (Bunescu
and Mooney, 2007; Mintz et al, 2009). While
this method can quickly generate training data, it
also generates many false examples. We reduce the
noise in such examples by using Amazon Mechani-
cal Turk (MTurk), which has been shown to produce
high quality annotations for a variety of natural lan-
guage processing tasks (Snow et al, 2008).
We use MTurk for annotation of textual relations
to establish an inexpensive and rapid method of cre-
ating data for slot filling. We present a two step an-
notation process: (1) automatic creation of noisy ex-
amples, and (2) human validation of examples.
2 Method
2.1 Automatic generation of noisy examples
To create noisy examples we use a similar approach
to Mintz et al (2009). We extract relations from a
KB in the form of tuples, (e, r, v), where e is an
entity, v is a value, and r is a relation that holds
between them; for example (J.R.R. Tolkien, occu-
pation, author). Our KB is Freebase1, an online
database of structured information, and our corpus
is from the TAC KBP task (McNamee and Dang,
2009)2. For each tuple, we find sentences in a cor-
pus that contain both an exact mention of the entity
e and of the value v. Of course, such sentences may
not attest to the relation r, so the process produces
many incorrect examples.
2.2 Human Intelligence Tasks
A Human Intelligence Task (HIT) is a short paid task
on MTurk. In our HITs, we present the turker with
ten relation examples as sentence/relation pairs. For
each example, the user is asked to select from three
annotation options: the sentence (1) expresses the
relation, (2) does not express the relation, or (3) the
1http://www.freebase.com
2http://projects.ldc.upenn.edu/kbp/
204
1. The sentence expresses the relation.
Sentence: For the past eleven years, James has
lived in Tucson.
Relation: ?Tucson? is the residence of ?James?
2. The sentence does not express the relation.
Sentence: Samuel first met Divya in 1990, while
she was still a student.
Relation: ?Divya? is a spouse of ?Samuel?
3. The relation does not make sense.
Sentence: Soojin was born in January.
Relation: ?January? is the birth place of ?Soojin?
Figure 1: The three annotation options with examples.
relation does not make sense (figure 1.)
Of the ten examples that comprise each HIT,
seven are automatically generated by the method
above. The correct answer is known for the three re-
maining examples; these are included for quality as-
surance (control examples.) The three control exam-
ples are a positive example (expresses the relation,) a
negative example (contradicts the true relation,) and
a nonsense example (relation is nonsensical.)
All control examples derive from a subset of the
automatically generated person examples. Positive
examples were randomly sampled and hand anno-
tated. Negative examples are familial relations in
which we change the relation type so that it would
not be expressed in the sentence. For example,
the relation ?Barack Obama is the parent of Malia
Obama? would be changed to ?Barack Obama is a
sibling of Malia Obama.? To generate nonsense ex-
amples we employ the same method for a different
mapping of relations, which produces relations like
?New Zealand is the gender of John Key.?
2.3 HIT Design
MTurk is a marketplace so users have total freedom
in choosing which HITs to complete. As such, HIT
design should maximize its appeal. We assume that
users find appealing those HITs through which they
may maximize their own monetary gain, while mini-
mizing moment-to-moment frustrations. We empha-
sized clarity and ease of use.
The layout consists of three sections (figure 2).
The leftmost section is a progress list, which shows
the user?s answers and current position; the middle
section contains the current relation example and an-
notation options; the rightmost section (not pictured)
# HITs Cost Time (hours)
Trial 50 $2.75 27
Batch 1 500 $27.50 34
Batch 2 765 $42.08 25
Batch 3 500 $27.50 22
Total 1815 $99.83 108
Table 1: Size, cost and time to complete each HITs batch.
contains instructions. All sections and all UI ele-
ments remain visible and in the same position for the
duration of the HIT, with only the text of the sen-
tence and relation changing according to question
number. Because only a single question is displayed
at a time, we are able to minimize user actions such
as scrolling, clicking small targets, or making large
mouse movements. Additionally, we can monitor
how much time a user spends on each question.
At all times the user is able to consult the instruc-
tions for the task, which include examples of each
annotation option. The user is also reminded of the
technical requirements for the HIT and expectations
for honesty and accuracy. A comment box provides
users with the opportunity to ask questions, make
suggestions, or clarify their responses.
3 Results
We submitted a trial run and three full batches of
HITs. Table 1 summarizes the costs and completion
times for all HITs. The HITs were labeled rapidly
and for a low cost ($0.05 per HIT, i.e., .5? per anno-
tation). Each HIT was assigned to five unique work-
ers. We found that 50% of the 352 different workers
completed 2 or more HITs (figure 3.) Our results
exclude a trial run of 50 hits. Across the 17,650 ex-
amples the mean time spent was 20.77 seconds, with
a standard deviation of 99.96 seconds. The median
time per example was 10.0 seconds.
3.1 Analysis
To evaluate the annotations, two of the authors an-
notated a random sample of 247 (10%) of the 2471
noisy examples. In addition, we analyzed the work-
ers agreement with the control examples.
We used two metrics to assess agreement. The
first metric is pairwise percent agreement (Pair-
wise): the average of the example agreement scores,
where the example agreement score is the percent of
205
Figure 2: An example HIT with instructions excluded.
0 
20 
40 
60 
80 
100 
120 
140 
0 50 100
 
150
 
200
 
250
 
300
 
350
 
# H
ITs 
Workers 
Figure 3: The number of HITs per worker, with columns
sorted left to right.
pairs of annotators that agreed for a particular exam-
ple. The second metric is the exact kappa coefficient
(Exact-?) (Conger, 1980), which takes into account
that agreement can occur by chance. The number of
annotators (R) varies with the test scenario.
Table 2 presents the inter-annotator agreement
scores for various subsets of the examples and com-
binations of annotators. On a sample of examples,
we evaluated agreement between the first and sec-
ond expert annotators (E1/E2) and also the agree-
ment between each expert and the majority vote of
the workers (E1/M and E2/M). The agreement be-
tween the two experts is substantially higher than
their individual agreements with the majority. Yet,
we achieve our goal of reducing noise.
We also analyzed the agreement between the
known control answer and the majority vote of the
workers (C/M). This high level of agreement sup-
ports our belief that the automatically generated neg-
ative and nonsense examples were easier to identify
# Ex. R Exact-? Pairwise
E1/E2 247 2 0.64 0.81
E1/M 247 2 0.29 0.60
E2/M 247 2 0.39 0.70
C/M 1059 2 0.90 0.93
T(sample) 247 5 0.31 0.69
T(control) 1059 5 0.52 0.68
T(all) 3530 5 0.45 0.68
Table 2: Inter-annotator agreement
than noisy negative and nonsense examples. Finally,
we evaluated the agreement between the five work-
ers for different subsets of the data: the sample of
noisy examples (T(sample)), the control examples
only (T(control)), and all examples (T(all)). Table 3
lists the number of examples collected and the agree-
ment scores for all workers for each relation type.
Table 4 shows the divergence of the workers? an-
notations from those of an expert. The high level
of confusability for those examples which the expert
annotated as Not Expressed suggests their inherent
difficulty. The workers labeled more examples as
Expressed than the expert, but both labeled few ex-
amples as Nonsense.
4 Quality Control
We identify spurious responses and unreliable users
in two ways. First, worker responses are compared
to control examples; greater agreement with controls
should indicate greater confidence in the user. We
filtered any worker whose agreement with the con-
trols was less than 0.85 (Control Filtered). The sec-
ond approach uses behavioral data. Because only a
single example is visible at any time, we can mea-
206
Relation # Ex. Exact-? Pairwise
siblings 13 0.67 0.82
children 12 0.57 0.83
gender 80 0.46 0.70
place of death 40 0.43 0.68
parent 12 0.40 0.64
spouse 54 0.37 0.65
title 71 0.30 0.78
residences 228 0.29 0.60
ethnicity 38 0.28 0.54
occupation 551 0.26 0.77
activism 4 0.26 0.55
religion 22 0.23 0.55
place of birth 160 0.20 0.64
nationality 1044 0.19 0.67
schools attended 8 0.16 0.55
employee of 132 0.16 0.70
charges 2 0.14 0.70
Total 2471 0.35 0.69
Table 3: Inter-annotator agreement across relation type.
# Ex. is the number of noisy examples. Exact-? and Pair-
wise agreement are among the five workers.
Worker
E NE Nn Total
E
xp
er
t-
1 E 561 89 20 670
NE 284 248 28 560
Nn 1 1 3 5
Total 846 338 51 1235
Table 4: Confusion matrix of expert-1 and user?s anno-
tations on the sample of noisy examples, for the choices
Expressed (E), Not Expressed (NE), and Nonsense (Nn)
sure how much time a user spends on each exam-
ple. The UI is designed to allow for the extremely
rapid completion of examples and of the HIT in gen-
eral. Thus, a user could complete the HIT in only a
few seconds without even reading any of the exam-
ples. Still other users spend only a moment on all-
but-one question, and then several minutes on the
remaining question. Here, we filter a user answering
three or more questions each in under three seconds
(Time Filtered). We combine these two approaches
(Control and Time), which yields the highest expert-
agreement levels (table 5.)
5 Conclusion
Using non-expert annotators from Amazon Mechan-
ical Turk for the correction of noisy, automatically
E1/M E2/M
Unfiltered 0.28 0.38
Time Filtered 0.32 0.43
Control Filtered 0.34 0.47
Control and Time 0.37 0.48
Table 5: Exact-? scores for three levels of quality control
and a baseline, between each expert and the majority vote
on 231 sampled examples. For a fair comparison, we re-
duced the sample size to include only examples for which
each level of quality control had at least one worker an-
notation remaining.
generated examples is inexpensive and fast. We
achieve good inter-annotator agreement using qual-
ity assurance measures to detect cheating. The result
is thousands of new annotated slot filling example
sentences for 17 person relations.
Acknowledgments
We would like to thank the 352 turkers who made
this work possible.
References
ACE. 2008. Automatic content extraction.
http://projects.ldc.upenn.edu/ace/.
R. Bunescu and R. Mooney. 2007. Learning to extract
relations from the web using minimal supervision. In
Association for Computational Linguistics (ACL).
A.J. Conger. 1980. Integration and generalization of
kappas for multiple raters. Psychological Bulletin,
88(2):322?328.
Z. GuoDong, M. Zhang, D. H Ji, and Z. H. U. QiaoM-
ing. 2007. Tree kernel-based relation extraction with
context-sensitive structured parse tree information. In
Empirical Methods in Natural Language Processing
(EMNLP).
Paul McNamee and Hoa Dang. 2009. Overview of the
TAC 2009 knowledge base population track. In Text
Analysis Conference (TAC).
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Association for Computational Linguistics
(ACL).
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Empirical
Methods in Natural Language Processing (EMNLP).
G. Zhou, L. Qian, and J. Fan. 2009. Tree kernel-based
semantic relation extraction with rich syntactic and se-
mantic information. Information Sciences.
207
