Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 285?288,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Utility Evaluation of Cross-document Information Extraction 

Heng Ji
a
, Zheng Chen
a
, Jonathan Feldman
a
, Antonio Gonzalez
a
, Ralph Grishman
b
, Vivek Upadhyay
a 
a
Computer Science Department, Queens College and the Graduate Center, City University of New York 
New York, NY 11367, USA 
b
Computer Science Department, New York University, New York, NY 10003, USA 
hengji@cs.qc.cuny.edu, zchen1@gc.cuny.edu, agonzalez117@qc.cuny.edu, grishman@cs.nyu.edu, 
vivekqc@gmail.com 
 
  
 
Abstract 
We describe a utility evaluation to determine 
whether cross-document information extrac-
tion (IE) techniques measurably improve user 
performance in news summary writing. Two 
groups of subjects were asked to perform the 
same time-restricted summary writing tasks, 
reading news under different conditions: with 
no IE results at all, with traditional single-
document IE results, and with cross-document 
IE results. Our results show that, in compari-
son to using source documents only, the qual-
ity of summary reports assembled using IE 
results, especially from cross-document IE, 
was significantly better and user satisfaction 
was higher. We also compare the impact of 
different user groups on the results.  
1 Introduction 
Information Extraction (IE) is a task of identifying 
??facts?? (entities, relations and events) within un-
structured documents, and converting them into 
structured representations (e.g., databases). IE 
techniques have been effectively applied to differ-
ent domains (e.g. daily news, Wikipedia, biomedi-
cal reports, financial analysis and legal 
documentations) and different languages. Recently 
we described a new cross-document IE task (Ji et 
al., 2009) to extract events across-documents and 
track them on a time line.  Compared to traditional 
single-document IE, this new task can extract more 
salient, accurate and concise event information. 
However, a significant question remains: will 
the events extracted by IE, especially this new 
cross-document IE task, actually help end-users to 
make better use of the large volumes of news? In 
order to investigate whether we have reached this 
goal, we performed an extrinsic utility (i.e., use-
fulness) and usability evaluation on IE results. 
Two groups of subjects were asked to perform the 
same time-restricted summary writing tasks, read-
ing news under different conditions: with no IE 
results at all, with traditional single-document IE 
results, and with cross-document IE results. Our 
results show that, in comparison to using source 
documents only, the quality of summary reports 
assembled using IE techniques, especially from 
cross-document IE, was significantly better. Also, 
as extraction quality increases from no IE at all to 
single-document IE and then to cross-document IE, 
user satisfaction increases. We also compare the 
impact of different user groups on the results. To 
the best of our knowledge, this is the first system-
atic evaluation of cross-document IE from a us-
ability perspective.  
2 Overview of IE Systems 
We applied the English single-document IE system 
(Ji and Grishman, 2008) and cross-document IE 
system presented in (Ji et al, 2009). Both systems 
were developed for the ACE program1.  
The single-document IE system can extract 
events from individual documents. The core stages 
include entity extraction, time expression extrac-
tion and normalization, relation extraction and 
event extraction. Events include the 33 distinct 
types defined in ACE05. The extraction results are 
presented in tabular form.  
The cross-document IE system can identify im-
portant person entities which are frequently in-
                                                          
1 http://www.itl.nist.gov/iad/mig/tests/ace/2005/ 
285
volved in events as ??centroid entities??; and then for 
each centroid entity, link and order the events cen-
tered around it on a time line and associate them to 
a geographical map. The event chains are pre-
sented in a user-friendly graphical interface (Ji and 
Chen, 2009). Both systems link the events back to 
their context documents.  
3 Evaluation Methods 
3.1 Study Execution 
Our measurement challenge is to assess how IE 
techniques affect users?? abilities to perform real-
world tasks. We followed the summary writing 
task described in the Integrated Feasibility Ex-
periment of the DARPA TIDES program (Colbath 
and Kubala, 2003) and the daily task conducted by 
intelligence analysts (Bodnar, 2003). Each task in 
our evaluation is based on writing a summary of 
ACE-type events involving a specific centroid en-
tity, using one of three levels of support: 
? Level (I): Read the news articles, with assistance 
of keyword based sentence search; 
? Level (II): (I) + with assistance from single-
document IE results; 
? Level (III): (I) + with assistance from cross-
document IE results. 
The summary writing task for each entity using 
any level should be finished in 10 minutes. The 
users can choose to trust the IE results to create 
new sentences or select relevant sentences from 
the source documents. The IE systems were ap-
plied to a corpus of 106 articles from ACE 2005 
training data. 
3.2 Summary Scoring 
We measure user responses in three aspects:  
? Observer-based Quantity -- How many sen-
tences are extracted in each summary? How 
many of them are uniquely correct? 
? Observer-based Quality-- How fluent and coher-
ent are the sentences in each summary?  
? User-based Usability -- How does the user feel 
about the system?  
3.3 User Group Selection 
We selected user groups based on the principles 
that we should run as many tests as we can afford 
(Nielsen, 1994), and at least 5 to insure that we 
detect any major usability problems (Faulkner, 
2003). Two different groups of users were asked to 
conduct the evaluation: 
(1) Hallway Evaluation 
We chose the first group of users with a ??Hallway 
Testing?? user-study method described in (Nielsen, 
1994). We randomly asked 11 PhD students in the 
field of natural language processing to conduct the 
evaluation. In order to evaluate these three levels 
independently, each student was asked to write at 
most one summary, using one of the three levels, 
for any single centroid entity. To avoid the impact 
of diverse text comprehension abilities, each stu-
dent was involved in all of these three levels for 
different centroid entities. 
(2) Remote Evaluation 
An effective utility evaluation will require users 
with a diversity of prior knowledge and computer 
experience. Therefore we asked the second group 
of 11 users in a remote usability testing mode 
(Hammontree et al, 1994). We sent out the request 
to university-wide undergraduate student mailing 
lists and found 11 users to work on the evaluation. 
The evaluation procedure follows the Hallway 
Testing method, except that the tests are carried 
out in the user??s own environment (rather than labs) 
helping further simulate real-life scenario testing. 
Also the users didn??t meet with the observers and 
thus they were not aware of any expectations for 
results. 
4 Evaluation Results 
In this section we will focus on reporting the re-
sults from Hallway Evaluation, while providing 
comparisons with Remote Evaluation. 
4.1 Observer-based Quantity 
The summaries were judged by two annotators and 
the judgements reconciled. A summary sentence is 
judged as uniquely correct if it: (1) includes rele-
vant events involving the centroid entity; and (2) 
the same information was not included in previous 
sentences in the current summary. This metric can 
be considered as an approximate com bination of 
the ??content responsiveness??, ??non-
redundancy??and ??focus?? criteria in the NIST TAC 
summarization track2.  Table 1 presents the  
                                                          
2http://www.nist.gov/tac/2009/Summarization/update.su
mm.09.guidelines.html 
286
Cen-
troid 
(I) (II) (III) Cen-
troid 
(I) (II) (III) Cen-
troid 
(I) (II) (III) 
Bush 3/1/0 5/1/2 6/0/0 Al-douri 4/3/3 4/2/0 6/0/1 Ba??asyir 3/1/0 3/0/0 5/0/0
Ibrahim 4/0/1 5/0/0 8/0/0 Giuliani 2/0/0 3/2/0 5/0/0 Erdogan 1/0/1 4/0/0 4/0/0
Toefting 0/0/0 7/1/0 4/0/0 Blair 2/0/1 3/0/0 5/0/0 Diller 3/0/0 4/1/0 3/0/0
Putin 2/1/0 4/3/2 7/1/1 Pasko 3/0/0 3/0/0 2/0/0 Overall 27/6/6 45/10/5 55/1/2
 
Table 1. # (uniquely correct sentences)/ #(redundant correct sentences)/ 
#(spurious sentences) in a summary in Hallway Evaluation 
 
quantified Hallway Testing results for each cen-
troid separately and the overall score. It shows that 
overall Level (II) contained 18 more correct sen-
tences than the baseline (I), while (III) achieved 11 
further correct sentences. (I) obtained significantly 
fewer sentences without assistance from IE tools. 
We conducted the Wilcoxon Matched-Pairs 
Signed-Ranks Test on a query entity basis for ac-
curacy - number of (uniquely correct sen-
tences)/number of (total extracted sentences in a 
summary). The results show that (III) is signifi-
cantly better than (I) at a 99.2% confidence level, 
and better than (II) at a 96.9% confidence level. (II) 
is not significantly better than (I). 
We can also see that for some centroid entities 
such as ??Putin??, ??Al-douri?? and ??Giuliani??, (II) 
generated more sentences but also introduced more 
redundant information. The user feedback has in-
dicated that they did not have enough time to re-
move redundancy. In contrast, (III) yielded much 
less redundant information. In fact, the average 
time the users spent using (III) was only about 7.2 
minutes. Therefore we can conclude that cross-
document IE can produce more informative sum-
maries in a more efficient way. 
Error analysis showed that the major error types 
propagated from IE to summaries are as follows. 
1. Event time errors. For example, the summary 
sentence ??Toefting was convicted in September 
2001 of assaulting a pair of restaurant workers in 
the capital?? was judged as incorrect because the 
time argument should be ??October 2002??. 
2. Pronoun resolution errors. When a pronoun is 
mistakenly linked to an entity, incorrect event ar-
guments will be included in the summaries.  
3. Event type errors. When an event is mis-
classified, the users tend to use incorrect templates 
and thus generate wrong summaries. 
4. Negative events. Sometimes the event attrib-
ute classifier makes mistakes and the users include 
negative events in the summaries. 
4.2 Impact of User Groups 
In the Remote Testing, the accuracy results from 
the three levels are as follows: 21/37, 28/37 and 
31/36. Thus both user groups benefited from using 
IE techniques, but the enhancements vary a lot. In 
the Hallway Testing, the users were better trained 
and more familiar with IE tools (including the 
graphical interface of cross-document IE); and thus 
they can benefit more from the IE techniques. In 
contrast, in the Remote Evaluation, the users had 
quite diverse knowledge backgrounds. For exam-
ple, one remote user was only able to find 1-2 sen-
tences using any of the three levels; while another, 
more skilled remote user found more than 5 sen-
tences with any level. However the Remote 
Evaluation is important to gather the feedback of 
the more subjective usability evaluation in section 
4.4. Because the users in Hallway Testing may be 
aware of the observations that the observer is hop-
ing to achieve, they may provide potentially biased 
feedback. 
4.3 Observer-based Quality 
The evaluation also showed that (III) produced 
summaries with better quality. We asked the ob-
servers to give a score between [1, 10] to each 
summary according to the following TAC summa-
rization quality criteria: Readability/Fluency, Ref-
erential Clarity and Structure/Coherence. Table 2 
shows the evaluation results for the three different 
methods. 
 
Criteria (I) (II) (III) 
Readability/Fluency 9.4 8.5 8.2 
Referential Clarity 6.1 8.3 8.7 
Structure/Coherence 7.1 7.6 8.5 
 
Table 2. Observer-based Average Quality 
 
In their detailed feedback, the users indicated 
that (III) has the following advantages: (1) Better 
287
Cen-nt-r eoindt (n-Ir )Bur sneor hn3Cdo or /-1r /hht0
e/ or  o3Cne/dr ne1oer 5oh/tior )222ur6/-r eohnAoer t-0
l-n4-r  (3or /eat3o- ir ti(-ar henii01nht3o- r
(-??oeo-hoyr)bur6/-rao-oe/ or/5i e/h (Aorit33/e(oiyr
mner  8or 5(nae/C8(h/dr oAo- ir )oyayr o3CdnG3o- uEr
in3or tioeir 4oeor /5dor  nr tior iCoh(??(hr  o3Cd/ oir
ith8r/irgTf7r4/ir8(eo1r5GrD7Pr/ rk2sf.r nr4e( or
it33/e(oiyr mner o#/3CdoEr /r io- o-hor gqti8r /-1r
qd/(er3o r/ r6/3Crc/A(1r/-1r 8orpHr 8eoor (3oir(-r
s/eh8r Bwwb.r 4/ir 1oe(Ao1r ??en3r  8eoor 1(????oeo- r
g6n- /h 0soo (-a.r oAo- ir (-r  8oroAo- r h8/(-iyr )vur
6/-r hn--oh r eod/ o1r oAo- ir (- nr 3neor hn-h(ior
it33/e(oiyrmnero#/3CdoErioAoe/droAo- ir4oeorhn-0
-oh o1r nrao-oe/ or  8or??nddn4(-ario- o-hoirgT/ilnr
4/ir /CCo/do1r??ner  eo/in-rhe(3orn-rLCe(dr ,WErBwwbr
and thenreodo/io1rn-rxt-or,MErBwwb.yrk8oreo/1/5(d0
( Gr ihneoir (-rk/5dorBr /dinr (-1(h/ or  8/ r /r3neoro??0
??oh (Aor  o3Cd/ or ao-oe/ (n-r 3o 8n1r i8ntd1r 5or
1oAodnCo1r nrCen1thor3neor??dto- rit33/e(oir5/io1r
n-r2freoitd iyr
4.4 User-based Usability 
k8or tioer ??oo15/hlr ??en3r 5n 8r oA/dt/ (n-ir /dinr
i8n4o1r  8/ r )22ur /-1r )222ur eoitd ir 4oeor  eti o1r /d0
3ni roSt/ddGEr/-1r)222ur4/irhd/(3o1r nrCenA(1or 8or
3ni r tio??tdr ??t-h (n-iyr k8or Cni( (Aor hn33o- ir
/5nt r)222ur(-hdt1orgko3Cne/drR(-l(-ar/ddn4irdna(0
h/dreo/in-(-ar/-1rao-oe/d(9/ (n-.Erg6o- en(1rio/eh8r
8odCir  nr ??nhtir (33o1(/ odG.Er g%C/ (/drR(-l(-ar /d0
dn4ir  nr 5en4ior /ddr  8orCd/hoir 48(h8r /rCoein-r 8/ir
A(i( o1.Er g??/3or 1(i/35(at/ (n-r 8odCir  nr ??(d oer (e0
eodoA/- r (-??ne3/ (n-.Er g6/-r ??(-1r loGr (-??ne3/ (n-r
??en3r oAo- r h8/(-i.Er gk(3od(-or 8odCir hneeod/ or
oAo- i.Ir /-1r  8or -oa/ (Aor hn33o- ir (-hdt1or
g%n3o (3oir 2fr oeeneir 3(ido/1r dnh/ (-ar  8or io-0
 o-hoi.Erg??nritCCne rn??r-/3orC/(erio/eh8r??ner3oo 0
(-ar oAo- i.Erg??nrhndnero3C8/i(ir n??r oAo- ir n-r  8or
ne(a(-/dr 1nht3o- i.r /-1r g??nr itaaoi (n-ir n??r  o30
Cd/ oir nrhn3Cniorit33/eGrio- o-hoi.yrr
5 Conclusion and Future Work 
k8enta8r /r t (d( Gr oA/dt/ (n-r n-r it33/eGr 4e( (-ar
4or 8/Aor CenAo1r  8/ r 2fr  oh8-(StoiEr oiCoh(/ddGr
henii01nht3o- r2fErh/-r/(1r-o4ir5en4i(-aErio/eh8r
/-1r/-/dGi(iyr2-rC/e (htd/eEr o3Cne/droAo- r e/hl(-ar
/heniir 1nht3o- ir 8odCir tioeir Coe??ne3r 5o  oer / r
??/h 0a/ 8oe(-ar  8/-r  8oGr1nr4( 8nt r2fyrpioeir /dinr
Cen1tho1r3neor(-??ne3/ (Aorit33/e(oir4( 8rhenii0
1nht3o- r2fr 8/-r4( 8r e/1( (n-/dri(-ado01nht3o- r
2fyr??or/dinrhn3C/eo1r/-1r/-/dG9o1r 8or1(????oeo-hoir
5o 4oo-r  4nr tioer aentCiyr %th8r 3o/iteoir n??r  8or
5o-o??( ir  nr  8or oAo- t/dr o-1r tioeir /dinr CenA(1o1r
??oo15/hlrn-r48/ r4nelir4oddr /-1r (1o- (??(o1r /11(0
 (n-/dr eoio/eh8r Cen5do3iEr ith8r /ir  nr o#C/-1r  8or
ho- en(1r  nr /rC/(ern??ro- ( (oir /-1r  nrCenA(1orhn-??(0
1o-hor3o e(hir(-r 8or(- oe??/hoyr2-r 8or??t teor4or/(3r
 nrio rtCr/-rn-d(-or-o4ir/e (hdor/-/dGi(iriGi o3r/-1r
Coe??ne3rd/eaoer/-1reoatd/ert (d( GroA/dt/ (n-iyrr
Acknowledgement 
k8(ir 4nelr 4/ir itCCne o1r 5Gr  8or py%yr ??%mr
6L7ff7r L4/e1r t-1oer Pe/- r 22%0wjMb,vjEr  8or
py%yr Le3Gr 7oio/eh8r R/5ne/ neGr t-1oer 6nnCoe/0
 (Aor Laeoo3o- r ??t35oer ??j,,??m0wj0B0wwMbEr
PnnadoEr 2-hyEr 6p??Fr 7oio/eh8r f-8/-ho3o- r Ten0
ae/3Erm/htd GrTt5d(h/ (n-rTenae/3r/-1rP7k2rTen0
ae/3yrk8orA(o4ir/-1rhn-hdti(n-irhn- /(-o1r(-r 8(ir
1nht3o- r /eor  8niorn??r  8or /t 8neir /-1r i8ntd1r -n r
5or(- oeCeo o1r/ireoCeoio- (-ar  8orn????(h(/drCnd(h(oiEr
o( 8oero#Ceoiio1rner(3Cd(o1Ern??r 8orLe3Gr7oio/eh8r
R/5ne/ neGr ner  8or py%yr PnAoe-3o- yr k8or py%yr
PnAoe-3o- r (ir /t 8ne(9o1r  nr eoCen1thor /-1r 1(i0
 e(5t oreoCe(- ir ??nerPnAoe-3o- rCteCnioir -n 4( 80
i /-1(-ar/-GrhnCGe(a8 r-n / (n-r8oeorn-yr
References  
xn8-r ??yr qn1-/eyr Bwwbyr ??/e-(-ar L-/dGi(ir ??ner  8or 2-0
??ne3/ (n-rLaoOr7o 8(-l(-ar  8or 2- odd(ao-horTenhoiiyr
Center for Strategic Intelligence Research, Joint 
Military Intelligence CollegeEr??/i8(-a n-Ercy6yr
%o/-r 6nd5/ 8r /-1rme/-h(ir Ht5/d/yrBwwbyr kLT0NROrL-r
Lt n3/ o1r L-/dGi :ir Lii(i /- yr Proc. HLT-NAACL 
2003 (demonstrations)yr
R/te/rm/tdl-oeyrBwwbyrqoGn-1r 8or??(Ao0tioer/iit3C (n-Or
qo-o??( irn??r(-heo/io1ri/3Cdori(9oir(-rti/5(d( Gr oi (-ayr
Behavior Research Methods Instruments and Com-
putersrbM)buErb;j0b[byr
sn- Gr ]/33n- eooEr T/tdr ??o(doer /-1r ??/-1(-(r ??/G/lyr
,jjvyr 7o3n or pi/5(d( Gr koi (-ayr Interactionsyr znd0
t3or,Er2iitorbyrT/aoiOrB,0BMyr
]o-arx(r /-1rQ8o-ar68o-yrBwwjyr6enii01nht3o- rko30
Cne/dr /-1r %C/ (/dr Toein-r ke/hl(-ar %Gi o3r co3n-0
i e/ (n-yrProc. HLT-NAACL 2009yr
]o-ar x(Er 7/dC8r Pe(i83/-Er Q8o-ar 68o-r /-1r Te/i8/- r
PtC /yr Bwwjyr 6enii01nht3o- r fAo- r f# e/h (n-Er
7/-l(-ar /-1r ke/hl(-ayr Proc. Recent Advances in 
Natural Language Processing 2009.r
x/ln5r ??(odio-yr ,jjvyr pi/5(d( Gr f-a(-ooe(-ayr snea/-r
H/t??3/--rTt5d(i8oeiyr
288
