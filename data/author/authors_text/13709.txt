Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 37?45,
Beijing, August 2010
Plagiarism Detection across Distant Language Pairs
Alberto Barro?n-Ceden?o Paolo Rosso
Natural Language Engineering Lab. - ELiRF
Universidad Polite?cnica de Valencia
{lbarron, prosso}@dsic.upv.es
Eneko Agirre Gorka Labaka
IXA NLP Group
Basque Country University
{e.agirre, gorka.labaka}@ehu.es
Abstract
Plagiarism, the unacknowledged reuse of
text, does not end at language boundaries.
Cross-language plagiarism occurs if a text
is translated from a fragment written in a
different language and no proper citation
is provided. Regardless of the change of
language, the contents and, in particular,
the ideas remain the same. Whereas dif-
ferent methods for the detection of mono-
lingual plagiarism have been developed,
less attention has been paid to the cross-
language case.
In this paper we compare two recently
proposed cross-language plagiarism de-
tection methods (CL-CNG, based on char-
acter n-grams and CL-ASA, based on sta-
tistical translation), to a novel approach
to this problem, based on machine trans-
lation and monolingual similarity analy-
sis (T+MA). We explore the effectiveness
of the three approaches for less related
languages. CL-CNG shows not be ap-
propriate for this kind of language pairs,
whereas T+MA performs better than the
previously proposed models.
1 Introduction
Plagiarism is a problem in many scientific and cul-
tural fields. Text plagiarism may imply differ-
ent operations: from a simple cut-and-paste, to
the insertion, deletion and substitution of words,
up to an entire process of paraphrasing. Differ-
ent models approach the detection of monolin-
gual plagiarism (Shivakumar and Garc??a-Molina,
1995; Hoad and Zobel, 2003; Maurer et al, 2006).
Each of these models is appropriate only in those
cases where all the implied documents are written
in the same language.
Nevertheless, the problem does not end at lan-
guage boundaries. Plagiarism is also committed if
the reused text is translated from a fragment writ-
ten in a different language and no citation is pro-
vided. When plagiarism is generated by a transla-
tion process, it is known as cross-language plagia-
rism (CLP).
Less attention has been paid to the detection of
this kind of plagiarism due to its enhanced diffi-
culty (Ceska et al, 2008; Barro?n-Ceden?o et al,
2008; Potthast et al, 2010). In fact, in the recently
held 1st International Competition on Plagiarism
Detection (Potthast et al, 2009), no participants
tried to approach it.
In order to describe the prototypical process of
automatic plagiarism detection, we establish the
following notation. Let dq be a plagiarism suspect
document. Let D be a representative collection
of reference documents. D presumably includes
the source of the potentially plagiarised fragments
in dq . Stein et al, (2007) divide the process into
three stages1:
1. heuristic retrieval of potential source doc-
uments: given dq, retrieving an appropri-
ate number of its potential source documents
D? ? D such that |D?|? |D|;
2. exhaustive comparison of texts: comparing
the text from dq and d ? D? in order to
identify reused fragments and their potential
1This schema was formerly proposed for monolingual
plagiarism detection. Nevertheless, it can be applied with-
out further modifications to the cross-language case.
37
sources; and
3. knowledge-based post-processing: those de-
tected fragments with proper citation are dis-
carded as they are not plagiarised.
The result is offered to the human expert to take
the final decision. In the case of cross-language
plagiarism detection (CLPD), the texts are written
in different languages: dq ? L and d? ? L?.
In this research we focus on step 2: cross-
language exhaustive comparison of texts, ap-
proaching it as an Information Retrieval problem
of cross-language text similarity. Step 1, heuristic
retrieval, may be approached by different CLIR
techniques, such as those proposed by Dumais et
al. (1997) and Pouliquen et al (2003).
Cross-language similarity between texts,
?(dq, d?), has been previously estimated on
the basis of different models: multilingual
thesauri (Steinberger et al, 2002; Ceska et
al., 2008), comparable corpora ?CL-Explicit
Semantic Analysis CL-ESA? (Potthast et
al., 2008), machine translation techniques
?CL-Alignment-based Similarity Analysis CL-
ASA? (Barro?n-Ceden?o et al, 2008; Pinto et al,
2009) and n-grams comparison ?CL-Character
n-Grams CL-CNG? (Mcnamee and Mayfield,
2004).
A comparison of CL-ASA, CL-ESA, and CL-
CNG was carried out recently by Potthast et
al. (2010). The authors report that in general,
despite its simplicity, CL-CNG outperformed the
other two models. Additionally, CL-ESA showed
good results in the cross-language retrieval of
topic-related texts, whereas CL-ASA obtained
better results in exact (human) translations.
However, most of the language pairs used in the
reported experiments (English-{German, Span-
ish, French, Dutch, Polish}) are related, whether
because they have common predecessors or be-
cause a large proportion of their vocabularies
share common roots. In fact, the lower syntactical
relation between the English-Polish pair caused
a performance degradation for CL-CNG, and for
CL-ASA to a lesser extent. In order to confirm
whether the closeness among languages is an im-
portant factor, this paper works with more dis-
tant language pairs: English-Basque and Spanish-
Basque.
The rest of the paper is structured as follows.
Section 2 describes the motivation for working
on this research topic, stressing the situation of
cross-language plagiarism among writers in less
resourced languages. A brief overview of the few
works on CLPD is included. The three similar-
ity estimation models compared in this research
work are presented in Section 3. The experimental
framework and the obtained results are included
in Section 4. Finally, Section 5 draws conclusions
and discusses further work.
2 Motivation
Cases of CLP are common nowadays because in-
formation in multiple languages is available on the
Web, but people still write in their own language.
This special kind of plagiarism occurs more often
when the target language is a less resourced one2,
as is the case of Basque.
Basque is a pre-indoeuropean language with
less than a million speakers in the world and
no known relatives in the language families
(Wikipedia, 2010a). Still, Basque shares a portion
of its vocabulary with its contact languages (Span-
ish and French). Therefore, we decided to work
with two language pairs: Basque with Spanish,
its contact language, and with English, perhaps
the language with major influence over the rest of
languages in the world. Although the considered
pairs share most of their alphabet, the vocabulary
and language typologies are very different. For
instance Basque is an agglutinative language.
In order to illustrate the relations among these
languages, Fig. 1 includes extracts from the En-
glish (en), Spanish (es) and Basque (eu) versions
of the same Wikipedia article. The fragments are
a sample of the lexical and syntactic distance be-
tween Basque and the other two languages. In
fact, these sentences are completely co-derived
and the corresponding entire articles are a sample
of the typical imbalance in text available in the dif-
ferent languages (around 2, 000, 1, 300, and only
2Less resourced language is that with a low degree of rep-
resentation on the Web (Alegria et al, 2009). Whereas the
available text for German, French or Spanish is less than for
English, the difference is more dramatic with other languages
such as Basque.
38
The Party of European Socialists (PES) is
a European political party comprising thirty-two
socialist, social democratic and labour parties
from each European Union member state and
Norway.
El Partido Socialista Europeo (PSE) es un
partido pol??tico pan-europeo cuyos miembros
son de partidos socialdemo?cratas, socialistas y
laboristas de estados miembros de la Unio?n Eu-
ropea, as?? como de Noruega.
Europako Alderdi Sozialista Europar Bata-
suneko herrialdeetako eta Norvegiako hogeita
hamahiru alderdi sozialista, sozialdemokrata eta
laborista biltzen dituen alderdia da.
Figure 1: First sentences from the Wikipedia arti-
cles ?Party of European Socialists? (en),?Partido
Socialista Europeo? (es), and ?Europako Alderdi
Sozialista? (eu) (Wikipedia, 2010b).
100 words are contained in the en, es and eu arti-
cles, respectively).
Of high relevance is that the two corpora used
in this work were manually constructed by trans-
lating English and Spanish text into Basque. In the
experiments carried out by Potthast et al (2010),
which inspired our work, texts from the JCR-
Acquis corpus (Steinberger et al, 2006) and
Wikipedia were used. The first one is a multilin-
gual corpus with no clear definition of source and
target languages, whereas in Wikipedia no spe-
cific relationship exists between the different lan-
guages in which a topic may be broached. In some
cases (cf. Fig. 1) they are clearly co-derived, but
in others they are completely independent.
CLPD has been investigated just recently,
mainly by adapting models formerly proposed
for cross-language information retrieval. This
is the case of cross-language explicit seman-
tic analysis (CL-ESA), proposed by Potthast et
al. (2008). In this case the comparison be-
tween texts is not carried out directly. Instead,
a comparable corpus CL,L? is required, contain-
ing documents on multiple topics in the two im-
plied languages. One of the biggest corpora
of this nature is Wikipedia. The similarity be-
tween dq ? L and every document c ? CL
is computed based on the cosine measure. The
same process is made for L?. This step gener-
ates two vectors [cos(dq, c1), . . . , cos(dq, c|CL|)]
and [cos(d?, c?1), . . . , cos(d?, c?|CL? |)], where each
dimension is comparable between the two vectors.
Therefore, the cosine between such vectors can be
estimated in order to ?indirectly? estimate how
similar dq and d? are. The authors suggest that this
model can be used for CLPD.
Another recent model is MLPlag, proposed by
Ceska et al (2008). It exploits the EuroWord-
Net Thesaurus3, that includes sets of synonyms in
multiple European languages, with common iden-
tifiers across languages. The authors report ex-
periments over a subset of documents of the En-
glish and Czech sections of the JRC-Acquis cor-
pus as well as a corpus of simplified vocabulary4 .
The main difficulty they faced was the amount of
words in the documents not included in the the-
saurus (approximately 50% of the vocabulary).
This is a very similar approach to that pro-
posed by Pouliquen et al (2003) for the identi-
fication of document translations. In fact, both
approaches have something in common: transla-
tions are searched at document level. It is assumed
that an entire document has been reused (trans-
lated). Nevertheless, a writer is free to plagiarise
text fragments from different sources, and com-
pose a mixture of original and reused text.
A third model is the cross-language alignment-
based similarity analysis (CL-ASA), proposed by
Barro?n-Ceden?o et al (2008), which is based on
statistical machine translation technology. This
model was proposed to detect plagiarised text
fragments (similar models have been proposed for
extraction of parallel sentences from comparable
corpora (Munteanu et al, 2004)). The authors
report experiments over a short set of texts from
which simulated plagiarism was created from En-
glish to Spanish. Human as well as automatic ma-
chine translations were included in the collection.
Further descriptions of this model are included in
Section 3, as it is one of those being assessed in
this research work.
To the best of our knowledge, no work (in-
cluding the three previously mentioned) has been
done considering less resourced languages. In this
research work we approach the not uncommon
problem of CLPD in Basque, with source texts
written in Spanish (the co-official language of the
3http://www.illc.uva.nl/EuroWordNet/
4The authors do not mention the origin of the documents.
39
low tok pd bd sd lem
T+MA   
CL-ASA   
CL-CNG    
Table 1: Text preprocessing operations re-
quired for the different models. low=lowercasing,
tok=tokenization, pd=punctuation marks deletion, bd=blank
space deletion, sd=symbols deletion, lem=lematization.
Basque Country) and English (the language with
most available texts in the world).
We compare three cross-language similarity
analysis methods: T+MA (translation followed
by monolingual analysis), a novel method based
on machine translation followed by a monolin-
gual similarity estimation; CL-CNG, a character
n-gram based comparison model; and CL-ASA
a model that combines translation and similarity
estimation in a single step. Neither MLPlag nor
CL-ESA are included in the comparison. On the
one hand, we are interested in plagiarism at sen-
tence level, and MLPlag is designed to compare
entire documents. On the other hand, in previous
experiments over exact translations, CL-ASA has
shown to outperform it on language pairs whose
alphabet or syntax are unrelated (Potthast et al,
2010). This is precisely the case of en-eu and
es-eu language pairs. Additionally, the amount
of Wikipedia articles in Basque available for the
construction of the required comparable corpus is
insufficient for the CL-ESA data requirements.
3 Definition of Models
In this section, we describe the three cross-
language similarity models we compare. For ex-
perimental purposes (cf. Section 4) we consider
dq to be a suspicious sentence written in L and
D? to be a collection of potential source sentences
written in L? (L 6= L?). The text pre-processing
required by the different models is summarised
in Table 1. Examples illustrating how the models
work are included in Section 4.3.
3.1 Translation + Monolingual Analysis
dq ? L is translated into L? on the basis of
the Giza++ (Och and Ney, 2003), Moses (Koehn
et al, 2007) and SRILM (Stolcke, 2002) tools,
generating d?q . The translation system uses a
log-linear combination of state-of-the-art features,
such as translation probabilities and lexical trans-
lation models on both directions and a target lan-
guage model. After translation, d?q and d? are
lexically related, making possible a monolingual
comparison.
Multiple translations from dq into d?q are pos-
sible. Therefore, performing a monolingual sim-
ilarity analysis based on ?traditional? techniques,
such as those based on word n-grams compari-
son (Broder, 1997) or hash collisions (Schleimer
et al, 2003), is not an option. Instead, we take the
approach of the bag-of-words, which has shown
good results in the estimation of monolingual text
similarity (Barro?n-Ceden?o et al, 2009). Words in
d?q and d? are weighted by the standard tf -idf , and
the similarity between them is estimated by the
cosine similarity measure.
3.2 CL-Alignment-based Similarity Analysis
In this model an estimation of how likely is that d?
is a translation of dq is performed. It is based on
the adaptation of the Bayes rule for MT:
p(d? | dq) = p(d
?) p(dq | d?)
p(dq)
. (1)
As p(dq) does not depend on d?, it is neglected.
From an MT point of view, the conditional prob-
ability p(dq | d?) is known as translation model
probability and is computed on the basis of a sta-
tistical bilingual dictionary. p(d?) is known as lan-
guage model probability; it describes the target
language L? in order to obtain grammatically ac-
ceptable translations (Brown et al, 1993).
Translating dq into L? is not the concern of
this method, rather it focuses on retrieving texts
written in L? which are potential translations of
dq . Therefore, Barro?n-Ceden?o et al (2008) pro-
posed replacing the language model (the one used
in T+MA) by that known as length model. This
model depends on text?s character lengths instead
of language structures.
Multiple translations from d into L? are possi-
ble, and it is uncommon to find a pair of translated
texts d and d? such that |d| = |d?|. Nevertheless,
the length of such translations is closely related
to a translation length factor. In accordance with
Pouliquen et al (2003), the length model is de-
fined as:
40
?(d?) = e
?0.5
0
B
@
|d?|
|dq| ??
?
1
C
A
2
, (2)
where ? and ? are the mean and the standard devi-
ation of the character lengths between translations
of texts from L into L?. If the length of d? is not the
expected given dq, it receives a low qualification.
The translation model probability is defined as:
p(d | d?) =
Y
x?d
X
y?d?
p(x, y), (3)
where p(x, y), a statistical bilingual dictionary,
represents the likelihood that x is a valid transla-
tion of y. After estimating p(x, y) from a parallel
corpus, on the basis of the IBM statistical trans-
lation models (Brown et al, 1993), we consider,
for each word x, only the k best translations y
(those with the highest probabilities) up to a min-
imum probability mass of 0.4. This threshold was
empirically selected as it eliminated noisy entries
without discarding an important amount of rele-
vant pairs.
The similarity estimation based on CL-ASA is
finally computed as:
?(dq, d?) = ?(d?) p(dq | d?). (4)
3.3 CL-Character n-Gram Analysis
This model, the simplest of those compared in this
research, has been used in (monolingual) Author-
ship Attribution (Keselj et al, 2003) as well as
cross-language Information Retrieval (Mcnamee
and Mayfield, 2004). The simplified alphabet con-
sidered is ? = {a, . . . , z, 0, . . . , 9}; any other
symbol is discarded (cf. Table 1). The resulting
text strings are codified into character 3-grams,
which are weighted by the standard tf -idf (con-
sidering this n has previously shown to produce
the best results). The similarity between such rep-
resentations of dq and d? is estimated by the cosine
similarity measure.
4 Experiments
The objective of our experiments is to compare
the performance of the three similarity estimation
models. Section 4.1 introduces the corpora we
have exploited. The experimental framework is
described in Section 4.2. Section 4.3 illustrates
how the models work, and the obtained results are
presented and discussed in Section 4.4.
4.1 Corpora
In other Information Retrieval tasks a plethora of
corpora is available for experimental and compar-
ison purposes. However, plagiarism implies an
ethical infringement and, to the best of our knowl-
edge, there is no corpora of actual cases available,
other than some seminal efforts on creating cor-
pora of text reuse (Clough et al, 2002), artificial
plagiarism (Potthast et al, 2009), and simulated
plagiarism (Clough and Stevenson, 2010). The
problem is worse for cross-language plagiarism.
Therefore, in our experiments we use two
parallel corpora: Software, an en-eu translation
memory of software manuals generously supplied
by Elhuyar Fundazioa5; and Consumer, a cor-
pus extracted from a consumer oriented mag-
azine that includes articles written in Spanish
along with their Basque, Catalan, and Galician
translations6 (Alca?zar, 2006). Software includes
288, 000 parallel sentences; 8.66 (6.83) words per
sentence in the English (Basque) section. Con-
sumer contains 58, 202 sentences; 19.77 (15.20)
words per sentence in Spanish (Basque). These
corpora also reflect the imbalance of text available
in the different languages.
4.2 Experimental Framework
We consider Dq and D? to be two entire docu-
ments from which plagiarised sentences and their
source are to be detected. We work at this level
of granularity, and not entire documents, for two
main reasons: (i) we are focused on the exhaus-
tive comparison stage of the plagiarism detection
process (cf. Section 1); and (ii) even a single sen-
tence could be considered a case of plagiarism,
as it transmits a complete idea. However, a pla-
giarised sentence is usually not enough to auto-
matically negate the validity of an entire docu-
ment. This decision is left to the human expert,
which can examine the documents where several
plagiarised sentences occur. Note that the task be-
comes computationally more expensive as, for ev-
ery sentence, we are looking through thousands
5http://www.elhuyar.org
6http://revista.consumer.es
41
es-eu en-eu
? ? ? ?
f1 1.1567 0.2346 1.0561 0.5497
f2 1.1569 0.2349 1.0568 0.5510
f3 1.1571 0.2349 1.0566 0.5433
f4 1.1565 0.2363 1.0553 0.5352
f5 1.1571 0.2348 1.0553 0.5467
avg. 1.1569 0.2351 1.0560 0.5452
Table 2: Length models estimated for each train-
ing partition f1,...,5. The values describe a normal distri-
bution centred in ? ? ?, representing the expected length of
the source text given the suspicious one.
0
0.2
0.4
0.6
0.8
1
0 50 100 150 200 250
Pr
o
ba
bi
lit
y
di
st
rib
u
tio
n
Length of the sentences
eu
es
en
Figure 2: Example length factor for a sentence
written in Basque (eu) dq , such that |dq| = 90.
The normal distributions represent the expected lengths for
the translation d?, either in Spanish (es) or English (en).
of topically-related sentences that are potential
sources of dq, and not only those of a specific doc-
ument.
CLPD is considered a ranking problem. Let
dq ? Dq be a plagiarism suspicious sentence and
d? ? D? be its source sentence. We consider that
the result of the process is correct if, given dq, d?
is properly retrieved. A 5-fold cross validation for
both en-eu and es-eu was performed. Bilingual
dictionaries, language and length models were es-
timated with the corresponding training partitions.
The computed values for ? and ? are those in-
cluded in Table 2. The values for the different
partitions are very similar, showing the low vari-
ability in the translation lengths. On the basis of
these estimated parameters, an example of length
factor for a specific sentence is plotted in Fig. 2.
In the test partitions, for each suspicious sen-
tence dq , 11, 640 source candidate sentences exist
for es-eu and 57, 290 for en-eu. This results in
more than 135 million and 3 billion comparisons
carried out for es-eu and en-eu respectively.
xeu yen p(x, y) xeu yen p(x, y)
beste another 0.288 beste other 0.348
dokumentu document 0.681 batzu some 0.422
makro macro 0.558 ezin not 0.179
ezin cannot 0.279 izan is 0.241
izan the 0.162 atzi access 0.591
. . 0.981
Table 3: Entries in the bilingual dictionary for the
words in dq. Relevant entries for the example are in bold.
4.3 Illustration of Models
In order to clarify how the different models work,
consider the following sentence pair, a suspicious
sentence dq written in Basque and its source d?
written in English (sentences are short for illustra-
tive purposes):
dq beste dokumentu batzuetako makroak ezin dira atzitu.
d? macros from other documents are not accessible.
CL-CNG Example
In this case, symbols and spaces are discarded.
Sentences become:
dq bestedokumentubatzuetakomakroakezindiraatzitu
d? macrosfromotherdocumentsarenotaccessible
Only three 3-grams appear in both sentences
(ume, men, ent). In order to keep the example sim-
ple, the 3-grams are weighted by tf only (in the
actual experiments, tf -idf is used), resulting in a
dot product of 3. The corresponding vectors mag-
nitudes are |dq| = 6.70 and |d?| = 5.65. There-
fore, the estimated similarity is ?(dq, d?) = 0.079.
CL-ASA Example
In this case, the text must be tokenised and lem-
matised, resulting in the following string:
dq beste dokumentu batzu makro ezin izan atzi .
d? macro from other document be not accessible .
The sentences? lengths are |dq| = 38 and |d?| =
39. Therefore, on the basis of Eq. 2, the length
factor between them is ?(dq, d?) = 0.998.
The relevant entries of the previously estimated
dictionary are included in Table 3. Such entries
are substituted in Eq. 3, and the overall process
results in a similarity ?(dq , d?) = 2.74. Whereas
not a stochastic value, this is a weight used when
ranking all the potential source sentences in D?.
T+MA Example
In this case, the same pre-processing than
in CL-ASA is performed. In T+MA dq is
translated into L?, resulting in the new pair:
d?q other document macro cannot be access .
d? macro from other document be not accessible .
42
Note that d?q is a valid translation of dq . Never-
theless, it has few syntactic relation to d?. There-
fore, applying more sophisticated codifications
than the cosine measure over bag-of-words is not
an option. The example is again simplified by
weighting the words based on tf . Five words ap-
pear in both sentences, resulting in a dot product
of 5. The vectors magnitudes are |d?q| = |d?| =?
7. The estimation by T+MA is ?(dq, d?) =
0.71, a high similarity level.
4.4 Results and Discussion
For evaluation we consider a standard measure:
Recall. More specifically Recall after n texts have
been retrieved (n = [1 . . . , 50]). Figure 3 plots the
average Recall value obtained in the 5-folds with
respect to the rank position (n).
In both language pairs, CL-CNG obtained
worse results than those reported for English-
Polish by Potthast et al (2010): R@50 = 0.68
vs. R@50 = 0.53 for es-eu and 0.28 for en-eu.
This is due to the fact that neither the vocabulary
nor its corresponding roots keep important rela-
tions. Therefore, when language pairs have a low
syntactical relationship, CL-CNG is not an op-
tion. Still, CL-CNG performs better with es-eu
than with en-eu because the first pair is composed
of contact languages (cf. Section 1).
About CL-ASA, the results obtained with es-
eu and en-eu are quite different: R@50 = 0.68
for en-eu and R@50 = 0.53 for es-eu. Whereas
in the first case they are comparable to those of
CL-CNG, in the second one CL-ASA completely
outperforms it. The improvement of CL-ASA ob-
tained for en-eu is due to the size of the training
corpus available in this case (approximately five
times the number of sentences available for es-
eu). This shows the sensitivity of the model with
respect to the size of the available resources.
Lastly, although T+MA is a simple approach
that reduces the cross-language similarity estima-
tion to a translation followed by a monolingual
process, it obtained a good performance (R@50=
0.77 for en-eu and R@50=0.89 for es-eu). More-
over, this method proved to be less sensitive than
CL-ASA to the lack of resources. This could
be due to the fact that it considers both direc-
tions of the translation model (e[n|s]-eu and eu-
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 10 20 50
R
ec
al
l
rank
CL-ASA
CL-CNG
T+MA
(a) es-eu
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 10 20 50
R
ec
al
l
rank
(b) en-eu
Figure 3: Evaluation of the cross-language rank-
ing. Results plotted as rank versus Recall for the three eval-
uated models and the two language pairs (R@[1, . . . , 50]).
e[n|s]). Additionally, the language model, applied
in order to compose syntactically correct transla-
tions, reduces the amount of wrong translations
and, indirectly, includes more syntactic informa-
tion in the process. On the contrary, CL-ASA
only considers one direction translation model eu-
e[n|s] and completely disregards syntactical rela-
tions between the texts.
Note that the better results come at the cost
of higher computational demand. CL-CNG only
requires easy to compute string comparisons.
CL-ASA requires translation probabilities from
aligned corpora, but once the probabilities are es-
timated, cross-language similarity can be com-
puted very fast. T+MA requires the previous
translation of all the texts, which can be very
costly for large collections.
5 Conclusions and Further Work
In a society where information in multiple lan-
guages is available on the Web, cross-language
43
plagiarism is occurring every day with increasing
frequency. Still, cross-language plagiarism de-
tection has not been approached sufficiently due
to its intrinsic complexity. Though few attempts
have been made, even less work has been made to
tackle this problem for less resourced languages,
and to explore distant language pairs.
We investigated the case of Basque, a lan-
guage where, due to the lack of resources, cross-
language plagiarism is often committed from texts
in Spanish and English. Basque has no known rel-
atives in the language family. However, it shares
some of its vocabulary with Spanish.
Two state-of-the-art methods based on trans-
lation probabilities and n-gram overlapping, and
a novel technique based on statistical machine
translation were evaluated. The novel technique
obtains the best results in both language pairs,
with the n-gram overlap technique performing
worst. In this sense, our results complement those
of Potthast et al (2010), which includes closely
related language pairs as well.
Our results also show that better results come at
the cost of more expensive processing time. For
the future, we would like to investigate such per-
formance trade-offs in more demanding datasets.
For future work we consider that exploring se-
mantic text features across languages could im-
prove the results. It could be interesting to fur-
ther analyse how the reordering of words through
translations might be relevant for this task. Addi-
tionally, working with languages even more dis-
tant from each other, such as Arabic or Hindi,
seems to be a challenging and interesting task.
Acknowledgements
The research work of the first two authors is partially funded
by CONACYT-Mexico and the MICINN project TEXT-
ENTERPRISE 2.0 TIN2009-13391-C04-03 (Plan I+D+i).
The research work of the last two authors is partially funded
by the MICINN projects OPENMT-2 TIN2009-14675-C03-
01 and KNOW2 TIN2009-14715-C04-01.
References
Alca?zar, Asier. 2006. Towards Linguistically Search-
able Text. In Proceedings of the BIDE 2005, Bilbao,
Basque Country.
Alegria, In?aki, Mikel L. Forcada, and Kepa Sara-
sola, editors. 2009. Proceedings of the SEPLN
2009 Workshop on Information Retrieval and Infor-
mation Extraction for Less Resourced Languages,
Donostia, Basque Country. University of the Basque
Country.
Barro?n-Ceden?o, Alberto, Paolo Rosso, David Pinto,
and Alfons Juan. 2008. On Cross-lingual Plagia-
rism Analysis Using a Statistical Model. In Stein,
Stamatatos, and Koppel, editors, ECAI 2008 Work-
shop on Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 2008), pages 9?13,
Patras, Greece. CEUR-WS.org.
Barro?n-Ceden?o, Alberto, Andreas Eiselt, and Paolo
Rosso. 2009. Monolingual Text Similarity Mea-
sures: A Comparison of Models over Wikipedia Ar-
ticles Revisions. In Sharma, Verma, and Sangal, ed-
itors, ICON 2009, pages 29?38, Hyderabad, India.
Macmillan Publishers.
Broder, Andrei Z. 1997. On the Resemblance and
Containment of Documents. In Compression and
Complexity of Sequences (SEQUENCES?97), pages
21?29. IEEE Computer Society.
Brown, Peter F., Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311.
Ceska, Zdenek, Michal Toman, and Karel Jezek. 2008.
Multilingual Plagiarism Detection. In Proceedings
of the 13th International Conference on Artificial
Intelligence, pages 83?92. Springer Verlag Berlin
Heidelberg.
Clough, Paul and Mark Stevenson. 2010. Developing
a Corpus of Plagiarised Short Answers. Language
Resources and Evaluation: Special Issue on Plagia-
rism and Authorship Analysis.
Clough, Paul, Robert Gaizauskas, and Scott Piao.
2002. Building and Annotating a Corpus for the
Study of Journalistic Text Reuse. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation (LREC 2002), volume V,
pages 1678?1691, Las Palmas, Spain.
Dumais, Susan T., Todd A. Letsche, Michael L.
Littman, and Thomas K. Landauer. 1997. Auto-
matic Cross-Language Retrieval Using Latent Se-
mantic Indexing. In AAAI-97 Spring Symposium
Series: Cross-Language Text and Speech Retrieval,
pages 24?26. Stanford University.
Hoad, Timothy C. and Justin Zobel. 2003. Meth-
ods for Identifying Versioned and Plagiarized Doc-
uments. Journal of the American Society for Infor-
mation Science and Technology, 54(3):203?215.
44
Keselj, Vlado, Fuchun Peng, Nick Cercone, and Calvin
Thomas. 2003. N-gram-based Author Profiles
for Authorship Attribution. In Proceedings of the
Conference Pacific Association for Computational
Linguistics, PACLING?03, pages 255?264, Halifax,
Canada.
Koehn, Philipp, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), demonstra-
tion session, Prague, Czech Republic.
Maurer, Hermann, Frank Kappe, and Bilal Zaka. 2006.
Plagiarism - A Survey. Journal of Universal Com-
puter Science, 12(8):1050?1084.
Mcnamee, Paul and James Mayfield. 2004. Character
N-Gram Tokenization for European Language Text
Retrieval. Information Retrieval, 7(1-2):73?97.
Munteanu, Dragos S., Alexander Fraser, and Daniel
Marcu. 2004. Improved Machine Translation
Performace via Parallel Sentence Extraction from
Comparable Corpora. In Proceedings of the Hu-
man Language Technology and North American As-
sociation for Computational Linguistics Conference
(HLT/NAACL 2004), Boston, MA.
Och, Frank Josef and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
See also http://www.fjoch.com/GIZA++.html.
Pinto, David, Jorge Civera, Alberto Barro?n-Ceden?o,
Alfons Juan, and Paolo Rosso. 2009. A Statistical
Approach to Crosslingual Natural Language Tasks.
Journal of Algorithms, 64(1):51?60.
Potthast, Martin, Benno Stein, and Maik Anderka.
2008. A Wikipedia-Based Multilingual Retrieval
Model. In Macdonald, Ounis, Plachouras, Ruthven,
and White, editors, 30th European Conference on
IR Research, ECIR 2008, Glasgow, volume 4956
LNCS of Lecture Notes in Computer Science, pages
522?530, Berlin Heidelberg New York. Springer.
Potthast, Martin, Benno Stein, Andreas Eiselt, Alberto
Barro?n-Ceden?o, and Paolo Rosso. 2009. Overview
of the 1st International Competition on Plagiarism
Detection. In Stein, Rosso, Stamatatos, Koppel, and
Agirre, editors, SEPLN 2009 Workshop on Uncov-
ering Plagiarism, Authorship, and Social Software
Misuse (PAN 09), pages 1?9, San Sebastian, Spain.
CEUS-WS.org.
Potthast, Martin, Alberto Barro?n-Ceden?o, Benno
Stein, and Paolo Rosso. 2010. Cross-Language Pla-
giarism Detection. Language Resources and Eval-
uation, Special Issue on Plagiarism and Authorship
Analysis.
Pouliquen, Bruno, Ralf Steinberger, and Camelia Ig-
nat. 2003. Automatic Identification of Docu-
ment Translations in Large Multilingual Document
Collections. In Proceedings of the International
Conference on Recent Advances in Natural Lan-
guage Processing (RANLP-2003), pages 401?408,
Borovets, Bulgaria.
Schleimer, Saul, Daniel S. Wilkerson, and Alex Aiken.
2003. Winnowing: Local Algorithms for Document
Fingerprinting. In Proceedings of the 2003 ACM
SIGMOD International Conference on Management
of Data, New York, NY. ACM.
Shivakumar, Narayanan and Hector Garc??a-Molina.
1995. SCAM: A Copy Detection Mechanism for
Digital Documents. In Proceedings of the 2nd An-
nual Conference on the Theory and Practice of Dig-
ital Libraries.
Stein, Benno, Sven Meyer zu Eissen, and Martin Pot-
thast. 2007. Strategies for Retrieving Plagiarized
Documents. In Clarke, Fuhr, Kando, Kraaij, and de
Vries, editors, Proceedings of the 30th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 825?
826, Amsterdam, The Netherlands. ACM.
Steinberger, Ralf, Bruno Pouliquen, and Johan Hag-
man. 2002. Cross-lingual Document Similarity
Calculation Using the Multilingual Thesaurus EU-
ROVOC. Computational Linguistics and Intelligent
Text Processing. Proceedings of the CICLing 2002,
2276:415?424.
Steinberger, Ralf, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Da?niel Varga. 2006. The JRC-Acquis: A multilin-
gual aligned parallel corpus with 20+ languages. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006),
volume 9, Genoa, Italy.
Stolcke, Andreas. 2002. SRILM - An Extensible Lan-
guage Modeling toolkit. In Intl. Conference on Spo-
ken Language Processing, Denver, Colorado.
Wikipedia. 2010a. Basque language. [Online; ac-
cessed 5-February-2010].
Wikipedia. 2010b. Party of European Socialists | Par-
tido Socialista Europeo | Europako Alderdi Sozial-
ista . [Online; accessed 10-February-2010].
45
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 65?69,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Developing an open-source FST grammar for verb chain transfer in a
Spanish-Basque MT System
Aingeru Mayor, Mans Hulden, Gorka Labaka
Ixa Group
University of the Basque Country
aingeru@ehu.es, mhulden@email.arizona.edu, gorka.labaka@ehu.es
Abstract
This paper presents the current status of de-
velopment of a finite state transducer gram-
mar for the verbal-chain transfer module in
Matxin, a Rule Based Machine Translation
system between Spanish and Basque. Due to
the distance between Spanish and Basque, the
verbal-chain transfer is a very complex mod-
ule in the overall system. The grammar is
compiled with foma, an open-source finite-
state toolkit, and yields a translation execution
time of 2000 verb chains/second.
1 Introduction
This paper presents the current status of develop-
ment of an FST (Finite State Transducer) grammar
we have developed for Matxin, a Machine Transla-
tion system between Spanish and Basque.
Basque is a minority language isolate, and it is
likely that an early form of this language was already
present in Western Europe before the arrival of the
Indo-European languages.
Basque is a highly inflected language with free
order of sentence constituents. It is an agglutinative
language, with a rich flexional morphology.
Basque is also a so-called ergative-absolutive lan-
guage where the subjects of intransitive verbs ap-
pear in the absolutive case (which is unmarked),
and where the same case is used for the direct ob-
ject of a transitive verb. The subject of the transi-
tive verb (that is, the agent) is marked differently,
with the ergative case (in Basque by the suffix -k).
The presence of this morpheme also triggers main
and auxiliary verbal agreement. Auxiliary verbs, or
?periphrastic? verbs, which accompany most main
verbs, agree not only with the subject, but also with
the direct object and the indirect object, if present.
Among European languages, this polypersonal sys-
tem (multiple verb agreement) is rare, and found
only in Basque, some Caucasian languages, and
Hungarian.
The fact that Basque is both a morphologically
rich and less-resourced language makes the use of
statistical approaches for Machine Translation dif-
ficult and raises the need to develop a rule-based
architecture which in the future could be combined
with statistical techniques.
The Matxin es-eu (Spanish-Basque) MT engine
is a classic transfer-based system comprising three
main modules: analysis of the Spanish text (based
on FreeLing, (Atserias et al, 2006)), transfer, and
generation of the Basque target text.
In the transfer process, lexical transfer is first
carried out using a bilingual dictionary coded in
the XML format of Apertium dictionary files (.dix)
(Forcada et al, 2009), and compiled, using the FST
library implemented in the Apertium project (the lt-
toolbox library), into a finite-state transducer that
can be processed very quickly.
Following this, structural transfer at the sentence
level is performed, and some information is trans-
ferred from some chunks1 to others while some
chunks may be deleted. Finally, the structural trans-
1A chunk is a non-recursive phrase (noun phrase, preposi-
tional phrase, verbal chain, etc.) which expresses a constituent
(Abney, 1991; Civit, 2003). In our system, chunks play a cru-
cial part in simplifying the translation process, due to the fact
that each module works only at a single level, either inside or
between chunks.
65
fer at the verb chunk level is carried out. The verbal
chunk transfer is a very complex module because of
the nature of Spanish and Basque auxiliary verb con-
structions, and is the main subject of this paper.
This verb chain transfer module is implemented
as a series of ordered replacement rules (Beesley and
Karttunen, 2003) using the foma finite-state toolkit
(Hulden, 2009). In total, the system consists of 166
separate replacement rules that together perform the
verb chunk translation. In practice, the input is given
to the first transducer, after which its output is passed
to the second, and so forth, in a cascade. Each rule in
the system is unambiguous in its output; that is, for
each input in a particular step along the verb chain
transfer, the transducers never produce multiple out-
puts (i.e. the transducers in question are functional).
Some of the rules are joined together with composi-
tion, yielding a total of 55 separate transducers. In
principle, all the rules could be composed together
into one monolithic transducer, but in practice the
size of the composed transducer is too large to be
feasible. The choice to combine some transduc-
ers while leaving others separate is largely a mem-
ory/translation speed tradeoff.
2 Spanish and Basque verb features and
their translation
In the following, we will illustrate some of the main
issues in translating Spanish verb chains to Basque.
Since both languages make frequent use of auxiliary
verb constructions, and since periphrastic verb con-
structions are frequent in Basque, transfer rules can
get quite complex in their design.
For example, in translating the phrase
(Yo) compro (una manzana)
(I) buy (an apple)
[PP1CSN00] [VMIP1S0] [DI0FS0] [NCFS000]
we can translate it using the imperfective partici-
ple form (erosten) of the verb erosi (to buy), and a
transitive auxiliary (dut) which itself contains both
subject agreement information (I: 1st sg.) and num-
ber agreement with the object (an apple: 3rd sg.):
(nik) (sagar bat) erosten dut. The participle carries
information concerning meaning, aspect and tense,
whereas the auxiliaries convey information about ar-
gument structure, tense and mood.
Table 1 illustrates the central idea of the verb
chunk transfer. In the first four examples the form of
the transitive auxiliary changes to express agreement
with different ergative arguments (the subject of the
clause), absolutive arguments (the direct object) and
dative arguments (the indirect object). In the fifth
example the future participle is used. The last ex-
ample shows the translation of a periphrastic con-
struction, in which the the Spanish and the Basque
word orders are completely different: this is re-
flected in the Spanish tengo que-construction (have
to) which appears before the main verb, whereas in
the Basque, the equivalent (behar) appears after the
main verb (erosi).
3 The FST grammar
We carry out the verbal chunk transfer using finite-
state transducers (Alegria et al, 2005). The gram-
mar rules take as input the Spanish verbal chunk,
perform a number of transformations on the input,
and then create and output the verbal chunk for
Basque.
To illustrate the functioning of the grammar, let us
consider the following example sentence in Spanish:
?Un tribunal ha negado los derechos constitu-
cionales a los presos polticos? (A court has denied
constitutional rights to political prisoners). The cor-
rect translation into Basque given by the system for
this example is as follows: Auzitegi batek eskubide
konstituzionalak ukatu dizkie preso politikoei. Fig-
ure 1 shows a detailed overview of how the whole
transfer of the verbal chunk is performed for this par-
ticular example.
First, the input to the grammar is assumed to be a
string containing (separated by the ?&? symbol) the
following information :
? the morphological information (using
EAGLES-style tags Leech and Wilson
(1996)) for all nodes (separated by ?+?
symbol) in the Spanish verbal chunk
(haber[VAIP3S0]+negar[VMP00SM]);
? the morphological information of the subject
([sub3s]), the direct object ([obj3p]) and the
indirect object ([iobj3p]);
? the translation of the main verb in Basque
(ukatu) and information about its transitivity
66
Spanish sentence English Basque translation
(Yo) compro (una manzana) (I) buy (an apple) (Nik) (sagar bat) erosten dut
(Yo) compro (manzanas) (I) buy (apples) (Nik) (sagarrak) erosten ditut
(Tu?) compras (manzanas) (You) buy (apples) (Zuk) (sagarrak) erosten dituzu
(Yo) (te) compro (una manzana) (I) buy (you) (an apple) (Nik) (zuri) (sagar bat) erosten dizut
(Yo) comprare? (una manzana) (I) will buy (an apple) (Nik) (sagar bat) erosiko dut
(Yo) tengo que comprar (manzanas) (I) must buy (apples) (Nik) (sagarrak) erosi behar ditut
Table 1: Examples of translations
  
Un    tribunal     ha negado    los    derechos    constitucionales         a   los    presos    pol?ticosA     court     has denied     (the)   rights           constitutional            to (the)   prisoners  political
uka  +tu              d     +i      +zki   +e    +?
 Subject                Verb                                     Object                                                      Indirect                                                                                                                                           Object
haber[VAIP3S0]+negar[VMP00SM]   &   [sub3s] [obj3p] [iobj3p]   &   ukatu [DIO] 
haber[VAIP3S0]+negar[VMP00SM]  &  [sub3s] [obj3p] [iobj3p]  & ukatu [DIO]SimpleVerb   (main) AspectMain  /  Aux TenseMood Abs Dat Erg
1. Identification      of the schema [ SimpleVerbEsType  -> ...  SimpleVerbEuSchema ]
niega[VMIP3S0]   &   [sub3s] [obj3s] [dat3p]   &  ukatu [DIO] + SimpleVerb   (main)[perfPart]  /  edun(aux) [indPres] [abs3p][dat3p][erg3s]
2. Resolution      of the values Attrib.               ->  Value             || Context                             AspectMain  -> [perfPart]  || ?* VAIP ?* SimpleVerb ?* _Aux  -> edun(aux) || ?* DIO ?* _TenseMood  -> [indPres] || ?* VAIP ?* _Abs  -> [abs3p] || ?* [obj3p] ?* edun(aux) ?* _Dat  -> [dat3p] || ?* [iobj3p] ?* _Erg  -> [erg3s] || ?* V???3S ?* edun(aux) ?* _
3. Elimination of     source information 
ukatu(main)[perfPart]   /  edun(aux) [indPres] [abs3p][dat3p][erg3s]
Input
Output
deny     perf.             ind.    trans.   3rdpl     3rdpl    3rdsg             part.            pres.  aux.   abs.     dat.    erg.           
Figure 1: Example of the transfer of a verbal chunk.
67
([DIO]), indicating a ditransitive construction:
haber[VAIP3S0]+negar[VMP00SM] &
[sub3s][obj3p][iobj3p] & ukatu[DIO]
The grammatical rules are organized into three
groups according to the three main steps defined for
translating verbal chunks:
1. Identification of the Basque verbal chunk
schema corresponding to the source verbal
chunk.
There are twelve rules which perform this task,
each of which corresponds to one of the follow-
ing verbal chunks in Spanish: non-conjugated
verbs, simple non-periphrastic verbs as well
as four different groups reserved for the pe-
riphrastic verbs.
The verbal chunk of the example in figure 1 is
a simple non-periphrastic one, and the rule that
handles this particular case is as follows:
[simpleVerbEsType -> ...
simpleVerbEuSchema]
When this rule matches the input string
representing a simple non-periphrastic ver-
bal chunk (simpleVerbEsType) it adds the
corresponding Basque verbal chunk schema
(simpleVerbEuSchema) to the end of the input
string. simpleVerbEsType is a complex au-
tomaton that has the definition of the Spanish
simple verbs. simpleVerbEuSchema is the type
of the verbal chunk (SimpleVerb) and an au-
tomaton that contains as strings the pattern of
elements (separated by the ?/? symbol) that the
corresponding Basque verb chunk will need to
have (in this case, the main verb and the auxil-
iary verb):
SimpleVerb (main) AspectMain /
Aux TenseMood Abs Dat Erg
2. Resolution of the values for the attributes in the
Basque schema.
A total of 150 replacement rules of this type
have been written in the grammar. Here are
some rules that apply to the above example:
[AspectMain -> [perfPart] || ?* VAIP
?* SimpleVerb ?* ]
[Aux -> edun(aux) || ?* DIO ?* ]
[Abs -> [abs3p] || ?* [obj3p] ?*
edun(aux) ?* ]
3. Elimination of source-language information (4
rules in total).
The output of the grammar for the example is:
ukatu(main)[perfPart] /
edun(aux)[indPres][abs3p][dat3p][erg3s]
The first node has the main verb (ukatu) with
the perfective participle aspect, and the sec-
ond one contains the auxiliary verb (edun) with
all its morphological information: indicative
present and argument structure.
In the output string, each of the elements contains
the information needed by the subsequent syntactic
generation and morphological generation phases.
4 Implementation
When the verbal chunk transfer module was first de-
veloped, there did not exist any efficient open-source
tools for the construction of finite state transduc-
ers. At the time, the XFST-toolkit (Beesley and
Karttunen, 2003) was used to produce the earlier
versions of the module: this included 25 separate
transducers of moderate size, occupying 2,795 kB
in total. The execution speed was roughly 250 verb
chains per second. Since Matxin was designed to be
open source, we built a simple compiler that con-
verted the XFST rules into regular expressions that
could then be applied without FST technology, at the
cost of execution speed. This verbal chunk transfer
module read and applied these regular expressions
at a speed of 50 verbal chunks per second.
In the work presented here, we have reimple-
mented and expanded the original rules written for
XFST with the foma2 toolkit (Hulden, 2009). Af-
ter adapting the grammar and compiling it, the 55
separate transducers occupy 607 kB and operate at
roughly 2,000 complete verb chains per second.3
Passing the strings from one transducer to the next in
the chain of 55 transducers in accomplished by the
depth-first-search transducer chaining functionality
available in the foma API.
2http://foma.sourceforge.net
3On a 2.8MHz Intel Core 2 Duo.
68
References
Abney, S. (1991). Principle-Based Parsing: Com-
putation and Psycholinguistics, chapter Parsing
by Chunks, pages 257?278. Kluwer Academic,
Boston.
Alegria, I., D??az de Ilarraza, A., Labaka, G., Ler-
sundi, M., Mayor, A., and Sarasola, K. (2005).
An FST grammar for verb chain transfer in a
Spanish?Basque MT system. In Finite-State
Methods and Natural Language Processing, vol-
ume 4002, pages 295?296, Germany. Springer
Verlag.
Atserias, J., Casas, B., Comelles, E., Gonza?lez, M.,
Padro?, L., and Padro?, M. (2006). Freeling 1.3:
Syntactic and semantic services in an open-source
NLP library. In Proceedings of LREC, volume 6,
pages 48?55.
Beesley, K. R. and Karttunen, L. (2003). Finite State
Morphology. CSLI Publications, Stanford, CA.
Civit, M. (2003). Criterios de etiquetacio?n y desam-
biguacio?n morfosinta?ctica de corpus en Espan?ol.
PhD thesis, Universidad de Barcelona.
Forcada, M., Bonev, B. I., Ortiz-Rojas, S.,
Pe?rez-Ortiz, J. A., Ram??rez-Sanchez, G.,
Sa?nchez-Mart??nez, F., Armentano-Oller, C.,
Montava, M. A., Tyers, F. M., and Ginest??-
Rosell, M. (2009). Documentation of the
open-source shallow-transfer machine trans-
lation platform Apertium. Technical report,
Departament de Llenguatges i Sistemes In-
formatics. Universitat d?Alacant. Available
at http://xixona.dlsi.ua.es/ fran/apertium2-
documentation.pdf.
Hulden, M. (2009). Foma: a finite-state compiler
and library. In Proceedings of EACL 2009, pages
29?32.
Leech, G. and Wilson, A. (1996). EAGLES rec-
ommendations for the morphosyntactic annota-
tion of corpora. Technical report, EAGLES Expert
Advisory Group on Language Engineering Stan-
dards, Istituto di Linguistica Computazionale,
Pisa, Italy.
69
