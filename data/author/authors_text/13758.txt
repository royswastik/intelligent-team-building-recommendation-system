Coling 2010: Poster Volume, pages 665?673,
Beijing, August 2010
Combining Constituent and Dependency Syntactic Views for  
Chinese Semantic Role Labeling 
Shiqi Li1, Qin Lu2, Tiejun Zhao1, Pengyuan Liu3 and Hanjing Li1 
1School of Computer Science and Technology, 
Harbin Institute of Technology 
{sqli,tjzhao,hjlee}@mtlab.hit.edu.cn 
2Department of Computing, 
The Hong Kong Polytechnic University 
csluqin@comp.polyu.edu.hk 
3Institute of Computational Linguistics, 
Peking University 
liupengyuan@pku.edu.cn 
 
Abstract 
This paper presents a novel feature-
based semantic role labeling (SRL) 
method which uses both constituent 
and dependency syntactic views. Com-
paring to the traditional SRL method 
relying on only one syntactic view, the 
method has a much richer set of syn-
tactic features. First we select several 
important constituent-based and de-
pendency-based features from existing 
studies as basic features. Then, we pro-
pose a statistical method to select dis-
criminative combined features which 
are composed by the basic features. 
SRL is achieved by using the SVM 
classifier with both the basic features 
and the combined features. Experimen-
tal results on Chinese Proposition Bank 
(CPB) show that the method outper-
forms the traditional constituent-based 
or dependency-based SRL methods. 
1 Introduction 
Semantic role labeling (SRL) is a major me-
thod in current semantic analysis which is im-
portant to NLP applications. The SRL task is 
to identify semantic roles (or arguments) of 
each predicate and then label them with their 
functional tags, such as 'Arg0' and 'ArgM' in 
PropBank (Palmer et al, 2005), or 'Agent' and 
'Patient' in FrameNet (Baker et al, 1998).  
The significance of syntactic analysis in 
SRL has been proven by (Gildea and Palmer, 
2002; Punyakanok et al, 2005), and syntactic 
parsing has been applied by almost all current 
studies. In terms of syntactic representations, 
the SRL approaches are mainly divided into 
three categories: constituent-based, chunk-
based and dependency-based. Constituent-
based SRL has been studied intensively with 
satisfactory results. Chunk-based SRL has 
been found to be less effective than the con-
stituent-based by (Punyakanok et al, 2005). In 
recent years, the dependency-based SRL has 
been greatly promoted by the CoNLL shared 
tasks on semantic parsing (Hajic et al, 2009). 
However, there is not much research on com-
bined use of different syntactic views (Pradhan 
et al, 2005), on the feature level of SRL.  
This paper introduces a novel method for 
Chinese SRL utilizing both constituent-based 
and dependency-based features. The method 
takes constituent as the basic unit of argument 
and adopts the labeling of PropBank. It follows 
the prevalent feature-based SRL methods to 
first turn predicate-argument pairs into flat 
structures by well-defined linguistic features, 
and then uses machine learning methods to 
predict the semantic labels. The method also 
involves two classification phases: semantic 
role identification (SRI) and semantic role 
classification (SRC). In addition, a heuristic-
based pruning preprocessing (Xue and Palmer, 
2004) is used to filter out a lot of apparently 
inappropriate constituents at the beginning.  
665
And it has been widely reported that, in fea-
ture-based SRL, the performance can be im-
proved by adding several combined features 
each of which is composed by two single fea-
tures (Xue and Palmer, 2004; Toutanova et al, 
2005; Zhao et al, 2009). Thus, in this work, 
we exploit combined use of both constituent-
based and dependency-based features in addi-
tion to using features of singular types of syn-
tactic view. We propose a statistical method to 
select effective combined features using both 
constituent-based and dependency-based fea-
tures to make full use of two syntactic views. 
2 Related Work 
In recent years, many advances have been 
made on SRL using singular syntactic view, 
such as constituent (Gildea and Jurafsky, 2002; 
Xue and Palmer, 2004; Surdeanu et al, 2007), 
dependency (Hacioglu, 2004; Johansson and 
Nugues, 2008; Zhao et al, 2009), and CCG 
(Chen and Rambow, 2003; Boxwell et al 
2009). However, there are few studies on the 
use of multiple syntactic views. We briefly 
review the relevant studies of SRL using 
multiple syntactic views as follows. 
Pradhan et al (2005) built three semantic 
role labelers using constituent, dependency and 
chunk syntactic views, and then heuristically 
combined them at the output level. The method 
was further improved in Pradhan et al (2008) 
which trains two semantic role labelers for 
constituents and dependency separately, and 
then uses the output of the two systems as ad-
ditional features in another labeler using chunk 
parsing. The result shows an improvement to 
each labeler alone. A possible reason for the 
improvement is that the errors caused by dif-
ferent syntactic parsers are compensated. Yet, 
the features of different syntactic views can 
hardly complement each other in labeling. And 
the complexity of using multiple syntactic 
parsers is extremely high. Hacioglu (2004) 
proposed a SRL method to combine constitu-
ent and dependency syntactic views where the 
dependency parses are ob-tained through auto-
matic mapping of constitu-ent parses. It uses 
the constituent parses to get candidates and 
then, the dependency parses to label them.  
Boxwell et al (2009) proposed a SRL me-
thod using features of three syntactic views: 
CCG, CFG and dependency. It primarily uses 
CCG-based features associated with 4 CFG-
based and 2 dependency-based features. The 
combination of these syntactic views leads to a 
substantial performance improvement. Nguyen 
et al (2009) proposed a composite kernel 
based on both constituent and dependency syn-
tactic views and achieved a significant im-
provement in a relation extraction application. 
3 
Compared to related work, the proposed me-
thod integrates the constituent and dependency 
views in a collaborative manner. First, we de-
fine a basic feature set containing features 
from constituent and dependency syntactic 
views. Then, to make better use of two syntac-
tic views, we introduce a statistical method to 
select effective combined features from the 
basic feature set. Finally we use both the basic 
features and the combined features to identify 
and label arguments. One of the drawbacks of 
the related work is the considerable complexity 
caused by multiple syntactic parsing processes. 
In our method, the cost of syntactic parsing 
will increase only slightly as we derive de-
pendency parsing from constituent parsing us-
ing a constituent-to-dependency converter in-
stead of using an additional dependency parser. 
In our method, the feature set used for SRL 
consists of two parts: the basic feature set and 
the combined feature set built upon the basic 
feature set. The basic feature set can be further 
divided into constituent-based features and 
dependency-based features. Constituent fea-
tures focus on hierarchical relations between 
multi-word constituents whereas dependency 
features focus on dependencies between indi-
vidual words, as shown in Figure 1. Take the 
predicate '??' (increased) as an example, in 
Figure 1(a), the NP constituent '?????' 
(China's position) is labeled as 'Arg0'. The ar-
gument and the predicate are connected by the 
path of node types: 'NP-IP-VP-VP'. But in 
Figure 1(b), the individual word '??' (posi-
tion) is labeled as 'Arg0'. And the connection 
between the argument and the predicate is only 
one edge with the relation 'nsubj', which is 
more explicit than the path in the constituent 
structure. So the two syntactic views can com-
plement each other on different linguistic units. 
Design Principle and Basic Features 
666
3.1 Constituent-Based Features 
As a prevalent syntactic feature set for SRL, 
constituent-based features have been 
extensively studied by many researchers. In 
this work, we simply take 26 constituent-based 
features tested by existing studies, and add 8 
new features define by us. Firstly, the 26 
constituent-based features used by others are: 
y The seven "standard" features: predicate (c1), 
path (c2), phrase type (c3), position (c4), 
voice (c5), head word (c6) and predicate 
subcategorization (c7) features proposed by 
(Gildea and Jurafsky, 2002). 
y Syntactic frame (c8) feature from (Xue and 
Palmer, 2004). 
y Head word POS (c9), partial path (c10), 
first/last word in constituent (c11/c12), 
first/last POS in constituent (c13/c14), 
left/right sibling constituent (c15/c16), 
left/right sibling head (c17/c18), left/right 
sibling POS (c19/c20), constituent tree dis-
tance (c21) and temporal cue words (c22) 
features from (Pradhan et al, 2004). 
y Predicate POS (c23), argument's parent 
constituent (c24), argument's parent con-
stituent head (c25) and argument's parent 
constituent POS (c26) inspired by (Pradhan 
et al, 2004). 
Secondly, the 8 new features that we define 
are (we take the 'Arg0' node in Figure 1(a) as 
the example to illustrate them): 
y Locational cue words (c27): a binary feature 
indicating whether the constituent contains 
location cue words, similar to the temporal 
cue words (c22). This feature is defined to 
distinguish the arguments with the 'ArgM-
LOC' type from others. 
y POS pattern of argument's children (c28): 
the left-to-right chain of the POS tags of the 
argument's children, e.g. 'NR-DEG-NN'. 
y Phrase type pattern of argument's children 
(c29): the left-to-right chain of the phrase 
type labels of the argument's children, simi-
lar with the POS pattern of argument's chil-
dren (c28), e.g. 'DNP-NP'. 
y Type of LCA and left child (c30): The phrase 
type of the Lowest Common Ancestor (LCA) 
combined with its left child, e.g. 'IP-NP'. 
y Type of LCA and right child (c31): The 
phrase type of the LCA combined with its 
right child, e.g. 'IP-VP'. 
Three features: bag of words of path (c32), 
bag of words of POS pattern (c33) and bag of 
words of type pattern (c34), for generalizing 
three sparse features: path (c2), POS pattern of 
argument's children (c28) and phrase type pat-
tern of argument's children (c29) by the bag-
of-words representation. 
3.2 Dependency-Based Features 
The dependency parse can effectively repre-
sent the head-dependent relationship between 
words, yet, it lacks constituent information. If 
we want to label constituents using depend-
ency-based features, we should firstly map 
each constituent to one or more appropriate 
words in the dependency tree. In this paper, we 
use the head word of a constituent to represent 
the constituent in the dependency parses.  
The selection method of dependency-based 
features is similar to the method of constitu-
ent-based features. The 35 selected dependen-
cy-based features include: 
y Predicate/Argument relation type (d1/d2), 
relation path (d3), POS pattern of predi-
cate?s children (d4) and relation pattern of 
predicate?s children (d5) features from (Ha-
cioglu, 2004). 
y Child relation set (d6), child POS set (d7), 
predicate/argument parent word (d8/d9), 
predicate/argument parent POS (d10/d11), 
left/right word (d12/d13), left/right POS 
(d14/d15), left/right relation (d16/d17), 
left/right sibling word (d18/d19), left/right 
sibling POS (d20/d21) and left/right sibling 
relation (d22/d23) features as described in 
(Johansson and Nugues, 2008). 
y Dep-exists (d24) and dep-type (d25) features 
from (Boxwell et al, 2009). 
y POS path (d26), POS path length (d27), REL 
path length (d28) from (Che et al, 2008). 
y High/low support verb (d29/d30), high/low 
support noun (d31/d32) features from (Zhao 
et al, 2009). 
y  LCA?s word/POS/relation (d33/d34/d35) 
inspired by (Toutanova et al, 2005). 
To maintain the consistency between two 
syntactic views, the dependency parses are 
generated by a constituent-to-dependency con-
verter (Marneffe et al, 2006), which is suitable 
for semantic analysis as it retrieves the seman-
tic head rather than the general syntactic head, 
using a set of modified Bikel's head rules.  
667
4 Selection of Combined Features 
The combined features, each of which consists 
of two different basic features, have proven to 
be positive for SRL. Several combined features 
have been widely used in SRL, such as 'predi-
cate+head word' and 'position+voice'. But to 
our knowledge, there is no prior report about 
the selection method of combined features for 
SRL. The common entropy-based criteria are 
invalid here because the combined features 
always take lots of distinct values. And the 
greedy method is too complicated to be practi-
cal due to the large number of combinations. 
In this paper, we define two statistical crite-
ria to efficiently estimate the classification per-
formance of each combined feature on the cor-
pus. Inspired by Fisher Linear Discriminant 
Analysis (FLDA) (Fisher, 1938) in which the 
separation of two classes is defined as the ratio 
of the variance between the classes to the vari-
ance within the classes, namely larger ratio can 
lead to better separation between two classes, 
and the discriminant plane can be achieved by 
maximizing the separation. Therefore, in this 
paper, we adopt the ratio of inter-class distance 
to intra-class distance to measure to what ex-
tent a combined feature can partition the data.  
Initially, the feature set contains only the N  
basic features. We construct one combined 
feature abf  at each iteration by combining two 
basic features af  and bf , where , [1, ]a b N?  
and a b? . We push abf  into the feature set and 
take it as the 1N + th feature. Then, all the 
training instances are represented by feature 
vectors using the new feature set, and we then 
quantize the feature vectors of positive and 
negative data orderly to keep their intrinsic 
statistical difference. If the training dataset is 
denoted as :{ , }pos negD D D , then the separation 
criterion, namely the ratio of inter-class to in-
tra-class distance for feature if  can be given as 
( ) ( , )
( , )
i
fi
f pos neg
i
pos neg
InterDist D D
g f
IntraDist D D
=
 
(1)
where the inter-class and the intra-class dis-
tance between posD  and negD  for feature if  are 
specified by (2) and (3), respectively. 
( )2( , ) ( ) ( )
i i if pos neg f pos f neg
InterDist D D Mean D Mean D= ? (2)
2 2( , ) ( ) ( )f fi iif pos neg pos negIntraDist D D S D S D= + (3)
( )
if
Mean D  in (2) and ( )
if
S D  in (3) repre-
sents the sample mean and the corresponding 
sample standard deviation of feature if  in 
dataset D  as given in (4) and (5). 
( )
( )
| |i
x D
f
x i
Mean D
D
?=
?
, [1, 1]i N? +  (4)
( )2( ) ( )
( )
i
i
f
x D
f
Mean D x i
S D
N
?
?
=
?
, [1, 1]i N? +
(5)
Essentially, the inter-class distance reflects 
the distance between the center of positive da-
taset and the center of negative dataset, and the 
intra-class distance indicates the intensity of all 
instances relative to the corresponding center. 
Therefore, larger ratio will lead to a better par-
tition for a feature, as has been pointed out by 
FLDA. In order to compare the ratio between 
different combined features, we further stan-
dardize the value of ( )ig f  by computing its z-
score ( )iZ f  which indicates how many stan-
dard deviations between a sample and its mean, 
as given in (6). 
( ) ( )
( ) i ii
G
g f g f
Z f
S
?=  (6)
where ( )ig f  represents the sample mean as 
given in (7), and GS  represents the sample 
standard deviation of the sequence ( )ig f  
where i  ranges from 1 to N+1 as given in (8).  
1
1
( )
( )
1
N
i
i
i
g f
g f
N
+
== +
?
, [1, 1]i N? +  
(7)
1
2
1
( ( ) ( ))
N
i i
i
G
g f g f
S
N
+
=
?
=
?
, [1, 1]i N? +  
(8)
After figuring out the ( )aZ f  and ( )bZ f  for 
the basic feature af  and bf , and ( )abZ f  for the 
combined feature abf  by (6), we define the 
other criterion, namely the improvement 
( )abI f  of the combined feature, as the smaller 
difference between the z-score of the combined 
668
feature and its two corresponding basic fea-
tures as given in (9). 
( )( ) ( ) Max ( ),  ( )ab ab a bI f Z f Z f Z f= ?  (9)
Finally, the combined feature with a nega-
tive ( )abI f  value is eliminated. Then, we will 
rank the combined features in terms of their z-
score, and use the top N of them for later clas-
sification. The selection method based on the 
two criteria can effectively filter out combined 
features whose means have no significant dif-
ference between positive and negative data, 
and hence retain the potentially useful com-
bined features for the separation. Meanwhile, it 
has a relatively fast speed when dealing with a 
large number of features in comparison to the 
greedy method due to its simplicity. 
5 Performance Evaluation 
5.1 Experimental Setting 
In our experiments, we adopt the three-step 
strategy proposed by (Xue and Palmer, 2004). 
First, argument candidates are generated from 
the input constituent parse tree using the preva-
lent heuristic-based pruning algorithm in (Xue 
and Palmer, 2004). Then, each predicate-
argument pair is converted to a flat feature 
structure by which the similarity between two 
instances can be easily measured. Finally we 
employ the Support Vector Machines (SVM) 
classifier to identify and classify the arguments. 
It is noteworthy that we use the same basic 
features, but different combined features for 
the identification and classification of argu-
ments. We present the result comparison be-
tween using gold-standard parsing and auto-
matic parsing, and also offer an analysis of the 
contribution of the combined features.  
To evaluate the proposed method and com-
pare it with others, we use the most commonly 
used corpus in Chinese SRL, Chinese Proposi-
tion Bank (CPB) version 1.0, as the dataset. 
The CPB corpus contains 760 documents, 
10,364 sentences, 37,183 target predicates and 
88,134 arguments. In this paper, we focus on 
six main types of semantic roles: Arg0, Arg1, 
Arg2, ArgM-ADV, ArgM-LOC and ArgM-
TMP. The number of semantic roles of the six 
types accounted for 95% of all the semantic 
roles in CPB. For SRC, we use the one-versus-
all approach, in which six SVMs will be 
trained to separate each semantic type from the 
remaining types. We divide the corpus into 
three parts: the first 99 documents 
(chtb_001.fid to chtb_099.fid) serve as the test 
data, the last 32 documents (chtb_900.fid to 
chtb_931.fid) serve as the development data 
and the left 629 documents (chtb_100.fid to 
chtb_899.fid) serve as the training data.  
We use the SVM-Light Toolkit version 6.02 
(Joachims, 1999) for the implementation of 
SVM, and use the Stanford Parser version 1.6 
(Levy and Manning, 2003) as the constituent 
parser and the constituent-to-dependency con-
verter. In classifications, we employ the linear 
kernel for SVM and set the regularization pa-
rameter to the default value which is the recip-
rocal of the average Euclidean norm of training 
data. The performance metrics are: accuracy 
(A), precision (P), recall (R) and F-score (F). 
5.2 Combined Feature Selection 
First, we select the combined features for clas-
sifications of SRI and SRC using the method 
described in Section 4 on the training data with 
gold-standard parse trees. Due to the limit of 
this paper, we only list the top-10 combined 
features for SRI and SRC for the 6 different 
types, as shown in Table 1 in which each com-
bined feature is expressed by the IDs of its two 
basic features with a plus sign between them. 
Rank SRI ARG0 ARG1 ARG2 ADV LOC TMP
1 c1+c6 c1+c6 c1+c6 c1+c6 c1+c6 c5+c27 c1+c6 
2 c1+d3 c32+c30 c30+d31 c1+d1 c30+d27 c9+d17 c22+c27
3 d25+d14 c7+c6 c30+d32 c1+c7 c30+d28 c9+d13 c7+c6 
4 c4+d25 c1+c2 c5+c30 c7+c6 c1+c11 c9+c2 d26+d27
5 d25+d22 c1+c12 c30+d24 c1+c5 c24+d33 c23+c27 d26+d28
6 d25+d20 c23+c6 c30+c21 c1+c23 c30+d25 c9+c20 c23+d26
7 d25+d21 c1+c3 c5+c4 c23+c6 c24+d9 c14+c32 c5+d26 
8 d25+d18 c10+d35 c1+c10 c1+c3 c27+c2 c14+c10 d26+d31
9 d25+d19 c10+d1 c30+d10 c5+c6 c22+c2 c9+c26 d26+d32
10 d25+d35 c10+d28 c4+c6 c1+d5 c24+d13 c14+c2 c23+c6 
Table 1. Top-10 combined features for SRI and 
SRC ranked by z-score 
Table 1 shows that the commonly used 
combined features, such as 'predicate+head 
word' (c1+c6) and 'position+voice' (c4+c5) 
proposed by (Xue and Palmer, 2004) are also 
included. In particular, the 'predicate+head 
word' feature takes first place in all semantic 
669
categories except LOC, in which the combina-
tion of the new feature 'locational cue words' 
(c27) and the 'voice (c5)' feature performs the 
best. The results also show that the most fre-
quently occurred basic features in the com-
bined set are 'predicate' (c1), 'head word' (c6), 
'type of LCA and left child' (c30), 'dep-type' 
(d25) and 'POS path' (d26). These basic fea-
tures should be more discriminative when 
combined with others. Additionally, we find 
some other latent effective combined features, 
such as 'predicate subcategorization+head 
word' (c7+c6), 'predicate POS+head word' 
(c23+c6) and 'predicate+phrase type' (c1+c3), 
whose performance will be further validated 
and analyzed later in this section. It is obvious 
that the obtained combined features for SRI 
and SRC are different, and the obtained com-
bined features for each type are also different 
as our selection method is based on positive 
and negative data which are completely differ-
ent for each argument type. In SRI phase, we 
will use the combined features for all the six 
semantic types (after removing duplicates). 
Then, we evaluate the performance of SRL 
based on the top-N combined features. The 
preliminary evaluation on the development set 
suggests that the performance becomes stable 
when N exceeds 20. Therefore, we vary the 
value of N to 5, 10 and 20 in the experiments 
to evaluate the performance of combined fea-
tures. Corresponding to the three different val-
ues of N, we finally obtained 28, 60 and 114 
combined features for the SRL, respectively. 
5.3 SRL Using Gold Parses  
To illustrate each component of the method, 
we constructed 6 SRL systems using 6 differ-
ent feature sets: 'Constituent Only' (CO) - uses 
the constituent-based features, as presented in 
Section 3.1; 'Dependency Only' (DO) - uses 
the dependency-based features, as presented in 
Section 3.2; 'CD' - uses both the constituent-
based features and the dependency-based fea-
tures, but no combined features; 'CD+Top5' - 
obtained by adding the top-5 combined fea-
tures to the 'CD' system; and similarly for the 
'CD+Top10' and the 'CD+Top20' systems. And 
'CO' serves as the baseline in our experiments. 
First, we evaluate the performance of SRI 
using the held-out test set with gold-standard 
constituent parse trees. The corresponding de-
pendency parse trees are automatically gener-
ated by the constituent-to-dependency con-
verter included in the Stanford Parser. The 
testing results of the six systems on the SRI 
phase are shown in Table 2. 
System A (%) P (%) R (%) F (%)
CO 97.87 97.04 97.30 97.17
DO 92.76 92.90 84.19 88.33
CD 97.98 97.44 97.25 97.34
CD+Top5 98.12 97.56 97.58 97.57
CD+Top10 98.15 97.61 97.62 97.61
CD+Top20 98.18 97.68 97.64 97.66
Table 2. Results of SRI using gold parses 
It can be seen from Table 2 that 'CD' and 
'CD+Top20' give only slightly improvement 
over 'CO' by less than 1% point. In other words, 
feature combinations do not seem to be very 
effective for SRI. Then we label all recognized 
constituents in the SRI phase with one of the 
six semantic role types. Table 3 displays the F-
score of each semantic type and the overall 
SRC on the test set with gold-standard parses. 
System Arg0 Arg1 Arg2 ADV LOC TMP ALL
CO 92.40 90.57 59.98 96.25 86.80 98.14 91.23 
DO 90.70 88.22 56.95 94.54 81.23 97.37 89.14 
CD 92.85 91.29 63.35 96.55 87.55 98.32 91.86 
CD+Top5 93.96 92.79 73.48 97.13 88.63 98.31 93.22*1
CD+Top10 94.15 93.23 74.18 97.42 87.17 98.57 93.41*
CD+Top20 94.10 93.19 75.13 97.23 88.05 98.48 93.46*
Table 3. Results of SRC using gold parses  
Table 3 shows that the proposed method 
performs much better in SRC. It improves the 
constituent-based method by more than 2% in 
SRC. The effectiveness of combined features 
can also be clearly seen because the overall F-
scores of the three systems using combined 
features all exceed 93%, significant greater 
than the systems using singular features. The 
improvement is noticeable for all semantic role 
types except the 'TMP' type. It means that the 
dependency parses cannot provide additional 
information to the labeling of this type. The 
results of Table 2 and Table 3 together show 
                                                 
1 The F-score value with an asterisk (*) indicates 
that there is a statistically significant difference 
between this system and the baseline ('CO') using 
the chi-square test (p<0.05). 
670
that our method using combined features can 
effectively improve the performance of SRL 
on the SRC phases, when using gold parses.  
5.4 SRL Using Automatic Parses  
To measure the performance of the algorithm 
in practical conditions, we replicate the above 
experiments using Stanford Parser on the raw 
texts of the test set, without segmentation or 
POS tagging. The dependency parses are also 
generated from the automatic constituent 
parses, as described in Section 5.3. The results 
are shown in Table 4. 
System A (%) P (%) R (%) F (%)
CO 71.54 68.72 70.62 69.66
DO 68.86 65.06 60.68 62.79
CD 73.53 70.63 72.75 71.67*
CD+Top5  73.62 70.69 72.98 71.82*
CD+Top10 73.65 70.71 73.08 71.88*
CD+Top20 73.67 70.70 73.16 71.91*
Table 4. Results of SRI using automatic parses 
Table 4 shows that the proposed method is 
also effective when using automatic parses 
despite the dramatic decrease in F-scores in 
comparison to using gold-standard parses. The 
decline is mainly caused by the heuristic-based 
pruning strategy in which a number of real ar-
guments are pruned when using the constituent 
parses with errors. Further analysis shows that, 
in SRI using gold parses, the ratio of incor-
rectly pruned arguments to the total is less than 
2%, but the ratio jumps to 17% when using 
automatic parses. Next, on the basis of the SRI 
results, we test the performance of SRC using 
the automatic parses, as shown in Table 5. 
System Arg0 Arg1 Arg2 ADV LOC TMP ALL
CO 89.20 88.90 54.47 93.93 81.80 94.38 88.24
DO 88.79 89.32 50.21 91.27 78.26 93.86 87.63
CD 89.75 89.87 57.71 95.28 84.22 94.71 89.16*
CD+Top5 90.75 90.97 65.64 95.53 84.45 94.45 90.16*
CD+Top10 90.96 91.37 67.25 95.31 84.49 94.61 90.45*
CD+Top20 90.94 91.29 67.42 95.22 84.39 94.65 90.42*
Table 5. Results of SRC using auto parses 
Table 5 shows only a slight decline in com-
parison with the result of using gold-standard 
parses, and it maintains the same trend of per-
formance for each semantic role in the Table 3, 
which proves the validity of the proposed me-
thod when using automatic parses. Table 6 
shows the F-score of the overall SRL on both 
the gold-standard and the automatic parse data. 
System Gold Parse (F%) Auto Parse (F%)
CO 89.29 63.13 
DO 82.69 60.34 
CD 90.01 65.56* 
CD+Top5 91.47* 66.37* 
CD+Top10 91.68* 66.61* 
CD+Top20 91.76* 66.61* 
Table 6. Results of overall SRL 
Table 6 shows that the F-score of the 
'CD+Top20' surpasses that of the 'CO' system 
by more than 2% on the gold parses, and more 
than 3% on the automatic parse. In other words, 
the method using constituent and dependency 
syntactic views performs even more effective 
for the automatic parses. The last three rows of 
Table 6 shows that the top-10 combined fea-
tures perform better than the top-5 features by 
adding 32 more features, but the top-20 com-
bined features obtain similar results to the top-
10 features by adding 54 more features. It sug-
gests that only several salient combined fea-
tures can actually improve the performance.  
5.5 Combined Feature Performance 
To evaluate the performance of each combined 
feature to identify the salient combined fea-
tures for SRL, we rank the 60 combined fea-
tures used by the 'CD+Top10' system on the 
test data with gold-standard parses, according 
to the F-score improvement achieved by each 
combined feature. Here we list the top 20 of 
them which are shown in Table 7.  
Rank Feature ? F(%) Rank Feature ? F(%)
1 c1+c6 0.611 11 c10+d1 0.413
2 c1+c10 0.593 12 c5+d26 0.404
3 c4+c6 0.557 13 c24+d9 0.395
4 c9+c20 0.503 14 d25+d35 0.395
5 c23+c6 0.494 15 c30+d24 0.377
6 c1+c3 0.458 16 c9+c26 0.377
7 c9+d13 0.449 17 c10+d28 0.368
8 c14+c10 0.431 18 c30+d29 0.365
9 c1+c5 0.422 19 c30+d30 0.361
10 c24+d33 0.413 20 c7+c6 0.361
Table 7. Top-20 combined features 
As can be seen from Table 7, a half of com-
bined features are composed by constituent 
671
features only, and the other half contain at least 
one dependency-based feature. This indicates 
that dependency features can be helpful to con-
struct combined features for SRL. Through 
analyzing the performance of each combined 
features, we have obtained some new and ef-
fective combined features which were not rec-
ognized before, such as 'predicate+partial 
path' (c1+c10), 'position+head word' (c4+c6), 
'Head word POS+right sibling POS' (c9+c20). 
Observation from these combined features 
suggests that not all combined features are 
composed by two significant basic features. 
Some not significant ones, such as 'partial 
path' (c10) and 'Head word POS' (c9) can also 
produce salient combined features. 
Furthermore, we find that the relative order 
of the combined features in Table 7 is not ex-
actly consistent with their orders in Table 1. 
The inconsistency indicates that the estimation 
criteria used for combined features selection is 
not perfect. In estimation, the effect of com-
bined features is evaluated simply based on the 
distance between the positive and the negative 
dataset by considering the efficiency. But in 
practice, the effects of them are determined 
through one-by-one classification. 
5.6 Comparison to Other Work 
Finally, we compare the proposed method with 
other four representative Chinese SRL systems. 
First, the 'Xue1' system (Xue and Palmer, 2005) 
is a typical feature-based system using 9 basic 
features, 2 combined features and the Maxi-
mum Entropy (ME) classifier. Second, the 'Liu' 
system (Liu et al 2007) which uses 19 basic 
features, 10 combined features and also the 
ME classifier. Third, the 'Che' (Che, 2008) sys-
tem use a hybrid convolution tree kernel to 
directly measure the similarity between two 
constituent structures. Fourth, the 'Xue2' sys-
tem described in (Xue, 2008), which is similar 
to 'Xue1' on basic framework, but using a new 
feature set. The 'Xue2' system evaluates the 
SRL of the verbal predicates and the nominal-
ized predicates separately, and offers no con-
solidated evaluation in (Xue, 2008). So in the 
comparison, we refer to its performance on the 
verbal predicates and the nominalized predi-
cates as 'Xue21' and 'Xue22'. 
All the four systems mentioned above use 
the constituent as the labeling unit and use the 
CPB corpus as the data set, the same as our 
method. And we use the same training and test 
data splits as in the 'Xue1' and 'Che' systems. 
Table 8 shows the comparison results in terms 
of F-score on both gold parses and auto parses.  
System Gold Parse (F%) Auto Parse (F%)
Xue22 69.6 57.3 
Xue1 91.3 61.3 
Liu 91.31 ? 
Che 91.67 65.42 
Ours 91.76 66.61 
Xue21 92.0 66.8 
Table 8. Comparison to other work 
Table 8 shows that our method performs 
better than the 'Xue1', 'Liu' and 'Che' systems 
on both gold parses and automatic parses. It is 
only slightly worse than the 'Xue21', namely the 
verbal predicates part of the 'Xue2' system. But 
for the other part of the 'Xue2' system for the 
nominalized predicates, namely the 'Xue22', our 
method performs much better than it. The re-
sults further verify the validity of the method. 
6 Conclusions 
This paper presents a novel feature-based SRL 
approach for Chinese. Compared to the tradi-
tional feature-based methods, the method can 
effectively integrate the constituent and the 
dependency syntactic views at the feature level. 
The method provides an effective way to con-
nect two syntactic views by a statistical selec-
tion method of combined features to substan-
tially improve the feature-based SRL method. 
The complexity of the method will not increase 
significantly compared to the method using 
one syntactic view as we use a constituent-to-
dependency conversion rather than additional 
dependency parsing. The effectiveness of the 
method has been proven by the experiments on 
CPB using SVM classifier with linear kernel.  
 
Acknowledgments 
This work is supported by the Key Program of 
National Natural Science Foundation of China 
under Grant No. 60736014, the Key Project of 
the National High Technology Research and 
Development Program of China under Grant 
No. 2006AA010108, and the Hong Kong Poly-
technic University under Grant No. G-U297 
and G-U596.  
672
References  
Collins F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. 
Proceedings of Coling-ACL-1998. 
Stephen A. Boxwell, Dennis Mehay, and Chris 
Brew. 2009. Brutus: A Semantic Role Labeling 
System Incorporating CCG, CFG, and Depend-
ency Features. Proceedings of ACL-2009. 
Wanxiang Che. 2008. Kernel-based Semantic Role 
Labeling. Ph.D. Thesis. Harbin Institute of 
Technology, Harbin, China. 
John Chen and Owen Rambow. 2003. Use of Deep 
Linguistic Features for the Recognition and La-
beling of Semantic Arguments. Proceedings of 
EMNLP-2003. 
Weiwei Ding and Baobao Chang. 2008. Improving 
Chinese Semantic Role Classification with Hier-
archical Feature Selection Strategy. Proceedings 
of EMNLP-2008. 
Ronald A. Fisher. 1938. The Statistical Utilization 
of Multiple Measurements. Annals of Eugenics, 
8:376-386. 
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic Labeling of Semantic Roles. Computa-
tional Linguistics, 28(3):245-288. 
Daniel Gildea and Martha Palmer. 2002. The Ne-
cessity of Syntactic Parsing for Predicate Argu-
ment Recognition. Proceedings of ACL-2002. 
Kadri Hacioglu. 2004. Semantic Role Labeling 
Using Dependency Trees. Proceedings of COL-
ING-2004. 
Jan Hajic, Massimiliano Ciaramita, Richard Jo-
hansson, et al The CoNLL-2009 Shared Task: 
Syntactic and Semantic Dependencies in Multi-
ple Languages. Proceedings of CoNLL-2009. 
Thorsten Joachims. 1999. Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods. 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed), MIT Press. 
Richard Johansson and Pierre Nugues. 2008. De-
pendency-based Semantic Role Labeling of 
PropBank. Proceedings of EMNLP-2008. 
Roger Levy and Christopher D. Manning. 2003. Is 
it harder to parse Chinese, or the Chinese Tree-
bank. Proceedings of ACL-2003. 
Huaijun Liu, Wanxiang Che, and Ting Liu. 2007. 
Feature Engineering for Chinese Semantic Role 
Labeling. Journal of Chinese Information Proc-
essing, 21(2):79-85. 
Marie-Catherine de Marneffe, Bill MacCartney, 
and Christopher D. Manning. 2006. Generating 
Typed Dependency Parses from Phrase Structure 
Parses. Proceedings of LREC-2006. 
Truc-Vien T. Nguyen, Alessandro Moschitti, and 
Giuseppe Riccardi. 2009. Convolution Kernels 
on Constituent, Dependency and Sequential 
Structures for Relation Extraction. Proceedings 
of EMNLP-2009. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguis-
tics, 31(1):71-106 
Sameer Pradhan, Wayne Waed, Kadri Haciolgu, 
and James H. Martin. 2004. Shallow Semantic 
Parsing using Support Vector Machines. Pro-
ceedings of HLT/NAACL-2004 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, 
James H. Martin, and Daniel Jurafsky. 2005. 
Semantic Role Labeling Using Different Syntac-
tic Views. Proceedings of ACL-2005. 
Sameer Pradhan, Wayne Ward, and James H. Mar-
tin. 2008. Towards Robust Semantic Role Label-
ing. Computational Linguistics, 34(2): 289-310. 
Vasin Punyakanok, Dan Roth, Wentau Yih. 2005. 
The Necessity of Syntactic Parsing for Semantic 
Role Labeling. Proceedings of IJCAI-2005. 
Mihai Surdeanu, Lluis Marquez, Xavier Carreras, 
and Pere R. Comas. 2007. Combination Strate-
gies for Semantic Role Labeling. Journal of 
Artificial Intelligence Research, 29:105-151. 
Kristina Toutanova, Aria Haghighi, and Christo-
pher D. Manning. 2005. Joint learning improves 
semantic role labeling. Proceedings of ACL-
2005. 
Nianwen Xue and Martha Palmer. 2004. Calibrat-
ing Features for Semantic Role Labeling. Pro-
ceedings of EMNLP-2004. 
Nianwen Xue and Martha Palmer. 2005 Automatic 
semantic role labeling for Chinese verbs. Pro-
ceedings of IJCAI-2005. 
Nianwen Xue. 2008. Labeling Chinese Predicates 
with Semantic Roles. Computational Linguistics, 
34(2):225-255. 
Hai Zhao, Wenliang Chen, and Chunyu Kit. 2009. 
Semantic Dependency Parsing of NomBank and 
PropBank: An Efficient Integrated Approach via 
a Large-scale Feature Selection. Proceedings of 
EMNLP-2009. 
673
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 304?307,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
PKU_HIT: An Event Detection System Based on Instances Expansion 
and Rich Syntactic Features 
 
 
Shiqi Li1, Pengyuan Liu2, Tiejun Zhao1, Qin Lu3 and Hanjing Li1 
1School of Computer Science and Technology, 
Harbin Institute of Technology, Harbin 150001, China 
{sqli,tjzhao,hjlee}@mtlab.hit.edu.cn 
2Institute of Computational Linguistics, 
Peking University, Beijing 100871, China 
liupengyuan@pku.edu.cn 
3Department of Computing, 
The Hong Kong Polytechnic University, Hong Kong, China 
csluqin@comp.polyu.edu.hk 
 
  
 
Abstract 
This paper describes the PKU_HIT system 
on event detection in the SemEval-2010 
Task. We construct three modules for the 
three sub-tasks of this evaluation. For 
target verb WSD, we build a Na?ve 
Bayesian classifier which uses additional 
training instances expanded from an 
untagged Chinese corpus automatically. 
For sentence SRL and event detection, we 
use a feature-based machine learning 
method which makes combined use of 
both constituent-based and dependency-
based features. Experimental results show 
that the Macro Accuracy of the WSD 
module reaches 83.81% and F-Score of 
the SRL module is 55.71%. 
1 Introduction 
In this paper, we describe the system submitted 
to the SemEval-2010 Task 11 on event detection 
in Chinese news sentences (Zhou, 2010). The 
objective of the task is to detect and analyze 
basic event contents in Chinese news sentences, 
similar to the frame semantic structure extraction 
task in SemEval-2007. However, this task is a 
more complex as it involves three interrelated 
subtasks: (1) target verb word sense 
disambiguation (WSD), (2) sentence semantic 
role labeling (SRL) and (3) event detection (ED).  
Therefore, the architecture of the system that 
we develop for the task consists of three modules: 
WSD, SRL and ED. First, the WSD module is to 
recognize key verbs or verb phrases which 
describe the basic event in a sentence, and then 
select an appropriate situation description 
formula for the recognized key verbs (or verb 
phrases); Then, the SRL module anchors the 
arguments to suitable constituents in the sentence, 
and then label each argument with three 
functional tags, namely constituent type tag, 
semantic role tags and event role tag. Finally, in 
the ED module, complete situation description of 
the sentence can be achieved by combining the 
results of the WSD module and the SRL module. 
For the WSD module, we consider the subtask 
as a general WSD problem. First of all, we 
automatically extract many instances from an 
untagged Chinese corpus using a heuristic rule 
inspired by Yarowsky (1993). Then we train a 
Na?ve Bayesian (NB) classifier based on both the 
extracted instances and the official training data. 
We then use the NB classifier to predict situation 
the description formula and natural explanation 
of each target verb in testing data. 
For the SRL module, we use a rich syntactic 
feature-based learning method. As the state-of-
the-art method in the field of SRL, feature-based 
method represents a predicate-argument structure 
(PAS) by a flat vector using a set of linguistic 
features. Then PAS can be directly classified by 
machine learning algorithms based on the 
corresponding vectors. In feature-based SRL, the 
304
significance of syntactic information in SRL was 
proven by (Punyakanok et al, 2005). In our 
method, we exploit a rich set of syntactic 
features from two syntactic views: constituent 
and dependency. As the two syntactic views 
focus on different syntactic elements, 
constituent-based features and dependency-based 
features can complement each other in SRL to 
some extent. Finally, the ED module can be 
readily implemented by combining the SRL and 
the WSD result using some simply rules.  
2 System Description 
2.1 Target Verb WSD 
The WSD module is based on a simple heuristic 
rule by which we can extract sense-labeled 
instances automatically. The heuristic rule 
assumes that one sense per 3-gram which is 
proposed by us initially through investigating a 
Chinese sense-tagged corpus STC (Wu et al, 
2006). The assumption is similar to the 
celebrated one sense per collocation supposition 
(Yarowsky, 1993), whereas ours has more 
expansibility. STC is an ongoing project which is 
to build a sense-tagged corpus containing sense-
tagged 1, 2 and 3 months of People?s Daily 2000 
now. According to our investigation, given a 
specific 3-gram (w-1wverbw1) to any target verb, 
on average, we expect to see the same label 
95.4% of the time. Based on this observation, we 
consider one sense per 3-gram (w-1wverbw1) or at 
least we can extract instances with this pattern. 
For all the 27 multiple-sense target verbs in 
the official training data, we found their 3-gram 
(w-1wverbw1) and extracted the instances with the 
same 3-gram from a Chinese monolingual corpus 
? the 2001 People?s Daily (about 116M bytes). 
We consider the same 3-gram instances should 
have the same label. Then an additional sense-
labeled training corpus is built automatically in 
expectation of having 95.4% precision at most. 
And this corpus has 2145 instances in total 
(official training data have 4608 instances). 
We build four systems to investigate the effect 
of our instances expansion using the Na?ve 
Bayesian classifier. System configuration is 
shown in Table 1. In column 1, BL means 
baseline, X means instance expansion, 3 and 15 
means the window size. In column 2, wi is the i-
th word relative to the target word, wi-1wi is the 2-
gram of words, wj/j is the word with position 
information (j?[-3,+3]). In the last column, ?O? 
means using only the original training data and 
?O+A? means using both the original and 
additional training data. Syntactic feature and 
parameter optimizing are not used in this module. 
 
System Features Window Size 
Training 
Data 
BL_3 
wi, wi-1wi, wj/j
?3 O 
X_3 ?3 O+A 
BL_15 ?15 O 
X_15 ?15 O+A 
Table 1: The system configuration 
2.2 Sentence SRL and Event Detection 
We use a feature-based machine learning method 
to implement the SRL module in which three 
tags are labeled, namely the semantic role tag, 
the event role tag and the phrase type tag. We 
consider the SRL task as a four-step pipeline: (1) 
parsing which generates a constituent parse tree 
for the input sentence; (2) pruning which filters 
out many apparently impossible constituents 
(Xue and Palmer, 2004); (3) semantic role 
identification (SRI) which identifies the 
constituent that will be the semantic role of a 
predicate in a sentence, and (4) semantic role 
classification (SRC) which determines the type 
of identified semantic role. The machine learning 
method takes PAS as the classification unit 
which consists of a target predicate and an 
argument candidate. The SRI step utilizes a 
binary classifier to determine whether the 
argument candidate in the PAS is a real argument. 
Finally, in the SRC step, the semantic role tag 
and the event role tag of each identified 
argument can be obtained by two multi-value 
classifications on the SRI results. The remaining 
phrase type tag can be directly extracted from the 
constituent parsing tree.  
The selection of the feature set is the most 
important factor for the feature-based SRL 
method. In addition to constituent-based features 
and dependency-based features, we also consider 
WSD-based features. To our knowledge, the 
combined use of constituents-based syntactic 
features and dependency-based syntactic features 
is the first attempts to use them both on the 
feature level of SRL. As a prevalent kind of 
syntactic features for SRL, constituent-based 
features have been extensively studied by many 
researchers. In this module, we use 34 
constituent-based features, 35 dependency-based 
features, and 2 WSD-based features. Among the 
constituent-based features, 26 features are 
manually selected from effective features proven 
by existing SRL studies and 8 new features are 
305
defined by us. Firstly, the 26 constituent-based 
features used by others are: 
y predicate (c1), path (c2), phrase type (c3), 
position (c4), voice (c5), head word (c6), 
predicate subcategorization (c7), syntactic 
frame (c8), head word POS (c9), partial path 
(c10), first/last word (c11/c12), first/last POS 
(c13/c14), left/right sibling type (c15/c16), 
left/right sibling head (c17/c18), left/right 
sibling POS (c19/c20), constituent tree 
distance (c21), temporal cue words (c22), 
Predicate POS (c23), argument's parent 
type(c24), argument's parent head (c25) and 
argument's parent POS (c26). 
And the 8 new features we define are: 
y Locational cue words (c27): a binary feature 
indicating whether the constituent contains 
location cue word.  
y POS pattern of argument (c28): the left-to-
right chain of POS tags of argument's children. 
y Phrase type pattern of argument (c29): the 
left-to-right chain of phrase type labels of 
argument's children. 
y Type of LCA and left child (c30): The phrase 
type of the Lowest Common Ancestor (LCA) 
combined with its left child. 
y Type of LCA and right child (c31): The phrase 
type of the LCA combined with its right child. 
y Three features: word bag of path (c32), word 
bag of POS pattern (c33) and word bag of type 
pattern (c34), for generalizing three sparse 
features: path (c7), POS pattern argument (c28) 
and phrase type pattern of argument (c29) by 
the bag-of-words representation. 
Secondly, the selection of dependency-based 
features is similar to that of constituent-based 
features. But dependency parsing lacks 
constituent information. If we want to use 
dependency-based features to label constituents, 
we should map a constituent to one or more 
appropriate words in dependency trees. Here we 
use head word of a constituent to represent it in 
dependency parses. The 35 dependency-based 
features we adopt are:  
y Predicate/Argument relation (d1/d2), relation 
path (d3), POS pattern of predicate?s children 
(d4), relation pattern of predicate?s children 
(d5) , child relation set (d6), child POS set (d7), 
predicate/argument parent word (d8/d9), 
predicate/argument parent POS (d10/d11), 
left/right word (d12/d13), left/right POS 
(d14/d15), left/right relation (d16/d17), 
left/right sibling word (d18/d19), left/right 
sibling POS (d20/d21), left/right sibling 
relation (d22/d23), dep-exists (d24) and dep-
type (d25), POS path (d26), POS path length 
(d27), relation path length (d28), high/low 
support verb (d29/d30), high/low support noun 
(d31/d32) and LCA?s word/POS/relation 
(d33/d34/d35). 
In this work, the dependency parse trees are 
generated from the constituent parse trees using a 
constituent-to-dependency converter (Marneffe 
et al, 2006). The converter is suitable for 
semantic analysis as it can retrieve the semantic 
head rather than the general syntactic head.  
Lastly, the 2 WSD-based features are: 
y Situation description formula (s1): predicate?s 
situation description formula generated by the 
WSD module. 
y Natural explanation (s2): predicate?s natural 
explanation generated by the WSD module. 
3 Experimental Results and Discussion 
3.1 Target Verb WSD 
System Micro-A (%) Macro-A (%) Rank
BL_3 81.30 83.81 3/7 
X_3 79.82 82.58 4/7 
BL_15 79.23 82.18 5/7 
X_15 77.74 81.42 6/7 
Table 2: Official results of the WSD systems 
Table 2 shows the official result of the WSD 
system. BL_3 with window size three using the 
original training corpus achieves the best result 
in our submission. It indicates the local features 
are more effective in our systems. There are two 
possible reasons why the performances of the X 
system with instance expansion are lower than 
the BL system. First, the additional instances 
extracted based on 3-gram provide a few local 
features but many topical features. But, local 
features are more effective for our systems as 
mentioned above. The local feature related 
information that the classifier gets from the 
additional instances is not sufficient. Second, the 
granularity of the WSD module is too small to be 
distinguished by 3-grams. As a result, the 
additional corpus built upon 3-gram has more 
exceptional instances (noises), and therefore it 
impairs the performance of X_3 and X_15. 
Taking the verb ??? ? (belong to ) as an 
example, it has two senses in the task, but both 
senses have the same natural explanation: ???
?????????? (part of or belong to), 
which is always considered as the sense in 
general SRL. The difference between the two 
senses is in their situation description formulas: 
?partof (x,y)+NULL? vs. ?belongto (x,y)+NULL?.  
306
3.2 Sentence SRL and Event Detection 
In the SRL module, we use the training data 
provided by SemEval-2010 to train the SVM 
classifiers without any external resources. The 
training data contain 4,608 sentences, 100 target 
predicates and 13,926 arguments. We use the 
SVM-Light Toolkit (Joachims, 1999) for the 
implementation of SVM, and use the Stanford 
Parser (Levy and Manning, 2003) as the parser 
and the constituent-to-dependency converter. We 
employ the linear kernel for SVM and set the 
regularization parameter to the default value 
which is the reciprocal of the average Euclidean 
norm of the training data. The evaluation results 
of our SRL module on the official test data are 
shown in Table 3, where ?AB?, ?SR?, ?PT? and 
?ER? represent argument boundary, semantic role 
tag, phrase type tag, and event role tag. 
 
Tag Precision(%) Recall(%) F-Score(%)
AB 73.10 66.83 69.82 
AB+SR 67.44 61.65 64.42 
AB+PT 61.78 56.48 59.01 
AB+ER 69.05 63.12 65.95 
Overall 58.33 53.32 55.71 
Table 3: Official results of the SRL system 
It is clear that ?AB? plays an important role as 
the labeling of the other three tags is directly 
based on it. Through analyzing the results, we 
find that errors in the recognition of ?AB? are 
mainly caused by two factors: the automatic 
constituent parsing and the pruning algorithm. It 
is inevitable that some constituents and 
hierarchical relations are misidentified in 
automatic parsing of Chinese. These errors are 
further enlarged by the heuristic-based pruning 
algorithm because the algorithm is built upon the 
gold-standard paring trees, and therefore a lot of 
real arguments are pruned out when using the 
noisy automatic parses. So the pruning algorithm 
is the current bottleneck of SRL in the evaluation.  
 
System Micro-A (%) Macro-A (%) Rank
BL_3 20.33 20.19 4/7 
X_3 20.05 20.23 5/7 
BL_15 20.05 20.22 6/7 
X_15 20.05 20.14 7/7 
Table 4: Official results of the ED systems 
From the fact that the results of ?AB+SR? and 
?AB+ER? are close to that of ?AB?, it can be 
inferred that the SR and ER results should be 
satisfactory if the errors in ?AB? are not 
propagated. Furthermore, the result of ?AB+PT? 
is low as the phrase types here is inconsistent 
with those in Stanford Parser. The problem 
should be improved by a set of mapping rules. 
Finally, in the ED module, we combine the 
results of WSD and SRL by filling variables of 
the situation description formula obtained by the 
WSD module with the arguments obtained by the 
SRL module according to their event role tags. 
Table 4 shows the final results which are 
generated by combining the results of WSD and 
SRL. Obviously the reduced overall ranking 
comparing to WSD is due to the SRL module. 
4 Conclusions 
In this paper, we propose a modular approach for 
the SemEval-2010 Task on Chinese event 
detection. Our system consists of three modules: 
WSD, SRL and ED. The WSD module is based 
on instances expansion, and the SRL module is 
based on rich syntactic features. Evaluation 
results show that our system is good at WSD, 
semantic role tagging and event role tagging, but 
poor at pruning and boundary detection. In future 
studies, we will modify the pruning algorithm to 
reduce the bottleneck of the current system. 
 
Acknowledgments 
This work is partially supported by the Hong 
Kong Polytechnic University under Grant No. G-
U297 and G-U596, and by the National Natural 
Science Foundation of China under Grant No. 
60736014 and 60803094. 
References  
Thorsten Joachims. 1999. Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods. 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed), MIT Press. 
Roger Levy and Christopher D. Manning. 2003. Is it 
harder to parse Chinese, or the Chinese Treebank. 
Proceedings of ACL-2003. 
Vasin Punyakanok, Dan Roth, and Wentau Yih. 2005. 
The necessity of syntactic parsing for semantic role 
labeling. Proceedings of IJCAI-2005. 
Yunfang Wu, Peng Jin, Yangsen Zhang, and Shiwen 
Yu. 2006. A Chinese corpus with word sense 
annotation. Proceedings of ICCPOL-2006. 
David Yarowsky. 1993. One sense per collocation. 
Proceedings of the ARPA Workshop on Human 
Language Technology. 
Qiang Zhou. 2010. SemEval-2010 task 11: Event 
detection in Chinese News Sentences. Proceedings 
of SemEval-2010. 
307
