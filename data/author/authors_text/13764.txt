Coling 2010: Poster Volume, pages 674?682,
Beijing, August 2010
Chinese Frame Identification using T-CRF Model 
Ru Li*, Haijing Liu+, Shuanghong Li? 
School of Computer and Information Technology 
Shanxi University 
*liru@sxu.edu.cn 
+bukaohuaxue@163.com 
?lishuanghong09@gmail.com 
 
Abstract 
As one of the important tasks of 
SemEval Evaluation, Frame Semantic 
Structure Extraction based on the Fra-
meNet has received much more atten-
tion in NLP field. This task is often di-
vided into three sub-tasks: recognizing 
target words which are word expres-
sions that evoke semantic frames, as-
signing the correct frame to them, name-
ly, Frame Identification (FI), and for 
each target word, detecting and labeling 
the corresponding frame elements prop-
erly. Frame identification is the founda-
tion of this task. Since the existence of 
links between frame semantics and syn-
tactic features, we attempt to study FI on 
the basis of dependency syntax. There-
fore, we adopt a tree-structured condi-
tional random field (T-CRF) model to 
solve Chinese frame identification based 
on Dependency Parsing. 7 typical lexi-
cal units which belong to more than one 
frame in Chinese FrameNet were se-
lected to be researched. 940 human an-
notated sentences serve as the training 
data, and evaluation on 128 test data 
achieved 81.46% precision. Compared 
with previous works, our result shows 
obvious improvement. 
1 Introduction 
In recent years, semantic research has roused 
great interest in NLP field. With the progress of  
many semantic lexicons, this research gradually 
becomes promising and exciting. As one of the 
tasks of SemEval Evaluation, Frame Semantic 
Structure Extraction based on the FrameNet 
grows to be highlighted for special attention. 
Given a sentence, the task of Frame Semantic 
Structure Extraction consists of the following 
three parts: recognizing the word expressions 
(target words) that evoke semantic frames; dis-
criminating the word sense (frame) of each 
evoking expression; for each target word, label-
ing its syntactic dependents with regard to 
which roles in that frame they fill (Baker et al, 
2006). Among of these three components, frame 
identification is the fundamental and key prob-
lem. However, current research of this task in 
Chinese is only focused on semantic role label-
ing based on the given target words and their 
corresponding frames (Xue, 2008). We insist 
that whether target words can be assigned cor-
rect frames in context is a crucial problem de-
manding prompt solution in this task. 
Chinese FrameNet (CFN) (You and Liu, 
2005), developed by Shanxi University, is an 
ongoing effort of building a semantic lexicon 
for Chinese based on the theory of Frame Se-
mantics (Fillmore, 1982), referencing the Fra-
meNet(Baker et al, 1998) and supported by 
corpus evidence. The CFN project currently 
contains more than 2100 lexical units, more 
than 300 semantic frames, and has exemplified 
more than 21600 annotated sentences. The ulti-
mate goal of this project is to generate informa-
tion about the articulation of the semantic and 
syntactic requirements of Chinese lexical items 
and presents this information in a variety of 
web-based reports and represents the lexical 
semantics of all the sentences in a Chinese text. 
674
According to statistics, there are 332 lexical 
units belonging to more than one frame in the 
current CFN databases. For example, lexical 
unit ??? ?can evoke the following three 
frames: ??? (Expressing_publicly) ?, ???
(Statement) ? and ???(representative) ?. In 
order to extract the semantic structure of a sen-
tence containing ambiguous target words, the 
first step is to assign the correct frame to the 
target words in a given context. 
This task is similar with the word sense dis-
ambiguation (WSD) task to a certain extent 
(Katrin Erk, 2005). WSD is to resolve the inher-
ent polysemia of words by determining the ap-
propriate sense for each ambiguous word in a 
given text, while frame identification is assign-
ing a correct frame for the ambiguous target 
word in the current sentence context. Neverthe-
less, essential difference exists between them. 
WSD prefers to disambiguation on static sense, 
whereas based on the frame semantics, frame 
identification lays particular emphasis on con-
sistency between sentence scene and the dy-
namic scene described by the candidate frames.  
Since the existence of links between frame 
semantics and syntactic features, we adopt a 
tree-structured conditional random field (T-CRF) 
model to solve Chinese frame identification 
based on Dependency Parsing. 7 typical lexical 
units which belong to more than one frame in 
CFN were selected to be researched. 940 human 
annotated sentences were collected for the train-
ing data, and 128 for test data. 
The rest of this paper is organized as follows. 
Section 2 introduces some related work. Section 
3 gives a simple system description. Section 4 
describes Chinese frame identification using T-
CRF model. Section 5 presents our experimen-
tal results and some analysis. Section 6 is the 
conclusions. 
2 Related Work 
With the development and improvement of 
FrameNet, the research based on this lexical 
resource is increasing gradually. Frame 
Semantic Structure Extraction based on 
FrameNet is such hot topics. One sub-tasks of 
this research is frame identification, which is the 
research problem in this paper. 
At present, there are some but not much work 
on frame identification. Main works are as fol-
lows: CL Research participated in the SemEval-
2007 task for Frame Semantic Structure Extrac-
tion. They integrated the use of FrameNet in the 
Text Parser component of the CL Research 
KMS. In particular, they created a FrameNet 
dictionary from the FrameNet databases with 
the CL Research DIMAP dictionary software 
and used this dictionary as a lexical resource. 
The current FrameNet DIMAP dictionary con-
tains 7575 entries, with many entries having 
multiple senses. For each sense, the FrameNet 
part of speech, the definition, the frame name, 
the ID number, and the definition source (identi-
fied as FN or COD) are captured from the Fra-
meNet files. When a lexical unit is recognized 
in processing the text, the first step is to retrieve 
the entry for that item in the dictionary and use 
the frame element realization patterns to disam-
biguate among the senses. A score is computed 
for each sense and the score with the highest 
sense was selected. They evaluated on three 
texts and the best result is 66.10% precision 
(Litkowski, 2007).  
Adrian Bejan and Hathaway (2007) selected 
from the FN lexicon 556 target words that 
evoke at least two semantic frames and have at 
least five sentences annotated for each frame. 
And then they assembled a multi-class classifier 
using two types of models: SVM and Maximum 
Entropy for each ambiguous target word. They 
extracted features used in word sense disam-
biguation (Florain et al, 2002), lexical features 
of the target word, and NAMED ENTITY 
FLAGS associated with the root vertex in a syn-
tactic parse tree. For the rest of the ambiguous 
target words that have less than five sentences 
annotated, they randomly chose a frame as be-
ing the correct frame in a given context. For FI 
sub-task, they obtained 76.71% accuracy com-
pared to a baseline of 60.72% accuracy that al-
ways predicts the most annotated frame for each 
of the 556 target words. 
Johansson and Nugues (2007) firstly used 
some filtering rules to detect target words, and 
for the target words left after the filtering, they 
trained a disambiguating SVM classifier on all 
ambiguous words listed in FrameNet. The clas-
sifier used the following features: target lemma, 
target word, sub categorization frame, the set of 
dependencies of the target, the set of words of 
the child vertexes, and the parent word of the 
target. Its accuracy was 84% on the ambiguous 
675
words, compared to a first-sense baseline score 
of 74%. 
The above researches focused on English 
based on FrameNet. To our knowledge, there 
exists no work for Chinese by far. Most meth-
ods mentioned above treat the frame identifica-
tion as an independent classification problem 
for each ambiguous target word in a sentence. 
However, because of neglecting the relations 
between the candidate frames, the resulting 
frame assignment may be semantically inconsis-
tent over the sentence. 
3 System Description 
Our system consists of three stages. The first 
is corpus construction of our experiments. We 
selected 7 typical lexical units from the current 
CFN lexicon which can evoke at least two se-
mantic frames. They are ? ?
??,???,???,???,???,????,????, re-
spectively. For each of them, we collected sen-
tences containing this word from Sogou Corpus 
and CCL Contemporary Chinese Corpus of Bei-
jing University. Through a series of refining, 
940 sentences annotated correct frame for each 
target word comprise a standard corpus as the 
training data. Another 128 sentences serve as 
the test data. 
 The second stage is dependency parsing. We 
used LTP of Information Retrieval Research 
Center, Harbin Institute of Technology (HIT-
CIR) to POS tagging and dependency parsing 
the training and test sentences. For the obvious 
lexical and syntax errors in the outputs, manu-
ally corrected was conducted. 
At last, Chinese frame identification task is 
regarded as a labeling task on the dependency 
tree structure. By using T-CRF, we can model 
this as the maximization of the probability of 
word sense (frame) trees, given the scores for 
vertexes and edges. In the training phase, ap-
propriate features of vertex and edge are ex-
tracted, and the weight vectors are optimized 
over the training data.  
Figure 1 gives an illustration of the system. 
Figure 1.  Framework of the system 
4 Chinese Frame Identification 
Given a sentence, frame identification is to 
determine an appropriate frame for each of 
target words by comparing consistency between 
sentence context and the dynamic scene 
described by their candidate frames. Currently, 
most researchers addressed this task as an 
independent classification problem for each 
target word in a sentence. Consequently, the 
resulting frame assignment for each target word 
may be semantically inconsistent over the 
sentence. 
We regard Chinese frame identification prob-
lem as a labeling task on the dependency tree 
structure due to the links between syntactic fea-
tures and frame semantics. Our empirical study 
shows that the frame of target word not only 
influenced by the adjacent words in position but 
also its governor and dependents words in syn-
tactic structure. Therefore, we try to solve this 
problem based on dependency parsing. T-CRF 
model is a special CRF model, which is differ-
ent from widely used linear-chain CRFs, in 
which the random variables are organized in a 
tree structure. As we can see, it should be feasi-
ble and reasonable to adopt a T-CRF model to 
frame identification after parsing the sentence.  
In this section, we firstly introduce the linear-
chain CRFs briefly, and then explain the T-CRF 
model for Chinese frame identification, espe-
cially the feature selection and parameter esti-
mation.   
4.1 Tree-Structured Conditional Random 
Field?T-CRF? 
Conditional Random Fields (CRFs) are undi-
rected graphical models (Lafferty et al 2001). 
For the observation sequence 
1 2 3 nX x x x x= " and its corresponding label 
sequence 1 2 3 nY y y y y= "  , CRF defines the 
conditional probability as: 
676
{
}
1
( | )
1
exp( ( , , ))
( )
exp( ( , ))
k k i i
i k
k k i
i k
P Y X
f y y X
Z X
g y X
?
?
?=
+
? ?
? ?
 
where X  is the observation sequence, and 
iy is the label at position i  in label sequence Y . 
( )kf ? and ( )kg ? are feature functions. 
k? and k? are the weight vectors. ( )Z X  is the 
normalization factor. CRFs are state-of-the-art 
methods for sequence labeling problem in many 
NLP tasks.  
Tree-Structured Conditional Random Field 
(Tang et al, 2006) is a particular case of CRFs, 
which can model dependencies across hierar-
chically laid-out information, such as depend-
ency syntactic relations between words in a sen-
tence.  
The graphical structure of T-CRF is a tree, in 
which three main relations exist for a vertex: 
parent-child, child-parent and sibling vertexes. 
In our experiments, we only used parent-child 
edges and child-parent edges. The sibling-
vertexes edges were ignored because of weak 
dependency syntactic relation between words in 
a sentence. So the probability distribution in our 
T-CRF model can be written as below. 
{ }
' '
'' ''
( | )
1
exp
( )
( , ( ), )
( , ( ), , , ( ))
( , ( ), , , ( ))
v V
j j
j
k k
k
l l
l
p y x
F G S
Z x
F f v y v x
G g v y v x v y v
S s v y v x v y v
?
?
?
?
= + +
=
=
=
?
?
?
?
  
where F ? G ? S  represent the feature 
functions of current vertex, feature functions of 
parent vertex of current vertex and feature func-
tions of child vertexes of current vertex, respec-
tively. v  is a word corresponding to the vertex 
in the tree, 'v is the parent vertex of v and ''v are 
the child vertexes of v . 
In Chinese frame identification, the observa-
tion x in T-CRF corresponds to a word in the 
current sentence. The label y thus corresponds 
to the frame name for the word. In the experi-
mental corpus, for the target word, y is anno-
tated its correct frame name, while for the other 
words left, y is annotated tag ?null?. These tar-
get words are the 7 lexical units we selected and 
their frames come from the current CFN lexicon. 
At present, only the frame identification of tar-
get word was studied, the disambiguation of the 
other multi-senses words in the sentence was 
not being processed. 
Although T-CRFs are relatively new models, 
they have already been applied to several NLP 
tasks, such as semantic role labeling, semantic 
annotation, word sense disambiguation, image 
modeling.(Cohn and Blunsom, 2005; Tang et al, 
2006; Jun et al, 2009; Awasthi et al, 2007). All 
these works proved this model to be useful in 
modeling the semantic structure in a sentence or 
a text. Our study is the first application of T-
CRFs to frame identification.  
4.2 Feature Selection 
In order to apply T-CRF model, it is neces-
sary to represent the sentence with a hierarchi-
cal structure. We used LTP of HIT-CIR to POS 
tagging and dependency parsing the training and 
test sentences. To facilitate the description of 
feature selection based on the dependency tree 
structure, figure 2 gives the dependency output 
of an example. 
 
                   ? 
       SBV 
                ADV       ADV        VOB 
?              VV 
 
??                         ?            ??                                                  
? 
ADV  
           ADV  VOB  VOB 
?? 
 
??             ??           ?? 
                                        
VOB      MT 
          
??             ? 
Figure 2.  Example of a dependency parsed sen-
tence.  
This example sentence is:???????
???????????????. In English, 
it reads ?He has been want to make films, and 
finally has the opportunity to realize his dream 
677
today?. In the dependency tree structure, arrow 
points from the parent vertex to child vertex, the 
label on a arc is the type of dependency relation 
between the parent and the child vertex. 
Feature selection is a core problem in se-
quence labeling model. In our experiments, 18 
template settings were conducted to discover the 
best features for frame identification. During 
this process, we considered two main factors: 
firstly, the number of features should not be too 
large so as to avoid the over-fitting phenomenon; 
secondly, the selected features should be able to 
provide enough information conditioned on tol-
erated computation, for the purpose of improv-
ing the performance of system. With the in-
creasing of the number of features and the cost 
of the system, if the performance of system can 
not be improved obviously, we stopped to add 
features and regard the parameter of current 
template as the best. At this moment, a good 
balance between the performance and cost of 
computation was achieved. 
We experimented with two different types of 
feature settings. One we used was the very basic 
feature sets based on the words and Part of 
Speech (POS) and their bigram features. In or-
der to see the effectiveness of dependency fea-
tures, the other type of feature settings include 
more informative tree features. These features 
capture information about a vertex?s parent, its 
children and the relation with its parent and 
children. These features are semantically and 
structurally very informative and we expect to 
improve our performance with them. The base 
and tree features we used are listed in table 1. 
In these features, the setting of basic features 
is fundamental and meaningful because it can 
be used to compare T-CRF and linear chain 
CRF. For the tree features, given the i -th vertex 
in the observation ix , ( , )p cf y y and 
( , )c pf y y represent whether the current vertex 
has a parent-child dependency with a parent 
vertex and whether it has a parent-child depend-
ency with a child vertex, respectively. In de-
pendency grammars (Igor' A. Melchuk, 1988), 
every vertex has only one parent as its governor, 
and may have more than one child as its de-
pendents. Words in a sentence through certain 
syntactic relations form the semantic structure 
of this sentence. Therefore, we argue that the  
Table 1.  Base Features & Tree Features 
words that have syntactic dependency rela-
tions with the target word are more impor-
tant than the ones neighboring with it in posi-
tion for frame identification. For this reason, we 
added the parent vertex and children vertexes 
into the tree features. With respective to the re-
lation type, we used the annotation sets defined 
by HIT-CIR in LTP, which contain 24 kinds of 
dependency relation types. One thing should be 
concerned is that we don?t consider all types of 
children vertexes. This is because that according 
to our empirical study, not all of the children 
have strong dependencies with the target word. 
On the contrary, more features would bring 
the noise and affect the efficiency seriously.  
Hence, we chose 4 types of children relation 
from the linguistic point of view. They are, 
?SBV(subject-verb)? representing ??????, 
?VOB(verb-object)? representing ??????, 
?ADV(adverbial)?  representing ?????? 
and ?ATT(attribute) ? representing ??????. 
From the point of grammars and semantics, 
these four relations are more influenced on the 
words in a sentence. As we know, the subject, 
predicate and object constitute the semantic core 
of a sentence. The good news is that experimen-
tal results proved this hypothesis relatively cor-
rect. 
Category Features 
Base 
features
Word and bigram of word,  
POS and bigram of POS 
Parent vertex of current 
word 
The edge between cur-
rent word and its par-
ent 
( , )p cf y y
 The dependency rela-
tion type between cur-
rent word and its par-
ent 
child vertex of current 
word 
The edge between cur-
rent word and its child
Tree 
features
( , )c pf y y
 The dependency rela-
tion type between  cur-
rent word and its child
678
4.3 Parameter Estimation 
The parameter estimation is to optimize the pa-
rameters { }1, 2,...; 1, 2,...? ? ? ? ?= from train-
ing data { }( 1, 1), ( 2, 2),...D x y x y with empirical 
distribution ( ),p x y . Nowadays, the commonly 
used method for parameter estimation is maxi-
mum likelihood function. That is  
argmax log( ( / ))i i
i
L p y x? ?= ?  given the 
observation sequences { }1 2, ,...x x and label se-
quences{ }1 2, ,...y y . 
In this paper, the conventional L-BFGS me-
thod was used to estimate the optimal parame-
ters { }1, 2,...; 1, 2,...? ? ? ? ?= (Jorge Nocedal 
and Stephen J. Wright. 1999). 
5 Experiments   
5.1 Data preparation  
So far, there has no research on Chinese frame 
identification, thus it is unfeasible to do experi-
ments based on readily available corpus. Ac-
cordingly, preparing a good and reasonable 
training and test data is our fundamental task.       
At present, there are 332 lexical units that can 
evoke at least two frames in the CFN lexicon. In 
this paper, we selected 7 typical ambiguous lex-
ical units to be researched. They are ??
??,???,???,???,???,????,????. The 
selection principle is following: first of all, it is 
time-consuming to construct corpus for all of 
the 332 lexical units, so currently we just stud-
ied part of them to prove the validity of the 
method we proposed. Secondly, the frames 
evoked by these lexical units should be distin-
guished clearly by human annotators. For ex-
ample, lexical unit ???? can evoke these three 
frames: ?????(Experiencer_obj)?, ????
? (Experiencer_subj)? and ? ? ? ? ?
(Emotion_directed)?. All these frames describe 
a tender feeling in psychology, so it is difficult 
to discriminate among them and thus hard to 
annotate sentences correctly. Thirdly, these 7 
lexical units are high frequency words so it is 
easier to collect sentences and make the ex-
periments more practical. 
For each of 7 lexical units, we collected sen-
tences containing this word from Sogou Corpus 
and Contemporary Chinese Corpus of Beijing 
University. After a preliminary screening, about 
1000 sentences compose the original and coarse 
corpus.  
Although these sentences were complete and 
relatively standard, some of them didn?t meet 
the criterion of Chinese frame identification 
research. Such cases mainly include three as-
pects. For one thing, the correct frame of am-
biguous target word is difficult to decide by 
human annotator. For the other, the meaning of 
target word can?t correspond to any frame defi-
nition in current CFN version. For example, 
lexical unit ??? can express the meaning of 
opinion and wish which have the corresponding 
frames in CFN, while the meaning of thinking 
and memory did not. Lastly, some words 
couldn?t evoke frames though their word forms 
are the same as lexical unit. We removed the 
sentences belonging to the above situations and 
got a refined corpus containing 940 sentences 
for training data and 128 for test data. And then, 
we used LTP to POS tagging and dependency 
parsing the training and test sentences. 
5.2 Experimental Results and Analysis 
For the linear-chain CRF, we defined the fea-
tures based on the words, POS of words and 
their bigram features as the base features. For T-
CRF, we used the base features and tree features. 
Six different types of template settings on these 
features are listed in table 2. 
template features 
T1 Base features 
T2 Add edge  between current word and its parent on T1 
T3 Add dependency type between current word and its parent  on T2
T4 
Add edge between current word 
and its four types children ver-
texes  on T1 
T5 
Add dependency type between 
current word and its four types 
children vertexes on T4 
T6 Add all these tree features  on T1
Table 2. Template settings on different features 
 For each of these template settings, we ex-
perimented on different observation window 
size of 1, 2 and 3, which represents one word, 
679
two words and three words previous and next to 
the current word respectively.  
   We use the
n
precision
s
= to evaluate our sys-
tem, where n is the number of target words la-
beled correctly, and s is the total number of tar-
get words need to be labeled. In our 128 test 
sentences, there are 151 target words because 
there are some sentences containing more than 
one ambiguous target word. Experimental re-
sults on 18 templates are listed in table 3. 
From the table 3, we can get four conclusions. 
Firstly, the best performance 81.46% in T-CRF 
model increases about 5% over the best per-
formance 76.82% in CRF model. This suggests 
the dependencies on the tree structure can cap-
ture more important characteristics than those 
on the linear chains do. Secondly, when we 
added the edge feature between current word 
and its parent, the performance declined unex-
pectedly. This can be explained in linguistics: in 
a dependency parsed sentence, the clique of a 
governor and its dependents forms ?a small 
world? which can express partial meaning of the 
sentence, while the parent of current vertex (ex-
cept the root vertex which has no parent) can 
not influence much on it because its parent has 
its own clique, and current word is just a tiny 
fragment of the clique of its parent, on the con-
trary, the parent vertex feature will bring nega-
tive effect on the current word. For example, the 
target word ??? in figure 2 can illustrate this 
case clearly. Thirdly, when we added the chil-
dren vertexes, the performance increased, that is 
because current word and its dependents to-
gether can form a semantic clique of the sen-
tence. Lastly, when we added the dependency 
relation type on the features of parent-child 
edge and child-parent edge, the performance 
improved slightly because the relation type of 
edge is coarser than the edge between parent 
and child. There are only 24 kinds of depend-
ency types but exist hundreds of edge combina-
tion possibilities between parent and child. Thus, 
this feature relived the data sparseness problem 
to a certain extent.  
    There are two main types of errors in the re-
sults: one is that the labeling frames of target 
words are not correct. For example, in the sen-
tence ??????????????????
????? , the correct frame of ????should 
be ??? ? instead of ??? ?, because it 
described the attitude of ???? not declared a 
fact or a phenomena. However, this kind of 
deep semantics of sentence couldn?t be capured 
by T-CRF model based on the dependency 
syntax. The other is that the labeling frames of 
some target words are tag ?null?. The reason is 
that some lexical units can?t evoke a frame 
sometimes, so in the training data, these words 
are annotated ?null?. 
5.3 Contrast Experiments 
Qu (2008) argues that any words in a sentence 
has a certain attraction between each other and 
thus constitute the grammars and semantic 
structure of the sentence. Based on this cogni-
tion, he proposed a generalized collocation the-
ory, which includes fixed collocation, loose col-
location and Co-occurrence collocation.  Ac-
cording to this theory, a context computing 
model RFR_SUM was presented to deal with 
the WSD task. 
In essence, frame identification also belongs 
to context computing, so it should be reasonable 
to solve this problem with the generalized col-
location theory. However, our current corpus is 
too insufficient to reflect all these three colloca-
tions in the statistical sense. Hence, we pro-
posed a method named compatibility of lexical 
unit based on the Co-occurrence collocation to 
identify frame for ambiguous target word. 
 
 
Precision Window 
size T1 T2 T3 T4 T5 T6 
1 0.7682 0.7219 0.7351 0.8013 0.8146 0.7947 
2 0.7682 0.7152 0.6887 0.7881 0.8146 0.7947 
3 0.7417 0.6623 0.6689 0.7351 0.8013 0.7616 
Table 3. Precisions of different templates based on three types of window size 
680
The connotation of compatibility of lexical 
unit is as follows. In the CFN frame database, 
every frame defines a lexical units set, in 
which each of lexical unit can evoke this 
frame.  When one of these lexical units serves 
as the target word in a sentence, we can use 
the compatibilities of other lexical units in this 
set with the sentence to reflect the consistency 
between this frame and current sentence. The 
compatibility of lexical unit with the sentence 
is computed by the Co-occurrence frequency 
of lexical unit and the notional words in the 
sentence in a large corpus. The calculation is 
as below.  
Suppose il in the lexical units set 
{ }1 2, ,... ,...,i mL l l l l= serves as the target word 
in the sentence S . The words in S except the 
functional words and il constitute a word 
set { }1 2, ,..., nW w w w= . And the compatibil-
ity of L  with S  is denoted as C . 
1 2( , ) ( , ) ... ( , )mc l W c l W c l WC
m
+ += , 
where m  is the number of lexical units in L .   
1 2( , ) ( , ) ... ( , )( , ) j j j nj
f l w f l w f l w
c l W
n
+ + += , 
where n is the number of words in W . 
( , )
( , ) j kj k
count l w
f l w
sum
= , where 
( , )j kcount l w  represents the number of sen-
tences, in which jl and kw  occur together, and 
these sentences come from the corpus of Pe-
king University People's Daily, January 1998. 
sum  is the total number of sentences in the 
same People's Daily corpus. 
In this way, the consistency between a 
frame and the current sentence is scored by 
the compatibility of L belonging to the candi-
date frame with this sentence, and the one 
with highest score is regarded as the correct 
frame. For our test data, 71.73% precision 
based on this method was obtained. 
This model displayed a decline in precision 
of about 10% over the T-CRF. Analysis of the 
results found that the compatibility based on 
Co-occurrence collocation can only reflect a 
weak correlation between words, neglecting 
the position and syntactic structure informa-
tion in a sentence. 
In addition, we used the most-frequency-
frame experiment as the baseline. In the cor-
pus consisted of 940 training sentences and 
128 test sentences, the frequency of each 
frame was counted for ranking. The result of 
this method obtained 61.23% precision, which 
proved that T-CRF model performed obvious 
improvement. 
6 Conclusions 
In this paper, we investigated the problem of 
Frame Identification in Chinese which is the 
first work on Chinese FrameNet. A tree-
structured conditional random field (T-CRF) 
model was applied to this task based on the 
dependency syntactic structure. This model 
provides a way to incorporating the long-
distance dependencies between target words 
and the syntactic related words with it. In our 
experiments, the syntactic dependency fea-
tures were shown to work effectively for 
Frame Identification, with 71.73%, 76.82%, 
and 81.46% precision for compatibility of lex-
ical unit, CRF and T-CRF, respectively.  
Although a relatively good performance 
was achieved on the test data, the small-scale 
and simplicity of sentence structure in corpus 
cannot be ignored compared with the Frame-
Net corpus. However, the experimental results 
that we gained is still promising, suggesting 
that our model is comparatively appropriate to 
the Frame Identification task and still has a 
great potential for improvement. The next 
work will focus on the three aspects: firstly, 
build a larger corpus containing various sen-
tence structures in Chinese; the other is that 
more semantic features will be tried to add in 
the T-CRF model, such as the frame elements 
and the semantic relations between frames, 
finally, we will try to identify frames of target 
words using other machine learning methods 
which has been proved high performance in 
this task. 
Acknowledgements 
This work is supported by NSFC Grant: 
60970053 and International Scientific and 
Technological Cooperation of Shanxi Prov-
ince Grant: 2010081044. In addition, the au-
681
thors would like to thank HIT-CIR for their 
LTP. 
References 
Charles J. Fillmore. 1982. Frame Semantics. In 
Linguistic in the Morning Calm, pages 111-137, 
Seoul, Korea: Hanshin Publishing Company. 
Collin Baker, Michael Ellsworth and Katrin Erk. 
2007. SemEval?07 Task 19: Frame Semantic 
Structure Extraction. In Proceedings of the 4th 
International Workshop on Semantic Evalua-
tions, pages 99-104, Prague. 
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet project. In 
Proceedings of the COLING-ACL, pages 86-90, 
Montreal, Canada. 
Cosmin Adrian Bejan and Hathaway Chris. 2007. 
UTD-SRL: A Pipeline Architecture for Extract-
ing Frame Semantic Structures. In 45th annual 
meeting of Association for Computational Lin-
guistics. pages 460-463, Prague. 
Igor A. Mel??cuk. 1988. Dependency Syntax: The-
ory and Practice. State University Press of New 
York, Albany. 
John Lafferty, Andrew McCallum and Fernando 
Pereira. 2001. Conditional Random Fields: 
Probabilistic Models for Segmenting and Label-
ing Sequence Data. In proceedings of the 18th 
International Conference on Machine Learning, 
pages 282-289, San Francisco, CA, USA. 
Jorge Nocedal and Stephen J. Wright. 1999. Nu-
merical Optimization. Springer, New York. 
Jun Hatori, Yusuke Miyao and Jun?ichi Tsujii. 
2009. On Contribution of Sense Dependencies 
to Word Sense Disambiguation. Natural Lan-
guage Processing, 16(5):51-77.  
Katrin Erk. 2005. Frame assignment as word sense 
disambiguation. In Proceedings of the 6th In-
ternational Workshop on Computational Se-
mantics (IWCS-6) . 
Ken. Litkowski. 2007. CLR: Integration of Fra-
meNet in a Text Representation System.  In 45th 
annual meeting of Association for Computa-
tional Linguistics. pages 113-116, Prague.  
Pranjal Awasthi, Aakanksha Gagrani and Balara-
man Ravindran. 2007. Image modeling using 
tree structured conditional random fields. In 
Proceedings of the 20th International Joint 
Conference on Artificial Intelligence (IJCAI 
2007). Pages 2060-2065. 
Qu Weiguang. 2008. Automatic Disambiguation of 
Modern Chinese Words in Word-level. Beijing: 
Science Press(in Chinese). 
Richard Johansson and Nugues Pierre. 2007. LTH: 
Semantic Structure Extraction using Nonprojec-
tive Dependency Trees. In 45th annual meeting 
of Association for Computational Linguistics. 
pages 227-230, Prague. 
Tang Jie, Mingcai Hong, Juanzi Li, and Bangyong 
Liang. 2006. Tree-structured Conditional Ran-
dom Fields for Semantic Annotation. In 
Proceedings of 5th International Conference of 
Semantic Web (ISWC?2006), Athens, GA, USA 
Trevor Cohn and Philip Blunsom. 2005. Semantic 
role labeling with tree conditional random fields. 
In Proceedings of CoNLL2005. 
Wang Ruiqin and Fansheng-Kong. 2009. The Re-
search of Unsupervised Word Sense Disam-
biguation. Journal of Software, (20)8: pages 
2138?2152. 
Xue Nianwen and Martha Palmer. 2005. Auto-
matic Semantic Role Labeling for Chinese 
Verbs. In Proceedings of the 19th International 
Joint Conference on Artificial Intelligence. Ed-
inburgh, Scotland. 
You Liping, Kaiying Liu. 2005. Building Chinese 
FrameNet database. In Proceedings of IEEE 
NLP-KE?05. 
 
 
 
682
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 74?79, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
SXUCFN-Core: STS Models Integrating FrameNet Parsing Information 
 
 
Sai Wang, Ru Li, RuiboWang, ZhiqiangWang, Xia Zhang 
Shanxi University, Taiyuan, China 
enrique.s.wang@gmail.com 
{liru, wangruibo}@sxu.edu.cn 
{zhiq.wang, caesarzhangx}@163.com 
 
 
 
 
 
 
 
Abstract 
This paper describes our system submitted to 
*SEM 2013 Semantic Textual Similarity (STS) 
core task which aims to measure semantic si-
milarity of two given text snippets. In this 
shared task, we propose an interpolation STS 
model named Model_LIM integrating Fra-
meNet parsing information, which has a good 
performance with low time complexity com-
pared with former submissions. 
1 Introduction 
The goal of Semantic Textual Similarity (STS) is 
to measure semantic similarity of two given text 
snippets. STS has been recently proposed by 
Agirre et al (2012) as a pilot task, which has close 
relationship with both tasks of Textual Entailment 
and Paraphrase, but not equivalent with them and it 
is more directly applicable to a number of NLP 
tasks such as Question Answering (Lin and Pantel, 
2001), Text Summarization (Hatzivassiloglou et al, 
1999), etc. And yet, the acquiring of sentence simi-
larity has been the most important and basic task in 
STS. Therefore, the STS core task of *SEM 2013 
conference, is formally defined as the degree of 
semantic equivalence between two sentences as 
follows: 
 
? 5: completely equivalent, as they mean 
the same thing.  
? 4: mostly equivalent, but some unimpor-
tant details differ. 
? 3: roughly equivalent, but some impor-
tant information differs/missing. 
? 2: not equivalent, but share some details.  
? 1: not equivalent, but are on the same top-
ic. 
? 0: on different topics. 
 
In this paper, we attempt to integrate semantic 
information into STS task besides the lower-level 
word and syntactic information. Evaluation results 
show that our STS model could benefit from se-
mantic parsing information of two text snippets. 
The rest of the paper is organized as follows: Sec-
tion 2 reviews prior researches on STS. Section 3 
illustrates three models measuring text similarity. 
Section 4 describes the linear interpolation model 
in detail. Section 5 provides the experimental re-
sults on the development set as well as the official 
results on all published datasets. Finally, Section 6 
summarizes our paper with direction for future 
works. 
2 Related Work  
Several techniques have been developed for STS. 
The typical approach to finding the similarity be-
tween two text segments is to use simple word 
matching method. In order to improve this simple 
method, Mihalcea et al (2006) combine two cor-
pus-based and six knowledge-based measures of 
word similarity, but the cost of their algorithm is 
expensive. In contrast, our method treats words 
and texts in essentially the same way. 
In 2012 STS task, 35 teams participate and sub-
mit 88 runs. The two top scoring systems are UKP 
74
and Takelab. The former system (B?r et al, 2012) 
uses a simple log-linear regression model to com-
bine multiple text similarity measures (related to 
content, structure and style) of varying complexity. 
While the latter system Takelab (?ari? et al, 2012) 
uses a support vector regression model with mul-
tiple features measuring word-overlap similarity 
and syntax similarity. 
The results of them score over 80%, far exceed-
ing that of a simple lexical baseline. But both share 
one characteristic: they integrate lexical and syntax 
information without semantic information, espe-
cially FrameNet parsing information. In addition, 
the complexity of these algorithms is very high. 
Therefore, we propose a different and simple mod-
el integrating FrameNet parsing information in this 
paper. 
3 Linear Interpolation Model  
In this paper, we propose a combination interpola-
tion model which is constructed by the results of 
three similarity models based on words, WordNet, 
FrameNet , which are called simWD(?), simWN(?) and 
simFN(?)  respectively. The overall similarity  
simLIM(S1, S2) between a pair of texts S1, S2 is com-
puted in the following equation: 
 
simLIM(S1, S2)= ?1 ? simWD(S1, S2)  
+?2 ? simWN(S1, S2) +?3 ? simFN(S1, S2) 
(1)
 
In which, ?1, ?2 and ?3 are respectively the 
weights of the similarity models, i.e., ?1 +?2 +?3 
= 1; and they are all positive hyperparameters. 
Now, we describe the three models used in this 
equation. 
3.1 Similarity Based on Words 
This model is motivated by Vector Space Model 
(Salton et al, 1975). We present each sentence as a 
vector in the multidimensional token space. Let Sc 
denote the set of all words in the c-th text snippets 
(c = 1, 2); the words of bag is W = S1 ? S2. Hence, 
the similarity of a pair of sentences, formally ex-
pressed as: 
simWD(S1, S2) = ?  ??,? ? ??,?
|?|
???
?? ??,??|?|???  ? ?? ??,??|?|???
  (2)
 
In which, we can find ??,? ? ?? ? ? 1,2, ? , |?|; 
? ? 1,2? by solving: 
 
??,? ? ?1, ?? ??,? ? ??0, ?????????  
 
 
(3)
From these two equations above, we can see the 
more identical words in a text pair, the more simi-
lar the two snippets are. Whereas, by intuition, 
many high-frequency functional words would not 
be helpful to the estimation of the similarity given 
in Eq.(2). Therefore, in the preprocessing stage, we 
compute the word frequencies per dataset, and then 
remove the high frequency words (top 1% in fre-
quency list) in each segment. 
3.2 Similarity Based on WordNet 
This model measures semantic similarity with the 
help of such resources that specifically encode re-
lations between words or concepts like WordNet 
(Fellbaum, 1998). We use the algorithms by Lin 
(1998) on WordNet to compute the similarity be-
tween two words a and b, which we call simLin(a, 
b). Let S1, S2 be the two word sets of two given text 
snippets, we use the method below: 
 
simWN(S1, S2)  
= ?  ??? ??????????,? ? ??,???
????|??|,|??|????
????|??|,|??|? 
 
(4)
In which, ??,? ? ???? ? 1,2?. In the numerator of 
Eq.(4),we try to max(?), avg(?) and mid(?) respec-
tively, then we find the max(?) is the best. 
3.3 Similarity Based on FrameNet 
FrameNet lexicon (Fillmore et al, 2003) is a rich 
linguistic resource containing expert knowledge 
about lexical and predicate-argument semantics in 
English. In a sentence, word or phrase tokens that 
evoke a frame are known as targets. Each frame 
definition also includes a set of frame elements, or 
roles, corresponding to different aspects of the 
concept represented by the frame, such as partici-
pants, props, and attributes. We use the term ar-
gument to refer to a sequence of word tokens 
annotated as filling a frame role. 
All the data are automatically parsed by 
SEMFOR1 (Das and Smith, 2012; Das and Smith, 
                                                          
1 See http://www.ark.cs.cmu.edu/SEMAFOR/. 
75
2011). Figure 1 shows the parser output of a sen-
tence pair given in Microsoft Research Video De-
scription Corpus with annotated targets, frames 
and role argument pairs. It can be noticed that 
FrameNet parsing information could give some 
clues of the similarity of two given snippets and 
we think that integrating this information could 
improve the accuracy of STS task. For example, 
the sentences in the Figure 1 both illustrate ?some-
body is moving?. However, our model depends on 
the precision of that parser. If it would be im-
proved, the results in STS task would be better. 
 
 Figure 1: This is a pair of sentences in 2013 STS train-
ing data: (a) Girls are walking on the stage; (b) Women 
models are walking down a catwalk. The words in bold 
correspond to targets, which evoke semantic frames that 
are denoted in capital letters. Every frame is shown in a 
distinct color; the arguments of each frame are anno-
tated with the same color, and marked below the sen-
tence, at different levels; the spans marked in the block 
of dotted liens fulfill a specific role. 
 
For a given sentence Sc (c = 1,2) with a set of 
evoked frame Fc = < f1,f2, ?, fn > (n is the number 
of evoked frames), a set of target word with each 
frame Tc = < t1, t2, ?, tn > and the set of roles 
(namely, frame elements) ?c = {Rc,1, Rc,2, ?,Rc,n}, 
each frame contains one or more arguments  
Rc,i = {rj} (i = 1, 2, ?, n; j is an integer that is 
greater or equal to zero). Take Figure 1 as an ex-
ample, 
 
T1 = <grils, walking>, 
F1 = <PEOPLE, SELF_MOTION>, ?1 = {R1,1, R1,2 }, 
 R1,1 = {girls},  
R1,2 = {girls, on the stage}; 
 
T2 = <women, models, walking, down>, 
F2 = <PEOPLE, VEHICLE, 
SELF_MOTION, DIRECTION>, 
?2 = {R2,1, R2,2, R2,3, R2,4}, 
R2,1 = {women}, R2,2 = {models}, 
R2,3 = {women models}, R2,4 = {down}. 
 
In order to compute simFr(?) simply, we also use 
a interpolation model to combine the similarities 
based on target words simTg(?), frames simFr(?) and 
frame relations simRe(?). They are estimated as the 
following: 
When computing the similarity on target word 
level simTg(S1, S2), we also consider each sentence 
as a vector of target words as is seen in Eq.(5). 
 
T = T1 ?  T2; 
simTg(S1, S2)= ?  ??,? ? ??,?
|T|
???
?? ??,??|?|???  ? ?? ??,??|?|???
 
 
 
 
(5)
In which, we can find t?,? ? ?? ? ? 1,2,? , |?|; 
? ? 1,2? by solving: 
 
??,? ? ?
1, ?? ??,? ? ?? ? ????,? ? ??
?? ? 1,2, ? , |?|?
0, ?????????
 
 
 
(6)
Let simFr(S1, S2) be the similarity on frame level 
as shown in Eq.(7), with each sentence as a vector 
of frames. We define f1,i, f2,i like ??,? in Eq.(3). 
 
F = F1 ?  F2; 
simFr(S1, S2)= 
?  ??,? ? ??,?|?|???
?? ??,??|?|???  ? ?? ??,??|?|???
 
 
 
 
(7)
Before computing the role relationship between 
the pair of sentences, we should find the contain-
ment relationship of each pair of frames in one 
sentence. We use a rule to define the containment 
relationship: 
Given two frames fc,i, fc,j in a sentence Sc, if  
??,? ? ??,? ?? ? ??, then fc,j contains fc,i - and that is 
fc,i is a child of fc,j. After that we add them into the 
set of frame relationship ????  ? ?? ??,?? , ??,?? ?
????? ?  ?????,?????? , ?? ?  0?. We consider the relationship between two 
frames in a sentence as a 2-tuple, and again use 
Figure 1 as an example, 
Rlt1 = ?<PEOPLE, SELF_MOTION>?; 
Rlt2 = ?<PEOPLE, SELF_MOTION>,  
<VEHICLE, SELF_MOTION >?. 
76
Besides, we do exactly the same with both 
frames, namely ????,? ? ???? ?c ?  1,2?  the value 
of ????,? is 1. The similarity on frame relationship 
level simRe(S1, S2) presents each sentence as a vec-
tor of roles as shown in Eq.(8).  
 
Rlt = Rlt1 ? Rlt2; 
simRe(S1, S2)= ?  ????,? ? ????,?
|???|
???
?? ????,??|???|???  ? ?? ????,??|???|???
 
 
(8)
Lastly, the shallow semantic similarity between 
two given sentences is computed as: 
 
SimFN(S1, S2)= ? ? simTg(S1, S2)  
+? ? simFr(S1, S2) +? ? simRe(S1, S2) 
 
 
(9)
In which, ? + ? + ? =1, and they are all positive 
hyperparameters. As shown in Figure 2, we plot 
the Pearson correlation (vertical axis) against the 
combination of parameters (horizontal axis) in all 
2013 STS train data (2012 STS data). We notice 
that generally the Pearson correlation is fluctuates, 
and the correlation peak is found at 32, which in 
Table 1 is ?=0.6, ?=0.3, ?=0.1. 
 
ID ? ? ? ID ? ? ? ID ? ? ? 
1 1 0 0 23 0.7 0.2 0.1 45 0 0.4 0.6
2 0.9 0 0.1 24 0.6 0.2 0.2 46 0.5 0.5 0 
3 0.8 0 0.2 25 0.5 0.2 0.3 47 0.4 0.5 0.1
4 0.7 0 0.3 26 0.4 0.2 0.4 48 0.3 0.5 0.2
5 0.6 0 0.4 27 0.3 0.2 0.5 49 0.2 0.5 0.3
6 0.5 0 0.5 28 0.2 0.2 0.6 50 0.1 0.5 0.4
7 0.4 0 0.6 29 0.1 0.2 0.7 51 0 0.5 0.5
8 0.3 0 0.7 30 0 0.2 0.8 52 0.4 0.6 0 
9 0.2 0 0.8 31 0.7 0.3 0 53 0.3 0.6 0.1
10 0.1 0 0.9 32 0.6 0.3 0.1 54 0.2 0.6 0.2
11 0 0 1 33 0.5 0.3 0.2 55 0.1 0.6 0.3
12 0.9 0.1 0 34 0.4 0.3 0.3 56 0 0.6 0.4
13 0.8 0.1 0.1 35 0.3 0.3 0.4 57 0.3 0.7 0 
14 0.7 0.1 0.2 36 0.2 0.3 0.5 58 0.2 0.7 0.1
15 0.6 0.1 0.3 37 0.1 0.3 0.6 59 0.1 0.7 0.2
16 0.5 0.1 0.4 38 0 0.3 0.7 60 0 0.7 0.3
17 0.4 0.1 0.5 39 0.6 0.4 0 61 0.2 0.8 0 
18 0.3 0.1 0.6 40 0.5 0.4 0.1 62 0.1 0.8 0.1
19 0.2 0.1 0.7 41 0.4 0.4 0.2 63 0 0.8 0.2
20 0.1 0.1 0.8 42 0.3 0.4 0.3 64 0.1 0.9 0 
21 0 0.1 0.9 43 0.2 0.4 0.4 65 0 0.9 0.1
22 0.8 0.2 0 44 0.1 0.4 0.5 66 0 1 0 
Table 1: Different combinations of ?, ?, ? (? + ? + 
? =1) with ID that is horizontal axis in Figure 2. 
This table also apples to different combinations of 
?1, ?2, ?3 (?1 +?2 +?3 =1) with ID that is hori-
zontal axis in Figure 3. 
 Figure 2: This graph shows the variation of Pearson 
correlation (vertical axis) in all 2013 STS train data 
(2012 STS data), with numbers (horizontal axis) indicat-
ing different combinations ?, ?, ? in Table 1 and when 
the value of result confidence is 100. The effect values 
are represented by a vertical line (i.e. ID = 32). 
 
4 Tuning Hyperparameters  
Eq.(1) is a very simple linear interpolation model, 
and we tune the hyperparameters on the whole 
2012 STS data. 
As shown in Figure 3,we plot the Pearson corre-
lation (vertical axis) for the different combination 
of parameters ?1, ?2 and ?3 (horizontal axis). We 
notice that generally the Pearson correlation fluc-
tuates with a dropping tendency in most cases, and 
the correlation peak presents at 13, which in Table 
1 is ?1=0.8, ?2=0.1, ?3=0.1. 
 
 Figure 3: This graph shows the variation of Pearson 
correlation (vertical axis) in all 2013 STS train data 
(2012 STS data), with numbers (horizontal axis) indicat-
ing different combinations ?1, ?2, ?3 in Table 1 and when the value of result confidence is 100. The effect 
values are represented by a vertical line (i.e. ID = 13). 
 
77
5 Results 
We submit four runs: the first one (Model_WD) is 
based on word similarity; the second one (Mod-
el_WN) which is only using the similarity based on 
WordNet, is submitted with the team name of 
SXULLL; the third one (Model_FN) which uses 
FrameNet similarity defined in Section 3.3; and the 
last one in which we combine the three similarities 
described in Section 4 together with an interpola-
tion model. In addition, we map our outputs mul-
tiply by five to the [0-5] range. 
It is worth notice that in the first model, we lo-
wercase all words and remove all numbers and 
punctuations. And in the third model, we extract all 
frame-semantic roles with SEMFOR. 
In the experiment, we use eight datasets totally - 
namely MSRpar, MSRvid, SMTeuroparl, OnWN, 
SMTnews, headlines, FNWN and SMT - with their 
gold standard file to evaluate the performance of 
the submitted systems. Evaluation is carried out 
using the official scorer which computes Pearson 
correlation between the human rated similarity 
scores and the system?s output. The final measure 
is the score that is weighted by the number of text 
pairs in each dataset (?Mean?). See Agirre et al 
(2012) for a full description of the metrics. 
5.1 Experiments on STS 2012 Data 
There is no new train data in 2013, so we use 2012  
data as train data. From Table 2, 3 we can see that 
the Model_LIM has better performance than the 
other three models. 
 
 MSRpar MSRvid SMTeuroparl Mean
Model_WD 0.4532  0.4487   0.6467 0.5153
Model_WN 0.2718  0.5410  0.6225  0.4774
Model_FN 0.4437  0.5530  0.5178  0.5048
Model_LIM 0.4896  0.5533  0.6681  0.5696
Table 2: Performances of the four models on 2012 train 
data. The highest correlation in each column is given in 
bold. 
 
From Table 2, we notice that all the models ex-
cept Model_FN, are apt to handle the SMTeuroparl 
that involves long sentences. For Model_FN, it 
performs well in computing on short and similarly 
structured texts such as MSRvid (This will be con-
firmed in test data later). Although WordNet and 
FrameNet model has a mere weight of 20% in 
Model_LIM (i.e. ?1 +?2 = 0.2), the run which in-
tegrate more semantic information displays a con-
sistent performance across the three train sets (es-
pecially in SMTeuroparl, the Pearson correlation 
rises from 0.5178 to 0.66808), when compared to 
the other three. 
 
 MSRpar MSRvid SMTeuroparl OnWN SMTnews Mean 
Baseline 0.4334 0.2996 0.4542 0.5864 0.3908 0.4356
Model_WD 0.4404 0.5464 0.5059 0.6751 0.4583 0.5346
Model_WN 0.1247 0.6608 0.0637 0.4089 0.3436 0.3417
Model_FN 0.3830 0.6082 0.3537 0.6091 0.4061 0.4905
Model_LIM 0.4489 0.6301 0.5086 0.6841 0.4872 0.5631
UKP_run2 0.6830 0.8739 0.5280 0.6641 0.4937 0.6773
Table 3: Performances of our three models as well as 
the baseline and UKP_run2 (that is ranked 1 in last STS 
task) results on 2012 test data. The highest correlation in 
each column is given in bold. 
 
The 2012 STS test results obtained by first rank-
ing UKP_run2 and baseline system are shown in 
Table 3, it is interesting to notice that performance 
of Model_WD is similar with Model_LIM except 
on MSRvid, the text segments in which there are 
fewer identical words because of the semantic 
equivalence. For Model_FN, we can see it per-
forms well on short and similarly structured texts 
(MSRvid and OnWN) as mentioned before. This is 
because the precision of FrameNet parser took ef-
fect on the FrameNet-based models performance. 
Compared to UKP_run2, the performance of Mod-
el_LIM is obviously better on OnWN set, while on 
SMTeuroparl and SMTnews this model scores 
slightly lower than UKP_run2. Finally, Mod-
el_LIM did not perform best on MSRpar and 
MSRvid compared with UKP_run2, but it has low 
time complexity and integrates semantic informa-
tion. 
5.2 Official Results on STS 2013 Test Data 
Table 4 provides the official results of our submit-
ted systems, along with the rank on each dataset. 
Generally, all results outperform the baseline, 
based on simple word overlap. However, the per-
formance of Model_LIM is not always the best in 
the three runs for each dataset. From the table we 
can note that a particular model always performs 
well on the dataset including the lexicon on which 
the model is based on e.g. Model_WN in OnWN, 
Model_FN in FNWN. Besides, Model_WD and 
Model_LIM almost have same scores except in 
OnWN set, because in Model_LIM is included 
with WordNet resource. 
78
 
 headlines OnWN FNWN SMT Mean 
Baseline 0.5399 (66)  0.2828 (80) 0.2146 (66)  0.2861 (65) 0.3639 (73)
Model_WD 0.6806 (24)  0.5355 (44) 0.3181 (48)  0.3980 (4)  0.5198 (27)
Model_WN 0.4840 (78)  0.7146 (12) 0.0415 (83)  0.1543 (86) 0.3944 (69)
Model_FN 0.4881 (76)  0.6146 (27) 0.4237 (9)  0.3844 (6)  0.4797 (46)
Model_LIM 0.6761 (29)  0.6481 (23) 0.3025 (51)  0.4003 (3) 0.5458 (14)
Table 4: Performances of our systems as well as base-
line on STS 2013 individual test data, accompanied by 
their rank (out of 90) shown in brackets. Scores in bold 
denote significant improvements over the baseline. 
 
As seen from the system rank in table, the op-
timal runs in the three submitted system remain 
with Model_LIM. Not only Model_LIM performs 
best on two occasions, but also Model_FN ranks 
top ten twice, in FNWN and SMT respectively, we 
owe this result to the contribution of FrameNet 
parsing information. 
6 Conclusion 
We have tested all the models on published STS 
datasets. Compared with the official results, Mod-
el_LIM system is apt to handle the SMT that in-
volves long sentences. Moreover, this system just 
integrates words, WordNet and FrameNet semantic 
information, thus it has low time complexity. 
There is still much room for improvement in our 
work. For example, we will attempt to use multiva-
riate regression software to tuning the hyperpara-
meters. 
Acknowledgments 
This work is supported by the National Nature 
Science Foundation of China (No.60970053), by 
the National High-tech Research and Development 
Projects (863) grant No.2006AA01Z142, by the 
State Language Commission of China No.YB125-
19 as well as by the International Cooperation of 
Shanxi Province, Contracts 2010081044. And we 
would like to thank the organizer for the tremend-
ous effort they put into formulating this challeng-
ing work. 
References  
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gon-
zalez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on 
Semantic Textual Similarity. In Proceedings of the 
6th International Workshop on Semantic Evaluation, 
in conjunction with the 1st Joint Conference on Lexi-
cal and Computational Semantics, 385?393. 
Dekang Lin, Patrick Pantel. 2001. Discovery of Infe-
rence Rules for Question Answering. Natural Lan-
guage Engineering, 7(4):343-360. 
Vasileios Hatzivassiloglou, Judith L. Klavans, and 
Eleazar Eskin. 1999. Detecting Text Similarity over 
Short Passages: Exploring Linguistic Feature Combi-
nations via Machine Learning. In proceedings of the 
Joint SIGDAT Conference on Empirical Methods in 
Natural Language Processing and Very Large Cor-
pora, 224-231. 
Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 
2006. Corpus-based and Knowledge-based Measures 
of Text Semantic Similarity. In Proceedings of the 
National Conference on Artificial Intelligence, 21(1): 
775-780. 
Daniel B?r, Chris Biemann, Iryna Gurevych, and Tors-
ten Zesch. 2012. UKP: Computing Semantic Textual 
Similarity by Combining Multiple Content Similarity 
Measures. In Proceedings of the 6th International 
Workshop on Semantic Evaluation, in conjunction 
with the 1st Joint Conference on Lexical and Compu-
tational Semantics, 435-440. 
Frane ?ari?, Goran Glava?, Mladen Karan, Jan ?najder, 
and Bojana Dalbelo Ba?i?. 2012. TakeLab: Systems 
for Measuring Semantic Text Similarity. In Proceed-
ings of the 6th International Workshop on Semantic 
Evaluation, in conjunction with the 1st Joint Confe-
rence on Lexical and Computational Semantics, 441-
448. 
G. Salton, A. Wong, C.S. Yang. 1975. A Vector Space 
Model for Automatic Indexing. Communications of 
the ACM, 18(11):613-620. 
C. J. Fillmore, C. R. Johnson and M. R.L. Petruck. 2003. 
Background to FrameNet. International Journal of 
Lexicography, 16: 235-250. 
Dipanjan Das and Noah A. Smith. 2012. Graph-Based 
Lexicon Expansion with Sparsity-Inducing Penalties. 
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational 
Linguistics, 677-687. 
Dipanjan Das and Noah A. Smith. 2011. Semi-
Supervised Frame-Semantic Parsing for Unknown 
Predicates. In Proceedings of Annual Meeting of the 
Association for Computational Linguistics, 1435-
1444.  
Christiane Fellbaum. 1998. WordNet: An Electronic 
Lexical Database. MIT Press. 
Dekang Lin. 1998. An information-theoretic definition 
of similarity. In Proceedings of International Confe-
rence on Machine Learning, 296-340. 
79
