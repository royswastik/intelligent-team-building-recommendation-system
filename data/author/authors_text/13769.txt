Automatic Information Transfer Between English And Chinese 
Jianmin Yao, Hao Yu, Tiejun Zhao  
School of Computer Science and Technology 
Harbin Institute of Technology 
Harbin, China, 150001 
james@mtlab.hit.edu.cn 
Xiaohong Li 
Department of Foreign Studies 
Harbin Institute of Technology 
Harbin, China, 150001 
goodtreeyale@yahoo.com.cn 
 
Abstract  
The translation choice and transfer modules 
in an English Chinese machine translation 
system are introduced. The translation 
choice is realized on basis of a grammar tree 
and takes the context as a word bag, with the 
lexicon and POS tag information as context 
features. The Bayes minimal error 
probability is taken as the evaluation 
function of the candidate translation. The 
rule-based transfer and generation module 
takes the parsing tree as the input and 
operates on the information of POS tag, 
semantics or even the lexicon. 
Introduction 
Machine translation is urgently needed to get 
away with the language barrier between 
different nations. The task of machine 
translation is to realize mapping from one 
language to another. At present there are three 
main methods for machine translation systems 
[Zhao 2000]: 1) pattern/rule based systems: 
production rules compose the main body of the 
knowledge base. The rules or patterns are often 
manually written or automatically acquired from 
training corpus; 2) example based method. The 
knowledge base is a bilingual corpus of source 
slices S? and their translations T? Given a source 
slice of input S, match S with the source slices 
and choose the most similar as the translation or 
get the translation from it. 3) Statistics based 
method: it is a method based on monolingual 
language model and bilingual language model. 
The probabilities are acquired from large-scale 
(bilingual) corpora.  
Machine translation is more than a 
manipulation of one natural language (e.g. 
Chinese). Not only the grammatical and 
semantic characteristics of the source language 
must be considered, but also those of the target 
language. To sum up, the characteristics of 
bilingual translation is the essence of a machine 
translation system.  
A machine translation system usually 
includes 3 sub-systems [Zhao 1999] ? (1) 
Analysis: to analyse the source language 
sentence and generate a syntactic tree with 
syntactic functional tags; (2) Transfer: map a 
source parsing tree into a target language parsing 
tree; (3) Generation: generate the target 
language sentence according to the target 
language syntactic tree.  
The MTS2000 system developed in Harbin 
Institute of Technology is a bi-directional 
machine translation system based on a 
combination of stochastic and rule-based 
methods. Figure 1 shows the flow of the system.  
 Input English Sentence  
 Morphology Analysis  
 
Syntactic Analysis  
 
Word Translation Choice  
 
Transfer and Generation  
 
 
 
Output Chinese Sentence 
Figure 1 Flowchart of MTS2000 System  
Analysis and transfer are separated in the 
architecture of the MTS2000 system. This 
modularisation is helpful to the integration of 
stochastic method and the rule based method. 
New techniques are easier to be integrated into 
the modularised system. Two modules 
implement the transfer step and the generation 
step after analysis of the source sentence. The 
specific task of transfer and generation is to 
produce a target language sentence given the 
source language syntactic tree. In details, given 
an English syntactic tree (e.g. S[PP[ In/IN 
BNP[our/PRP$ workshop/NN]] BNP[ there/EX] 
VP[ is/VBZ NP[ no/DT NP[ NN[ machine/NN 
tool/NN] SBAR[ but/CC VP[ is/VBZ 
made/VBN PP[ in/IN BNP[ China/NNP ]]]]]]]]), 
using knowledge sources such as grammatical 
features, simple semantic features, construct a 
Chinese syntactic tree, whose terminal nodes 
compromise in sequence the Chinese translation.  
The input sentence are analysed using the 
morphology analyser, part-of-speech tagger, and 
syntactic analyser. After these steps, a syntactic 
parsing tree is obtained which has multiple 
levels with functional tags [Meng 2000]. 
Followed is the parser flow: 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Parser based on Hybrid Methods 
At present, our English parser is able to 
generate syntactic tree ble 
way. The English parsin
information about relatio
in the source sentence
information of the nodes
of transfer and generatio
the nodes is the starting
generation. After syntact
transfer and generation in
choice of ambiguous 
adjustment and inserti
functional words. Trans
implemented using two
word translation choice, 
transfer and translation m
1 Parsing Based Tran
First we will give a f
translation choice in 
[Manning 1999]: Suppose the source sentence to 
be translated to be ES. In the sentence the 
ambiguous word EW has M target translations 
CW1, CW2, ... CWM. And the translations 
occurs in a specific context C with probabilities 
P(CW1 | C)?P(CW2 | C), ... P(CWM|C)?From 
the Bayes minimum error probability formula, 
we get: 
CW = argmax[P(CWk|C)] 
= argmax[logP(CWk) + logP(C|CWk) ] (1) 
Generally when the condition fulfills 
P(CW1|C)>P(CW2|C)>...>P(CWM|C), we may 
choose CW1 as the translation for EW. From the 
Na?ve Bayes formula? 
P(C|CWk) = P({vj | vj in C}|CWk) 
 = ?Vj in C P(vj|sk)              (2) 
So formula (1) can be rewritten as: 
CW = argmax[P(CWk|C)] 
Input Sentence Statistics Knowledge = argmax[logP(CWk)+?Vj in ClogP(vj|CWk)] (3) 
Where P(CWk) denotes the probability that 
CWk occurs in the corpus; P (vj| CWk) denotes 
the probability that the context feature vj 
co-occurs with translation CWk? 
A general algorithm of supervised word 
sense disambiguation is as follows: 
1. comment: Training 
2. for all senses sk of w do 
3.    for all words vj in the vocabulary do 
4.       P(vj|sk) = C(vj, sk)/C(vj) 
5.    end 
6. end 
7. for all senses sk of w do 
8.    P(sk) = C(sk)/C(w) 
9. end 
POS Tagger Manual Rule 
Base 
PPA Resolution 
Layered Parsing eein comparative usa
g 
nsh
, a
, is
n. 
 p
ic 
clu
w
on
fer 
 m
the
od
sla
orm
mParsing Trtree, with the basic 
ip among the nodes 
lso with semantic 
 input to the module 
The information of 
oint of transfer and 
parsing, the task of 
des word translation 
ords, word order 
/deletion of some 
and generation are 
odules: one is for 
 other for structure 
ification. 
tion Choice 
al description for 
achine translation 
10. comment: Disambiguation 
11. for all sense sk of w do 
12.    Score(sk) = logP(sk) 
13.    for all words vj in the context window c do
14.       score(sk) = score(sk) + logP(vj|sk) 
15.    end 
16. end 
17. choose s? = argmaxskscore(sk) 
Figure 5. Bayesian disambiguation 
From the above formal description we can 
see that the key to the stochastic word 
translation is to select proper context and context 
features Vj. Present methods often define a word 
window of some size, i.e. to suppose only words 
within the window contributes to the translation 
choice of the ambiguous word. For example, 
[Huang 1997] uses a word window of length 6 
words for word sense disambiguation; [Xun 
1998] define a moveable window of length 4 
words; [Ng 1997] uses a word window with 
offset ?2. But two problems exist for this 
method: (1) some words that are informative to 
sense disambiguation may not be covered by the 
window; (2) some words that are covered by the 
word window really contribute nothing to the 
sense choice, but only bring noise information. 
After a broad investigation for large-scale 
ambiguous words, we choose the context 
according to the correlation of the context words 
with the ambiguous word, but not only the 
distance from the word. 
From the above analysis, we choose the 
translation choice method based on syntactic 
analysis. Place the module of translation choice 
between the parser and the generator; acquire a 
context set for the ambiguous word. When 
choosing the translation, we may take the 
context set as a word bag, i.e. the grammatical 
context as word bag. No single word is 
considered but only that lexical and 
part-of-speech information are taken as context 
features. Bayes minimum error probability is 
taken as evaluation function for word translation 
choice. 
In this paper, grammatical context is 
considered for word translation choice. The 
structure related features of the ambiguous 
words are taken into account for fully use of the 
parsing result. It has the characteristics below: (1) 
The window size is not defined by human but on 
basis of the grammatical structure of the 
sentence, so we can acquire more efficiently the 
useful context features; (2) The unrelated 
context features in sentence structure are filtered 
out for translation choice; (3) The features are 
based on the structure relationship, but not 100% 
right parsing result. From the above 
characteristics, we can see the method is really 
practical. 
2 Rule Based Transfer & Generation 
For MTS2000, structural transfer is to start from 
the syntactic parsing tree and construct the 
Chinese syntactic tree. While the generation of 
Chinese is to generate a word link from the 
Chinese tree and build the translation sentence 
[Yao 2001]. This module has adopted the 
rule-based knowledge representation method. 
The design of the rule system is highly related to 
the performance of the machine translation 
system. 
The rule description language of the 
machine translation system is in the form of 
production rules, i.e. a rule composed of a 
conditional part and an operational part. The 
conditional part is a scan window of variable 
length, which uses the context constraint 
conditions such as phrases or some linguistic 
features. The operational part generates the 
corresponding translation or some corresponding 
generation features in the operational part. If the 
conditions are met, the operations will be 
performed. The representation of the rule system 
has shown a characteristic of the system, that is 
the integration of transfer and generation. The 
rule description language is similar to natural 
language and consistent with human habits. 
Multiple description methods are implemented. 
The conditional part of the rules is 
composed of node numbers and ?+? symbols 
that is used to link the nodes. The operation part 
consists of corresponding conditional parts and 
translations and also, if necessary, some action 
functions. 
For example, the rule to combine an 
adjective and a noun to generate a noun phrase is 
as follows:  
0:Cate=A + 1:Cate=N 
->0:* + 1:* + _NodeUf(N?0?1) 
in which, ?*? stands for corresponding 
translation of the nodes, _NodeUf() is a function 
that combines the nodes to generate a new node. 
The new translation is generated at the same 
time with the combination of nodes. 
In general, the English Chinese machine 
translation system has the following features in 
the transfer and generation phase: 
1) The grammatical and semantic features are 
described by a string composed of frame 
name and values linked with ?=?; 
2) The conditions may be operated by ?and?, 
?or? and ?not?; 
3) Nodes in the same level of the sentence may 
be scanned and tested arbitrarily; 
4) The action functions and test functions can 
generate corresponding features for feature 
transmission and test. 
The rules are organized into various levels. 
All the rules are put in the knowledge base with 
part-of-speech as the entry feature. The rules 
have different priorities, which decide their 
sequence in rule matching. In general, the more 
specific the rule, the higher is its priority. The 
more general the rule, the lower is its priority. 
The levels of the rules help resolve rule 
collision. 
Conclusion 
The system prototype has been implemented and 
large-scale development and refinement are 
under progress. From our knowledge of the 
system, knowledge acquisition and rule base 
organization is the bottleneck for MTS2000 
system and similar natural language processing 
systems. The knowledge acquisition for word 
translation choice needs large-scale word 
aligned bilingual corpus. We are making 
research on new word translation methods on 
basis of our 60,000-sentence aligned bilingual 
corpus. The transfer and generation knowledge 
base are facing much knowledge collision and 
redundancy problem. The organization 
technique of knowledge base is also an 
important issue in the project. 
References  
Tie-Jun Zhao, En-Dong Xun, Bin Chen, Xiao-Hu 
Liu,Sheng Li, Research on Word Sense 
Disambiguation based on Target Language 
Statistics, Applied Fundamental and Engineering 
Journal, 1999?7?1??101-110 
Meng Yao, Zhao Tiejun, Yu Hao, Li Sheng, A 
Decision Tree Based Corpus Approach to English 
Base Noun Phrase Identification, Proceedings 
International conference on East-Asian Language 
Processing and Internet Information Technology, 
Shenyang, 2000: 5-10 
Christopher D. Manning, Hinrich Sch ? tze, 
Foundation of Statistical Natural Language 
Processing. The MIT Press. pp229-262. 1999. 
Chang-Ning Huang, Juan-Zi Li, A language model 
for word sense disambiguation, 10th anniversary 
for Chinese Linguistic Society, October, 1997, 
Fuzhou  
En-Dong Xun, Sheng Li, Tie-Jun Zhao, Bi-gram 
co-occurrence based stochastic method for word 
sense disambiguation, High Technologies, 1998, 
10(8): 21-25  
Hwee Tou Ng. Exemplar-Based Word Sense 
Disambiguation: Some Recent Improvements. In 
Proceedings of the Second Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP-2), August 1997  
Tie-Jun Zhao etc, Principle of Machine Translation, 
Press of Harbin Institute of Technology, 2000. 
Jian-Min Yao, Jing Zhang, Hao Yu, Tie-Jun 
Zhao,Sheng Li, Transfer from an English parsing 
tree to a Chinese syntactic tree, Joint Conference of 
the Society of Computational Linguistics, 2001, 
Taiyuan.-138.  
  
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358?366,
Beijing, August 2010
A Large Scale Ranker-Based System  
for Search Query Spelling Correction 
 
Jianfeng Gao 
Microsoft Research, Redmond 
jfgao@microsoft.com 
Xiaolong Li 
Microsoft Corporation 
xiaolong.li@microsoft.com 
Daniel Micol 
Microsoft Corporation 
danielmi@microsoft.com 
Chris Quirk 
Microsoft Research, Redmond 
chrisq@microsoft.com 
Xu Sun 
University of Tokyo 
xusun@mist.i.u-tokyo.ac.jp 
 
 
Abstract 
This paper makes three significant extensions to a 
noisy channel speller designed for standard writ-
ten text to target the challenging domain of search 
queries. First, the noisy channel model is sub-
sumed by a more general ranker, which allows a 
variety of features to be easily incorporated. Se-
cond, a distributed infrastructure is proposed for 
training and applying Web scale n-gram language 
models. Third, a new phrase-based error model is 
presented. This model places a probability distri-
bution over transformations between multi-word 
phrases, and is estimated using large amounts of 
query-correction pairs derived from search logs. 
Experiments show that each of these extensions 
leads to significant improvements over the state-
of-the-art baseline methods. 
1 Introduction 
Search queries present a particular challenge for 
traditional spelling correction methods. New 
search queries emerge constantly. As a result, 
many queries contain valid search terms, such as 
proper nouns and names, which are not well es-
tablished in the language. Therefore, recent re-
search has focused on the use of Web corpora 
and search logs, rather than human-compiled lex-
icons, to infer knowledge about spellings and 
word usages in search queries (e.g., Whitelaw et 
al., 2009; Cucerzan and Brill, 2004).  
The spelling correction problem is typically 
formulated under the framework of the noisy 
channel model. Given an input query   
       , we want to find the best spelling correc-
tion           among all candidates: 
         
 
       (1) 
Applying Bayes' Rule, we have 
         
 
           (2) 
where the error model        models the trans-
formation probability from C to Q, and the lan-
guage model (LM)      models the likelihood 
that C is a correctly spelled query. 
This paper extends a noisy channel speller de-
signed for regular text to search queries in three 
ways: using a ranker (Section 3), using Web scale 
LMs (Section 4), and using phrase-based error 
models (Section 5). 
First of all, we propose a ranker-based speller 
that covers the noisy channel model as a special 
case. Given an input query, the system first gen-
erates a short list of candidate corrections using 
the noisy channel model. Then a feature vector is 
computed for each query and candidate correc-
tion pair. Finally, a ranker maps the feature vec-
tor to a real-valued score, indicating the likeli-
hood that this candidate is a desirable correction. 
We will demonstrate that ranking provides a flex-
ible modeling framework for incorporating a 
wide variety of features that would be difficult to 
model under the noisy channel framework. 
Second, we explore the use of Web scale LMs 
for query spelling correction. While traditional 
LM research focuses on how to make the model 
?smarter? via how to better estimate the probabil-
ity of unseen words (Chen and Goodman, 1999); 
and how to model the grammatical structure of 
language (e.g., Charniak, 2001), recent studies 
show that significant improvements can be 
achieved using ?stupid? n-gram models trained 
on very large corpora (e.g., Brants et al, 2007). 
We adopt the latter strategy in this study. We pre-
sent a distributed infrastructure to efficiently train 
and apply Web scale LMs. In addition, we ob-
serve that search queries are composed in a lan-
guage style different from that of regular text. We 
thus train multiple LMs using different texts as-
sociated with Web corpora and search queries. 
Third, we propose a phrase-based error model 
that captures the probability of transforming one 
358
multi-term phrase into another multi-term phrase. 
Compared to traditional error models that account 
for transformation probabilities between single 
characters or substrings (e.g., Kernighan et al, 
1990; Brill and Moore, 2000), the phrase-based 
error model is more effective in that it captures 
inter-term dependencies crucial for correcting 
real-word errors, prevalent in search queries. We 
also present a novel method of extracting large 
amounts of query-correction pairs from search 
logs. These pairs, implicitly judged by millions of 
users, are used for training the error models. 
Experiments show that each of the extensions 
leads to significant improvements over its base-
line methods that were state-of-the-art until this 
work, and that the combined method yields a sys-
tem which outperforms the noisy channel speller 
by a large margin: a 6.3% increase in accuracy on 
a human-labeled query set. 
2 Related Work 
Prior research on spelling correction for regular 
text can be grouped into two categories: correct-
ing non-word errors and real-word errors. The 
former focuses on the development of error mod-
els based on different edit distance functions (e.g., 
Kucich, 1992; Kernighan et al, 1990; Brill and 
Moore, 2000; Toutanova and Moore, 2002). Brill 
and Moore?s substring-based error model, con-
sidered to be state-of-the-art among these models, 
acts as the baseline against which we compare 
our models. On the other hand, real-word spelling 
correction tries to detect incorrect usages of a 
valid word based on its context, such as "peace" 
and "piece" in the context "a _ of cake". N-gram 
LMs and na?ve Bayes classifiers are commonly 
used models (e.g., Golding and Roth, 1996; 
Mangu and Brill, 1997; Church et al, 2007). 
While almost all of the spellers mentioned 
above are based on a pre-defined dictionary (ei-
ther a lexicon against which the edit distance is 
computed, or a set of real-word confusion pairs), 
recent research on query spelling correction fo-
cuses on exploiting noisy Web corpora and query 
logs to infer knowledge about spellings and word 
usag in queries (Cucerzan and Brill 2004; Ahmad 
and Kondrak, 2005; Li et al, 2006; Whitelaw et 
al., 2009).  Like those spellers designed for regu-
lar text, most of these query spelling systems are 
also based on the noisy channel framework. 
3 A Ranker-Based Speller 
The noisy channel model of Equation (2) does 
not have the flexibility to incorporate a wide va-
riety of features useful for spelling correction, 
e.g., whether a candidate appears as a Wikipedia 
document title. We thus generalize the speller to 
a ranker-based system. Let f be a feature vector 
of a query and candidate correction pair (Q, C). 
The ranker maps f to a real value y that indicates 
how likely C is a desired correction. For example, 
a linear ranker maps f to y with a weight vector w 
such as      , where w is optimized for accu-
racy on human-labeled       pairs. Since the 
logarithms of the LM and error model probabili-
ties can be included as features, the ranker covers 
the noisy channel model as a special case. 
For efficiency, our speller operates in two dis-
tinct stages: candidate generation and re-ranking. 
In candidate generation, an input query is first 
tokenized into a sequence of terms. For each term 
q, we consult a lexicon to identify a list of 
spelling suggestions c whose edit distance from q 
is lower than some threshold. Our lexicon con-
tains around 430,000 high frequency query uni-
gram and bigrams collected from 1 year of query 
logs. These suggestions are stored in a lattice.  
We then use a decoder to identify the 20-best 
candidates from the lattice according to Equation 
(2), where the LM is a backoff bigram model 
trained on 1 year of query logs, and the error 
model is approximated by weighted edit distance:  
                         (3) 
The decoder uses a standard two-pass algorithm. 
The first pass uses the Viterbi algorithm to find 
the best C according to the model of Equations 
(2) and (3).  The second pass uses the A-star al-
gorithm to find the 20-best corrections, using the 
Viterbi scores computed at each state in the first 
pass as heuristics. 
The core component in the second stage is a 
ranker, which re-ranks the 20-best candidate cor-
rections using a set of features extracted from 
     . If the top C after re-ranking is different 
from Q, C is proposed as the correction. We use 
96 features in this study. In addition to the two 
features derived from the noisy channel model, 
the rest of the features can be grouped into the 
following 5 categories. 
1. Surface-form similarity features, which 
check whether C and Q differ in certain patterns, 
359
e.g., whether C is transformed from Q by adding 
an apostrophe, or by adding a stop word at the 
beginning or end of Q. 
2. Phonetic-form similarity features, which 
check whether the edit distance between the met-
aphones (Philips, 1990) of a query term and its 
correction candidate is below some thresholds. 
3. Entity features, which check whether the 
original query is likely to be a proper noun based 
on an in-house named entity recognizer. 
4. Dictionary features, which check whether 
a query term or a candidate correction are in one 
or more human-compiled dictionaries, such as the 
extracted Wiki, MSDN, and ODP dictionaries. 
5. Frequency features, which check whether 
the frequency of a query term or a candidate cor-
rection is above certain thresholds in different 
datasets, such as query logs and Web documents. 
4 Web Scale Language Models 
An n-gram LM assigns a probability to a word 
string   
            according to  
    
   ? (  |  
   )
 
   
 ? (  |      
   )
 
   
 (4) 
where the approximation is based on a Markov 
assumption that each word depends only upon the 
immediately preceding n-1 words. In a speller, 
the log of n-gram LM probabilities of an original 
query and its candidate corrections are used as 
features in the ranker.  
While recent research reports the benefits of 
large LMs trained on Web corpora on a variety of 
applications (e.g. Zhang et al, 2006; Brants et al, 
2007), it is also clear that search queries are com-
posed in a language style different from that of 
the body or title of a Web document. Thus, in this 
study we developed a set of large LMs from dif-
ferent text streams of Web documents and query 
logs. Below, we first describe the n-gram LM 
collection used in this study, and then present a 
distributed n-gram LM platform based on which 
these LMs are built and served for the speller. 
4.1 Web Scale Language Models 
Table 1 summarizes the data sets and Web scale 
n-gram LMs used in this study. The collection is 
built from high quality English Web documents 
containing trillions of tokens, served by a popular 
commercial search engine. The collection con-
sists of several data sets built from different Web 
sources, including the different text fields from 
the Web documents (i.e., body, title, and anchor 
texts) and search query logs. The raw texts ex-
tracted from these different sources were pre- 
processed in the following manner: texts are to-
kenized based on white-space and upper case let-
ters are converted to lower case. Numbers are 
retained, and no stemming/inflection is per-
formed. The n-gram LMs are word-based backoff 
models, where the n-gram probabilities are esti-
mated using Maximum Likelihood Estimation 
with smoothing. Specifically, for a trigram mod-
el, the smoothed probability is computed as 
                (5) 
{
               (             )
           
                   
                              
 
where      is the count of the n-gram in the train-
ing corpus and   is a normalization factor.      
is a discount function for smoothing. We use 
modified absolute discounting (Gao et al, 2001), 
whose parameters can be efficiently estimated 
and performance converges to that of more elabo-
rate state-of-the-art techniques like Kneser-Ney 
smoothing in large data (Nguyen et al 2007).  
4.2 Distributed N-gram LM Platform 
The platform is developed on a distributed com-
puting system designed for storing and analyzing 
massive data sets, running on large clusters con-
sisting of hundreds of commodity servers con-
nected via high-bandwidth network.  
We use the SCOPE (Structured Computations 
Optimized for Parallel Execution) programming 
model (Chaiken et al, 2008) to train the Web 
scale n-gram LMs shown in Table 1. The SCOPE 
scripting language resembles SQL which many 
programmers are familiar with. It also supports 
Dataset Body Anchor Title Query 
Total tokens 1.3T 11.0B 257.2B 28.1B 
Unigrams 1.2B 60.3M 150M 251.5M 
Bigrams 11.7B 464.1M 1.1B 1.3B 
Trigrams 60.0B 1.4B 3.1B 3.1B 
4-grams 148.5B 2.3B 5.1B 4.6B 
Size on disk# 12.8TB 183GB 395GB 393GB 
# N-gram entries as well as other model parameters are 
stored. 
Table 1: Statistics of the Web n-gram LMs collection (count 
cutoff = 0 for all models). These models will be accessible at 
Microsoft (2010). 
360
C# expressions so that users can easily plug-in 
customized C# classes. SCOPE supports writing 
a program using a series of simple data transfor-
mations so that users can simply write a script to 
process data in a serial manner without wonder-
ing how to achieve parallelism while the SCOPE 
compiler and optimizer are responsible for trans-
lating the script into an efficient, parallel execu-
tion plan. We illustrate the usage of SCOPE for 
building LMs using the following example of 
counting 5-grams from the body text of English 
Web pages. The flowchart is shown in Figure 1.  
The program is written in SCOPE as a step-
by- step of computation, where a command takes 
the output of the previous command as its input. 
ParsedDoc=SELECT docId, TokenizedDoc 
FROM @?/shares/?/EN_Body.txt? 
USING DefaultTextExtractor; 
NGram=PROCESS ParsedDoc 
PRODUCE NGram, NGcount 
USING NGramCountProcessor(-stream       
TokenizedDoc -order 5 ?bufferSize 
20000000); 
NGramCount=REDUCE NGram 
ON NGram 
PRODUCE NGram, NGcount 
USING NGramCountReducer; 
 
OUTPUT TO @?Body-5-gram-count.txt?; 
The first SCOPE command is a SELECT 
statement that extracts parsed Wed body text. The 
second command uses a build-in Processor 
(NGramCountProcessor) to map the parsed doc-
uments into separate n-grams together with their 
counts. It generates a local hash at each node 
(i.e., a core in a multi-core server) to store the (n-
gram, count) pairs. The third command (RE-
DUCE) aggregates counts from different nodes 
according to the key (n-gram string). The final 
command (OUTPUT) writes out the resulting to a 
data file. 
The smoothing method can be implemented 
similarly by the customized smoothing Proces-
sor/Reducer. They can be imported from the ex-
isting C# codes (e.g., developed for building LMs 
in a single machine) with minor changes.  
It is straightforward to apply the built LMs for 
the ranker in the speller. The n-gram platform 
provides a DLL for n-gram batch lookup. In the 
server, an n-gram LM is stored in the form of 
multiple lists of key-value pairs, where the key is 
the hash of an n-gram string and the value is ei-
ther the n-gram probability or backoff parameter.  
5 Phrase-Based Error Models 
The goal of an error model is to transform a cor-
rectly spelled query C into a misspelled query Q. 
Rather than replacing single words in isolation, 
the phrase-based error model replaces sequences 
of words with sequences of words, thus incorpo-
rating contextual information. The training pro-
cedure closely follows Sun et al (2010). For in-
stance, we might learn that ?theme part? can be 
replaced by ?theme park? with relatively high 
probability, even though ?part? is not a mis-
spelled word. We use this generative story: first 
the correctly spelled query C is broken into K 
non-empty word sequences c1, ?, ck, then each is 
replaced with a new non-empty word sequence 
q1, ?, qk, finally these phrases are permuted and 
concatenated to form the misspelled Q. Here, c 
and q denote consecutive sequences of words. 
To formalize this generative process, let S de-
note the segmentation of C into K phrases c1?cK, 
and let T denote the K replacement phrases 
q1?qK ? we refer to these (ci, qi) pairs as bi-
phrases. Finally, let M denote a permutation of K 
elements representing the final reordering step. 
Figure 2 demonstrates the generative procedure. 
Next let us place a probability distribution 
over rewrite pairs. Let B(C, Q) denote the set of S, 
T, M triples that transform C into Q. Assuming a 
uniform probability over segmentations, the 
phrase-based probability can be defined as: 
Recursive 
Reducer
Node 1 Node 2 Node N?...
?...
Output
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
 
Figure 1. Distributed 5-gram counting. 
C: ?disney theme park? correct query 
S: [?disney?, ?theme park?] segmentation 
T: [?disnee?, ?theme part?] translation 
M: (1 ? 2, 2? 1) permutation 
Q: ?theme part disnee? misspelled query 
Figure 2: Example demonstrating the generative procedure 
behind the phrase-based error model. 
361
       ?                    
            
 (6) 
As is common practice in SMT, we use the max-
imum approximation to the sum:  
          
            
                    (7) 
5.1 Forced Alignments 
Although we have defined a generative model for 
transforming queries, our goal is not to propose 
new queries, but rather to provide scores over 
existing Q and C pairs that will act as features for 
the ranker. Furthermore, the word-level align-
ments between Q and C can most often be identi-
fied with little ambiguity. Thus we restrict our 
attention to those phrase transformations con-
sistent with a good word-level alignment. 
Let J be the length of Q, L be the length of C, 
and A = a1?aJ  be a hidden variable representing 
the word alignment between them. Each ai takes 
on a value ranging from 1 to L indicating its cor-
responding word position in C, or 0 if the ith 
word in Q is unaligned. The cost of assigning k 
to ai is equal to the Levenshtein edit distance 
(Levenshtein, 1966) between the ith word in Q 
and the kth word in C, and the cost of assigning 0 
to ai is equal to the length of the i
th word in Q. 
The least cost alignment A* between Q and C is 
computed efficiently using the A-star algorithm. 
When scoring a given candidate pair, we fur-
ther restrict our attention to those S, T, M triples 
that are consistent with the word alignment, 
which we denote as B(C, Q, A*). Here, consisten-
cy requires that if two words are aligned in A*, 
then they must appear in the same bi-phrase (ci, 
qi). Once the word alignment is fixed, the final 
permutation is uniquely determined, so we can 
safely discard that factor. Thus we have: 
          
       
       
         (8) 
For the sole remaining factor P(T|C, S), we 
make the assumption that a segmented query T = 
q1? qK is generated from left to right by trans-
forming each phrase c1?cK independently: 
         ?         
 
   , (9) 
where          is a phrase transformation prob-
ability, the estimation of which will be described 
in Section 5.2.  
To find the maximum probability assignment 
efficiently, we use a dynamic programming ap-
proach, similar to the monotone decoding algo-
rithm described in Och (2002).  
5.2 Training the Error Model  
Given a set of (Q, C) pairs as training data, we 
follow a method commonly used in SMT (Och 
and Ney, 2004) to extract bi- phrases and esti-
mate their replacement probabilities. A detailed 
description is discussed in Sun et al (2010). 
We now describe how (Q, C) pairs are gener-
ated automatically from massive query reformu-
lation sessions of a commercial Web browser. 
A query reformulation session contains a list 
of URLs that record user behaviors that relate to 
the query reformulation functions, provided by a 
Web search engine. For example, most commer-
cial search engines offer the "did you mean" 
function, suggesting a possible alternate interpre-
tation or spelling of a user-issued query. Figure 3 
shows a sample of the query reformulation ses-
sions that record the "did you mean" sessions 
from three of the most popular search engines. 
These sessions encode the same user behavior: A 
user first queries for "harrypotter sheme part", 
Google: 
http://www.google.com/search? 
hl=en&source=hp& 
q=harrypotter+sheme+part&aq=f&oq=&aqi= 
http://www.google.com/search? 
hl=en&ei=rnNAS8-oKsWe_AaB2eHlCA& 
sa=X&oi=spell&resnum=0&ct= 
result&cd=1&ved=0CA4QBSgA& 
q=harry+potter+theme+park&spell=1 
Yahoo: 
http://search.yahoo.com/search; 
_ylt=A0geu6ywckBL_XIBSDtXNyoA? 
p=harrypotter+sheme+part& 
fr2=sb-top&fr=yfp-t-701&sao=1 
http://search.yahoo.com/search? 
ei=UTF-8&fr=yfp-t-701& 
p=harry+potter+theme+park 
&SpellState=n-2672070758_q-tsI55N6srhZa. 
qORA0MuawAAAA%40%40&fr2=sp-top 
Bing: 
http://www.bing.com/search? 
q=harrypotter+sheme+part&form=QBRE&qs=n 
http://www.bing.com/search? 
q=harry+potter+theme+park&FORM=SSRE 
Figure 3.  A sample of query reformulation sessions from 3 
popular search engines. These sessions show that a user first 
issues the query "harrypotter sheme part", and then clicks on 
the resulting spell suggestion "harry potter theme park". 
362
and then clicks on the resulting spelling sugges-
tion "harry potter theme park". We can "reverse-
engineer" the parameters from the URLs of these 
sessions, and deduce how each search engine en-
codes both a query and the fact that a user arrived 
at a URL by clicking on the spelling suggestion 
of the query ? an strong indication that the 
spelling suggestion is desired. In this study, from 
1 year of sessions, we extracted ~120 million 
pairs. We found the data set very clean because 
these spelling corrections are actually clicked, 
and thus judged implicitly, by many users. 
In addition to the "did you mean" functionali-
ty, recently some search engines have introduced 
two new spelling suggestion functions. One is the 
"auto-correction" function, where the search en-
gine is confident enough to automatically apply 
the spelling correction to the query and execute it 
to produce search results. The other is the "split 
pane" result page, where one half portion of the 
search results are produced using the original 
query, while the other half, usually visually sepa-
rate portion of results, are produced using the 
auto-corrected query. 
In neither of these functions does the user ever 
receive an opportunity to approve or disapprove 
of the correction. Since our extraction approach 
focuses on user-approved spelling suggestions, 
we ignore the query reformulation sessions re-
cording either of the two functions. Although by 
doing so we could miss some basic, obvious 
spelling corrections, our experiments show that 
the negative impact on error model training is 
negligible. One possible reason is that our base-
line system, which does not use any error model 
learned from the session data, is already able to 
correct these basic, obvious spelling mistakes. 
Thus, including these data for training is unlikely 
to bring any further improvement. 
We found that the error models trained using 
the data directly extracted from the query refor-
mulation sessions suffer from the problem of un-
derestimating the self-transformation probability 
of a query P(Q2=Q1|Q1), because we only includ-
ed in the training data the pairs where the query is 
different from the correction. To deal with this 
problem, we augmented the training data by in-
cluding correctly spelled queries, i.e., the pairs 
(Q1, Q2) where Q1 = Q2.  First, we extracted a set 
of queries from the sessions where no spell sug-
gestion is presented or clicked on. Second, we 
removed from the set those queries that were rec-
ognized as being auto-corrected by a search en-
gine. We do so by running a sanity check of the 
queries against our baseline noisy channel 
speller, which will be described in Section 6. If 
the system consider a query misspelled, we as-
sumed it an obvious misspelling, and removed it. 
The remaining queries were assumed to be cor-
rectly spelled and were added to the training data. 
6 Experiments 
We perform the evaluation using a manually an-
notated data set containing 24,172 queries sam-
pled from one year?s query logs from a commer-
cial search engine. The spelling of each query is 
manually corrected by four independent annota-
tors. The average length of queries in the data 
sets is 2.7 words. We divided the data set into 
non-overlapped training and test data sets. The 
training data contain 8,515       pairs, among 
which 1,743 queries are misspelled (i.e.    ). 
The test data contain 15,657       pairs, among 
which 2,960 queries are misspelled.  
The speller systems we developed in this 
study are evaluated using the following metrics. 
? Accuracy: The number of correct outputs 
generated by the system divided by the total 
number of queries in the test set. 
? Precision: The number of correct spelling 
corrections for misspelled queries generated 
by the system divided by the total number of 
corrections generated by the system. 
? Recall: The number of correct spelling cor-
rections for misspelled queries generated by 
the system divided by the total number of 
misspelled queries in the test set. 
We also perform a significance test, a t-test 
with a significance level of 0.05. 
In our experiments, all the speller systems are 
ranker-based. Unless otherwise stated, the ranker 
is a two-layer neural net with 5 hidden nodes. 
The free parameters of the neural net are trained 
to optimize accuracy on the training data using 
the back propagation algorithm (Burges et al, 
2005) .  
6.1 System Results 
Table 1 summarizes the main results of different 
spelling systems. Row 1 is the baseline speller 
where the noisy channel model of Equations (2) 
363
and (3) is used. The error model is based on the 
weighted edit distance function and the LM is a 
backoff bigram model trained on 1 year of query 
logs, with count cutoff 30. Row 2 is the speller 
using a linear ranker to incorporate all ranking 
features described in Section 3. The weights of 
the linear ranker are optimized using the Aver-
aged Perceptron algorithm (Freund and Schapire, 
1999). Row 3 is the speller where a nonlinear 
ranker (i.e., 2-layer neural net) is trained atop the 
features. Rows 4, 5 and 6 are systems that incor-
porate the additional features derived from the 
phrase-based error model (PBEM) described in 
Section 5 and the four Web scale LMs (WLMs) 
listed in Table 1. 
The results show that (1) the ranker is a very 
flexible modeling framework where a variety of 
fine-grained features can be easily incorporated, 
and a ranker-based speller outperforms signifi-
cantly (p < 0.01) the traditional system based on 
the noisy channel model (Row 2 vs. Row 1); (2) 
the speller accuracy can be further improved by 
using more sophisticated rankers and learning 
algorithms (Row 3 vs. Row 2); (3) both WLMs 
and PBEM bring significant improvements 
(Rows 4 and 5 vs. Row 3); and (4) interestingly, 
the gains from WLMs and PBEM are additive 
and the combined leads to a significantly better 
speller (Row 6 vs. Rows 4 and 5) than that of 
using either of them individually. 
In what follows, we investigate in detail how 
the WLMs and PBEM trained on massive Web 
content and search logs improve the accuracy of 
the speller system. We will compare our models 
with the state-of-the-art models proposed previ-
ously. From now on, the system listed in Row 3 
of Table 1 will be used as baseline. 
6.2 Language Models 
The quality of n-gram LMs depends on the order 
of the model, the size of the training data, and 
how well the training data match the test data. 
Figure 4 illustrates the perplexity results of the 
four LMs trained on different data sources tested 
on a random sample of 733,147 queries. The re-
sults show that (1) higher order LMs produce 
lower perplexities, especially when moving be-
yond unigram models; (2) as expected, the query 
LMs are most predictive for the test queries, 
though they are from independent query log 
snapshots; (3) although the body LMs are trained 
on much larger amounts of data than the title and 
anchor LMs, the former lead to much higher per-
plexity values, indicating that both title and an-
chor texts are quantitatively much more similar to 
queries than body texts. 
Table 2 summarizes the spelling results using 
different LMs. For comparison, we also built a 4-
gram LM using the Google 1T web 5-gram cor-
pus (Brants and Franz, 2006). This model is re-
ferred to as the G1T model, and is trained using 
the ?stupid backoff? smoothing method (Brants et 
al., 2007). Due to the high count cutoff applied 
by the Google corpus (i.e., n-grams must appear 
at least 40 times to be included in the corpus), we 
found the G1T model results to a higher OOV 
rate (i.e., 6.5%) on our test data than that of the 4 
Web scale LMs (i.e., less than 1%). 
The results in Table 2 are more or less con-
sistent with the perplexity results: the query LM 
is the best performer; there is no significant dif-
ference among the body, title and anchor LMs 
though the body LM is trained on a much larger 
amount of data; and all the 4 Web scale LMs out-
perform the G1T model substantially due to the 
significantly lower OOV rates. 
6.3 Error Models 
This section compares the phrase-based error 
model (PBEM) described in Section 5, with one 
of the state-of-the-art error models, proposed by 
Brill and Moore (2000), henceforth referred to as 
# System Accuracy Precision Recall 
1 Noisy channel 85.3 72.1 35.9 
2 Linear ranker 88.0 74.0 42.8 
3 Nonlinear ranker 89.0 74.1 49.6 
4 3 + PBEM 90.7 78.7 58.2 
5 3 + WLMs 90.4 75.1 58.7 
6 3 + PBEM + WLMs  91.6 79.1 63.9 
Table 1. Summary of spelling correction results. 
 
Figure 4. Perplexity results on test queries, using n-
gram LMs with different orders, derived from differ-
ent data sources. 
 
364
the B&M model. B&M is a substring error mod-
el. It estimates        as 
          
    
           
?        
   
   
  (10) 
where R is a partitioning of correction term c into 
adjacent substrings, and T is a partitioning of 
query term q, such that |T|=|R|. The partitions are 
thus in one-to-one alignment. To train the B&M 
model, we extracted 1 billion term-correction 
pairs       from the set of 120 million query-
correction pairs      , derived from the search 
logs as described in Section 5.2.  
Table 3 summarizes the comparison results. 
Rows 1 and 2 are our ranker-based baseline sys-
tems with and without the error model (EM) fea-
ture. The error model is based on weighted edit 
distance of Eq. (3), where the weights are learned 
on some manually annotated word-correction 
pairs (which is not used in this study). Rows 3 
and 4 are the B&M models using different maxi-
mum substring lengths, specified by L. L=1 re-
duces B&M to the weighted edit distance model 
in Row 2. Rows 5 and 6 are PBEMs with differ-
ent maximum phrase lengths. L=1 reduces PBEM 
to a word-based error model. The results show 
the benefits of capturing context information in 
error models. In particular, the significant im-
provements resulting from PBEM demonstrate 
that the dependencies between words are far 
more effective than that between characters 
(within a word) for spelling correction. This is 
largely due to the fact that there are many real-
word spelling errors in search queries. We also 
notice that PBEM is a more powerful model  than   
# # of word pairs Accuracy Precision Recall 
1 Baseline w/o EM 88.55 71.95 46.97 
2 1M 89.15 73.71 50.74 
3 10M 89.22 74.11 50.92 
4 100M 89.20 73.60 51.06 
5 1B 89.21 73.72 50.99 
Table 4. The performance of B&M error model (L=3) as a 
function of the size of training data (# of word pairs). 
# # of (Q, C) pairs Accuracy Precision Recall 
1 Baseline w/o EM 88.55 71.95 46.97 
2 5M 89.59 77.01 52.34 
3 15M 90.23 77.87 56.67 
4 45M 90.45 78.56 57.02 
5 120M 90.70 78.49 58.12 
Table 5. The performance of PBEM (L=3) as a function of 
the size of training data (# of (Q, C) pairs). 
B&M in that it can benefit more from increasing-
ly larger training data. As shown in Tables 4 and 
5, whilst the performance of B&M saturates 
quickly with the increase of training data, the per-
formance of PBEM does not appear to have 
peaked ? further improvements are likely given a 
larger data set. 
7 Conclusions and Future Work 
This paper explores the use of massive Web cor-
pora and search logs for improving a ranker- 
based search query speller. We show significant 
improvements over a noisy channel speller using 
fine-grained features, Web scale LMs, and a 
phrase-based error model that captures intern- 
word dependencies. There are several techniques 
we are exploring to make further improvements. 
First, since a query speller is developed for im-
proving the Web search results, it is natural to use 
features from search results in ranking, as studied 
in Chen et al (2007). The challenge is efficiency. 
Second, in addition to query reformulation ses-
sions, we are exploring other search logs from 
which we might extract more       pairs for er-
ror model training. One promising data source is 
clickthrough data (e.g., Agichtein et al 2006; 
Gao et al, 2009). For instance, we might try to 
learn a transformation from the title or anchor 
text of a document to the query that led to a click 
on that document. Finally, the phrase-based error 
model is inspired by phrase-based SMT systems. 
We are introducing more SMT techniques such 
as alignment and translation rule exaction. In a 
broad sense, spelling correction can be viewed as 
a monolingual MT problem where we translate 
bad English queries into good ones. 
# System Accuracy Precision Recall 
1 Baseline 89.0 74.1 49.6 
2 1+ query 4-gram 90.1 75.6 56.3 
3 1 + body 4-gram 89.9 75.7 54.4 
4 1 + title 4-gram 89.8 75.4 54.7 
5 1 + anchor 4-gram 89.9 75.1 55.6 
6 1 + G1T 4-gram 89.4 75.1 51.5 
Table 2. Spelling correction results using different LMs 
trained on different data sources. 
# System Accuracy Precision Recall 
1 Baseline w/o EM 88.6 72.0 47.0 
2 Baseline 89.0 74.1 49.6 
3 1 + B&M, L=1 89.0 73.3 50.1 
4 1 + B&M, L=3 89.2 73.7 51.0 
5 1 + PBEM, L=1 90.1 76.7 55.6 
6 1 + PBEM, L=3 90.7 78.5 58.1 
Table 3. Spelling correction results using different error 
models. 
365
Acknowledgments 
The authors would like to thank Andreas Bode, 
Mei Li, Chenyu Yan and Kuansan Wang for the 
very helpful discussions and collaboration. The 
work was done when Xu Sun was visiting Mi-
crosoft Research Redmond. 
References 
Agichtein, E., Brill, E. and Dumais, S. 2006. Improv-
ing web search ranking by incorporating user be-
havior information. In SIGIR, pp. 19-26. 
Ahmad, F., and Kondrak, G. 2005. Learning a spelling 
error model from search query logs. In HLT-
EMNLP, pp. 955-962. 
Brants, T., and Franz, A. 2006. Web 1T 5-gram corpus 
version 1.1. Technical report, Google Research. 
Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J. 
2007. Large language models in machine translation. 
In EMNLP-CoNLL, pp. 858 - 867. 
Brill, E., and Moore, R. C. 2000. An improved error 
model for noisy channel spelling correction. In ACL, 
pp. 286-293. 
Burges, C., Shaked, T., Renshaw, E., Lazier, A., 
Deeds, M., Hamilton, and Hullender, G. 2005. 
Learning to rank using gradient descent. In ICML, 
pp. 89-96.  
 Chaiken, R., Jenkins, B., Larson, P., Ramsey, B., 
Shakib, D., Weaver, S., and Zhou, J. 2008. SCOPE: 
easy and efficient parallel processing f massive data 
sets. In Proceedings of the VLDB Endowment, pp. 
1265-1276. 
Charniak, E. 2001. Immediate-head parsing for lan-
guage models. In ACL/EACL, pp. 124-131. 
Chen, S. F., and Goodman, J. 1999. An empirical 
study of smoothing techniques for language model-
ing. Computer Speech and Language, 13(10):359-
394. 
Chen, Q., Li, M., and Zhou, M. 2007. Improving que-
ry spelling correction using web search results. In 
EMNLP-CoNLL, pp. 181-189. 
Church, K., Hard, T., and Gao, J. 2007. Compressing 
trigram language models with Golomb coding. In 
EMNLP-CoNLL, pp. 199-207. 
Cucerzan, S., and Brill, E. 2004. Spelling correction as 
an iterative process that exploits the collective 
knowledge of web users. In EMNLP, pp. 293-300. 
Freund, Y. and Schapire, R. E. 1999. Large margin 
classification using the perceptron algorithm. In 
Machine Learning, 37(3): 277-296. 
Gao, J., Goodman, J., and Miao, J. 2001. The use of 
clustering techniques for language modeling -
application to Asian languages. Computational Lin-
guistics and Chinese Language Processing, 
6(1):27?60, 2001.  
Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 
2009. Smoothing clickthrough data for web search 
ranking. In SIGIR, pp. 355-362.  
Golding, A. R., and Roth, D. 1996. Applying winnow 
to context-sensitive spelling correction. In ICML, pp. 
182-190. 
Joachims, T. 2002. Optimizing search engines using 
clickthrough data. In SIGKDD, pp. 133-142. 
Kernighan, M. D., Church, K. W., and Gale, W. A. 
1990. A spelling correction program based on a 
noisy channel model. In COLING, pp. 205-210. 
Koehn, P., Och, F., and Marcu, D. 2003. Statistical 
phrase-based translation. In HLT/NAACL, pp. 127-
133. 
Kucich, K. 1992. Techniques for automatically 
correcting words in text. ACM Computing Surveys, 
24(4):377-439. 
Levenshtein, V. I. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet 
Physics Doklady, 10(8):707-710. 
Li, M., Zhu, M., Zhang, Y., and Zhou, M. 2006. Ex-
ploring distributional similarity based models for 
query spelling correction. In ACL, pp. 1025-1032. 
Mangu, L., and Brill, E. 1997. Automatic rule acquisi-
tion for spelling correction. In ICML, pp. 187-194. 
Microsoft Microsoft web n-gram services. 2010. 
http://research.microsoft.com/web-ngram 
Nguyen, P., Gao, J., and Mahajan, M. 2007. MSRLM: 
a scalable language modeling toolkit. Technical re-
port TR-2007-144, Microsoft Research. 
Och, F. 2002. Statistical machine translation: from 
single-word models to alignment templates. PhD 
thesis, RWTH Aachen. 
Och, F., and Ney, H. 2004. The alignment template 
approach to statistical machine translation. 
Computational Linguistics, 30(4): 417-449. 
Philips, L. 1990. Hanging on the metaphone. Comput-
er Language Magazine, 7(12):38-44. 
Sun, X., Gao, J., Micol, D., and Quirk, C. 2010. 
Learning phrase-based spelling error models from 
clickthrough data. In ACL.  
Toutanova, K., and Moore, R. 2002. Pronunciation 
modeling for improved spelling correction. In ACL, 
pp. 144-151.  
Whitelaw, C., Hutchinson, B., Chung, G. Y., and Ellis, 
G. 2009. Using the web for language independent 
spellchecking and autocorrection. In EMNLP, pp. 
890-899. 
Zhang, Y., Hildebrand, Al. S., and Vogel, S. 2006. 
Distributed language modeling for n-best list re-
ranking. In EMNLP, pp. 216-233. 
366
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 45?48,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An Overview of Microsoft Web N-gram Corpus and Applications

Kuansan Wang        Christopher Thrasher       Evelyne Viegas 
Xiaolong Li        Bo-june (Paul) Hsu         
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052, USA 
webngram@microsoft.com 
  
 
 
Abstract 
This document describes the properties and 
some applications of the Microsoft Web N-
gram corpus. The corpus is designed to have 
the following characteristics. First, in contrast 
to static data distribution of previous corpus 
releases, this N-gram corpus is made publicly 
available as an XML Web Service so that it 
can be updated as deemed necessary by the 
user community to include new words and 
phrases constantly being added to the Web. 
Secondly, the corpus makes available various 
sections of a Web document, specifically, the 
body, title, and anchor text, as separates mod-
els as text contents in these sections are found 
to possess significantly different statistical 
properties and therefore are treated as distinct 
languages from the language modeling point 
of view. The usages of the corpus are demon-
strated here in two NLP tasks: phrase segmen-
tation and word breaking. 
1 Introduction 
Since Banko and Brill?s pioneering work almost a 
decade ago (Banko and Brill 2001), it has been 
widely observed that the effectiveness of statistical 
natural language processing (NLP) techniques is 
highly susceptible to the data size used to develop 
them. As empirical studies have repeatedly shown 
that simple algorithms can often outperform their 
more complicated counterparts in wide varieties of 
NLP applications with large datasets, many have 
come to believe that it is the size of data, not the 
sophistication of the algorithms that ultimately 
play the central role in modern NLP (Norvig, 
2008). Towards this end, there have been consider-
able efforts in the NLP community to gather ever 
larger datasets, culminating the release of the Eng-
lish Giga-word corpus (Graff and Cieri, 2003) and 
the 1 Tera-word Google N-gram (Thorsten and 
Franz, 2006) created from arguably the largest text 
source available, the World Wide Web. 
Recent research, however, suggests that studies 
on the document body alone may no longer be suf-
ficient in understanding the language usages in our 
daily lives. A document, for example, is typically 
associated with multiple text streams. In addition 
to the document body that contains the bulk of the 
contents, there are also the title and the file-
name/URL the authors choose to name the docu-
ment. On the web, a document is often linked with 
anchor text or short messages from social network 
applications that other authors use to summarize 
the document, and from the search logs we learn 
the text queries formulated by the general public to 
specify the document. A large scale studies reveal 
that these text streams have significantly different 
properties and lead to varying degrees of perfor-
mance in many NLP applications (Wang et al 
2010, Huang et al 2010). Consequently from the 
statistical modeling point of view, these streams 
are better regarded as composed in distinctive lan-
guages and treated as such. 
This observation motivates the creation of Mi-
crosoft Web N-gram corpus in which the materials 
from the body, title and anchor text are made 
available separately. Another notable feature of the 
corpus is that Microsoft Web N-gram is available 
as a cross-platform XML Web service1 that can be 
freely and readily accessible by users through the 
Internet anytime and anywhere. The service archi-
tecture also makes it straightforward to perform on 
                                                          
1 Please visit http://research.microsoft.com/web-ngram for 
more information. 
45
demand updates of the corpus with the new con-
tents that can facilitate the research on the dynam-
ics of the Web.2 
2 General Model Information  
Like the Google N-gram, Microsoft Web N-gram 
corpus is based on the web documents indexed by 
a commercial web search engine in the EN-US 
market, which, in this case, is the Bing service 
from Microsoft. The URLs in this market visited 
by Bing are at the order of hundreds of billion, 
though the spam and other low quality web pages 
are actively excluded using Bing?s proprietary al-
gorithms. The various streams of the web docu-
ments are then downloaded, parsed and tokenized 
by Bing, in which process the text is lowercased 
with the punctuation marks removed. However, no 
stemming, spelling corrections or inflections are 
performed.  
Unlike the Google N-gram release which con-
tains raw N-gram counts, Microsoft Web N-gram 
provides open-vocabulary, smoothed back-off N-
gram models for the three text streams using the 
CALM algorithm (Wang and Li, 2009) that dy-
namically adapts the N-gram models as web doc-
uments are crawled. The design of CALM ensures 
that new N-grams are incorporated into the models 
as soon as they are encountered in the crawling and 
become statistically significant. The models are 
therefore kept up-to-date with the web contents. 
CALM is also designed to make sure that dupli-
cated contents will not have outsized impacts in 
biasing the N-gram statistics. This property is use-
ful as Bing?s crawler visits URLs in parallel and on 
the web many URLs are pointing to the same con-
tents. Currently, the maximum order of the N-gram 
available is 5, and the numbers of N-grams are 
shown in Table 1. 
 
Table 1: Numbers of N-grams for various streams 
 Body Title Anchor 
1-gram 1.2B 60M 150M 
2-gram 11.7B 464M 1.1B 
3-gram 60.1B 1.4B 3.2B 
4-gram 148.5B 2.3B 5.1B 
5-gram 237B 3.8B 8.9B 
                                                          
2 The WSDL for the web service is located at http://web-
ngram.research.microsoft.com/Lookup.svc/mex?wsdl. 
CALM algorithm adapts the model from a seed 
model based on the June 30, 2009 snapshot of the 
Web with the algorithm described and imple-
mented in the MSRLM toolkit (Nguyen et al 
2007). The numbers of tokens in the body, title, 
and anchor text in the snapshot are of the order of 
1.4 trillion, 12.5 billion, and 357 billion, respec-
tively. 
3 Search Query Segmentation 
In this demonstration, we implement a straightfor-
ward algorithm that generates hypotheses of the 
segment boundaries at all possible placements in a 
query and rank their likelihoods using the N-gram 
service. In other words, a query of T terms will 
have 2T-1 segmentation hypotheses. Using the fam-
ous query ?mike siwek lawyer mi? described in 
(Levy, 2010) as an example, the likelihoods and 
the segmented queries for the top 5 hypotheses are 
shown in Figure 1. 
Body: 
  
Title: 
 
Anchor: 
 
 
Figure 1: Top 5 segmentation hypotheses under 
body, title, and anchor language models. 
 
As can be seen, the distinctive styles of the lan-
guages used to compose the body, title, and the 
anchor text contribute to their respective models 
producing different outcomes on the segmentation 
46
task, many of which research issues have been ex-
plored in (Huang et al 2010). It is hopeful that the 
release of Microsoft Web N-gram service can ena-
ble the community in general to accelerate the re-
search on this and related areas. 
4 Word Breaking Demonstration 
Word breaking is a challenging NLP task, yet the 
effectiveness of employing large amount of data to 
tackle word breaking problems has been demon-
strated in (Norvig, 2008). To demonstrate the ap-
plicability of the web N-gram service for the work 
breaking problem, we implement the rudimentary 
algorithm described in (Norvig, 2008) and extend 
it to use body N-gram for ranking the hypotheses. 
In essence, the word breaking task can be regarded 
as a segmentation task at the character level where 
the segment boundaries are delimitated by white 
spaces. By using a larger N-gram model, the demo 
can successfully tackle the challenging word 
breaking examples as mentioned in (Norvig, 2008). 
Figure 2 shows the top 5 hypotheses of the simple 
algorithm. We note that the word breaking algo-
rithm can fail to insert desired spaces into strings 
that are URL fragments and occurred in the docu-
ment body frequently enough. 
 
 
 
Figure 2: Norvig's word breaking examples (Norvig, 
2008) re-examined with Microsoft Web N-gram 
 
 
 
 
 
47
Two surprising side effects of creating the N-
gram models from the web in general are worth 
noting. First, as more and more documents contain 
multi-lingual contents, the Microsoft Web N-gram 
corpus inevitably include languages other than EN-
US, the intended language. Figure 3 shows exam-
ples in German, French and Chinese (Romanized) 
each.  
 
 
 
 
 
Figure 3: Word breaking examples for foreign lan-
guages: German (top), French and Romanized Chi-
nese 
 
Secondly, since the web documents contain many 
abbreviations that are popular in short messaging, 
the consequent N-gram model lends the simple 
word breaking algorithm to cope with the common 
short hands surprisingly well. An example that de-
codes the short hand for ?wait for you? is shown in 
Figure 4. 
 
 
Figure 4: A word breaking example on SMS-style 
message. 
References  
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram 
Version 1. Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia. 
Michel Banko and Eric Brill. 2001. Mitigating the pauc-
ity-of-data problem: exploring the effect of training 
corpus size on classifier performance for natural lan-
guage processing. Proc. 1st Internal Conference on 
human language technology research, 1-5, San Di-
ego, CA.  
David Graff and Christopher Cieri. 2003. English Gi-
gaword. Linguistic Data Consortium, ISBN: 1-
58563-260-0, Philadelphia. 
Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li, 
Kuansan Wang, and Fritz Behr. 2010. Exploring web 
scale language models for search query processing. 
In Proc. 19th International World Wide Web Confe-
rence (WWW-2010), Raleigh, NC. 
Steven Levy, 2010. How Google?s algorithm rules the 
web. Wired Magazine, February.  
Patrick Nguyen, Jianfeng Gao, and Milind Mahajan. 
2007. MSRLM: a scalable language modeling tool-
kit. Microsoft Research Technical Report MSR-TR-
2007-144. 
Peter Norvig. 2008. Statistical learning as the ultimate 
agile development tool. ACM 17th Conference on In-
formation and Knowledge Management Industry 
Event (CIKM-2008), Napa Valley, CA. 
Kuansan Wang, Jianfeng Gao, and Xiaolong Li. 2010. 
The multi-style language usages on the Web and 
their implications on information retrieval. In sub-
mission. 
Kuansan Wang, Xiaolong Li and Jianfeng Gao, 2010. 
Multi-style language model for web scale informa-
tion retrieval. In Proc. ACM 33rd Conference on Re-
search and Development in Information Retrieval 
(SIGIR-2010), Geneva, Switzerland. 
Kuansan Wang and Xiaolong Li, 2009. Efficacy of a 
constantly adaptive language modeling technique for 
web scale application. In Proc. IEEE International 
Conference on Acoustics, Speech, and Signal 
Processing (ICASSP-2009), Taipei, Taiwan. 
48
