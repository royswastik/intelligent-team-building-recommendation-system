Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 641?650,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Transfer Learning Based Cross-lingual
Knowledge Extraction for Wikipedia
Zhigang Wang?, Zhixing Li?, Juanzi Li?, Jie Tang?, and Jeff Z. Pan?
? Tsinghua National Laboratory for Information Science and Technology
DCST, Tsinghua University, Beijing, China
{wzhigang,zhxli,ljz,tangjie}@keg.cs.tsinghua.edu.cn
? Department of Computing Science, University of Aberdeen, Aberdeen, UK
jeff.z.pan@abdn.ac.uk
Abstract
Wikipedia infoboxes are a valuable source
of structured knowledge for global knowl-
edge sharing. However, infobox infor-
mation is very incomplete and imbal-
anced among the Wikipedias in differen-
t languages. It is a promising but chal-
lenging problem to utilize the rich struc-
tured knowledge from a source language
Wikipedia to help complete the missing in-
foboxes for a target language.
In this paper, we formulate the prob-
lem of cross-lingual knowledge extraction
from multilingual Wikipedia sources, and
present a novel framework, called Wiki-
CiKE, to solve this problem. An instance-
based transfer learning method is utilized
to overcome the problems of topic drift
and translation errors. Our experimen-
tal results demonstrate that WikiCiKE out-
performs the monolingual knowledge ex-
traction method and the translation-based
method.
1 Introduction
In recent years, the automatic knowledge extrac-
tion using Wikipedia has attracted significant re-
search interest in research fields, such as the se-
mantic web. As a valuable source of structured
knowledge, Wikipedia infoboxes have been uti-
lized to build linked open data (Suchanek et al,
2007; Bollacker et al, 2008; Bizer et al, 2008;
Bizer et al, 2009), support next-generation in-
formation retrieval (Hotho et al, 2006), improve
question answering (Bouma et al, 2008; Fer-
ra?ndez et al, 2009), and other aspects of data ex-
ploitation (McIlraith et al, 2001; Volkel et al,
2006; Hogan et al, 2011) using semantic web s-
tandards, such as RDF (Pan and Horrocks, 2007;
Heino and Pan, 2012) and OWL (Pan and Hor-
rocks, 2006; Pan and Thomas, 2007; Fokoue et
al., 2012), and their reasoning services.
However, most infoboxes in different Wikipedi-
a language versions are missing. Figure 1 shows
the statistics of article numbers and infobox infor-
mation for six major Wikipedias. Only 32.82%
of the articles have infoboxes on average, and the
numbers of infoboxes for these Wikipedias vary
significantly. For instance, the English Wikipedi-
a has 13 times more infoboxes than the Chinese
Wikipedia and 3.5 times more infoboxes than the
second largest Wikipedia of German language.
English German French Dutch Spanish Chinese
0
0.5
1
1.5
2
2.5
3
3.5
4
x 106
Languages
N
um
be
r o
f I
ns
ta
nc
es
 
 
Article
Infobox
Figure 1: Statistics for Six Major Wikipedias.
To solve this problem, KYLIN has been pro-
posed to extract the missing infoboxes from un-
structured article texts for the English Wikipedi-
a (Wu and Weld, 2007). KYLIN performs
well when sufficient training data are available,
and such techniques as shrinkage and retraining
have been used to increase recall from English
Wikipedia?s long tail of sparse infobox classes
(Weld et al, 2008; Wu et al, 2008). The extraction
performance of KYLIN is limited by the number
of available training samples.
Due to the great imbalance between different
Wikipedia language versions, it is difficult to gath-
er sufficient training data from a single Wikipedia.
Some translation-based cross-lingual knowledge
641
extraction methods have been proposed (Adar et
al., 2009; Bouma et al, 2009; Adafre and de Rijke,
2006). These methods concentrate on translating
existing infoboxes from a richer source language
version of Wikipedia into the target language. The
recall of new target infoboxes is highly limited
by the number of equivalent cross-lingual arti-
cles and the number of existing source infoboxes.
Take Chinese-English1 Wikipedias as an example:
current translation-based methods only work for
87,603 Chinese Wikipedia articles, 20.43% of the
total 428,777 articles. Hence, the challenge re-
mains: how could we supplement the missing in-
foboxes for the rest 79.57% articles?
On the other hand, the numbers of existing in-
fobox attributes in different languages are high-
ly imbalanced. Table 1 shows the comparison
of the numbers of the articles for the attributes
in template PERSON between English and Chi-
nese Wikipedia. Extracting the missing value for
these attributes, such as awards, weight, influences
and style, inside the single Chinese Wikipedia is
intractable due to the rarity of existing Chinese
attribute-value pairs.
Attribute en zh Attribute en zh
name 82,099 1,486 awards 2,310 38
birth date 77,850 1,481 weight 480 12
occupation 66,768 1,279 influences 450 6
nationality 20,048 730 style 127 1
Table 1: The Numbers of Articles in TEMPLATE
PERSON between English(en) and Chinese(zh).
In this paper, we have the following hypothesis:
one can use the rich English (auxiliary) informa-
tion to assist the Chinese (target) infobox extrac-
tion. In general, we address the problem of cross-
lingual knowledge extraction by using the imbal-
ance between Wikipedias of different languages.
For each attribute, we aim to learn an extractor to
find the missing value from the unstructured arti-
cle texts in the target Wikipedia by using the rich
information in the source language. Specifically,
we treat this cross-lingual information extraction
task as a transfer learning-based binary classifica-
tion problem.
The contributions of this paper are as follows:
1. We propose a transfer learning-based cross-
lingual knowledge extraction framework
1Chinese-English denotes the task of Chinese Wikipedia
infobox completion using English Wikipedia
called WikiCiKE. The extraction perfor-
mance for the target Wikipedia is improved
by using rich infoboxes and textual informa-
tion in the source language.
2. We propose the TrAdaBoost-based extractor
training method to avoid the problems of top-
ic drift and translation errors of the source
Wikipedia. Meanwhile, some language-
independent features are introduced to make
WikiCiKE as general as possible.
3. Chinese-English experiments for four typ-
ical attributes demonstrate that WikiCiKE
outperforms both the monolingual extrac-
tion method and current translation-based
method. The increases of 12.65% for pre-
cision and 12.47% for recall in the template
named person are achieved when only 30 tar-
get training articles are available.
The rest of this paper is organized as follows.
Section 2 presents some basic concepts, the prob-
lem formalization and the overview of WikiCiKE.
In Section 3, we propose our detailed approaches.
We present our experiments in Section 4. Some re-
lated work is described in Section 5. We conclude
our work and the future work in Section 6.
2 Preliminaries
In this section, we introduce some basic con-
cepts regarding Wikipedia, formally defining the
key problem of cross-lingual knowledge extrac-
tion and providing an overview of the WikiCiKE
framework.
2.1 Wiki Knowledge Base and Wiki Article
We consider each language version of Wikipedia
as a wiki knowledge base, which can be represent-
ed as K = {ai}pi=1, where ai is a disambiguated
article in K and p is the size of K .
Formally we define a wiki article a ? K as a
5-tuple a = (title, text, ib, tp, C), where
? title denotes the title of the article a,
? text denotes the unstructured text description
of the article a,
? ib is the infobox associated with a; specif-
ically, ib = {(attri, valuei)}qi=1 represents
the list of attribute-value pairs for the article
a,
642
Figure 2: Simplified Article of ?Bill Gates?.
? tp = {attri}ri=1 is the infobox template as-
sociated with ib, where r is the number of
attributes for one specific template, and
? C denotes the set of categories to which the
article a belongs.
Figure 2 gives an example of these five impor-
tant elements concerning the article named ?Bill
Gates?.
In what follows, we will use named subscripts,
such as aBill Gates, or index subscripts, such as ai,
to refer to one particular instance interchangeably.
We will use ?name in TEMPLATE PERSON?
to refer to the attribute attrname in the template
tpPERSON . In this cross-lingual task, we use the
source (S) and target (T) languages to denote the
languages of auxiliary and target Wikipedias, re-
spectively. For example, KS indicates the source
wiki knowledge base, and KT denotes the target
wiki knowledge base.
2.2 Problem Formulation
Mining new infobox information from unstruc-
tured article texts is actually a multi-template,
multi-slot information extraction problem. In our
task, each template represents an infobox template
and each slot denotes an attribute. In the Wiki-
CiKE framework, for each attribute attrT in an
infobox template tpT , we treat the task of missing
value extraction as a binary classification prob-
lem. It predicts whether a particular word (token)
from the article text is the extraction target (Finn
and Kushmerick, 2004; Lafferty et al, 2001).
Given an attribute attrT and an instance
(word/token) xi, XS = {xi}ni=1 and XT =
{xi}n+mi=n+1 are the sets of instances (words/tokens)
in the source and the target language respectively.
xi can be represented as a feature vector according
to its context. Usually, we have n ? m in our set-
ting, with much more attributes in the source that
those in the target. The function g : X 7? Y maps
the instance from X = XS ? XT to the true la-
bel of Y = {0, 1}, where 1 represents the extrac-
tion target (positive) and 0 denotes the background
information (negative). Because the number of
target instances m is inadequate to train a good
classifier, we combine the source and target in-
stances to construct the training data set as TD =
TDS ? TDT , where TDS = {xi, g(xi)}ni=1 and
TDT = {xi, g(xi)}n+mi=n+1 represent the source
and target training data, respectively.
Given the combined training data set TD, our
objective is to estimate a hypothesis f : X 7? Y
that minimizes the prediction error on testing data
in the target language. Our idea is to determine the
useful part of TDS to improve the classification
performance in TDT . We view this as a transfer
learning problem.
2.3 WikiCiKE Framework
WikiCiKE learns an extractor for a given attribute
attrT in the target Wikipedia. As shown in Fig-
ure 3, WikiCiKE contains four key components:
(1) Automatic Training Data Generation: given
the target attribute attrT and two wiki knowledge
bases KS and KT , WikiCiKE first generates the
training data set TD = TDS ? TDT automati-
cally. (2) WikiCiKE Training: WikiCiKE uses
a transfer learning-based classification method to
train the classifier (extractor) f : X 7? Y by using
TDS ? TDT . (3) Template Classification: Wi-
kiCiKE then determines proper candidate articles
which are suitable to generate the missing value of
attrT . (4) WikiCiKE Extraction: given a candi-
date article a, WikiCiKE uses the learned extractor
f to label each word in the text of a, and generate
the extraction result in the end.
3 Our Approach
In this section, we will present the detailed ap-
proaches used in WikiCiKE.
643
Figure 3: WikiCiKE Framework.
3.1 Automatic Training Data Generation
To generate the training data for the target at-
tribute attrT , we first determine the equivalen-
t cross-lingual attribute attrS . Fortunately, some
templates in non-English Wikipedia (e.g. Chinese
Wikipedia) explicitly match their attributes with
their counterparts in English Wikipedia. There-
fore, it is convenient to align the cross-lingual at-
tributes using English Wikipedia as bridge. For
attributes that can not be aligned in this way, cur-
rently we manually align them. The manual align-
ment is worthwhile because thousands of articles
belong to the same template may benefit from it
and at the same time it is not very costly. In Chi-
nese Wikipedia, the top 100 templates have cov-
ered nearly 80% of the articles which have been
assigned a template.
Once the aligned attribute mapping attrT ?
attrS is obtained, we collect the articles from both
KS and KT containing the corresponding attr.
The collected articles from KS are translated into
the target language. Then, we use a uniform au-
tomatic method, which primarily consists of word
labeling and feature vector generation, to generate
the training data set TD = {(x, g(x))} from these
collected articles.
For each collected article a =
{title, text, ib, tp, C} and its value of attr,
we can automatically label each word x in text
according to whether x and its neighbors are
contained by the value. The text and value are
processed as bags of words {x}text and {x}value.
Then for each xi ? {x}text we have:
g(xi) =
?
???
???
1 xi ? {x}value, |{x}value| = 1
1 xi?1, xi ? {x}value or xi, xi+1 ? {x}value,
|{x}value| > 1
0 otherwise
(1)
After the word labeling, each instance
(word/token) is represented as a feature vec-
tor. In this paper, we propose a general feature
space that is suitable for most target languages.
As shown in Table 2, we classify the features
used in WikiCiKE into three categories: format
features, POS tag features and token features.
Category Feature Example
Format First token of sentence `}L
feature Hello World!
In first half of sentence `}L
Hello World!
Starts with two digits 1231?
31th Dec.
Starts with four digits 1999t)
1999?s summer
Contains a cash sign 10?or 10$
Contains a percentage 10%
symbol
Stop words ?,0,?&
of, the, a, an
Pure number 365
Part of an anchor text 5q?
Movie Director
Begin of an anchor text 8??
Game Designer
POS tag POS tag of current token
features POS tags of
previous 5 tokens
POS tags of
next 5 tokens
Token Current token
features Previous 5 tokens
Next 5 tokens
Is current token
contained by title
Is one of previous 5
tokens contained by title
Table 2: Feature Definition.
The target training data TDT is directly gener-
ated from articles in the target language Wikipedi-
a. Articles from the source language Wikipedia
are translated into the target language in advance
and then transformed into training data TDS . In
next section, we will discuss how to train an ex-
tractor from TD = TDS ? TDT .
3.2 WikiCiKE Training
Given the attribute attrT , we want to train a clas-
sifier f : X 7? Y that can minimize the prediction
644
error for the testing data in the target language.
Traditional machine learning approaches attempt
to determine f by minimizing some loss function
L on the prediction f(x) for the training instance
x and its real label g(x), which is
f? = argmin
f??
?
L(f(x), g(x)) where (x, g(x)) ? TDT
(2)
In this paper, we use TrAdaBoost (Dai et al,
2007), which is an instance-based transfer learn-
ing algorithm that was first proposed by Dai to find
f? . TrAdaBoost requires that the source training
instances XS and target training instances XT be
drawn from the same feature space. In WikiCiKE,
the source articles are translated into the target
language in advance to satisfy this requirement.
Due to the topic drift problem and translation er-
rors, the joint probability distribution PS(x, g(x))
is not identical to PT (x, g(x)). We must adjust the
source training data TDS so that they fit the dis-
tribution on TDT . TrAdaBoost iteratively updates
the weights of all training instances to optimize the
prediction error. Specifically, the weight-updating
strategy for the source instances is decided by the
loss on the target instances.
For each t = 1 ? T iteration, given a weight
vector pt normalized from wt(wt is the weight
vector before normalization), we call a basic clas-
sifier F that can address weighted instances and
then find a hypothesis f that satisfies
f?t = argmin
f??F
?
L(pt, f(x), g(x))
(x, g(x)) ? TDS ? TDT
(3)
Let ?t be the prediction error of f?t at the tth iter-
ation on the target training instances TDT , which
is
?t = 1?n+m
k=n+1 wtk
?
n+m?
k=n+1
(wtk ? |f?t(xk)? yk|) (4)
With ?t, the weight vector wt is updated by the
function:
wt+1 = h(wt, ?t) (5)
The weight-updating strategy h is illustrated in
Table 3.
Finally, a final classifier f? can be obtained by
combining f?T/2 ? f?T .
TrAdaBoost has a convergence rate of
O(
?
ln(n/N)), where n and N are the number
of source samples and number of maximum
iterations respectively.
TrAdaBoost AdaBoost
Target + wt wt
samples ? wt ? ??1t wt ? ??1t
Source + wt ? ??1 No source training
samples ? wt ? ? sample available
+: correctly labelled ?: miss-labelled
wt: weight of an instance at the tth iteration
?t = ?t ? (1? ?t)
? = 1/(1 +
?
2 lnnT )
Table 3: Weight-updating Strategy of TrAd-
aBoost.
3.3 Template Classification
Before using the learned classifier f to extrac-
t missing infobox value for the target attribute
attrT , we must select the correct articles to be pro-
cessed. For example, the article aNew Y ork is not
a proper article for extracting the missing value of
the attribute attrbirth day .
If a already has an incomplete infobox, it is
clear that the correct tp is the template of its own
infobox ib. For those articles that have no infobox-
es, we use the classical 5-nearest neighbor algo-
rithm to determine their templates (Roussopoulos
et al, 1995) using their category labels, outlinks,
inlinks as features (Wang et al, 2012). Our classi-
fier achieves an average precision of 76.96% with
an average recall of 63.29%, and can be improved
further. In this paper, we concentrate on the Wiki-
CiKE training and extraction components.
3.4 WikiCiKE Extraction
Given an article a determined by template classi-
fication, we generate the missing value of attr
from the corresponding text. First, we turn the
text into a word sequence and compute the fea-
ture vector for each word based on the feature
definition in Section 3.1. Next we use f to label
each word, and we get a labeled sequence textl as
textl = {xf(x1)1 ...x
f(xi?1)
i?1 x
f(xi)
i x
f(xi+1)
i+1 ...x
f(xn)
n }
where the superscript f(xi) ? {0, 1} represents
the positive or negative label by f . After that, we
extract the adjacent positive tokens in text as the
predict value. In particular, the longest positive to-
ken sequence and the one that contains other pos-
itive token sequences are preferred in extraction.
E.g., a positive sequence ?comedy movie director?
is preferred to a shorter sequence ?movie direc-
tor?.
645
4 Experiments
In this section, we present our experiments to e-
valuate the effectiveness of WikiCiKE, where we
focus on the Chinese-English case; in other words,
the target language is Chinese and the source lan-
guage is English. It is part of our future work to
try other language pairs which two Wikipedias of
these languages are imbalanced in infobox infor-
mation such as English-Dutch.
4.1 Experimental Setup
4.1.1 Data Sets
Our data sets are from Wikipedia dumps2 generat-
ed on April 3, 2012. For each attribute, we collect
both labeled articles (articles that contain the cor-
responding attribute attr) and unlabeled articles
in Chinese. We split the labeled articles into two
subsets AT and Atest(AT ? Atest = ?), in which
AT is used as target training articles and Atest is
used as the first testing set. For the unlabeled arti-
cles, represented as A?test, we manually label their
infoboxes with their texts and use them as the sec-
ond testing set. For each attribute, we also collect a
set of labeled articles AS in English as the source
training data. Our experiments are performed on
four attributes, which are occupation, nationality,
alma mater in TEMPLATE PERSON, and coun-
try in TEMPLATE FILM. In particular, we extract
values from the first two paragraphs of the texts
because they usually contain most of the valuable
information. The details of data sets on these at-
tributes are given in Table 4.
Attribute |AS| |AT| |Atest| |A?test|
occupation 1,000 500 779 208
alma mater 1,000 200 215 208
nationality 1,000 300 430 208
country 1,000 500 1,000 ?
|A|: the number of articles in A
Table 4: Data Sets.
4.1.2 Comparison Methods
We compare our WikiCiKE method with two dif-
ferent kinds of methods, the monolingual knowl-
edge extraction method and the translation-based
method. They are implemented as follows:
1. KE-Mon is the monolingual knowledge ex-
tractor. The difference between WikiCiKE
and KE-Mon is that KE-Mon only uses the
Chinese training data.
2http://dumps.wikimedia.org/
2. KE-Tr is the translation-based extractor. It
obtains the values by two steps: finding their
counterparts (if available) in English using
Wikipedia cross-lingual links and attribute
alignments, and translating them into Chi-
nese.
We conduct two series of evaluation to compare
WikiCiKE with KE-Mon and KE-Tr, respectively.
1. We compare WikiCiKE with KE-Mon on the
first testing data set Atest, where most val-
ues can be found in the articles? texts in those
labeled articles, in order to demonstrate the
performance improvement by using cross-
lingual knowledge transfer.
2. We compare WikiCiKE with KE-Tr on the
second testing data set A?test, where the
existences of values are not guaranteed in
those randomly selected articles, in order to
demonstrate the better recall of WikiCiKE.
For implementation details, the weighted-SVM
is used as the basic learner f both in WikiCiKE
and KE-Mon (Zhang et al, 2009), and Baidu
Translation API3 is used as the translator both in
WikiCiKE and KE-Tr. The Chinese texts are pre-
processed using ICTCLAS4 for word segmenta-
tion.
4.1.3 Evaluation Metrics
Following Lavelli?s research on evaluation of in-
formation extraction (Lavelli et al, 2008), we per-
form evaluation as follows.
1. We evaluate each attr separately.
2. For each attr, there is exactly one value ex-
tracted.
3. No alternative occurrence of real value is
available.
4. The overlap ratio is used in this paper rather
than ?exactly matching? and ?containing?.
Given an extracted value v? = {w?} and its
corresponding real value v = {w}, two measure-
ments for evaluating the overlap ratio are defined:
recall: the rate of matched tokens w.r.t. the real
value. It can be calculated using
R(v?, v) = |v ? v
?|
|v|
3http://openapi.baidu.com/service
4http://www.ictclas.org/
646
precision: the rate of matched tokens w.r.t. the
extracted value. It can be calculated using
P (v?, v) = |v ? v
?|
|v?|
We use the average of these two measures to
evaluate the performance of our extractor as fol-
lows:
R = avg(Ri(v?, v)) ai ? Atest
P = avg(Pi(v?, v)) ai ? Atest and vi? 6= ?
The recall and precision range from 0 to 1 and
are first calculated on a single instance and then
averaged over the testing instances.
4.2 Comparison with KE-Mon
In these experiments, WikiCiKE trains extractors
on AS ? AT , and KE-Mon trains extractors just
on AT . We incrementally increase the number of
target training articles from 10 to 500 (if available)
to compare WikiCiKE with KE-Mon in different
situations. We use the first testing data set Atest to
evaluate the results.
Figure 4 and Table 5 show the experimental re-
sults on TEMPLATE PERSON and FILM. We can
see that WikiCiKE outperforms KE-Mon on all
three attributions especially when the number of
target training samples is small. Although the re-
call for alma mater and the precision for nation-
ality of WikiCiKE are lower than KE-Mon when
only 10 target training articles are available, Wi-
kiCiKE performs better than KE-Mon if we take
into consideration both precision and recall.
10 30 50 100 200 300 500
0
0.2
0.4
0.6
0.8
number of target training articles
 
 
P(KE?Mon)
P(WikiCiKE)
R(KE?Mon)
R(WikiCiKE)
(a) occupation
10 30 50 100 200
0.4
0.5
0.6
0.7
0.8
0.9
1
number of target training articles
 
 
P(KE?Mon)
P(WikiCiKE)
R(KE?Mon)
R(WikiCiKE)
(b) alma mater
10 30 50 100 200 300
0.5
0.6
0.7
0.8
0.9
1
number of target training articles
 
 
P(KE?Mon)
P(WikiCiKE)
R(KE?Mon)
R(WikiCiKE)
(c) nationality
10 30 50 100 200 300 500
0
5
10
15
20
pe
rc
en
t(%
)
number of target training articles
performance gain
 
 
P
R
(d) average improvements
Figure 4: Results for TEMPLATE PERSON.
Figure 4(d) shows the average improvements
yielded by WikiCiKE w.r.t KE-Mon on TEM-
PLATE PERSON. We can see that WikiCiKE
yields significant improvements when only a few
articles are available in target language and the im-
provements tend to decrease as the number of tar-
get articles is increased. In this case, the articles
in the target language are sufficient to train the ex-
tractors alone.
# KE-Mon WikiCiKEP R P R
10 81.1% 63.8% 90.7% 66.3%
30 78.8% 64.5% 87.5% 69.4%
50 80.7% 66.6% 87.7% 72.3%
100 82.8% 68.2% 87.8% 72.1%
200 83.6% 70.5% 87.1% 73.2%
300 85.2% 72.0% 89.1% 76.2%
500 86.2% 73.4% 88.7% 75.6%
# Number of the target training articles.
Table 5: Results for country in TEMPLATE
FILM.
4.3 Comparison with KE-Tr
We compare WikiCiKE with KE-Tr on the second
testing data set A?test.
From Table 6 it can be clearly observed that Wi-
kiCiKE significantly outperforms KE-Tr both in
precision and recall. The reasons why the recal-
l of KE-Tr is extremely low are two-fold. First,
because of the limit of cross-lingual links and in-
foboxes in English Wikipedia, only a very smal-
l set of values is found by KE-Tr. Furthermore,
many values obtained using the translator are in-
correct because of translation errors. WikiCiKE
uses translators too, but it has better tolerance to
translation errors because the extracted value is
from the target article texts instead of the output
of translators.
Attribute KE-Tr WikiCiKEP R P R
occupation 27.4% 3.40% 64.8% 26.4%
nationality 66.3% 4.60% 70.0% 55.0%
alma mater 66.7% 0.70% 76.3% 8.20%
Table 6: Results of WikiCiKE vs. KE-Tr.
4.4 Significance Test
We conducted a significance test to demonstrate
that the difference between WikiCiKE and KE-
Mon is significant rather than caused by statistical
errors. As for the comparison between WikiCiKE
and KE-Tr, significant improvements brought by
647
WikiCiKE can be clearly observed from Table 6
so there is no need for further significance test.
In this paper, we use McNemar?s significance test
(Dietterich and Thomas, 1998).
Table 7 shows the results of significance test
calculated for the average on all tested attributes.
When the number of target training articles is less
than 100, the ? is much less than 10.83 that cor-
responds to a significance level 0.001. It suggests
that the chance that WikiCiKE is not better than
KE-Mon is less than 0.001.
# 10 30 50 100 200 300 500
? 179.5 107.3 51.8 32.8 4.1 4.3 0.3
# Number of the target training articles.
Table 7: Results of Significance Test.
4.5 Overall Analysis
As shown in above experiments, we can see that
WikiCiKE outperforms both KE-Mon and KE-Tr.
When only 30 target training samples are avail-
able, WikiCiKE reaches comparable performance
of KE-Mon using 300-500 target training samples.
Among all of the 72 attributes in TEMPLATE
PERSON of Chinese Wikipedia, 39 (54.17%) and
55 (76.39%) attributes have less than 30 and 200
labeled articles respectively. We can see that Wi-
kiCiKE can save considerable human labor when
no sufficient target training samples are available.
We also examined the errors by WikiCiKE and
they can be categorized into three classes. For at-
tribute occupation when 30 target training sam-
ples are used, there are 71 errors. The first cat-
egory is caused by incorrect word segmentation
(40.85%). In Chinese, there is no space between
words so we need to segment them before extrac-
tion. The result of word segmentation directly
decide the performance of extraction so it caus-
es most of the errors. The second category is be-
cause of the incomplete infoboxes (36.62%). In
evaluation of KE-Mon, we directly use the val-
ues in infoboxex as golden values, some of them
are incomplete so the correct predicted values will
be automatically judged as the incorrect in these
cases. The last category is mismatched words
(22.54%). The predicted value does not match the
golden value or a part of it. In the future, we can
improve the performance of WikiCiKE by polish-
ing the word segmentation result.
5 Related Work
Some approaches of knowledge extraction from
the open Web have been proposed (Wu et al,
2012; Yates et al, 2007). Here we focus on the
extraction inside Wikipedia.
5.1 Monolingual Infobox Extraction
KYLIN is the first system to autonomously ex-
tract the missing infoboxes from the correspond-
ing article texts by using a self-supervised learn-
ing method (Wu and Weld, 2007). KYLIN per-
forms well when enough training data are avail-
able. Such techniques as shrinkage and retraining
are proposed to increase the recall from English
Wikipedia?s long tail of sparse classes (Wu et al,
2008; Wu and Weld, 2010). Different from Wu?s
research, WikiCiKE is a cross-lingual knowledge
extraction framework, which leverags rich knowl-
edge in the other language to improve extraction
performance in the target Wikipedia.
5.2 Cross-lingual Infobox Completion
Current translation based methods usually con-
tain two steps: cross-lingual attribute alignmen-
t and value translation. The attribute alignmen-
t strategies can be grouped into two categories:
cross-lingual link based methods (Bouma et al,
2009) and classification based methods (Adar et
al., 2009; Nguyen et al, 2011; Aumueller et al,
2005; Adafre and de Rijke, 2006; Li et al, 2009).
After the first step, the value in the source lan-
guage is translated into the target language. E.
Adar?s approach gives the overall precision of
54% and recall of 40% (Adar et al, 2009). How-
ever, recall of these methods is limited by the
number of equivalent cross-lingual articles and the
number of infoboxes in the source language. It is
also limited by the quality of the translators. Wi-
kiCiKE attempts to mine the missing infoboxes
directly from the article texts and thus achieves
a higher recall compared with these methods as
shown in Section 4.3.
5.3 Transfer Learning
Transfer learning can be grouped into four cate-
gories: instance-transfer, feature-representation-
transfer, parameter-transfer and relational-
knowledge-transfer (Pan and Yang, 2010).
TrAdaBoost, the instance-transfer approach, is
an extension of the AdaBoost algorithm, and
demonstrates better transfer ability than tradition-
648
al learning techniques (Dai et al, 2007). Transfer
learning have been widely studied for classifica-
tion, regression, and cluster problems. However,
few efforts have been spent in the information
extraction tasks with knowledge transfer.
6 Conclusion and Future Work
In this paper we proposed a general cross-lingual
knowledge extraction framework called Wiki-
CiKE, in which extraction performance in the tar-
get Wikipedia is improved by using rich infobox-
es in the source language. The problems of topic
drift and translation error were handled by using
the TrAdaBoost model. Chinese-English exper-
imental results on four typical attributes showed
that WikiCiKE significantly outperforms both the
current translation based methods and the mono-
lingual extraction methods. In theory, WikiCiKE
can be applied to any two wiki knowledge based
of different languages.
We have been considering some future work.
Firstly, more attributes in more infobox templates
should be explored to make our results much
stronger. Secondly, knowledge in a minor lan-
guage may also help improve extraction perfor-
mance for a major language due to the cultural and
religion differences. A bidirectional cross-lingual
extraction approach will also be studied. Last but
not least, we will try to extract multiple attr-value
pairs at the same time for each article.
Furthermore, our work is part of a more ambi-
tious agenda on exploitation of linked data. On the
one hand, being able to extract data and knowl-
edge from multilingual sources such as Wikipedi-
a could help improve the coverage of linked data
for applications. On the other hand, we are also
investigating how to possibly integrate informa-
tion, including subjective information (Sensoy et
al., 2013), from multiple sources, so as to better
support data exploitation in context dependent ap-
plications.
Acknowledgement
The work is supported by NSFC (No. 61035004),
NSFC-ANR (No. 61261130588), 863 High Tech-
nology Program (2011AA01A207), FP7-288342,
FP7 K-Drive project (286348), the EPSRC WhatIf
project (EP/J014354/1) and THU-NUS NExT Co-
Lab. Besides, we gratefully acknowledge the as-
sistance of Haixun Wang (MSRA) for improving
the paper work.
References
S. Fissaha Adafre and M. de Rijke. 2006. Find-
ing Similar Sentences across Multiple Languages
in Wikipedia. EACL 2006 Workshop on New Text:
Wikis and Blogs and Other Dynamic Text Sources.
Sisay Fissaha Adafre and Maarten de Rijke. 2005.
Discovering Missing Links in Wikipedia. Proceed-
ings of the 3rd International Workshop on Link Dis-
covery.
Eytan Adar, Michael Skinner and Daniel S. Weld.
2009. Information Arbitrage across Multi-lingual
Wikipedia. WSDM?09.
David Aumueller, Hong Hai Do, Sabine Massmann and
Erhard Rahm?. 2005. Schema and ontology match-
ing with COMA++. SIGMOD Conference?05.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
So?ren Auer, Christian Becker, Richard Cyganiak
and Sebastian Hellmann. 2009. DBpedia - A crys-
tallization Point for the Web of Data. J. Web Sem..
Christian Bizer, Tom Heath, Kingsley Idehen and Tim
Berners-Lee. 2008. Linked data on the web (L-
DOW2008). WWW?08.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-
turge and Jamie Taylor. 2008. Freebase: a Collabo-
ratively Created Graph Database for Structuring Hu-
man Knowledge. SIGMOD?08.
Gosse Bouma, Geert Kloosterman, Jori Mur, Gertjan
Van Noord, Lonneke Van Der Plas and Jorg Tiede-
mann. 2008. Question Answering with Joost at
CLEF 2007. Working Notes for the CLEF 2008
Workshop.
Gosse Bouma, Sergio Duarte and Zahurul Islam.
2009. Cross-lingual Alignment and Completion of
Wikipedia Templates. CLIAWS3 ?09.
Wenyuan Dai, Qiang Yang, Gui-Rong Xue and Yong
Yu. 2007. Boosting for Transfer Learning. ICM-
L?07.
Dietterich and Thomas G. 1998. Approximate Statis-
tical Tests for Comparing Supervised Classification
Learning Algorithms. Neural Comput..
Sergio Ferra?ndez, Antonio Toral, ??scar Ferra?ndez, An-
tonio Ferra?ndez and Rafael Mun?oz. 2009. Exploit-
ing Wikipedia and EuroWordNet to Solve Cross-
Lingual Question Answering. Inf. Sci..
Aidan Finn and Nicholas Kushmerick. 2004. Multi-
level Boundary Classification for Information Ex-
traction. ECML.
Achille Fokoue, Felipe Meneguzzi, Murat Sensoy and
Jeff Z. Pan. 2012. Querying Linked Ontological
Data through Distributed Summarization. Proc. of
the 26th AAAI Conference on Artificial Intelligence
(AAAI2012).
649
Yoav Freund and Robert E. Schapire. 1997.
A Decision-Theoretic Generalization of On-Line
Learning and an Application to Boosting. J. Com-
put. Syst. Sci..
Norman Heino and Jeff Z. Pan. 2012. RDFS Rea-
soning on Massively Parallel Hardware. Proc. of
the 11th International Semantic Web Conference
(ISWC2012).
Aidan Hogan, Jeff Z. Pan, Axel Polleres and Yuan Ren.
2011. Scalable OWL 2 Reasoning for Linked Data.
Reasoning Web. Semantic Technologies for the Web
of Data.
Andreas Hotho, Robert Ja?schke, Christoph Schmitz
and Gerd Stumme. 2006. Information Retrieval in
Folksonomies: Search and Ranking. ESWC?06.
John D. Lafferty, Andrew McCallum and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. ICML?01.
Alberto Lavelli, MaryElaine Califf, Fabio Ciravegna,
Dayne Freitag, Claudio Giuliano, Nicholas Kush-
merick, Lorenza Romano and Neil Ireson. 2008.
Evaluation of Machine Learning-based Information
Extraction Algorithms: Criticisms and Recommen-
dations. Language Resources and Evaluation.
Juanzi Li, Jie Tang, Yi Li and Qiong Luo. 2009. Ri-
MOM: A Dynamic Multistrategy Ontology Align-
ment Framework. IEEE Trans. Knowl. Data Eng..
Xiao Ling, Gui-Rong Xue, Wenyuan Dai, Yun Jiang,
Qiang Yang and Yong Yu. 2008. Can Chinese We-
b Pages be Classified with English Data Source?.
WWW?08.
Sheila A. McIlraith, Tran Cao Son and Honglei Zeng.
2001. Semantic Web Services. IEEE Intelligent
Systems.
Thanh Hoang Nguyen, Viviane Moreira, Huong N-
guyen, Hoa Nguyen and Juliana Freire. 2011. Mul-
tilingual Schema Matching for Wikipedia Infoboxes.
CoRR.
Jeff Z. Pan and Edward Thomas. 2007. Approximat-
ing OWL-DL Ontologies. 22nd AAAI Conference
on Artificial Intelligence (AAAI-07).
Jeff Z. Pan and Ian Horrocks. 2007. RDFS(FA): Con-
necting RDF(S) and OWL DL. IEEE Transaction
on Knowledge and Data Engineering. 19(2): 192 -
206.
Jeff Z. Pan and Ian Horrocks. 2006. OWL-Eu: Adding
Customised Datatypes into OWL. Journal of Web
Semantics.
Sinno Jialin Pan and Qiang Yang. 2010. A Survey on
Transfer Learning. IEEE Trans. Knowl. Data Eng..
Nick Roussopoulos, Stephen Kelley and Fre?de?ric Vin-
cent. 1995. Nearest Neighbor Queries. SIGMOD
Conference?95.
Murat Sensoy, Achille Fokoue, Jeff Z. Pan, Timothy
Norman, Yuqing Tang, Nir Oren and Katia Sycara.
2013. Reasoning about Uncertain Information and
Conflict Resolution through Trust Revision. Proc.
of the 12th International Conference on Autonomous
Agents and Multiagent Systems (AAMAS2013).
Fabian M. Suchanek, Gjergji Kasneci and Gerhard
Weikum. 2007. Yago: a Core of Semantic Knowl-
edge. WWW?07.
Max Volkel, Markus Krotzsch, Denny Vrandecic,
Heiko Haller and Rudi Studer. 2006. Semantic
Wikipedia. WWW?06.
Zhichun Wang, Juanzi Li, Zhigang Wang and Jie Tang.
2012. Cross-lingual Knowledge Linking across Wi-
ki Knowledge Bases. 21st International World Wide
Web Conference.
Daniel S. Weld, Fei Wu, Eytan Adar, Saleema Amer-
shi, James Fogarty, Raphael Hoffmann, Kayur Pa-
tel and Michael Skinner. 2008. Intelligence in
Wikipedia. AAAI?08.
Fei Wu and Daniel S. Weld. 2007. Autonomously Se-
mantifying Wikipedia. CIKM?07.
Fei Wu and Daniel S. Weld. 2010. Open Information
Extraction Using Wikipedia. ACL?10.
Fei Wu, Raphael Hoffmann and Daniel S. Weld. 2008.
Information Extraction from Wikipedia: Moving
down the Long Tail. KDD?08.
Wentao Wu, Hongsong Li, Haixun Wang and Kenny
Qili Zhu. 2012. Probase: a Probabilistic Taxonomy
for Text Understanding. SIGMOD Conference?12.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead and Stephen
Soderland. 2007. TextRunner: Open Information
Extraction on the Web. NAACL-Demonstrations?07.
Xinfeng Zhang, Xiaozhao Xu, Yiheng Cai and Yaowei
Liu. 2009. A Weighted Hyper-Sphere SVM. IC-
NC(3)?09.
650
Charting the Potential of Description Logic
for the Generation of Referring Expressions
Yuan Ren and Kees van Deemter and Jeff Z. Pan
Department of Computing Science
University of Aberdeen
Aberdeen, UK
Abstract
The generation of referring expressions
(GRE), an important subtask of Natural
Language Generation (NLG) is to gener-
ate phrases that uniquely identify domain
entities. Until recently, many GRE algo-
rithms were developed using only simple
formalisms, which were taylor made for
the task. Following the fast development
of ontology-based systems, reinterpreta-
tions of GRE in terms of description logic
(DL) have recently started to be studied.
However, the expressive power of these
DL-based algorithms is still limited, not
exceeding that of older GRE approaches.
In this paper, we propose a DL-based ap-
proach to GRE that exploits the full power
of OWL2. Unlike existing approaches, the
potential of reasoning in GRE is explored.
1 GRE and KR: the story so far
Generation of Referring Expressions (GRE) is the
subtask of Natural Language Generation (NLG)
that focuses on identifying objects in natural lan-
guage. For example, Fig.1 depicts the relations
between several women, dogs and cats. In such
a scenario, a GRE algorithm might identify d1 as
?the dog that loves a cat?, singling out d1 from
the five other objects in the domain. Reference
has long been a key issue in theoretical linguis-
tics and psycholinguistics, and GRE is a crucial
component of almost every practical NLG sys-
tem. In the years following seminal publications
such as (Dale and Reiter, 1995), GRE has be-
come one of the most intensively studied areas of
NLG, with links to many other areas of Cogni-
tive Science. After plan-based contributions (e.g.,
(Appelt, 1985)), recent work increasingly stresses
the human-likeness of the expressions generated
in simple situations, culminating in two evalua-
tion campaigns in which dozens of GRE algo-
rithms were compared to human-generated ex-
pressions (Belz and Gatt, 2008; Gatt et al, 2009).
Figure 1: An example in which edges from women
to dogs denote feed relations, from dogs to cats
denote love relations.
Traditional GRE algorithms are usually based
on very elementary, custom-made, forms of
Knowledge Representation (KR), which allow
little else than atomic facts (with negation of
atomic facts left implicit), often using a simple
?Attribute : V alue? format, e.g ?Type : Dog?.
This is justifiable as long as the properties ex-
pressed by these algorithms are simple one-place
predicates (e.g., being a dog), but when logically
more complex descriptions are involved, the po-
tential advantages of ?serious? KR become over-
whelming. (This point will become clearer in later
sections.) This realisation is now motivating a
modest new line of research which stresses logi-
cal and computational issues, asking what proper-
ties a KR framework needs to make it suitable to
generate all the referring expressions that people
can produce (and to generate them in reasonable
time). In this new line of work, which is proceed-
ing in tandem with the more empirically oriented
work mentioned above, issues of human-likeness
are temporarily put on the backburner. These and
other empirical issues will be brought to bear once
it is better understood what types of KR system are
best suitable for GRE, and what is the best way to
pursue GRE in them.
A few proposals have started to combine GRE
with KR. Following on from work based on la-
belled directed graphs (cf. (Krahmer et al, 2003))
? a well-understood mathematical formalism that
offers no reasoning support ? (Croitoru and van
Deemter, 2007) analysed GRE as a projection
problem in Conceptual Graphs. More recently,
(Areces et al, 2008) analysed GRE as a problem in
Description Logic (DL), a formalism which, like
Conceptual Graphs, is specifically designed for
representing and reasoning with potentially com-
plex information. The idea is to produce a for-
mula such as Dog u ?love.Cat (the set of dogs
intersected with the set of objects that love at least
one cat); this is, of course, a successful reference
if there exists exactly one dog who loves at least
one cat. This approach forms the starting point for
the present paper, which aims to show that when a
principled, logic based approach is chosen, it be-
comes possible to refer to objects which no exist-
ing approach to GRE (including that of Areces et
al.) has been able to refer to. To do this, we de-
viate substantially from these earlier approaches.
For example, while Areces et al use one finite in-
terpretation for model checking, we consider arbi-
trary (possibly infinite) interpretations, hence rea-
soning support becomes necessary.
We shall follow many researchers in focussing
on the semantic core of the GRE problem: we
shall generate descriptions of semantic content,
leaving the decision of what words to use for ex-
pressing this content (e.g., ?the ancient dog?, or
?the dog which is old?) to later stages in the NLG
pipeline. Furthermore, we assume that all domain
objects are equally salient (Krahmer and Theune,
2002). As explained above, we do not consider
here the important matter of the naturalness or ef-
ficacy of the descriptions generated. We shall be
content producing uniquely referring expressions
whenever such expressions are possible, leaving
the choice of the optimal referring expression in
each given situation for later.
In what follows, we start by explaining how DL
has been applied in GRE before (Sec. 2) , point-
ing out the limitations of existing work. In Sec.3
we discuss which kinds of additional expressivity
are required and how they can be achieved through
modern DL. In Sec.4 we present a generic algo-
rithm to compute these expressive REs. Sec.5
concludes the paper by comparing its aims and
achievements with current practise in GRE.
2 DL for GRE
2.1 Description Logics
Description Logic (DLs) come in different
flavours, based on decidable fragments of first-
order logic. A DL-based KB represents the
domain with descriptions of concepts, relations,
and their instances. DLs underpin the Web On-
tology Language (OWL), whose latest version,
OWL2 (Motik et al, 2008), is based on DL
SROIQ (Horrocks et al, 2006).
An SROIQ ontology ? usually consists of a
TBox T and an ABox A. T contains a set of con-
cept inclusion axioms of the formC v D, relation
inclusion axioms such as R v S (the relation R is
contained in the relation S), R1 ? . . . ? Rn v S,
and possibly more complex information, such as
the fact that a particular relation is functional, or
symmetric; A contains axioms about individuals,
e.g. a : C (a is an instance of C), (a, b) : R (a has
an R relation with b).
Given a set of atomic concepts, the entire set
of concepts expressible by SROIQ is defined re-
cursively. First, all atomic concepts are concepts.
Furthermore, if C and D are concepts, then so are
> | ? | ?C | C uD | C unionsqD | ?R.C | ?R.C | ?
nR.C | ? nR.C | ?R.Self | {a1, . . . , an},
where > is the top concept, ? the bottom con-
cept, n a non-negative integer number, ?R.Self
the self-restriction ((i.e., the set of those x such
that (x, x) : R holds)), ai individual names and
R a relation which can either be an atomic rela-
tion or the inverse of another relation (R?). We
call a set of individual names {a1, . . . , an} a nom-
inal, and use CN , RN and IN to denote the set
of atomic concept names, relation names and indi-
vidual names, respectively.
An interpretation I is a pair ??I , I? where ?I
is a non-empty set and I is a function that maps
atomic concept A to AI ? ?I , atomic role r to
rI ? ?I ? ?I and individual a to aI ? ?I .
The interpretation of complex concepts and ax-
ioms can be defined inductively based on their se-
mantics, e.g. (C uD)I = CI ?DI , etc.
I is a model of ?, written I |= ?, iff all the ax-
ioms in ? are satisfied in I. It should be noted
that one ? can have multiple models. For ex-
ample when T = ?,A = {a : A unionsq B}, there
can be a model I1 s.t. ?I1 = {a}, aI1 =
a,AI1 = {a}, BI1 = ?, and another model I2
s.t. ?I2 = {a}, aI2 = a,BI2 = {a}, AI2 = ?.
In other words, the world is open. For details, see
(Horrocks et al, 2006).
The possibly multiple models indicate that an
ontology is describing an open world. In GRE,
researchers usually impose a closed world. From
the DL point of view, people can (partially) close
the ontology with a DBox D (Seylan et al, 2009),
which is syntactically similar to the ABox, except
that D contains only atomic formulas. Further-
more, every concept or relation appearing in D
is closed. Its extension is exactly defined by the
contents of D, i.e. if D 6|= a : A then a : ?A,
thus is the same in all the models. The concepts
and relations not appearing in D can still remain
open. DL reasoning can be exploited to infer
implicit information from ontologies. For exam-
ple, given T = {Dog v ?feed?.Woman} (ev-
ery dog is fed by some woman) and A = {d1 :
Dog,w1 : Woman}, we know that there must be
some Woman who feeds d1. When the domain
is closed as D = A we can further infer that this
Woman is w1 although there is no explicit rela-
tion between w1 and d1. Note that the domain ?I
in an interpretation ofD is not fixed, but it includes
all the DBox individuals.
However, closing ontologies by means of the
DBox can restrict the usage of implicit knowledge
(from T ). More precisely, the interpretations of
the concepts and relations appearing inD are fixed
therefore no implicit knowledge can be inferred.
To address this issue, we introduce the notion of
NBox to support Negation as Failure (NAF): Un-
der NAF, an ontology is a triple O = (T ,A,N ),
where T is a TBox, A an ABox and N is a subset
of CNorRN . We callN an NBox. NAF requires
that O satisfy the following conditions:
1. Let x ? IN and A ? N uCN . Then
(T ,A) 6|= x : A implies O |= x : ?A.
2. Let x, y ? IN and r ? N u RN .
Then (T ,A) 6|= (x, y) : r implies O |=
(x, y) : ?r.
Like the DBox approach, the NBox N defines
conditions in which ?unknown? should be treated
as ?failure?. But, instead of hard-coding this, it
specifies a vocabulary on which such treatment
should be applied. Different from the DBox ap-
proach, inferences on this NAF vocabulary is still
possible. An example of inferring implicit knowl-
edge with NAF will be shown in later sections.
2.2 Background Assumptions
When applying DL to GRE, people usually im-
pose the following assumptions.
? Individual names are not used in REs. For
example, ?the Woman who feeds d1? would
be invalid, because d1 is a name. Names are
typically outlawed in GRE because, in many
applications, many objects do not have names
that readers/hearers would be familiar with.
? Closed World Assumption (CWA): GRE re-
searchers usually assume a closed world,
without defining what this means. As ex-
plained above, DL allows different interpre-
tations of the CWA. Our solution does not de-
pend on a specific definition of CWA. In what
follows, however, we use NAF to illustrate
our idea. Furthermore, the domain is usually
considered to be finite and consists of only
individuals appearing in A.
? Unique Name Assumption (UNA): Different
names denote different individuals. If, for
example, w1 and w2 may potentially be the
same woman, then we can not distinguish one
from the other.
We follow these assumptions when discussing ex-
isting works and presenting our approach. In ad-
dition, we consider the entire KB, including A,
T and N . It is also worth mentioning that, in
the syntax of SROIQ, negation of relations are
not allowed in concept expressions, e.g. you can-
not compose a concept ??feed.Dog. However,
if feed ? N , then we can interpret (?feed)I =
?I ??I \ feedI . In the rest of the paper, we use
this as syntactic sugar.
2.3 Motivation: DL Reasoning and GRE
Every DL concept can be interpreted as a set. If
the KB allows one to prove that this set is a sin-
gleton then the concept is a referring expression.
It is this idea (Gardent and Striegnitz, 2007) that
(Areces et al, 2008) explored. In doing so, they
say little about the TBox, appearing to consider
only the ABox, which contains only axioms about
instances of atomic concepts and relations. For ex-
ample, the domain in Fig.1 can be described as
KB1: T1 = ?, A1 = {w1 : Woman,
w2 : Woman, d1 : Dog, d2 : Dog,
c1 : Cat, c2 : Cat, (w1, d1) : feed,
(w2, d1) : feed, (w2, d2) : feed,
(d1, c1) : love}
Assuming that this represents a Closed World,
Areces et al propose an algorithm that is able
to generate descriptions by partitioning the do-
main.1 More precisely, the algorithm first finds
out which objects are describable through increas-
ingly large conjunctions of (possibly negated)
atomic concepts, then tries to extend these con-
junctions with complex concepts of the form
(?)?R1.Concept, then with concepts of the form
(?)?R2.(Concept u (?)?R1.Concept), and so
on. At each stage, only those concepts that have
been acceptable through earlier stages are used.
Consider, for instance, KB1 above. Regardless of
what the intended referent is, the algorithm starts
partitioning the domain stage by stage as follows.
Each stage makes use of all previous stages. Dur-
ing stage (3), e.g., the object w2 could only be
identified because d2 was identified in stage (2):
1. Dog = {d1, d2},
?Dog uWoman = {w1, w2},
?Dog u ?Woman = {c1, c2}.
2. Dog u ?love.(?Dog u ?Woman) = {d1},
Dogu??love.(?Dogu?Woman) = {d2}.
3. (?Dog u Woman) u ?feed.(Dog u
??love.(?Dog u ?Woman)) = {w2},
(?Dog u Woman) u ??feed.(Dog u
??love.(?Dog u ?Woman)) = {w1}.
As before, we disregard the important question
of the quality of the descriptions generated, other
than whether they do or do not identify a given
referent uniquely. Other aspects of quality depend
in part on details, such as the question in which
order atomic concepts are combined during phase
(1), and analogously during later phases.
However this approach does not extend the ex-
pressive power of GRE. This is not because of
some specific lapse on the part of the authors: it
seems to have escaped the GRE community as a
whole that relations can enter REs in a variety of
alternative ways.
Furthermore, the above algorithm considers
only the ABox, therefore background information
1Areces et al (Areces et al, 2008) consider several DL
fragments (e.g., ALC and EL). Which referring expressions
are expressible, in their framework, depends on which DL
fragment is chosen. Existential quantification, however, is
the only quantifier that was used, and inverse relations are
not considered.
will not be used. It follows that the domain al-
ways has a fixed single interpretation/model. Con-
sequently the algorithm essentially uses model-
checking, rather than full reasoning. We will
show that when background information is in-
volved, reasoning has to be taken into account.
For example, suppose we extend Fig.1 with back-
ground (i.e., TBox) knowledge saying that one
should always feed any animal loved by an ani-
mal whom one is feeding, while also adding a love
edge (Fig.2) between d2 and c2:
Figure 2: An extended example of Fig.1. Edges
from women to cats denote feed relations.
Dashed edges denote implicit relations.
If we close the domain with NAF, the ontology
can be described as follows:
KB2: T2 = {feed ? love v feed},
A2 = A1 ? {(d2, c2) : love}, N2 =
{Dog,Woman, feed, love}
The TBox axiom enables the inference of implicit
facts: the facts (w1, c2) : feed, (w2, c1) : feed,
and (w2, c2) : feed can be inferred using DL rea-
soning under the above NBox N2. Axioms of this
kind allow a much more natural, insightful and
concise representation of information than would
otherwise be possible.
Continuing to focus on the materialised KB2,
we note another limitation of existing works: if
only existential quantifiers are used then some ob-
jects are unidentifiable (i.e., it is not possible to
distinguish them uniquely). These objects would
become identifiable if other quantifiers and inverse
relations were allowed. For example,
? The cat which is fed by at least 2 women =
Catu ? 2feed?.Woman = {c1},
? The woman feeding only those fed by at
least 2 women = Woman u ?feed. ?
2.feed?.Woman = {w1},
? The woman who feeds all the dogs = {w2}.
It thus raises the question: which quantifiers
would it be natural to use in GRE, and how might
DL realise them?
3 Beyond Existential Descriptions
In this section, we show how more expressive DLs
can make objects referable that were previously
unreferable. This will amount to a substantial re-
formulation which allows the approach based on
DL reasoning to move well beyond other GRE al-
gorithms in its expressive power.
3.1 Expressing Quantifiers in OWL2
Because the proposal in (Areces et al, 2008) uses
only existential quantification, it fails to identify
any individual in Fig.2. Before filling this gap,
we pause to ask what level of expressivity ought
to be achieved. In doing so, we make use of
a conceptual apparatus developed in an area of
formal semantics and mathematical logic known
as the theory of Generalized Quantifiers (GQ),
where quantifiers other than all and some are stud-
ied (Mostowski, 1957). The most general format
for REs that involves a relation R is, informally,
the N1 who R Q N2?s, where N1 and N2
denote sets, R denotes a relation, and Q a gener-
alized quantifier. (Thus for example the women
who feed SOME dogs.) An expression of this
form is a unique identifying expression if it corre-
sponds to exactly one domain element. Using a
set-theoretic notation, this means that the follow-
ing set has a cardinality of 1:
{y ? N1 : Qx ? N2 | Ryx}
where Q is a generalized quantifier. For example,
if Q is the existential quantifier, while N1 denotes
the set of women, N2 the set of dogs, and R the
relation of feeding, then this says that the number
of women who feed SOME dog is one. If Q is the
quantifier at least two, then it says that the num-
ber of women who feed at least two dogs is one.
It will be convenient to write the formula above
in the standard GQ format where quantifiers are
cast as relations between sets of domain objects
A,B. Using the universal quantifier as an exam-
ple, instead of writing ?x ? A | x ? B, we write
?(AB). Thus, the formula above is written
{y ? N1 : Q(N2{z : Ryz)}}.
Instantiating this as before, we get {y ?Woman :
?(Dog{z : Feed yz)}}, or ?women who feed a
dog?, where Q is ?, A = Dog and B = {z :
Feed yz} for some y.
Mathematically characterising the class of all
quantifiers that can be expressed in referring
expressions is a complex research programme
to which we do not intend to contribute here,
partly because this class includes quantifiers that
are computationally problematic; for example, a
quantifiers such as most (in the sense of more than
50%), which is not first-order expressible, as is
well known.
To make transparent which quantifiers are ex-
pressible in the logic that we are using, let us think
of quantifiers in terms of simple quantitative con-
straints on the sizes of the sets A?B, A?B, and
B?A, as is often done in GQ theory, asking what
types of constraints can be expressed in referring
expressions based on SROIQ. The findings are
summarised in Tab.1. OWL2 can express any of
the following types of descriptions, plus disjunc-
tions and conjunctions of anything it can express.
Table 1: Expressing GQ in DL
QAB DL
1 ? nN2{z : Ryz} y :? nR.N2
2 ? nN2?{z : Ryz} y :? n?R.N2
3 ? n?N2{z : Ryz} y :? nR.?N2
4 ? n?N2?{z : Ryz} y :? n?R.?N2
5 ? nN2{z : Ryz} y :? nR.N2
6 ? nN2?{z : Ryz} y :? n?R.N2
7 ? n?N2{z : Ryz} y :? nR.?N2
8 ? n?N2?{z : Ryz} y :? n?R.?N2
When n = 1, for example, type 1 becomes
?R.N2, i.e. the existential quantifier. When n = 0
type 7 becomes ?R.N2, i.e. the quantifier only.
When n = 0 type 6 becomes ??R.?N2, i.e. the
quantifier all. In types 2, 4, 6 and 8, negation of
a relation is used. This is not directly supported
in SROIQ but, as we indicated earlier, given
R ? N , ?R can be used in concepts.
Together, this allows the expression of a de-
scription such as ?women who feed at least one
but at most 7 dogs?, by conjoining type 1 (with
n = 1) with type 5 (with n = 7). Using nega-
tion, it can say ?women who do not feed all dogs
and who feed at least one non-dog? (Woman u
???Feed.?Dog u ?Feed.?Dog). In addition
to Tab.1, SROIQ can even represent reflexive
relation such as ?the dog who loves itself? by
Dogu?love.Self , which was regarded infeasible
in (Gardent and Striegnitz, 2007).
Comparing the quantifiers that become express-
ible through OWL2?s apparatus with classes of
quantifiers studied in the theory of GQ, it is clear
that OWL2 is highly expressive: it does not only
include quantifiers expressible in the binary tree
of numbers, e.g. (van Benthem, 1986) ? which is
generally regarded as highly general ? but much
else besides. Even wider classes of referring ex-
pressions can certainly be conceived, but these are
not likely to have overwhelming practical utility in
today?s NLG applications.
4 Generating SROIQ-enabled REs
In this section, we present an algorithm that com-
putes the descriptions discussed in sect.3. A GRE
algorithm should have the following behaviour: if
an entity is distinguishable from all the others, the
algorithm should find a unique description; oth-
erwise, the algorithm should say there exists no
unique description. In this paper, we follow Are-
ces et al?s strategy of generating REs for all ob-
jects simultaneously, but we apply it to a much
larger search space, because many more constructs
are taken into account.
4.1 GROWL: an algorithm for Generating
Referring expressions using OWL-2.
In this section we show how the ideas of pre-
vious sections can be implemented. To do this,
we sketch an algorithm scheme called GROWL.
GROWL applies a generate-and-test strategy that
composes increasingly complicated descriptions
and uses DL reasoning to test whether a de-
scription denotes a singleton w.r.t. the KB. To
avoid considering unnecessarily complicated de-
scriptions, the algorithm makes use of the (syntac-
tic) depth of a description, defined as follows:
Definition 1 (Depth) Given a description d, its
depth |d| is calculated as follows:
1. |d| = 1 for d := >|?|A|?A, where A is
atomic.
2. |d u d?| = |d unionsq d?| = max(|d|, |d?|) + 1.
3. |?r.d| = |?r.d| = | ? nr.d| = | ? nr.d| =
| = nr.d| = |d|+ 1.
Different descriptions can mean the same of
course, e.g. ??R.A ? ?R.?A. We do not know
which syntactic variant should be used but focus,
for simplicity, on generating their unique negated
normal form (NNF). The NNF of a formula ?
can be obtained by pushing all the ? inward un-
til only before atomic concepts (including > and
?), atomic relations, nominals or self restrictions
(e.g. ?r.Self ). Without loss of generality, in what
follows we assume all the formulas are in their
NNF. To avoid confusion, the NNF of negation
of a formula ? is denoted by ~? instead of ??.
For example ~(A unionsq B) = ?A u ?B if A and B
are atomic. Obviously, ~(~A) = A, ~(~R) = R,
(R?)? = R, and (~R)? =~R?. The use of NNF
substantially reduces the redundancies generated
by the algorithm. For example, we won?t generate
both ??R.A and ?R.?A but only the later.
Given an ontology ?, we initialise GROWL
with the following sets:
1. The relation name set RN is the minimal set
satisfying:
? if R is an atomic relation in ?, then R ?
RN ;
? if R ? RN , then ~R ? RN ;
? if R ? RN , then R? ? RN ;
2. The concept name set CN is the minimal set
satisfying:
? > ? CN ;
? if A is an atomic concept in ?, then A ?
CN ;
? if R ? RN , then ?R.Self ? CN ;
? if A ? CN , then ~A ? CN ;
3. The natural number set N contains
1, 2, . . . , n where n is the number of
individuals in ?.
4. The construct set S contains all the con-
structs supported by a particular language.
For SROIQ, S = {?,u,unionsq,?,?,?,?,=}.
We assume here that nominals are disallowed
(cf. sect.2).
Algorithm GROWL:
Construct? description(?, CN,RN,N, S)
INPUT: ?, CN,RN,N, S
OUTPUT: Description Queue D
1: D := ?
2: for e ? CN do
3: D := Add(D, e)
4: for d = fetch(D) do
5: for each s ? S do
6: if s = u or s = unionsq then
7: for each d? ? D do
8: D := Add(D, d s d?)
9: if s = ? or s = ? then
10: for each r ? RN do
11: D := Add(D, s r.d)
12: if s =? or s =? or s is = then
13: for each r ? RN , each k ? N do
14: D := Add(D, s k r.d)
15: return D
Algorithm ADD:Add(D, e)
INPUT: D, e
OUTPUT: (Extended) Description Queue D
1: for d ? D do
2: if |d| < |e| and d v? e then
3: return D
4: else if |d| = |e| and d v? e and e u ?d is
satisfiable then
5: return D
6: if e is satisfiable in ? then
7: D := D ? {e}
8: return D
GROWL takes an ontology ? as its input and
output a queue D of descriptions by adding in-
creasingly complex concepts e to D, using the
function Add(D, e), which is implemented as the
algorithm ADD. Because of the centrality of ADD
we start by explaining how this function works.
In the simple algorithm we are proposing in this
paper ? which represents only one amongst many
possibilities ? addition is governed by the heuris-
tic that more complex descriptions should have
smaller extensions. To this end, a candidate de-
scription e is compared with each existing descrip-
tion d ? D. Step 2 ensures that if there exists a
simpler description d (|d| < |e|) whose extension
is no larger than e (d v? e), then e is not added
into D (because the role of e can be taken by the
simpler description d). Similarly, step 4 ensures
that if there exists d with same depth (|d| = |e|)
but smaller extension (d v? e and e u ?d is satis-
fiable), then e should not be added into D. The
subsumption checking in Step 2 and 4, and the
instance retrieval in Step 6, must be realised by
DL reasoning, in which TBox, ABox and NBox
must all be taken into account. ADD guaran-
tees that when the complexity of descriptions in-
creases, their extensions are getting smaller.
We now turn to the main algorithm, GROWL. In
Step 1 of this algorithm,D is initialised to ?. Steps
2 to 3 add all satisfiable elements of CN to D.
From Steps 4 to 14, we recursively ?process? ele-
ments ofD one by one, by which we mean that the
constructors in S are employed to combine these
elements with other elements of D (e.g., an ele-
ment is intersected with all other elements, and so
on). We use fetch(D) to retrieve the first unpro-
cessed element of D. New elements are added to
the end of D. Thus D is a first-come-first-served
queue (note that processed elements are not re-
moved from D).
To see in more detail how elements of D are
processed, consider Steps 5-14 once again. For
each element d of D, Step 5 uses a construct s to
extend it:
1. If s is u or unionsq, in Step 7 and 8, we extend d
with each element ofD and add new descrip-
tions to D.
2. If s is ? or ?, in Step 10 and 11, we extend
d with all relations of RN and add new de-
scriptions to D. In Areces et el.?s work, ? is
also available when using ? and ? together,
however due to their algorithm they can never
generates descriptions like ?r.A.
3. If s is ?,? or =, in Step 13 and 14, we ex-
tend d with all relations in RN and all num-
bers in N , and add new descriptions to D.
Because the = construct can be equivalently
substituted by the combination of?,? and u
constructs (= kr.d is semantically equivalent
to ? kr.du ? kr.d), it is a modelling choice
to use either ?,?, or only =, or all of them.
In this algorithm we use them all.
Because we compute only the NNF and we
disallow the use of individual identifiers, nega-
tion ? appears only in front of atomic concept
names. For this reason, processing does not con-
sider s = ?. Note that GROWL leaves some
important issues open. In particular, the or-
der in which constructs, relations, integers and
conjuncts/disjuncts are chosen is left unspecified.
Note that D,RN,N, S are all assumed to be fi-
nite, hence Steps 5 to 14 terminate for a given
d ? D. Because Steps 5 to 14 generate descrip-
tions whose depth increases with one constructor
at a time, there are finitely many d ? D such that
|d| = n (for a given n).
GROWL extends the algorithm presented by
Areces et al The example in Fig.2 shows that
many referring expressions generated by our algo-
rithm cannot be generated by our predecessors; in
fact, some objects that are not referable for them
are referable by GROWL. For example, if we ap-
ply the algorithm to the KB in Fig.2, a possible
solution is as follows:
1. {w1} = Womanu??feed.Cat, the woman
that does not feed all cats.
2. {w2} =? 0?feed.Cat , the woman that
feeds all cats.
3. {d1} = Dogu ? 0?feed?.Woman, the
dog that is fed by all women.
4. {d2} = Dog u ??feed?.Woman, the dog
that is not fed by all women.
5. {c1} = Catu ? 0?feed?.Woman, the cat
that is fed by all women.
6. {c2} = Cat u ??feed?.Woman, the cat
that is not fed by all women.
It is worth reiterating here that our algorithm fo-
cusses on finding uniquely referring expressions,
leaving aside which of all the possible ways in
which an object can be referred to is ?best?. For
this reason, empirical validation of our algorithm
? a very sizable enterprise in itself, which should
probably be based on descriptions elicited by hu-
man speakers ? is not yet in order.
4.2 Discussion
Let us revisit the basic assumptions of Sec.2.2, to
see what can be achieved if they are abandoned.
1. In natural language, people do using names,
e.g. ?the husband of Marie Curie?. To allow
REs of this kind, we can extend our Algo-
rithm A-1 by including singleton classes such
as {Maria Curie} in CN .
2. Traditional GRE approaches have always as-
sumed a single model with complete knowl-
edge. Without this assumption, our approach
can still find interesting REs. For example,
if a man?s nationality is unknown, but he is
known to be the Chinese or Japanese, we can
refer to him/her as Chinese unionsq Japanese.
However, models should be finite to guaran-
tee that N is finite.
3. Individuals with multiple names. DL im-
poses the UNA by explicitly asserting the
inequality of each two individuals. With-
out UNA, reasoning can still infer some re-
sults, e.g. {Woman uMan v ?, David :
Man,May : Woman} |= David 6= May.
Thus we can refer to David as ?the man? if
the domain is closed.
5 Widening the remit of GRE
This paper has shown some of the benefits that
arise when the power of KR is brought to bear
on an important problem in NLG, namely the gen-
eration of referring expressions (GRE). We have
done this by using DL as a representation and
reasoning formalism, extending previous work in
GRE in two ways. First, we have extended GRE
by allowing the generation of REs that involve
quantifiers other than ?. By relating our algo-
rithm to the theory of Generalised Quantifiers, we
were able to formally characterise the set of quan-
tifiers supported by our algorithm, making exact
how much expressive power we have gained. Sec-
ondly, we have demonstrated the benefits of im-
plicit knowledge through inferences that exploit
TBox-information, thereby allowing facts to be
represented more efficiently and elegantly, and al-
lowing GRE to tap into kinds of generic (as op-
posed to atomic) knowledge that it had so far left
aside, except for hints in (Gardent and Striegnitz,
2007) and in (Croitoru and van Deemter, 2007).
Thirdly, we have allowed GRE to utilise incom-
plete knowledge, as when we refer to someone as
?the man of Japanese or Chinese nationality?.
Current work on reference is overwhelmingly
characterised by an emphasis on empirical accu-
racy, often focussing on very simple referring ex-
pressions, which are constituted by conjunctions
of 1-place relations (as in ?the grey poodle?), and
asking which of these conjunctions are most likely
to be used by human speakers (or which of these
would be most useful to a human hearer). The
present work stresses different concerns: we have
focussed on questions of expressive power, fo-
cussing on relatively complex descriptions, asking
what referring expressions are possible when re-
lations between domain objects are used. We be-
lieve that, at the present stage of work in GRE, it
is of crucial importance to gain insight into ques-
tions of this kind, since this will tell us what types
of reference are possible in principle. Once such
insight, we hope to explore how the newly gained
expressive power can be put to practical use.
References
Douglas Appelt. 1985. Planning English Sentences.
Cambridge University Press, Cambridge, UK.
Carlos Areces, Alexander Koller, and Kristina Strieg-
nitz. 2008. Referring expressions as formulas of
description logic. In Proceedings of the 5th INLG,
Salt Fork, Ohio.
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic
evaluation measures for referring expression gener-
ation. In HLT ?08: Proceedings of the 46th Annual
Meeting of the Association for Computational Lin-
guistics on Human Language Technologies, pages
197?200.
Madalina Croitoru and Kees van Deemter. 2007. A
conceptual graph approach to the generation of re-
ferring expressions. In Proceedings of the 20th IJ-
CAI.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretations of the gricean maxims in the generation
of referring expressions. CoRR, cmp-lg/9504020.
Claire Gardent and Kristina Striegnitz. 2007. Gen-
erating bridging definite descriptions. Computing
Meaning, 3:369?396.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The
TUNA-REG Challenge 2009: Overview and eval-
uation results. In Proceedings of the 12th ENLG
(ENLG 2009), pages 174?182, Athens, Greece,
March. Association for Computational Linguistics.
Ian Horrocks, Oliver Kutz, and Ulrike Sattler. 2006.
The Even More Irresistible SROIQ. In KR 2006.
Emiel Krahmer and Mariet Theune. 2002. Efficient
context-sensitive generation of descriptions in con-
text. Information Sharing: Givenness and Newness
in Language, pages 223?264.
Emiel Krahmer, Sebastiaan van Erk, and Andr Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
A Mostowski. 1957. On a generalization of quanti-
fiers. Fund. Math., 44:235?273.
Boris Motik, Bernardo Cuenca Grau, Ian Horrocks,
Zhe Wu, Achille Fokoue, and Carsten Lutz. 2008.
Owl 2 web ontology language: Profiles. W3c work-
ing draft, W3C, October.
Inanc? Seylan, Enrico Franconi, and Jos de Bruijn.
2009. Effective query rewriting with ontologies over
dboxes. In IJCAI 2009.
Johan van Benthem. 1986. Essays in Logical Seman-
tics. Reidel.
