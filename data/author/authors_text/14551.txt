  DLUT: Chinese Personal Name Disambiguation with Rich 
Features 
Dongliang Wang 
Department of Computer Science 
and Engineering, Dalian University 
of Technology 
wdl129@163.com 
Degen Huang 
Department of Computer Science 
and Engineering, Dalian University 
of Technology 
huangdg@dlut.edu.cn 
 
Abstract 
In this paper we describe a person clus-
tering system for a given document set 
and report the results we have obtained 
on the test set of Chinese personal name 
(CPN) disambiguation task of CIPS-
SIGHAN 2010. This task consists of 
clustering a set of Xinhua news docu-
ments that mention an ambiguous CPN 
according to named entity in reality. 
Several features including named entities 
(NE) and common nouns generated from 
the documents and a variety of rules are 
employed in our system. This system 
achieves F = 86.36% with B_Cubed 
scoring metrics and F = 90.78% with pu-
rity_based metrics. 
1 Introduction 
As the amount of web information expands at an 
ever more rapid pace, extraction of information 
for specific named entity is more and more im-
portant. Usually there are named-entity ambigu-
ity in web data, for example more than one per-
son use a same name, therefore it is difficult to 
decide which document refers to a specific 
named entity. 
The goal of CPN disambiguation is to cluster-
ing input Xinhua news corpus by the entity each 
document refers to. The new documents which 
span a time of fourteen years are extracted on 
web. 
As description of CPN disambiguation task of 
CIPS-SIGHAN 2010, Chinese personal name 
disambiguation is potentially more challenging 
due to the need for word segmentation, which 
could introduce errors that can in large part be 
avoided in the English task.  
In this paper we employ a CPN disambigua-
tion system that extracts NE and common nouns 
from the input corpus as features, and then com-
putes the similarity of each two documents in the 
corpus based on feature vector. Hierarchical Ag-
glomerative Clustering (HAC) algorithm (AK 
Jain et al, 1999) is used to implement clustering. 
After a great deal of analysis of news corpus, 
we constitute several rules, the experiments 
show that these rules can improve the result of 
this task. 
The remainder of this paper is organized as 
follows. Section 2 introduces the preprocessing 
of test corpus, and in section 3 we present the 
methodology of our system. In section 4 we pre-
sent the experimental results and give a conclu-
sion in section 5. 
2 Preprocessing 
In this step, we mainly complete the works as 
follows. 
Firstly, corpuses including a given name 
string are in different files, one document one 
file. In order to convenient for processing, we 
combine these documents into one file, distin-
guish them with document id.  
Secondly, some news corpuses have several 
subtitles but usually only part of them including 
focused name string, the others are noise of dis-
ambiguate of focused named entity, for example 
a news about sports may contain several subti-
tles about basketball, swimming, race and so on. 
These noises are removed from the corpus by us. 
Lastly, there is a lack of date-line in a few 
documents; in general, these data-lines are rec-
ognized as part of text, they can be recognized 
through simple matching method. Because data-
lines have consistent format as ????**?*?
??. 
3 Methodology 
The system follows a procedure include: word 
segmentation, the detection of ambiguous ob-
jects, feature extractions, computation of docu-
ment similarity and clustering. 
First, the text is segmented by a word segmen-
tation system explored by Luo and Huang 
(2009). The second step is extract all features 
from segmented text, all features are put into 
two feature vectors: NE vector and common 
noun vector. Then we will compute the distance 
between corresponding vectors of each two 
documents, the standard SoftTFIDF (Chen and 
Martin, 2007) are employed to compute the dis-
tance between two feature vectors. Lastly, we 
use the HAC algorithm for clustering of docu-
ments. 
3.1 Word Segmentation 
Word segmentation is a base and difficult work 
of natural language processing (NLP) and a 
precondition of feature extraction. In this paper, 
the word segmentation system explored by Luo 
and Huang (2009) are employed to do this work. 
This system training on the corpus of 2000?s 
?People?s Daily?. In addition, this system can 
recognize named entities including personal 
name, location name and organization name. We 
can extract these NEs by part-of-speech (POS) 
directly. 
3.2 The Detection of Ambiguous Entities 
Given a name string, the documents can be di-
vided into three groups: 
(1) Documents which contain names that are 
exactly match the query name string. 
(2) Documents which contain names that have 
a substring exactly match the query name string. 
(3) Documents which contain the query name 
string that is not personal name. 
After word segmentation, all personal names 
are labeled by system, when we find one per-
sonal name or its substring match the query 
name string; we will cluster this document ac-
cording to the name. If we failure all over the 
document, it?s considered that this document 
belong to category (3), it will be discarded. 
The ambiguous personal name in a document 
may refer to multiple entities, for example a 
news about party of namesakes, but this is a very 
small probability event, so we assume that all 
mentions in one document refers to the same 
entity, viz. ?one person one document?. 
Although we assume that ?one person one 
document?, the same personal name may occur 
more than once. Some times the word segmenta-
tion system will give the same personal name 
different labels in one document, for example a 
personal name ????? may be recognized as 
???? and ????? in different sentence in 
one document. Suppose that P1, P2, ? , Pn are  
recognized names that match the query name 
string, T1, T2, ? , Tn are the corresponding oc-
cur times.  We use the following method to en-
sure the final needed personal name: 
(1) If Ti > Tj for j = 1, 2, ?, i-1, i+1, ? , n, 
Pi is selected as the final needed personal name, 
else go to step (2). 
(2) Define S = { T1, T2, ? , Tn }, E1 = {T11, 
T12, ?, T1m}, E2 = S ? E1 satisfying T11 = T12 
= ...= T1m, E1 ?  S and Ti > Tj (Ti ?  E1, Tj ?  E2). 
Fi shows the word before Pi and Bi after Pi. For 
each Ti ?  E1, connect Fi, Ti and Bi into a new 
string named Ri, we can get R = {R11, R12, ?, 
R1m} corresponding to E1, the longest common 
substring of R are considered the final needed 
personal name. 
3.3 Features 
We define local sentence as sentences which 
contain the query name string, the features ex-
tracted from local sentences named local fea-
tures. Otherwise, all sentences except local sen-
tences in a document are named global sentences; 
the features extracted from global sentences are 
global features. The reason to distinguish them is 
because they have different contribution to simi-
larity computation. Local features are generally 
considered more important than global features, 
therefore a high weight should be given to local 
features. 
Named entities are important information 
about focused name. In this paper, NEs include 
personal names, location names and organization 
names. Location name and organization name 
usually indicate the region and department of 
focused name, and personal names usually have 
high co-occurrence rate, for example ????? 
and ???? are two names of table tennis players, 
so they always appear in a same news document 
about table tennis. The NE features which have 
been tagged by segmentation system can be ex-
tracted from the document directly. 
We also consider the features of common 
nouns. Semantically independent common nouns 
such as person?s job and person?s hobby etc usu-
ally include some useful information about the 
ambiguous object. We attempt to capture these 
noun features and use them as elements in fea-
ture vector. 
Location names in data-line. The location 
name in the data-line indicates the place the 
news had occurred, if two documents have the 
same date-line location name, and then there is a 
good chance that these two documents refer the 
same person.  
Appellation of query name. Appellation usu-
ally demonstrate a person?s identity, for example, 
if the appellation of the query name is ????, it 
shows that he or she is a journalist. As location 
names in data-line, if two query names have the 
same appellation, the possibility of them refer to 
the same person increased. The word segmenta-
tion system doesn?t clearly marked out appella-
tion but marked as common noun. In generally, 
appellations appear neighbor in front of name, so 
we collect the common nouns neighbor front of 
query names as their appellations. 
So far, we have developed four feature vec-
tors: local NE vector, local common noun vector, 
global NE vector and global common noun vec-
tor. Given feature vectors, we need to find a way 
to learn the similarity matrix. In this paper, we 
choose the standard TF-IDF method to calculate 
the similarity matrix. Location name in date-line 
and appellation of query name will be used in 
rule method without similarity calculation. 
3.4 Similarity Matrix 
Given a pair of feature vectors consisting of NEs 
or common nouns, we need to choose a similar-
ity scheme to calculate the similarity matrix. The 
standard TF-IDF method is introduced here, then 
a little change for Chinese string. 
Standard TF-IDF: Given a pair of vector S 
and T, S = (s1, s2, ?, sn), T = (t1, t2, ?, tm). 
Here, si (i = 1, ?, n) and tj (j = 1, ?, m) are NE 
or common noun. We define: 
}),(,,;{
);;(
?
?
>???
=
vwdistTvSww
TSCLOSE
    (1) 
Where dist(w;v) is the Jaro-Winkler dis-
tance function (Winkler, 1999), which will 
be introduced later. 
);(max);( vwdistTwD Tv?=              (2) 
Then the standard TF-IDF SoftTFIDF is com-
puted as: 
=),( TSSoftTFIDF  
),(*),(*),();;( TwDTwVSwVTSCLOSEw? ? ? (3) 
? ?
=
Sw
SwV
SwVSwV
2'
'
),(
),(),(            (4) 
)log(*)1log(),(
,
'
wSw IDFTFSwV +=    (5) 
Where SwTF ,  is the frequency of substring 
w in S, and wIDF is the inverse of the fraction 
of documents in the corpus that contain w . Sup-
pose Nt is total number of documents, Nw is total 
number of documents which contain w . Then 
wIDF  computed as: 
w
t
N
NIDF =?                          (6) 
The Jaro-Winkler distance Jw of two given 
strings s1 and s2 as shown in formula (7), l is 
the length of common prefix at the start of the 
string up to a maximum of 4 characters, p is a 
constant scaling factor for how much the score is 
adjusted upwards for having common prefixes, 
the value for p is 0.1. 
)1( jjw dlpdd ?+=                    (7) 
)|2||1|( m
tm
s
m
s
md j
?
++=              (8) 
In formula (8) m is the number of matching 
characters, t is the number of transpositions. In 
order to be consistent with the English strings, a 
Chinese character is seen as two English charac-
ters. 
Corresponding to four feature vectors, we can 
calculate the four similarities: S(gNE), S(gCN), 
S(lNE), S(lCN). The similarity between two 
documents (DS) is computed as: 
 
2
)()(
*)1(
2
)()(
*
gCNSlCNSgNESlNES
DS
+
?+
+
=
??                                                                         
(9) 
As time is tight, we just give ?  a value of 0.8 
with out experiment because we consider NEs 
have stronger instructions. 
3.5 Clustering 
Clustering is a key work of this task, it is very 
important to choose a clustering algorithm. Here 
we use HAC algorithm to do clustering. HAC 
algorithm is an unsupervised clustering algo-
rithm, which can be described as follows: 
(1) Initialization. Every document is re-
garded as a separate class. 
(2) Repetition. Computing the similarity of 
each of the two classes, merge the two classes 
whose similarity are the highest and higher than 
the threshold value of ?  into a new class. 
(3) Termination. Repeat step (2) until all 
classes don?t satisfy the clustering condition. 
Suppose document class F = {f1, f2, ?, fn} 
and K = {k1, k2, ?, km}, fi and kj are documents 
in class F and class K, then the similarity be-
tween F and K is: 
nm
kfS
KJS ji ji
*
),(
),( ,?=                  (9) 
If two documents have different query name, 
obviously they refer to different person, only 
documents which have same query name will be 
clustered. Before clustering, several rules are 
afforded to improve the clustering condition. 
These rules are generally applicable to news 
corpus. 
(1) If two documents have the same query 
name and both of them are reporter, and both 
date-lines have the same location name, then 
combine the two documents into one class. 
(2) If two documents have the same query 
name and another same personal name, then 
combine the two documents into one class. 
(3) If two documents have the same query 
name and both date-lines have the same location 
name, then double the similarity, else halve the 
similarity. 
(4) If two documents have the same query 
name and both personal names have the same 
appellation, then double the similarity, else halve 
the similarity. 
4 Evaluation 
In order to prove the validity of the rule ap-
proach, a group of experiments are performed on 
the train set of Chinese personal name disam-
biguation task of CIPS-SIGHAN 2010. The re-
sult is shown in Table 1. R1 is the result without 
rules, and R2 shows the accuracy after adding 
the rules. 
The system performance on the test set of 
CPN disambiguation task of CIPS-SIGHAN 
2010 is F = 90.78% evaluated with P_IP evalua-
tion, and F = 86.36% with B_Cubed evaluation. 
The accuracy is shown in Table 2. 
 
 
B_Cubed Precision Recall F 
R1 70.56 86.77 74.74 
R2 78.05 84.99 79.60 
P_IP Purity Inverse 
Purity 
F 
R1 77.22 90.48 81.20 
R2 82.92 88.30 84.29 
Table 1. Experimental results for system with 
rules and without rules on training set 
 
B_Cubed Precision Recall F 
 82.96 91.33 86.36 
P_IP Purity Inverse 
Purity 
F 
 87.94 94.21 90.78 
Table 2. The results on test set 
5  Conclusion 
We described our system that disambiguates 
Chinese personal names in Xinhua corpus. We 
mainly focus on extracting rich features from 
documents and computing the similarity of each 
two documents. Several rules are introduced to 
improve the accuracy and have proved effective. 
References 
Anil K. Jain, M. Narasimha Murty, and Patrick J. 
Flynn. 1999. Data clustering: A review. ACM 
Computing Surveys, 31(3): 264-323. 
Bradley Malin. 2005. Unsupervised Name Disam-
biguation via Network Similarity. In proceedings 
SIAM Conference on Data Mining, 2005. 
Chen Ying, James Martin. 2007. CU-COMSEM: Ex-
ploring Rich Features for Unsupervised Web Per-
sonal Name Disambiguation. In proceedings of 
Semeval 2007, Association for Computational 
Linguistics, 2007. 
Chen Ying, Sophia Y. M. Lee and Churen Huang. 
2009. PolyUHK:A Robust Information Extraction 
System for Web Personal Names. In proceedings 
of Semeval 2009, Association for Computational 
Linguistics, 2009. 
Gusfield, Dan. 1997. Algorithms on Strings, Trees 
and Sequences. Cambridge University Press, 
Cambridge, UK 
Javier Artiles, J. Gonzalo and S. Sekine. WePS2 
Evaluation Campaign: Overview of the Web Peo-
ple Search Clustering Task. In proceedings of Se-
meval 2009, Association for Computational Lin-
guistics, 2009. 
Luo Yanyan, Degen Huang. 2009. Chinese word seg-
mentation based on the marginal probabilities 
Generated by CRFs. Journal of Chinese Informa-
tion Processing, 23(5): 3-8. 
Octavian Popescu, B. Magnini. 2007. IRST-BP: Web 
People Search Using Name Entities. In proceed-
ings of Semeval 2007, Association for Computa-
tional Linguistics, 2007. 
William E. Winkler. 1999. The state of record linkage 
and current research problems. Statistics of In-
come Division, Internal Revenue Service Publica-
tion R99/04. 
 
