Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 359?367,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Recognizing Named Entities in Tweets
Xiaohua Liu ? ?, Shaodian Zhang? ?, Furu Wei ?, Ming Zhou ?
?School of Computer Science and Technology
Harbin Institute of Technology, Harbin, 150001, China
?Department of Computer Science and Engineering
Shanghai Jiao Tong University, Shanghai, 200240, China
?Microsoft Research Asia
Beijing, 100190, China
?{xiaoliu, fuwei, mingzhou}@microsoft.com
? zhangsd.sjtu@gmail.com
Abstract
The challenges of Named Entities Recogni-
tion (NER) for tweets lie in the insufficient
information in a tweet and the unavailabil-
ity of training data. We propose to com-
bine a K-Nearest Neighbors (KNN) classi-
fier with a linear Conditional Random Fields
(CRF) model under a semi-supervised learn-
ing framework to tackle these challenges. The
KNN based classifier conducts pre-labeling to
collect global coarse evidence across tweets
while the CRF model conducts sequential la-
beling to capture fine-grained information en-
coded in a tweet. The semi-supervised learn-
ing plus the gazetteers alleviate the lack of
training data. Extensive experiments show the
advantages of our method over the baselines
as well as the effectiveness of KNN and semi-
supervised learning.
1 Introduction
Named Entities Recognition (NER) is generally un-
derstood as the task of identifying mentions of rigid
designators from text belonging to named-entity
types such as persons, organizations and locations
(Nadeau and Sekine, 2007). Proposed solutions to
NER fall into three categories: 1) The rule-based
(Krupka and Hausman, 1998); 2) the machine learn-
ing based (Finkel and Manning, 2009; Singh et al,
2010) ; and 3) hybrid methods (Jansche and Abney,
2002). With the availability of annotated corpora,
such as ACE05, Enron (Minkov et al, 2005) and
? This work has been done while the author was visiting
Microsoft Research Asia.
CoNLL03 (Tjong Kim Sang and DeMeulder, 2003),
the data driven methods now become the dominating
methods.
However, current NER mainly focuses on for-
mal text such as news articles (Mccallum and Li,
2003; Etzioni et al, 2005). Exceptions include stud-
ies on informal text such as emails, blogs, clini-
cal notes (Wang, 2009). Because of the domain
mismatch, current systems trained on non-tweets
perform poorly on tweets, a new genre of text,
which are short, informal, ungrammatical and noise
prone. For example, the average F1 of the Stan-
ford NER (Finkel et al, 2005) , which is trained
on the CoNLL03 shared task data set and achieves
state-of-the-art performance on that task, drops from
90.8% (Ratinov and Roth, 2009) to 45.8% on tweets.
Thus, building a domain specific NER for tweets
is necessary, which requires a lot of annotated tweets
or rules. However, manually creating them is tedious
and prohibitively unaffordable. Proposed solutions
to alleviate this issue include: 1) Domain adaption,
which aims to reuse the knowledge of the source do-
main in a target domain. Two recent examples are
Wu et al (2009), which uses data that is informa-
tive about the target domain and also easy to be la-
beled to bridge the two domains, and Chiticariu et
al. (2010), which introduces a high-level rule lan-
guage, called NERL, to build the general and do-
main specific NER systems; and 2) semi-supervised
learning, which aims to use the abundant unlabeled
data to compensate for the lack of annotated data.
Suzuki and Isozaki (2008) is one such example.
Another challenge is the limited information in
tweet. Two factors contribute to this difficulty. One
359
is the tweet?s informal nature, making conventional
features such as part-of-speech (POS) and capital-
ization not reliable. The performance of current
NLP tools drops sharply on tweets. For example,
OpenNLP 1, the state-of-the-art POS tagger, gets
only an accuracy of 74.0% on our test data set. The
other is the tweet?s short nature, leading to the ex-
cessive abbreviations or shorthand in tweets, and
the availability of very limited context information.
Tackling this challenge, ideally, requires adapting
related NLP tools to fit tweets, or normalizing tweets
to accommodate existing tools, both of which are
hard tasks.
We propose a novel NER system to address these
challenges. Firstly, a K-Nearest Neighbors (KNN)
based classifier is adopted to conduct word level
classification, leveraging the similar and recently
labeled tweets. Following the two-stage predic-
tion aggregation methods (Krishnan and Manning,
2006), such pre-labeled results, together with other
conventional features used by the state-of-the-art
NER systems, are fed into a linear Conditional Ran-
dom Fields (CRF) (Lafferty et al, 2001) model,
which conducts fine-grained tweet level NER. Fur-
thermore, the KNN and CRF model are repeat-
edly retrained with an incrementally augmented
training set, into which high confidently labeled
tweets are added. Indeed, it is the combination of
KNN and CRF under a semi-supervised learning
framework that differentiates ours from the exist-
ing. Finally, following Lev Ratinov and Dan Roth
(2009), 30 gazetteers are used, which cover com-
mon names, countries, locations, temporal expres-
sions, etc. These gazetteers represent general knowl-
edge across domains. The underlying idea of our
method is to combine global evidence from KNN
and the gazetteers with local contextual information,
and to use common knowledge and unlabeled tweets
to make up for the lack of training data.
12,245 tweets are manually annotated as the test
data set. Experimental results show that our method
outperforms the baselines. It is also demonstrated
that integrating KNN classified results into the CRF
model and semi-supervised learning considerably
boost the performance.
Our contributions are summarized as follows.
1http://sourceforge.net/projects/opennlp/
1. We propose to a novel method that combines
a KNN classifier with a conventional CRF
based labeler under a semi-supervised learning
framework to combat the lack of information in
tweet and the unavailability of training data.
2. We evaluate our method on a human anno-
tated data set, and show that our method outper-
forms the baselines and that both the combina-
tion with KNN and the semi-supervised learn-
ing strategy are effective.
The rest of our paper is organized as follows. In
the next section, we introduce related work. In Sec-
tion 3, we formally define the task and present the
challenges. In Section 4, we detail our method. In
Section 5, we evaluate our method. Finally, Section
6 concludes our work.
2 Related Work
Related work can be roughly divided into three cat-
egories: NER on tweets, NER on non-tweets (e.g.,
news, bio-logical medicine, and clinical notes), and
semi-supervised learning for NER.
2.1 NER on Tweets
Finin et al (2010) use Amazons Mechanical Turk
service 2 and CrowdFlower 3 to annotate named en-
tities in tweets and train a CRF model to evaluate
the effectiveness of human labeling. In contrast, our
work aims to build a system that can automatically
identify named entities in tweets. To achieve this,
a KNN classifier with a CRF model is combined
to leverage cross tweets information, and the semi-
supervised learning is adopted to leverage unlabeled
tweets.
2.2 NER on Non-Tweets
NER has been extensively studied on formal text,
such as news, and various approaches have been pro-
posed. For example, Krupka and Hausman (1998)
use manual rules to extract entities of predefined
types; Zhou and Ju (2002) adopt Hidden Markov
Models (HMM) while Finkel et al (2005) use CRF
to train a sequential NE labeler, in which the BIO
(meaning Beginning, the Inside and the Outside of
2https://www.mturk.com/mturk/
3http://crowdflower.com/
360
an entity, respectively) schema is applied. Other
methods, such as classification based on Maximum
Entropy models and sequential application of Per-
ceptron or Winnow (Collins, 2002), are also prac-
ticed. The state-of-the-art system, e.g., the Stanford
NER, can achieve an F1 score of over 92.0% on its
test set.
Biomedical NER represents another line of active
research. Machine learning based systems are com-
monly used and outperform the rule based systems.
A state-of-the-art biomedical NER system (Yoshida
and Tsujii, 2007) uses lexical features, orthographic
features, semantic features and syntactic features,
such as part-of-speech (POS) and shallow parsing.
A handful of work on other domains exists. For
example, Wang (2009) introduces NER on clinical
notes. A data set is manually annotated and a linear
CRF model is trained, which achieves an F-score of
81.48% on their test data set; Downey et al (2007)
employ capitalization cues and n-gram statistics to
locate names of a variety of classes in web text;
most recently, Chiticariu et al (2010) design and im-
plement a high-level language NERL that is tuned
to simplify the process of building, understanding,
and customizing complex rule-based named-entity
annotators for different domains.
Ratinov and Roth (2009) systematically study
the challenges in NER, compare several solutions
and report some interesting findings. For exam-
ple, they show that a conditional model that does
not consider interactions at the output level per-
forms comparably to beam search or Viterbi, and
that the BILOU (Beginning, the Inside and the Last
tokens of multi-token chunks as well as Unit-length
chunks) encoding scheme significantly outperforms
the BIO schema (Beginning, the Inside and Outside
of a chunk).
In contrast to the above work, our study focuses
on NER for tweets, a new genre of texts, which are
short, noise prone and ungrammatical.
2.3 Semi-supervised Learning for NER
Semi-supervised learning exploits both labeled and
un-labeled data. It proves useful when labeled data
is scarce and hard to construct while unlabeled data
is abundant and easy to access.
Bootstrapping is a typical semi-supervised learn-
ing method. It iteratively adds data that has been
confidently labeled but is also informative to its
training set, which is used to re-train its model. Jiang
and Zhai (2007) propose a balanced bootstrapping
algorithm and successfully apply it to NER. Their
method is based on instance re-weighting, which
allows the small amount of the bootstrapped train-
ing sets to have an equal weight to the large source
domain training set. Wu et al (2009) propose an-
other bootstrapping algorithm that selects bridging
instances from an unlabeled target domain, which
are informative about the target domain and are also
easy to be correctly labeled. We adopt bootstrapping
as well, but use human labeled tweets as seeds.
Another representative of semi-supervised learn-
ing is learning a robust representation of the input
from unlabeled data. Miller et al (2004) use word
clusters (Brown et al, 1992) learned from unla-
beled text, resulting in a performance improvement
of NER. Guo et al (2009) introduce Latent Seman-
tic Association (LSA) for NER. In our pilot study of
NER for tweets, we adopt bag-of-words models to
represent a word in tweet, to concentrate our efforts
on combining global evidence with local informa-
tion and semi-supervised learning. We leave it to
our future work to explore which is the best input
representation for our task.
3 Task Definition
We first introduce some background about tweets,
then give a formal definition of the task.
3.1 The Tweets
A tweet is a short text message containing no
more than 140 characters in Twitter, the biggest
micro-blog service. Here is an example of
tweets: ?mycraftingworld: #Win Microsoft Of-
fice 2010 Home and Student *2Winners* #Con-
test from @office and @momtobedby8 #Giveaway
http://bit.ly/bCsLOr ends 11/14?, where ?mycraft-
ingworld? is the name of the user who published
this tweet. Words beginning with the ?#? char-
acter, like ??#Win?, ?#Contest? and ?#Giveaway?,
are hash tags, usually indicating the topics of the
tweet; words starting with ?@?, like ?@office?
and ?@momtobedby8?, represent user names, and
?http://bit.ly/bCsLOr? is a shortened link.
Twitter users are interested in named entities, such
361
Figure 1: Portion of different types of named entities in
tweets. This is based on an investigation of 12,245 ran-
domly sampled tweets, which are manually labeled.
as person names, organization names and product
names, as evidenced by the abundant named entities
in tweets. According to our investigation on 12,245
randomly sampled tweets that are manually labeled,
about 46.8% have at least one named entity. Figure
1 shows the portion of named entities of different
types.
3.2 The Task
Given a tweet as input, our task is to identify both the
boundary and the class of each mention of entities of
predefined types. We focus on four types of entities
in our study, i.e., persons, organizations, products,
and locations, which, according to our investigation
as shown in Figure 1, account for 89.0% of all the
named entities.
Here is an example illustrating our task.
The input is ?...Me without you is like an
iphone without apps, Justin Bieber without
his hair, Lady gaga without her telephone, it
just wouldn...? The expected output is as fol-
lows:?...Me without you is like an <PRODUCT
>iphone</PRODUCT>without apps,
<PERSON>Justin Bieber</PERSON>without his
hair,<PERSON>Lady gaga</PERSON> without
her telephone, it just wouldn...?, meaning that
?iphone? is a product, while ?Justin Bieber? and
?Lady gaga? are persons.
4 Our Method
Now we present our solution to the challenging task
of NER for tweets. An overview of our method
is first given, followed by detailed discussion of its
core components.
4.1 Method Overview
NER task can be naturally divided into two sub-
tasks, i.e., boundary detection and type classifica-
tion. Following the common practice , we adopt
a sequential labeling approach to jointly resolve
these sub-tasks, i.e., for each word in the input
tweet, a label is assigned to it, indicating both the
boundary and entity type. Inspired by Ratinov and
Roth (2009), we use the BILOU schema.
Algorithm 1 outlines our method, where: trains
and traink denote two machine learning processes
to get the CRF labeler and the KNN classifier, re-
spectively; reprw converts a word in a tweet into a
bag-of-words vector; the reprt function transforms
a tweet into a feature matrix that is later fed into the
CRF model; the knn function predicts the class of
a word; the update function applies the predicted
class by KNN to the inputted tweet; the crf function
conducts word level NE labeling;? and ? represent
the minimum labeling confidence of KNN and CRF,
respectively, which are experimentally set to 0.1 and
0.001; N (1,000 in our work) denotes the maximum
number of new accumulated training data.
Our method, as illustrated in Algorithm 1, repeat-
edly adds the new confidently labeled tweets to the
training set 4 and retrains itself once the number
of new accumulated training data goes above the
threshold N . Algorithm 1 also demonstrates one
striking characteristic of our method: A KNN clas-
sifier is applied to determine the label of the current
word before the CRF model. The labels of the words
that confidently assigned by the KNN classifier are
treated as visible variables for the CRF model.
4.2 Model
Our model is hybrid in the sense that a KNN clas-
sifier and a CRF model are sequentially applied to
the target tweet, with the goal that the KNN classi-
fier captures global coarse evidence while the CRF
model fine-grained information encoded in a single
tweet and in the gazetteers. Algorithm 2 outlines the
training process of KNN, which records the labeled
word vector for every type of label.
Algorithm 3 describes how the KNN classifier
4The training set ts has a maximum allowable number of
items, which is 10,000 in our work. Adding an item into it will
cause the oldest one being removed if it is full.
362
Algorithm 1 NER for Tweets.
Require: Tweet stream i; output stream o.
Require: Training tweets ts; gazetteers ga.
1: Initialize ls, the CRF labeler: ls = trains(ts).
2: Initialize lk, the KNN classifier: lk = traink(ts).
3: Initialize n, the # of new training tweets: n = 0.
4: while Pop a tweet t from i and t ?= null do
5: for Each word w ? t do
6: Get the feature vector w?: w? =
reprw(w, t).
7: Classify w? with knn: (c, cf) =
knn(lk, w?).
8: if cf > ? then
9: Pre-label: t = update(t, w, c).
10: end if
11: end for
12: Get the feature vector t?: t? = reprt(t, ga).
13: Label t? with crf : (t, cf) = crf(ls, t?).
14: Put labeled result (t, cf) into o.
15: if cf > ? then
16: Add labeled result t to ts , n = n + 1.
17: end if
18: if n > N then
19: Retrain ls: ls = trains(ts).
20: Retrain lk: lk = traink(ts).
21: n = 0.
22: end if
23: end while
24: return o.
Algorithm 2 KNN Training.
Require: Training tweets ts.
1: Initialize the classifier lk:lk = ?.
2: for Each tweet t ? ts do
3: for Each word,label pair (w, c) ? t do
4: Get the feature vector w?: w? =
reprw(w, t).
5: Add the w? and c pair to the classifier: lk =
lk ? {(w?, c)}.
6: end for
7: end for
8: return KNN classifier lk.
predicts the label of the word. In our work, K is
experimentally set to 20, which yields the best per-
formance.
Two desirable properties of KNN make it stand
out from its alternatives: 1) It can straightforwardly
incorporate evidence from new labeled tweets and
retraining is fast; and 2) combining with a CRF
Algorithm 3 KNN predication.
Require: KNN classifier lk ;word vector w?.
1: Initialize nb, the neighbors of w?: nb =
neigbors(lk, w?).
2: Calculate the predicted class c?: c? =
argmaxc
?
(w?? ,c? )?nb ?(c, c
?) ? cos(w?, w??).
3: Calculate the labeling confidence cf : cf =
?
(w?? ,c? )?nb ?(c,c
?
)?cos(w?,w?
?
)
?
(w?? ,c? )?nb cos(w?,w?
? ) .
4: return The predicted label c? and its confidence cf .
model, which is good at encoding the subtle interac-
tions between words and their labels, compensates
for KNN?s incapability to capture fine-grained evi-
dence involving multiple decision points.
The Linear CRF model is used as the fine model,
with the following considerations: 1) It is well-
studied and has been successfully used in state-of-
the-art NER systems (Finkel et al, 2005; Wang,
2009); 2) it can output the probability of a label
sequence, which can be used as the labeling con-
fidence that is necessary for the semi-supervised
learning framework.
In our experiments, the CRF++ 5 toolkit is used to
train a linear CRF model. We have written a Viterbi
decoder that can incorporate partially observed la-
bels to implement the crf function in Algorithm 1.
4.3 Features
Given a word in a tweet, the KNN classifier consid-
ers a text window of size 5 with the word in the mid-
dle (Zhang and Johnson, 2003), and extracts bag-of-
word features from the window as features. For each
word, our CRF model extracts similar features as
Wang (2009) and Ratinov and Roth (2009), namely,
orthographic features, lexical features and gazetteers
related features. In our work, we use the gazetteers
provided by Ratinov and Roth (2009).
Two points are worth noting here. One is that
before feature extraction for either the KNN or the
CRF, stop words are removed. The stop words
used here are mainly from a set of frequently-used
words 6. The other is that tweet meta data is normal-
ized, that is, every link becomes *LINK* and every
5http://crfpp.sourceforge.net/
6http://www.textfixer.com/resources/common-english-
words.txt
363
account name becomes *ACCOUNT*. Hash tags
are treated as common words.
4.4 Discussion
We now discuss several design considerations re-
lated to the performance of our method, i.e., addi-
tional features, gazetteers and alternative models.
Additional Features. Features related to chunking
and parsing are not adopted in our final system, be-
cause they give only a slight performance improve-
ment while a lot of computing resources are required
to extract such features. The ineffectiveness of these
features is linked to the noisy and informal nature of
tweets. Word class (Brown et al, 1992) features are
not used either, which prove to be unhelpful for our
system. We are interested in exploring other tweet
representations, which may fit our NER task, for ex-
ample the LSA models (Guo et al, 2009).
Gazetteers. In our work, gazetteers prove to be sub-
stantially useful, which is consistent with the obser-
vation of Ratinov and Roth (2009). However, the
gazetteers used in our work contain noise, which
hurts the performance. Moreover, they are static,
directly from Ratinov and Roth (2009), thus with
a relatively lower coverage, especially for person
names and product names in tweets. We are devel-
oping tools to clean the gazetteers. In future, we plan
to feed the fresh entities correctly identified from
tweets back into the gazetteers. The correctness of
an entity can rely on its frequency or other evidence.
Alternative Models. We have replaced KNN by
other classifiers, such as those based on Maximum
Entropy and Support Vector Machines, respectively.
KNN consistently yields comparable performance,
while enjoying a faster retraining speed. Similarly,
to study the effectiveness of the CRF model, it is re-
placed by its alternations, such as the HMM labeler
and a beam search plus a maximum entropy based
classifier. In contrast to what is reported by Ratinov
and Roth (2009), it turns out that the CRF model
gives remarkably better results than its competitors.
Note that all these evaluations are on the same train-
ing and testing data sets as described in Section 5.1.
5 Experiments
In this section, we evaluate our method on a man-
ually annotated data set and show that our system
outperforms the baselines. The contributions of the
combination of KNN and CRF as well as the semi-
supervised learning are studied, respectively.
5.1 Data Preparation
We use the Twigg SDK 7 to crawl all tweets
from April 20th 2010 to April 25th 2010, then drop
non-English tweets and get about 11,371,389, from
which 15,800 tweets are randomly sampled, and are
then labeled by two independent annotators, so that
the beginning and the end of each named entity are
marked with<TYPE> and</TYPE>, respectively.
Here TYPE is PERSON, PRODUCT, ORGANIZA-
TION or LOCATION. 3555 tweets are dropped be-
cause of inconsistent annotation. Finally we get
12,245 tweets, forming the gold-standard data set.
Figure 1 shows the portion of named entities of dif-
ferent types. On average, a named entity has 1.2
words. The gold-standard data set is evenly split into
two parts: One for training and the other for testing.
5.2 Evaluation Metrics
For every type of named entity, Precision (Pre.), re-
call (Rec.) and F1 are used as the evaluation met-
rics. Precision is a measure of what percentage the
output labels are correct, and recall tells us to what
percentage the labels in the gold-standard data set
are correctly labeled, while F1 is the harmonic mean
of precision and recall. For the overall performance,
we use the average Precision, Recall and F1, where
the weight of each name entity type is proportional
to the number of entities of that type. These metrics
are widely used by existing NER systems to evaluate
their performance.
5.3 Baselines
Two systems are used as baselines: One is the
dictionary look-up system based on the gazetteers;
the other is the modified version of our system
without KNN and semi-supervised learning. Here-
after these two baselines are called NERDIC and
NERBA, respectively. The OpenNLP and the Stan-
ford parser (Klein and Manning, 2003) are used to
extract linguistic features for the baselines and our
method.
7It is developed by the Bing social search team, and cur-
rently is only internally available.
364
System Pre.(%) Rec.(%) F1(%)
NERCB 81.6 78.8 80.2
NERBA 83.6 68.6 75.4
NERDIC 32.6 25.4 28.6
Table 1: Overall experimental results.
System Pre.(%) Rec.(%) F1(%)
NERCB 78.4 74.5 76.4
NERBA 83.6 68.4 75.2
NERDIC 37.1 29.7 33.0
Table 2: Experimental results on PERSON.
5.4 Basic Results
Table 1 shows the overall results for the baselines
and ours with the name NERCB . Here our sys-
tem is trained as described in Algorithm 1, combin-
ing a KNN classifier and a CRF labeler, with semi-
supervised learning enabled. As can be seen from
Table 1, on the whole, our method significantly out-
performs (with p < 0.001) the baselines. Tables 2-5
report the results on each entity type, indicating that
our method consistently yields better results on all
entity types.
5.5 Effects of KNN Classifier
Table 6 shows the performance of our method
without combining the KNN classifier, denoted by
NERCB?KNN . A drop in performance is observed
then. We further check the confidently predicted la-
bels of the KNN classifier, which account for about
22.2% of all predications, and find that its F1 is as
high as 80.2% while the baseline system based on
the CRF model achieves only an F1 of 75.4%. This
largely explains why the KNN classifier helps the
CRF labeler. The KNN classifier is replaced with
its competitors, and only a slight difference in per-
formance is observed. We do observe that retraining
KNN is obviously faster.
System Pre.(%) Rec.(%) F1(%)
NERCB 81.3 65.4 72.5
NERBA 82.5 58.4 68.4
NERDIC 8.2 6.1 7.0
Table 3: Experimental results on PRODUCT.
System Pre.(%) Rec.(%) F1(%)
NERCB 80.3 77.5 78.9
NERBA 81.6 69.7 75.2
NERDIC 30.2 30.0 30.1
Table 4: Experimental results on LOCATION.
System Pre.(%) Rec.(%) F1(%)
NERCB 83.2 60.4 70.0
NERBA 87.6 52.5 65.7
NERDIC 54.5 11.8 19.4
Table 5: Experimental results on ORGANIZATION.
5.6 Effects of the CRF Labeler
Similarly, the CRF model is replaced by its alterna-
tives. As is opposite to the finding of Ratinov and
Roth (2009), the CRF model gives remarkably bet-
ter results, i.e., 2.1% higher in F1 than its best fol-
lowers (with p < 0.001). Table 7 shows the overall
performance of the CRF labeler with various feature
set combinations, where Fo, Fl and Fg denote the
orthographic features, the lexical features and the
gazetteers related features, respectively. It can be
seen from Table 7 that the lexical and gazetteer re-
lated features are helpful. Other advanced features
such as chunking are also explored but with no sig-
nificant improvement.
5.7 Effects of Semi-supervised Learning
Table 8 compares our method with its modified ver-
sion without semi-supervised learning, suggesting
that semi-supervised learning considerably boosts
the performance. To get more details about self-
training, we evenly divide the test data into 10 parts
and feed them into our method sequentially; we
record the average F1 score on each part, as shown
in Figure 2.
5.8 Error Analysis
Errors made by our system on the test set fall into
three categories. The first kind of error, accounting
for 35.5% of all errors, is largely related to slang ex-
pressions and informal abbreviations. For example,
our method identifies ?Cali?, which actually means
?California?, as a PERSON in the tweet ?i love Cali
so much?. In future, we can design a normalization
365
System Pre.(%) Rec.(%) F1(%)
NERCB 81.6 78.8 80.2
NERCB?KNN 82.6 74.8 78.5
Table 6: Overall performance of our system with and
without the KNN classifier, respectively.
Features Pre.(%) Rec.(%) F1(%)
Fo 71.3 42.8 53.5
Fo + Fl 76.2 44.2 55.9
Fo + Fg 80.5 66.2 72.7
Fo + Fl + Fg 82.6 74.8 78.5
Table 7: Overview performance of the CRF labeler (com-
bined with KNN) with different feature sets.
component to handle such slang expressions and in-
formal abbreviations.
The second kind of error, accounting for 37.2%
of all errors, is mainly attributed to the data sparse-
ness. For example, for this tweet ?come to see jaxon
someday?, our method mistakenly labels ?jaxon?
as a LOCATION, which actually denotes a PER-
SON. This error is understandable somehow, since
this tweet is one of the earliest tweets that mention
?jaxon?, and at that time there was no strong evi-
dence supporting that it represents a person. Possi-
ble solutions to these errors include continually en-
riching the gazetteers and aggregating additional ex-
ternal knowledge from other channels such as tradi-
tional news.
The last kind of error, which represents 27.3%
of all errors, somehow links to the noise prone na-
ture of tweets. Consider this tweet ?wesley snipes
ws cought 4 nt payin tax coz ths celebz dnt take it
cirus.?, in which ?wesley snipes? is not identified
as a PERSON but simply ignored by our method,
because this tweet is too noisy to provide effective
features. Tweet normalization technology seems a
possible solution to alleviate this kind of error.
Features Pre.(%) Rec.(%) F1(%)
NERCB 81.6 78.8 80.2
NER?CB 82.1 71.9 76.7
Table 8: Performance of our system with and without
semi-supervised learning, respectively.
Figure 2: F1 score on 10 test data sets sequentially fed
into the system, each with 600 instances. Horizontal and
vertical axes represent the sequential number of the test
data set and the averaged F1 score (%), respectively.
6 Conclusions and Future work
We propose a novel NER system for tweets, which
combines a KNN classifier with a CRF labeler under
a semi-supervised learning framework. The KNN
classifier collects global information across recently
labeled tweets while the CRF labeler exploits infor-
mation from a single tweet and from the gazetteers.
A serials of experiments show the effectiveness of
our method, and particularly, show the positive ef-
fects of KNN and semi-supervised learning.
In future, we plan to further improve the per-
formance of our method through two directions.
Firstly, we hope to develop tweet normalization
technology to make tweets friendlier to the NER
task. Secondly, we are interested in integrating
new entities from tweets or other channels into the
gazetteers.
Acknowledgments
We thank Long Jiang, Changning Huang, Yunbo
Cao, Dongdong Zhang, Zaiqing Nie for helpful dis-
cussions, and the anonymous reviewers for their
valuable comments. We also thank Matt Callcut for
his careful proofreading of an early draft of this pa-
per.
References
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18:467?479.
366
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010. Domain adaptation of rule-based annotators
for named-entity recognition tasks. In EMNLP, pages
1002?1012.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In EMNLP, pages 1?8.
Doug Downey, Matthew Broadhead, and Oren Etzioni.
2007. Locating Complex Named Entities in Web Text.
In IJCAI.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell., 165(1):91?134.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in twitter data with crowd-
sourcing. In CSLDAMT, pages 80?88.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Nested named entity recognition. In EMNLP, pages
141?150.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In ACL, pages 363?370.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named entity
recognition. In NAACL, pages 281?289.
Martin Jansche and Steven P. Abney. 2002. Informa-
tion extraction from voicemail transcripts. In EMNLP,
pages 320?327.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In ACL, pages 264?
271.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL, pages 423?430.
Vijay Krishnan and Christopher D. Manning. 2006. An
effective two-stage model for exploiting non-local de-
pendencies in named entity recognition. In ACL, pages
1121?1128.
George R. Krupka and Kevin Hausman. 1998. Isoquest:
Description of the netowlTM extractor system as used
in muc-7. In MUC-7.
John D. Lafferty, AndrewMcCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282?289.
Andrew Mccallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In HLT-NAACL, pages 188?191.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In HLT-NAACL, pages 337?342.
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from email: apply-
ing named entity recognition to informal text. In HLT,
pages 443?450.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Linguisti-
cae Investigationes, 30:3?26.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL, pages 147?155.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In HLT-NAACL, pages 73?81.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In ACL, pages 665?673.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: language-
independent named entity recognition. In HLT-
NAACL, pages 142?147.
Yefeng Wang. 2009. Annotating and recognising named
entities in clinical notes. In ACL-IJCNLP, pages 18?
26.
Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named en-
tity recognition. In EMNLP, pages 1523?1532.
Kazuhiro Yoshida and Jun?ichi Tsujii. 2007. Reranking
for biomedical named-entity recognition. In BioNLP,
pages 209?216.
Tong Zhang and David Johnson. 2003. A robust risk
minimization based named entity recognition system.
In HLT-NAACL, pages 204?207.
GuoDong Zhou and Jian Su. 2002. Named entity recog-
nition using an hmm-based chunk tagger. In ACL,
pages 473?480.
367
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 92?99,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Hedge Detection and Scope Finding by Sequence Labeling
with Normalized Feature Selection?
Shaodian Zhang12, Hai Zhao123?, Guodong Zhou3 and Bao-Liang Lu12
1Center for Brain-Like Computing and Machine Intelligence
Dept of Computer Science and Engineering, Shanghai Jiao Tong University
2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong University
3School of Computer Science and Technology, Soochow University
zhangsd.sjtu@gmail.com, zhaohai@cs.sjtu.edu.cn
gdzhou@suda.edu.cn, blu@cs.sjtu.edu.cn
Abstract
This paper presents a system which adopts
a standard sequence labeling technique for
hedge detection and scope finding. For
the first task, hedge detection, we formu-
late it as a hedge labeling problem, while
for the second task, we use a two-step la-
beling strategy, one for hedge cue label-
ing and the other for scope finding. In par-
ticular, various kinds of syntactic features
are systemically exploited and effectively
integrated using a large-scale normalized
feature selection method. Evaluation on
the CoNLL-2010 shared task shows that
our system achieves stable and competi-
tive results for all the closed tasks. Fur-
thermore, post-deadline experiments show
that the performance can be much further
improved using a sufficient feature selec-
tion.
1 Introduction
Hedges are linguistic devices representing spec-
ulative parts of articles. Previous works such as
(Hyland, 1996; Marco and Mercer, 2004; Light et
al., 2004; Thompson et al, 2008) present research
on hedge mainly as a linguistic phenomenon.
Meanwhile, detecting hedges and their scopes au-
tomatically are increasingly important tasks in nat-
ural language processing and information extrac-
tion, especially in biomedical community. The
shared task of CoNLL-2010 described in Farkas
et al (2010) aims at detecting hedges (task 1)
and finding their scopes (task 2) for the literature
? This work is partially supported by the National
Natural Science Foundation of China (Grants 60903119,
60773090, 90820018 and 90920004), the National Basic Re-
search Program of China (Grant No. 2009CB320901), and
the National High-Tech Research Program of China (Grant
No.2008AA02Z315).
?corresponding author
from BioScope corpus (Szarvas et al, 2008) and
Wikipedia. This paper describes a system adopt-
ing sequence labeling which performs competitive
in the official evaluation, as well as further test.
In addition, a large-scale feature selection proce-
dure is applied in training and development. Con-
sidering that BioScope corpus is annotated by two
independent linguists according to a formal guide-
line (Szarvas, 2008), while Wikipedia weasels are
tagged by netizens who are diverse in background
and various in evaluation criterion, it is needed to
handle them separately. Our system selects fea-
tures for Wikipedia and BioScope corpus indepen-
dently and evaluate them respectively, leading to
fine performances for all of them.
The rest of the paper is organized as follows.
The next section presents the technical details of
our system of hedge detection and scope finding.
Section 3 gives information of features. Section
4 shows the evaluation results, including official
results and further ones after official outputs col-
lection. Section 5 concludes the paper.
2 Methods
Basically, the tasks are formulated as sequence la-
beling in our approach. The available label set dif-
fers between task 1 and 2. In addition, it is needed
to introduce an indicator in order to find scopes for
the multi-hedge sentences properly.
2.1 Hedge detection
The valid label set of task 1, hedge detection, con-
tains only two labels: ?Hedge? and ? ?, which
represent that a word is in a hedge cue or not
respectively. Since results of hedge detection in
this shared task are evaluated at sentence level, a
sentence will be classified as ?uncertain? in the
post-process if it has one or more words labeled
?Hedge? in it and otherwise ?certain?.
92
2.2 Scope finding
The second task is divided into two steps in our
system. The first step is quite the same as what
the system does in task 1: labeling the words as in
hedge cues or not. Then the scope of each hedge
will be labeled by taking advantage of the result
of the first step. A scope can be denoted by a
beginning word and an ending word to represent
the first and the last element. In scope finding the
available label set contains ?Begin?, ?End?, ?Mid-
dle? and ? ?, representing the first and last word in
the scope, in-scope and out-of-scope. As an exam-
ple, a sentence with hedge cue and scope labeling
is given in Table 1. Hedge cue ?indicating? with
its scope from ?indicating? itself to ?transcription?
are labeled. While evaluating outputs, only ?Be-
gin?s and ?End?s will be taken into consideration
and be treated as the head and tail tokens of the
scopes of specific hedge cues.
Furthermore ...
, ...
inhibition ...
can ...
be ...
blocked ...
by ...
actinomycin ...
D ...
, ...
indicating ... Hedge Begin
a ... Middle
requirement ... Middle
for ... Middle
de ... Middle
novo ... Middle
transcription ... End
. ...
Table 1: A sentence with hedge cue and scope la-
beling
It seems that the best labeling result of task 1
can be used directly to be the proper intermediate
representation of task 2. However, the complexity
of scope finding for multi-hedge sentences forces
us to modify the intermediate result of task 2 for
the sake of handling the sentences with more than
one hedge cue correctly. Besides, since task 1 is
a sentence classification task essentially, while the
goal of the first step of task 2 is to label the words
as accurately as possible, it is easy to find that
the optimal labeling results of task 1 may not be
optimal to be the intermediate representations for
task 2. This problem can be solved if sentence-
level hedge detection and intermediate representa-
tion finding are treated as two separate tasks with
independent feature selection procedures. The de-
tails of feature selection will be given in section
3.
2.3 Scope finding for multi-hedge cases
Sentences with more than one hedge cue are quite
common in both datasets of BioScope corpus and
Wikipedia. By counting hedges in every sentence,
we find that about one fourth of the sentences with
hedges have more than one hedge cue in all three
data sources (Table 2). In Morante and Daele-
mans (2009), three classifiers predict whether each
token is Begin, End or None and a postprocess-
ing is needed to associate Begins and Ends with
their corresponding hedge cues. In our approach,
in order to decrease ambiguous or illegal outputs
e.g. inequivalent numbers of Begins and Ends, a
pair of Begin and End without their correspond-
ing hedge cue between them, etc., sentences with
more than one hedge cue will be preprocessed by
making copies as many as the number of hedges
and be handled separately.
The sentence which is selected as a sample has
two hedge cues: ?suggesting? and ?may?, so our
system preprocesses the sentence into two single-
hedge ones, which is illustrated in Table 3. Now it
comes to the problem of finding scope for single-
hedge sentence. The two copies are labeled sep-
arately, getting one scope from ?suggesting? to
?mitogenesis? for the hedge cue ?suggesting? and
the other from ?IFN-alpha? to ?mitogenesis? for
?may?. Merging the two results will give the final
scope resolution of the sentence.
However, compared with matching Begins and
Ends in postprocessing given by Morante and
Daelemans (2009), the above method gives rise
to out of control of projections of the scopes,
i.e. scopes of hedges may partially overlap after
copies are merged. Since scopes should be in-
tact constituents of sentences, namely, subtrees in
syntax tree which never partly overlap with each
other, results like this are linguistically illegal and
should be discarded. We solve this problem by in-
troducing an instructional feature called ?Indica-
tor?. For sentences with more than one hedge cue,
namely more than one copy while finding scopes,
words inside the union of existing (labeled) scopes
will be tagged as ?Indicator? in unhandled copies
before every labeling. For example, after finding
scope for the first copy in Table 3 and words from
93
Dataset # Sentence # No-hedge ratio # One-hedge ratio # Multi-hedge ratio
Biomedical Abstracts 11871 9770 82.3% 1603 13.5% 498 4.2%
Biomedical Fulltexts 2670 2151 80.6% 385 14.4% 134 5.0%
Wikipedia 11111 8627 77.6% 1936 17.4% 548 4.9%
Table 2: Statistics of hedge amount
IFN-alpha IFN-alpha
also also
sensitized sensitized
T T
cells cells
to to
IL-2-induced IL-2-induced
proliferation proliferation
, ,
further further
suggesting Hedge suggesting
that that
IFN-alpha IFN-alpha
may may Hedge
be be
involved involved
in in
the the
regulation regulation
of of
T-cell T-cell
mitogenesis mitogenesis
. .
Table 3: An example of 2-hedge sentence before
scope finding
?suggesting? to ?mitogenesis? are put in the scope
of cue ?suggesting?, these words should be tagged
?Indicator? in the second copy, whose result is il-
lustrated in Table 4. If not in a scope, any word is
tagged ? ? as the indicator. The ?Indicator?s tag-
ging from ?suggesting? to ?mitogenesis? in Table
4 mean that no other than the situations of a) ?Be-
gin? is after or at ?suggesting? and ?End? is before
or at ?mitogenesis? b) Both ?Begin? and ?End? are
before ?suggesting? c) Both next ?Begin? and next
?End? are after ?mitogenesis? can be accepted. In
other words, new labeling should keep the projec-
tions of scopes in the result. Although it is only
an instructional indicator and does not have any
coerciveness, the evaluation result of experiment
shows it effective.
3 Feature selection
Since hedge and scope finding are quite novel
tasks and it is not easy to determine the effective
features by experience, a greedy feature selection
is conducted. As it mentioned in section 2, our
system divides scope finding into two sub-tasks:
IFN-alpha ...
also ...
sensitized ...
T ...
cells ...
to ...
IL-2-induced ...
proliferation ...
, ...
further ...
suggesting ... Indicator
that ... Indicator
IFN-alpha ... Indicator Begin
may ... Indicator Hedge Middle
be ... Indicator Middle
involved ... Indicator Middle
in ... Indicator Middle
the ... Indicator Middle
regulation ... Indicator Middle
of ... Indicator Middle
T-cell ... Indicator Middle
mitogenesis ... Indicator End
. ...
Table 4: Scope resolution with instructional fea-
ture: ?Indicator?
a) Hedge cue labeling
b) Scope labeling
The first one is the same as hedge detection task
in strategy, but quite distinct in target of feature
set, because hedge detection is a task of sentence
classification while the first step of scope find-
ing aims at high accuracy of labeling hedge cues.
Therefore, three independent procedures of fea-
ture selection are conducted for BioScope corpus
dataset. AsWikipedia is not involved in the task of
scope finding, it only needs one final feature set.
About 200 feature templates are initially con-
sidered for each task. We mainly borrow ideas and
are enlightened by following sources while initial-
izing feature template sets:
a) Previous papers on hedge detection and
scope finding (Light et al, 2004; Medlock,
2008; Medlock and Briscoe, 2008; Kilicoglu
and Bergler, 2008; Szarvas, 2008; Ganter
and Strube, 2009; Morante and Daelemans,
2009);
94
b) Related works such as named entity recog-
nition (Collins, 1999) and text chunking
(Zhang et al, 2001);
c) Some literature on dependency parsing
(Nivre and Scholz, 2004; McDonald et al,
2005; Nivre, 2009; Zhao et al, 2009c; Zhao
et al, 2009a);
3.1 Notations of Feature Template
A large amount of advanced syntactic features in-
cluding syntactic connections, paths, families and
their concatenations are introduced. Many of these
features come from dependency parsing, which
aims at building syntactic tree expressed by depen-
dencies between words. More details about de-
pendency parsing are given in Nivre and Scholz
(2004) and McDonald et al (2005). The parser
in Zhao et al (2009a) is used to construct de-
pendency structures in our system, and some of
the notations in this paper adopt those presented
in Zhao et al (2009c). Feature templates are from
various combinations or integrations of the follow-
ing basic elements.
Word Property. This part of features includes
word form (form), lemma (lemma), part-of-speech
tag (pos), syntactic dependency (dp) , syntactic de-
pendency label (dprel).
Syntactic Connection. This includes syntactic
head (h), left(right) farthest(nearest) child (lm, ln,
rm and rn) and high (low) support verb, noun or
preposition. Here we specify the last one as an
example, support verb(noun/preposition). From a
given word to the syntactic root along the syntac-
tic tree, the first verb/noun/preposition that is met
is called its low support verb/noun/preposition,
and the nearest one to the root(farthest to
the given word) is called as its high support
verb/noun/preposition. The concept of support
verb was broadly used (Toutanova et al, 2005;
Xue, 2006; Jiang and Ng, 2006), and it is extended
to nouns and prepositions in Zhao et al (2009b).
In addition, a slightly modified syntactic head, pp-
head, is introduced, it returns the left most sibling
of a given word if the word is headed by a prepo-
sition, otherwise it returns the original head.
Path. There are two basic types of path. One
is the linear path (linePath) in the sequence, the
other is the path in the syntactic parsing tree (dp-
Path). For example, m:n|dpPath represents the
dependency path from word m to n. Assuming
that the two paths from m and n to the root are
pm and pn, m:n|dpPathShare, m:n|dpPathPred
and m:n|dpPathArgu represent the common part
of pm and pn, part of pm which does not belong
to pn and part of pn which does not belong to pm,
respectively.
Family. A children set includes all syntactic
children(children) are used in the template nota-
tions.
Concatenation of Elements. For all collected
elements according to dpPath, children and so on,
we use three strategies to concatenate all those
strings to produce the feature value. The first is
seq, which concatenates all collected strings with-
out doing anything. The second is bag, which
removes all duplicated strings and sort the rest.
The third is noDup, which removes all duplicated
neighbored strings.
Hedge Cue Dictionary and Scope Indicator.
Hedge cues in the training set are collected and put
in a dictionary. Whether a word in the training or
testing set is in the dictionary (dic) is introduced
into feature templates. As the evaluation is non-
open, we do not put in any additional hedge cues
from other resources. An indicator (indicator) is
given for multi-hedge scope finding, as specified
in section 2.At last, in feature set for scope label-
ing, hedge represents that the word is in a hedge
cue.
At last, we take x as current token to be labeled,
and xm to denote neighbor words. m > 0 repre-
sents that it is a word goes mth after current word
and m < 0 for word ?mth before current word.
3.2 Feature template sets for each task
As optimal feature template subsets cannot be ex-
pected to be extracted from so large sets by hand,
greedy feature selections according to Zhao et al
(2009b) are applied. The normalized feature selec-
tion has been proved to be effective in quite a lot
of NLP tasks and can often successfully select an
optimal or very close to optimal feature set from a
large-scale superset. Although usually it needs 3
to 4 loops denoted by ?While? in the Algorithm 1
of Zhao et al (2009b) to get the best template set,
we only complete one before official outputs col-
lection because of time limitation, which to a large
extent hinders the performance of the system.
Three template sets are selected for BioScope
corpus. One with the highest accuracy for
sentence-level hedge detection (Set B), one with
the best performance for word-level hedge cue la-
95
beling (Set H) and another one with the maximal
F-score for scope finding (Set S). In addition, one
set is discovered for sentence-level hedge detec-
tion of Wikipedia (Set W)1 . Table 52 lists some
selected feature templates which are basic word or
hedging properties for the three sets of BioScope
corpus and Wikipedia. From the table we can see
it is clear that the combinations of lemma, POS
and word form of words in context, which are usu-
ally basic and common elements in NLP, are also
effective for hedge detection. And as we expected,
the feature that represents whether the word is in
the hedge list or not is very useful especially in
hedge cue finding, indicating that methods based
on a hedge cue lists (Light et al, 2004) or keyword
selection (Szarvas, 2008) are quite significant way
to accomplish such tasks.
Some a little complicated syntactic features
based on dependencies are systemically exploited
as features for tasks. Table 6 enumerates some of
the syntactic features which proves to be highly
effective. We noticed that lowSupportNoun, high-
SupportNoun and features derived from dpPath is
notably useful. It can be explained by the aware-
ness that hedge labeling and scope finding are to
process literatures in the level of semantics where
syntactic features are often helpful.
We continue our feature selection procedures
for BioScope corpus after official outputs collec-
tion and obtain feature template sets that bring bet-
ter performance. Table 7 gives some of the fea-
tures in the optimized sets for BioScope corpus
resolution. One difference between the new sets
and the old ones is the former contain more syntac-
tic elements, indicating that exploiting syntactic
feature is a correct choice. Another difference is
the new sets assemble more information of words
before or after the current word, especially words
linearly far away but close in syntax tree. Appear-
ance of combination of these two factors such as
x?1.lm.form seems to provide an evidence of the
insufficiency training and development of our sys-
tem submitted to some extent.
4 Evaluation results
Two tracks (closed and open challenges) are pro-
vided for CoNLL-2010 shared task. We partici-
pated in the closed challenge, select features based
1num in the set of Wikipedia represents the sequential
number of word in the sentence
2Contact the authors to get the full feature lists, as well as
entire optimized sets in post-deadline experiment
- x.lemma + x1.lemma + x?1.lemma
+ x.dic + x1.dic + x?1.dic
- x.lemma + x1.pos + x?1.pos + x.pos
+ x1.lemma + x?1.lemma
- x.form
Set B x.pos + x1.pos + x?1.pos + x2.pos
+ x?2.pos
- x.dic + x1.dic + x?1.dic
- x1.pos
- x.dic + x1.dic + x?1.dic + x2.dic
+ x?2.dic
- x.pos + x?1.pos
- x.dic
Set H x.dic + x.lemma + x.pos + x.form
- x.pos + x1.pos + x?1.pos + x2.pos
+ x?2.pos
- x?2.form + x?2.lemma
- x?1.form + x.form
- x.dic + x1.dic + x?1.dic
- x.dic + x1.dic + x?1.dic + x2.dic
+ x?2.dic + x3.dic + x?3.dic
- x.indicator
- x.hedge + x1.hedge + x?1.hedge
Set S x.lemma + x1.pos + x?1.pos + x.pos
+ x1.lemma + x?1.lemma
- x.pos + x.hedge + x.dp + x.dprel
- x1.pos
- x.pos + x1.pos + x?1.pos + x2.pos
+ x?2.pos
- x.lemma + x1.lemma + x?1.lemma
- + x.dic + x1.dic + x?1.dic
- x.lemma + x1.lemma + x?1.lemma
+x2.lemma + x?2.lemma + x.dic
+ x1.dic + x?1.dic + x2.dic + x?2.dic
- x.lemma + x1.lemma
Set W x.hedge + x1.hedge + x?1.hedge
+ x2.hedge + x?2.hedge + x3.hedge
+ x?3.hedge
- x.pos + x1.pos + x?1.pos +x2.pos
+ x?2.pos + x.dic + x1.dic + x?1.dic
+ x2.dic + x?2.dic
- x.pos + x.dic
- x.num + x.dic
Table 5: Selected feature template sets
96
- x.lowSupportNoun:x | dpPathArgu.dprel.seq
- x.lowSupportNoun:x|dpPathArgu.dprel.seq
+ x.lowSupportProp:x|dpPathArgu.dprel.seq
- x.lowSupoortNoun.pos
- x.pos + x.children.dprel.bag
- x.rm.dprel + x.form
Set B x.pphead.lemma
- x.form + x.children.dprel.bag
- x.lowSupportNoun:x?dpTreeRelation
- x.lowSupportProp.lemma
- x.form + x.children.dprel.noDup
- x.highSupportNoun:x|dpTreeRelation + x.form
- x.lowSupportVerb.form
- x.lowSupportProp:x|dpPathShared.dprel.seq
- x.lowSupportProp:x|dpPathShared.pos.seq
- x.highSupportNoun.pos
- x.highSupportNoun:x|dpTreeRelation
- x.highSupportNoun:x|dpPathArgu.dprel.seq
Set H + x.highSupportProp:x|dpPathArgu.dprel.seq
- xlowSupportProp.lemma
- x.rm.dprel
- x.lm.form
- x.lemma + x.pphead.form
- x.lowSupportVerb.form
- x.rm.lemma + x.rm.form
- x.children.dprel.noDup
- x.children.dprel.bag
- x.highSupportNoun:x|dpTreeRelation
- x.lemma + x.pphead.form
Set S x.highSupportNoun:x|dpTreeRelation + x.form
- x.lowSupportVerb.form
- x.lowSupportVerb.lemma
- x.h.children.dprel.bag
- x.highSupportVerb.form
- x.lm.form
- x.lemma + x.pphead.form
- x.lm.dprel + x.pos
- x.lowSupportProp:x|dpPathPred.dprel.seq
- x.pphead.lemma
Set W x.rm.lemma
- x.lowSupportProp:x|dpTreeRelation
- x.lowSupportVerb:x|dpPathPred.dprel.seq
- x.lowSupportVerb:x|dpPathPred.pos.seq
- x.lowSupportVerb:x|dpPathShared.pos.seq
- x.lowSupportProp:x|dpPathShared.pos.seq
- x.lowSupportProp.form
Table 6: Syntactic features
- x?1.lemma
- x.dic + x1.dic + x?1.dic + x2.dic
+ x?2.dic + x3.dic + x?3.dic
- x?1.pos + x1.pos
Set H x.rm.lemma
- x.rm.dprel
- x.lm.dprel + x.pos
- x.lowSupportNoun:x | dpPathArgu.dprel.seq
- x.lowSupportNoun:x|dpPathArgu.dprel.seq
+ x.lowSupportProp:x|dpPathArgu.dprel.seq
- x?1.lemma
- x.lemma + x1.lemma + x?1.lemma + x.dic
+ x1.dic + x?1.dic
- x.form + x.lemma + x.pos + x.dic
Set B x?2.form + x?1.form
- x.highSupportNoun:x|dpTreeRelation
- x.highSupportNoun:x|dpPathArgu.dprel.seq
- x.lowSupportProp:x|dpPathShared.dprel.seq
- x?1.lm.form
- x1.form
- x.pos + x.dic
- x.hedge + x1.hedge + x?1.hedge
- x.pos + x1.pos + x?1.pos + x2.pos + x?2.pos
Set S x.children.dprel.bag
- x.lemma + x.pphead.form
- x.highSupportVerb.form
- x.highSupportNoun:x|dpTreeRelation + x.form
- x.lowSupportNoun:x|dpTreeRelation + x.form
Table 7: Selected improved feature template sets
for BioScope corpus
on the in-domain data and evaluated our system
on the in-domain and cross-domain evaluation set.
All the experiments are implemented and run by
Maximum Entropy Markov Models (McCallum,
2000).
4.1 Official results
The official results for tasks are in Table 8, in
which three in-domain tests and cue matching
result for biomedical texts are listed. For the
first task for BioCorpus, our system gives F-score
0.8363 in in-domain test and for Wikipedia we
give F-score 0.5618 in closed evaluation. For the
second task, our system gives results in closed and
open test, with F-score 0.4425 and 0.4441 respec-
tively.
We compare the F-score of our system with the
best in the final result in Table 9. We rank pretty
high in Wikipedia hedge detection, while other
three are quite steady but not prominent. This is
mainly due to two reasons:
1. Feature selection procedures are not perfectly
conducted.
2. Abstracts and fulltexts in BioScope are mixed
to be the training set, which proves quite in-
appropriate when the evaluation set contains
97
only fulltext literature, since abstract and full-
text are quite different in terms of hedging.
Dataset F-score Best
Task1-closed 0.8363 0.8636
BioScope Task2-closed 0.4425 0.5732
Cue-matching 0.7853 0.8134
Wikipedia Task1-closed 0.5618 0.6017
Table 9: Comparing results with the best
4.2 Further results
Intact feature selection procedures for BioScope
corpus are conducted after official outputs collec-
tions. The results of evaluation with completely
selected features compared with the incomplete
one are given in Table 7. The system performs a
higher score on evaluation data (Table 10), which
is more competitive in both tasks on BioScope cor-
pus. The improvement for task 2 is significant, but
the increase of performance of hedge cue detec-
tion is less remarkable. We believe that a larger
fulltext training set and a more considerate train-
ing plan will help us to do better job in the future
work.
Dataset Complete Incomplete
Task1-closed 0.8522 0.8363
BioScope Task2-closed 0.5151 0.4425
Cue-matching 0.7990 0.7853
Table 10: Comparing improved outputs with the
best
5 Conclusion
We describe the system that uses sequence label-
ing with normalized feature selection and rich fea-
tures to detect hedges and find scopes for hedge
cues. Syntactic features which are derived from
dependencies are exploited, which prove to be
quite favorable. The evaluation results show that
our system is steady in performance and does
pretty good hedging and scope finding in both Bio-
Scope corpus and Wikipedia, especially when the
feature selection procedure is carefully and totally
conducted. The results suggest that sequence la-
beling and a feature-oriented method are effective
in such NLP tasks.
References
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176, Suntec, Singapore, 4,
August.
Ken Hyland. 1996. Writing without conviction: Hedg-
ing in science research articles. Applied Linguistics,
17:433?54.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic
role labeling of NomBank: A maximum entropy ap-
proach. In Proceedings of the EMNLP-2006, pages
138?145, Sydney, Australia.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9.
Marc Light, Xin Ying Qiu, and Padimini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proc. of the
BioLINK 2004, pages 17?24.
Chrysanne Di Marco and Robert E. Mercer. 2004.
Hedging in scientific articles as a means of classify-
ing citations. In Working Notes of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications, pages 50?54.
Andrew McCallum. 2000. Maximum entropy markov
models for information extraction and segmentation.
In Proceedings of ICML 2000, pages 591?598, Stan-
ford, California.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of HLT/EMNLP 05, pages 523?530, Vancouver,
Canada, October.
Ben Medlock and Ted Briscoe. 2008. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of 45th Annual Meeting
of the ACL, pages 992?999, Prague, Czech Repub-
lic, June.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41:636?654.
98
Dataset TP FP FN precision recall F-score
BioScope Task1-closed 669 141 121 0.8259 0.8468 0.8363
Task2-closed 441 519 592 0.4594 0.4269 0.4425
Cue-matching 788 172 259 0.8208 0.7526 0.7853
Wikipedia Task1-closed 991 303 1243 0.7658 0.4436 0.5618
Table 8: Official results of our submission for in-domain tasks
Roser Morante andWalter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the Workshop on BioNLP, pages 28?36,
Boulder, Colorado, June.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING-2004, pages 64?70, Geneva, Switzer-
land, August 23rd-27th.
Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of ACL-
IJCNLP 2009, pages 351?359, Suntec, Singapore,
2-7 August.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, and
Ja?nos Csirik. 2008. The BioScope corpus: anno-
tation for negation, uncertainty and their scope in
biomedical texts. In Proceedings of BioNLP 2008,
pages 38?45, Columbus, Ohio, USA, June.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of ACL-08, pages 281?
289, Columbus, Ohio, USA, June.
Paul Thompson, Giulia Venturi, John McNaught,
Simonetta Montemagni, and Sophia Ananiadou.
2008. Categorising modality in biomedical texts. In
Proc. of the LREC 2008 Workshop on Building and
Evaluating Resources for Biomedical Text Mining,
pages 27?34, Marrakech.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL-2005, pages
589?596, Ann Arbor, USA.
Nianwen Xue. 2006. Semantic role labeling of nom-
inalized predicates in Chinese. In Proceedings of
the Human Language Technology Conference of the
NAACL (NAACL-2006), pages 431?438, New York
City, USA, June.
Tong Zhang, Fred Damerau, and David Johnson. 2001.
Text chunking using regularized winnow. In Pro-
ceedings of the 39th Annual Meeting on Associa-
tion for Computational Linguistics, pages 539?546,
Toulouse, France.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009a. Multi-
lingual dependency learning: Exploiting rich fea-
tures for tagging syntactic and semantic dependen-
cies. In Proceedings of CoNLL-2009, June 4-5,
Boulder, Colorado, USA.
Hai Zhao, Wenliang Chen, and Chunyu Kit. 2009b.
Semantic dependency parsing of NomBank and
PropBank: An efficient integrated approach via a
large-scale feature selection. In Proceedings of
EMNLP-2009, pages 30?39, Singapore.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009c. Multilingual dependency learning:
A huge feature engineering method to semantic de-
pendency parsing. In Proceedings of CoNLL-2009,
June 4-5, Boulder, Colorado, USA.
99
