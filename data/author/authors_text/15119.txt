Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 56?60,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Unrestricted Coreference Resolution via Global Hypergraph Partitioning
Jie Cai and ?Eva Mu?jdricza-Maydt and Michael Strube
Natural Language Processing Group
Heidelberg Institute for Theoretical Studies gGmbH
Heidelberg, Germany
(jie.cai|eva.mujdriczamaydt|michael.strube)@h-its.org
Abstract
We present our end-to-end coreference res-
olution system, COPA, which implements a
global decision via hypergraph partitioning.
In constrast to almost all previous approaches,
we do not rely on separate classification and
clustering steps, but perform coreference res-
olution globally in one step. COPA represents
each document as a hypergraph and partitions
it with a spectral clustering algorithm. Various
types of relational features can be easily incor-
porated in this framwork. COPA has partici-
pated in the open setting of the CoNLL shared
task on modeling unrestricted coreference.
1 Introduction
Coreference resolution is the task of grouping men-
tions of entities into sets so that all mentions in
one set refer to the same entity. Most recent ap-
proaches to coreference resolution divide this task
into two steps: (1) a classification step which de-
termines whether a pair of mentions is coreferent or
which outputs a confidence value, and (2) a cluster-
ing step which groups mentions into entities based
on the output of step 1.
In this paper we present an end-to-end corefer-
ence resolution system, COPA, which avoids the di-
vision into two steps and instead performs a global
decision in one step. The system presents a doc-
ument as a hypergraph, where the vertices denote
mentions and the edges denote relational features
between mentions. Coreference resolution is then
performed globally in one step by partitioning the
hypergraph into subhypergraphs so that all mentions
in one subhypergraph refer to the same entity (Cai
and Strube, 2010). COPA assigns edge weights by
applying simple descriptive statistics on the tranin-
ing data. Since COPA does not need to learn an
explicit model, we used only 30% of the CoNLL
shared task training data. We did this not for effi-
ciency reasons, only for convenience.
While COPA has been developed originally to
perform coreference resolution on MUC and ACE
data (Cai and Strube, 2010), the move to the
OntoNotes data (Weischedel et al, 2011) required
mainly to update the mention detector and the fea-
ture set. Since several off-the-shelf preprocessing
components are used, COPA participated in the open
setting of the CoNLL shared task on modeling unre-
stricted coreference (Pradhan et al, 2011). We did
not make extensive use of information beyond infor-
mation from the closed class setting.
2 Preprocessing
COPA is implemented on top of the BART-toolkit
(Versley et al, 2008). Documents are transformed
into the MMAX2-format (Mu?ller and Strube, 2006)
which allows for easy visualization and (linguis-
tic) debugging. Each document is stored in several
XML-files representing different layers of annota-
tions. These annotations are created by a pipeline
of preprocessing components. We use the Stan-
ford MaxentTagger (Toutanova et al, 2003) for part-
of-speech tagging, and the Stanford Named En-
tity Recognizer (Finkel et al, 2005) for annotat-
ing named entities. In order to derive syntactic
information, we use the Charniak/Johnson rerank-
ing parser (Charniak and Johnson, 2005) com-
56
bined with a constituent-to-dependency conversion
Tool (http://nlp.cs.lth.se/software/
treebank_converter). The preprocessing
models are not trained on CoNLL data, so we only
participated in the open task.
We have implemented an in-house mention detec-
tor, which makes use of the parsing output, the part-
of-speech tags, as well as the chunks from the Yam-
cha Chunker (Kudoh and Matsumoto, 2000). For
the OntoNotes data, the mention detector annotates
the biggest noun phrase spans.
3 COPA: Coreference Partitioner
The COPA system consists of modules which derive
hyperedges from features and assign edge weights
indicating a positive correlation with the coreference
relation, and resolution modules which create a hy-
pergraph representation for the testing data and per-
form partitioning to produce subhypergraphs, each
of which represents an entity.
3.1 HyperEdgeCreator
COPA needs training data only for computing the
hyperedge weights. Hyperedges represent features.
Each hyperedge corresponds to a feature instance
modeling a simple relation between two or more
mentions. This leads to initially overlapping sets of
mentions. Hyperedges are assigned weights which
are calculated on the training data as the percentage
of the initial edges being in fact coreferent. Due to
the simple strategy of assigning edge weights, only
a reasonable size of training data is needed.
3.2 Coreference Resolution Modules
Unlike pairwise models, COPA processes a docu-
ment globally in one step, taking care of the pref-
erence information among all the mentions simul-
taneously and clustering them into sets directly. A
document is represented as a single hypergraph with
multiple edges. The hypergraph resolver partitions
the hypergraph into several sub-hypergraphs, each
corresponding to one set of coreferent mentions.
3.2.1 HGModelBuilder
A single document is represented in a hypergraph
with basic relational features. Each hyperedge in a
graph corresponds to an instance of one of those fea-
tures with the weight assigned by the HyperEdge-
Learner. Instead of connecting nodes with the tar-
get relation as usually done in graph models, COPA
builds the graph directly out of low dimensional fea-
tures without assuming a distance metric.
3.2.2 HGResolver
In order to partition the hypergraph we adopt a
spectral clustering algorithm (Agarwal et al, 2005).
All experimental results are obtained using symmet-
ric Laplacians (Lsym) (von Luxburg, 2007).
We apply the recursive variant of spectral clus-
tering, recursive 2-way partitioning (R2 partitioner)
(Cai and Strube, 2010). This method does not need
any information about the number of target sets (the
number k of clusters). Instead a stopping criterion
?? has to be provided which is adjusted on develop-
ment data.
3.3 Complexity of HGResolver
Since edge weights are assigned using simple de-
scriptive statistics, the time HGResolver needs for
building the graph Laplacian matrix is not substan-
tial. For eigensolving, we use an open source library
provided by the Colt project1which implements a
Householder-QL algorithm to solve the eigenvalue
decomposition. When applied to the symmetric
graph Laplacian, the complexity of the eigensolv-
ing is given by O(n3), where n is the number of
mentions in a hypergraph. Since there are only a
few hundred mentions per document in our data, this
complexity is not an issue. Spectral clustering gets
problematic when applied to millions of data points.
4 Features
In our system, features are represented as types of
hyperedges. Any realized edge is an instance of the
corresponding edge type. All instances derived from
the same type have the same weight, but they may
get reweighed by the distance feature (see Cai and
Strube (2010)). We use three types of features:
negative: prevent edges between mentions;
positive: generate strong edges between mentions;
weak: add edges to an existing graph without intro-
ducing new vertices;
1http://acs.lbl.gov/
?
hoschek/colt/
57
In the following subsections we describe the fea-
tures used in our experiments. Some of the fea-
tures described in Cai and Strube (2010) had to be
changed to cope with the OntoNotes data. We also
introduced a few more features (in particular in or-
der to deal with the dialogue section in the data).
4.1 Negative Features
Negative features describe pairwise relations which
are most likely not coreferent. While we imple-
mented this information as weak positive features in
Cai and Strube (2010), here we apply these features
before graph construction as global variables.
When two mentions are connected by a negative
relation, no edges will be built between them in the
graph. For instance, no edges are allowed between
the mention Hillary Clinton and the mention he due
to incompatible gender.
(1) N Gender, (2) N Number: Two mentions do
not agree in gender or number.
(3) N SemanticClass: Two mentions do not
agree in semantic class (only the Object, Date and
Person top categories derived from WordNet (Fell-
baum, 1998) are used).
(4) N Mod: Two mentions have the same syntac-
tic heads, and the anaphor has a pre-modifier which
does not occur in the antecedent and does not con-
tradict the antecedent.
(5) N DSPrn: Two first person pronouns in direct
speeches assigned to different speakers.
(6) N ContraSubjObj: Two mentions are in the
subject and object positions of the same verb, and
the anaphor is a non-possesive pronoun.
4.2 Positive Features
The majority of well studied coreference features
(e.g. Stoyanov et al (2009)) are actually positive
coreference indicators. In our system, the mentions
which participate in positive relations are included
in the graph representation.
(7) StrMatch Npron & (8) StrMatch Pron: Af-
ter discarding stop words, if the strings of mentions
completely match and are not pronouns, they are put
into edges of the StrMatch Npron type. When the
matched mentions are pronouns, they are put into
the StrMatch Pron type edges.
(9) Alias: After discarding stop words, if men-
tions are aliases of each other (i.e. proper names with
partial match, full names and acronyms, etc.).
(10) HeadMatch: If the syntactic heads of men-
tions match.
(11) Nprn Prn: If the antecedent is not a pro-
noun and the anaphor is a pronoun. This feature is
restricted to a sentence distance of 2. Though it is
not highly weighted, it is crucial for integrating pro-
nouns into the graph.
(12) Speaker12Prn: If the speaker of the second
person pronoun is talking to the speaker of the first
person pronoun. The mentions contain only first or
second person pronouns.
(13) DSPrn: If one of the mentions is the subject
of a speak verb, and other mentions are first person
pronouns within the corresponding direct speech.
(14) ReflexivePrn: If the anaphor is a reflexive
pronoun, and the antecedent is subject of the sen-
tence.
(15) PossPrn: If the anaphor is a possesive pro-
noun, and the antecedent is the subject of the sen-
tence or the subclause.
(16) GPEIsA: If the antecedent is a Named Entity
of GPE entity type (i.e. one of the ACE entity type
(NIST, 2004)), and the anaphor is a definite expres-
sion of the same type.
(17) OrgIsA: If the antecedent is a Named En-
tity of Organization entity type, and the anaphor is a
definite expression of the same type.
4.3 Weak Features
Weak features are weak coreference indicators. Us-
ing them as positive features would introduce too
much noise to the graph (i.e. a graph with too many
singletons). We apply weak features only to men-
tions already integrated in the graph, so that weak
information provides it with a richer structure.
(18) W Speak: If mentions occur with a word
meaning to say in a window size of two words.
(19) W Subject: If mentions are subjects.
(20) W Synonym: If mentions are synonymous
as indicated by WordNet.
5 Results
We submitted COPA?s results to the open setting
in the CoNLL shared task on modeling unrestricted
coreference. We used only 30% of the training data
58
(randomly selected) and the 20 features described in
Section 4.
The stopping criterion ?? (see Section 3) is tuned
on development data to optimize the final corefer-
ence scores. A value of 0.06 is chosen for testing.
COPA?s results on development set (which con-
sists of 202 files) and on testing set are displayed in
Table 1 and Table 2 respectively. The Overall num-
bers in both tables are the average scores of MUC,
BCUBED and CEAF (E).
Metric R P F1
MUC 52.69 57.94 55.19
BCUBED 64.26 73.39 68.52
CEAF (M) 54.44 54.44 54.44
CEAF (E) 45.73 40.92 43.19
BLANC 69.78 75.26 72.13
Overall 55.63
Table 1: COPA?s results on CoNLL development set
Metric R P F1
MUC 56.73 58.90 57.80
BCUBED 64.60 71.03 67.66
CEAF (M) 53.37 53.37 53.37
CEAF (E) 42.71 40.68 41.67
BLANC 69.77 73.96 71.62
Overall 55.71
Table 2: COPA?s results on CoNLL testing set
6 Mention Detection Errors
As described in Section 2, our mention detection is
based on automatically extracted information, such
as syntactic parses and basic noun phrase chunks.
Since there is no minimum span information pro-
vided in the OntoNotes data (in constrast to the pre-
vious standard corpus, ACE), exact mention bound-
ary detection is required. A lot of the spurious
mentions in our system are generated due to mis-
matches of ending or starting punctuations, and the
OntoNotes annotation is also not consistent in this
regard. Our current mention detector does not ex-
tract verb phrases. Therefore it misses all the Event
mentions in the OntoNotes corpus.
We are planning to include idiomatic expression
identification into our mention detector, which will
help to avoid detecting a lot of spurious mentions,
such as God in the phrase for God?s sake.
7 COPA Errors
Besides the fact that the current COPA is not resolv-
ing any event coreferences, our in-house mention de-
tector performs weakly in extracting date mentions
too. As a result, the system outputs several spuri-
ous coreference sets, for instance a set containing
the September from the mention 15th September.
A large amount of the recall loss in our system is
due to the lack of the world knowledge. For exam-
ple, COPA does not resolve the mention the Europe
station correctly into the entity Radio Free Europe,
for it has no knowledge that the entity is a station.
Some more difficult coreference phenomena in
OntoNotes data might require a reasoning mecha-
nism. To be able to connect the mention the vic-
tim with the mention the groom?s brother, the event
of the brother being killed needs to be intepreted by
the system.
We also observed from the experiments that the
resolution of the it mentions are quite inaccurate.
Although our mention detector takes care of dis-
carding pleonastic it?s, there are still a lot of them
left which introduce wrong coreference sets. Since
the it?s do not contain enough information by them-
selves, more features exploring their local syntax are
necessary.
8 Conclusions
In this paper we described a coreference resolution
system, COPA, which implements a global decision
in one step via hypergraph partitioning. COPA?s
hypergraph-based strategy is a general preference
model, where the preference for one mention de-
pends on information on all other mentions.
The system implements three types of relational
features ? negative, positive and weak features, and
assigns the edge weights according to the statitics
from the training data. Since the weights are robust
with respect to the amount of training data we used
only 30% of the training data.
Acknowledgements. This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a HITS
PhD. scholarship.
59
References
Sameer Agarwal, Jonwoo Lim, Lihi Zelnik-Manor, Pietro
Perona, David Kriegman, and Serge Belongie. 2005.
Beyond pairwise clustering. In Proceedings of the
IEEE Computer Society Conference on Computer Vi-
sion and Pattern Recognition (CVPR?05), volume 2,
pages 838?845.
Jie Cai and Michael Strube. 2010. End-to-end coref-
erence resolution via hypergraph partitioning. In
Proceedings of the 23rd International Conference on
Computational Linguistics, Beijing, China, 23?27 Au-
gust 2010, pages 143?151.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
Ann Arbor, Mich., 25?30 June 2005, pages 173?180.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics, Ann
Arbor, Mich., 25?30 June 2005, pages 363?370.
Taku Kudoh and Yuji Matsumoto. 2000. Use of Support
Vector Machines for chunk identification. In Proceed-
ings of the 4th Conference on Computational Natural
Language Learning, Lisbon, Portugal, 13?14 Septem-
ber 2000, pages 142?144.
Christoph Mu?ller and Michael Strube. 2006. Multi-level
annotation of linguistic data with MMAX2. In Sabine
Braun, Kurt Kohn, and Joybrato Mukherjee, editors,
Corpus Technology and Language Pedagogy: New Re-
sources, New Tools, New Methods, pages 197?214. Pe-
ter Lang: Frankfurt a.M., Germany.
NIST. 2004. The ACE evaluation plan:
Evaluation of the recognition of ACE en-
tities, ACE relations and ACE events.
http://www.itl.nist.gov/iad/mig//tests/ace/2004/doc/
ace04-evalplan-v7.pdf.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Shared Task of 15th Conference on Computational
Natural Language Learning, Portland, Oreg., 23?24
June 2011.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the Association for Computational
Linguistics and the 4th International Joint Conference
on Natural Language Processing, Singapore, 2?7 Au-
gust 2009, pages 656?664.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics, Edmonton, Al-
berta, Canada, 27 May ?1 June 2003, pages 252?259.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Companion Volume to the Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, Columbus, Ohio, 15?20 June 2008, pages
9?12.
Ulrike von Luxburg. 2007. A tutorial on spectral cluster-
ing. Statistics and Computing, 17(4):395?416.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-
anwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
60
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 100?106,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
A Multigraph Model for Coreference Resolution
Sebastian Martschat, Jie Cai, Samuel Broscheit, E?va Mu?jdricza-Maydt, Michael Strube
Natural Language Processing Group
Heidelberg Institute for Theoretical Studies gGmbH
Heidelberg, Germany
(sebastian.martschat|jie.cai|michael.strube)@h-its.org
Abstract
This paper presents HITS? coreference reso-
lution system that participated in the CoNLL-
2012 shared task on multilingual unrestricted
coreference resolution. Our system employs a
simple multigraph representation of the rela-
tion between mentions in a document, where
the nodes correspond to mentions and the
edges correspond to relations between the
mentions. Entities are obtained via greedy
clustering. We participated in the closed tasks
for English and Chinese. Our system ranked
second in the English closed task.
1 Introduction
Coreference resolution is the task of determining
which mentions in a text refer to the same entity.
This paper describes HITS? system for the CoNLL-
2012 Shared Task on multilingual unrestricted coref-
erence resolution, where the goal is to build a system
for coreference resolution in an end-to-end multilin-
gual setting (Pradhan et al, 2012). We participated
in the closed tasks for English and Chinese and fo-
cused on English. Our system ranked second in the
English closed task.
Being conceptually similar to and building upon
Cai et al (2011b), our system is based on a directed
multigraph representation of a document. A multi-
graph is a graph where two nodes can be connected
by more than one edge. In our model, nodes rep-
resent mentions and edges are built from relations
between the mentions. The entities to be inferred
correspond to clusters in the multigraph.
Our model allows for directly representing any
kind of relations between pairs of mentions in a
graph structure. Inference over this graph can har-
ness structural properties and the rich set of encoded
relations. In order to serve as a basis for further
work, the components of our system were designed
to work as simple as possible. Therefore our system
relies mostly on local information between pairs of
mentions.
2 Architecture
Our system is implemented on top of the BART
toolkit (Versley et al, 2008). To compute the coref-
erence clusters in a document, we first extract a set
of mentions M = {m1, . . . ,mn} ordered according
to their position in the text (Section 2.1). We then
build a directed multigraph where the set of nodes
is M and edges are induced by relations between
mentions (Section 2.4). The relations we use in our
system are coreference indicators like string match-
ing or alias (Section 3). For every relation R, we
compute a weight wR using the training data (Sec-
tion 2.3). We then assign the weight wR to any edge
that is induced by the relation R. Depending on dis-
tance and connectivity properties of the graph the
weights may change (Section 2.4.1). Given the con-
structed graph with edge weights, we go through the
mentions according to their position in the text and
perform greedy clustering (Section 2.6). For Chi-
nese, we employ spectral clustering (Section 2.5) as
adopted in Cai et al (2011b) before the greedy clus-
tering step to reduce the number of candidate an-
tecedents for a mention. The components of our sys-
tem are described in the following subsections.
100
2.1 Mention Extraction
Noun phrases are extracted from the provided parse
and named entity annotation layers. For embedded
mentions with the same head, we only keep the men-
tion with the largest span.
2.1.1 English
For English we identify eight different mention
types: common noun, proper noun, personal pro-
noun, demonstrative pronoun, possessive pronoun,
coordinated noun phrase, quantifying noun phrase
(some of ..., 17 of ...) and quantified noun phrase
(the armed men in one of the armed men). The head
for a common noun or a quantified noun is com-
puted using the SemanticHeadFinder from the Stan-
ford Parser1. The head for a proper noun starts at
the first token tagged as a noun until a punctuation,
preposition or subclause is encountered. Coordina-
tions have the CC tagged token as head and quanti-
fying noun phrases have the quantifier as head.
In a postprocessing step we filter out adjectival
use of nations and named entities with semantic
class Money, Percent or Cardinal. We discard men-
tions whose head is embedded in another mention?s
head. Pleonastic pronouns are identified and dis-
carded via a modified version of the patterns used
by Lee et al (2011).
2.1.2 Chinese
For Chinese we detect four mention types: com-
mon noun, proper noun, pronoun and coordination.
The head detection for Chinese is provided by the
SunJurafskyChineseHeadFinder from the Standford
Parser, except for proper nouns whose head is set to
the mention?s rightmost token.
The remaining processing is similar to the men-
tion detection for English.
2.2 Preprocessing
We extract the information in the provided an-
notation layers and transform the predicted con-
stituent parse trees into dependency parse trees.
We work with two different dependency represen-
tations, one obtained via the converter implemented
1http://nlp.stanford.edu/software/
lex-parser.shtml
in Stanford?s NLP suite2, the other using LTH?s
constituent-to-dependency conversion tool3. For
pronouns, we determine number and gender using
tables containing a mapping of pronouns to their
gender and number.
2.2.1 English
For English, number and gender for common
nouns are computed via a comparison of head
lemma to head and using the number and gender
data of Bergsma and Lin (2006). Quantified noun
phrases are always plural. We compute semantic
classes via a WordNet (Fellbaum, 1998) lookup.
2.2.2 Chinese
For Chinese, we simply determine number and
gender by searching for the corresponding desig-
nators, since plural mentions mostly end with ?,
while ?? (sir) and ?? (lady) often suggest gen-
der information. To identify demonstrative and defi-
nite noun phrases, we check whether they start with
a definite/demonstrative indicator (e.g. ? (this) or
? (that)). We use lists of named entities extracted
from the training data to determine named entities
and their semantic class in development and testing
data.
2.3 Computing Weights for Relations
We compute weights for relations using simple de-
scriptive statistics on training documents. Since this
is a robust approach to learning weights for the type
of graph model we employ (Cai et al, 2011b; Cai
et al, 2011a), we use only a fraction of the available
training data. We took a random subset consisting of
around 20% for English and 15% for Chinese of the
training data. For every document in this set and ev-
ery relation R, we go through the set M of extracted
mentions and compute for every pair (mi,mj) with
i > j whether R holds for this pair. The weight wR
for R is then the number of coreferent pairs with R
divided by the number of all pairs with R.
2.4 Graph Construction
The set of relations we employ consists of two sub-
sets: negative relations R? which enforce that no
2http://nlp.stanford.edu/software/
stanford-dependencies.shtml
3http://nlp.cs.lth.se/software/treebank_
converter/
101
edge is built between two mentions, and positive re-
lations R+ that induce edges. Again, we go through
M in a left-to-right fashion. If for two mentions mi,
mj with i > j a negative relation R? holds, no edge
between mi and mj can be built. Otherwise we add
an edge from mi to mj for every positive relation
R+ such that R+(mi,mj) is true. The structure ob-
tained by this construction is a directed multigraph.
We handle copula relations similar to Lee et al
(2011): if mi is this and the pair (mi,mj) is in a
copula relation (like This is the World), we remove
mj and replace mj in all edges involving it by mi.
For Chinese, we handle ?role appositives? as intro-
duced by Haghighi and Klein (2009) analogously.
2.4.1 Assigning Weights to Edges
Initially, any edge (mi,mj) induced by the rela-
tion R has the weight wR computed as described
in Section 2.3. If R is a transitive relation, we di-
vide the weight by the number of mentions con-
nected by this relation. This corresponds to the way
edge weights are assigned during the spectral em-
bedding in Cai et al (2011b). If R is a relation sen-
sitive to distance like compatibility between a com-
mon/proper noun and a pronoun, the weight is al-
tered according to the distance between mi and mj .
2.4.2 An Example
We demonstrate the graph construction by a sim-
ple example. Consider a document consisting of the
following three sentences.
Barack Obama and Nicolas Sarkozy met
in Toronto yesterday. They discussed the
financial crisis. Sarkozy left today.
Let us assume that our system identifies Barack
Obama (m1), Nicolas Sarkozy (m2), Barack Obama
and Nicolas Sarkozy (m3), They (m4) and Sarkozy
(m5) as mentions. We consider these mentions and
the relations N Number, P Nprn Prn, P Alias and
P Subject described in Section 3. The graph con-
structed according to the algorithm described in this
section is displayed in Figure 1.
Observe the effect of the negative relation
N Number: while P Nprn Prn holds for the pair
Barack Obama (m1) and They (m4), the mentions
do not agree in number. Hence N Number holds for
this pair and no edge from m4 to m1 can be built.
m2 m5
m3 m4
P Alias
P Nprn Prn
P Subject
Figure 1: An example graph. Nodes represent mentions,
edges are induced by relations between the mentions.
2.5 Spectral Clustering
For Chinese we apply spectral clustering before the
final greedy clustering phase. In order to be able to
apply spectral clustering, we make the graph undi-
rected and merge parallel edges into one edge, sum-
ming up all weights. Due to the way edge weights
are computed, the resulting undirected simple graph
corresponds to the graph Cai et al (2011b) use as
input to the spectral clustering algorithm. Spectral
clustering is now performed as in Cai et al (2011b).
2.6 Greedy Clustering
To describe our clustering algorithm, we use some
additional terminology: if there exists an edge from
m to n we say that m is a parent of n and that n is a
child of m.
In the last step, we go through the mentions from
left to right. Let mi be the mention in focus. For
English, we consider all children of mi as possible
antecedents. For Chinese we restrict the possible an-
tecedents to all children that are in the same cluster
obtained by spectral clustering.
If mi is a pronoun, we determine mj such that
the sum over all weights of edges from mi to mj is
maximized. We then assign mi and mj to the same
entity. In English, if mi is a parent of a noun phrase
m that embeds mj , we instead assign mi and m to
the same entity.
For Chinese, all other noun phrases are assigned
to the same entity as all their children in the cluster
obtained by spectral clustering. For English, we are
more restrictive: definites and demonstratives are as-
signed to the same cluster as their closest (according
to the position of the mentions in the text) child.
Negative relations may also be applied as con-
straints in this phase. Before assigning mi to the
same entity as a set of mentions C, we check for
102
every m ? C and every negative relation R?
that we want to incorporate as a constraint whether
R?(mi,m) holds. If yes, we do not assign mi to the
same entity as the mentions in C.
2.7 Complexity
Our algorithms for weight computation, graph con-
struction and greedy clustering look at all pairs of
mentions in a document and perform simple calcu-
lations, which leads to a time complexity of O
(
n2
)
per document, where n is the number of mentions
in a document. When performing spectral cluster-
ing, this increases to O
(
n3
)
. Since we deal with
at most a few hundred mentions per document, the
cubic running time is not an issue.
3 Relations
In our system relations serve as templates for build-
ing or disallowing edges between mentions. We
distinguish between positive and negative relations:
negative relations disallow edges between mentions,
positive relations build edges between mentions.
Negative relations can also be used as constraints
during clustering, while positive relations may also
be applied as ?weak? relations: in this case, we only
add the induced edge when the two mentions under
consideration are already included in the graph after
considering all the non-weak relations.
Most of the relations presented here were already
used in our system for last year?s shared task (Cai et
al., 2011b). The set of relations was enriched mainly
to resolve pronouns in dialogue and to resolve pro-
nouns that do not carry much information by them-
selves like it and they.
3.1 Negative Relations
(1) N Gender, (2) N Number: Two mentions do
not agree in gender or number.
(3) N SemanticClass: Two mentions do not agree
in semantic class (only the Object, Date and Per-
son top categories derived from WordNet (Fell-
baum, 1998) are used).
(4) N ItDist: The anaphor is it or they and the sen-
tence distance to the antecedent is larger than
one.
(5) N BarePlural: Two mentions that are both bare
plurals.
(6) N Speaker12Prn: Two first person pronouns
or two second person pronouns with different
speakers, or one first person pronoun and one
second person pronoun with the same speaker.
(7) N DSprn: Two first person pronouns in direct
speech assigned to different speakers.
(8) N ContraSubjObj: Two mentions are in the
subject and object positions of the same verb,
and the anaphor is a non-possessive pronoun.
(9) N Mod: Two mentions have the same syntac-
tic heads, and the anaphor has a pre- or post-
modifier which does not occur in the antecedent
and does not contradict the antecedent.
(10) N Embedding: Two mentions where one em-
beds the other, which is not a reflexive or posses-
sive pronoun.
(11) N 2PrnNonSpeech: Two second person pro-
nouns without speaker information and not in di-
rect speech.
3.2 Positive Relations
(12) P StrMatch Npron, (13) P StrMatch Pron:
After discarding stop words, if the strings of
mentions completely match and are not pro-
nouns, the relation P StrMatch Npron holds.
When the matched mentions are pronouns,
P StrMatch Pron holds.
(14) P HeadMatch: If the syntactic heads of men-
tions match.
(15) P Nprn Prn: If the antecedent is not a pro-
noun and the anaphor is a pronoun. This relation
is restricted to a sentence distance of 1.
(16) P Alias: If mentions are aliases of each other
(i.e. proper names with partial match, full names
and acronyms, etc.).
(17) P Speaker12Prn: If the speaker of the second
person pronoun is talking to the speaker of the
first person pronoun. The mentions contain only
first or second person pronouns.
(18) P DSPrn: If one mention is subject of a speak
verb, and the other mention is a first person pro-
noun within the corresponding direct speech.
(19) P ReflPrn: If the anaphor is a reflexive pro-
noun, and the antecedent is the subject of the
sentence.
103
(20) P PossPrn: If the anaphor is a possessive pro-
noun, and the antecedent is the subject of the
sentence or the subclause.
(21) P GPEIsA: If the antecedent is a Named En-
tity of GPE entity type and the anaphor is a def-
inite expression of the same type.
(22) P PossPrnEmbedding: If the anaphor is a
possessive pronoun and is embedded in the an-
tecedent.
(23) P VerbAgree: If the anaphor is a pronoun and
has the same predicate as the antecedent.
(24) P Subject & (25) P Object: If both mentions
are subjects/objects (applies only if the anaphor
is it or they).
(26) P SemClassPrn: If the anaphor is a pronoun,
the antecedent is not a pronoun, and both have
semantic class Person.
For English, we used all relations except for (21) and
(26). Relations (1), (2) and (10) were incorporated
as constraints during greedy clustering. For Chinese,
we used relations (1) ? (6), (12) ? (15), (21) and (26).
(26) was incorporated as a weak relation.
4 Results
We submitted to the closed tasks for English and
Chinese. The results on the English development
set and testing set are displayed in Table 1 and Table
2 respectively. To indicate the progress we achieved
within one year, Table 3 shows the performance of
our system on the CoNLL ?11 development data set
compared to last year?s results (Cai et al, 2011b).
The Overall number is the average of MUC, B3
and CEAF (E), MD is the mention detection score.
Overall, we gained over 5% F1 some of which can
be attributed to improved mention detection.
Metric R P F1
MD 73.96 75.69 74.81
MUC 64.93 68.69 66.76
B3 68.42 75.77 71.91
CEAF (M) 61.23 61.23 61.23
CEAF (E) 49.61 45.60 47.52
BLANC 77.81 80.75 79.19
Overall 62.06
Table 1: Results on the English CoNLL ?12 development
set
Metric R P F1
MD 74.23 76.10 75.15
MUC 65.21 68.83 66.97
B3 66.50 74.69 70.36
CEAF (M) 59.61 59.61 59.61
CEAF (E) 48.64 44.72 46.60
BLANC 73.29 78.94 75.73
Overall 61.31
Table 2: Results on the English CoNLL ?12 testing set
Metric R P F1 2011 F1
MD 70.84 73.08 71.94 66.28
MUC 60.80 65.09 62.87 55.19
B3 68.37 75.89 71.94 68.52
CEAF (M) 60.42 60.42 60.42 54.44
CEAF (E) 50.40 46.11 48.16 43.19
BLANC 75.44 79.26 77.19 72.13
Overall 60.99 55.63
Table 3: Results on the English CoNLL ?11 development
set compared to Cai et al (2011b)
Table 4 and Table 5 display our results on Chinese
development data and testing data respectively.
Metric R P F1
MD 52.45 71.50 60.51
MUC 45.90 67.07 54.50
B3 58.94 84.26 69.36
CEAF (M) 53.60 53.60 53.60
CEAF (E) 50.73 34.24 40.89
BLANC 66.17 83.11 71.45
Overall 54.92
Table 4: Results on the Chinese CoNLL ?12 development
set
Metric R P F1
MD 48.49 74.02 58.60
MUC 42.71 67.80 52.41
B3 55.37 85.24 67.13
CEAF (M) 51.30 51.30 51.30
CEAF (E) 51.81 32.46 39.92
BLANC 63.96 82.81 69.18
Overall 53.15
Table 5: Results on the Chinese CoNLL ?12 testing set
Because none of our team members has knowl-
edge of the Arabic language we did not attempt to
104
run our system on the Arabic datasets and therefore
our official score for this language is considered to
be 0. The combined official score of our submission
is (0.0 + 53.15 + 61.31)/3 = 38.15. In the closed
task our system was the second best performing sys-
tem for English and the eighth best performing sys-
tem for Chinese.
5 Error analysis
We did not attempt to resolve event coreference and
did not incorporate world knowledge which is re-
sponsible for many recall errors our system makes.
Since we use a simple greedy strategy for clus-
tering that goes through the mentions left-to-right,
errors in clustering propagate, which gives rise to
cluster-level inconsistencies. We observed a drop in
performance when using more negative relations as
constraints. A more sophisticated clustering strat-
egy that allows a more refined use of constraints is
needed.
5.1 English
Our detection of copula and appositive relations is
quite inaccurate, which is why we limit the incor-
poration of copulas to cases where the antecedent is
this and left appositives out.
We aim for high precision regarding the usage of
the negative relation N Modifier. This leads to some
loss in recall. For example, our system does not as-
sign the just-completed Paralympics and the 12-day
Paralympics to the same entity. Such cases require a
more involved reasoning scheme to decide whether
the modifiers are actually contradicting each other.
Non-referring pronouns constitute another source
of errors. While we improved detection of pleonas-
tic it compared to last year?s system, a lot of them
are not filtered out. Our system also does not distin-
guish well between generic and non-generic uses of
you and we, which hurts precision.
5.2 Chinese
Since each Chinese character carries its own mean-
ing, there are multiple ways to express the same en-
tity by combining different characters into a word.
Both syntactic heads and modifiers can be replaced
by similar words or by abbreviated versions. From
??? (outside people) to ???? (outside eth-
nic group) the head is replaced, while from ??
? (Diana) to ?? ?? ? ?? (charming Di
Princess) the name is abbreviated.
Modifier replacement is more difficult to cope
with, our system does not recognize that ?? ?
??? (starting-over counting-votes job) and??
?? (verifying-votes job) are coreferent. It is also
not trivial to separate characters from words (e.g. by
separating ? and ?) to resolve such cases, since
it will introduce too much noise as a consequence.
In order to tackle this problem, a smart scheme to
propagate similarities from partial words to the en-
tire mentions and a knowledge base upon which re-
liable similarities can be retrieved are necessary.
In contrast to English there is no strict enforce-
ment of using definite noun phrases when referring
to an antecedent in Chinese. Both ???? (the
talk) and?? (talk) can corefer with the antecedent
??????????? (Clinton?s talk during
Hanoi election). This makes it very difficult to dis-
tinguish generic expressions from referential ones.
In the submitted version of our system, we simply
ignore the nominal anaphors which do not start with
definite articles or demonstratives.
6 Conclusions
In this paper we presented a graph-based model for
coreference resolution. It captures pairwise relations
between mentions via edges induced by relations.
Entities are obtained by graph clustering. Discrim-
inative information can be incorporated as negative
relations or as constraints during clustering.
We described our system?s architecture and the re-
lations it employs, highlighting differences and sim-
ilarities to our system from last year?s shared task.
Designed to work as a basis for further work, our
system works mainly by exploring the relationship
between pairs of mentions. Due to its modular archi-
tecture, our system can be extended by components
taking global information into account, for example
for weight learning or clustering.
We focused on the closed task for English in
which our system achieved competitive perfor-
mance, being ranked second out of 15 participants.
Acknowledgments. This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first and the second authors have been
supported by a HITS PhD. scholarship.
105
References
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, Sydney, Australia, 17?
21 July 2006, pages 33?40.
Jie Cai, E?va Mu?jdricza-Maydt, Yufang Hou, and Michael
Strube. 2011a. Weakly supervised graph-based coref-
erence resolution for clinical data. In Proceedings of
the 5th i2b2 Shared Tasks and Workshop on Chal-
lenges in Natural Language Processing for Clinical
Data, Washington, D.C., 20-21 October 2011. To ap-
pear.
Jie Cai, E?va Mu?jdricza-Maydt, and Michael Strube.
2011b. Unrestricted coreference resolution via global
hypergraph partitioning. In Proceedings of the Shared
Task of the 15th Conference on Computational Natu-
ral Language Learning, Portland, Oreg., 23?24 June
2011, pages 56?60.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Singapore,
6?7 August 2009, pages 1152?1161.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Shared Task of the 15th Conference on Compu-
tational Natural Language Learning, Portland, Oreg.,
23?24 June 2011, pages 28?34.
Sameer Pradhan, Alessandro Moschitti, and Nianwen
Xue. 2012. CoNLL-2012 Shared Task: Modeling
multilingual unrestricted coreference in OntoNotes. In
Proceedings of the Shared Task of the 16th Confer-
ence on Computational Natural Language Learning,
Jeju Island, Korea, 12?14 July 2012. This volume.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Companion Volume to the Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, Columbus, Ohio, 15?20 June 2008, pages
9?12.
106
