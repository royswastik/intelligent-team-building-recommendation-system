Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1524?1534,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Named Entity Recognition in Tweets:
An Experimental Study
Alan Ritter, Sam Clark, Mausam and Oren Etzioni
Computer Science and Engineering
University of Washington
Seattle, WA 98125, USA
{aritter,ssclark,mausam,etzioni}@cs.washington.edu
Abstract
People tweet more than 100 Million times
daily, yielding a noisy, informal, but some-
times informative corpus of 140-character
messages that mirrors the zeitgeist in an un-
precedented manner. The performance of
standard NLP tools is severely degraded on
tweets. This paper addresses this issue by
re-building the NLP pipeline beginning with
part-of-speech tagging, through chunking, to
named-entity recognition. Our novel T-NER
system doubles F1 score compared with the
Stanford NER system. T-NER leverages the
redundancy inherent in tweets to achieve this
performance, using LabeledLDA to exploit
Freebase dictionaries as a source of distant
supervision. LabeledLDA outperforms co-
training, increasing F1 by 25% over ten com-
mon entity types.
Our NLP tools are available at: http://
github.com/aritter/twitter_nlp
1 Introduction
Status Messages posted on Social Media websites
such as Facebook and Twitter present a new and
challenging style of text for language technology
due to their noisy and informal nature. Like SMS
(Kobus et al, 2008), tweets are particularly terse
and difficult (See Table 1). Yet tweets provide a
unique compilation of information that is more up-
to-date and inclusive than news articles, due to the
low-barrier to tweeting, and the proliferation of mo-
bile devices.1 The corpus of tweets already exceeds
1See the ?trending topics? displayed on twitter.com
the size of the Library of Congress (Hachman, 2011)
and is growing far more rapidly. Due to the vol-
ume of tweets, it is natural to consider named-entity
recognition, information extraction, and text mining
over tweets. Not surprisingly, the performance of
?off the shelf? NLP tools, which were trained on
news corpora, is weak on tweet corpora.
In response, we report on a re-trained ?NLP
pipeline? that leverages previously-tagged out-of-
domain text, 2 tagged tweets, and unlabeled tweets
to achieve more effective part-of-speech tagging,
chunking, and named-entity recognition.
1 The Hobbit has FINALLY started filming! I
cannot wait!
2 Yess! Yess! Its official Nintendo announced
today that they Will release the Nintendo 3DS
in north America march 27 for $250
3 Government confirms blast n nuclear plants n
japan...don?t knw wht s gona happen nw...
Table 1: Examples of noisy text in tweets.
We find that classifying named entities in tweets is
a difficult task for two reasons. First, tweets contain
a plethora of distinctive named entity types (Compa-
nies, Products, Bands, Movies, and more). Almost
all these types (except for People and Locations) are
relatively infrequent, so even a large sample of man-
ually annotated tweets will contain few training ex-
amples. Secondly, due to Twitter?s 140 character
limit, tweets often lack sufficient context to deter-
mine an entity?s type without the aid of background
2Although tweets can be written on any subject, following
convention we use the term ?domain? to include text styles or
genres such as Twitter, News or IRC Chat.
1524
knowledge.
To address these issues we propose a distantly su-
pervised approach which applies LabeledLDA (Ra-
mage et al, 2009) to leverage large amounts of unla-
beled data in addition to large dictionaries of entities
gathered from Freebase, and combines information
about an entity?s context across its mentions.
We make the following contributions:
1. We experimentally evaluate the performance of
off-the-shelf news trained NLP tools when ap-
plied to Twitter. For example POS tagging
accuracy drops from about 0.97 on news to
0.80 on tweets. By utilizing in-domain, out-
of-domain, and unlabeled data we are able to
substantially boost performance, for example
obtaining a 52% increase in F1 score on seg-
menting named entities.
2. We introduce a novel approach to distant super-
vision (Mintz et al, 2009) using Topic Models.
LabeledLDA is applied, utilizing constraints
based on an open-domain database (Freebase)
as a source of supervision. This approach in-
creases F1 score by 25% relative to co-training
(Blum and Mitchell, 1998; Yarowsky, 1995) on
the task of classifying named entities in Tweets.
The rest of the paper is organized as follows.
We successively build the NLP pipeline for Twitter
feeds in Sections 2 and 3. We first present our ap-
proaches to shallow syntax ? part of speech tagging
(?2.1), and shallow parsing (?2.2). ?2.3 describes a
novel classifier that predicts the informativeness of
capitalization in a tweet. All tools in ?2 are used
as features for named entity segmentation in ?3.1.
Next, we present our algorithms and evaluation for
entity classification (?3.2). We describe related work
in ?4 and conclude in ?5.
2 Shallow Syntax in Tweets
We first study two fundamental NLP tasks ? POS
tagging and noun-phrase chunking. We also discuss
a novel capitalization classifier in ?2.3. The outputs
of all these classifiers are used in feature generation
for named entity recognition in the next section.
For all experiments in this section we use a dataset
of 800 randomly sampled tweets. All results (Tables
Accuracy Error
Reduction
Majority Baseline (NN) 0.189 -
Word?s Most Frequent Tag 0.760 -
Stanford POS Tagger 0.801 -
T-POS(PTB) 0.813 6%
T-POS(Twitter) 0.853 26%
T-POS(IRC + PTB) 0.869 34%
T-POS(IRC + Twitter) 0.870 35%
T-POS(PTB + Twitter) 0.873 36%
T-POS(PTB + IRC + Twitter) 0.883 41%
Table 2: POS tagging performance on tweets. By training
on in-domain labeled data, in addition to annotated IRC
chat data, we obtain a 41% reduction in error over the
Stanford POS tagger.
2, 4 and 5) represent 4-fold cross-validation experi-
ments on the respective tasks.3
2.1 Part of Speech Tagging
Part of speech tagging is applicable to a wide range
of NLP tasks including named entity segmentation
and information extraction.
Prior experiments have suggested that POS tag-
ging has a very strong baseline: assign each word
to its most frequent tag and assign each Out of Vo-
cabulary (OOV) word the most common POS tag.
This baseline obtained a 0.9 accuracy on the Brown
corpus (Charniak et al, 1993). However, the appli-
cation of a similar baseline on tweets (see Table 2)
obtains a much weaker 0.76, exposing the challeng-
ing nature of Twitter data.
A key reason for this drop in accuracy is that Twit-
ter contains far more OOV words than grammatical
text. Many of these OOV words come from spelling
variation, e.g., the use of the word ?n? for ?in? in Ta-
ble 1 example 3. Although NNP is the most frequent
tag for OOV words, only about 1/3 are NNPs.
The performance of off-the-shelf news-trained
POS taggers also suffers on Twitter data. The state-
of-the-art Stanford POS tagger (Toutanova et al,
2003) improves on the baseline, obtaining an accu-
racy of 0.8. This performance is impressive given
that its training data, the Penn Treebank WSJ (PTB),
is so different in style from Twitter, however it is a
huge drop from the 97% accuracy reported on the
3We used Brendan O?Connor?s Twitter tokenizer
1525
Gold Predicted Stanford
Error
T-POS Error Error
Reduction
NN NNP 0.102 0.072 29%
UH NN 0.387 0.047 88%
VB NN 0.071 0.032 55%
NNP NN 0.130 0.125 4%
UH NNP 0.200 0.036 82%
Table 3: Most common errors made by the Stanford POS
Tagger on tweets. For each case we list the fraction of
times the gold tag is misclassified as the predicted for
both our system and the Stanford POS tagger. All verbs
are collapsed into VB for compactness.
PTB. There are several reasons for this drop in per-
formance. Table 3 lists common errors made by
the Stanford tagger. First, due to unreliable capi-
talization, common nouns are often misclassified as
proper nouns, and vice versa. Also, interjections
and verbs are frequently misclassified as nouns. In
addition to differences in vocabulary, the grammar
of tweets is quite different from edited news text.
For instance, tweets often start with a verb (where
the subject ?I? is implied), as in: ?watchng american
dad.?
To overcome these differences in style and vocab-
ulary, we manually annotated a set of 800 tweets
(16K tokens) with tags from the Penn TreeBank tag
set for use as in-domain training data for our POS
tagging system, T-POS.4 We add new tags for the
Twitter specific phenomena: retweets, @usernames,
#hashtags, and urls. Note that words in these cate-
gories can be tagged with 100% accuracy using sim-
ple regular expressions. To ensure fair comparison
in Table 2, we include a postprocessing step which
tags these words appropriately for all systems.
To help address the issue of OOV words and
lexical variations, we perform clustering to group
together words which are distributionally similar
(Brown et al, 1992; Turian et al, 2010). In particu-
lar, we perform hierarchical clustering using Jcluster
(Goodman, 2001) on 52 million tweets; each word
is uniquely represented by a bit string based on the
path from the root of the resulting hierarchy to the
word?s leaf. We use the Brown clusters resulting
from prefixes of 4, 8, and 12 bits. These clusters are
often effective in capturing lexical variations, for ex-
4Using MMAX2 (Mu?ller and Strube, 2006) for annotation.
ample, following are lexical variations on the word
?tomorrow? from one cluster after filtering out other
words (most of which refer to days):
?2m?, ?2ma?, ?2mar?, ?2mara?, ?2maro?,
?2marrow?, ?2mor?, ?2mora?, ?2moro?, ?2mo-
row?, ?2morr?, ?2morro?, ?2morrow?, ?2moz?,
?2mr?, ?2mro?, ?2mrrw?, ?2mrw?, ?2mw?,
?tmmrw?, ?tmo?, ?tmoro?, ?tmorrow?, ?tmoz?,
?tmr?, ?tmro?, ?tmrow?, ?tmrrow?, ?tm-
rrw?, ?tmrw?, ?tmrww?, ?tmw?, ?tomaro?,
?tomarow?, ?tomarro?, ?tomarrow?, ?tomm?,
?tommarow?, ?tommarrow?, ?tommoro?, ?tom-
morow?, ?tommorrow?, ?tommorw?, ?tomm-
row?, ?tomo?, ?tomolo?, ?tomoro?, ?tomorow?,
?tomorro?, ?tomorrw?, ?tomoz?, ?tomrw?,
?tomz?
T-POS uses Conditional Random Fields5 (Laf-
ferty et al, 2001), both because of their ability to
model strong dependencies between adjacent POS
tags, and also to make use of highly correlated fea-
tures (for example a word?s identity in addition to
prefixes and suffixes). Besides employing the Brown
clusters computed above, we use a fairly standard set
of features that include POS dictionaries, spelling
and contextual features.
On a 4-fold cross validation over 800 tweets,
T-POS outperforms the Stanford tagger, obtaining a
26% reduction in error. In addition we include 40K
tokens of annotated IRC chat data (Forsythand and
Martell, 2007), which is similar in style. Like Twit-
ter, IRC data contains many misspelled/abbreviated
words, and also more pronouns, and interjections,
but fewer determiners than news. Finally, we also
leverage 50K POS-labeled tokens from the Penn
Treebank (Marcus et al, 1994).
Overall T-POS trained on 102K tokens (12K from
Twitter, 40K from IRC and 50K from PTB) results
in a 41% error reduction over the Stanford tagger,
obtaining an accuracy of 0.883. Table 3 lists gains
on some of the most common error types, for ex-
ample, T-POS dramatically reduces error on inter-
jections and verbs that are incorrectly classified as
nouns by the Stanford tagger.
2.2 Shallow Parsing
Shallow parsing, or chunking is the task of identi-
fying non-recursive phrases, such as noun phrases,
5We use MALLET (McCallum, 2002).
1526
Accuracy Error
Reduction
Majority Baseline (B-NP) 0.266 -
OpenNLP 0.839 -
T-CHUNK(CoNLL) 0.854 9%
T-CHUNK(Twitter) 0.867 17%
T-CHUNK(CoNLL + Twitter) 0.875 22%
Table 4: Token-Level accuracy at shallow parsing tweets.
We compare against the OpenNLP chunker as a baseline.
verb phrases, and prepositional phrases in text. Ac-
curate shallow parsing of tweets could benefit sev-
eral applications such as Information Extraction and
Named Entity Recognition.
Off the shelf shallow parsers perform noticeably
worse on tweets, motivating us again to annotate in-
domain training data. We annotate the same set of
800 tweets mentioned previously with tags from the
CoNLL shared task (Tjong Kim Sang and Buchholz,
2000). We use the set of shallow parsing features de-
scribed by Sha and Pereira (2003), in addition to the
Brown clusters mentioned above. Part-of-speech tag
features are extracted based on cross-validation out-
put predicted by T-POS. For inference and learning,
again we use Conditional Random Fields. We utilize
16K tokens of in-domain training data (using cross
validation), in addition to 210K tokens of newswire
text from the CoNLL dataset.
Table 4 reports T-CHUNK?s performance at shal-
low parsing of tweets. We compare against the off-
the shelf OpenNLP chunker6, obtaining a 22% re-
duction in error.
2.3 Capitalization
A key orthographic feature for recognizing named
entities is capitalization (Florian, 2002; Downey et
al., 2007). Unfortunately in tweets, capitalization
is much less reliable than in edited texts. In addi-
tion, there is a wide variety in the styles of capital-
ization. In some tweets capitalization is informative,
whereas in other cases, non-entity words are capital-
ized simply for emphasis. Some tweets contain all
lowercase words (8%), whereas others are in ALL
CAPS (0.6%).
To address this issue, it is helpful to incorporate
information based on the entire content of the mes-
6http://incubator.apache.org/opennlp/
P R F1
Majority Baseline 0.70 1.00 0.82
T-CAP 0.77 0.98 0.86
Table 5: Performance at predicting reliable capitalization.
sage to determine whether or not its capitalization
is informative. To this end, we build a capitaliza-
tion classifier, T-CAP, which predicts whether or not
a tweet is informatively capitalized. Its output is
used as a feature for Named Entity Recognition. We
manually labeled our 800 tweet corpus as having
either ?informative? or ?uninformative? capitaliza-
tion. The criteria we use for labeling is as follows:
if a tweet contains any non-entity words which are
capitalized, but do not begin a sentence, or it con-
tains any entities which are not capitalized, then its
capitalization is ?uninformative?, otherwise it is ?in-
formative?.
For learning , we use Support Vector Ma-
chines.7 The features used include: the frac-
tion of words in the tweet which are capitalized,
the fraction which appear in a dictionary of fre-
quently lowercase/capitalized words but are not low-
ercase/capitalized in the tweet, the number of times
the word ?I? appears lowercase and whether or not
the first word in the tweet is capitalized. Results
comparing against the majority baseline, which pre-
dicts capitalization is always informative, are shown
in Table 5. Additionally, in ?3 we show that fea-
tures based on our capitalization classifier improve
performance at named entity segmentation.
3 Named Entity Recognition
We now discuss our approach to named entity recog-
nition on Twitter data. As with POS tagging and
shallow parsing, off the shelf named-entity recog-
nizers perform poorly on tweets. For example, ap-
plying the Stanford Named Entity Recognizer to one
of the examples from Table 1 results in the following
output:
[Yess]ORG! [Yess]ORG! Its official
[Nintendo]LOC announced today that they
Will release the [Nintendo]ORG 3DS in north
[America]LOC march 27 for $250
7http://www.chasen.org/?taku/software/
TinySVM/
1527
The OOV word ?Yess? is mistaken as a named en-
tity. In addition, although the first occurrence of
?Nintendo? is correctly segmented, it is misclassi-
fied, whereas the second occurrence is improperly
segmented ? it should be the product ?Nintendo
3DS?. Finally ?north America? should be segmented
as a LOCATION, rather than just ?America?. In gen-
eral, news-trained Named Entity Recognizers seem
to rely heavily on capitalization, which we know to
be unreliable in tweets.
Following Collins and Singer (1999), Downey et
al. (2007) and Elsner et al (2009), we treat classi-
fication and segmentation of named entities as sepa-
rate tasks. This allows us to more easily apply tech-
niques better suited towards each task. For exam-
ple, we are able to use discriminative methods for
named entity segmentation and distantly supervised
approaches for classification. While it might be ben-
eficial to jointly model segmentation and (distantly
supervised) classification using a joint sequence la-
beling and topic model similar to that proposed by
Sauper et al (2010), we leave this for potential fu-
ture work.
Because most words found in tweets are not part
of an entity, we need a larger annotated dataset to ef-
fectively learn a model of named entities. We there-
fore use a randomly sampled set of 2,400 tweets for
NER. All experiments (Tables 6, 8-10) report results
using 4-fold cross validation.
3.1 Segmenting Named Entities
Because capitalization in Twitter is less informative
than news, in-domain data is needed to train models
which rely less heavily on capitalization, and also
are able to utilize features provided by T-CAP.
We exhaustively annotated our set of 2,400 tweets
(34K tokens) with named entities.8 A convention on
Twitter is to refer to other users using the @ sym-
bol followed by their unique username. We deliber-
ately choose not to annotate @usernames as entities
in our data set because they are both unambiguous,
and trivial to identify with 100% accuracy using a
simple regular expression, and would only serve to
inflate our performance statistics. While there is am-
biguity as to the type of @usernames (for example,
8We found that including out-of-domain training data from
the MUC competitions lowered performance at this task.
P R F1 F1 inc.
Stanford NER 0.62 0.35 0.44 -
T-SEG(None) 0.71 0.57 0.63 43%
T-SEG(T-POS) 0.70 0.60 0.65 48%
T-SEG(T-POS, T-CHUNK) 0.71 0.61 0.66 50%
T-SEG(All Features) 0.73 0.61 0.67 52%
Table 6: Performance at segmenting entities varying the
features used. ?None? removes POS, Chunk, and capital-
ization features. Overall we obtain a 52% improvement
in F1 score over the Stanford Named Entity Recognizer.
they can refer to people or companies), we believe
they could be more easily classified using features
of their associated user?s profile than contextual fea-
tures of the text.
T-SEG models Named Entity Segmentation as a
sequence-labeling task using IOB encoding for rep-
resenting segmentations (each word either begins, is
inside, or is outside of a named entity), and uses
Conditional Random Fields for learning and infer-
ence. Again we include orthographic, contextual
and dictionary features; our dictionaries included a
set of type lists gathered from Freebase. In addition,
we use the Brown clusters and outputs of T-POS,
T-CHUNK and T-CAP in generating features.
We report results at segmenting named entities in
Table 6. Compared with the state-of-the-art news-
trained Stanford Named Entity Recognizer (Finkel
et al, 2005), T-SEG obtains a 52% increase in F1
score.
3.2 Classifying Named Entities
Because Twitter contains many distinctive, and in-
frequent entity types, gathering sufficient training
data for named entity classification is a difficult task.
In any random sample of tweets, many types will
only occur a few times. Moreover, due to their
terse nature, individual tweets often do not contain
enough context to determine the type of the enti-
ties they contain. For example, consider following
tweet:
KKTNY in 45min..........
without any prior knowledge, there is not enough
context to determine what type of entity ?KKTNY?
refers to, however by exploiting redundancy in the
data (Downey et al, 2010), we can determine it is
likely a reference to a television show since it of-
1528
ten co-occurs with words such as watching and pre-
mieres in other contexts.9
In order to handle the problem of many infre-
quent types, we leverage large lists of entities and
their types gathered from an open-domain ontology
(Freebase) as a source of distant supervision, allow-
ing use of large amounts of unlabeled data in learn-
ing.
Freebase Baseline: Although Freebase has very
broad coverage, simply looking up entities and their
types is inadequate for classifying named entities in
context (0.38 F-score, ?3.2.1). For example, accord-
ing to Freebase, the mention ?China? could refer to
a country, a band, a person, or a film. This prob-
lem is very common: 35% of the entities in our data
appear in more than one of our (mutually exclusive)
Freebase dictionaries. Additionally, 30% of entities
mentioned on Twitter do not appear in any Freebase
dictionary, as they are either too new (for example a
newly released videogame), or are misspelled or ab-
breviated (for example ?mbp? is often used to refer
to the ?mac book pro?).
Distant Supervision with Topic Models: To
model unlabeled entities and their possible types, we
apply LabeledLDA (Ramage et al, 2009), constrain-
ing each entity?s distribution over topics based on
its set of possible types according to Freebase. In
contrast to previous weakly supervised approaches
to Named Entity Classification, for example the Co-
Training and Na??ve Bayes (EM) models of Collins
and Singer (1999), LabeledLDA models each entity
string as a mixture of types rather than using a single
hidden variable to represent the type of each men-
tion. This allows information about an entity?s dis-
tribution over types to be shared across mentions,
naturally handling ambiguous entity strings whose
mentions could refer to different types.
Each entity string in our data is associated with a
bag of words found within a context window around
all of its mentions, and also within the entity itself.
As in standard LDA (Blei et al, 2003), each bag of
words is associated with a distribution over topics,
Multinomial(?e), and each topic is associated with a
distribution over words, Multinomial(?t). In addi-
tion, there is a one-to-one mapping between topics
and Freebase type dictionaries. These dictionaries
9Kourtney & Kim Take New York.
constrain ?e, the distribution over topics for each en-
tity string, based on its set of possible types, FB[e].
For example, ?Amazon could correspond to a distribu-
tion over two types: COMPANY, and LOCATION,
whereas ?Apple might represent a distribution over
COMPANY, and FOOD. For entities which aren?t
found in any of the Freebase dictionaries, we leave
their topic distributions ?e unconstrained. Note that
in absence of any constraints LabeledLDA reduces
to standard LDA, and a fully unsupervised setting
similar to that presented by Elsner et. al. (2009).
In detail, the generative process that models our
data for Named Entity Classification is as follows:
for each type: t = 1 . . . T do
Generate ?t according to symmetric Dirichlet
distribution Dir(?).
end for
for each entity string e = 1 . . . |E| do
Generate ?e over FB[e] according to Dirichlet
distribution Dir(?FB[e]).
for each word position i = 1 . . . Ne do
Generate ze,i from Mult(?e).
Generate the word we,i from Mult(?ze,i).
end for
end for
To infer values for the hidden variables, we apply
Collapsed Gibbs sampling (Griffiths and Steyvers,
2004), where parameters are integrated out, and the
ze,is are sampled directly.
In making predictions, we found it beneficial to
consider ?traine as a prior distribution over types forentities which were encountered during training. In
practice this sharing of information across contexts
is very beneficial as there is often insufficient evi-
dence in an isolated tweet to determine an entity?s
type. For entities which weren?t encountered dur-
ing training, we instead use a prior based on the dis-
tribution of types across all entities. One approach
to classifying entities in context is to assume that
?traine is fixed, and that all of the words inside theentity mention and context, w, are drawn based on
a single topic, z, that is they are all drawn from
Multinomial(?z). We can then compute the poste-
rior distribution over types in closed form with a
simple application of Bayes rule:
P (z|w) ?
?
w?w
P (w|z : ?)P (z : ?traine )
During development, however, we found that rather
than making these assumptions, using Gibbs Sam-
1529
Type Top 20 Entities not found in Freebase dictionaries
PRODUCT nintendo ds lite, apple ipod, generation black, ipod nano, apple iphone, gb black, xperia, ipods, verizon
media, mac app store, kde, hd video, nokia n8, ipads, iphone/ipod, galaxy tab, samsung galaxy, playstation
portable, nintendo ds, vpn
TV-SHOW pretty little, american skins, nof, order svu, greys, kktny, rhobh, parks & recreation, parks & rec, dawson
?s creek, big fat gypsy weddings, big fat gypsy wedding, winter wipeout, jersey shores, idiot abroad, royle,
jerseyshore, mr . sunshine, hawaii five-0, new jersey shore
FACILITY voodoo lounge, grand ballroom, crash mansion, sullivan hall, memorial union, rogers arena, rockwood
music hall, amway center, el mocambo, madison square, bridgestone arena, cat club, le poisson rouge,
bryant park, mandalay bay, broadway bar, ritz carlton, mgm grand, olympia theatre, consol energy center
Table 7: Example type lists produced by LabeledLDA. No entities which are shown were found in Freebase; these are
typically either too new to have been added, or are misspelled/abbreviated (for example rhobh=?Real Housewives of
Beverly Hills?). In a few cases there are segmentation errors.
pling to estimate the posterior distribution over types
performs slightly better. In order to make predic-
tions, for each entity we use an informative Dirich-
let prior based on ?traine and perform 100 iterations of
Gibbs Sampling holding the hidden topic variables
in the training data fixed (Yao et al, 2009). Fewer
iterations are needed than in training since the type-
word distributions, ? have already been inferred.
3.2.1 Classification Experiments
To evaluate T-CLASS?s ability to classify entity
mentions in context, we annotated the 2,400 tweets
with 10 types which are both popular on Twitter,
and have good coverage in Freebase: PERSON,
GEO-LOCATION, COMPANY, PRODUCT, FACIL-
ITY, TV-SHOW, MOVIE, SPORTSTEAM, BAND,
and OTHER. Note that these type annotations are
only used for evaluation purposes, and not used dur-
ing training T-CLASS, which relies only on distant
supervision. In some cases, we combine multi-
ple Freebase types to create a dictionary of entities
representing a single type (for example the COM-
PANY dictionary contains Freebase types /busi-
ness/consumer company and /business/brand). Be-
cause our approach does not rely on any manually
labeled examples, it is straightforward to extend it
for a different sets of types based on the needs of
downstream applications.
Training: To gather unlabeled data for inference,
we run T-SEG, our entity segmenter (from ?3.1), on
60M tweets, and keep the entities which appear 100
or more times. This results in a set of 23,651 dis-
tinct entity strings. For each entity string, we col-
lect words occurring in a context window of 3 words
from all mentions in our data, and use a vocabulary
of the 100K most frequent words. We run Gibbs
sampling for 1,000 iterations, using the last sample
to estimate entity-type distributions ?e, in addition
to type-word distributions ?t. Table 7 displays the
20 entities (not found in Freebase) whose posterior
distribution ?e assigns highest probability to selected
types.
Results: Table 8 presents the classification re-
sults of T-CLASS compared against a majority base-
line which simply picks the most frequent class
(PERSON), in addition to the Freebase baseline,
which only makes predictions if an entity appears
in exactly one dictionary (i.e., appears unambigu-
ous). T-CLASS also outperforms a simple super-
vised baseline which applies a MaxEnt classifier us-
ing 4-fold cross validation over the 1,450 entities
which were annotated for testing. Additionally we
compare against the co-training algorithm of Collins
and Singer (1999) which also leverages unlabeled
data and uses our Freebase type lists; for seed rules
we use the ?unambiguous? Freebase entities. Our
results demonstrate that T-CLASS outperforms the
baselines and achieves a 25% increase in F1 score
over co-training.
Tables 9 and 10 present a breakdown of F1 scores
by type, both collapsing types into the standard
classes used in the MUC competitions (PERSON,
LOCATION, ORGANIZATION), and using the 10
popular Twitter types described earlier.
Entity Strings vs. Entity Mentions: DL-Cotrain
and LabeledLDA use two different representations
for the unlabeled data during learning. LabeledLDA
groups together words across all mentions of an en-
1530
System P R F1
Majority Baseline 0.30 0.30 0.30
Freebase Baseline 0.85 0.24 0.38
Supervised Baseline 0.45 0.44 0.45
DL-Cotrain 0.54 0.51 0.53
LabeledLDA 0.72 0.60 0.66
Table 8: Named Entity Classification performance on the
10 types. Assumes segmentation is given as in (Collins
and Singer, 1999), and (Elsner et al, 2009).
Type LL FB CT SP N
PERSON 0.82 0.48 0.65 0.83 436
LOCATION 0.74 0.21 0.55 0.67 372
ORGANIZATION 0.66 0.52 0.55 0.31 319
overall 0.75 0.39 0.59 0.49 1127
Table 9: F1 classification scores for the 3 MUC types
PERSON, LOCATION, ORGANIZATION. Results are
shown using LabeledLDA (LL), Freebase Baseline (FB),
DL-Cotrain (CT) and Supervised Baseline (SP). N is the
number of entities in the test set.
Type LL FB CT SP N
PERSON 0.82 0.48 0.65 0.86 436
GEO-LOC 0.77 0.23 0.60 0.51 269
COMPANY 0.71 0.66 0.50 0.29 162
FACILITY 0.37 0.07 0.14 0.34 103
PRODUCT 0.53 0.34 0.40 0.07 91
BAND 0.44 0.40 0.42 0.01 54
SPORTSTEAM 0.53 0.11 0.27 0.06 51
MOVIE 0.54 0.65 0.54 0.05 34
TV-SHOW 0.59 0.31 0.43 0.01 31
OTHER 0.52 0.14 0.40 0.23 219
overall 0.66 0.38 0.53 0.45 1450
Table 10: F1 scores for classification broken down by
type for LabeledLDA (LL), Freebase Baseline (FB), DL-
Cotrain (CT) and Supervised Baseline (SP). N is the num-
ber of entities in the test set.
P R F1
DL-Cotrain-entity 0.47 0.45 0.46
DL-Cotrain-mention 0.54 0.51 0.53
LabeledLDA-entity 0.73 0.60 0.66
LabeledLDA-mention 0.57 0.52 0.54
Table 11: Comparing LabeledLDA and DL-Cotrain
grouping unlabeled data by entities vs. mentions.
System P R F1
COTRAIN-NER (10 types) 0.55 0.33 0.41
T-NER(10 types) 0.65 0.42 0.51
COTRAIN-NER (PLO) 0.57 0.42 0.49
T-NER(PLO) 0.73 0.49 0.59
Stanford NER (PLO) 0.30 0.27 0.29
Table 12: Performance at predicting both segmentation
and classification. Systems labeled with PLO are evalu-
ated on the 3 MUC types PERSON, LOCATION, ORGA-
NIZATION.
tity string, and infers a distribution over its possi-
ble types, whereas DL-Cotrain considers the entity
mentions separately as unlabeled examples and pre-
dicts a type independently for each. In order to
ensure that the difference in performance between
LabeledLDA and DL-Cotrain is not simply due to
this difference in representation, we compare both
DL-Cotrain and LabeledLDA using both unlabeled
datasets (grouping words by all mentions vs. keep-
ing mentions separate) in Table 11. As expected,
DL-Cotrain performs poorly when the unlabeled ex-
amples group mentions; this makes sense, since Co-
Training uses a discriminative learning algorithm,
so when trained on entities and tested on individual
mentions, the performance decreases. Additionally,
LabeledLDA?s performance is poorer when consid-
ering mentions as ?documents?. This is likely due
to the fact that there isn?t enough context to effec-
tively learn topics when the ?documents? are very
short (typically fewer than 10 words).
End to End System: Finally we present the end
to end performance on segmentation and classifica-
tion (T-NER) in Table 12. We observe that T-NER
again outperforms co-training. Moreover, compar-
ing against the Stanford Named Entity Recognizer
on the 3 MUC types, T-NER doubles F1 score.
4 Related Work
There has been relatively little previous work on
building NLP tools for Twitter or similar text styles.
Locke and Martin (2009) train a classifier to recog-
nize named entities based on annotated Twitter data,
handling the types PERSON, LOCATION, and OR-
GANIZATION. Developed in parallel to our work,
Liu et al (2011) investigate NER on the same 3
types, in addition to PRODUCTs and present a semi-
1531
supervised approach using k-nearest neighbor. Also
developed in parallel, Gimpell et al (2011) build a
POS tagger for tweets using 20 coarse-grained tags.
Benson et. al. (2011) present a system which ex-
tracts artists and venues associated with musical per-
formances. Recent work (Han and Baldwin, 2011;
Gouws et al, 2011) has proposed lexical normaliza-
tion of tweets which may be useful as a preprocess-
ing step for the upstream tasks like POS tagging and
NER. In addition Finin et. al. (2010) investigate
the use of Amazon?s Mechanical Turk for annotat-
ing Named Entities in Twitter, Minkov et. al. (2005)
investigate person name recognizers in email, and
Singh et. al. (2010) apply a minimally supervised
approach to extracting entities from text advertise-
ments.
In contrast to previous work, we have demon-
strated the utility of features based on Twitter-
specific POS taggers and Shallow Parsers in seg-
menting Named Entities. In addition we take a dis-
tantly supervised approach to Named Entity Classi-
fication which exploits large dictionaries of entities
gathered from Freebase, requires no manually anno-
tated data, and as a result is able to handle a larger
number of types than previous work. Although we
found manually annotated data to be very beneficial
for named entity segmentation, we were motivated
to explore approaches that don?t rely on manual la-
bels for classification due to Twitter?s wide range of
named entity types. Additionally, unlike previous
work on NER in informal text, our approach allows
the sharing of information across an entity?s men-
tions which is quite beneficial due to Twitter?s terse
nature.
Previous work on Semantic Bootstrapping has
taken a weakly-supervised approach to classifying
named entities based on large amounts of unla-
beled text (Etzioni et al, 2005; Carlson et al, 2010;
Kozareva and Hovy, 2010; Talukdar and Pereira,
2010; McIntosh, 2010). In contrast, rather than
predicting which classes an entity belongs to (e.g.
a multi-label classification task), LabeledLDA esti-
mates a distribution over its types, which is then use-
ful as a prior when classifying mentions in context.
In addition there has been been work on Skip-
Chain CRFs (Sutton, 2004; Finkel et al, 2005)
which enforce consistency when classifying multi-
ple occurrences of an entity within a document. Us-
ing topic models (e.g. LabeledLDA) for classifying
named entities has a similar effect, in that informa-
tion about an entity?s distribution of possible types
is shared across its mentions.
5 Conclusions
We have demonstrated that existing tools for POS
tagging, Chunking and Named Entity Recognition
perform quite poorly when applied to Tweets. To
address this challenge we have annotated tweets and
built tools trained on unlabeled, in-domain and out-
of-domain data, showing substantial improvement
over their state-of-the art news-trained counterparts,
for example, T-POS outperforms the Stanford POS
Tagger, reducing error by 41%. Additionally we
have shown the benefits of features generated from
T-POS and T-CHUNK in segmenting Named Entities.
We identified named entity classification as a par-
ticularly challenging task on Twitter. Due to their
terse nature, tweets often lack enough context to
identify the types of the entities they contain. In ad-
dition, a plethora of distinctive named entity types
are present, necessitating large amounts of training
data. To address both these issues we have presented
and evaluated a distantly supervised approach based
on LabeledLDA, which obtains a 25% increase in F1
score over the co-training approach to Named En-
tity Classification suggested by Collins and Singer
(1999) when applied to Twitter.
Our POS tagger, Chunker Named Entity Rec-
ognizer are available for use by the research
community: http://github.com/aritter/
twitter_nlp
Acknowledgments
We would like to thank Stephen Soderland, Dan
Weld and Luke Zettlemoyer, in addition to the
anonymous reviewers for helpful comments on a
previous draft. This research was supported in part
by NSF grant IIS-0803481, ONR grant N00014-11-
1-0294, Navy STTR contract N00014-10-M-0304, a
National Defense Science and Engineering Graduate
(NDSEG) Fellowship 32 CFR 168a and carried out
at the University of Washington?s Turing Center.
1532
References
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In The
49th Annual Meeting of the Association for Computa-
tional Linguistics, Portland, Oregon, USA. To appear.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn. Res.
Avrim Blum and Tom M. Mitchell. 1998. Combining
labeled and unlabeled sata with co-training. In COLT,
pages 92?100.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka, Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the third ACM interna-
tional conference on Web search and data mining,
WSDM ?10.
Eugene Charniak, Curtis Hendrickson, Neil Jacobson,
and Mike Perkowitz. 1993. Equations for part-of-
speech tagging. In AAAI, pages 784?789.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Empirical
Methods in Natural Language Processing.
Doug Downey, Matthew Broadhead, and Oren Etzioni.
2007. Locating complex named entities in web text.
In Proceedings of the 20th international joint confer-
ence on Artifical intelligence.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2010. Analysis of a probabilistic model of redundancy
in unsupervised information extraction. Artif. Intell.,
174(11):726?748.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsupervised
named-entity clustering. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, NAACL ?09.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with crowd-
sourcing. In Proceedings of the NAACL Workshop on
Creating Speech and Text Language Data With Ama-
zon?s Mechanical Turk. Association for Computational
Linguistics, June.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL ?05.
Radu Florian. 2002. Named entity recognition as a house
of cards: classifier stacking. In Proceedings of the 6th
conference on Natural language learning - Volume 20,
COLING-02.
Eric N. Forsythand and Craig H. Martell. 2007. Lexical
and discourse analysis of online chat dialog. In Pro-
ceedings of the International Conference on Semantic
Computing.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In ACL.
Joshua T. Goodman. 2001. A bit of progress in language
modeling. Technical report, Microsoft Research.
Stephan Gouws, Donald Metzler, Congxing Cai, and Ed-
uard Hovy. 2011. Contextual bearing on linguistic
variation in social media. In ACL Workshop on Lan-
guage in Social Media, Portland, Oregon, USA. To
appear.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, April.
Mark Hachman. 2011. Humanity?s tweets: Just 20 ter-
abytes. In PCMAG.COM.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a #twitter. In
The 49th Annual Meeting of the Association for Com-
putational Linguistics, Portland, Oregon, USA. To ap-
pear.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing sms: are two metaphors
better than one ? In COLING, pages 441?448.
Zornitsa Kozareva and Eduard H. Hovy. 2010. Not all
seeds are equal: Measuring the quality of text mining
seeds. In HLT-NAACL.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets.
In ACL.
Brian Locke and James Martin. 2009. Named entity
recognition: Adapting to microblogging. In Senior
Thesis, University of Colorado.
1533
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. In http://mallet.
cs.umass.edu.
Tara McIntosh. 2010. Unsupervised discovery of nega-
tive categories in lexicon bootstrapping. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10.
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from email: apply-
ing named entity recognition to informal text. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, HLT ?05, pages 443?450, Morristown, NJ,
USA. Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL-IJCNLP
2009.
Christoph Mu?ller and Michael Strube. 2006. Multi-level
annotation of linguistic data with MMAX2. In Sabine
Braun, Kurt Kohn, and Joybrato Mukherjee, editors,
Corpus Technology and Language Pedagogy: New Re-
sources, New Tools, New Methods, pages 197?214. Pe-
ter Lang, Frankfurt a.M., Germany.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled lda: a super-
vised topic model for credit attribution in multi-labeled
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 1 - Volume 1, EMNLP ?09, pages 248?256,
Morristown, NJ, USA. Association for Computational
Linguistics.
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2010. Incorporating content structure into text anal-
ysis applications. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 377?387, Morristown,
NJ, USA. Association for Computational Linguistics.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology - Volume 1, NAACL ?03.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In Human Language Technologies:
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT).
Charles Sutton. 2004. Collective segmentation and la-
beling of distant entities in information extraction.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1473?1481. Associ-
ation for Computational Linguistics.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: chunking.
In Proceedings of the 2nd workshop on Learning lan-
guage in logic and the 4th conference on Computa-
tional natural language learning - Volume 7, ConLL
?00.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference on
streaming document collections. In Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics, ACL ?95.
1534
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 425?429, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SwatCS: Combining simple classifiers with estimated accuracy
Sam Clark and Richard Wicentowski
Department of Computer Science
Swarthmore College
Swarthmore, PA 19081 USA
sclark2@sccs.swarthmore.edu and richardw@cs.swarthmore.edu
Abstract
This paper is an overview of the SwatCS
system submitted to SemEval-2013 Task 2A:
Contextual Polarity Disambiguation. The sen-
timent of individual phrases within a tweet
are labeled using a combination of classifiers
trained on a range of lexical features. The
classifiers are combined by estimating the ac-
curacy of the classifiers on each tweet. Perfor-
mance is measured when using only the pro-
vided training data, and separately when in-
cluding external data.
1 Introduction
Spurred on by the wide-spread use of the social net-
works to communicate with friends, fans and cus-
tomers around the globe, Twitter has been adopted
by celebrities, athletes, politicians, and major com-
panies as a platform that mitigates the interaction be-
tween individuals.
Analysis of this Twitter data can provide insights
into how users express themselves. For example,
many new forms of expression and language fea-
tures have emerged on Twitter, including expres-
sions containing mentions, hashtags, emoticons, and
abbreviations. This research leverages the lexical
features in tweets to predict whether a phrase within
a tweet conveys a positive or negative sentiment.
2 Related Work
A common goal of past research has been to discover
and extract features from tweets that accurately in-
dicate sentiment (Liu, 2010). The importance of
feature selection and machine learning in sentiment
analysis has been explored prior to the rise of so-
cial networks. For example, Pang and Lee (2004)
apply machine learning techniques to extracted fea-
tures from movie reviews.
More recent feature-based systems include a
lexicon-based approach (Taboada et al, 2011), and
a more focused study on the importance of both ad-
verbs and adjectives in determining sentiment (Be-
namara et al, 2007). Other examples include us-
ing looser descriptions of sentiment rather than rigid
positive/negative labelings (Whitelaw et al, 2005)
and investigating how connections between users
can be used to predict sentiment (Tan et al, 2011).
This task differs from past work in sentiment anal-
ysis of tweets because we aim to build a model capa-
ble of predicting the sentiment of sub-phrases within
the tweet rather than considering the entire tweet.
Specifically, ?given a message containing a marked
instance of a word or a phrase, determine whether
that instance is positive, negative or neutral in that
context? (Wilson et al, 2013). Research on context-
oriented polarity predates the emergence of social
networks: (Nasukawa and Yi, 2003) predict senti-
ment of subsections in a larger document.
N-gram features, part of speech features and
?micro-blogging features? have been used as accu-
rate indicators of polarity (Kouloumpis et al, 2011).
The ?micro-blogging features? are of particular in-
terest as they provide insight into how users have
adapted Twitter tokens to natural language to por-
tray sentiment. These features include hashtags and
emoticons (Kouloumpis et al, 2011).
425
3 Data
The task organizers provided a manually-labeled set
of tweets. For parts of this study, their data was sup-
plemented with external data (Go et al, 2009).
As part of pre-processing, all tweets were
part-of-speech tagged using the ARK TweetNLP
tools (Owoputi et al, 2013). All punctuation was
stripped, except for #hashtags, @mentions,
emoticons :), and exclamation marks. All hyper-
links were replaced with a common string, ?URL?.
3.1 Common Data
The provided training data was a collection of ap-
proximately 15K tweets, manually labeled for senti-
ment (positive, negative, neutral, or objective) (Wil-
son et al, 2013). These sentiment labels applied
to a specific phrase within the tweet and did not
necessarily match the sentiment of the entire tweet.
Each tweet had at least one labeled phrase, though
some tweets had multiple phrases labeled individu-
ally. Overall, 37% of tweets had one labeled phrase,
with an average of 2.58 labeled phrases per tweet.
Each of our classifiers were binary classifiers, la-
beling phrases as either positive or negative. As
such, approximately 10.5K phrases labeled as objec-
tive or neutral were pruned from the training data,
resulting in a final training set containing 5362 la-
beled phrases, 3445 positive and 1917 negative.
The test data consisted of tweets and SMS mes-
sages, although the training data contained only
tweets. The test set for the phrase-level task (Task A)
contained 4435 tweets and 2334 SMS messages.
3.2 Outside Data
Task organizers allowed two submissions, a con-
strained submission using only the provided training
data, and an unconstrained submission allowing the
use of external data. For the unconstrained submis-
sion, we used a data set built by Go et al (2009). The
data set was automatically labeled using emoticons
to predict sentiment. We used a 50K tweet subset
containing 25K positive and 25K negative tweets.
3.3 Phrase Isolation
For tweets containing a single labeled phrase, we use
the entire tweet as the context for the phrase. For
tweets containing two labeled phrases, we use the
unigram label bigram label
happy pos not going neg
good pos looking forward pos
great pos happy birthday pos
love pos last episode neg
best pos i?m mad neg
Table 1: The 5 most influential unigram and bigrams
ranked by information gain.
context from the start of the tweet to the end of the
first phrase as the context for the first phrase, and the
context from the start of the second phrase to the end
of the tweet for the second phrase. If more than two
phrases are present, the context for any phrase in the
middle of the tweet is limited to only the words in
the labeled phrase.
4 Classifiers
The system uses a combination of naive Bayes clas-
sifiers to label the input. Each classifier is trained on
a single feature extracted from the tweet. The classi-
fiers are combined using a confidence-weighted vot-
ing scheme. The system applies a simple negation
scheme to all of the language features used by the
classifiers. Any word following a negation term in
the phrase has the substring ?NOT? prefixed to it.
This negation scheme was applied to n-gram fea-
tures and lexicon features.
4.1 N-gram Features
Rather than use all of the n-grams as features, we
ranked each n-gram (w/POS tags) by calculating its
chi-square-based information gain. The top 2000
n-grams (1000 positive, 1000 negative) are used as
features in the n-gram classifier. Both a unigram and
bigram classifier use these ranked (word/POS) fea-
tures. Table 1 shows the highest ranked unigrams
and bigrams using this method.
4.2 Sentiment Lexicon Features
A second classifier uses the MPQA subjectivity lex-
icon (Wiebe et al, 2005). We extract both the po-
larity and the polarity strength for each word/POS
in the lexicon matching a word/POS in the phrase?s
context. We refer to this classifier as the lexicon
classifier.
426
 0.78 0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.96 0.98
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9Confidence/classifier accuracy alpha = abs(P(pos) - P(neg))Alpha vs. Classifier accuracy
Figure 1: Classifier accuracy increases as the difference
between the probabilities of the labelings increases.
4.3 Part of Speech and Special Token Features
Three additional classifiers were built using features
extracted from the tweets. Our third classifier uses
only the raw counts of specific part of speech tags:
adjectives, adverbs, interjections, and emoticons.
The fourth classifier uses the emoticons as a fea-
ture. To reduce the noise in the emoticon feature set,
many (over 25) different emoticons are mapped to
the basic ?:)? and ?:(? expressions. Some emoticons
such as ?xD? did not map to these basic expressions.
A fifth classifier gives added weight to words with
extraneous repeated letters. Words containing two
or more repeated letters (that are not in a dictionary,
e.g. ?heyyyyy?, ?sweeeet?) are mapped to their pre-
sumed correct spelling (e.g. ?hey?, ?sweet?).
5 Confidence-Based Classification
To combine all of the classifiers, the system esti-
mates the confidence of each classifier and only ac-
cepts the classification output if the confidence is
higher than a specified baseline. To establish a clas-
sifier?s confidence, we take the absolute value of
the difference between a classifier?s positive output
probability and negative output probability, which
we call alpha. Alpha values close to 1 indicate high
confidence in the predicted label; values close to 0
indicate low confidence in the predicted label.
5.1 Classifier Voting
The predicted accuracy of each classifier is deter-
mined after the trained classifiers are evaluated us-
ing a development set with known labels. Using the
dev set, we calculate the accuracy of each classi-
rank classifier data polarity acc
1 unigrams (C) positive 0.89
2 unigrams (U) positive 0.88
3 lexicon (C) negative 0.83
4 lexicon (U) negative 0.81
5 tagcount (C) positive 0.78
6 bigrams (C) positive 0.75
7 tagcount (U) novote <0.65
8 bigrams (U) novote <0.65
Table 2: An example of the polarity and corresponding
accuracy output for each classifier for a single tweet. The
labels (C) and (U) indicate whether the classifier was
trained on constrained training data or on unconstrained
data (Go et al, 2009).
fier at alpha values between 0 and 1. The result is
a trained classifier with an approximation of overall
classification accuracy at a given alpha value. Fig-
ure 1 shows the relationship between alpha value
and overall classifier accuracy. As expected, classi-
fication accuracy increases as confidence increases.
Table 2 shows the breakdown of classifier accu-
racy for a single tweet using both provided and ex-
ternal data. The accuracy listed is the classifier-
specific accuracy determined by the alpha value for
that phrase in the tweet. Using a dev set, we ex-
perimentally established the most effective baseline
to be 0.65. In the voting system described below,
only classifiers with confidence above the baseline
(per marked phrase) are used. Therefore, the spe-
cific combination of classifiers used for each phrase
may be different.
An unlabeled phrase is assigned a polarity and
confidence value from each classifier. These proba-
bilities are combined using a voting system to deter-
mine a single output. This voting system calculates
the final labeling by computing the average proba-
bility for each label only for those classifiers with
estimated accuracies above the baseline. The label
with the highest overall probability is selected.
6 Results
The constrained submission only allowed for train-
ing on the provided data and placed 17 out of 23
entries. The unconstrained submission was trained
on both the provided data and the external data and
placed 6 out of 8 entries. Both submissions were
427
unigram label bigram label lexicon label
aint neg school tomorrow neg bad neg
excited pos not going neg excited pos
sucks neg didn?t get neg tired neg
sick neg might not neg dead neg
poor neg gonna miss neg poor neg
smh pos still haven?t neg happy pos
tough pos breakout kings neg black neg
greatest pos work tomorrow neg good pos
f*ck neg ray lewis pos hate neg
nets neg can?t wait pos sorry neg
Table 3: The most influential features from the unigram,
bigram, and lexicon classifiers.
evaluated using the Twitter and SMS data described
in Section 3.1. As mentioned, our system used a bi-
nary classifier, predicting only positive and negative
labels, making no neutral classifications.
The constrained system evaluated on the Twitter
test set had an F-measure of .672, with a high dis-
parity between the F-measure for tweets labeled as
positive versus those labeled as negative (.79 vs .53).
The unconstrained system on the Twitter test set un-
derperformed our constrained system, with an F-
measure of only .639.
The constrained system on the SMS test set
yielded an F-measure of .660; the unconstrained sys-
tem on the same data yielded an F-measure of .679.
6.1 Features Extracted
The most important features extracted by the un-
igram, bigram and lexicon classifiers are shown
in Table 3. Features such as ?ray lewis?, ?smh?,
?school tomorrow?, ?work tomorrow?, ?breakout
kings? and ?nets? demonstrate that the classifiers
formed a relationship between sentiment and collo-
quial language. An example of this understanding is
assigning a strong negative sentiment to ?sucks? (as
the verb ?to suck? does not carry sentiment). The bi-
grams ?breakout kings?, ?ray lewis? and ?nets? are
interesting features because their sentiment is highly
cultural: ?breakout kings? is a popular TV show that
was canceled, ?ray lewis? a high profile player for
an NFL team, and ?nets? a reference to the strug-
gling NBA basketball team. Expressions such as
?smh? (a widely-used abbreviation for ?shaking my
head?) show how detecting tweet- and SMS-specific
language is important to understanding sentiment in
this domain.
7 Discussion
This supervised system combines many features
to classify positive and negative sentiment at the
phrase-level. Phrase-based isolation (Section 3.3)
limits irrelevant context in the model. By estimat-
ing classifier confidence on a per-phrase basis, the
system can prioritize confident classifiers and ignore
less-confident ones before combination.
Similar results on the Twitter and SMS data sets
indicates the similarity between the domains. The
external data improved the system on the SMS data
and reduced system accuracy on the Twitter data.
This difference in performance may be an indication
that the supplemental data set was noisier than we
expected, or that it was more applicable to the SMS
domain (SMS) than we anticipated.
There was a noticeable difference between pos-
itive and negative classification accuracy for all of
the submissions. This difference is likely due to ei-
ther a positive bias in training set used (the provided
training data is 64% positive, 36% negative) or a se-
lection of features that favored positive sentiment.
7.1 Improvements and Future Work
Unfortunately, the time constraints of the evalua-
tion exercise led to a programming bug that wasn?t
caught until after the submission deadline. In pre-
processing, we accidentally stripped most of the
emoticon features out of the text. While it is un-
clear how much this would have effected our final
performance, such features have been demonstrated
as valuable in similar tasks. After fixing this bug
the system performs better in both constrained and
unconstrained situations (as evaluated on the devel-
opment set).
We would like to increase the size of external data
set to include all of the approximately 380K tweets
(rather than the 50K subset we used). This expanded
training set would likely improve the robustness of
the system. Specifically, we would expect classifiers
with limited coverage, such as the repeat-letter clas-
sifier, to yield increased performance.
428
References
Farah Benamara, Carmine Cesarano, Antonio Picariello,
Diego Reforgiato, and V Subrahmanian. 2007. Senti-
ment analysis: Adjectives and adverbs are better than
adjectives alone. In Proceedings of the International
Conference on Weblogs and Social Media (ICWSM).
A. Go, R. Bhayani, and Huang. L. 2009. Twitter senti-
ment classification using distant supervision. Techni-
cal report, Stanford University.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg. In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media, pages 538?541.
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of natural language processing, 2:568.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In Proceedings of the 2nd international
conference on Knowledge capture, K-CAP ?03, pages
70?77.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL 2013.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, ACL ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational linguistics,
37(2):267?307.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings of
the 17th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1397?
1405.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proceedings of the 14th ACM international con-
ference on Information and knowledge management,
CIKM ?05, pages 625?631.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval?13.
429
