Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 654?663,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Probabilistic Hierarchical Clustering of
Morphological Paradigms
Burcu Can
Department of Computer Science
University of York
Heslington, York, YO10 5GH, UK
burcucan@gmail.com
Suresh Manandhar
Department of Computer Science
University of York
Heslington, York, YO10 5GH, UK
suresh@cs.york.ac.uk
Abstract
We propose a novel method for learning
morphological paradigms that are struc-
tured within a hierarchy. The hierarchi-
cal structuring of paradigms groups mor-
phologically similar words close to each
other in a tree structure. This allows detect-
ing morphological similarities easily lead-
ing to improved morphological segmen-
tation. Our evaluation using (Kurimo et
al., 2011a; Kurimo et al 2011b) dataset
shows that our method performs competi-
tively when compared with current state-of-
art systems.
1 Introduction
Unsupervised morphological segmentation of a
text involves learning rules for segmenting words
into their morphemes. Morphemes are the small-
est meaning bearing units of words. The learn-
ing process is fully unsupervised, using only raw
text as input to the learning system. For example,
the word respectively is split into morphemes re-
spect, ive and ly. Many fields, such as machine
translation, information retrieval, speech recog-
nition etc., require morphological segmentation
since new words are always created and storing
all the word forms will require a massive dictio-
nary. The task is even more complex, when mor-
phologically complicated languages (i.e. agglu-
tinative languages) are considered. The sparsity
problem is more severe for more morphologically
complex languages. Applying morphological seg-
mentation mitigates data sparsity by tackling the
issue with out-of-vocabulary (OOV) words.
In this paper, we propose a paradigmatic ap-
proach. A morphological paradigm is a pair
(StemList, SuffixList) such that each concatena-
tion of Stem+Suffix (where Stem ? StemList and
Suffix ? SuffixList) is a valid word form. The
learning of morphological paradigms is not novel
as there has already been existing work in this area
such as Goldsmith (2001), Snover et al(2002),
Monson et al(2009), Can and Manandhar (2009)
and Dreyer and Eisner (2011). However, none of
these existing approaches address learning of the
hierarchical structure of paradigms.
Hierarchical organisation of words help cap-
ture morphological similarities between words in
a compact structure by factoring these similarities
through stems, suffixes or prefixes. Our inference
algorithm simultaneously infers latent variables
(i.e. the morphemes) along with their hierarchical
organisation. Most hierarchical clustering algo-
rithms are single-pass, where once the hierarchi-
cal structure is built, the structure does not change
further.
The paper is structured as follows: section 2
gives the related work, section 3 describes the
probabilistic hierarchical clustering scheme, sec-
tion 4 explains the morphological segmenta-
tion model by embedding it into the clustering
scheme and describes the inference algorithm
along with how the morphological segmentation
is performed, section 5 presents the experiment
settings along with the evaluation scores, and fi-
nally section 6 presents a discussion with a com-
parison with other systems that participated in
Morpho Challenge 2009 and 2010 .
2 Related Work
We propose a Bayesian approach for learning of
paradigms in a hierarchy. If we ignore the hierar-
chical aspect of our learning algorithm, then our
654
walk walking talked  talks
{walk}{0,ing} {talk}{ed,s} {quick}{0,ly}
quick quickly
{walk, talk, quick}{0,ed,ing,ly, s}
{walk, talk}{0,ed,ing,s}
Figure 1: A sample tree structure.
method is similar to the Dirichlet Process (DP)
based model of Goldwater et al(2006). From
this perspective, our method can be understood
as adding a hierarchical structure learning layer
on top of the DP based learning method proposed
in Goldwater et al(2006). Dreyer and Eisner
(2011) propose an infinite Diriclet mixture model
for capturing paradigms. However, they do not
address learning of hierarchy.
The method proposed in Chan (2006) also
learns within a hierarchical structure where La-
tent Dirichlet Allocation (LDA) is used to find
stem-suffix matrices. However, their work is su-
pervised, as true morphological analyses of words
are provided to the system. In contrast, our pro-
posed method is fully unsupervised.
3 Probabilistic Hierarchical Model
The hierarchical clustering proposed in this work
is different from existing hierarchical clustering
algorithms in two aspects:
? It is not single-pass as the hierarchical struc-
ture changes.
? It is probabilistic and is not dependent on a
distance metric.
3.1 Mathematical Definition
In this paper, a hierarchical structure is a binary
tree in which each internal node represents a clus-
ter.
Let a data set be D = {x1, x2, . . . , xn} and
T be the entire tree, where each data point xi is
located at one of the leaf nodes (see Figure 2).
Here, Dk denotes the data points in the branch
Tk. Each node defines a probabilistic model for
words that the cluster acquires. The probabilistic
Di
Dk
Dj
X1 X2 X3 X4
Figure 2: A segment of a tree with with internal nodes
Di, Dj , Dk having data points {x1, x2, x3, x4}. The
subtree below the internal node Di is called Ti, the
subtree below the internal node Dj is Tj , and the sub-
tree below the internal node Dk is Tk.
model can be denoted as p(xi|?) where ? denotes
the parameters of the probabilistic model.
The marginal probability of data in any node
can be calculated as:
p(Dk) =
?
p(Dk|?)p(?|?)d? (1)
The likelihood of data under any subtree is de-
fined as follows:
p(Dk|Tk) = p(Dk)p(Dl|Tl)p(Dr|Tr) (2)
where the probability is defined in terms of left Tl
and right Tr subtrees. Equation 2 provides a re-
cursive decomposition of the likelihood in terms
of the likelihood of the left and the right sub-
trees until the leaf nodes are reached. We use the
marginal probability (Equation 1) as prior infor-
mation since the marginal probability bears the
probability of having the data from the left and
right subtrees within a single cluster.
4 Morphological Segmentation
In our model, data points are words to be clus-
tered and each cluster represents a paradigm. In
the hierarchical structure, words will be organised
in such a way that morphologically similar words
will be located close to each other to be grouped
in the same paradigms. Morphological similarity
refers to at least one common morpheme between
words. However, we do not make a distinction be-
tween morpheme types. Instead, we assume that
each word is organised as a stem+suffix combina-
tion.
4.1 Model Definition
Let a dataset D consist of words to be analysed,
where each word wi has a latent variable which is
655
the split point that analyses the word into its stem
si and suffix mi:
D = {w1 = s1 +m1, . . . , wn = sn +mn}
The marginal likelihood of words in the node k
is defined such that:
p(Dk) = p(Sk)p(Mk)
= p(s1, s2, . . . , sn)p(m1,m2, . . . ,mn)
The words in each cluster represents a
paradigm that consists of stems and suffixes. The
hierarchical model puts words sharing the same
stems or suffixes close to each other in the tree.
Each word is part of all the paradigms on the
path from the leaf node having that word to the
root. The word can share either its stem or suffix
with other words in the same paradigm. Hence,
a considerable number of words can be generated
through this approach that may not be seen in the
corpus.
We postulate that stems and suffixes are gen-
erated independently from each other. Thus, the
probability of a word becomes:
p(w = s+m) = p(s)p(m) (3)
We define two Dirichlet processes to generate
stems and suffixes independently:
Gs|?s, Ps ? DP (?s, Ps)
Gm|?m, Pm ? DP (?m, Pm)
s|Gs ? Gs
m|Gm ? Gm
where DP (?s, Ps) denotes a Dirichlet process
that generates stems. Here, ?s is the concentration
parameter, which determines the number of stem
types generated by the Dirichlet process. The
smaller the value of the concentration parameter,
the less likely to generate new stem types the pro-
cess is. In contrast, the larger the value of concen-
tration parameter, the more likely it is to generate
new stem types, yielding a more uniform distribu-
tion over stem types. If ?s < 1, sparse stems are
supported, it yields a more skewed distribution.
To support a small number of stem types in each
cluster, we chose ?s < 1.
Here, Ps is the base distribution. We use the
base distribution as a prior probability distribu-
tion for morpheme lengths. We model morpheme
?s ?m
Ps PmGs Gm
si mi
wi
L N
n
Figure 3: The plate diagram of the model, representing
the generation of a word wi from the stem si and the
suffix mi that are generated from Dirichlet processes.
In the representation, solid-boxes denote that the pro-
cess is repeated with the number given on the corner
of each box.
lengths implicitly through the morpheme letters:
Ps(si) =
?
ci?si
p(ci) (4)
where ci denotes the letters, which are distributed
uniformly. Modelling morpheme letters is a way
of modelling the morpheme length since shorter
morphemes are favoured in order to have fewer
factors in Equation 4 (Creutz and Lagus, 2005b).
The Dirichlet process,DP (?m, Pm), is defined
for suffixes analogously. The graphical represen-
tation of the entire model is given in Figure 3.
Once the probability distributions G =
{Gs, Gm} are drawn from both Dirichlet pro-
cesses, words can be generated by drawing a stem
from Gs and a suffix from Gm. However, we do
not attempt to estimate the probability distribu-
tions G; instead, G is integrated out. The joint
probability of stems is calculated by integrating
out Gs:
p(s1, s2, . . . , sM )
=
?
p(Gs)
L
?
i=1
p(si|Gs)dGs
(5)
where L denotes the number of stem tokens. The
joint probability distribution of stems can be tack-
led as a Chinese restaurant process. The Chi-
nese restaurant process introduces dependencies
between stems. Hence, the joint probability of
656
stems S = {s1, . . . , sL} becomes:
p(s1, s2, . . . , sL)
= p(s1)p(s2|s1) . . . p(sM |s1, . . . , sM?1)
= ?(?s)
?(L+ ?s)
?K?1s
K
?
i=1
Ps(si)
K
?
i=1
(nsi ? 1)!
(6)
where K denotes the number of stem types. In
the equation, the second and the third factor corre-
spond to the case where novel stems are generated
for the first time; the last factor corresponds to the
case in which stems that have already been gener-
ated for nsi times previously are being generated
again. The first factor consists of all denominators
from both cases.
The integration process is applied for proba-
bility distributions Gm for suffixes analogously.
Hence, the joint probability of suffixes M =
{m1, . . . ,mN} becomes:
p(m1,m2, . . . ,mN )
= p(m1)p(m2|m1) . . . p(mN |m1, . . . ,mN?1)
= ?(?)
?(N + ?)
?T
T
?
i=1
Pm(mi)
T
?
i=1
(nmi ? 1)!
(7)
where T denotes the number of suffix types and
nmi is the number of stem types mi which have
been already generated.
Following the joint probability distribution of
stems, the conditional probability of a stem given
previously generated stems can be derived as:
p(si|S?si , ?s, Ps)
=
?
?
?
nS?sisi
L?1+?s if si ? S
?si
?s?Ps(si)
L?1+?s otherwise
(8)
where nS?sisi denotes the number of stem in-
stances si that have been previously generated,
where S?si denotes the stem set excluding the
new instance of the stem si.
The conditional probability of a suffix given the
other suffixes that have been previously generated
is defined similarly:
p(mi|M?mi , ?m, Pm)
=
?
?
?
nM?mimi
N?1+?m if mi ?M
?mi
?m?Pm(mi)
N?1+?m otherwise
(9)
where nM
?i
k
mi is the number of instances mi that
have been generated previously where M?mi is
plugg+ed skew+ed
exclaim+ed
borrow+s borrow+ed
liken+s liken+ed
consist+s
consist+ed
Figure 4: A portion of a sample tree.
the set of suffixes, excluding the new instance of
the suffix mi.
A portion of a tree is given in Figure 4. As
can be seen on the figure, all words are lo-
cated at leaf nodes. Therefore, the root node
of this subtree consists of words {plugg+ed,
skew+ed, exclaim+ed, borrow+s, borrow+ed,
liken+s, liken+ed, consist+s, consist+ed}.
4.2 Inference
The initial tree is constructed by randomly choos-
ing a word from the corpus and adding this into a
randomly chosen position in the tree. When con-
structing the initial tree, latent variables are also
assigned randomly, i.e. each word is split at a ran-
dom position (see Algorithm 1).
We use Metropolis Hastings algorithm (Hast-
ings, 1970), an instance of Markov Chain Monte
Carlo (MCMC) algorithms, to infer the optimal
hierarchical structure along with the morphologi-
cal segmentation of words (given in Algorithm 2).
During each iteration i, a leaf node Di = {wi =
si +mi} is drawn from the current tree structure.
The drawn leaf node is removed from the tree.
Next, a node Dk is drawn uniformly from the tree
657
Algorithm 1 Creating initial tree.
1: input: data D = {w1 = s1 + m1, . . . , wn =
sn +mn},
2: initialise: root? D1 where
D1 = {w1 = s1 +m1}
3: initialise: c? n? 1
4: while c >= 1 do
5: Draw a word wj from the corpus.
6: Split the word randomly such that wj =
sj +mj
7: Create a new node Dj where Dj =
{wj = sj +mj}
8: Choose a sibling node Dk for Dj
9: Merge Dnew ? Dj ?Dk
10: Remove wj from the corpus
11: c? c? 1
12: end while
13: output: Initial tree
to make it a sibling node to Di. In addition to a
sibling node, a split point wi = s
?
i + m
?
i is drawn
uniformly. Next, the node Di = {wi = s
?
i + m
?
i}
is inserted as a sibling node to Dk. After updating
all probabilities along the path to the root, the new
tree structure is either accepted or rejected by ap-
plying the Metropolis-Hastings update rule. The
likelihood of data under the given tree structure is
used as the sampling probability.
We use a simulated annealing schedule to up-
date PAcc:
PAcc =
(
pnext(D|T )
pcur(D|T )
)
1
?
(10)
where ? denotes the current temperature,
pnext(D|T ) denotes the marginal likelihood
of the data under the new tree structure, and
pcur(D|T ) denotes the marginal likelihood of
data under the latest accepted tree structure. If
(pnext(D|T ) > pcur(D|T )) then the update is
accepted (see line 9, Algorithm 2), otherwise, the
tree structure is still accepted with a probability
of pAcc (see line 14, Algorithm 2). In our
experiments (see section 5) we set ? to 2. The
system temperature is reduced in each iteration
of the Metropolis Hastings algorithm:
? ? ? ? ? (11)
Most tree structures are accepted in the earlier
stages of the algorithm, however, as the tempera-
Algorithm 2 Inference algorithm
1: input: data D = {w1 = s1 + m1, . . . , wn =
sn + mn}, initial tree T , initial temperature
of the system ?, the target temperature of the
system ?, temperature decrement ?
2: initialise: i ? 1, w ? wi = si + mi,
pcur(D|T )? p(D|T )
3: while ? > ? do
4: Remove the leaf node Di that has the
word wi = si +mi
5: Draw a split point for the word such that
wi = s
?
i +m
?
i
6: Draw a sibling node Dj
7: Dm ? Di ?Dj
8: Update pnext(D|T )
9: if pnext(D|T ) >= pcur(D|T ) then
10: Accept the new tree structure
11: pcur(D|T ) ? pnext(D|T )
12: else
13: random ? Normal(0, 1)
14: if random <
(
pnext(D|T )
pcur(D|T )
)
1
? then
15: Accept the new tree structure
16: pcur(D|T ) ? pnext(D|T )
17: else
18: Reject the new tree structure
19: Re-insert the node Di at its pre-
vious position with the previous
split point
20: end if
21: end if
22: w ? wi+1 = si+1 +mi+1
23: ? ? ? ? ?
24: end while
25: output: A tree structure where each node
corresponds to a paradigm.
ture decreases only tree structures that lead lead to
a considerable improvement in the marginal prob-
ability p(D|T ) are accepted.
An illustration of sampling a new tree structure
is given in Figure 5 and 6. Figure 5 shows that
D0 will be removed from the tree in order to sam-
ple a new position on the tree, along with a new
split point of the word. Once the leaf node is re-
moved from the tree, the parent node is removed
from the tree, as the parent node D5 will consist
of only one child. Figure 6 shows that D8 is sam-
pled to be the sibling node of D0. Subsequently,
the two nodes are merged within a new cluster that
658
D5
D1
D6
D2 D3 D4D0
D7
D8
Figure 5: D0 will be removed from the tree.
D9
D1
D6
D2 D3 D4 D0
D7
D8
Figure 6: D8 is sampled to be the sibling of D0.
introduces a new node D9.
4.3 Morphological Segmentation
Once the optimal tree structure is inferred, along
with the morphological segmentation of words,
any novel word can be analysed. For the segmen-
tation of novel words, the root node is used as it
contains all stems and suffixes which are already
extracted from the training data. Morphological
segmentation is performed in two ways: segmen-
tation at a single point and segmentation at multi-
ple points.
4.3.1 Single Split Point
In order to find single split point for the mor-
phological segmentation of a word, the split point
yielding the maximum probability given inferred
stems and suffixes is chosen to be the final analy-
sis of the word:
argmax
j
p(wi = sj +mj |Droot, ?m, Pm, ?s, Ps)
(12)
where Droot refers to the root of the entire tree.
Here, the probability of a segmentation of a
given word given Droot is calculated as given be-
low:
p(wi = sj +mj |Droot, ?m, Pm, ?s, Ps) =
p(sj |Sroot, ?s, Ps) p(mj |Mroot, ?m, Pm)
(13)
where Sroot denotes all the stems in Droot and
Mroot denotes all the suffixes in Droot. Here
p(sj |Sroot, ?s, Ps) is calculated as given below:
p(si|Sroot, ?s, Ps) =
?
?
?
nSrootsi
L+?s if si ? Sroot
?s?Ps(si)
L+?s otherwise
(14)
Similarly, p(mj |Mroot, ?m, Pm) is calculated
as:
p(mi|Mroot, ?m, Pm) =
?
?
?
nMrootmi
N+?m if mi ?Mroot
?m?Pm(mi)
N+?m otherwise
(15)
4.3.2 Multiple Split Points
In order to discover words with multiple split
points, we propose a hierarchical segmentation
where each segment is split further. The rules for
generating multiple split points is given by the fol-
lowing context free grammar:
w ? s1 m1|s2 m2 (16)
s1 ? s m|s s (17)
s2 ? s (18)
m1 ? m m (19)
m2 ? s m|m m (20)
Here, s is a pre-terminal node that generates all
the stems from the root node. And similarly, m is
a pre-terminal node that generates all the suffixes
from the root node. First, using Equation 16, the
word (e.g. housekeeper) is split into s1 m1 (e.g.
housekeep+er) or s2 m2 (house+keeper). The first
segment is regarded as a stem, and the second
segment is either a stem or a suffix, consider-
ing the probability of having a compound word.
Equation 12 is used to decide whether the sec-
ond segment is a stem or a suffix. At the sec-
ond segmentation level, each segment is split once
more. If the first production rule is followed in
the first segmentation level, the first segment s1
can be analysed as s m (e.g. housekeep+?) or s s
659
!"#$%&%%'%(
!"#$% &%%'%(
!"#$% ) &%%' %(
Figure 7: An example that depicts how the word
housekeeper can be analysed further to find more split
points.
(e.g. house+keep) (Equation 17). The decision
to choose which production rule to apply is made
using:
s1 ?
{
s s if p(s|S, ?s, Ps) > p(m|M,?m, Pm)
s m otherwise
(21)
where S and M denote all the stems and suffixes
in the root node.
Following the same production rule, the second
segment m1 can only be analysed as m m (er+?).
We postulate that words cannot have more than
two stems and suffixes always follow stems. We
do not allow any prefixes, circumfixes, or infixes.
Therefore, the first production rule can output two
different analyses: s m m m and s s m m (e.g.
housekeep+er and house+keep+er).
On the other hand, if the word is analysed as
s2 m2 (e.g. house+keeper), then s2 cannot be
analysed further. (e.g. house). The second seg-
ment m2 can be analysed further, such that s m
(stem+suffix) (e.g. keep+er, keeper+?) or m m
(suffix+suffix). The decision to choose which pro-
duction rule to apply is made as follows:
m2 ?
{
s m if p(s|S, ?s, Ps) > p(m|M,?m, Pm)
mm otherwise
(22)
Thus, the second production rule yields two
different analyses: s s m and s m m (e.g.
house+keep+er or house+keeper).
5 Experiments & Results
Two sets of experiments were performed for the
evaluation of the model. In the first set of exper-
iments, each word is split at single point giving a
single stem and a single suffix. In the second set
of experiments, potentially multiple split points

	

  
   

 
 
 
  
     





































