T2: Structured Sparsity in Natural Language 
Processing: Models, Algorithms and 
Applications 
Andr? F. T. Martins, M?rio A. T. Figueiredo, and Noah A. Smith 
ABSTRACT
This tutorial will cover recent advances in sparse modeling with diverse applications in 
natural language processing (NLP). A sparse model is one that uses a relatively small 
number of features to map an input to an output, such as a label sequence or parse 
tree. The advantages of sparsity are, among others, compactness and interpretability; in 
fact, sparsity is currently a major theme in statistics, machine learning, and signal 
processing. The goal of sparsity can be seen in terms of earlier goals of feature 
selection and therefore model selection (Della Pietra et al, 1997; Guyon and Elisseeff, 
2003; McCallum, 2003). 
This tutorial will focus on methods which embed sparse model selection into the 
parameter estimation problem. In such methods, learning is carried out by minimizing a 
regularized empirical risk functional composed of two terms: a "loss term," which 
controls the goodness of fit to the data (e.g., log loss or hinge loss), and a "regularizer 
term," which is designed to promote sparsity. The simplest example is L1-norm 
regularization (Tibshirani, 2006), which penalizes weight components individually, and 
has been explored in various NLP applications (Kazama and Tsujii, 2003; Goodman, 
2004; Gao, 2007). More sophisticated regularizers, those that use mixed norms and 
groups of weights, are able to promote "structured" sparsity: i.e., they promote sparsity 
patterns that are compatible with a priori knowledge about the structure of the feature 
space. These kind of regularizers have been proposed in the statistical and signal 
processing literature (Yuan and Lin, 2006; Zhao et al, 2009; Kim et al, 2010; Bach et 
al., 2011) and are a recent topic of research in NLP (Eisenstein et al, 2011; Martins et 
al, 2011, Das and Smith, 2012). Sparsity-inducing regularizers require the use of 
specialized optimization routines for learning (Wright et al, 2009; Xiao, 2009; Langford 
et al, 2009). 
The tutorial will consist of three parts: (1) how to formulate the problem, i.e., how to 
choose the right regularizer for the kind of sparsity pattern intended; (2) how to solve the 
optimization problem efficiently; and (3) examples of the use of sparsity within natural 
language processing problems. 
OUTLINE
1. Introduction 
(30 minutes) 
o What is sparsity? 
o Why sparsity is often desirable in NLP 
o Feature selection: wrappers, filters, and embedded methods 
o What has been done in other areas: the Lasso and group-Lasso, 
compressive sensing, and recovery guarantees 
o Theoretical and practical limitations of previous methods to typical NLP 
problems 
o Beyond cardinality: structured sparsity 
2. Group-Lasso and Mixed-Norm Regularizers 
(45 minutes) 
o Selecting columns in a grid-shaped feature space 
o Examples: multiple classes, multi-task learning, multiple kernel learning 
o Mixed L2/L1 and Linf/L1 norms: the group Lasso 
o Non-overlapping groups 
o Example: feature template selection 
o Tree-structured groups 
o The general case: a DAG 
o Coarse-to-fine regularization 
3. Coffee Break 
(15 minutes) 
4. Optimization Algorithms 
(45 minutes) 
o Non-smooth optimization: limitations of subgradient algorithms 
o Quasi-Newton methods: OWL-QN 
o Proximal gradient algorithms: iterative soft-thresholding, forward-backward 
and other splittings 
o Computing proximal steps 
o Other algorithms: FISTA, Sparsa, ADMM, Bregman iterations 
o Convergence rates 
o Online algorithms: limitations of stochastic subgradient descent 
o Online proximal gradient algorithms 
o Managing general overlapping groups 
o Memory footprint, time/space complexity, etc. 
o The "Sparseptron" algorithm and debiasing 
5. Applications 
(30 minutes): 
o Sociolinguistic association discovery 
o Sequence problems: named entity recognition, chunking 
o Multilingual dependency parsing 
o Lexicon expansion 
6. Closing Remarks and Discussion 
(15 minutes) 
BIOS
Andr? F. T. Martins
Instituto de Telecomunica??es, Instituto Superior T?cnico 
Av. Rovisco Pais, 1, 1049-001 Lisboa, Portugal, 
and Priberam Inform?tica 
Al. Afonso Henriques, 41 - 2., 1000-123 Lisboa, Portugal 
afm--AT--cs.cmu.edu 
A. Martins is a final year Ph.D. student in Carnegie Mellon's School of Computer 
Science and the Instituto de Telecomunica??es at Instituto Superior T?cnico, where he 
is working on a degree in Language Technologies. Martins' research interests include 
natural language processing, machine learning, convex optimization, and sparse 
modeling. His dissertation focuses on new models and algorithms for structured 
prediction with non-local features. His paper "Concise Integer Linear Programming 
Formulations for Dependency Parsing" received a best paper award at ACL 2009. 
M?rio A. T. Figueiredo
Instituto de Telecomunica??es, Instituto Superior T?cnico 
Av. Rovisco Pais, 1, 1049-001 Lisboa, Portugal 
mario.figueiredo--AT--lx.it.pt 
M. Figueiredo is a professor of electrical and computer engineering at Instituto Superior 
T?cnico (the engineering school of the Technical University of Lisbon) and his main 
research interests include machine learning, statistical signal processing, and 
optimization. He recently guest co-edited a special issue of the IEEE Journal on Special 
Topics in Signal Processing devoted to compressive sensing (one of the central areas 
of research on sparsity) and gave several invited talks (and a tutorial at ICASSP 2012) 
on optimization algorithms for problems involving sparsity. 
Noah A. Smith
School of Computer Science, Carnegie Mellon University 
5000 Forbes Avenue, Pittsburgh, PA 15213, USA 
nasmith--AT--cs.cmu.edu 
N. Smith is Finmeccanica associate professor in language technologies and machine 
learning at CMU. His research interests include statistical natural language processing, 
especially unsupervised methods, machine learning for structured data, and 
applications of natural language processing, including machine translation and 
statistical modeling of text and quantitative social data. He recently published a book, 
Linguistic Structure Prediction, about many of these topics; in 2009 he gave a tutorial at 
ICML about structured prediction in NLP. 
ACKNOWLEDGMENTS
This tutorial was enabled by support from the following organizations: 
? National Science Foundation (USA), CAREER grant IIS-1054319. 
? Funda??o para a Ci?ncia e Tecnologia (Portugal), grant PEst-
OE/EEI/LA0008/2011. 
? Funda??o para a Ci?ncia e Tecnologia and Information and Communication 
Technologies Institute (Portugal/USA), through the CMU-Portugal Program. 
? QREN/POR Lisboa (Portugal), EU/FEDER programme, Discooperio project, 
contract 2011/18501. 
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 209?217,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
An Exact Dual Decomposition Algorithm
for Shallow Semantic Parsing with Constraints
Dipanjan Das? Andre? F. T. Martins?? Noah A. Smith?
?Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{dipanjan,afm,nasmith}@cs.cmu.edu
Abstract
We present a novel technique for jointly predict-
ing semantic arguments for lexical predicates. The
task is to find the best matching between seman-
tic roles and sentential spans, subject to struc-
tural constraints that come from expert linguistic
knowledge (e.g., in the FrameNet lexicon). We
formulate this task as an integer linear program
(ILP); instead of using an off-the-shelf tool to
solve the ILP, we employ a dual decomposition
algorithm, which we adapt for exact decoding via
a branch-and-bound technique. Compared to a
baseline that makes local predictions, we achieve
better argument identification scores and avoid all
structural violations. Runtime is nine times faster
than a proprietary ILP solver.
1 Introduction
Semantic knowledge is often represented declara-
tively in resources created by linguistic experts. In
this work, we strive to exploit such knowledge in
a principled, unified, and intuitive way. An ex-
ample resource where a wide variety of knowledge
has been encoded over a long period of time is the
FrameNet lexicon (Fillmore et al, 2003),1 which
suggests an analysis based on frame semantics (Fill-
more, 1982). This resource defines hundreds of
semantic frames. Each frame represents a gestalt
event or scenario, and is associated with several se-
mantic roles, which serve as participants in the event
that the frame signifies (see Figure 1 for an exam-
ple). Along with storing the above data, FrameNet
also provides a hierarchy of relationships between
frames, and semantic relationships between pairs of
roles. In prior NLP research using FrameNet, these
interactions have been largely ignored, though they
1http://framenet.icsi.berkeley.edu
have the potential to improve the quality and consis-
tency of semantic analysis.
In this paper, we present an algorithm that finds
the full collection of arguments of a predicate given
its semantic frame. Although we work within the
conventions of FrameNet, our approach is general-
izable to other semantic role labeling (SRL) frame-
works. We model this argument identification task
as constrained optimization, where the constraints
come from expert knowledge encoded in a lexi-
con. Following prior work on PropBank-style SRL
(Kingsbury and Palmer, 2002) that dealt with simi-
lar constrained problems (Punyakanok et al, 2004;
Punyakanok et al, 2008, inter alia), we incorporate
this declarative knowledge in an integer linear pro-
gram (ILP).
Because general-purpose ILP solvers are propri-
etary and do not fully exploit the structure of the
problem, we turn to a class of optimization tech-
niques called dual decomposition (Komodakis et
al., 2007; Rush et al, 2010; Martins et al, 2011a).
We derive a modular, extensible, parallelizable ap-
proach in which semantic constraints map not just
to declarative components of the algorithm, but also
to procedural ones, in the form of ?workers.? While
dual decomposition algorithms only solve a relax-
ation of the original problem, we make a novel con-
tribution by wrapping the algorithm in a branch-and-
bound search procedure, resulting in exact solutions.
We experimentally find that our algorithm
achieves accuracy comparable to a state-of-the-art
system, while respecting all imposed linguistic con-
straints. In comparison to inexact beam search that
violates many of these constraints, our exact decoder
has less than twice the runtime; furthermore, it de-
codes nine times faster than CPLEX, a state-of-the-
art, proprietary, general-purpose exact ILP solver.
209
Austria , once expected to waltz smoothly into the European Union , is elbowing its partners , 
treading on toes and pogo-dancing in a most un-Viennese manner .
SELF_MOTION COLLABORATION
CONDUCTGoalManner Partner_1 Partner_2MannerAgentSelf_mover
Figure 1: An example sentence from the annotations released as part of FrameNet 1.5 with three predicates marked in
bold. Each predicate has its evoked semantic frame marked above it, in a distinct color. For each frame, its semantic
roles are shown in the same color, and the spans fulfilling the roles are underlined. For example, manner evokes the
CONDUCT frame, and has the Agent and Manner roles fulfilled by Austria and most un-Viennese respectively.
2 Collective Argument Identification
Here, we take a declarative approach to modeling
argument identification using an ILP and relate our
formulation to prior work in shallow semantic pars-
ing. We show how knowledge specified in a lin-
guistic resource can be used to derive the constraints
used in our ILP. Finally, we draw connections of our
specification to graphical models, a popular formal-
ism in AI, and describe how the constraints can be
treated as factors in a factor graph.
2.1 Declarative Specification
Let us denote a predicate by t and the semantic
frame it evokes within a sentence x by f . In this
work, we assume that the semantic frame f is given,
which is traditionally the case in controlled exper-
iments used to evaluate SRL systems (Ma`rquez et
al., 2008). Given the semantic frame of a predicate,
the semantic roles that might be filled are assumed
to be given by the lexicon (as in PropBank and
FrameNet). Let the set of roles associated with the
frame f be Rf . In sentence x, the set of candidate
spans of words that might fill each role is enumer-
ated, usually following an overgenerating heuristic;2
let this set of spans be St. We include the null span ?
in St; connecting it to a role r ? Rf denotes that the
role is not overt. Our approach assumes a scoring
function that gives a strength of association between
roles and candidate spans. For each role r ? Rf and
span s ? St, this score is parameterized as:
c(r, s) = ? ? h(t, f,x, r, s), (1)
where ? are model weights and h is a feature func-
tion that looks at the predicate t, the evoked frame
f , sentence x, and its syntactic analysis, along with
2Here, as in most SRL literature, role fillers are assumed to be
expressed as contiguous spans, though such an assumption is
easy to relax in our framework.
r and s. The SRL literature provides many feature
functions of this form and many ways to use ma-
chine learning to acquire ?. Our presented method
does not make any assumptions about the score ex-
cept that it has the form in Eq. 1.
We define a vector z of binary variables zr,s ?
{0, 1} for every role and span pair. We have that:
z ? {0, 1}d, where d = |Rf | ? |St|. zr,s = 1 means
that role r is filled by span s. Given the binary z vec-
tor, it is straightforward to recover the collection of
arguments by checking which components zr,s have
an assignment of 1; we use this strategy to find argu-
ments, as described in ?4.2 (strategies 4 and 6). The
joint argument identification task can be represented
as a constrained optimization problem:
maximize
?
r?Rf
?
s?St c(r, s)? zr,s
with respect to z ? {0, 1}d
such that Az ? b. (2)
The last line imposes constraints on the mapping be-
tween roles and spans; these are motivated on lin-
guistic grounds and are described next.3
Uniqueness: Each role r is filled by at most one
span in St. This constraint can be expressed by:
?r ? Rf ,
?
s?St zr,s = 1. (3)
There are O(|Rf |) such constraints. Note that since
St contains the null span ?, non-overt roles are also
captured using the above constraints. Such a con-
straint is used extensively in prior literature (Pun-
yakanok et al, 2008, ?3.4.1).
Overlap: SRL systems commonly constrain roles
to be filled by non-overlapping spans. For example,
Toutanova et al (2005) used dynamic programming
over a phrase structure tree to prevent overlaps be-
tween arguments, and Punyakanok et al (2008) used
3Note that equality constraints a ?z = b can be transformed into
double-side inequalities a ? z ? b and ?a ? z ? ?b.
210
constraints in an ILP to respect this requirement. In-
spired by the latter, we require that each input sen-
tence position of x be covered by at most one argu-
ment. For each role r ? Rf , we define:
Gr(i) = {s | s ? St, s covers position i in x}. (4)
We can define our overlap constraints in terms of Gr
as follows, for every sentence position i:
?i ? {1, . . . , |x|},
?
r?Rf
?
s?Gr(i) zr,s ? 1, (5)
This gives us O(|x|) constraints.
Pairwise ?Exclusions?: For many predicate
classes, there are pairs of roles forbidden to appear
together in the analysis of a single predicate token.
Consider the following two sentences:
A blackberry
Entity 1
resembles a loganberry
Entity 2
. (6)
Most berries
Entities
resemble each other. (7)
Consider the uninflected predicate resemble in
both sentences, evoking the same meaning. In exam-
ple 6, two roles, which we call Entity 1 and Entity 2
describe two entities that are similar to each other.
In the second sentence, a phrase fulfills a third role,
called Entities, that collectively denotes some ob-
jects that are similar. It is clear that the roles Entity 1
and Entities cannot be overt for the same predicate
at once, because the latter already captures the func-
tion of the former; a similar argument holds for the
Entity 2 and Entities roles. We call this phenomenon
the ?excludes? relationship. Let us define a set of
pairs fromRf that have this relationship:
Exclf = {(ri, rj) | ri and rj exclude each other}
Using the above set, we define the constraint:
?(ri, rj) ? Exclf , zri,? + zrj ,? ? 1 (8)
In English: if both roles are overt in a parse, this
constraint will be violated, and we will not respect
the ?excludes? relationship between the pair. If nei-
ther or only one of the roles is overt, the constraint
is satisfied. The total number of such constraints is
O(|Exclf |), which is the number of pairwise ?ex-
cludes? relationships of a given frame.
Pairwise ?Requirements?: The sentence in exam-
ple 6 illustrates another kind of constraint. The pred-
icate resemble cannot have only one of Entity 1 and
Entity 2 as roles in text. For example,
* A blackberry
Entity 1
resembles. (9)
Enforcing the overtness of two roles sharing this
?requires? relationship is straightforward. We define
the following set for a frame f :
Reqf = {(ri, rj) | ri and rj require each other}
This leads to constraints of the form
?(ri, rj) ? Reqf , zri,? ? zrj ,? = 0 (10)
If one role is overt (or absent), so must the other
be. A related constraint has been used previously
in the SRL literature, enforcing joint overtness re-
lationships between core arguments and referential
arguments (Punyakanok et al, 2008, ?3.4.1), which
are formally similar to the example above.4
Integer Linear Program and Relaxation: Plug-
ging the constraints in Eqs. 3, 5, 8 and 10 into the
last line of Eq. 2, we have the argument identifica-
tion problem expressed as an ILP, since the indica-
tor variables z are binary. In this paper, apart from
the ILP formulation, we will consider the follow-
ing relaxation of Eq. 2, which replaces the binary
constraint z ? {0, 1}d by a unit interval constraint
z ? [0, 1]d, yielding a linear program:
maximize
?
r?Rf
?
s?St c(r, s)? zr,s
with respect to z ? [0, 1]d
such that Az ? b. (11)
There are several LP and ILP solvers available,
and a great deal of effort has been spent by the
optimization community to devise efficient generic
solvers. An example is CPLEX, a state-of-the-art
solver for mixed integer programming that we em-
ploy as a baseline to solve the ILP in Eq. 2 as well
as its LP relaxation in Eq. 11. Like many of the best
implementations, CPLEX is proprietary.
4 We noticed in the annotated data, in some cases, the ?requires?
constraint is violated by the FrameNet annotators. This hap-
pens mostly when one of the required roles is absent in the
sentence containing the predicate, but is rather instantiated in
an earlier sentence; see Gerber and Chai (2010). We apply the
hard constraint in Eq. 10, though extending our algorithm to
seek arguments outside the sentence is straightforward (Chen
et al, 2010).
211
2.2 Linguistic Constraints from FrameNet
Although enforcing the four different sets of con-
straints above is intuitive from a general linguistic
perspective, we ground their use in definitive lin-
guistic information present in the FrameNet lexicon
(Fillmore et al, 2003). FrameNet, along with lists
of semantic frames, associated semantic roles, and
predicates that could evoke the frames, gives us a
small number of annotated sentences with frame-
semantic analysis. From the annotated data, we
gathered that only 3.6% of the time is a role instanti-
ated multiple times by different spans in a sentence.
This justifies the uniqueness constraint enforced by
Eq. 3. Use of such a constraint is also consistent
with prior work in frame-semantic parsing (Johans-
son and Nugues, 2007; Das et al, 2010a). Similarly,
we found that in the annotations, no arguments over-
lapped with each other for a given predicate. Hence,
the overlap constraints in Eq. 5 are also justified.
Our third and fourth sets of constraints, presented
in Eqs. 8 and 10, come from FrameNet, too; more-
over, they are explicitly mentioned in the lexicon.
Examples 6?7 are instances where the predicate re-
semble evokes the SIMILARITY frame, which is de-
fined in FrameNet as: ?Two or more distinct en-
tities, which may be concrete or abstract objects
or types, are characterized as being similar to each
other. Depending on figure/ground relations, the
entities may be expressed in two distinct frame el-
ements and constituents, Entity 1 and Entity 2, or
jointly as a single frame element and constituent,
Entities.?
For this frame, the lexicon lists several roles other
than the three roles we have already observed, such
as Dimension (the dimension along which the enti-
ties are similar), Differentiating fact (a fact that re-
veals how the concerned entities are similar or dif-
ferent), and so forth. Along with the roles, FrameNet
also declares the ?excludes? and ?requires? relation-
ships noted in our discussion in Section 2.1. The
case of the SIMILARITY frame is not unique; in Fig. 1,
the frame COLLABORATION, evoked by the predicate
partners, also has two roles Partner 1 and Partner 2
that share the ?requires? relationship. In fact, out
of 877 frames in FrameNet 1.5, the lexicon?s latest
edition, 204 frames have at least a pair of roles that
share the ?excludes? relationship, and 54 list at least
a pair of roles that share the ?requires? relationship.
2.3 Constraints as Factors in a Graphical Model
The LP in Eq. 11 can be represented as a maxi-
mum a posteriori (MAP) inference problem in an
undirected graphical model. In the factor graph,
each component of z corresponds to a binary vari-
able, and each instantiation of a constraint in
Eqs. 3, 5, 8 and 10 corresponds to a factor. Smith
and Eisner (2008) and Martins et al (2010) used
such a representation to impose constraints in a de-
pendency parsing problem; the latter discussed the
equivalence of linear programs and factor graphs for
representing discrete optimization problems. Each
of our constraints take standard factor forms we can
describe using the terminology of Smith and Eisner
(2008) and Martins et al (2010). The uniqueness
constraint in Eq. 3 corresponds to an XOR factor,
while the overlap constraint in Eq. 5 corresponds to
an ATMOSTONE factor. The constraints in Eq. 8
enforcing the ?excludes? relationship can be repre-
sented with an OR factor. Finally, each ?requires?
constraints in Eq. 10 is equivalent to an XORWITH-
OUTPUT factor.
In the following section, we describe how we ar-
rive at solutions for the LP in Eq. 11 using dual de-
composition, and how we adapt it to efficiently re-
cover the exact solution of the ILP (Eq. 2), without
the need of an off-the-shelf ILP solver.
3 ?Augmented? Dual Decomposition
Dual decomposition methods address complex op-
timization problems in the dual, by dividing them
into simple worker problems, which are repeat-
edly solved until a consensus is reached. The
most simple technique relies on the subgradient
algorithm (Komodakis et al, 2007; Rush et al,
2010); as an alternative, an augmented Lagrangian
technique was proposed by Martins et al (2011a,
2011b), which is more suitable when there are many
small components?commonly the case in declara-
tive constrained problems, such as the one at hand.
Here, we present a brief overview of the latter, which
is called Dual Decomposition with the Alternating
Direction Method of Multipliers (AD3).
Let us start by establishing some notation. Let
m ? {1, . . . ,M} index a factor, and denote by i(m)
212
the vector of indices of variables linked to that fac-
tor. (Recall that each factor represents the instantia-
tion of a constraint.) We introduce a new set of vari-
ables, u ? Rd, called the ?witness? vector. We split
the vector z into M overlapping pieces z1, . . . , zM ,
where each zm ? [0, 1]|i(m)|, and add M constraints
zm = ui(m) to impose that all the pieces must agree
with the witness (and therefore with each other).
Each of the M constraints described in ?2 can be
encoded with its own matrix Am and vector bm
(which jointly define A and b in Eq. 11). For conve-
nience, we denote by c ? Rd the score vector, whose
components are c(r, s), for each r ? Rf and s ? St
(Eq. 1), and define the following scores for the mth
subproblem:
cm(r, s) = ?(r, s)
?1c(r, s), ?(r, s) ? i(m), (12)
where ?(r, s) is the number of constraints that in-
volve role r and span s. Note that according to this
definition, c ? z =
?M
m=1 cm ? zm. We can rewrite
the LP in Eq. 11 in the following equivalent form:
maximize
M?
m=1
cm ? zm
with respect to u ? Rd, zm ? [0, 1]i(m), ?m
such that Amzm ? bm, ?m
zm = ui(m), ?m. (13)
We next augment the objective with a quadratic
penalty term ?2
?M
m=1 ?zm?ui(m)?
2 (for some ? >
0). This does not affect the solution of the problem,
since the equality constraints in the last line force
this penalty to vanish. However, as we will see, this
penalty will influence the workers and will lead to
faster consensus. Next, we introduce Lagrange mul-
tipliers ?m for those equality constraints, so that the
augmented Lagrangian function becomes:
L?(z,u,?) =
M?
m=1
(cm + ?m) ? zm ? ?m ? ui(m)
?
?
2
?zm ? ui(m)?
2. (14)
The AD3 algorithm seeks a saddle point of L? by
performing alternating maximization with respect to
z and u, followed by a gradient update of ?. The re-
sult is shown as Algorithm 1. Like dual decomposi-
tion approaches, it repeatedly performs a broadcast
operation (the zm-updates, which can be done in pa-
Algorithm 1 AD3 for Argument Identification
1: input:
? role-span matching scores c := ?c(r, s)?r,s,
? structural constraints ?Am,bm?Mm=1,
? penalty ? > 0
2: initialize u uniformly (i.e., u(r, s) = 0.5, ?r, s)
3: initialize each ?m = 0, ?m ? {1, . . . ,M}
4: initialize t? 1
5: repeat
6: for each m = 1, . . . ,M do
7: make a zm-update by finding the best scoring
analysis for the mth constraint, with penalties
for deviating from the consensus u:
zt+1m ? argmax
Amzm?bm
(cm+?m)?zm?
?
2
?zm?ui(m)?
2
8: end for
9: make a u-update by updating the consensus solu-
tion, averaging z1, . . . , zm:
ut+1(r, s)?
1
?(r, s)
?
m:(r,s)?i(m)
zt+1m (r, s)
10: make a ?-update:
?t+1m ? ?
t
m ? ?(z
(t+1)
m ? u
(t+1)
i(m) ), ?m
11: t? t+ 1
12: until convergence.
13: output: relaxed primal solution u? and dual solution
??. If u? is integer, it will encode an assignment of
spans to roles. Otherwise, it will provide an upper
bound of the true optimum.
-rallel, one constraint per ?worker?) and a gather op-
eration (the u- and ?-updates). Each u-operation
can be seen as an averaged voting which takes into
consideration each worker?s results.
Like in the subgradient method, the?-updates can
be regarded as price adjustments, which will affect
the next round of zm-updates. The only difference
with respect to the subgradient method (Rush et al,
2010) is that each subproblem involved in a zm-
update also has a quadratic penalty that penalizes de-
viations from the previous average voting; it is this
term that accelerates consensus and therefore con-
vergence. Martins et al (2011b) also provide stop-
ping criteria for the iterative updates using primal
and dual residuals that measure convergence; we re-
fer the reader to that paper for details.
A key attraction of this algorithm is all the com-
ponents of the declarative specification remain intact
213
in the procedural form. Each worker corresponds
exactly to one constraint in the ILP, which corre-
sponds to one linguistic constraint. There is no need
to work out when, during the procedure, each con-
straint might have an effect, as in beam search.
Solving the subproblems. In a different appli-
cation, Martins et al (2011b, ?4) showed how
to solve each zm-subproblem associated with the
XOR, XORWITHOUTPUT and OR factors in runtime
O(|i(m)| log |i(m)|). The only subproblem that re-
mains is that of the ATMOSTONE factor, to which
we now turn. The problem can be transformed into
that of projecting a point (a1, . . . , ak) onto the set
Sm =
{
zm ? [0, 1]|i(m)|
?
?
?|i(m)|
j=1 zm,j ? 1
}
.
This projection can be computed as follows:
1. Clip each aj into the interval [0, 1] (i.e., set
a?j = min{max{aj , 0}, 1}). If the result satisfies
?k
j=1 a
?
j ? 1, then return (a
?
1, . . . , a
?
k).
2. Otherwise project (a1, . . . , ak) onto the probabil-
ity simplex:
{
zm ? [0, 1]|i(m)|
?
?
?|i(m)|
j=1 zm,j = 1
}
.
This is precisely the XOR subproblem and can be
solved in time O(|i(m)| log |i(m)|).
Caching. As mentioned by Martins et al (2011b),
as the algorithm comes close to convergence, many
subproblems become unchanged and their solutions
can be cached. By caching the subproblems, we
managed to reduce runtime by about 60%.
Exact decoding. Finally, it is worth recalling that
AD3, like other dual decomposition algorithms,
solves a relaxation of the actual problem. Although
we have observed that the relaxation is often tight?
cf. ?4?this is not always the case. Specifically, a
fractional solution may be obtained, which is not in-
terpretable as an argument, and therefore it is de-
sirable to have a strategy to recover the exact solu-
tion. Two observations are noteworthy. First, the
optimal value of the relaxed problem (Eq. 11) pro-
vides an upper bound to the original problem (Eq. 2).
This is because Eq. 2 has the additional integer con-
straint on the variables. In particular, any feasible
dual point provides an upper bound to the original
problem?s optimal value. Second, during execution
of the AD3 algorithm, we always keep track of a se-
quence of feasible dual points. Therefore, each it-
eration constructs tighter and tighter upper bounds.
With this machinery, we have all that is necessary for
implementing a branch-and-bound search that finds
the exact solution of the ILP. The procedure works
recursively as follows:
1. Initialize L = ?? (our best value so far).
2. Run Algorithm 1. If the solution u? is integer, re-
turn u? and set L to the objective value. If along
the execution we obtain an upper bound less than
L, then Algorithm 1 can be safely stopped and
return ?infeasible??this is the bound part. Oth-
erwise (if u? is fractional) go to step 3.
3. Find the ?most fractional? component of u? (call
it u?j ) and branch: constrain uj = 0 and go to
step 2, eventually obtaining an integer solution u?0
or infeasibility; and then constrain uj = 1 and do
the same, obtaining u?1. Return the u
? ? {u?0,u
?
1}
that yields the largest objective value.
Although this procedure may have worst-case expo-
nential runtime, we found it empirically to rapidly
obtain the exact solution in all test cases.
4 Experiments and Results
4.1 Dataset, Preprocessing, and Learning
In our experiments, we use FrameNet 1.5, which
contains a lexicon of 877 frames and 1,068 role
labels, and 78 documents with multiple predicate-
argument annotations (a superset of the SemEval
shared task dataset; Baker et al, 2007). We used the
same split as Das and Smith (2011), with 55 doc-
uments for training (containing 19,582 frame anno-
tations) and 23 for testing (with 4,458 annotations).
We randomly selected 4,462 predicates in the train-
ing set as development data. The raw sentences in all
the training and test documents were preprocessed
using MXPOST (Ratnaparkhi, 1996) and the MST
dependency parser (McDonald et al, 2005).
The state-of-the-art system for this task is SE-
MAFOR, an open source tool (Das et al, 2010a)5
that provides a baseline benchmark for our new al-
gorithm. We use the components of SEMAFOR
as-is to define the features h and train the weights
? used in the scoring function c. We also use its
5http://www.ark.cs.cmu.edu/SEMAFOR
214
heuristic mechanism to find potential spans St for a
given predicate t. SEMAFOR learns weights using
`2-penalized log-likelihood; we augmented its dev
set-tuning procedure to tune both the regularization
strength and the AD3 penalty strength ?. We ini-
tialize ? = 0.1 and follow Martins et al (2011b)
in dynamically adjusting it. Note that we do not use
SEMAFOR?s automatic frame identification compo-
nent in our presented experiments, as we assume that
we have gold frames on each predicate. This lets us
compare the different argument identification meth-
ods in a controlled fashion.
4.2 Decoding Strategies
We compare the following algorithms:
1. Local: this is a na??ve argument identification
strategy that selects the best span for each role r,
according to the score function c(r, s). It ignores
all constraints except ?uniqueness.?
2. SEMAFOR: this strategy employs greedy beam
search to eliminate overlaps between predicted ar-
guments (Das et al, 2010b, Algorithm 1). Note
that it does not try to respect the ?excludes? and
?requires? constraints between pairs of roles. The
default size of the beam in SEMAFOR was a safe
10,000; this resulted in extremely slow decoding
times. We also tried beam sizes of 100 and 2
(the latter being the smallest size that achieves the
same F1 score on the dev set as beam width 100.)
3. CPLEX, LP: this uses CPLEX to solve the re-
laxed LP in Eq. 11. To handle fractional z, for
each role r, we choose the best span s?, such that
s? = argmaxs?Sr zr,s, solving ties arbitrarily.
4. CPLEX, exact: this tackles the actual ILP (Eq. 2)
with CPLEX.
5. AD3, LP: this is the counterpart of the LP version
of CPLEX, where the relaxed problem is solved
using AD3. We choose the spans for each role in
the same way as in strategy 3.
6. AD3, exact: this couples AD3 with branch-and-
bound search to get the exact integer solution.
4.3 Results
Table 1 shows performance of the different decoding
strategies on the test set. We report precision, recall,
and F1 scores.6 Since these scores do not penal-
6We use the evaluation script from SemEval 2007 shared task,
modified to evaluate only the argument identification output.
ize structural violations, we also report the number
of overlap, ?excludes,? and ?requires? constraints
that were violated in the test set. Finally, we tab-
ulate each setting?s decoding time in seconds on the
whole test set averaged over 5 runs.7 The Local
model is very fast but suffers degradation in pre-
cision and violates one constraint roughly per nine
predicates. SEMAFOR used a default beam size of
10,000, which is extremely slow; a faster version of
beam size 100 results in the same precision and re-
call values, but is 15 times faster. Beam size 2 results
in slightly worse precision and recall values, but is
even faster. All of these, however, result in many
constraint violations. Strategies involving CPLEX
and AD3 perform similarly to each other and SE-
MAFOR on precision and recall, but eliminate most
or all of the constraint violations. SEMAFOR with
beam size 2 is 11-16 times faster than the CPLEX
strategies, but is only twice as fast than AD3, and re-
sults in significantly more structural violations. The
exact algorithms are slower than the LP versions, but
compared to CPLEX, AD3 is significantly faster and
has a narrower gap between its exact and LP ver-
sions. We found that relaxation was tight 99.8% of
the time on the test examples.
The example in Fig. 1 is taken from our test set,
and shows an instance where two roles, Partner 1
and Partner 2 share the ?requires? relationship; for
this example, the beam search decoder misses the
Partner 2 role, which is a violation, while our AD3
decoder identifies both arguments correctly. Note
that beam search makes plenty of linguistic viola-
tions, but has precision and recall values that are
marginally better than AD3. We found that beam
search, when violating many ?requires? constraints,
often finds one role in the pair, which increases its
recall. AD3 is sometimes more conservative in such
cases, predicting neither role. A second issue, as
noted in footnote 4, is that the annotations some-
times violate these constraints. Overall, we found
it interesting that imposing the constraints did not
have much effect on standard measures of accuracy.
7We used a 64-bit machine with 2 2.6GHz dual-core CPUs (i.e.,
4 processors in all) with a total of 8GB of RAM. The work-
ers in AD3 were not parallelized, while CPLEX automatically
parallelized execution.
215
Violations
Method P R F1 Overlap Requires Excludes Time in Secs.
Local 67.69 59.76 63.48 441 45 15 1.26 ? 0.01
SEMAFOR (beam = 2) 70.18 59.54 64.42 0 49 0 2.74 ? 0.10
SEMAFOR (beam = 100) 70.43 59.64 64.59 0 50 1 29.00 ? 0.25
SEMAFOR (beam = 10000) 70.43 59.64 64.59 0 50 1 440.67 ? 5.53
CPLEX, LP 70.34 59.43 64.43 0 1 0 32.67 ? 1.29
CPLEX, exact 70.31 59.45 64.43 0 0 0 43.12 ? 1.26
AD3, LP 70.30 59.45 64.42 2 2 0 4.17 ? 0.01
AD3, exact 70.31 59.45 64.43 0 0 0 4.78 ? 0.04
Table 1: Comparison of decoding strategies in ?4.2. We evaluate in terms of precision, recall and F1 score on a test
set containing 4,458 predicates. We also compute the number of structural violations each model makes: number
of overlapping arguments and violations of the ?requires? and ?excludes? constraints of ?2. Finally decoding time
(without feature computation steps) on the whole test set is shown in the last column averaged over 5 runs.
5 Related Work
Semantic role labeling: Most SRL systems use
conventions from PropBank (Kingsbury and Palmer,
2002) and NomBank (Meyers et al, 2004), which
store information about verbal and nominal pred-
icates and corresponding symbolic and meaning-
specific semantic roles. A separate line of work,
including this paper, investigates SRL systems that
use FrameNet conventions; while less popular, these
systems, pioneered by Gildea and Jurafsky (2002),
consider predicates of a wider variety of syntactic
categories, use semantic frame abstractions, and em-
ploy explicit role labels. A common trait in prior
work has been the use of a two-stage model that
identifies arguments first, then labels them. They are
treated jointly here, unlike what has typically been
done in PropBank-style SRL (Ma`rquez et al, 2008).
Dual decomposition: Rush et al (2010) proposed
subgradient-based dual decomposition as a way of
combining models which are tractable individually,
but not jointly, by solving a relaxation of the origi-
nal problem. This was followed by work adopting
this method for syntax and translation (Koo et al,
2010; Auli and Lopez, 2011; DeNero and Macherey,
2011; Rush and Collins, 2011; Chang and Collins,
2011). Recently, Martins et al (2011b) showed that
the success of subgradient-based dual decomposi-
tion strongly relies on breaking down the original
problem into a ?good? decomposition, i.e., one with
few overlapping components. This leaves out many
declarative constrained problems, for which such a
good decomposition is not readily available. For
those, Martins et al (2011b) proposed the AD3 al-
gorithm, which retains the modularity of previous
methods, but can handle thousands of small over-
lapping components.
Exact decoding: This paper contributes an exact
branch-and-bound technique wrapped around AD3.
A related line of research is that of Rush and Collins
(2011), who proposed a tightening procedure for
dual decomposition, which can be seen as a cutting
plane method (another popular approach in combi-
natorial optimization).
6 Conclusion
We presented a novel algorithm for incorporating
declarative linguistic knowledge as constraints in
shallow semantic parsing. It outperforms a na??ve
baseline that is oblivious to the constraints. Further-
more, it is significantly faster than a decoder em-
ploying a state-of-the-art proprietary solver, and less
than twice as slow as beam search, which is inexact
and does not respect all linguistic constraints. Our
method is easily amenable to the inclusion of more
constraints, which would require minimal program-
ming effort. Our implementation of AD3 within
SEMAFOR will be publicly released at http://
www.ark.cs.cmu.edu/SEMAFOR.
Acknowledgments
We thank the three anonymous reviewers for their valu-
able feedback. This material is based upon work sup-
ported by NSF grant IIS-1054319, Google?s support
of the Wordly Knowledge Project, a FCT/ICTI grant
through the CMU-Portugal Program, and by Priberam,
through the Discooperio project, contract 2011/18501 of
the EU/FEDER program.
216
References
M. Auli and A. Lopez. 2011. A comparison of loopy be-
lief propagation and dual decomposition for integrated
ccg supertagging and parsing. In Proc. of ACL.
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: Frame semantic structure extraction. In
Proc. of SemEval.
Y.-W. Chang and Michael Collins. 2011. Exact decoding
of Phrase-Based translation models through lagrangian
relaxation. In Proc. of EMNLP. Association for Com-
putational Linguistics.
D. Chen, N. Schneider, D. Das, and N. A. Smith.
2010. SEMAFOR: Frame argument resolution with
log-linear models. In Proc. of SemEval.
D. Das and N. A. Smith. 2011. Semi-supervised frame-
semantic parsing for unknown predicates. In Proc. of
ACL.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010a.
Probabilistic frame-semantic parsing. In Proc. of
NAACL-HLT.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010b.
SEMAFOR 1.0: a probabilistic frame-semantic parser.
Technical report, CMU-LTI-10-001.
J. DeNero and K. Macherey. 2011. Model-based aligner
combination using dual decomposition. In Proc. of
ACL.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3).
C. J. Fillmore. 1982. Frame Semantics. In Linguistics in
the Morning Calm. Hanshin.
M. Gerber and J. Y. Chai. 2010. Beyond nombank: A
study of implicit arguments for nominal predicates. In
ACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
R. Johansson and P. Nugues. 2007. LTH: semantic struc-
ture extraction using nonprojective dependency trees.
In Proc. of SemEval.
P. Kingsbury and M. Palmer. 2002. From TreeBank to
PropBank. In Proc. of LREC.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In ICCV.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. of EMNLP.
L. Ma`rquez, X. Carreras, K. C. Litkowski, and S. Steven-
son. 2008. Semantic role labeling: an introduction to
the special issue. Computational Linguistics, 34(2).
A. F. T. Martins, N. A. Smith, E. P. Xing, M. A. T.
Figueiredo, and P. M. Q. Aguiar. 2010. Turbo parsers:
Dependency parsing by approximate variational infer-
ence. In EMNLP.
A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,
N. A. Smith, and E. P. Xing. 2011a. An augmented
Lagrangian approach to constrained MAP inference.
In Proc. of ICML.
A F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and
M. A. T. Figueiredo. 2011b. Dual decomposition with
many overlapping components. In Proc. of EMNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The
NomBank project: An interim report. In Proc. of
NAACL/HLT Workshop on Frontiers in Corpus Anno-
tation.
V. Punyakanok, D. Roth, W.-T. Yih, and D. Zimak. 2004.
Semantic role labeling via integer linear programming
inference. In Proc. of COLING.
V. Punyakanok, D. Roth, and W Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34:257?287.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
A. M. Rush and M. Collins. 2011. Exact decoding of
syntactic translation models through lagrangian relax-
ation. In Proc. of ACL.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear programming
relaxations for natural language processing. In Pro-
ceedings of EMNLP.
D. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In EMNLP.
K. Toutanova, A. Haghighi, and C. Manning. 2005. Joint
learning improves semantic role labeling. In Proc. of
ACL.
217
