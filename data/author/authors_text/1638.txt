Disti l l ing dialogues - A method using natural dialogue 
dialogue systems development 
Arne  JSnsson  and  N i l s  Dah lb~ick  
Depar tment  of Computer  and  In format ion  Sc ience 
L inkSp ing  Un ivers i ty  
S-581 83, L INKOPING 
SWEDEN 
nilda@ida.liu.se, arnjo@ida.liu.se 
corpora for 
Abst ract  
We report on a method for utilising corpora col- 
lected in natural settings. It is based on distilling 
(re-writing) natural dialogues to elicit the type of 
dialogue that would occur if one the dialogue par- 
ticipants was a computer instead of a human. The 
method is a complement toother means uch as Wiz- 
ard of Oz-studies and un-distilled natural dialogues. 
We present he distilling method and guidelines for 
distillation. We also illustrate how the method af- 
fects a corpus of dialogues and discuss the pros and 
cons of three approaches in different phases of dia- 
logue systems development. 
1 In t roduct ion  
It has been known for quite some time now, that 
the language used when interacting with a comput- 
er is different from the one used in dialogues between 
people, (c.f. JSnsson and Dahlb~ick (1988)). Given 
that we know that the language will be different, 
but not how it will be different, we need to base 
our development of natural language dialogue sys- 
tems on a relevant set of dialogue corpora. It is our 
belief that we need to clarify a number of different 
issues regarding the collection and use of corpora in 
the development of speech-only and multimodal dia- 
logue systems. Exchanging experiences and develop- 
ing guidelines in this area are as important as, and in 
some sense a necessary pre-requisite to, the develop- 
ment of computational models of speech, language, 
and dialogue/discourse. It is interesting to note the 
difference in the state of art in the field of natu- 
ral language dialogue systems with that of corpus 
linguistics, where issues of the usefulness of different 
samples, the necessary sampling size, representative- 
ness in corpus design and other have been discussed 
for quite some time (e.g. (Garside t al., 1997; Atkins 
et al, 1992; Crowdy, 1993; Biber, 1993)). Also the 
neighboring area of evaluation of NLP systems (for 
an overview, see Sparck Jones and Galliers (1996)) 
seems to have advanced further. 
Some work have been done in the area of natu- 
ral language dialogue systems, e.g. on the design 
of Wizard of Oz-studies (Dahlb~ck et al, 1998), 
on measures for inter-rater eliability (Carletta, 
1996), on frameworks for evaluating spoken dialogue 
agents (Walker et al, 1998) and on the use of differ- 
ent corpora in the development of a particular sys- 
tem (The Carnegie-Mellon Communicator, Eskenazi 
et al (1999)). 
The question we are addressing in this paper is 
how to collect and analyse relevant corpora. We be- 
gin by describing what we consider to be the main 
advantages and disadvantages of the two currently 
used methods; studies of human dialogues and Wiz- 
ard of Oz-dialogues, especially focusing on the eco- 
logical validity of the methods. We then describe a 
method called 'distilling dialogues', which can serve 
as a supplement to the other two. 
2 Natural and Wizard of 
Oz-Dialogues 
The advantage of using real dialogues between peo- 
ple is that they will illustrate which tasks and needs 
that people actually bring to a particular service 
provider. Thus, on the level of the users' general 
goals, such dialogues have a high validity. But there 
are two drawbacks here. First; it is not self-evident 
that users will have the same task expectations from 
a computer system as they have with a person. Sec- 
ond, the language used will differ from the language 
used when interacting with a computer. 
These two disadvantages have been the major 
force behind the development of Wizard of Oz- 
methods. The advantage here is that the setting will 
be human-computer interaction. But there are im- 
portant disadvantages, too. First, on the practical 
side, the task of setting up a high quality simulation 
environment and training the operators ('wizards') 
to use this is a resource consuming task (Dahlb~ck et 
al., 1998). Second, and probably even more impor- 
tant, is that we cannot hen observe real users using 
a system for real life tasks, where they bring their 
own needs, motivations, resources, and constraints 
to bear. To some extent this problem can be over- 
come using well-designed so called 'scenarios'. As 
pointed out in Dahlb~ck (1991), on many levels of 
analysis the artificiality of the situation will not af- 
44 
fect the language used. An example of this is the 
pattern of pronoun-antecedent relations. But since 
the tasks given to the users are often pre-described 
by the researchers, this means that this is not a good 
way of finding out which tasks the users actually 
want to perform. Nor does it provide a clear enough 
picture on how the users will act to find something 
that satisfies their requirements. If e.g. the task is 
one of finding a charter holiday trip or buying a TV- 
set within a specified set of constraints (economical 
and other), it is conceivable that people will stay 
with the first item that matches the specification, 
whereas in real life they would probably look for 
alternatives. In our experience, this is primarily a 
concern if the focus is on the users' goals and plans, 
but is less a problem when the interest is on lower- 
level aspects, such as, syntax or patterns of pronoun- 
antecedent relationship (c.f. Dahlb~ick (1991)). 
To summarize; real life dialogues will provide a 
reasonably correct picture of the way users' ap- 
proach their tasks, and what tasks they bring to 
the service provider, but the language used will not 
give a good approximation of what the system un- 
der construction will need to handle. Wizard of Oz- 
dialogues, on the other hand, will give a reasonable 
approximation of some aspects of the language used, 
but in an artificial context. 
The usual approach has been to work in three 
steps. First analyse real human dialogues, and based 
on these, in the second phase, design one or more 
Wizard of Oz-studies. The final step is to fine-tune 
the system's performance on real users. A good ex- 
ample of this method is presented in Eskenazi et al 
(1999). But there are also possible problems with 
this approach (though we are not claiming that this 
was the case in their particular project). Eskenazi et 
al. (1999) asked a human operator to act 'computer- 
like' in their Wizard of Oz-phase. The advantage 
is of course that the human operator will be able 
to perform all the tasks that is usually provided by 
this service. The disadvantage is that it puts a heavy 
burden on the human operator to act as a comput- 
er. Since we know that lay-persons' ideas of what 
computers can and cannot do are in many respects 
far removed from what is actually the case, we risk 
introducing some systematic distortion here. And 
since it is difficult to perform consistently in similar 
situations, we also risk introducing non-systematic 
distortion here, even in those cases when the 'wiz- 
ard' is an NLP-professional. 
Our suggestion is therefore to supplement he 
above mentioned methods, and bridge the gap be- 
tween them, by post-processing human dialogues to 
give them a computer-like quality. The advantage, 
compared to having people do the simulation on the 
fly, is both that it can be done with more consis- 
tency, and also that it can be done by researchers 
that actually know what human-computer natural 
language dialogues can look like. A possible dis- 
advantage with using both Wizard of Oz-and real 
computer dialogues, is that users will quickly adapt 
to what the system can provide them with, and will 
therefore not try to use it for tasks they know it 
cannot perform. Consequently, we will not get a full 
picture of the different services they would like the 
system to provide. 
A disadvantage with this method is, of course, 
that post-processing takes some time compared to 
using the natural dialogues as they are. There is al- 
so a concern on the ecological validity of the results, 
as discussed later. 
3 Distilling dialogues 
Distilling dialogues, i.e. re-writing human interac- 
tions in order to have them reflect what a human- 
computer interaction could look like involves a num- 
ber of considerations. The main issue is that in cor- 
pora of natural dialogues one of the interlocutors i
not a dialogue system. The system's task is instead 
performed by a human and the problem is how to 
anticipate the behaviour of a system that does not 
exist based on the performance of an agent with dif- 
ferent performance characteristics. One important 
aspect is how to deal with human features that are 
not part of what the system is supposed to be able  
to handle, for instance if the user talks about things 
outside of the domain, such as discussing an episode 
of a recent TV show. It also involves issues on how 
to handle situations where one of the interlocuters 
discusses with someone lse on a different opic, e.g. 
discussing the up-coming Friday party with a friend 
in the middle of an information providing dialogue 
with a customer. 
It is important for the distilling process to have at 
least an outline of the dialogue system that is under 
development: Will it for instance have the capacity 
to recognise users' goals, even if not explicitly stat- 
ed? Will it be able to reason about the discourse 
domain? What services will it provide, and what 
will be outside its capacity to handle? 
In our case, we assume that the planned dialogue 
system has the ability to reason on various aspects 
of dialogue and properties of the application. In our 
current work, and in the examples used for illustra- 
tion in this paper, we assume a dialogue model that 
can handle any relevant dialogue phenomenon and 
also an interpreter and speech recogniser being able 
to understand any user input that is relevant o the 
task. There is is also a powerful domain reason- 
ing module allowing for more or less any knowledge 
reasoning on issues that can be accomplished with- 
in the domain (Flycht-Eriksson, 1999). Our current 
system does, however, not have an explicit user task 
model, as opposed to a system task model (Dahlb~ick 
45 
and JSnsson, 1999), which is included, and thus, we 
can not assume that the 'system' remembers utter- 
ances where the user explains its task. Furthermore, 
as our aim is system development we will not con- 
sider interaction outside the systems capabilities as 
relevant o include in the distilled dialogues. 
The context of our work is the development a 
multi-modal dialogue system. However, in our cur- 
rent work with distilling dialogues, the abilities of 
a multi-modal system were not fully accounted for. 
The reason for this is that the dialogues would be 
significantly affected, e.g. a telephone conversation 
where the user always likes to have the next con- 
nection, please will result in a table if multi-modal 
output is possible and hence a fair amount of the di- 
alogne is removed. We have therefore in this paper 
analysed the corpus assuming a speech-only system, 
since this is closer to the original telephone conversa- 
tions, and hence needs fewer assumptions on system 
performance when distilling the dialogues. 
4 Dis t i l l a t ion  gu ide l ines  
Distilling dialogues requires guidelines for how to 
handle various types of utterances. In this section 
we will present our guidelines for distilling a corpus 
of telephone conversations between a human infor- 
mation provider on local buses 1to be used for devel- 
oping a multimodal dialogue system (Qvarfordt and 
JSnsson, 1998; Flycht-Eriksson and JSnsson, 1998; 
Dahlb~ick et al, 1999; Qvarfordt, 1998). Similar 
guidelines are used within another project on devel- 
oping Swedish Dialogue Systems where the domain 
is travel bureau information. 
We can distinguish three types of contributors: 
'System' (i.e. a future systems) utterances, User ut- 
terances, and other types, such as moves by other 
speakers, and noise. 
4.1 Modifying system utterances 
The problem of modifying 'system' utterances can 
be divided into two parts: how to change and when 
to change. They are in some respects intertwined, 
but as the how-part affects the when-part more we 
will take this as a starting point. 
? The 'system' provides as much relevant infor- 
mation as possible at once. This depends on 
the capabilities of the systems output modal- 
ities. If we have a screen or similar output 
device we present as much as possible which 
normally is all relevant information. If we, on 
the other hand, only have spoken output the 
amount of information that the hearer can inter- 
pret in one utterance must be considered when 
1The bus time table dialogues are collected at 
LinkSping University and are available (in Swedish) on 
http://www.ida.l iu.se/~arnjo/kfb/dialoger.html 
distilling. The system might in such cases pro- 
vide less information. The principle of provid- 
ing all relevant information is based on the as- 
sumption that a computer system often has ac- 
cess to all relevant information when querying 
the background system and can also present it 
more conveniently, especially in a multimodal 
system (Ahrenberg et al, 1996). A typical ex- 
ample is the dialogue fragment in figure 1. In 
this fragment he system provides information 
on what train to take and how to change to a 
bus. The result of distilling this fragment pro- 
vides the revised fragment of figure 2. As seen in 
the fragment of figure 2 we also remove a num- 
ber of utterances typical for human interaction, 
as discussed below. 
* System utterances are made more computer-l ike 
and do not include irrelevant information. The 
latter is seen in $9 in the dialogue in figure 3 
where the provided information is not relevant. 
It could also be possible to remove $5 and re- 
spond with $7 at once. This, however, depends 
on if the information grounded in $5-U6 is need- 
ed for the 'system' in order to know the arrival 
time or if that could be concluded from U4. 
This in turn depends on the system's capabili- 
ties. If we assume that the dialogue system has 
a model of user tasks, the information in $5-U6 
could have been concluded from that. We will, 
in this case, retain $5-U6 as we do not assume a
user task model (Dahlb/ick and JSnsson, 1999) 
and in order to stay as close to the original di- 
alogue as possible. 
The next problem concerns the case when 'system' 
utterances are changed or removed. 
? Dialogue contributions provided by something or 
someone other than the user or the 'system' are 
removed. These are regarded as not being part 
of the interaction. This means that if some- 
one interrupts the current interaction, say that 
the telephone rings during a face-to-face inter- 
action, the interrupting interaction is normally 
removed from the corpus. 
Furthermore, 'system' interruptions are re- 
moved. A human can very well interrupt anoth- 
er human interlocuter, but a computer system 
will not do that. 
However, this guideline could lead to problems, 
for instance, when users follow up such interrup- 
tions. If no information is provided or the in- 
terrupted sequence does not affect the dialogue, 
we have no problems removing the interruption. 
The problem is what to do when information 
from the 'system' is used in the continuing dia- 
logue. For such cases we have no fixed strategy, 
46 
U4: 
$5: 
U6: 
$7: 
U8: 
$9: 
U10: 
$11: 
U12: 
S13: 
U14: 
$15: 
yes I wonder if you have any mm buses or (.) like express buses leaving from LinkSping 
to Vadstena (.) on sunday 
ja ville undra om ni hade ndgra 5h bussar eUer (.) typ expressbussar sore dkte frdn LinkSping 
till Vadstena (.) pd sSnda 
no the bus does not run on sundays 
nej bussen g~r inte pd sSndagar 
how can you (.) can you take the train and then change some way (.) because (.) 
to MjSlby 'n' so 
hur kan man (.) kan man ta tdg d sen byta p~ ndtt sStt (.) fSr de (.) 
till mjSlby ~ sd 
that you can do too yes 
de kan du gSra ocksd ja 
how (.) do you have any such suggestions 
hut (.) har du n~ra n~gra s~na fSrslag 
yes let's see (4s) a moment (15s) now let us see here (.) was it on the sunday you should travel 
ja ska se h~ir (4s) eft 5gonblick (15s) nu ska vise hSr (.) va de p~ sSndagen du skulle dka pd 
yes right afternoon preferably 
ja just de eftermidda ggirna 
afternoon preferable (.) you have train from LinkSping fourteen twenty nine 
eftermidda gSrna (.) du hat t~g frdn LinkSping fjorton d tjugonie 
mm 
mm 
and then you will change from MjSlby station six hundred sixty 
sd byter du frdn MjSlby station sexhundrasexti 
sixhundred sixty 
sexhundrasexti 
fifteen and ten 
femton ~ tie 
Figure 1: Dialogue fragment from a real interaction on bus time-table information 
U4: I wonder if you have any buses or (.) like express buses going from LinkSping 
to Vadstena (.) on sunday 
S5: no the bus does not run on sundays 
U6: how can you (.) can you take the train and then change some way (.) because (.) 
to MjSlby and so 
$7: you can take the train from LinkSping fourteen and twenty nine and then you will 
change at MjSlby station to bus six hundred sixty at fifteen and ten 
Figure 2: A distilled version of the dialogue in figure 1 
the dialogue needs to be rearranged epending 
on how the information is to be used (c.f. the 
discussion in the final section of this paper). 
? 'System' utterances which are no longer valid 
are removed. Typical examples of this are the 
utterances $7, $9, $11 and $13 in the dialogue 
fragment of figure 1. 
* Remove sequences of utterances where the 'sys- 
tem' behaves in a way a computer would not do. 
For instance jokes, irony, humor, commenting 
on the other dialogue participant, or dropping 
the telephone (or whatever is going on in $7 
in figure 4). A common case of this is when 
the 'system' is talking while looking for infor- 
mation, $5 in the dialogue fragment of figure 4 
is an example of this. Related to this is when 
the system provides its own comments. If we 
can assume that it has such capabilities they 
are included, otherwise we remove them. 
The system does not repeat information that has 
already been provided unless explicitly asked to 
do so. In human interaction it is not uncommon 
to repeat what has been uttered for purposes 
other than to provide grounding information or 
feedback. This is for instance common during 
47  
U4: 'n' I must be at Resecentrum before fourteen and thirty five (.) 'cause we will going to the 
interstate buses 
ja ska va p~ rececentrum innan \]jorton ~ trettifem (.) f5 vi ska till 
l~ngf~irdsbussarna 
$5: aha (.) 'n' then you must be there around twenty past two something then 
jaha (.) ~ dd behhver du va here strax e~ter tjuge 5vet tvd n~nting d~ 
U6: yes around that 
ja ungefgir 
$7: let's see here ( l ls)  two hundred and fourteen Ryd end station leaves forty six (.) thirteen 'n' 
forty six then you will be down fourteen oh seven (.) 
d~ ska vise hSr (11s) tv~hundrafjorton Ryd 5ndh~llplatsen gdr ~5rtisex (.) tretton d 
\]Srtisex d~ dr du nere ~jorton noll sju 5) 
U8: aha 
jaha 
$9: 'n' (.) the next one takes you there (.) fourteen thirty seven (.) but that is too late 
(.) ndsta dr du nere 5) ~jorton d trettisju (.) men de 5 ju ~Sr sent 
Figure 3: Dialogue fragment from a real interaction on bus time-table information 
U2: Well, hi (.) I am going to Ugglegatan eighth 
ja hej (.) ja ska till Ugglegatan dtta 
$3: Yes 
ja 
U4: and (.) I wonder (.) it is somewhere in Tannefors 
och (.) jag undrar (.) det ligger ndnstans i Tannefors 
$5: Yes (.) I will see here one one I will look exactly where it is one moment please 
ja (.) jag ska se hhr eft eft jag ska titta exakt vat det ligger eft 6gonblick barn 
U6: Oh Yeah 
jar~ 
$7: (operator disconnects) (25s) mm (.) okey (hs) what the hell (2s) 
(operator connects again) hello yes 
((Telefonisten kopplar ur sig)) (25s) iihh (.) okey (hs) de va sore \]aan (2s) 
((Telefonisten kopplar in sig igen)) halld ja 
U8: Yes hello 
ja hej 
$9: It is bus two hundred ten which runs on old tannefors road that you have to take and get off at 
the bus stop at that bus stop named vetegatan 
det ~i buss tv~hundratio sore g~r gamla tanne~orsvSgen som du ~r  ~ka ~ g~ av rid 
den hdllplatsen rid den hdllplatsen sore heter vetegatan. 
Figure 4: Dialogue fragment from a natural bus timetable interaction 
search procedures as discussed above. 
? The system does not ask for information it has 
already achieved. For instance asking again if it 
is on Sunday as in $9 in figure 1. This is not un- 
common in human interaction and such utter- 
ances from the user are not removed. However, 
we can assume that the dialogue system does 
not forget what has been talked about before. 
4.2 Mod i fy ing  user  u t te rances  
The general rule is to change user utterances as lit- 
tle as possible. The reason for this is that we do not 
want to develop systems where the user needs to 
restrict his/her behaviour to the capabilities of the 
dialogue system. However, there are certain changes 
made to user utterances, in most cases as a conse- 
quence of changes of system utterances. 
Utterances that are no longer valid are removed. 
The most common cases are utterances whose 
request has already been answered, as seen in 
the distilled dialogue in figure 2 of the dialogue 
in figure 1. 
48 
Sl1: sixteen fifty five 
sexton \]emti/em 
U12: sixteen fifty five (.) aha 
sexton femti/em (.) jaha 
S13: bus line four hundred thirty five 
linje \]yrahundra tretti/em 
Figure 5: Dialogue fragment from a natural bus 
timetable interaction 
? Utterances are removed where the user discuss- 
es things that are in the environment. For 
instance commenting the 'systems' clothes or 
hair. This also includes other types of commu- 
nicative signals such as laughter based on things 
outside the interaction, for instance, in the en- 
vironment of the interlocuters. 
? User utterances can also be added in order to 
make the dialogue continue. In the dialogue in 
figure 5 there is nothing in the dialogue xplain- 
ing why the system utters S13. In such cases 
we need to add a user utterance, e.g. Which 
bus is that?. However, it might turn out that 
there are cues, such as intonation, found when 
listening to the tapes. If  such detailed analyses 
are carried out, we will, of course, not need to 
add utterances. Furthermore, it is sometimes 
the case that the telephone operator deliberate- 
ly splits the information into chunks that can 
be comprehended by the user, which then must 
be considered in the distillation. 
5 App ly ing  the  method 
To illustrate the method we will in this section try to 
characterise the results from our distillations. The 
illustration is based on 39 distilled dialogues from 
the previously mentioned corpus collected with a 
telephone operator having information on local bus 
time-tables and persons calling the information ser- 
vice. 
The distillation took about three hours for all 39 
dialogues, i.e. it is reasonably fast. The distilled 
dialogues are on the average 27% shorter. However, 
this varies between the dialogues, at most 73% was 
removed but there were also seven dialogues that 
were not changed at all. 
At the most 34 utterances where removed from 
one single dialogue and that was from a dialogue 
with discussions on where to find a parking lot, i.e. 
discussions outside the capabilities of the applica- 
tion. There was one more dialogue where more than 
30 utterances were removed and that dialogue is a 
typical example of dialogues where distillation actu- 
ally is very useful and also indicates what is normal- 
ly removed from the dialogues. This particular dia- 
logue begins with the user asking for the telephone 
number to 'the Lost property office' for a specific bus 
operator. However, the operator starts a discussion 
on what bus the traveller traveled on before provid- 
ing the requested telephone number. The reason for 
this discussion is probably that the operator knows 
that different bus companies are utilised and would 
like to make sure that the user really understands 
his/her request. The interaction that follows can, 
thus, in that respect be relevant, but for our pur- 
pose of developing systems based on an overall goal 
of providing information, not to understand human 
interaction, our dialogue system will not able to han- 
dle such phenomenon (JSnsson, 1996). 
The dialogues can roughly be divided into five dif- 
ferent categories based on the users task. The dis- 
cussion in twenty five dialogues were on bus times 
between various places, often one departure and one 
arrival but five dialogues involved more places. In 
five dialogues the discussion was one price and var- 
ious types of discounts. Five users wanted to know 
the telephone number to 'the Lost property office', 
two discussed only bus stops and two discussed how 
they could utilise their season ticket to travel out- 
side the trafficking area of the bus company. It is 
interesting to note that there is no correspondence 
between the task being performed uring the inter- 
action and the amount of changes made to the dia-  
logue. Thus, if we can assume that the amount of 
distillation indicates omething about a user's inter- 
action style, other factors than the task are impor- 
tant when characterising user behaviour. 
Looking at what is altered we find that the most 
important distilling principle is that the 'system' 
provides all relevant information at once, c.f. fig- 
ures 1 and 2. This in turn removes utterances pro- 
vided by both 'system' and user. 
Most added utterances, both from the user and 
the 'system', provide explicit requests for informa- 
tion that is later provided in the dialogue, e.g. ut- 
terance $3 in figure 6. We have added ten utterances 
in all 39 dialogues, five 'system' utterances and five 
user utterances. Note, however, that we utilised the 
transcribed ialogues, without information on into- 
nation. We would probably not have needed to add 
this many utterances if we had utilised the tapes. 
Our reason for not using information on intonation 
is that we do not assume that our system's peech 
recogniser can recognise intonation. 
Finally, as discussed above, we did not utilise the 
full potential of multi-modality when distilling the 
dialogues. For instance, some dialogues could be 
further distilled if we had assumed that the system 
had presented a time-table. One reason for this is 
that we wanted to capture as many interesting as- 
pects intact as possible. The advantage is, thus, that 
we have a better corpus for understanding human- 
49 
U2: Yees hi Anna Nilsson is my name and I would like to take the bus from Ryd center to Resecentrum 
in LinkSping 
jaa hej Anna Nilsson heter jag och jag rill ~ka buss ~r~n Ryds centrum till resecentrum 
i LinkSping. 
$3: mm When do you  want  to  leave? 
mm N~ir r i l l  du  ?ka? 
U4: 'n' I must be at Resecentrum before fourteen and thirty five (.) 'cause we will going to the 
interstate buses 
ja ska va p~ rececentrum innan fjorton d trettifem (.) f5 vi ska till 
l~ngfiirdsbussarna 
Figure 6: Distilled dialogue fragment with added utterance 
computer interaction and can from that corpus do 
a second distillation where we focus more on multi- 
modal interaction. 
6 Discuss ion 
We have been presenting a method for distilling hu- 
man dialogues to make them resemble human com- 
puter interaction, in order to utilise such dialogues 
as a knowledge source when developing dialogue sys- 
tems. Our own main purpose has been to use them 
for developing multimodal systems, however, as dis- 
cussed above, we have in this paper rather assumed 
a speech-only system. But we believe that the basic 
approach can be used also for multi-modal systems 
and other kinds of natural language dialogue sys- 
tems. 
It is important o be aware of the limitations of 
the method, and how 'realistic' the produced result 
will be, compared to a dialogue with the final sys- 
tem. Since we are changing the dialogue moves, by 
for instance providing all required information in one 
move, or never asking to be reminded of what the us- 
er has previously requested, it is obvious that what 
follows after the changed sequence would probably 
be affected one way or another. A consequence of 
this is that the resulting dialogue is less accurate as 
a model of the entire dialogue. It is therefore not an 
ideal candidate for trying out the systems over-all 
performance during system development. But for 
the smaller sub-segments or sub-dialogues, we be- 
lieve that it creates a good approximation of what 
will take place once the system is up and running. 
Furthermore, we believe distilled dialogues in some 
respects to be more realistic than Wizard of Oz- 
dialogues collected with a wizard acting as a com- 
puter. 
Another issue, that has been discussed previously 
in the description of the method, is that the distilling 
is made based on a particular view of what a dialogue 
with a computer will look like. While not necessari- 
ly being a detailed and specific model, it is at least 
an instance of a class of computer dialogue models. 
One example of this is whether the system is meant 
to acquire information on the user's underlying mo- 
tivations or goals or not. In the examples presented, 
we have not assumed such capabilities, but this as- 
sumption is not an absolute necessity. We believe, 
however, that the distilling process should be based 
on one such model, not the least to ensure a con- 
sistent treatment of similar recurring phenomena t 
different places in the corpora. 
The validity of the results based on analysing dis- 
tilled dialogues depends part ly on how the distilla- 
tion has been carried out. Even when using natural 
dialogues we can have situations where the interac- 
tion is somewhat mysterious, for instance, if some of 
the dialogue participants behaves irrational such as 
not providing feedback or being too elliptical. How- 
ever, if careful considerations have been made to stay 
as close to the original dialogues as possible, we be- 
lieve that distilled dialogues will reflect what a hu- 
man would consider to be a natural interaction. 
Acknowledgments  
This work results from a number of projects on de- 
velopment of natural language interfaces upported 
by The Swedish Transport & Communications Re- 
search Board (KFB) and the joint Research Program 
for Language Technology (HSFR/NUTEK) .  We are 
indebted to the participants of the Swedish Dialogue 
Systems project, especially to Staffan Larsson, Lena 
Santamarta, and Annika Flycht-Eriksson for inter- 
esting discussions on this topic. 
Re ferences  
Lars Ahrenberg, Nils Dahlb~ck, Arne JSnsson, 
and /~ke Thur~e. 1996. Customizing interac- 
tion for natural language interfaces. LinkSpin9 
Electronic articles in Computer and Informa- 
tion Science, also in Notes from Workshop on 
Pragmatics in Dialogue, The XIV:th Scandi- 
navian Conference of Linguistics and the VI- 
II:th Conference of Nordic and General Linguis- 
50 
tics, GSteborg, Sweden, 1993, 1(1), October, 1. 
http :/ / www.ep.liu.se / ea /cis /1996 / O01/. 
Sue Atkins, Jeremy Clear, and Nicholas Ostler. 
1992. Corpus design criteria. Literary and Lin- 
guistic Computing, 7(1):1-16. 
Douglas Biber. 1993. Representativeness in cor- 
pus design. Literary and Linguistic Computing, 
8(4):244-257. 
Jean Carletta. 1996. Assessing agreement on classi- 
fication tasks: The kappa statistic. Computation- 
al Linguistics, 22(2):249-254. 
Steve Crowdy. 1993. Spoken corpus design. Literary 
and Linguistic Computing, 8(4):259-265. 
Nils Dahlb/ick and Arne JSnsson. 1999. Knowledge 
sources in spoken dialogue systems. In Proceed- 
ings of Eurospeech'99, Budapest, Hungary. 
Nils Dahlb/ick, Arne JSnsson, and Lars Ahrenberg. 
1998. Wizard of oz studies - why and how. 
In Mark Maybury & Wolfgang Wahlster, editor, 
Readings in Intelligent User Interfaces. Morgan 
Kaufmann. 
Ntis Dahlb/ick, Annika Flycht-Eriksson, Arne 
JSnsson, and Pernilla Qvarfordt. 1999. An ar- 
chitecture for multi-modal natural dialogue sys- 
tems. In Proceedings of ESCA Tutorial and Re- 
search Workshop (ETRW) on Interactive Dialogue 
in Multi-Modal Systems, Germany. 
Nils Dahlb/ick. 1991. Representations ofDiscourse, 
Cognitive and Computational Aspects. Ph.D. the- 
sis, LinkSping University. 
Maxine Eskenazi, Alexander Rudnicki, Karin Grego- 
ry, Paul Constantinides, Robert Brennan, Christi- 
na Bennett, and Jwan Allen. 1999. Data collec- 
tion and processing in the carnegie mellon com- 
municator. In Proceedings of Eurospeech'99, Bu- 
dapest, Hungary. 
Annika Flycht-Eriksson and Arne JSnsson. 1998. A 
spoken dialogue system utilizing spatial informa- 
tion. In Proceedings of ICSLP'98, Sydney, Aus- 
tralia. 
Annika Flycht-Eriksson. 1999. A survey of knowl- 
edge sources in dialogue systems. In Proceedings 
of lJCAI-99 Workshop on Knowledge and Reason- 
ing in Practical Dialogue Systems, August, Stock- 
holm. 
Roger Garside, Geoffrey Leech, and Anthony 
MeEnery. 1997. Corpus Annotation. Longman. 
Arne JSnsson and Nils Dahlb/ick. 1988. Talking to a 
computer is not like talking to your best friend. In 
Proceedings of the First Scandinavian Conference 
on Artificial InterUigence, Tvoms?. 
Arne JSnsson. 1996. Natural language generation 
without intentions. In Proceedings of ECAI'96 
Workshop Gaps and Bridges: New Directions 
in Planning and Natural Language Generation, 
pages 102-104. 
Pernilla Qvarfordt and Arne JSnsson. 1998. Effects 
of using speech in timetable information systems 
for www. In Proceedings of ICSLP'98, Sydney, 
Australia. 
Pernilla Qvarfordt. 1998. Usability of multimodal 
timetables: Effects of different levels of do- 
main knowledge on usability. Master's thesis, 
LinkSping University. 
Karen Sparck Jones and Julia R. Galliers. 1996. 
Evaluating Natural Language Processing Systems. 
Springer Verlag. 
Marilyn A. Walker, Diane J. Litman, Candace A. 
Kamm, and Alicia Abella. 1998. Paradise: A 
framework for evaluating spoken dialogue agents. 
In Mark Maybury & Wolfgang Wahlster, editor, 
Readings in Intelligent User Interfaces. Morgan 
Kaufmann. 
51 
