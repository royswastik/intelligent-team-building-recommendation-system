In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 51?60,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Connectionist-Inspired Incremental PCFG Parsing
Marten van Schijndel
The Ohio State University
vanschm@ling.ohio-state.edu
Andy Exley
University of Minnesota
exley@cs.umn.edu
William Schuler
The Ohio State University
schuler@ling.ohio-state.edu
Abstract
Probabilistic context-free grammars (PCFGs)
are a popular cognitive model of syntax (Ju-
rafsky, 1996). These can be formulated to
be sensitive to human working memory con-
straints by application of a right-corner trans-
form (Schuler, 2009). One side-effect of the
transform is that it guarantees at most a sin-
gle expansion (push) and at most a single re-
duction (pop) during a syntactic parse. The
primary finding of this paper is that this prop-
erty of right-corner parsing can be exploited to
obtain a dramatic reduction in the number of
random variables in a probabilistic sequence
model parser. This yields a simpler structure
that more closely resembles existing simple
recurrent network models of sentence compre-
hension.
1 Introduction
There may be a benefit to using insights from human
cognitive modelling in parsing. Evidence for in-
cremental processing can be seen in garden pathing
(Bever, 1970), close shadowing (Marslen-Wilson,
1975), and eyetracking studies (Tanenhaus et al,
1995; Allopenna et al, 1998; Altmann and Kamide,
1999), which show humans begin attempting to pro-
cess a sentence immediately upon receiving lin-
guistic input. In the cognitive science community,
this incremental interaction has often been mod-
elled using recurrent neural networks (Elman, 1991;
Mayberry and Miikkulainen, 2003), which utilize
a hidden context with a severely bounded repre-
sentational capacity (a fixed number of continuous
units or dimensions), similar to models of activation-
based memory in the prefrontal cortex (Botvinick,
2007), with the interesting possibility that the dis-
tributed behavior of neural columns (Horton and
Adams, 2005) may directly implement continuous
dimensions of recurrent hidden units. This paper
presents a refinement of a factored probabilistic se-
quence model of comprehension (Schuler, 2009) in
the direction of a recurrent neural network model
and presents some observed efficiencies due to this
refinement.
This paper will adopt an incremental probabilis-
tic context-free grammar (PCFG) parser (Schuler,
2009) that uses a right-corner variant of the left-
corner parsing strategy (Aho and Ullman, 1972)
coupled with strict memory bounds, as a model of
human-like parsing. Syntax can readily be approxi-
mated using simple PCFGs (Hale, 2001; Levy, 2008;
Demberg and Keller, 2008), which can be easily
tuned (Petrov and Klein, 2007). This paper will
show that this representation can be streamlined to
exploit the fact that a right-corner parse guarantees
at most one expansion and at most one reduction can
take place after each word is seen (see Section 2.2).
The primary finding of this paper is that this prop-
erty of right-corner parsing can be exploited to ob-
tain a dramatic reduction in the number of random
variables in a probabilistic sequence model parser
(Schuler, 2009) yielding a simpler structure that
more closely resembles connectionist models such
as TRACE (McClelland and Elman, 1986), Short-
list (Norris, 1994; Norris and McQueen, 2008), or
recurrent models (Elman, 1991; Mayberry and Mi-
ikkulainen, 2003) which posit functional units only
for cognitively-motivated entities.
The rest of this paper is structured as follows:
Section 2 gives the formal background of the right-
corner parser transform and probabilistic sequence
51
model parsing. The simplification of this model is
described in Section 3. A discussion of the interplay
between cognitive theory and computational mod-
elling in the resulting model may be found in Sec-
tion 4. Finally, Section 5 demonstrates that such
factoring also yields large benefits in the speed of
probabilistic sequence model parsing.
2 Background
2.1 Notation
Throughout this paper, PCFG rules are defined over
syntactic categories subscripted with abstract tree
addresses (c??). These addresses describe a node?s
location as a path from a given ancestor node. A 0
on this path represents a leftward branch and a 1 a
rightward branch. Positions within a tree are repre-
sented by subscripted ? and ? so that c?0 is the left
child of c? and c?1 is the right child of c?. The set of
syntactic categories in the grammar is denoted by C.
Finally, J?K denotes an indicator probability which
is 1 if ? and 0 otherwise.
2.2 Right-Corner Parsing
Parsers such as that of Schuler (2009) model hierar-
chically deferred processes in working memory us-
ing a coarse analogy to a pushdown store indexed
by an embedding depth d (to a maximum depth D).
To make efficient use of this store, a CFG G must
be transformed using a right-corner transform into
another CFG G? with no right recursion. Given an
optionally arc-eager attachment strategy, this allows
the parser to clear completed parse constituents from
the set of incomplete constituents in working mem-
ory much earlier than with a conventional syntactic
structure. The right-corner transform operates de-
terministically over a CFG following three mapping
rules:
c? ? c?0 c?1 ? G
c?/c?1 ? c?0 ? G?
(1)
c?? ? c??0 c??1 ? G, c? ? C
c?/c??1 ? c?/c?? c??0 ? G?
(2)
c?? ? x?? ? G, c? ? C
c? ? c?/c?? c?? ? G?
(3)
A bottom-up incremental parsing strategy com-
bined with the way the right-corner transform pulls
each subtree into a left-expanding hierarchy ensures
at most a single expansion (push) will occur at
any given observation. That is, each new observa-
tion will be the leftmost leaf of a right-expanding
subtree. Additionally, by reducing multiply right-
branching subtrees to single rightward branches, the
transform also ensures that at most a single reduc-
tion (pop) will take place at any given observation.
Schuler et al (2010) show near complete cover-
age of the Wall Street Journal portion of the Penn
Treebank (Marcus et al, 1993) can be achieved with
a right-corner incremental parsing strategy using no
more than four incomplete contituents (deferred pro-
cesses), in line with recent estimates of human work-
ing memory capacity (Cowan, 2001).
Section 3 will show that, in addition to being de-
sirable for bounded working memory restrictions,
the single expansion/reduction guarantee reduces
the search space between words to only two decision
points ? whether to expand and whether to reduce.
This allows rapid processing of each candidate parse
within a sequence modelling framework.
2.3 Model Formulation
This transform is then extended to PCFGs and inte-
grated into a sequence model parser. Training on
an annotated corpus yields the probability of any
given syntactic state executing an expansion (creat-
ing a syntactic subtree) or a reduction (completing
a syntactic subtree) to transition from every suffi-
ciently probable (in this sense active) hypothesis in
the working memory store.
The probability of the most likely sequence of
store states q?1..D1..T can then be defined as the prod-
uct of nonterminal ?Q, preterminal ?P,d, and termi-
nal ?X factors:
q?1..D1..T
def= argmax
q1..D1..T
T
?
t=1
P?Q(q1..Dt | q1..Dt?1 pt?1)
? P?P,d? (pt | b
d?
t ) ? P?X (xt | pt) (4)
where all incomplete constituents qdt are factored
into active adt and awaited bdt components:
qdt
def= adt /bdt (5)
and d? determines the deepest non-empty incomplete
constituent of q1..Dt :
52
d? def= max{d | qdt 6= ???} (6)
The preterminal model ?P,d denotes the expecta-
tion of a subtree containing a given preterminal, ex-
pressed in terms of side- and depth-specific gram-
mar rules P?Gs,d(c? ? c?0 c?1) and expected counts
of left progeny categories E?G?,d(c?
?? c?? ...) (see
Appendix A):
P?P,d(c?? | c?)
def= E?G?,d(c?
?? c?? ...)
?
?
x??
P?GL,d(c?? ? x??) (7)
and the terminal model ?X is simply:
P?X (x? | c?)
def=
P?G(c? ? x?)
?
x? P?G(c? ? x?)
(8)
The Schuler (2009) nonterminal model ?Q is
computed from a depth-specific store element
model ?Q,d and a large final state model ?F,d:
P?Q(q1..Dt | q1..Dt?1 pt?1)
def=
?
f1..Dt
D
?
d=1
P?F,d(fdt | fd+1t qdt?1 qd?1t?1 )
? P?Q,d(qdt | fd+1t fdt qdt?1 qd?1t ) (9)
After each time step t and depth d, ?Q generates
a set of final states to generate a new incomplete
constituent qdt . These final states fdt are factored
into categories cfdt and boolean variables (0 or 1)
encoding whether a reduction has taken place at
depth d and time step t. The depth-specific final state
model ?F,d gives the probability of generating a final
state fdt from the preceding qdt and qd?1t which is the
probability of executing a reduction or consolidation
of those incomplete constituents:
P?F,d(fdt | fd+1t qdt?1 qd?1t?1 )
def=
{
if fd+1t = ??? : Jfdt = 0K
if fd+1t 6= ??? : P?F,d,R(fdt | qdt?1 qd?1t?1 )
(10)
With these depth-specific fdt in hand, the model can
calculate the probabilities of each possible qdt for
each d and t based largely on the probability of tran-
sitions (?Q,d,T ) and expansions (?Q,d,E) from the in-
complete constituents at the previous time step:
P?Q,d(qdt | fd+1t fdt qdt?1 qd?1t?1 )
def=
?
?
?
if fd+1t = ???, fdt = ??? : Jqdt = qdt?1K
if fd+1t 6= ???, fdt = ??? : P?Q,d,T (qdt | fd+1t fdt qdt?1 qd?1t )
if fd+1t 6= ???, fdt 6= ??? : P?Q,d,E (qdt | qd?1t )
(11)
This model is shown graphically in Figure 1.
The probability distributions over reductions
(?F,d,R), transitions (?Q,d,T ) and expansions
(?Q,d,E) are then defined, also in terms of side- and
depth-specific grammar rules P?Gs,d(c? ? c?0 c?1)
and expected counts of left progeny cate-
gories E?G?,d(c?
?? c?? ...) (see Appendix A):
P?Q,d,T (qdt | fd+1t fdt qdt?1qd?1t )
def=
{
if fdt 6= ???: P?Q,d,A(qdt | qd?1t fdt )
if fdt = ???: P?Q,d,B (qdt | qdt?1fd+1t )
(12)
P?F,d,R(fdt | fd+1t qdt?1qd?1t?1 )
def=
{
if cfd+1t 6=xt : Jf
d
t = ???K
if cfd+1t =xt : P?F,d,R(f
d
t | qdt?1qd?1t?1 )
(13)
P?Q,d,E (c??/c??? | /c?)
def=
E?G?,d(c?
?? c?? ...) ? Jx?? = c??? = c??K (14)
The subcomponent models are obtained by ap-
plying the transform rules to all possible trees pro-
portionately to their probabilities and marginalizing
over all constituents that are not used in the models:
? for active transitions (from Transform Rule 1):
P?Q,d,A(c??/c??1 | /c? c??0)
def=
E?G?,d(c?
?? c?? ...) ? P?GL,d(c?? ? c??0 c??1)
E?G?,d(c?
+? c??0 ...)
(15)
53
p1
x1
q11
q21
q31
q41
p2
x2
q12
q22
q32
q42
p3
x3
q13
q23
q33
q43
p4
x4
q14
q24
q34
q44
p5
x5
q15
q25
q35
q45
p6
x6
q16
q26
q36
q46
p7
x7
q17
q27
q37
q47
f12
f22
f32
f42
f13
f23
f33
f43
f14
f24
f34
f44
f15
f25
f35
f45
f16
f26
f36
f46
f17
f27
f37
f47
=
DT
=
the
=
0,D
T
=
NP
/N
N
=
NN
=
fun
d
=
0,N
P
=
S/V
P
=
VB
=
bo
ug
ht
=
0,V
B = S
/V
P
=
VP
/N
P
=
DT
=
tw
o
=
1,D
T
=
S/V
P
=
VP
/N
N
=
JJ
=
re
gio
na
l
=
1,J
J
=
S/V
P
=
VP
/N
N
=
NN
=
ban
ks
=
1,V
P = S
/RB
=
RB
=
tod
ay
Figure 1: Schuler (2009) Sequence Model
? for awaited transitions (Transform Rule 2):
P?Q,d,B (c?/c??1 | c??/c?? c??0)
def=
Jc? = c??K ?
P?GR,d(c?? ? c??0 c??1)
E?G?,d(c??
0? c??0 ...)
(16)
? for reductions (from Transform Rule 3):
P?F,d,R(c??,1 | /c? c???/ )
def=
Jc?? = c???K ?
E?G?,d(c?
0? c?? ...)
E?G?,d(c?
?? c?? ...)
(17)
P?F,d,R(c??,0 | /c? c???/ )
def=
Jc?? = c???K ?
E?G?,d(c?
+? c?? ...)
E?G?,d(c?
?? c?? ...)
(18)
3 Simplified Model
As seen in the previous section, the right-corner
parser of Schuler (2009) makes the center embed-
ding depth explicit and each memory store element
is modelled as a combination of an active and an
awaited component. Each input can therefore either
increase (during an expansion) or decrease (during
a reduction) the store of incomplete constituents or
it can alter the active or awaited component of the
deepest incomplete constituent (the affectable ele-
ment). Alterations of the awaited component of the
affectable element can be thought of as the expan-
sion and immediate reduction of a syntactic con-
stituent. The grammar models transitions in the ac-
tive component implicitly, so these are conceptual-
ized as consisting of neither an expansion nor a re-
duction.
Removing some of the variables in this model re-
sults in one that looks much more like a neural net-
work (McClelland and Elman, 1986; Elman, 1991;
Norris, 1994; Norris and McQueen, 2008) in that
all remaining variables have cognitive correllates ?
in particular, they correspond to incomplete con-
stituents in working memory ? while still maintain-
ing the ability to explicitly represent phrase struc-
ture. This section will demonstrate how it is possi-
ble to exploit this to obtain a large reduction in the
number of modelled random variables.
In the Schuler (2009) sequence model, eight ran-
dom variables are used to model the hidden states
at each time step (see Figure 1). Half of these vari-
ables are joint consisting of two further (active and
awaited) constituent variables, while the other half
are merely over intermediate final states. Although
the entire store is carried from time step to time
step, only one memory element is affectable at any
one time, and this element may be reduced zero or
54
one times (using an intermediate final state), and ex-
panded zero or one times (using an incomplete con-
stituent state), yielding four possible combinations.
This means the model only actually needs one of its
intermediate final states.
The transition model ?Q can therefore be simpli-
fied with terms ?F,d for the probability of expand-
ing the incomplete constituent at d, and terms ?A,d
and ?B,d for reducing the resulting constituent
(defining the active and awaited components of
a new incomplete constituent), along with terms
for copying incomplete constituents above this af-
fectable element, and for emptying the elements be-
low it:
P?Q(q1..Dt | q1..Dt?1 pt?1)
def=P?F,d? (?+? | bd
?
t?1 pt?1) ? P?A,d? (??? | b
d??1
t?1 ad
?
t?1)
? Jad??1t =ad
??1
t?1 K ? P?B,d??1(b
d??1
t | bd
??1
t?1 ad
?
t?1)
? Jq1..d??2t =q1..d
??2
t?1 K ? Jqd
?..D
t = ???K
+P?F,d? (?+? | bd
?
t?1 pt?1) ? P?A,d? (ad
?
t | bd
??1
t?1 ad
?
t?1)
? P?B,d? (b
d?
t | ad
?
t ad
?+1
t?1 )
? Jq1..d??1t =q1..d
??1
t?1 K ? Jqd
?+1..D
t = ???K
+P?F,d? (??? | bd
?
t?1 pt?1) ? P?A,d? (??? | bd
?
t?1 pt?1)
? Jad?t =ad
?
t?1K ? P?B,d? (b
d?
t | bd
?
t?1 pt?1)
? Jq1..d??1t =q1..d
??1
t?1 K ? Jqd
?+1..D
t = ???K
+P?F,d? (??? | bd
?
t?1 pt?1) ? P?A,d? (a
d?+1
t | bd
?
t?1 pt?1)
? P?B,d? (b
d?+1
t | ad
?+1
t pt?1)
? Jq1..d?t =q1..d
?
t?1 K ? Jqd
?+2..D
t = ???K (19)
The first element of the sum in Equation 19 com-
putes the probability of a reduction with no expan-
sion (decreasing d?). The second corresponds to the
probability of a store undergoing neither an expan-
sion nor a reduction (a transition to a new active con-
stituent at the same embedding depth). In the third
is the probability of an expansion and a reduction
(a transition among awaited constituents at the same
embedding depth). Finally, the last term yields the
probability of an expansion without a reduction (in-
creasing d?).
From Equation 19 it may be seen that the unaf-
fected store elements of each time step are main-
tained sans change as guaranteed by the single-
reduction feature of the right-corner parser. This re-
sults in a large representational economy by mak-
ing the majority of store state decisions determinis-
tic. This representational economy will later trans-
late into computational efficiencies (see section 5).
In this sense, cognitive modelling contributes to a
practical speed increase.
Since the bulk of the state remains the same,
the recognizer can access the affectable variable
and operate solely over the transition possibili-
ties from that variable to calculate the distribu-
tion over store states for the next time step to ex-
plore. Reflecting this change, the hidden states
now model a single final-state variable (f) for
results of the expansion decision, and the af-
fectable variable resulting from the reduction de-
cision (both its active (a) and awaited (b) cate-
gories), as well as the preterminal state (p) defined
in the previous section. These models are again ex-
pressed in terms of side- and depth-specific grammar
rules P?Gs,d(c? ? c?0 c?1) and expected counts of
left progeny categories E?G?,d(c?
?? c?? ...) (see
Appendix A).
Expansion probabilities are modelled as a binary
decision depending on whether or not the awaited
component of the affectable variable c? is likely to
expand immediately into an anticipated pretermi-
nal c?? (resulting in a non-empty final state: ?+?) or
if intervening embeddings are necessary given the
affectable active component (yielding no final state:
???):
P?F,d(f | c? c??)
def=
?
?
?
?
?
?
?
?
?
if f= ?+? :
E?G?,d (c?
0
?c?? ...)
E?G?,d (c?
?
?c?? ...)
if f= ??? :
E?G?,d (c?
+
?c?? ...)
E?G?,d (c?
?
?c?? ...)
(20)
The active component category c?? is defined as de-
pending on the category of the awaited component
above it c? and its left-hand child c??0:
P?A,d(c?? | c? c??0)
def=
E?G?,d (c?
1
?c??0 ...)
E?G?,d (c?
+
?c??0 ...)
? Jc??= ???K
+
E?G?,d (c?
+
?c?? ...)?P?GL,d (c???c??0 ...)
E?G?,d (c?
+
?c??0 ...)
(21)
The awaited component category c?1 is defined as
55
depending on the category of its parent c? and the
preceding sibling c?0:
P?B,d(c?1 | c? c?0)
def=
P?GR,d (c??c?0 c?1)
E?G?,d (c?
1
?c?0 ...)
(22)
Both of these make sense given the manner in which
the right-corner parser shifts dependencies to the left
down the tree in order to obtain incremental infor-
mation about upcoming constituents.
3.1 Graphical Representation
In order to be represented graphically, the working
memory store ?Q is factored into a single expansion
term ?F and a product of depth-specific reduction
terms ?Q,d:
P?Q(q1..Dt | q1..Dt?1 pt?1)
def=
?
ft
P?F (ft | q1..Dt?1 )
?
D
?
d=1
P?Q,d(qdt | q1..Dt?1 pt?1 ft qd+1t ) (23)
and the depth-specific reduction model ?Q,d is fac-
tored into individual decisions over each random
variable:
P?Q,d(qdt | q1..Dt?1 pt?1 ft qd+1t )
def=
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
if qd+1t = ???, ft 6= ???, d=d??1 :
Jadt =adt?1K ? P?B,d(bdt | bdt?1 ad+1t?1 )
if qd+1t = ???, ft 6= ???, d=d? :
P?A,d(adt | bd?1t?1 adt?1) ? P?B,d(bdt | adt adt?1)
if qd+1t = ???, ft= ???, d=d? :
Jadt =adt?1K ? P?B,d(bdt | bdt?1 pt?1)
if qd+1t = ???, ft= ???, d=d?+1 :
P?A,d(adt | bd?1t?1 pt?1) ? P?B,d(bdt | adt pt?1)
if qd+1t 6= ??? : Jqdt =qdt?1K
otherwise : Jqdt = ???K
(24)
This dependency structure is represented graphically
in Figure 2.
The first conditional in Equation 24 checks
whether the input causes a reduction but no expan-
sion (completing a subtree parse). In this case, d? is
reduced from the previous t, and the relevant qdt?1 is
copied to qdt except the awaited constituent is altered
to reflect the completion of its preceding awaited
subtree. In the second case, the parser makes an
active transition as it completes a left subtree and
begins exploring the right subtree. The third case
is similar to the first except it transitions between
two like depths (awaited transition), and depends on
the preterminal just seen to contrive a new subtree
to explore. In the fourth case, d? is incremented as
another incomplete constituent opens up in working
memory. The final two cases simply update the un-
affected store states to reflect their previous states at
time t? 1.
4 Discussion
This factoring of redundant hidden states out of the
Schuler (2009) probabilistic sequence model shows
that cognitive modelling can more closely approx-
imate a simple recurrent network model of lan-
guage processing (Elman, 1991). Probabilistic se-
quence model parsers have previously been mod-
elled with random variables over incomplete con-
stituents (Schuler, 2009). In the current implementa-
tion, each variable can be thought of as a bank of ar-
tificial neurons. These artificial neurons inhibit one
another through the process of normalization. Con-
versely, they activate artificial neurons at subsequent
time steps by contributing probability mass through
the transformed grammar. This point was made by
Norris and McQueen (2008) with respect to lexical
access; this model extends it to parsing.
Recurrent networks can parse simple sentences
but run into problems when running over more com-
plex datasets. This limitation comes from the unsu-
pervised methods typically used to train them, which
have difficulty scaling to sufficiently large training
sets for more complex constructions. The approach
described in this paper uses a hidden context simi-
lar to that of a recurrent network to inform the pro-
gression of the parse, except that the context is in
terms of random variables with distributions over a
set of explicit syntactic categories. By framing the
variable domains in a linguistically-motivated fash-
ion, the problem of acquisition can be divested from
the problem of processing. This paper then uses the
semi-supervised grammar training of Petrov et al
(2006) in order to develop a simple, accurate model
for broad-coverage parsing independent of scale.
56
p1
x1
q11
q21
q31
q41
p2
x2
q12
q22
q32
q42
p3
x3
q13
q23
q33
q43
p4
x4
q14
q24
q34
q44
p5
x5
q15
q25
q35
q45
p6
x6
q16
q26
q36
q46
p7
x7
q17
q27
q37
q47
f2 f3 f4 f5 f6 f7 f8=D
T
=the
=NP/
NN
=NN
=fund
=+
=S/V
P
=VB
=bou
ght
=S/V
P
=VP/
NP
=DT
=two
=S/V
P
=VP/
NN
=JJ
=re
giona
l
=S/V
P
=VP/
NN
=NN
=ban
ks
=+
=S/R
B
=RB
=toda
y
=+
Figure 2: Parse using Simplified Model
Like Schuler (2009), the incremental parser dis-
cussed here operates in O(n) time where n is the
length of the input. Further, by its incremental na-
ture, this parser is able to run continuously on a
stream of input, which allows any other processes
dependent on the input (such as discourse integra-
tion) to run in parallel regardless of the length of the
input.
5 Computational Benefit
Due to the decreased number of decisions required
by this simplified model, it is substantially faster
than previous similar models. To test this speed in-
crease, the simplified model was compared with that
of Schuler (2009). Both parsers used a grammar that
had undergone 5 iterations of the Petrov et al (2006)
split-merge-smooth algorithm as found to be opti-
mal by Petrov and Klein (2007), and both used a
beam-width of 500 elements. Sections 02-21 of the
Wall Street Journal Treebank were used in training
the grammar induction for both parsers according to
Petrov et al (2006), and Section 23 was used for
evaluation. No tuning was done as part of the trans-
form to a sequence model. Speed results can be seen
in Table 1. While the speed is not state-of-the-art in
the field of parsing at large, it does break new ground
for factored sequence model parsers.
To test the accuracy of this parser, it was com-
pared using varying beam-widths to the Petrov and
Klein (2007) and Roark (2001) parsers. With the
exception of the Roark (2001) parser, all parsers
used 5 iterations of the Petrov et al (2006) split-
System Sec/Sent
Schuler 2009 74
Current Model 12
Table 1: Speed comparison with an unfactored proba-
bilistic sequence model using a beam-width of 500 ele-
ments
System P R F
Roark 2001 86.6 86.5 86.5
Current Model (500) 86.6 87.3 87.0
Current Model (2000) 87.8 87.8 87.8
Current Model (5000) 87.8 87.8 87.8
Petrov Klein (Binary) 88.1 87.8 88.0
Petrov Klein (+Unary) 88.3 88.6 88.5
Table 2: Accuracy comparison with state-of-the-art mod-
els. Numbers in parentheses are number of parallel acti-
vated hypotheses
merge-smooth algorithm, and the training and test-
ing datasets remained the same. These results may
be seen in Table 2. Note that the Petrov and Klein
(2007) parser allows unary branching within the
phrase structure, which is not well-defined under the
right-corner transform. To obtain a fair comparison,
it was also run with strict binarization. The cur-
rent approach achieves comparable accuracy to the
Petrov and Klein (2007) parser assuming a strictly
binary-branching phrase structure.
57
6 Conclusion
The primary goal of this paper was to demonstrate
that a cognitively-motivated factoring of an exist-
ing probabilistic sequence model parser (Schuler,
2009) is not only more attractive from a modelling
perspective but also more efficient. Such factor-
ing yields a much slimmer model where every vari-
able has cognitive correlates to working memory el-
ements. This also renders several transition prob-
abilities deterministic and the ensuing representa-
tional economy leads to a 5-fold increase in pars-
ing speed. The results shown here suggest cognitive
modelling can lead to computational benefits.
References
Alfred V. Aho and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation and Compiling; Volume. I:
Parsing. Prentice-Hall, Englewood Cliffs, New Jersey.
P. D. Allopenna, J. S. Magnuson, and M. K. Tanenhaus.
1998. Tracking the time course of spoken word recog-
nition using eye movements: evidence for continuous
mapping models. Journal of Memory and Language,
38:419?439.
G. T. M. Altmann and Y. Kamide. 1999. Incremental
interpretation at verbs: restricting the domain of sub-
sequent reference. Cognition, 73:247?264.
Richard Bellman. 1957. Dynamic Programming.
Princeton University Press, Princeton, NJ.
Thomas G. Bever. 1970. The cognitive basis for lin-
guistic structure. In J. ?R. Hayes, editor, Cognition and
the Development of Language, pages 279?362. Wiley,
New York.
Matthew Botvinick. 2007. Multilevel structure in behav-
ior and in the brain: a computational model of fusters
hierarchy. Philosophical Transactions of the Royal So-
ciety, Series B: Biological Sciences, 362:1615?1626.
Nelson Cowan. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage ca-
pacity. Behavioral and Brain Sciences, 24:87?185.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
Jeffrey L. Elman. 1991. Distributed representations,
simple recurrent networks, and grammatical structure.
Machine Learning, 7:195?225.
John Hale. 2001. A probabilistic earley parser as a psy-
cholinguistic model. In Proceedings of the Second
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 159?166,
Pittsburgh, PA.
Jonathan C Horton and Daniel L Adams. 2005. The cor-
tical column: a structure without a function. Philo-
sophical Transactions of the Royal Society of London
- Series B: Biological Sciences, 360(1456):837?862.
Daniel Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science: A Multidisciplinary Journal, 20(2):137?194.
Roger Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106(3):1126?1177.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
William D. Marslen-Wilson. 1975. Sentence per-
ception as an interactive parallel process. Science,
189(4198):226?228.
Marshall R. Mayberry, III and Risto Miikkulainen. 2003.
Incremental nonmonotonic parsing through semantic
self-organization. In Proceedings of the 25th Annual
Conference of the Cognitive Science Society, pages
798?803, Boston, MA.
James L. McClelland and Jeffrey L. Elman. 1986. The
trace model of speech perception. Cognitive Psychol-
ogy, 18:1?86.
Dennis Norris and James M. McQueen. 2008. Shortlist
b: A bayesian model of continuous speech recognition.
Psychological Review, 115(2):357?395.
Dennis Norris. 1994. Shortlist: A connectionist model
of continuous speech recognition. Cognition, 52:189?
234.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
NAACL HLT 2007, pages 404?411, Rochester, New
York, April. Association for Computational Linguis-
tics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics (COLING/ACL?06).
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-coverage incremental
parsing using human-like memory constraints. Com-
putational Linguistics, 36(1):1?30.
William Schuler. 2009. Parsing with a bounded stack
using a model-based right-corner transform. In Pro-
ceedings of NAACL/HLT 2009, NAACL ?09, pages
344?352, Boulder, Colorado. Association for Compu-
tational Linguistics.
58
Michael K. Tanenhaus, Michael J. Spivey-Knowlton,
Kathy M. Eberhard, and Julie E. Sedivy. 1995. Inte-
gration of visual and linguistic information in spoken
language comprehension. Science, 268:1632?1634.
A Grammar Formulation
Given D memory elements indexed by d (see Sec-
tion 2.2) and a PCFG ?G, the probability ?(k)Ts,d of a
tree rooted at a left or right sibling s ? {L,R} of
category c? ? C requiring d ? 1..D memory ele-
ments is defined recursively over paths of increasing
length k:
P?(0)Ts,d
(1 | c?) def= 0 (25)
P?(k)TL,d
(1 | c?) def=
?
x?
P?G(c? ? x?)
+
?
c?0,c?1
P?G(c? ? c?0 c?1)
? P?(k?1)TL,d (1 | c?0) ? P?(k?1)TR,d(1 | c?1)
(26)
P?(k)TR,d
(1 | c?) def=
?
x?
P?G(c? ? x?)
+
?
c?0,c?1
P?G(c? ? c?0 c?1)
? P?(k?1)TL,d+1(1 | c?0) ? P?(k?1)TR,d(1 | c?1)
(27)
Note that the center embedding depth d increases
only for left children of right children. This is be-
cause in a binary branching structure, center embed-
dings manifest as zigzags. Since the model is also
sensitive to the depth d of each decomposition, the
side- and depth-specific probabilities of ?GL,d and
?GR,d are defined as follows:
P?GL,d(c? ? x?)
def=
P?G(c? ? x?)
P?(?)TL,d
(1 | c?)
(28)
P?GR,d(c? ? x?)
def=
P?G(c? ? x?)
P?(?)TR,d
(1 | c?)
(29)
P?GL,d(c? ? c?0 c?1)
def= P?G(c? ? c?0 c?1)
? P?(?)TL,d(1 | c?0) ? P?(?)TR,d(1 | c?1)
? P?(?)TL,d(1 | c?)
?1 (30)
P?GR,d(c? ? c?0 c?1)
def= P?G(c? ? c?0 c?1)
? P?(?)TL,d+1(1 | c?0) ? P?(?)TR,d(1 | c?1)
? P?(?)TR,d(1 | c?)
?1 (31)
The model will also need an expected count
E?G?,d(c?
?? c?? ...) of the given child constituent
c?? dominating a prefix of constituent c?. Expected
versions of these counts may later be used to derive
probabilities of memory store state transitions (see
Sections 2.3, 3).
E?G?,d(c?
0? c? ...) def=
?
x?
P?GR,d(c? ? x?)
(32)
E?G?,d(c?
1? c?0 ...) def=
?
c?1
P?GR,d(c? ? c?0 c?1)
(33)
E?G?,d(c?
k? c??0 ...) def=
?
c??
E?G?,d(c?
k?1? c?? ...)
?
?
c??1
P?GL,d(c?? ? c??0 c??1)
(34)
E?G?,d(c?
?? c?? ...) def=
?
?
k=0
E?G?,d(c?
k? c?? ...)
(35)
E?G?,d(c?
+? c?? ...) def=
?
?
k=1
E?G?,d(c?
k? c?? ...)
(36)
Equation 32 gives the probability of a constituent
appearing as an observation, and Equation 33 gives
the probability of a constituent appearing as a left
59
child. Equation 34 extends the previous two equa-
tions to account for a constituent appearing at an ar-
bitrarily deep embedded path of length k. Taking
the sum of all k path lengths (as in Equation 35)
allows the model to account for constituents any-
where in the left progeny of the dominated subtree.
Similarly, Equation 36 gives the expectation that the
constituent is non-immediately dominated by c?. In
practice the infinite sum is estimated to some con-
stant K using value iteration (Bellman, 1957).
60
