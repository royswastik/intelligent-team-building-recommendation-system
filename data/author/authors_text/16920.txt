Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 92?96,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A platform for collaborative semantic annotation
Valerio Basile and Johan Bos and Kilian Evang and Noortje Venhuizen
{v.basile,johan.bos,k.evang,n.j.venhuizen}@rug.nl
Center for Language and Cognition Groningen (CLCG)
University of Groningen, The Netherlands
Abstract
Data-driven approaches in computational
semantics are not common because there
are only few semantically annotated re-
sources available. We are building a
large corpus of public-domain English texts
and annotate them semi-automatically with
syntactic structures (derivations in Com-
binatory Categorial Grammar) and seman-
tic representations (Discourse Representa-
tion Structures), including events, thematic
roles, named entities, anaphora, scope, and
rhetorical structure. We have created a
wiki-like Web-based platform on which a
crowd of expert annotators (i.e. linguists)
can log in and adjust linguistic analyses in
real time, at various levels of analysis, such
as boundaries (tokens, sentences) and tags
(part of speech, lexical categories). The
demo will illustrate the different features of
the platform, including navigation, visual-
ization and editing.
1 Introduction
Data-driven approaches in computational seman-
tics are still rare because there are not many
large annotated resources that provide empiri-
cal information about anaphora, presupposition,
scope, events, tense, thematic roles, named en-
tities, word senses, ellipsis, discourse segmenta-
tion and rhetorical relations in a single formal-
ism. This is not surprising, as it is challenging and
time-consuming to create such a resource from
scratch.
Nevertheless, our objective is to develop a
large annotated corpus of Discourse Representa-
tion Structures (Kamp and Reyle, 1993), com-
prising most of the aforementioned phenomena:
the Groningen Meaning Bank (GMB). We aim to
reach this goal by:
1. Providing a wiki-like platform supporting
collaborative annotation efforts;
2. Employing state-of-the-art NLP software for
bootstrapping semantic analysis;
3. Giving real-time feedback of annotation ad-
justments in their resulting syntactic and se-
mantic analysis;
4. Ensuring kerfuffle-free dissemination of
our semantic resource by considering only
public-domain texts for annotation.
We have developed the wiki-like platform from
scratch simply because existing annotation sys-
tems, such as GATE (Dowman et al 2005), NITE
(Carletta et al 2003), or UIMA (Hahn et al
2007), do not offer the functionality required for
deep semantic annotation combined with crowd-
sourcing.
In this description of our platform, we motivate
our choice of data and explain how we manage it
(Section 2), we describe the complete toolchain
of NLP components employed in the annotation-
feedback process (Section 3), and the Web-based
interface itself is introduced, describing how lin-
guists can adjust boundaries of tokens and sen-
tences, and revise tags of named entities, parts of
speech and lexical categories (Section 4).
2 Data
The goal of the Groningen Meaning Bank is to
provide a widely available corpus of texts, with
deep semantic annotations. The GMB only com-
prises texts from the public domain, whose dis-
tribution isn?t subject to copyright restrictions.
Moreover, we include texts from various genres
and sources, resulting in a rich, comprehensive
92
corpus appropriate for use in various disciplines
within NLP.
The documents in the current version of the
GMB are all in English and originate from four
main sources: (i) Voice of America (VOA), an on-
line newspaper published by the US Federal Gov-
ernment; (ii) the Manually Annotated Sub-Corpus
(MASC) from the Open American National Cor-
pus (Ide et al 2010); (iii) country descriptions
from the CIA World Factbook (CIA) (Central In-
telligence Agency, 2006), in particular the Back-
ground and Economy sections, and (iv) a col-
lection of Aesop?s fables (AF). All these docu-
ments are in the public domain and are thus redis-
tributable, unlike for example the WSJ data used
in the Penn Treebank (Miltsakaki et al 2004).
Each document is stored with a separate file
containing metadata. This may include the lan-
guage the text is written in, the genre, date of
publication, source, title, and terms of use of the
document. This metadata is stored as a simple
feature-value list.
The documents in the GMB are categorized
with different statuses. Initially, newly added doc-
uments are labeled as uncategorized. As we man-
ually review them, they are relabeled as either
accepted (document will be part of the next sta-
ble version, which will be released in regular in-
tervals), postponed (there is some difficulty with
the document that can possibly be solved in the
future) or rejected (something is wrong with the
document form, i.e., character encoding, or with
the content, e.g., it contains offensive material).
Currently, the GMB comprises 70K English
text documents (Table 1), corresponding to 1,3
million sentences and 31,5 million tokens.
Table 1: Documents in the GMB, as of March 5, 2012
Documents VOA MASC CIA AF All
Accepted 4,651 34 515 0 5,200
Uncategorized 61,090 0 0 834 61,924
Postponed 2,397 339 3 1 2,740
Rejected 184 27 4 0 215
Total 68,322 400 522 835 70,079
3 The NLP Toolchain
The process of building the Groningen Meaning
Bank takes place in a bootstrapping fashion. A
chain of software is run, taking the raw text docu-
ments as input. The output of this automatic pro-
cess is in the form of several layers of stand-off
annotations, i.e., files with links to the original,
raw documents.
We employ a chain of NLP components that
carry out, respectively, tokenization and sentence
boundary detection, POS tagging, lemmatization,
named entity recognition, supertagging, parsing
using the formalism of Combinatory Categorial
Grammar (Steedman, 2001), and semantic and
discourse analysis using the framework of Dis-
course Representation Theory (DRT) (Kamp and
Reyle, 1993) with rhetorical relations (Asher,
1993).
The lemmatizer used is morpha (Minnen et al
2001), the other steps are carried out by the C&C
tools (Curran et al 2007) and Boxer (Bos, 2008).
3.1 Bits of Wisdom
After each step in the toolchain, the intermediate
result may be automatically adjusted by auxiliary
components that apply annotations provided by
expert users or other sources. These annotations
are represented as ?Bits of Wisdom? (BOWs): tu-
ples of information regarding, for example, token
and sentence boundaries, tags, word senses or dis-
course relations. They are stored in a MySQL
database and can originate from three different
sources: (i) explicit annotation changes made by
experts using the Explorer Web interface (see Sec-
tion 4); (ii) an annotation game played by non-
experts, similar to ?games with a purpose? like
Phrase Detectives (Chamberlain et al 2008) and
Jeux de Mots (Artignan et al 2009); and (iii) ex-
ternal NLP tools (e.g. for word sense disambigua-
tion or co-reference resolution).
Since BOWs come from various sources, they
may contradict each other. In such cases, a judge
component resolves the conflict, currently by pre-
ferring the most recent expert BOW. Future work
will involve the application of different judging
techniques.
3.2 Processing Cycle
The widely known open-source tool GNU make
is used to orchestrate the toolchain while avoid-
ing unnecessary reprocessing. The need to rerun
the toolchain for a document arises in three sit-
uations: a new BOW for that document is avail-
able; a new, improved version of one of the com-
ponents is available; or reprocessing is forced by
a user via the ?reprocess? button in the Web inter-
face. A continually running program, the ?updat-
93
Figure 1: A screenshot of the web interface, displaying a tokenised document.
ing daemon?, is responsible for calling make for
the right document at the right time. It checks the
database for new BOWs or manual reprocessing
requests in very short intervals to ensure immedi-
ate response to changes experts make via the Web
interface. It also updates and rebuilds the compo-
nents in longer intervals and continuously loops
through all documents, remaking them with the
newest versions of the components. The number
of make processes that can run in parallel is con-
figurable; standard techniques of concurrent pro-
gramming are used to prevent more than one make
process from working simultaneously on the same
document.
4 The Expert Interface
We developed a wiki-like Web interface, called
the GMB Explorer, that provides users access to
the Groningen Meaning Bank. It fulfills three
main functions: navigation and search through the
documents, visualization of the different levels of
annotation, and manual correction of the annota-
tions. We will discuss these functions below.
4.1 Navigation and Search
The GMB Explorer allows navigation through the
documents of the GMB with their stand-off an-
notations (Figure 1). The default order of docu-
ments is based on their size in terms of number
of tokens. It is possible to apply filters to restrict
the set of documents to be shown: showing only
documents from a specific subcorpus, or specifi-
cally showing documents with/without warnings
generated by the NLP toolchain.
The Explorer interface comes with a built-in
search engine. It allows users to pose single- or
multi-word queries. The search results can then
be restricted further by looking for a specific lex-
ical category or part of speech. A more advanced
search system that is based on a semantic lexicon
with lexical information about all levels of anno-
tation is currently under development.
4.2 Visualization
The different visualization options for a document
are placed in tabs: each tab corresponds to a spe-
cific layer of annotation or additional informa-
tion. Besides the raw document text, users can
view its tokenized version, an interactive deriva-
tion tree per sentence, and the semantic represen-
tation of the entire discourse in graphical DRS
format. There are three further tabs in the Ex-
plorer: a tab containing the warnings produced by
the NLP pipeline (if any), one containing the Bits
of Wisdom that have been collected for the docu-
ment, and a tab with the document metadata.
The sentences view allows the user to show or
hide sub-trees per sentence and additional infor-
mation such as POS-tags, word senses, supertags
and partial, unresolved semantics. The deriva-
tions are shown using the CCG notation, gener-
ated by XSLT stylesheets applied to Boxer?s XML
output. An example is shown in Figure 2.
The discourse view shows a fully resolved
semantic representation in the form of a DRS with
Figure 2: An example of a CCG derivation as shown
in GMB Explorer.
94
Figure 3: An example of the semantic representations
in the GMB, with DRSs representing discourse units.
rhetorical relations. Clicking on discourse units
switches the visualization between text and se-
mantic representation. Figure 3 shows how DRSs
are visualized in the Web interface.
4.3 Editing
Some of the tabs in the Explorer interface have an
?edit? button. This allows registered users to man-
ually correct certain types of annotations. Cur-
rently, the user can edit the tokenization view and
on the derivation view. Clicking ?edit? in the to-
kenization view gives an annotator the possibility
to add and remove token and sentence boundaries
in a simple and intuitive way, as Figure 4 illus-
trates. This editing is done in real-time, following
the WYSIWYG strategy, with tokens separated
by spaces and sentences separated by new lines.
In the derivation view, the annotator can change
part-of-speech tags and named entity tags by se-
lecting a tag from a drop-down list (Figure 5).
Figure 4: Tokenization edit mode. Clicking on the
red ??? removes a sentence boundary after the token;
clicking on the green ?+? adds a sentence boundary.
Figure 5: Tag edit mode, showing derivation with par-
tial DRSs and illustrating how to adjust a POS tag.
As the updating daemon is running continu-
ally, the document is immediately reprocessed af-
ter editing so that the user can directly view the
new annotation with his BOW taken into account.
Re-analyzing a document typically takes a few
seconds, although for very large documents it can
take longer. It is also possible to directly rerun
the NLP toolchain on a specific document via the
?reprocess? button, in order to apply the most re-
cent version of the software components involved.
The GMB Explorer shows a timestamp of the last
processing for each document.
We are currently working on developing new
editing options, which allow users to change dif-
ferent aspects of the semantic representation, such
as word senses, thematic roles, co-reference and
scope.
5 Demo
In the demo session we show the functionality of
the various features in the Web-based user inter-
face of the GMB Explorer, which is available on-
line via: http://gmb.let.rug.nl.
We show (i) how to navigate and search
through all the documents, including the refine-
ment of search on the basis of the lexical cate-
gory or part of speech, (ii) the operation of the dif-
ferent view options, including the raw, tokenized,
derivation and semantics view of each document,
and (iii) how adjustments to annotations can be re-
alised in the Web interface. More concretely, we
demonstrate how boundaries of tokens and sen-
tences can be adapted, and how different types of
tags can be changed (and how that affects the syn-
tactic, semantic and discourse analysis).
In sum, the demo illustrates innovation in the
way changes are made and how they improve the
linguistic analysis in real-time. Because it is a
web-based platform, it paves the way for a collab-
orative annotation effort. Currently it is actively
in use as a tool to create a large semantically an-
notated corpus for English texts: the Groningen
Meaning Bank.
95
References
Guillaume Artignan, Mountaz Hascoe?t, and Mathieu
Lafourcade. 2009. Multiscale visual analysis of
lexical networks. In 13th International Confer-
ence on Information Visualisation, pages 685?690,
Barcelona, Spain.
Nicholas Asher. 1993. Reference to Abstract Objects
in Discourse. Kluwer Academic Publishers.
Johan Bos. 2008. Wide-Coverage Semantic Analy-
sis with Boxer. In J. Bos and R. Delmonte, editors,
Semantics in Text Processing. STEP 2008 Confer-
ence Proceedings, volume 1 of Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
J. Carletta, S. Evert, U. Heid, J. Kilgour, J. Robert-
son, and H. Voormann. 2003. The NITE XML
toolkit: flexible annotation for multi-modal lan-
guage data. Behavior Research Methods, Instru-
ments, and Computers, 35(3):353?363.
Central Intelligence Agency. 2006. The CIA World
Factbook. Potomac Books.
John Chamberlain, Massimo Poesio, and Udo Kr-
uschwitz. 2008. Addressing the Resource Bottle-
neck to Create Large-Scale Annotated Texts. In
Johan Bos and Rodolfo Delmonte, editors, Seman-
tics in Text Processing. STEP 2008 Conference Pro-
ceedings, volume 1 of Research in Computational
Semantics, pages 375?380. College Publications.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically Motivated Large-Scale NLP with
C&C and Boxer. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 33?36, Prague,
Czech Republic.
Mike Dowman, Valentin Tablan, Hamish Cunning-
ham, and Borislav Popov. 2005. Web-assisted an-
notation, semantic indexing and search of television
and radio news. In Proceedings of the 14th Interna-
tional World Wide Web Conference, pages 225?234,
Chiba, Japan.
U. Hahn, E. Buyko, K. Tomanek, S. Piao, J. Mc-
Naught, Y. Tsuruoka, and S. Ananiadou. 2007.
An annotation type system for a data-driven NLP
pipeline. In Proceedings of the Linguistic Annota-
tion Workshop, pages 33?40, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: a community resource for and by the
people. In Proceedings of the ACL 2010 Confer-
ence Short Papers, pages 68?73, Stroudsburg, PA,
USA.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. The Penn Discourse Tree-
bank. In In Proceedings of LREC 2004, pages
2237?2240.
Guido Minnen, John Carroll, and Darren Pearce.
2001. Applied morphological processing of en-
glish. Journal of Natural Language Engineering,
7(3):207?223.
Mark Steedman. 2001. The Syntactic Process. The
MIT Press.
96
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 301?309,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UGroningen: Negation detection with Discourse Representation Structures
Valerio Basile and Johan Bos and Kilian Evang and Noortje Venhuizen
{v.basile,johan.bos,k.evang,n.j.venhuizen}@rug.nl
Center for Language and Cognition Groningen (CLCG)
University of Groningen, The Netherlands
Abstract
We use the NLP toolchain that is used to con-
struct the Groningen Meaning Bank to address
the task of detecting negation cue and scope,
as defined in the shared task ?Resolving the
Scope and Focus of Negation?. This toolchain
applies the C&C tools for parsing, using the
formalism of Combinatory Categorial Gram-
mar, and applies Boxer to produce seman-
tic representations in the form of Discourse
Representation Structures (DRSs). For nega-
tion cue detection, the DRSs are converted
to flat, non-recursive structures, called Dis-
course Representation Graphs (DRGs). DRGs
simplify cue detection by means of edge la-
bels representing relations. Scope detection
is done by gathering the tokens that occur
within the scope of a negated DRS. The re-
sult is a system that is fairly reliable for cue
detection and scope detection. Furthermore, it
provides a fairly robust algorithm for detect-
ing the negated event or property within the
scope.
1 Introduction
Nothing is more home to semantics than the phe-
nomenon of negation. In classical theories of mean-
ing all states of affairs are divided in two truth val-
ues, and negation plays a central role to determine
which truth value is at stake for a given sentence.
Negation lies at the heart of deductive inference, of
which consistency checking (searching for contra-
dictions in texts) is a prime example in natural lan-
guage understanding.
It shouldn?t therefore come as a surprise that
detecting negation and adequately representing its
scope is of utmost importance in computational se-
mantics. In this paper we present and evaluate a sys-
tem that transforms texts into logical formulas ? us-
ing the C&C tools and Boxer (Bos, 2008) ? in the
context of the shared task on recognising negation
in English texts (Morante and Blanco, 2012).
We will first sketch the background and the basics
of the formalism that we employ in our analysis of
negation (Section 2). In Section 3 we explain how
we detect negation cues and scope. Finally, in Sec-
tion 4 we present the results obtained in the shared
task, and we discuss them in Section 5.
2 Background
The semantic representations that are used in this
shared task on detecting negation in texts are con-
structed by means of a pipeline of natural language
processing components, of which the backbone is
provided by the C&C tools and Boxer (Curran et
al., 2007). This tool chain is currently in use semi-
automatically for constructing a large semantically
annotated corpus, the Groningen Meaning Bank
(Basile et al, 2012).
The C&C tools are applied for tagging the data
with part-of-speech and super tags and for syntactic
parsing, using the formalism of Combinatory Cate-
gorial Grammar, CCG (Steedman, 2001). The out-
put of the parser, CCG derivations, form the in-
put of Boxer, producing formal semantic representa-
tions in the form of Discourse Representation Struc-
tures (DRSs), the basic meaning-carrying structures
in the framework of Discourse Representation The-
ory (Kamp and Reyle, 1993). DRT is a widely ac-
cepted formal theory of natural language meaning
that has been used to study a wide range of linguistic
301
<<
IPRPNP?v0. ( x1
person(x1)
 ? (v0 @ x1) )
>
sawVBD(S[dcl]\NP)/NP?v0. ?v1. ?v2. (v1 @ ?v3. (v0 @ ?v4. ( e5
see(e5)agent(e5, v3)patient(e5, v4)
 ; (v2 @ e5) ) ) )
<
nothingDTNP?v0. ? ( x1
thing(x1)
 ; (v0 @ x1) )
*
suspiciousJJS[adj]\NP?v0. ?v1. (v0 @ ?v2. ( suspicious(v2)  ; (v1 @ v2) ) )
suspiciousNP\NP?v0. ?v1. (v0 @ ?v2. ( suspicious(v2)  ; (v1 @ v2)) )
nothing suspiciousNP?v0. ? ( x1
thing(x1)suspicious(x1)
 ; (v0 @ x1) )
saw nothing suspiciousS[dcl]\NP?v0. ?v1. (v0 @ ?v2. ? ( x3 e4
thing(x3)suspicious(x3)see(e4)agent(e4, v2)patient(e4, x3)
 ; (v1 @ e4) ) )
I saw nothing suspiciousS[dcl]?v0. ( x1
person(x1)
 ? ? ( x2 e3
thing(x2)suspicious(x2)see(e3)agent(e3, x1)patient(e3, x2)
 ; (v0 @ e3) ) )
..S[dcl]\S[dcl]?v0.v0
I saw nothing suspicious .S[dcl]?v0. ( x1
person(x1)
 ? ? ( x2 e3
thing(x2)suspicious(x2)see(e3)agent(e3, x1)patient(e3, x2)
 ; (v0 @ e3) ) )
Figure 1: CCG derivation and unresolved semantics for the sentence ?I saw nothing suspicious?
phenomena, such as anaphoric pronouns, temporal
relations (Kamp and Reyle, 1993), presuppositions
(Van der Sandt, 1992), abstract anaphora and rhetor-
ical relations (Asher, 1993; Asher and Lascarides,
2003).
A DRS contains two parts: a set of of discourse
referents, and a set of conditions. Negation is repre-
sented in a condition by a unary operator in DRT. As
an example, Figure 1 shows the derivation for one
sentence as produced by the pipeline, illustrating
how lexical semantic entries are used to construct
a DRS for a whole sentence, guided by the syntac-
tic parse tree. Here, machinery of the ?-calculus is
employed to deal with variable renaming when re-
quired.
DRSs are recursive structures by nature. They can
be produced in several formats (in Prolog or XML)
and translated into first-order formulas. The rep-
resentations can also be generated as a set of tu-
ples, forming together a directed graph equivalent
to the original DRS, where discourse referents and
symbols are nodes and predicates and relations are
viewed as labelled edges. These ?flat? Discourse
Representation Graphs, DRGs, are often more suit-
able for certain processing tasks. The tuples also
hold additional information, mapping DRS condi-
tions to surface tokens. This mapping is important
in tasks where surface realisation plays a role. We
also use it in this shared task to get back from com-
plex structures to a flat, token-based annotation of
scope.
3 Method
The shared task aims at detecting negation in text ?
systems are supposed to label tokens that are in the
scope of negation, and also identify the token that
triggered the negation. The basic idea of our method
was to run the existing Boxer system for semantic
analysis, then traverse the produced DRSs, and, on
encountering an embbeded negated DRS, output the
302
tokens associated with this negation, as well as the
token triggering it.
As this isn?t what Boxer is usually asked to do,
it required some bookkeeping adjustments. Boxer?s
anaphora resolution feature was turned off because
it is not necessary for the task and would lead
to unwanted inclusion of antecedents into negation
scopes. Also, its features for representing tense in-
formation and rhetorical relations were not used.
The rest of this section pays a closer look at how
negation cues are detected and how scope is as-
signed to tokens. We address the issues of trans-
lating a formal representation such as DRS into the
format required by the shared task ? a represen-
tation more oriented at the surface form. We sub-
mitted two runs of our system, which both used the
C&C tools and Boxer. For the second run, we added
some postprocessing steps that tune the result to-
wards a higher performance, especially on scope de-
tection. While these postprocessing steps improve
performance, many of them may be specific to the
genre and style of the texts used in the shared task.
3.1 Cue detection
Since Boxer has been developed as a system to
generate full semantic representations, its lexicon
implicitly contains a list of negation cues: those
words giving rise to semantic representations of the
form ?B, where B is the DRS representing the
meaning of the scope of the negation. Key examples
here are determiners and noun phrases (no, none, no-
one), and verb phrase negation (not).
However, negation detection is not the primary
function of Boxer, as it is part of the larger aim of
providing interpretable semantic representation for
English texts, and doing so robustly. So for the cur-
rent task, after investigating the development data
made available by the organisers, Boxer?s lexicon
was revised at a few points to account for particu-
lar negation cues that Boxer originally did not de-
tect. This included the detection of never as negation
cue, as well as words with a negative prefix or suffix
(e.g. inadequate, motionless). These affix negations
were detected using an automatically generated list
of negatively affixed nouns, adjectives and adverbs
from WordNet (Fellbaum, 1998). The list was cre-
ated by means of an algorithm that returns all nouns,
adjectives and adverbs in WordNet that start with
one of a, an, dis, in, il, im, ir, non, non-, un, or end
with one of less, lessness, lessly, and have a direct
antonym such that the lemma form equals the stem
of the affixed negation (i.e., without the affix).
On the other hand, not everything that introduces
a negated DRS in Boxer is a typical negation cue.
A case in point is the quantifier all, which up un-
til the shared task received a semantics similar to
?P?Q.??x(P (x)??Q(x)) in Boxer?s lexicon. As
a consequence, Boxer predicted all to be a nega-
tion cue trigger, in contrast to the shared task gold
standard data. Such instances were replaced by log-
ically equivalent representations (in the case of all:
?P?Q.?x(P (x) ? Q(x))).
In order to obtain the tokens that triggered the
negated DRS, Boxer?s DRG output was used. Oc-
currences of predicates, relations and connectives in
the DRG output carry explicit associations with the
tokens in the input whose lexical entries they come
from. For basic cue detection, the system annotates
as a negation cue those tokens (or affixes) associated
with the connective? (represented in the DRG as the
relation subordinates:neg). Example (1) shows a
part of the DRG?s tuple format that represents the
negation cue ?no?. Argument structure tuples (la-
beled concept and instance) are also shown, cor-
responding to a noun in the negation scope, as in
?no problem?. The first and third columns represent
nodes of the DRG graph (both discourse units in this
example), the second column represents the label of
the edge between the nodes, and the fourth column
shows the token associated with the relation (if any).
(1)
... ... ... ...
k1 subordinates:neg k6 no
k6 concept c1:problem
c1:problem instance k6:x1 problem
... ... ... ...
In this case, the token ?no? is detected as negation
cue because it is associated with the relation subor-
dinates:neg.
In the case of affix negation, ideally only the af-
fix should be associated with the negation tuple, and
the stem with a corresponding instance tuple. How-
ever, since the last column contains tokens, this does
not easily fit into the format. We therefore associate
the whole affix-negated token with the negation tu-
ple and use separate tuples for affix and stem in order
to preserve the information which part of the word
303
is the cue and which part is in the scope of the nega-
tion. The resulting three tuples from a sentence con-
taining the word ?injustice? are shown in the follow-
ing example:
(2)
... ... ... ...
k3 subordinates:neg k4 injustice
k4 concept c2:in:71
k4 concept c3:justice:1
... ... ... ...
The target nodes of the two argument structure tu-
ples (labeled concept because ?injustice? is a noun)
are labeled with the relevant part of the affix-negated
word, and a special ID to indicate the presence of
a prefix or suffix. This information is used by the
script producing the token-based result format. Al-
though multi-word cues, such as neither...nor and on
the contrary, were not correctly predicted as such by
Boxer, no effort was made to include them. Due to
the token-based detection approach, the cue detec-
tion algorithm would have to be severly complicated
to include these cases as one negation cue. Because
of the relatively small frequency of multi-word cues,
we decided not to include special processing steps to
account for them.
The second run includes some postprocessing
steps implemented on top of the basic output. Since
Boxer is not designed to deal with dialogue, inter-
jections were originally ignored as negation cues.
Therefore, the postprocessing script added the word
?no? as a negation cue (with empty scope) when it
occurred as an interjection (tagged ?UH?). It also ex-
cluded negations with the cue ?no? when occurring
as part of the expression ?no doubt? and not imme-
diately preceded by a verb with the lemma ?have?
or ?be? as in ?I have no doubt that...?, which is to be
annotated as a negation. High precision and recall
for cue detection on the training data suggested that
no further processing steps were worth the effort.
3.2 Scope detection
The tokens in the scope of a negation are deter-
mined on the basis of the detected negation cue. It
is associated with the negation connective of some
negated DRS ?B, so the system annotates as scope
all the tokens (and stems in the case of affix nega-
tion) directly or indirectly associated with predicates
and relations inside B. This includes tokens di-
rectly associated with predicates that appear within
the negated DRS, as well as those predicates outside
of the negated DRS whose discourse referent occurs
within the negation scope as the second argument of
a thematic role relation.
<P
RN?v0. <P(
x <1 Np
vNN Np(
ers.o <1(
vnvRs)s0nv <1(
?oN.e Np@><P(
VrNBN Np@><1(
Figure 2: DRS for the sentence ?I saw nothing suspi-
cious?
An example is given in Figure 2, where e.g. the
tokens see and suspicious are associated, respec-
tively, with see(e4) and suspicious(x3). Although
the predicate person(x2) associated with the pro-
noun I occurs outside of the negated DRS, its refer-
ent occurs as an argument within the negated DRS
in Agent(e4, x2) and therefore it is taken to be part
of the scope of the negation. The desired scope is
thus detected, containing the tokens I, saw and sus-
picious.
Again, in the second run some postprocessing
steps were implemented to improve performance.
We observed that the scopes in the manually anno-
tated data were usually continuous, except for nega-
tion cues within them. However, the scopes pro-
duced by the DRS algorithm contained many ?gaps?
between the tokens of the detected scope, due to an
intrinsic feature of the DRS representation. Conven-
tionally, DRSs only explicitly contain content words
(i.e. nouns, verbs, adjectives, adverbs), while func-
tion words, such as determiners, modals and auxil-
iary verbs, are represented e.g. as structural proper-
ties or temporal features, or not at all, as in the case
of the infinitival to. Thus, when retrieving the sur-
face representation of the negated scopes from the
DRSs, not all structural properties can be directly as-
sociated with a surface token and thus not all tokens
required for the scope are retrieved. Because in the
gold standard annotation these function words were
considered part of the negation scope, we designed
an ad hoc mechanism to include them, namely filling
all the gaps that occur in the negation scope (leaving
304
out the negation cue). For the same reason, deter-
miners immediately preceding the detected scopes
were added in postprocessing. Finally, conjunc-
tions were removed from the beginning of negation
scopes, because they were sometimes wrongly rec-
ognized by our pipeline as adverbs.
3.3 Negated event/property detection
Although not among our main goals, we also ad-
dressed the issue of detecting the ?negated event or
property? in negation scopes within factual state-
ments. This is done using a heuristic algorithm that
uses the detected scope, as well as the syntax tree
provided as part of the data.
Since the scope is provided as a set of tokens, the
first step is to identify what we call the scope con-
stituent, i.e. a constituent in the syntax tree that cor-
responds to the scope. This is done by going through
the tokens in the scope from left to right and de-
termining for each token the largest constituent that
starts with this token. The first constituent found in
this way the category of whose root is one of SBAR,
S and VP is taken to be the scope constituent.
In the second step, the scope VP is determined
as the first VP encountered when doing a pre-order,
left-to-right traversal of the scope constituent. The
first verb directly dominated by this VP node deter-
mines how the process continues: (i) For non-factual
modals (e.g. may, must, should), no event/property
is annotated. (ii) For futurity modals (e.g. would,
will, shall), the negated event/property is determined
recursively by taking the first embedded VP as the
new scope VP. (iii) For forms of the verb be, the
algorithm first looks for the head of an embedded
ADJP or NP. If one is found, this is annotated as a
negated property. Otherwise, the verb is assumed to
be a passive auxiliary and the negated event/property
is again determined recursively on the basis of the
first embedded VP. (iv) In all other cases, the verb
itself is annotated as the negated event.
To limit the undesired detection of negated
events/properties outside of factual statements, the
algorithm is not applied to any sentence that con-
tains a question mark.
4 Results
Here we discuss our results on the Shared Task as
compared to the gold standard annotations provided
by (Morante and Daelemans, 2012). The output of
our two runs will be discussed with respect to Task 1.
The first run includes the results of our system with-
out postprocessing steps and in the second run the
system is augmented with the postprocessing steps,
as discussed in Section 3.
During the process of evaluating the results of the
training data, an issue with the method of evaluation
was discovered. In the first version of the evaluation
script precision was calculated using the standard
formula: tptp+fp . However, partial matches are ex-
cluded from this calculation (they are only counted
as false negatives), which means that in the case
of scopes(cue match), precision is calculated as the
number of exact scope matches (true positives) di-
vided by the number of exact scope matches plus
the number of completely wrong instances with no
overlap (false positives). As precision is a measure
for calculating correctly detected instances among
all detected instances, it seems that partial matches
should also be taken into account as detected in-
stance. Therefore, we proposed a new evaluation
method (B): tpsystem , where system includes all de-
tected negations of the current system (including
partial matches). However, this measure may be too
strict as it penalizes a system harder for outputting a
partially correct scope than for outputting no scope
at all.1 This choice between two evils seems to in-
dicate that precision is too simple a measure for tar-
gets where partial matches are possible. Therefore,
in our evaluation of scope detection, we will focus
on the scope tokens measure where there are no par-
tial matches. For cue and negated event/property de-
tection, we use the stricter, but more meaningful B
version. The difference here is almost negligible be-
cause these targets typically have just one token.
4.1 Run 1 (without postprocessing)
Table 1 shows the results of the basic system with-
out postprocessing, with the most important results
for our system highlighted. As we can see, the
basic system performs well on cue detection (F1=
1This was pointed out by an anonymous reviewer.
305
Table 1: Results of the first run (without postprocessing)
Task gold system tp fp fn precision (%) recall (%) F1 (%)
Cues: 264 261 219 33 45 86.90 82.95 84.88
Scopes(cue match): 249 261 32 37 217 46.38 12.85 20.12
Scopes(no cue match): 249 261 32 37 217 46.38 12.85 20.12
Scope tokens(no cue match): 1805 1821 1269 552 536 69.69 70.30 69.99
Negated(no cue match): 173 169 89 76 82 53.94 52.05 52.98
Full negation: 264 261 20 33 244 37.74 7.58 12.62
Cues B: 264 261 219 33 45 83.91 82.95 83.43
Scopes B (cue match): 249 261 32 37 217 12.26 12.85 12.55
Scopes B (no cue match): 249 261 32 37 217 12.26 12.85 12.55
Negated B (no cue match): 173 169 89 76 82 52.66 52.05 52.35
Full negation B: 264 261 20 33 244 7.66 7.58 7.62
Table 2: Results of the second run (with postprocessing)
Task gold system tp fp fn precision (%) recall (%) F1 (%)
Cues: 264 261 224 28 40 88.89 84.85 86.82
Scopes(cue match): 249 256 102 32 147 76.12 40.96 53.26
Scopes(no cue match): 249 256 102 32 147 76.12 40.96 53.26
Scope tokens(no cue match): 1805 2146 1485 661 320 69.20 82.27 75.17
Negated(no cue match): 173 201 111 85 59 56.63 65.29 60.65
Full negation: 264 261 72 28 192 72.00 27.27 39.56
Cues B: 264 261 224 28 40 85.82 84.85 85.33
Scopes B (cue match): 249 256 102 32 147 39.84 40.96 40.39
Scopes B (no cue match): 249 256 102 32 147 39.84 40.96 40.39
Negated B (no cue match): 173 201 111 85 59 55.22 65.29 59.83
Full negation B: 264 261 72 28 192 27.59 27.27 27.43
83.43%), and reasonably well on the detection of
scope tokens (F1= 69.99%).
Note that the results for Scopes(cue match) and
Scopes(no cue match) are the same for our system.
Since we make use of token-based cue detection, the
only cases of partial cue detection are instances of
multi-word cues, which, as discussed above, were
not accounted for in our system. In these cases, the
part of the cue that is not detected has a large chance
of becoming part of the scope of the cue that is de-
tected due to collocation. So, we hypothesize that
Scopes(cue match) and Scopes(no cue match) are the
same because in all cases of partial cue detection, the
scope incorrectly contains part of the gold-standard
cue, which affects both measures negatively.
There is a large discrepancy between the detec-
tion of scope tokens and the detection of com-
plete scopes, as the latter is low on both precision
(12.26%) and recall (12.85%). The relatively high
precision and recall for scope tokens (69.69% and
70.30%, respectively) suggests that there are many
cases of partial scope detection, i.e. cases where the
scope is either under- or overdetected with respect
to the gold standard scope. Since the postprocessing
steps for scope detection were developed to reduce
exactly this under- and overdetection, we expect that
the results for the second run are significantly better.
The same holds for negated/event property detection
(F1= 52.98%) since it uses the results from scope
detection.
4.2 Run 2 (with postprocessing)
Table 2 reports the results of the extended system,
which extends the basic system with postprocessing
steps for cue detection and especially for scope de-
tection. The postprocessing steps indeed result in
higher precision and recall for all tasks, except for
Scope tokens, which shows a negligible decrease in
precision (from 69.69% to 69.20%). This suggests
that there are more cases of overdetected scopes
than underdetected scopes, because the number of
wrongly detected tokens (false positives) increased
while the number of undetected scope tokens (false
negatives) decreased. This is probably due to the
gap-filling mechanism that was implemented as a
postprocessing step for scope detection, generaliz-
306
ing that all scopes should be continuous. We will
elaborate more on this point in the discussion in Sec-
tion 5.
As expected, the detection of complete scopes
shows the highest increase in F1 score (from 12.55%
to 40.39%). This indicates that the postprocessing
steps effectively targeted the weak points of the ba-
sic system.
While there are no postprocessing steps for
negated event or property detection, the F1 score for
this task also increases (from 52.35% to 59.83%),
as expected, due to the improvement in scope detec-
tion.
5 Discussion
Overall, we can say that both of our systems perform
well on cue detection, with a small increase when in-
cluding the postprocessing steps. This was expected
since the postprocessing for cue detection targeted
only two specific types of cues, namely, interjections
and occurrences of ?no doubt?. The scope detection
benefits consideraby from adding the postprocessing
steps, as was their main goal. In the final results of
the shared task, run 2 of our system ended second
out of five in the open track, while run 1 was ranked
last. We will here discuss some points that deserve
special attention.
5.1 Affix Negation
As discussed above, affix negations received a spe-
cial treatment because they were not originally de-
tected as negation cues in Boxer. In the DRS, the
token containing the affixed negation cue is associ-
ated with two predicates, representing the negative
affix and the negated stem. The algorithm secures
that only the affix is annotated as the negation cue
and that the negated stem is annotated as part of the
scope. An example of a sentence containing affix
negation is shown in (3) (cardboard 31).2
(3) a. [You do yourself an] in[justice]. gold
b. You do yourself an in[justice]. run1
c. You do yourself [an] in[justice]. run2
2In the following, boldfaced tokens represent the negation
cues, brackets embed the scope and underlining signifies the
negated event or property (subscripts added in case of multiple
negation cues).
Table 3: Results of negated event/property detection on
gold standard cue and scope annotation
Task prec.(%) rec.(%) F1(%)
Negated (no cue match): 64.06 76.88 69.89
Negated B (no cue match): 59.71 76.88 67.22
Note that in neither of the runs the complete scope
from the gold standard is detected, although post-
processing increases the recall of scope tokens by
adding the determiner ?an? to the scope of the nega-
tion. However, examples like this are not unambigu-
ous with respect to their negation scope. For ex-
ample, the sentence in (3) can be interpreted in two
ways: ?It is not the case that you do yourself (some-
thing that is) justice? and ?It is the case that you do
yourself (something that is) not justice?. While the
gold standard annotation assumes the former, wide-
scope reading, our system predicts the narrow scope
reading for the negation. The narrow scope read-
ing can be motivated by means of Grice?s Maxim of
Manner (Grice, 1975); the choice of an affix nega-
tion instead of a verbal negation signals a narrow
scope, because in case a wide scope negation is in-
tended, a verbal negation would be more perspicu-
ous. Thus, the difference in the output of our sys-
tem and the gold standard annotation is in this case
caused by a different choice in disambiguating nega-
tion scope, rather than by a shortcoming of the de-
tection algorithm.
5.2 Negated event/property detection
Although correct detection of the negated event or
property was not our prime concern, the results
obtained with our algorithm were quite promising.
Among the systems participating in the closed track
of task 1, our extended system is ranked third out
of seven for negated event/property detection even
though the performance on scope detection is lower
than all of the other systems in this track. Since
negated event/property detection depends on the de-
tected scope, it seems that our heuristic algorithm
for detecting the negated event/property is very ro-
bust against noisy input. The performance of the de-
tection algorithm on the gold-standard annotation of
scopes is shown in Table 3. Although we cannot
compare these results to the performance of other
systems on the gold standard data, it should be noted
307
that the results shown here are unmatched by the
test results of any other system. It would therefore
be worthwile for future work to refine the negated
event/property detection algorithm outlined here.
5.3 Postprocessing
The results for the two versions of our system
showed that the postprocessing steps implemented
in the extended system improved the results consid-
erably, especially for scope detection. Example (4)
(cardboard 62) shows the effect of postprocessing on
the detection of scopes for negative interjections.
(4) a. ?No1, [I saw]2 nothing2.? gold
b. ?[No], [I saw] nothing.? run1
c. ?[No1, I saw]2 nothing2.? run2
In Run 1, the system correctly detects the cue ?noth-
ing? and the event ?saw?, although the detected
scope is too wide due to an error in the output of
the parser we used. In Run 2, postprocessing also
correctly recognizes the interjection ?no? as a nega-
tion cue. Gap filling in this case makes the scope
overdetection worse by also adding the comma to
the scope. A similar case of the overdetection of
scope is shown in (5) (cardboard 85).
(5) a. [The box] is a half-pound box of honey-
dew tobacco and [does] not [help us in
any way]. gold
b. [The box] is a half-pound box of hon-
eydew tobacco and does not [help us in
any way]. run1
c. [The box is a half-pound box of honey-
dew tobacco and does] not [help us in
any way]. run2
Note that in Run 1 the first part of the coordinated
structure is correctly excluded from the scope of
the negation, but the auxiliary ?does? is incorrectly
not counted as scope. The gap-filling mechanism
then adds the intermediary part to the scope of the
negation, resulting in an increase in recall for scope
tokens detection (since ?does? is now part of the
scope) but a lower precision because of the overgen-
eration of the coordinated part.
Nevertheless, the increased precision and recall
for scope detection can mainly be ascribed to the
gap-filling mechanism implemented in the postpro-
cessing steps for scope detection. As discussed
above, the presence of gaps in the original output
is due to the non-sequential nature of the DRT rep-
resentation and the fact that function words are not
directly associated with any element in the represen-
tations. This suggests that future work on surface re-
alisation from DRSs should focus on translating the
structural properties of DRSs into function words.
5.4 Differences between texts
We noted that there was a difference between the
performance on text 1 (The Adventure of the Red
Circle) and text 2 (The Adventure of the Cardboard
Box). The results for text 2 were overall higher than
the results for text 1 (except for a 1% decline in re-
call for Full negation). There was a higher scope
precision for text 2 and after the postprocessing steps
an even larger difference was found for scope de-
tection (15% versus 44% increase in F1 score for
Scopes). We hypothesize that this difference may be
due to a higher number of multiword expressions in
text 1 (7 vs. 2) and to the fact that text 1 seems to
have more scopes containing gaps. This latter ob-
servation is supported by the fact that gap filling re-
sults in more overgeneration (more false positives),
which is reflected in the ratios of false positives in
text 1 (38%) and text 2 (27%). Thus, while the post-
processing steps improve performance, they seem to
be genre and style dependent. This motivates further
development of the ?clean?, theoretically motivated
version of our system in order to secure domain-
independent broad coverage of texts, which is the
goal of the Groningen Meaning Bank project.
6 Conclusion
Participating in this shared task on negation detec-
tion gave us a couple of interesting insights into our
natural language processing pipeline that we are de-
veloping in the context of the Groningen Meaning
Bank. It also showed that it is not easy to trans-
fer the information about negation from a formal,
logical representation of scope to a theory-neutral
surface-oriented approach. The results were in line
with what we expected beforehand, with the highest
loss appearing in the awkward translation from one
formalism to another.
308
References
Nicholas Asher and Alex Lascarides. 2003. Logics of
conversation. Studies in natural language processing.
Cambridge University Press.
Nicholas Asher. 1993. Reference to Abstract Objects in
Discourse. Kluwer Academic Publishers.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically an-
notated corpus. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?12). To appear.
Johan Bos. 2008. Wide-Coverage Semantic Analysis
with Boxer. In J. Bos and R. Delmonte, editors, Se-
mantics in Text Processing. STEP 2008 Conference
Proceedings, volume 1 of Research in Computational
Semantics, pages 277?286. College Publications.
James Curran, Stephen Clark, and Johan Bos. 2007. Lin-
guistically Motivated Large-Scale NLP with C&C and
Boxer. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 33?36, Prague, Czech Republic.
Christiane Fellbaum, editor. 1998. WordNet. An Elec-
tronic Lexical Database. The MIT Press.
H. P. Grice. 1975. Logic and conversation. In P. Cole
and J. L. Morgan, editors, Syntax and Semantics: Vol.
3: Speech Acts, pages 41?58. Academic Press, San
Diego, CA.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic; An Introduction to Modeltheoretic Semantics of
Natural Language, Formal Logic and DRT. Kluwer,
Dordrecht.
Roser Morante and Eduardo Blanco. 2012. *SEM 2012
Shared Task: Resolving Scope and Focus of Negation.
In Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics (*SEM 2012), Mon-
treal, Canada. To appear.
Roser Morante and Walter Daelemans. 2012.
ConanDoyle-neg: Annotation of negation in Conan
Doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?12). To appear.
Mark Steedman. 2001. The Syntactic Process. The MIT
Press.
Rob Van der Sandt. 1992. Presupposition Projection as
Anaphora Resolution. Journal of Semantics, 9:333?
377.
309
