First Joint Conference on Lexical and Computational Semantics (*SEM), pages 571?574,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
JU_CSE_NLP: Multi-grade Classification of Semantic Similarity  
Between Text Pairs 
 
    Snehasis Neogi1, Partha Pakray2, Sivaji Bandyopadhyay1            Alexander Gelbukh 
                           1Computer Science & Engineering Department                        Center for Computing Research 
                           Jadavpur University, Kolkata, India                                 National Polytechnic Institute         
                  2Computer Science & Engineering Department                               Mexico City, Mexico 
                           Jadavpur University, Kolkata, India                                 gelbukh@gelbukh.com 
                        Intern at Xerox Research Centre Europe 
                                          Grenoble, France 
                  {snehasis1981,parthapakray}@gmail.com 
            sbandyopadhyay@cse.jdvu.ac.in 
 
Abstract 
This article presents the experiments car-
ried out at Jadavpur University as part of 
the participation in Semantic Textual Si-
milarity (STS) of Task 6 @ Semantic 
Evaluation Exercises (SemEval-2012). 
Task-6 of SemEval- 2012 focused on se-
mantic relations of text pair. Task-6 pro-
vides five different text pair files to 
compare different semantic relations and 
judge these relations through a similarity 
and confidence score. Similarity score is 
one kind of multi way classification in the 
form of grade between 0 to 5. We have 
submitted one run for the STS task. Our 
system has two basic modules - one deals 
with lexical relations and another deals 
with dependency based syntactic relations 
of the text pair. Similarity score given to a 
pair is the average of the scores of the 
above-mentioned modules. The scores 
from each module are identified using rule 
based techniques. The Pearson Correlation 
of our system in the task is 0.3880. 
1 Introduction 
Task-61 [1] of SemEval-2012 deals with seman-
tic similarity of text pairs. The task is to find the 
similarity between the sentences in the text pair 
(s1 and s2) and return a similarity score and an 
optional confidence score. There are five datasets 
                                                          
1 http://www.cs.york.ac.uk/semeval-2012/task6/ 
in the test data and with tab separated text pairs. 
The datasets are as follows: 
 
? MSR-Paraphrase, Microsoft Research Pa-
raphrase Corpus (750 pairs of sentences.) 
? MSR-Video, Microsoft Research Video De-
scription Corpus (750 pairs of sentences.) 
? SMTeuroparl: WMT2008 development data-
set (Europarl section) (459 pairs of sen-
tences.)  
? SMTnews: news conversation sentence pairs 
from WMT.(399 pairs of sentences.) 
? OnWN: pairs of sentences where the first 
comes from Ontonotes and the second from a 
WordNet definition. (750 pairs of sentences.) 
 
Similarity score ranges from 0 to 5 and confi-
dence score from 0 to 100. An s1-s2 pair gets a 
similarity score of 5 if they are completely 
equivalent. Similarity score 4 is allocated for 
mostly equivalent s1-s2 pair. Similarly, score 3 is 
allocated for roughly equivalent pair. Score 2, 1 
and 0 are allocated for non-equivalent details 
sharing, non-equivalent topic sharing and totally 
different pairs respectively. Major challenge of 
this task is to find the similarity score based simi-
larity for the text pair. Generally text entailment 
tasks refer whether sentence pairs are entailed or 
not: binary classification (YES, NO) [2] or multi-
classification (Forward, Backward, bidirectional 
or no entailment) [3][4]. But multi grade classifi-
cation of semantic similarity assigns a score to 
the sentence pair. Our system considers lexical 
and dependency based syntactic measures for 
semantic similarity. Similarity scores are the ba-
sic average of these module scores. A subsequent 
571
section describes the system architecture. Section 
2 describes JU_NLP_CSE system for STS task. 
Section 3 describes evaluation and experimental 
results. Conclusions are drawn in Section 4.  
2 System Architecture  
The system of Semantic textual similarity task 
has two main modules: one is lexical module and 
another one is dependency parsing based syntac-
tic module. Both these module have some pre-
processing tasks such as stop word removal, co-
reference resolution and dependency parsing etc. 
Figure 1 displays the architecture of the system.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: System Architecture 
2.1 Pre-processing Module 
The system separates the s1-s2 sentence pairs 
contained in the different STS task datasets. 
These separated pairs are then passed through the 
following sub modules: 
i. Stop word Removal: Stop words are removed 
from s1 - s2 sentence pairs. 
ii. Co-reference: Co-reference resolutions are 
carried out on the datasets before passing through 
the TE module. The objective is to increase the 
score of the entailment percentage. A word or 
phrase in the sentence is used to refer to an entity 
introduced earlier or later in the discourse and 
both having same things then they have the same 
referent or co reference. When the reader must 
look back to the previous context, reference is 
called "Anaphoric Reference". When the reader 
must look forward, it is termed "Cataphoric Ref-
erence". To address this problem we used a tool 
called JavaRAP2 (A java based implementation 
of Anaphora Procedure (RAP) - an algorithm by 
Lappin and Leass (1994)). 
iii. Dependency Parsing: Separated s1 ? s2 sen-
tences are parsed using Stanford dependency 
parser3 to produce the dependency relations in 
the texts. These dependency relations are used 
for WordNet based syntactic matching.     
2.2 Lexical Matching Module 
In this module the TE system calculates different 
matching scores such as N ? Gram match, Text 
Similarity, Chunk match, Named Entity match 
and POS match.  
 
i. N-Gram Match module: The N-Gram match 
basically measures the percentage match of the 
unigram, bigram and trigram of hypothesis 
present in the corresponding text. These scores 
are simply combined to get an overall N ? Gram 
matching score for a particular pair.  
 
ii. Chunk Match module: In this sub module 
our system evaluates the key NP-chunks of both 
text (s1) and hypothesis (s2) using NP Chunker 
v1.13 (The University of Sheffield). The hypo-
thesis NP chunks are matched in the text NP 
chunks. System calculates an overall value for 
the chunk matching, i.e., number of text NP 
chunks that match the hypothesis NP chunks. If 
the chunks are not similar in their surface form 
then our system goes for wordnet synonyms 
matching for the words and if they match in 
wordnet synsets information, it will be encoun-
tered as a similar chunk. WordNet [5] is one of 
most important resource for lexical analysis. The 
WordNet 2.0 has been used for WordNet based 
chunk matching. The API for WordNet Search-
ing (JAWS)4 is an API that provides Java appli-
cations with the ability to retrieve data from the 
WordNet synsets. 
iii. Text Similarity Module: System takes into 
consideration several text similarities calculated 
                                                          
2 http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html 
3 http://www.dcs.shef.ac.uk/~mark/phd/software/ 
4 http://lyle.smu.edu/~tspell/jaws/index.html 
 
 
572
over the s1-s2 pair. These text similarity values 
are summed up to produce a total score for a par-
ticular s1-s2 pair. Major Text similarity measures 
that our system considers are: 
 
? Cosine Similarity 
? Lavenstine Distance 
? Euclidean Distance 
? MongeElkan Distance 
? NeedlemanWunch Distance 
? SmithWaterman Distance 
? Block Distance 
? Jaro Similarity 
? MatchingCoefficient Distance 
? Dice Similarity 
? OverlapCoefficient 
? QGrams Distance 
 
iv. Named Entity Matching: It is based on the 
detection and matching of Named Entities in the 
s1-s2 pair. Stanford Named Entity Recognizer5 is 
used to tag the named entities in both s1 and s2. 
System simply maps the number of hypothesis 
(s2) NEs present in the text (s1). A score is allo-
cated for the matching. 
 
NE_match = (Number of common NEs in Text 
and Hypothesis) / (Number of NE in Hypothesis). 
 
v. Part ?of ? Speech (POS) Matching: This 
module basically deals with matching the com-
mon POS tags between s1 and s2 sentences. 
Stanford POS tagger6 is used to tag the part of 
speech in both s1 and s2. System matches the 
verb and noun POS words in the hypothesis that 
match in the text. A score is allocated based on 
the number of POS matching. 
 
POS_match = (Number of common verb and 
noun POS in Text and Hypothesis) / (Total num-
ber of verb and noun POS in hypothesis). 
 
System calculates the sum of the entire sub mod-
ule (modules described in section 2.2) scores and 
forms a single percentage score for the lexical 
matching. This score is then compared with some 
predetermined threshold value to assign a final 
lexical score for each pair. If percentage value is 
                                                          
5 http://nlp.stanford.edu/software/CRF-NER.shtml 
6 http://nlp.stanford.edu/software/tagger.shtml 
above 0.80 then lexical score 5 is allocated. If the 
value is between 0.60 to 0.80 then lexical score 4 
is allocated. Similarly, lexical score 3 is allocated 
for percentage score of 0.40 to 0.60 and so on. 
One lexical score is finally generated for each 
text pair.     
2.3. Syntactic Matching Module: 
TE system considers the preprocessed dependen-
cy parsed text pairs (s1 ? s2) and goes for word 
net based matching technique. After parsing the 
sentences, they have some attributes like subject, 
object, verb, auxiliaries and prepositions tagged 
by the dependency parser tag set. System uses 
these attributes for the matching procedure and 
depending on the nature of matching a score is 
allocated to the s1-s2 pair. Matching procedure is 
basically done through comparison of the follow-
ing features that are present in both the text and 
the hypothesis.    
? Subject ? Subject comparison. 
? Verb ? Verb Comparison. 
? Subject ? Verbs Comparison. 
? Object ? Object Comparison. 
? Cross Subject ? Object Comparison. 
? Object ? Verbs Comparison. 
? Prepositional phrase comparison. 
 
Each of these comparisons produces one match-
ing score for the s1-s2 pair that are finally com-
bined with previously generated lexical score to 
generate the final similarity score by taking sim-
ple average of lexical and syntactic matching 
scores. The basic heuristics are as follows: 
(i) If the feature of the text (s1) directly matches 
the same feature of the hypothesis (s2), matching 
score 5 is allocated for the text pair. 
(ii) If the feature of either text (s1) or hypothesis 
(s2) matches with the wordnet synsets of the cor-
responding text (s1) or hypothesis (s2), matching 
score 4 is allocated.     
(iii) If wordnet synsets of the feature of the text 
(s1) match with one of the synsets of the feature 
of the hypothesis (s2), matching score 3 is given 
to the pair. 
(iv) If wordnet synsets of the feature of either 
text (s1) or hypothesis (s2) match with the syn-
sets of the corresponding text (s1) or hypothesis 
(s2) then matching score 2 is allocated for the 
pair. 
573
(v) Similarly if in both the cases match occurs in 
the second level of wordnet synsets, matching 
score 1is allocated. 
(vi) Matching score 0 is allocated for the pair 
having no match in their features. 
After execution of the module, system generates 
some scores. Lexical module generates one lexi-
cal score and wordnet based syntactic matching 
module generates seven matching scores. At the 
final stage of the system all these scores are 
combined and the mean is evaluated on this 
combined score. This mean gives the similarity 
score for a particular s1-s2 pair of different data-
sets of STS task. Optional confidence score is 
also allocated which is basically the similarity 
score multiplied by 10, i.e., if the similarity score 
is 5.22, the confidence score will be 52.2.     
3. Experiments on Dataset and Result  
We have submitted one run in SemEval-2012 
Task 6. The results for Run on STS Test set are 
shown in Table 1. 
 
task6-JU_CSE_NLP-
Semantic_Syntactic_Approach 
Correlations 
ALL    0.3880 
ALLnrm 0.6706 
Mean 0.4111 
MSRpar  0.3427 
MSRvid 0.3549 
SMT-eur 0.4271 
On-WN 0.5298 
SMT-news 0.4034 
Table 1: Results of Test Set 
ALL: Pearson correlation with the gold standard 
for the five datasets and the corresponding rank 
82. 
ALLnrm: Pearson correlation after the system 
outputs for each dataset are fitted to the gold 
standard using least squares and the correspond-
ing rank 86. 
Mean: Weighted mean across the 5 datasets, 
where the weight depends on the number of pairs 
in the dataset and the corresponding rank 76. 
The subsequent rows show the pearson correla-
tion scores for each of the individual datasets. 
 
4. Conclusion 
Our JU_CSE_NLP system for the STS task 
mainly focus on lexical and syntactic approaches. 
There are some limitations in the lexical match-
ing module that shows a correlation that is not 
higher in the range. In case of simple sentences 
lexical matching is helpful for entailment but for 
complex and compound sentences the lexical 
matching module loses its accuracy. Semantic 
graph matching or conceptual graph implementa-
tion can improve the system. That is not consi-
dered in our present work. Machine learning 
tools can be used to learn the system based on the 
features. It can also improve the correlation. In 
future work our system will include semantic 
graph matching and a machine-learning module.  
Acknowledgments 
The work was done under support of the DST 
India-CONACYT Mexico project ?Answer Vali-
dation through Textual Entailment? funded by 
DST, Government of India.  
References  
[1] Eneko Agirre, Daniel Cer, Mona Diab and Aitor 
Gonzalez.  SemEval-2012 Task 6: A Pilot on Se-
mantic Textual Similarity. In Proceedings of the 
6th International Workshop on Semantic Evalua-
tion (SemEval 2012), in conjunction with the First 
Joint Conference on Lexical and Computational 
Semantics (*SEM 2012). (2012) 
[2] Dagan, I., Glickman, O., Magnini, B.: The 
PASCAL Recognising Textual Entailment Chal-
lenge. Proceedings of the First PASCAL Recogniz-
ing Textual Entailment Workshop. (2005). 
[3] H. Shima, H. Kanayama, C.-W. Lee, C.-J. Lin,T. 
Mitamura, S. S. Y. Miyao, and K. Takeda. Over-
view of ntcir-9 rite: Recognizing inference in text. 
In NTCIR-9 Proceedings,2011. 
[4] Pakray, P., Neogi, S., Bandyopadhyay, S., Gel-
bukh, A.: A Textual Entailment System using Web 
based Machine Translation System. NTCIR-9, Na-
tional Center of Sciences, Tokyo, Japan. December 
6-9, 2011. (2011). 
[5] Fellbaum, C.: WordNet: An Electronic Lexical 
Database. MIT Press (1998). 
574
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 689?695,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
JU_CSE_NLP: Language Independent Cross-lingual  
Textual Entailment System 
 
Snehasis Neogi1, Partha Pakray2, Sivaji Bandyopadhyay1, 
Alexander Gelbukh3 
1
Computer Science & Engineering Department 
Jadavpur University, Kolkata, India 
2
Computer Science & Engineering Department 
Jadavpur University, Kolkata, India 
Intern at Xerox Research Centre Europe 
Grenoble, France 
3
Center for Computing Research 
National Polytechnic Institute 
Mexico City, Mexico 
{snehasis1981,parthapakray}@gmail.com 
sbandyopadhyay@cse.jdvu.ac.in 
gelbukh@gelbukh.com 
 
Abstract 
This article presents the experiments car-
ried out at Jadavpur University as part of 
the participation in Cross-lingual Textual 
Entailment for Content Synchronization 
(CLTE) of task 8 @ Semantic Evaluation 
Exercises (SemEval-2012). The work ex-
plores cross-lingual textual entailment as a 
relation between two texts in different lan-
guages and proposes different measures 
for entailment decision in a four way clas-
sification tasks (forward, backward, bidi-
rectional and no-entailment). We set up 
different heuristics and measures for eva-
luating the entailment between two texts 
based on lexical relations. Experiments 
have been carried out with both the text 
and hypothesis converted to the same lan-
guage using the Microsoft Bing translation 
system. The entailment system considers 
Named Entity, Noun Chunks, Part of 
speech, N-Gram and some text similarity 
measures of the text pair to decide the en-
tailment judgments. Rules have been de-
veloped to encounter the multi way 
entailment issue. Our system decides on 
the entailment judgment after comparing 
the entailment scores for the text pairs. 
Four different rules have been developed 
for the four different classes of entailment. 
The best run is submitted for Italian ? 
English language with accuracy 0.326. 
1 Introduction 
Textual Entailment (TE) (Dagan and Glick-
man, 2004) is one of the recent challenges of 
Natural Language Processing (NLP). The Task 
8 of SemEval-20121 [1] defines a textual en-
tailment system that specifies two major as-
pects: the task is based on cross-lingual 
corpora and the entailment decision must be 
four ways. Given a pair of topically related text 
fragments (T1 and T2) in different languages, 
the CLTE task consists of automatically anno-
tating it with one of the following entailment 
judgments: 
i. Bidirectional (T1 ->T2 & T1 <- T2): the two 
fragments entail each other (semantic equiva-
lence)  
ii. Forward (T1 -> T2 & T1!<- T2): unidirec-
tional  entailment from T1 to T2 . 
iii. Backward (T1! -> T2 & T1 <- T2): unidirec-
tional entailment from T2 to T1.  
iv. No Entailment (T1! -> T2 & T1! <- T2): 
there is no entailment between T1 and T2. 
CLTE (Cross Lingual Textual Entailment) task 
consists of 1,000 CLTE dataset pairs (500 for 
                                                          
1http://www.cs.york.ac.uk/semeval2012/index.php?id=tasks 
689
training and 500 for test) available for the fol-
lowing language combinations: 
     - Spanish/English (spa-eng)  
     - German/English (deu-eng). 
     - Italian/English (ita-eng)  
     - French/English (fra-eng) 
 
Seven Recognizing Textual Entailment (RTE) 
evaluation tracks have already been held: RTE-1 
in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 
2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, 
RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE 
task produces a generic framework for entail-
ment task across NLP applications. The RTE 
challenges have moved from 2 ? way entailment 
task (YES, NO) to 3 ? way task (YES, NO, 
UNKNOWN). EVALITA/IRTE [9] task is simi-
lar to the RTE challenge for the Italian language. 
So far, TE has been applied only in a monolin-
gual setting. Cross-lingual Textual Entailment 
(CLTE) has been proposed ([10], [11], [12]) as 
an extension of Textual Entailment. In 2010, 
Parser Training and Evaluation using Textual 
Entailment [13] was organized by SemEval-2. 
Recognizing Inference in Text (RITE)2 orga-
nized by NTCIR-9 in 2011 is the first to expand 
TE as a 5-way entailment task (forward, back-
ward, bi-directional, contradiction and indepen-
dent) in a monolingual scenario [14].  
We have participated in RTE-5 [15], RTE-6 
[16], RTE-7 [17], SemEval-2 Parser Training 
and Evaluation using Textual Entailment Task 
and RITE [18]. 
Section 2 describes our Cross-lingual Textual 
Entailment system. The various experiments 
carried out on the development and test data sets 
are described in Section 3 along with the results. 
The conclusions are drawn in Section 4. 
2 System Architecture  
Our system for CLTE task is based on a set of 
heuristics that assigns entailment scores to a text 
pair based on lexical relations. The text and the 
hypothesis in a text pair are translated to the 
same language using the Microsoft Bing ma-
chine translation system. The system separates 
the text pairs (T1 and T2) available in different 
languages and preprocesses them. After prepro-
                                                          
2 http://artigas.lti.cs.cmu.edu/rite/Main_Page 
cessing we have used several techniques such as 
Word Overlaps, Named Entity matching, Chunk 
matching, POS matching to evaluate the sepa-
rated text pairs. These modules return a set of 
score statistics, which helps the system to go for 
multi-class entailment decision based on the 
predefined rules. We have submitted 3 runs for 
each language pair for the CLTE task and there 
are some minor differences in the architectures 
that constitute the 3 runs. The three system ar-
chitectures are described in section 2.1, section 
2.2 and section 2.3. 
2.1 System Architecture 1: CLTE Task 
with  Translated English Text  
The system architecture of Cross-lingual textual 
entailment consists of various components such 
as Preprocessing Module, Lexical Similarity 
Module, Text Similarity Module. Lexical Simi-
larity module again is divided into subsequent 
modules like POS matching, Chunk matching 
and Named Entity matching. Our system calcu-
lates these measures twice once considering T1 
as text and T2 as hypothesis and once T2 as text 
and T1 as hypothesis. The mapping is done in 
both directions T1-to-T2 and T2-to-T1 to arrive 
at the appropriate four way entailment decision 
using a set of rules. Each of these modules is 
now being described in subsequent subsections. 
Figure 1 shows our system architecture where 
the text sentence is translated to English. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: System Architecture 
 
CLTE Task Data 
(T1, T2) 
T1.txt 
T2.txt 
 
Translated in 
Eng.  Using Bing 
Translator 
Preprocessing 
(Stop word removal, 
Co referencing) 
 
N-Gram Module 
Preprocessing 
(Stop word removal, 
Co referencing) 
Chunking Module 
Text Similarity Module Named Entity POS Module 
? Lexical Score (S1) 
S1 
? Lexical Score (S2) 
S1 
If (S1>S2) Then Entailment = ?forward? 
If (S1<S2) Then Entailment = ?backward? 
If (S1=S2) or (abs (S1-S2) <Threshold) Then Entailment = ?bidirectional? 
(fra, ita, deu, 
spa language) 
T1- Text 
T2- Hypothesis 
 
T1 ? Hypothesis 
T2 - Text 
 
If (S1=S2 and (S1=S2) <Threshold) Then Entailment = ?no_entailment? 
(English 
language) 
 
690
2.1.1 Preprocessing Module 
The system separates the T1 and T2 pair from 
the CLTE task data. T1 sentences are in differ-
ent languages (In French, Italian, German and 
Spanish) where as T2 sentences are in English. 
Microsoft Bing translator3 API for Bing transla-
tor (microsoft-translator-java-api-0.4-jar-with-
dependencies.jar) is being used to translate the 
T1 text sentences into English. The translated 
T1 and T2 sentences are passed through the two 
sub modules. 
i. Stop word Removal: Stop words are removed 
from the T1 and T2 sentences. 
ii. Co-reference: Co?reference chains are eva-
luated for the datasets before passing them to the 
TE module. The objective is to increase the en-
tailment score after substituting the anaphors 
with their antecedents. A word or phrase in the 
sentence is used to refer to an entity introduced 
earlier or later in the discourse and both having 
same things then they have the same referent or 
co-reference. When the reader must look back to 
the previous context, co-reference is called 
"Anaphoric Reference". When the reader must 
look forward, it is termed "Cataphoric Refer-
ence". To address this problem we used a tool 
called JavaRAP4 (A java based implementation 
of Anaphora Procedure (RAP) - an algorithm by 
Lappin and Leass (1994)). It has been observed 
that the presence of co ? referential expressions 
are very small in sentence based paradigm.   
2.1.2 Lexical Based Textual Entailment 
(TE) Module 
T1 - T2 pairs are the inputs to the system. The 
TE module is executed once by considering T1 
as text and T2 as hypothesis and again by consi-
dering T2 as text and T1 as hypothesis. The 
overall TE module is a collection of several lex-
ical based sub modules.  
i. N-Gram Match module: The N-Gram match 
basically measures the percentage match of the 
unigram, bigram and trigram of hypothesis 
present in the corresponding text. These scores 
are simply combined to get an overall N ? Gram 
matching score for a particular pair. By running 
                                                          
3 http://code.google.com/p/microsoft-translator-java-api/ 
4 http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html 
the module we get two scores, one for T1-T2 
pair and another for T2-T1 pair. 
       
ii. Chunk Similarity module: In this sub mod-
ule our system evaluates the key NP-chunks of 
both text and hypothesis identified using NP 
Chunker v1.15. Then our system checks the 
presence of NP-Chunks of hypothesis in the cor-
responding text. System calculates the overall 
value for the chunk matching, i.e., number of 
text NP-chunks that match with hypothesis NP-
chunks. If the chunks are not similar in their sur-
face form then our system goes for WordNet 
matching for the words and if they match in 
WordNet synsets information, the chunks are 
considered as similar. 
WordNet [19] is one of most important resource 
for lexical analysis. The WordNet 2.0 has been 
used for WordNet based chunk matching. The 
API for WordNet Searching (JAWS)6 is an API 
that provides Java applications with the ability 
to retrieve data from the WordNet database. Let 
us consider the following example taken from 
training data: 
 
T1: Due/JJ to/TO [an/DT error/NN of/IN com-
munication/NN] between/IN [the/DT police/NN] 
? 
T2: On/IN [Tuesday/NNP] [a/DT failed/VBN 
communication/NN] between/IN? 
 
The chunk in T1 [error communication] matches 
with T2 [failed communication] via WordNet 
based synsets information. A weight is assigned 
to the score depending upon the nature of chunk 
matching. 
 
 
 
                   M[i] = Wm[i] * ? / Wc[i] 
Where N= Total number of chunk containing 
hypothesis. 
M[i] = Match Score of the ith  Chunk. 
Wm[i] = Number of words matched in the i
th 
chunk. 
Wc[i] = Total words in the i
th chunk. 
                    1 if surface word matches. 
and ? = 
                ? if matche via WordNet 
                                                          
5 http://www.dcs.shef.ac.uk/~mark/phd/software/ 
6 http://lyle.smu.edu/~tspell/jaws/index.html 
691
System takes into consideration several text si-
milarity measures calculated over the T1-T2 
pair. These text similarity measures are summed 
up to produce a total score for a particular text 
pair. Similar to the Lexical module, text simi-
larity module is also executed for both T1-T2 
and T2-T1 pairs.   
iii. Text Distance Module: The following major 
text similarity measures have been considered 
by our system. The text similarity measure 
scores are added to generate the final text dis-
tance score. 
 
?   Cosine Similarity 
?   Levenshtein Distance 
?   Euclidean Distance 
?   MongeElkan Distance 
?   NeedlemanWunch Distance 
?   SmithWaterman Distance 
?   Block Distance 
?   Jaro Similarity 
?   MatchingCoefficient Similarity 
?   Dice Similarity 
?   OverlapCoefficient 
?   QGrams Distance 
 
iv. Named Entity Matching: It is based on the 
detection and matching of Named Entities in the 
T1-T2 pair. Stanford Named Entity Recognizer7 
(NER) is used to tag the Named Entities in both 
T1 and T2. System simply matches the number 
of hypothesis NEs present in the text. A score is 
allocated for the matching. 
NE_match = (Number of common NEs in Text 
and Hypothesis)/(Number of NEs in Hypothe-
sis). 
v. Part-of-Speech (POS) Matching: This mod-
ule basically deals with matching the common 
POS tags between T1 and T2 pair. Stanford POS 
tagger8 is used to tag the part of speech in both 
T1 and T2. System matches the verb and noun 
POS words in the hypothesis that match in the 
text. A score is allocated based on the number of 
POS matching.  
 
POS_match = (Number of verb and noun                            
POS in Text and Hypothesis)/(Total number of 
verb and noun POS in hypothesis).    
                                                          
7 http://nlp.stanford.edu/software/CRF-NER.shtml 
8 http://nlp.stanford.edu/software/tagger.shtml 
System adds all the lexical matching scores to 
evaluate the total score for a particular T1- T2 
pair, i.e.,  
    Pair1:  (T1 ? Text and T2 ? Hypothesis) 
    Pair2:   (T1 ? Hypothesis and T2 - Text). 
Total lexical score for each pair can be mathe-
matically represented by: 
 
 
where S1 represents the score for the pair with 
T1 as text and T2 as hypothesis while S2 
represents the score from T1 to T2. The figure 2 
shows the sample output values of the TE mod-
ule. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: output values of this module 
 
The system finally compares the above two val-
ues S1 and S2 as obtained from the lexical mod-
ule to go for four-class entailment decision. If 
score S1, i.e., the mapping score with T1 as text 
and T2 as hypothesis is greater than the score 
S2, i.e., mapping score with T2 as text and T1 as 
hypothesis, then the entailment class will be 
?forward?. Similarly if S1 is less than S2, i.e., 
T2 now acts as the text and T1 acts as the hypo-
thesis then the entailment class will be ?back-
ward?. Similarly if both the scores S1 and S2 are 
equal the entailment class will be ?bidirectional? 
(entails in both directions). Measuring ?bidirec-
tional? entailment is much more difficult than 
any other entailment decision due to combina-
tions of different lexical scores. As our system 
produces a final score (S1 and S2) that is basi-
cally the sum over different similarity measures, 
 
692
the tendency of identical S1 ? S2 will be quite 
small. As a result we establish another heuristic 
for ?bidirectional? class. If the absolute value 
difference between S1 and S2 is below the thre-
shold value, our system recognizes the pair as 
?bidirectional? (abs (S1 ? S2) < threshold). This 
threshold has been set as 5 based on observation 
from the training file. If the individual scores S1 
and S2 are below a certain threshold, again set 
based on the observation in the training file, then 
system concludes the entailment class as 
?no_entailment?. This threshold has been set as 
20 based on observation from the training file. 
2.2 System Architecture 2: CLTE Task 
with translated hypothesis  
System Architecture 2 is based on lexical match-
ing between the text pairs (T1, T2) and basically 
measures the same attributes as in the architec-
ture 1. In this architecture, the English hypothe-
sis sentences are translated to the language of 
the text sentence (French, Italian, Spanish and 
German) using the Microsoft Bing Translator. 
The CLTE dataset is preprocessed after separat-
ing the (T1, T2) pairs. Preprocessing module 
includes stop word removal and co-referencing. 
After preprocessing, the system executes the TE 
module for lexical matching between the text 
pairs. This module comprises N-Gram matching, 
Text Similarity, Named Entity Matching, POS 
matching and Chunking. The TE module is ex-
ecuted once with T1 as text and T2 as hypothe-
sis and again with T1 as hypothesis and T2 as 
text. But in this architecture N-Gram matching 
and text similarity modules differ from the pre-
vious architecture. In system architecture 1, the 
N-Gram matching and text similarity values are 
calculated on the English text translated from T1 
(i.e., Text in Spanish, German, French and Ital-
ian languages). In system architecture 2, the Mi-
crosoft Bing translator is used to translate T2 
texts (in English) to different languages (i.e. in 
Spanish, German, French and Italian) and calcu-
late N ? Gram matching and Text Similarity 
values on these (T1 ? newly translated T2) pairs. 
Other lexical sub modules are executed as be-
fore. These lexical matching scores are stored 
and compared according to the heuristic defined 
in section 2.1.    
2.3 System Architecture 3: CLTE task 
using Voting 
The system considers the output of the previous 
two systems (Run 1 from System architecture 1 
and Run 2 from System architecture 2) as input. 
If the entailment decision of both the runs agrees 
then this is output as the final entailment label. 
Otherwise, if they do not agree, the final entail-
ment label will be ?no_entailment?. The voting 
rule can be defined as the ANDing rule where 
logical AND operation of the two inputs are 
considered to arrive at the final evaluation class. 
3 Experiments on Datasets and Results   
Three runs (Run 1, Run 2 and Run 3) for each 
language were submitted for the SemEval-3 
Task 8. The descriptions of submissions for the 
CLTE task are as follows: 
 
? Run1: Lexical matching between text pairs 
(Based on system Architecture ? 1). 
? Run2: Lexical matching between text pairs  
    (Based on System Architecture ? 2). 
? Run3: ANDing Module between Run1 and  
          Run2. (Based on System Architecture ?3). 
 
The CLTE dataset consists of 500 training 
CLTE pairs and 500 test CLTE pairs. The re-
sults for Run 1, Run 2 and Run 3 for each lan-
guage on CLTE Development set are shown in 
Table 1.  
 
Run Name Accuracy 
JU-CSE-NLP_deu-eng_run1 0.284 
JU-CSE-NLP_deu-eng_run2 0.268 
JU-CSE-NLP_deu-eng_run3 0.270 
JU-CSE-NLP_fra-eng_run1 0.290 
JU-CSE-NLP_fra-eng_run2 0.320 
JU-CSE-NLP_fra-eng_run3 0.278 
JU-CSE-NLP_ita-eng_run1 0.302 
JU-CSE-NLP_ita-eng_run2 0.298 
JU-CSE-NLP_ita-eng_run3 0.298 
JU-CSE-NLP_spa-eng_run1 0.270 
JU-CSE-NLP_spa-eng_run2 0.262 
JU-CSE-NLP_spa-eng_run3 0.262 
 
Table 1: Results on Development set 
 
693
The comparison of the runs for different lan-
guages shows that in case of deu-eng language 
pair system architecture ? 1 is useful for devel-
opment data whereas system architecture ? 2 is 
more accurate for test data. For fra-eng language 
pair, system architecture - 2 is more accurate for 
development data whereas voting helps to get 
more accurate results for test data. Similar to the 
deu-eng language pair, ita-eng language pair 
shows same trends, i.e., system architecture ? 1 
is more helpful for development data and system 
architecture ? 2 is more accurate for test data. In 
case of spa-eng language pair system architec-
ture ? 1 is helpful for both development and test 
data. 
 
The results for Run 1, Run 2 and Run 3 for each 
language on CLTE Test set are shown in Table 
2. 
 
Run Name Accuracy 
JU-CSE-NLP_deu-eng_run1 0.262 
JU-CSE-NLP_deu-eng_run2 0.296 
JU-CSE-NLP_deu-eng_run3 0.264 
JU-CSE-NLP_fra-eng_run1 0.288 
JU-CSE-NLP_fra-eng_run2 0.294 
JU-CSE-NLP_fra-eng_run3 0.296 
JU-CSE-NLP_ita-eng_run1 0.316 
JU-CSE-NLP_ita-eng_run2 0.326 
JU-CSE-NLP_ita-eng_run3 0.314 
JU-CSE-NLP_spa-eng_run1 0.274 
JU-CSE-NLP_spa-eng_run2 0.266 
JU-CSE-NLP_spa-eng_run3 0.272 
 
Table 2: Results on Test Set 
4 Conclusions and Future Works 
We have participated in Task 8 of Semeval-2012 
named Cross Lingual Textual Entailment mainly 
based on lexical matching and translation of text 
and hypothesis sentences in the cross lingual 
corpora. Both lexical matching and translation 
have their limitations. Lexical matching is useful 
for simple sentences but fails to retain high ac-
curacy for complex sentences with number of 
clauses. Semantic graph matching or conceptual 
graph is a good substitution to overcome these 
limitations. Machine learning technique is 
another important tool for multi-class entailment 
task. Features can be trained by some machine 
learning tools (such as SVM, Na?ve Bayes or 
Decision tree etc.) with multi-way entailment 
(forward, backward, bi-directional, no-
entailment) as its class. Works have been started 
in these directions. 
Acknowledgments 
The work was carried out under partial support 
of the DST India-CONACYT Mexico project 
?Answer Validation through Textual Entail-
ment? funded by DST, Government of India and 
partial support of the project CLIA Phase II 
(Cross Lingual Information Access) funded by 
DIT, Government of India. 
References  
[1] Negri, M., Marchetti, A., Mehdad, Y., Bentivogli, 
L., and Giampiccolo, D.: Semeval-2012 Task 8: 
Cross-lingual Textual Entailment for Content Syn-
chronization. In Proceedings of the 6th International 
Workshop on Semantic Evaluation (SemEval 2012). 
[2]  Dagan, I., Glickman, O., Magnini, B.: The 
PASCAL Recognising Textual Entailment Chal-
lenge. Proceedings of the First PASCAL Recog-
nizing Textual Entailment Workshop. (2005). 
[3] Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., 
Giampiccolo, D., Magnini, B., Szpektor, I.: The-
Seond PASCAL Recognising Textual Entailment 
Challenge. Proceedings of the Second PASCAL 
Challenges Workshop on Recognising Textual En-
tailment, Venice, Italy (2006). 
[4] Giampiccolo, D., Magnini, B., Dagan, I., Dolan, 
B.: The Third PASCAL Recognizing Textual En-
tailment Challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and 
Paraphrasing, Prague, Czech Republic. (2007). 
[5] Giampiccolo, D., Dang, H. T., Magnini, B., Da-
gan, I., Cabrio, E.: The Fourth PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2008 
Proceedings. (2008) 
[6] Bentivogli, L., Dagan, I., Dang. H.T., Giampicco-
lo, D., Magnini, B.: The Fifth PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2009 
Workshop, National Institute of Standards and 
Technology Gaithersburg, Maryland USA. (2009). 
[7] Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa 
Trang Dang,Danilo Giampiccolo: The Sixth 
PASCAL Recognizing Textual Entailment Chal-
694
lenge. In TAC 2010 Notebook Proceedings. 
(2010) 
[8] Bentivogli, L., Clark, P., Dagan, I., Dang, H., 
Giampiccolo, D.: The Seventh PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2011 
Notebook Proceedings. (2011) 
[9] Bos, Johan, Fabio Massimo Zanzotto, and Marco 
Pennacchiotti. 2009. Textual Entailment at 
EVALITA 2009: In Proceedings of EVALITA 
2009. 
[10] Mehdad, Yashar, Matteo Negri, and Marcello 
Federico.2010. Towards Cross-Lingual Textual 
entailment. In Proceedings of the 11th Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 
NAACL-HLT 2010. LA, USA. 
[11] Negri, Matteo, and Yashar Mehdad. 2010. 
Creating a Bilingual Entailment Corpus through 
Translations with Mechanical Turk: $100 for a 
10-day Rush. In Proceedings of the NAACL-HLT 
2010, Creating Speech and Text Language Data 
With Amazon's Mechanical Turk Workshop. LA, 
USA. 
[12] Mehdad, Yashar, Matteo Negri, Marcello Fede-
rico. 2011. Using Bilingual Parallel Corpora for 
Cross-Lingual Textual Entailment. In Proceedings 
of ACL 2011. 
[13] Yuret, D., Han, A., Turgut, Z.: SemEval-2010 
Task 12: Parser Evaluation using Textual Entail-
ments. Proceedings of the SemEval-2010 Evalua-
tion Exercises on Semantic Evaluation. (2010).  
 
[14] H. Shima, H. Kanayama, C.-W. Lee, C.-J. Lin,T. 
Mitamura, S. S. Y. Miyao, and K. Takeda. Over-
view of ntcir-9 rite: Recognizing inference in text. 
In NTCIR-9 Proceedings,2011. 
[15]  Pakray, P., Bandyopadhyay, S., Gelbukh, A.: 
Lexical based two-way RTE System at RTE-5. Sys-
tem Report, TAC RTE Notebook. (2009) 
 
[16] Pakray, P., Pal, S., Poria, S., Bandyopadhyay, S., 
, Gelbukh, A.: JU_CSE_TAC: Textual Entailment 
Recognition System at TAC RTE-6. System Re-
port, Text Analysis Conference Recognizing Tex-
tual Entailment Track (TAC RTE) Notebook. 
(2010) 
 
[17] Pakray, P., Neogi, S., Bhaskar, P., Poria, S., 
Bandyopadhyay, S., Gelbukh, A.: A Textual En-
tailment System using Anaphora Resolution. Sys-
tem Report. Text Analysis Conference 
Recognizing Textual Entailment Track Notebook, 
November 14-15. (2011) 
 
[18] Pakray, P., Neogi, S., Bandyopadhyay, S., Gel-
bukh, A.: A Textual Entailment System using Web 
based Machine Translation System. NTCIR-9, Na-
tional Center of Sciences, Tokyo, Japan. Decem-
ber 6-9, 2011. (2011) 
 
[19]  Fellbaum, C.: WordNet: An Electronic Lexical 
Database. MIT Press (1998). 
695
