Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 443?449, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI-(SA): Using a ranking algorithm and informal features 
to solve Sentiment Analysis in Twitter 
Yoan Guti?rrez, Andy Gonz?lez, 
Roger P?rez, Jos? I. Abreu 
University of Matanzas, Cuba 
{yoan.gutierrez, roger.perez 
,jose.abreu}@umcc.cu, 
andy.gonzalez@infonet.umcc.cu 
Antonio Fern?ndez Orqu?n, 
Alejandro Mosquera, Andr?s 
Montoyo, Rafael Mu?oz 
University of Alicante, Spain 
antonybr@yahoo.com, 
{amosquera, montoyo, 
rafael}@dlsi.ua.es 
Franc Camara 
Independent Consultant 
USA 
info@franccamara
.com 
 
Abstract 
In this paper, we describe the development 
and performance of the supervised system 
UMCC_DLSI-(SA). This system uses corpora 
where phrases are annotated as Positive, 
Negative, Objective, and Neutral, to achieve 
new sentiment resources involving word 
dictionaries with their associated polarity. As 
a result, new sentiment inventories are 
obtained and applied in conjunction with 
detected informal patterns, to tackle the 
challenges posted in Task 2b of the Semeval-
2013 competition. Assessing the effectiveness 
of our application in sentiment classification, 
we obtained a 69% F-Measure for neutral and 
an average of 43% F-Measure for positive 
and negative using Tweets and SMS 
messages. 
1 Introduction 
Textual information has become one of the most 
important sources of data to extract useful and 
heterogeneous knowledge from. Texts can provide 
factual information, such as: descriptions, lists of 
characteristics, or even instructions to opinion-
based information, which would include reviews, 
emotions, or feelings. These facts have motivated 
dealing with the identification and extraction of 
opinions and sentiments in texts that require 
special attention.  
Many researchers, such as (Balahur et al, 2010; 
Hatzivassiloglou et al, 2000; Kim and Hovy, 
2006; Wiebe et al, 2005) and many others have 
been working on this and related areas. 
Related to assessment Sentiment Analysis (SA) 
systems, some international competitions have 
taken place. Some of those include: Semeval-2010 
(Task 18: Disambiguating Sentiment Ambiguous 
Adjectives 1 ) NTCIR (Multilingual Opinion 
Analysis Task (MOAT 2)) TASS 3  (Workshop on 
Sentiment Analysis at SEPLN workshop) and 
Semeval-2013 (Task 2 4  Sentiment Analysis in 
Twitter) (Kozareva et al, 2013). 
In this paper, we introduce a system for Task 2 
b) of the Semeval-2013 competition. 
1.1 Task 2 Description 
In participating in ?Task 2: Sentiment Analysis in 
Twitter? of Semeval-2013, the goal was to take a 
given message and its topic and classify whether it 
had a positive, negative, or neutral sentiment 
towards the topic. For messages conveying, both a 
positive and negative sentiment toward the topic, 
the stronger sentiment of the two would end up as 
the classification. Task 2 included two sub-tasks. 
Our team focused on Task 2 b), which provides 
two training corpora as described in Table 3, and 
two test corpora: 1) sms-test-input-B.tsv (with 
2094 SMS) and 2) twitter-test-input-B.tsv (with 
3813 Twit messages). 
The following section shows some background 
approaches. Subsequently, in section 3, we 
describe the UMCC_DLSI-(SA) system that was 
used in Task 2 b). Section 4 describes the 
assessment of the obtained resource from the 
Sentiment Classification task. Finally, the 
conclusion and future works are presented in 
section 5. 
2 Background 
The use of sentiment resources has proven to be a 
necessary step for training and evaluating  systems 
that implement sentiment analysis, which also 
                                                 
1 http://semeval2.fbk.eu/semeval2.php 
2 http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
3 http://www.daedalus.es/TASS/ 
4http://www.cs.york.ac.uk/semeval-2013/task2/ 
443
include fine-grained opinion mining (Balahur, 
2011). 
In order to build sentiment resources, several 
studies have been conducted. One of the first is the 
relevant work by (Hu and Liu, 2004) using lexicon 
expansion techniques by adding synonymy and 
antonym relations provided by WordNet 
(Fellbaum, 1998; Miller et al, 1990) Another one 
is the research described by (Hu and Liu, 2004; 
Liu et al, 2005) which obtained an Opinion 
Lexicon compounded by a list of positive and 
negative opinion words or sentiment words for 
English (around 6800 words). 
A similar approach has been used for building 
WordNet-Affect (Strapparava and Valitutti, 2004) 
which expands six basic categories of emotion; 
thus, increasing the lexicon paths in WordNet. 
Nowadays, many sentiment and opinion 
messages are provided by Social Media. To deal 
with the informalities presented in these sources, it 
is necessary to have intermediary systems that 
improve the level of understanding of the 
messages. The following section offers a 
description of this phenomenon and a tool to track 
it. 
2.1 Text normalization 
Several informal features are present in opinions 
extracted from Social Media texts. Some research 
has been conducted in the field of lexical 
normalization for this kind of text. TENOR 
(Mosquera and Moreda, 2012) is a multilingual 
text normalization tool for Web 2.0 texts with an 
aim to transform noisy and informal words into 
their canonical form. That way, they can be easily 
processed by NLP tools and applications. TENOR 
works by identifying out-of-vocabulary (OOV) 
words such as slang, informal lexical variants, 
expressive lengthening, or contractions using a 
dictionary lookup and replacing them by matching 
formal candidates in a word lattice using phonetic 
and lexical edit distances. 
2.2 Construction of our own Sentiment 
Resource  
Having analyzed the examples of SA described in 
section 2, we proposed building our own sentiment 
resource (Guti?rrez et al, 2013) by adding lexical 
and informal patterns to obtain classifiers that can 
deal with Task 2b of Semeval-2013. We proposed 
the use of a method named RA-SR (using Ranking 
Algorithms to build Sentiment Resources) 
(Guti?rrez et al, 2013) to build sentiment word 
inventories based on senti-semantic evidence 
obtained after exploring text with annotated 
sentiment polarity information. Through this 
process, a graph-based algorithm is used to obtain 
auto-balanced values that characterize sentiment 
polarities, a well-known technique in Sentiment 
Analysis. This method consists of three key stages: 
(I) Building contextual word graphs; (II) Applying 
a ranking algorithm; and (III) Adjusting the 
sentiment polarity values. 
These stages are shown in the diagram in Figure 1, 
which the development of sentimental resources 
starts off by giving four corpora of annotated 
sentences (the first with neutral sentences, the 
second with objective sentences, the third with 
positive sentences, and the last with negative 
sentences). 
 
 
Figure 1. Resource walkthrough development 
process. 
2.3 Building contextual word graphs 
Initially, text preprocessing is performed by 
applying a Post-Tagging tool (using Freeling 
(Atserias et al, 2006) tool version 2.2 in this case) 
to convert all words to lemmas 5 . After that, all 
obtained lists of lemmas are sent to RA-SR, then 
divided into four groups: neutral, objective, 
positive, and negative candidates. As the first set 
                                                 
5 Lemma denotes canonic form of the words. 
Phr se 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W4 W5 W6
W1
W7
Phrase 1 Positve
Phrases
W5 W6 W8 W9
W8 W9 W7
W6 W9 W10 W11
W6
W1 W8
Negative
Phrases
Phrase 3
Phrase 2
Phrase 1
Positive 
Words
Negative 
Words
W1 W2 W3 W4
W5
W6 W7
W5
W6
W7
W8
W9
W10
W11
(I)
(II) Reinforcing words 
Weight = 1
(II) (II) 
(I)
W ight =1
W ight =1
Weight =1
Weight =1
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
(III) 
W1
Default Weight = 1/N Default Weight = 1/N
W1 W2 W3
W4W5
Phrase 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W1 W2 W4
W1
W5
Phrase 1 Neutral 
Phrases
W1 W6 W7 W8
W8 W7 W3
W6 W8 W7 W5
W5
W5 W2
Objective 
Phrases
Phrase 3
Phrase 2
Phrase 1
(II) 
W1 W2 W3 W4 W5 W6 W7 W8
(II) 
W1 W2 W3
W5
W6 W7
W8
Default Weight = 1/N
(I)(I)
Default Weight = 1/N
444
of results, four contextual graphs are 
obtained:  ????,   ???? , ????,  and ???? , where 
each graph includes the words/lemmas from the 
neutral, objective, positive and negative sentences 
respectively. These graphs are generated after 
connecting all words for each sentence into 
individual sets of annotated sentences in 
concordance with their annotations (??? , ??? , 
???, ??? ). 
Once the four graphs representing neutral, 
objective, positive and negative contexts are 
created, we proceed to assign weights to apply 
graph-based ranking techniques in order to auto-
balance the particular importance of each vertex ?? 
into ????, ????, ???? and ????. 
As the primary output of the graph-based ranking 
process, the positive, negative, neutral, and 
objective values are calculated using the PageRank 
algorithm and normalized with equation (1). For a 
better understanding of how the contextual graph 
was built see (Guti?rrez et al, 2013). 
2.4 Applying a ranking algorithm 
To apply a graph-based ranking process, it is 
necessary to assign weights to the vertices of the 
graph. Words involved into ????, ????, ???? 
and ???? take the default of 1/N as their weight 
to define the weight of ? vector, which is used in 
our proposed ranking algorithm. In the case where 
words are identified on the sentiment repositories 
(see Table 4) as positive or negative, in relation to 
their respective graph, a weight value of 1 (in a 
range [0?1] ) is assigned. ?  represents the 
maximum quantity of words in the current graph. 
After that, a graph-based ranking algorithm is 
applied in order to structurally raise the graph 
vertexes? voting power. Once the reinforcement 
values are applied, the proposed ranking algorithm 
is able to increase the significance of the words 
related to these empowered vertices. 
The PageRank (Brin and Page, 1998) 
adaptation, which was popularized by (Agirre and 
Soroa, 2009) in Word Sense Disambiguation 
thematic, and which has obtained relevant results, 
was an inspiration to us in our work. The main 
idea behind this algorithm is that, for each edge 
between ?i and ?j in graph ?, a vote is made from 
? i to ? j. As a result, the relevance of ? j is 
increased. 
On top of that, the vote strength from ?  to ? 
depends on ???? relevance. The philosophy behind 
it is that, the more important the vertex is, the 
more strength the voter would have. Thus, 
PageRank is generated by applying a random 
walkthrough from the internal interconnection of 
? , where the final relevance of ??  represents the 
random walkthrough probability over ? , and 
ending on ??.  
In our system, we apply the following 
configuration: dumping factor ? = 0.85 and, like 
in (Agirre and Soroa, 2009) we used 30 iterations. 
A detailed explanation about the PageRank 
algorithm can be found in (Agirre and Soroa, 
2009)  
After applying PageRank, in order to obtain 
standardized values for both graphs, we normalize 
the rank values by applying the equation (1), 
where ???(??) obtains the maximum rank value 
of ?? vector (rankings? vector). 
??? = ???/???(??) (1) 
2.5 Adjusting the sentiment polarity values 
After applying the PageRank algorithm on????, 
???? , ????  and ???? , having normalized their 
ranks, we proceed to obtain a final list of lemmas 
(named ?? ) while avoiding repeated elements. 
?? is represented by ???  lemmas, which would 
have, at that time, four assigned values: Neutral, 
Objective, Positive, and Negative, all of which 
correspond to a calculated rank obtained by the 
PageRank algorithm.  
At that point, for each lemma from ??,  the 
following equations are applied in order to select 
the definitive subjectivity polarity for each one: 
??? =  {
??? ? ??? ;  ??? > ???
0                ; ?????????
 (2) 
??? =  {
??? ? ??? ;  ??? > ???
0                ; ?????????
 (3) 
Where ???  is the Positive value and ???  the 
Negative value related to each lemma in ??. 
In order to standardize again the ???  and ??? 
values and making them more representative in a 
[0?1] scale, we proceed to apply a normalization 
process over the ??? and ??? values. 
From there, based on the objective features 
commented by (Baccianella et al, 2010), we 
assume the same premise to establish an 
alternative objective value of the lemmas. 
Equation (4) is used for that: 
?????? = 1 ? |??? ? ???| (4) 
Where ??????  represents the alternative 
objective value. 
445
As a result, each word obtained in the sentiment 
resource has an associated value of: positivity 
(??? , see equation (2)), negativity (??? , see 
equation (3)), objectivity(????_???,  obtained by 
PageRank over ????  and normalized with 
equation (1)), calculated-objectivity (??????, now 
cited as ???_???????? ) and neutrality (??? , 
obtained by PageRank over ???? and normalized 
with equation (1)). 
3  System Description 
The system takes annotated corpora as input from 
which two models are created. One model is 
created by using only the data provided at 
Semeval-2013 (Restricted Corpora, see Table 3), 
and the other by using extra data from other 
annotated corpora (Unrestricted Corpora, see 
Table 3). In all cases, the phrases are pre-
processed using Freeling 2.2 pos-tagger (Atserias 
et al, 2006) while a dataset copy is normalized 
using TENOR (described in section 2.1). 
The system starts by extracting two sets of 
features. The Core Features (see section 3.1) are 
the Sentiment Measures and are calculated for a 
standard and normalized phrase. The Support 
Features (see section 3.2) are based on regularities, 
observed in the training dataset, such as 
emoticons, uppercase words, and so on. 
The supervised models are created using Weka6 
and a Logistic classifier, both of which the system 
uses to predict the values of the test dataset. The 
selection of the classifier was made after analyzing 
several classifiers such as: Support Vector 
Machine, J48 and REPTree. Finally, the Logistic 
classifier proved to be the best by increasing the 
results around three perceptual points. 
The test data is preprocessed in the same way 
the previous corpora were. The same process of 
feature extraction is also applied. With the 
aforementioned features and the generated models, 
the system proceeds to classify the final values of 
Positivity, Negativity, and Neutrality.  
3.1 The Core Features 
The Core Features is a group of measures based on 
the resource created early (see section 2.2). The 
system takes a sentence preprocessed by Freeling 
2.2 and TENOR. For each lemma of the analyzed 
sentence, ??? , ??? , ???_???????? ,  ????_???, 
                                                 
6 http://www.cs.waikato.ac.nz/ 
and ???  are calculated by using the respective 
word values assigned in RA-SR. The obtained 
values correspond to the sum of the corresponding 
values for each intersecting word between the 
analyzed sentence (lemmas list) and the obtained 
resource by RA-SR. Lastly, the aforementioned 
attributes are normalized by dividing them by the 
number of words involved in this process. 
Other calculated attributes are: ???_????? , 
???_????? , ???_????????_????? , 
???_????_????? and ???_?????. These attributes 
count each involved iteration for each feature type 
( ??? , ??? , ????_??? , ??????  and ??? 
respectively, where the respective value may be 
greater than zero. 
Attributes ???  and cnn are calculated by 
counting the amount of lemmas in the phrases 
contained in the Sentiment Lexicons (Positive and 
Negative respectively).  
All of the 12 attributes described previously are 
computed for both, the original, and the 
normalized (using TENOR) phrase, totaling 24 
attributes. The Core features are described next.  
Feature Name Description 
??? 
Sum of respective value of each word. 
??? 
???_???????? 
????_??? 
??? 
???_????? 
Counts the words where its respective value 
is greater than zero 
???_????? 
???_????????_????? 
????_???_????? 
???_????? 
??? (to positive) Counts the words contained in the 
Sentiment Lexicons for their respective 
polarities. 
??? (to negative) 
Table 1. Core Features 
3.2 The Support Features 
The Support Features is a group of measures based 
on characteristics of the phrases, which may help 
with the definition on extreme cases. The emotPos 
and emotNeg values are the amount of Positive 
and Negative Emoticons found in the phrase. The 
exc and itr are the amount of exclamation and 
interrogation signs in the phrase. The following 
table shows the attributes that represent the 
support features: 
Feature Name Description 
??????? 
Counts the respective Emoticons 
??????? 
??? (exclamation marks (?!?)) 
Counts the respective marks 
??? (question marks (???)) 
?????_????? Counts the uppercase words 
?????_??? Sums the respective values of the 
Uppercase words ?????_??? 
?????_???_?????_??? (to Counts the Uppercase words 
446
positivity) contained in their respective 
Graph ?????_???_?????_???(to 
negativity) 
?????_???_?????_???? (to 
positivity) 
Counts the Uppercase words 
contained in the Sentiment 
Lexicons 7 for their respective 
polarity  
?????_???_?????_???? (to 
negativity) 
???????_????? Counts the words with repeated 
chars  
???????_??? Sums the respective values of the 
words with repeated chars ???????_??? 
???????_???_?????_???? (in 
negative lexical resource ) 
Counts the words with repeated 
chars contained in the respective 
lexical resource ???????_???_?????_???? (in 
positive lexical resource ) 
???????_???_?????_??? (in 
positive graph ) 
Counts the words with repeated 
chars contained in the respective 
graph ???????_???_?????_???  (in 
negative graph ) 
Table 2. The Support Features 
4 Evaluation 
In the construction of the sentiment resource, we 
used the annotated sentences provided by the 
corpora described in Table 3. The resources listed 
in Table 3 were selected to test the functionality of 
the words annotation proposal with subjectivity 
and objectivity. Note that the shadowed rows 
correspond to constrained runs corpora: tweeti-b-
sub.dist_out.tsv 8  (dist), b1_tweeti-objorneu-
b.dist_out.tsv 9  (objorneu), twitter-dev-input-
B.tsv10 (dev). 
The resources from Table 3 that include 
unconstrained runs corpora are: all the previously 
mentioned ones, Computational-intelligence11 (CI) 
and stno12 corpora. 
The used sentiment lexicons are from the 
WordNetAffect_Categories13 and opinion-words14 
files as shown in detail in Table 4. 
Some issues were taken into account throughout 
this process. For instance, after obtaining a 
contextual graph ?, factotum words are present in 
most of the involved sentences (i.e., verb ?to be?). 
This issue becomes very dangerous after applying 
the PageRank algorithm because the algorithm 
                                                 
7 Resources described in Table 4. 
8Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b). 
9Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b). 
10 http://www.cs.york.ac.uk/semeval-2013/task2/ 
11A sentimental corpus obtained applying techniques 
developed by GPLSI department. See 
(http://gplsi.dlsi.ua.es/gplsi11/allresourcespanel) 
12NTCIR Multilingual Opinion Analysis Task (MOAT) 
http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
13 http://wndomains.fbk.eu/wnaffect.html 
14 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 
strengthens the nodes possessing many linked 
elements. For that reason, the subtractions ??? ?
??? and ??? ? ??? are applied, where the most 
frequent words in all contexts obtain high values. 
The subtraction becomes a dumping factor.  
As an example, when we take the verb ?to be?, 
before applying equation (1), the verb achieves the 
highest values in each subjective context graph 
(????  and ????)  namely, 9.94 and 18.67 rank 
values respectively. These values, once equation 
(1) is applied, are normalized obtaining both 
??? =  1 and ??? =  1 in a range [0...1]. At the 
end, when the following steps are executed 
(Equations (2) and (3)), the verb ?to be? 
achieves ??? = 0 , ??? = 0  and 
therefore  ?????? = 1 . Through this example, it 
seems as though we subjectively discarded words 
that appear frequently in both contexts (Positive 
and Negative). 
Corpus N P O Neu 
Obj 
or Neu 
Unk T 
C UC 
dist 176 368 110 34 - - 688 X X 
objorneu 828 1972 788 1114 1045 - 5747 X X 
dev 340 575 - 739 - - 1654 X X 
CI 6982 6172 - - - - 13154  X 
stno15 1286 660 - 384 - 10000 12330  X 
T 9272 9172 898 1532 1045 10000 31919   
Table 3. Corpora used to apply RA-SR. Positive (P), 
Negative (N), Objective (Obj/O), Unknow (Unk), Total 
(T), Constrained (C), Unconstrained (UC). 
Sources P N T 
WordNet-Affects_Categories 
 (Strapparava and Valitutti, 2004) 
629 907 1536 
opinion-words  
(Hu and Liu, 2004; Liu et al, 2005) 
2006 4783 6789 
Total 2635 5690 8325 
Table 4. Sentiment Lexicons. Positive (P), Negative 
(N) and Total (T). 
   Precision (%) Recall (%) Total (%) 
 C Inc P  N  Neu P N Neu Prec Rec F1 
Run1 8032 1631 80,7 83,8 89,9 90,9 69,5 86,4 84,8 82,3 82,9 
Run2 19101 4671 82,2 77,3 89,4 80,7 81,9 82,3 83,0 81,6 80,4 
Table 5. Training dataset evaluation using cross-
validation (Logistic classifier (using 10 folds)). 
Constrained (Run1), Unconstrained (Run2), Correct(C), 
Incorrect (Inc). 
4.1 The training evaluation 
In order to assess the effectiveness of our trained 
classifiers, we performed some evaluation tests.  
Table 5 shows relevant results obtained after 
applying our system to an environment (specific 
domain). The best results were obtained with the 
                                                 
15 NTCIR Multilingual Opinion Analysis Task (MOAT) 
http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
447
restricted corpus. The information used to increase 
the knowledge was not balanced or perhaps is of 
poor quality. 
4.2 The test evaluation 
The test dataset evaluation is shown in Table 6, 
where system results are compared with the best 
results in each case. We notice that the constrained 
run is better in almost every aspect. In the few 
cases where it was lower, there was a minimal 
difference. This suggests that the information used 
to increase our Sentiment Resource was 
unbalanced (high difference between quantity of 
tagged types of annotated phrases), or was of poor 
quality. By comparing these results with the ones 
obtained by our system on the test dataset, we 
notice that on the test dataset, the results fell in the 
middle of the effectiveness scores. After seeing 
these results (Table 5 and Table 6), we assumed 
that our system performance is better in a 
controlled environment (or specific domain). To 
make it more realistic, the system must be trained 
with a bigger and more balanced dataset. 
Table 6 shows the results obtained by our 
system while comparing them to the best results of 
Task 2b of Semeval-2013. In Table 5, we can see 
the difference between the best systems. They are 
the ones in bold and underlined as target results.  
These results have a difference of around 20 
percentage points. The grayed out ones correspond 
to our runs. 
      Precision (%) Recall (%) Total 
Runs C Inc P N Neu P N Neu Prec Rec F 1 
1_tw 2082 1731 60,9 46,5 52,8 49,8 41,4 64,1 53,4 51,8 49,3 
1_tw_cnd 2767 1046 81,4 69,7 67,7 66,7 60,4 82,6 72,9 69,9 69,0 
2_tw 2026 1787 58,0 42,2 42,2 52,2 43,9 57,4 47,4 51,2 49,0 
2_tw_ter 2565 1248 71,1 54,6 68,6 74,7 59,4 63,1 64,8 65,7 64,9 
1_sms 1232 862 43,9 46,1 69,5 55,9 31,7 68,9 53,2 52,2 43,4 
1_sms_cnd 1565 529 73,1 55,4 85,2 73,0 75,4 75,3 71,2 74,5 68,5 
2_sms 1023 1071 38,4 31,4 68,3 60,0 38,3 47,8 46,0 48,7 40,7 
2_sms_ava 1433 661 60,9 49,4 81,4 65,9 63,7 71,0 63,9 66,9 59,5 
Table 6. Test dataset evaluation using official scores. 
Corrects(C), Incorrect (Inc). 
Table 6 run descriptions are as follows:  
? UMCC_DLSI_(SA)-B-twitter-constrained 
(1_tw), 
? NRC-Canada-B-twitter-constrained 
(1_tw_cnd),  
? UMCC_DLSI_(SA)-B-twitter-unconstrained 
(2_tw), 
? teragram-B-twitter-unconstrained (2_tw_ter), 
? UMCC_DLSI_(SA)-B-SMS-constrained 
(1_sms), 
? NRC-Canada-B-SMS-constrained 
(1_sms_cnd), UMCC_DLSI_(SA)-B-SMS-
unconstrained (2_sms), 
? AVAYA-B-sms-unconstrained (2_sms_ava). 
As we can see in the training and testing 
evaluation tables, our training stage offered more 
relevant scores than the best scores in Task2b 
(Semaval-2013). This means that we need to 
identify the missed features between both datasets 
(training and testing). 
For that reason, we decided to check how many 
words our system (more concretely, our Sentiment 
Resource) missed. Table 7 shows that our system 
missed around 20% of the words present in the test 
dataset. 
 hits miss miss (%) 
twitter 23807 1591 6,26% 
sms 12416 2564 17,12% 
twitter nonrepeat   2426 863 26,24% 
sms norepeat 1269 322 20,24% 
Table 7. Quantity of words used by our system over 
the test dataset. 
5 Conclusion and further work 
Based on what we have presented, we can say that 
we could develop a system that would be able to 
solve the SA challenge with promising results. The 
presented system has demonstrated election 
performance on a specific domain (see Table 5) 
with results over 80%. Also, note that our system, 
through the SA process, automatically builds 
sentiment resources from annotated corpora.  
For future research, we plan to evaluate RA-SR 
on different corpora. On top of that, we also plan 
to deal with the number of neutral instances and 
finding more words to evaluate the obtained 
sentiment resource. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government through 
the project PROMETEO 
(PROMETEO/2009/199). 
448
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 
12th conference of the European chapter of the 
Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009.  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
semantic services in an opensource NLP library. 
Proceedings of LREC'06, Genoa, Italy, 2006.  
Baccianella, S.; A. Esuli and F. Sebastiani. 
SENTIWORDNET 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion 
Mining. 7th Language Resources and Evaluation 
Conference, Valletta, MALTA., 2010. 2200-2204 p.  
Balahur, A. Methods and Resources for Sentiment 
Analysis in Multilingual Documents of Different 
Text Types. Department of Software and Computing 
Systems. Alacant, Univeristy of Alacant, 2011. 299. 
p. 
Balahur, A.; E. Boldrini; A. Montoyo and P. Martinez-
Barco. The OpAL System at NTCIR 8 MOAT. 
Proceedings of NTCIR-8 Workshop Meeting, 
Tokyo, Japan., 2010. 241-245 p.  
Brin, S. and L. Page The anatomy of a large-scale 
hypertextual Web search engine Computer Networks 
and ISDN Systems, 1998, 30(1-7): 107-117. 
Fellbaum, C. WordNet. An Electronic Lexical 
Database.  University of Cambridge, 1998. p. The 
MIT Press.  
Guti?rrez, Y.; A. Gonz?lez; A. F. Orqu?n; A. Montoyo 
and R. Mu?oz. RA-SR: Using a ranking algorithm to 
automatically building resources for subjectivity 
analysis over annotated corpora. 4th Workshop on 
Computational Approaches to Subjectivity, 
Sentiment & Social Media Analysis (WASSA 2013), 
Atlanta, Georgia, 2013.  
Hatzivassiloglou; Vasileios and J. Wiebe. Effects of 
Adjective Orientation and Gradability on Sentence 
Subjectivity. International Conference on 
Computational Linguistics (COLING-2000), 2000.  
Hu, M. and B. Liu. Mining and Summarizing Customer 
Reviews. Proceedings of the ACM SIGKDD 
International Conference on Knowledge Discovery 
and Data Mining (KDD-2004), USA, 2004.  
Kim, S.-M. and E. Hovy. Extracting Opinions, Opinion 
Holders, and Topics Expressed in Online News 
Media Text. In Proceedings of workshop on 
sentiment and subjectivity in text at proceedings of 
the 21st international conference on computational 
linguistics/the 44th annual meeting of the association 
for computational linguistics (COLING/ACL 2006), 
Sydney, Australia, 2006. 1-8 p.  
Kozareva, Z.; P. Nakov; A. Ritter; S. Rosenthal; V. 
Stoyonov and T. Wilson. Sentiment Analysis in 
Twitter. in:  Proceedings of the 7th International 
Workshop on Semantic Evaluation. Association for 
Computation Linguistics, 2013. 
Liu, B.; M. Hu and J. Cheng. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
Proceedings of the 14th International World Wide 
Web conference (WWW-2005), Japan, 2005.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross and 
K. Miller. Five papers on WordNet. Princenton 
University, Cognositive Science Laboratory, 1990. 
Mosquera, A. and P. Moreda. TENOR: A Lexical 
Normalisation Tool for Spanish Web 2.0 Texts. in:  
Text, Speech and Dialogue - 15th International 
Conference (TSD 2012). Springer, 2012. 
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language Resources 
and Evaluation (LREC 2004), Lisbon, 2004. 1083-
1086 p.  
Wiebe, J.; T. Wilson and C. Cardie. Annotating 
Expressions of Opinions and Emotions in Language. 
Kluwer Academic Publishers, Netherlands, 2005.  
 
  
 
449
Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 1?7,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Mining Lexical Variants from Microblogs: An Unsupervised Multilingual
Approach
Alejandro Mosquera
University of Alicante
San Vicente del Raspeig s/n - 03690
Alicante, Spain
amosquera@dlsi.ua.es
Paloma Moreda
University of Alicante
San Vicente del Raspeig s/n - 03690
Alicante, Spain
moreda@dlsi.ua.es
Abstract
User-generated content has become a re-
current resource for NLP tools and ap-
plications, hence many efforts have been
made lately in order to handle the noise
present in short social media texts. The
use of normalisation techniques has been
proven useful for identifying and replac-
ing lexical variants on some of the most
informal genres such as microblogs. But
annotated data is needed in order to train
and evaluate these systems, which usu-
ally involves a costly process. Until now,
most of these approaches have been fo-
cused on English and they were not taking
into account demographic variables such
as the user location and gender. In this pa-
per we describe the methodology used for
automatically mining a corpus of variant
and normalisation pairs from English and
Spanish tweets.
1 Introduction
User-generated content (UGC), and specially the
microblog genre, has become an interesting re-
source for Natural Language Processing (NLP)
tools and applications. Many are the advantages
of exploiting this real-time stream of multilingual
textual data. Popular applications such as Twit-
ter has an heterogeneous user base of almost 600
million users that generate more than 60 million
new tweets every day. For this reason, Twitter
has become one of the most used sources of tex-
tual data for NLP with several applications such
as sentiment analysis (Tumasjan et al., 2010) or
realtime event detection (Sakaki et al., 2010). Re-
cent advances on machine translation or informa-
tion retrieval systems have been also making an
extensive use of UGC for both training and evalu-
ation purposes. However, tweets can be very noisy
and sometimes hard to understand for both hu-
mans (Mosquera et al., 2012) and NLP applica-
tions (Wang and Ng, 2013), so an additional pre-
processing step is usually required.
There have been different perceptions regard-
ing the lexical quality of social media (Rello and
Baeza-Yates, 2012) (Baldwin et al., 2013) and
even others suggested that 40% of the messages
of Twitter were ?pointless babble? (PearAnalyt-
ics, 2009). Most of the out of vocabulary (OOV)
words present in social media texts can be cata-
logued as lexical variants (e.g. ?See u 2moro? ?
?See you tomorrow?), that are words lexically re-
lated with their canonic form.
The use of text normalisation techniques has
been proven useful in order to clean short and in-
formal texts such as tweets. However, the eval-
uation of these systems requires annotated data,
which usually involves costly human annotations.
There are previous works about automatically con-
structing normalisation dictionaries, but until now,
most of these approaches have been focused on
English and they were not taking into account de-
mographic variants. In this paper we describe the
methodology used for automatically mining lexi-
cal variants from English and Spanish tweets as-
sociated to a set of headwords. These formal and
informal pairs can be later used to train and eval-
uate existing social media text normalisation sys-
tems. Additional metadata from Twitter such as
geographic location and user gender is also col-
lected, opening the possibility to model and anal-
yse gender or location-specific variants.
This paper is organised as follows. We describe
the related work in Section 2. We then describe
our variant mining methodology in Section 3. The
obtained results are presented in Section 4. Sec-
tion 5, draws the conclusions and future work.
1
2 Related Work
One way to handle the performance drop of NLP
tools on user-generated content (Foster et al.,
2011) is to re-train existing models on these in-
formal genres (Gimpel et al., 2011), (Liu et al.,
2011b). Another approaches make use of pre-
processing techniques such as text normalisation
in order to minimise the social media textual noise
(Han et al., 2013), (Mosquera and Moreda, 2012)
where OOV words were first identified and then
substituted using lexical and phonetic edit dis-
tances. In order to enhance both precision and
recall both OOV detection and translation dic-
tionaries were used. Moreover, the creative na-
ture of informal writing and the low availability
of manually-annotated corpora can make the im-
provement and evaluation of these systems chal-
lenging.
Motivated by the lack of annotated data and the
large amount of OOV words contained in Twitter,
several approaches for automatically construct-
ing a lexical normalisation dictionary were pro-
posed; In (Gouws et al., 2011) a normalisation
lexicon is generated based on distributional and
string similarity (Lodhi et al., 2002) from Twit-
ter. Using a similar technique, a wider-coverage
dictionary is constructed in (Han et al., 2012)
based on contextually-similar (OOV, IV) pairs.
More recently, (Hassan and Menezes, 2013) intro-
duced another context-based approach using ran-
dom walks on a contextual similarity graph.
Distributional-based methods can have some
drawbacks: they rely heavily on pairwise com-
parisons that make them computationally expen-
sive, and as the normalisation candidates are se-
lected based on context similarity they can be sen-
sitive to domain-specific variants that share similar
contexts. Moreover, these approaches were focus-
ing on extracting English lexical variants from so-
cial media texts, but due the heterogeneity of its
users, lexical distributions can be influenced by
geographical factors (Eisenstein et al., 2010) or
even gender (Thomson and Murachver, 2001).
To the best of our knowledge, there are not
multilingual approaches for mining lexical vari-
ants from short, noisy texts that also take into ac-
count demographic variables. For this reason, we
present an unsupervised method for mining En-
glish and Spanish lexical variants from Twitter that
collects demographic and contextual information.
These obtained pairs can be later used for training
and evaluating text normalisation and inverse text
normalisation systems.
3 Lexical Variant Mining
Lexical variants are typically formed from their
standard forms through regular processes (Thur-
low and Brown, 2003) and these can be mod-
elled by using a set of basic character transfor-
mation rules such as letter insertion, deletion or
substitution (Liu et al., 2011a) e.g. (?tmrrw? ?
?2morrow?) and combination of these (?2moro?).
The relation between formal and informal pairs is
not always 1-to-1, two different formal words can
share the same lexical variant (?t? in Spanish can
represent ?te? or ?t?u?) and one formal word can
have many different variants (e.g. ?see you? us
commonly shortened as ?c ya? or ?see u?). As
a difference with previous approaches based on
contextual and distributional similarity, we have
chosen to model the generation of variant candi-
dates from a set of headwords using transforma-
tion rules. These candidates are later validated
based on their presence on a popular microblog
service, used in this case as a high-coverage cor-
pus.
3.1 Candidate Generation
We have defined a set of 6 basic transforma-
tion rules (see Table 1) in order to automati-
cally generate candidate lexical variants from the
300k most frequent words of Web 1T 5-gram (En-
glish) (Brants and Franz, 2006) and SUBTLEX-
SP (Spanish) (Cuetos et al., 2011) corpora.
Rule Example
a) Character duplication ?goal?? ?gooal?
b) Number transliteration ?cansados?? ?cansa2?
c) Character deletion ?tomorrow?? ?tomrrw?
d) Character replacement ?friend?? ?freend?
e) Character transposition ?maybe?? ?mabye?
f) Phonetic substitution ?coche?? ?coxe?
g) Combination of above ?coche?? ?coxeee?
Table 1: Transformation rules.
As modelling some variants may need more
than one basic operation, and lexically-related
variants are usually in an edit distance t where
t <= 3 (Han et al., 2013), the aforementioned
rules were implemented using an engine based on
stacked transducers with the possibility to apply a
maximum of three concurrent transformations:
(a) Character duplication: For words with n
characters, while n>19 each character were
2
duplicated n times (? n>0, n<4), generating
n
3
candidate variants.
(b) Number transliteration: Words and num-
bers are transliterated following the language
rules defined in Table 2.
Rule Lang.
?uno?? ?1? SP
?dos?? ?2? SP
?one?? ?1? EN
?two?? ?2? EN
?to?? ?2? EN
?three?? ?3? EN
?for?? ?4? EN
?four?? ?4? EN
?eight?? ?8? EN
?be?? ?b? EN
?a?? ?4? EN
?e?? ?3? EN
?o?? ?0? EN
?s?? ?5? EN
?g?? ?6? EN
?t?? ?7? EN
?l?? ?1? EN
Table 2: Transliteration table for English and
Spanish.
(c) Character deletion: The candidate variants
from all possible one character deletion com-
binations plus the consonant skeleton of the
word will be generated.
(d) Character replacement: Candidate variants
are generated by replacing n characters (?
n>0, n<7) by their neighbours taking into
account a QWERTY keyboard and an edit
distance of 1.
(e) Character transposition: In order to generate
candidate lexical variants the position of ad-
jacent characters are exchanged.
(f) Phonetic substitution: A maximum of three
character n-grams are substituted for char-
acters that sound similar following different
rules for Spanish (Table 3) and English (Ta-
ble 4).
3.2 Candidate Selection
We have explored several approaches for filtering
common typographical errors and misspellings, as
these are unintentional and can not be technically
considered lexical variants, in order to do this
we have used supervised machine learning tech-
niques. Also, with aim to filter uncommon or
Rule
?b??[?v? or ?w?]
?c??[?k?]
?s??[?z?]
?z??[?s?]
?c??[?s?]
?x??[?s?]
??n??[?ni?]
?ch??[?x?]
?gu??[?w?]
?qu??[?k?]
?ll??[?y?]
?ge??[?je?]
?gi??[?ji?]
?ll??[?i?]
?hue??[?we?]
Table 3: Phonetic substitution table for Spanish.
low quality variants, the Rovereto Twitter corpus
(Herdagdelen, 2013) was initially used in order
to rank the English candidates present in the cor-
pus by their frequencies. The 38% of the variants
generated by one transformation were successfully
found, however, performing direct Twitter search
API queries resulted to have better coverage than
using a static corpus (90% for English variants).
3.2.1 Intentionality Filtering
Given an OOV word a and its IV version b we have
extracted character transformation rules from a to
b using the longest common substring (LCS) algo-
rithm (See Table 5). These lists of transformations
were encoded as a numeric array where the num-
ber each transformation counts were stored. We
have used NLTK (Bird, 2006) and the Sequence-
Matcher Python class in order to extract those sets
of transformations taking into account also the po-
sition of the character (beginning, middle or at the
end of the word).
A two-class SVM (Vapnik, 1995) model has
ben trained using a linear kernel with a corpus
composed by 4200 formal-variant pairs extracted
from Twitter
1
, SMS
2
and a corpus of the 4200
most common misspellings
3
. In table 6 we show
the k-fold cross-validation results (k=10) of the
model, obtaining a 87% F1. This model has been
used in order to filter the English candidate vari-
ants classified as not-intentional.
To the best of our knowledge there are not simi-
lar annotated resources for Spanish, so this clas-
sifier was developed only for English variants.
However, would be possible to adapt it to work for
1
http : //ww2.cs.mu.oz.au/ hanb/emnlp.tgz
2
http : //www.cel.iitkgp.ernet.in/ monojit/sms
3
http : //aspell.net/test/common? all/
3
Rule
?i??[?e?]
?o??[?a?]
?u??[?o?]
?s??[?z?]
?f??[?ph?]
?j??[?ge? or ?g?]
?n??[?kn? or ?gn?]
?r??[?wr?]
?z??[?se? or ?s?]
?ea??[?e?]
?ex??[?x?]
?ae??[?ay? or ?ai? or ?a?]
?ee??[?ea? or ?ie? or ?e?]
?ie??[?igh? or ?y? or ?i?]
?oe??[?oa? or ?ow? or ?o?]
?oo??[?ou? or ?u?]
?ar??[?a?]
?ur??[?ir? or ?er? or ?ear? or ?or?]
?or??[?oor? or ?ar?]
?au??[?aw? or ?a?]
?er??[?e?]
?ow??[?ou?]
?oi??[?oy?]
?sh??[?ss? or ?ch?]
?ex??[?x?]
?sh??[?ss? or ?ch?]
?ng??[?n?]
?air??[?ear? or ?are?]
?ear??[?eer? or ?ere?]
Table 4: Phonetic substitution table for English.
another languages if the adequate corpora is pro-
vided. Because of the lack of this intentionality
detection step, the number of generated candidate
variants for Spanish was filtered by taking into ac-
count the number of transformations, removing all
the variants generated by more than two opera-
tions.
3.2.2 Twitter Search
The variants filtered during the previous step were
searched on the real time Twitter stream for a pe-
riod of two months by processing more than 7.5
million tweets. Their absolute frequencies n were
used as a weighting factor in order to discard not
used words (n > 0). Additionally, variants present
in another languages rather than English or Span-
ish were ignored by using the language identifica-
tion tags present in Twitter metadata.
There were important differences between the
final number of selected candidates for Spanish,
with 6 times less variant pairs and English (see Ta-
ble 7). Spanish language uses diacritics that are
commonly ignored on informal writing, for this
reason there is a higher number of possible com-
binations for candidate words that would not gen-
erate valid or used lexical variants.
Formal/Informal pair Transf. Pos.
house ? h0use o ? 0 middle
campaign ? campaing n ? ? end
? ? n middle
happy ? :) happy ? :) middle
embarrass ? embarass r? ? middle
acquaintance ? ?? q middle
aqcuaintance q ? ? middle
virtually ? virtualy l? ? middle
cats ? catz s? z end
Table 5: Example of formal/informal pairs and the
extract transformations.
Method Precision Recall F1
SVM 0.831 0.824 0.827
SVM+Pos. 0.878 0.874 0.876
Formal/Informal pair Verdict
you ? yu intentional
accommodate ? acommodate unintentional
business ? bussiness unintentional
doing ? doin intentional
acquaintance ? aqcuaintance unintentional
basically ? basicly unintentional
rules ? rulez intentional
Table 6: Cross-validation results of intentionality
classification with examples.
4 Results
Besides the original message and the context of
the searched variant, additional metadata has been
collected from each tweet such as the gender and
the location of the user. In Twitter the gender is not
explicitly available, for this reason we applied an
heuristic approach based on the first name as it is
reported in the user profile. In order to do this, two
list of male and female names were used: the 1990
US census data
4
and popular baby names from
the US Social Security Administration?s statistics
between 1960 and 2010
5
.
We have analysed the gender and language dis-
tribution of the 6 transformation rules across the
mined pairs (see Figure 1). On the one hand, lex-
ical variants generated by duplicating characters
were the most popular specially between female
4
census.gov/genealogy/www/data/1990surnames
5
ssa.gov/cgi? bin/popularnames.cgi
4
Candidates Selected Lang.
2456627 48550 EN
1374078 8647 SP
Table 7: Number of generated and selected vari-
ants after Twitter search.
Figure 1: Transformation trends by gender.
users with a 5% more than their male counter-
parts. On the other hand, variants generated by
character replacement and deletion were found a
2% more on tweets from male users. The differ-
ences between English and Spanish were notable,
mostly regarding the use of transliterations, that
were not found on Spanish tweets, and phonetic
substitutions, ten times less frequent than in En-
glish tweets.
For the distribution of transformations across
geographic areas, we have just taken into account
the countries where the analysed languages have
an official status. Lexical variants found in Tweets
from another areas are grouped into the ?Non-
official? label (see Figure 2). The biggest dif-
ferences were found on the use of translitera-
tions (higher in UK and Ireland with more than
a 5%) and phonetic substitutions (higher in Pak-
istani users with more than a 22%). Transforma-
tion frequencies from non-official English speak-
ing countries were very similar as the ones regis-
tered for users based on United States and Canada.
Spanish results were less uniform and showed
more variance respect the use of character dupli-
cation (57% in Argentina), character replacement
(more than 24% in Mexico and Guatemala) and
character transposition (with more than a 19% for
users from Cuba, Colombia and Mexico) (see Fig-
ure 3).
5 Conclusions and Future Work
In this paper we have described a multilingual
and unsupervised method for mining English and
Spanish lexical variants from Twitter with aim to
close the gap regarding the lack of annotated cor-
pora. These obtained pairs can be later used for
the training and evaluation of text normalisation
systems without the need of costly human anno-
tations. Furthermore, the gathered demographic
and contextual information can be used in order to
model and generate variants similar to those that
can be found on specific geographic areas. This
has interesting applications in the field of inverse
text normalisation, that are left to a future work.
We also intend to explore the benefits of feature
engineering for the detection and categorisation
of lexical variants using machine learning tech-
niques.
Acknowledgments
This research is partially funded by the Eu-
ropean Commission under the Seventh (FP7 -
2007- 2013) Framework Programme for Re-
search and Technological Development through
the FIRST project (FP7-287607). This pub-
lication reflects the views only of the author,
and the Commission cannot be held responsi-
ble for any use which may be made of the in-
formation contained therein. Moreover, it has
been partially funded by the Spanish Govern-
ment through the project ?An?alisis de Tenden-
cias Mediante T?ecnicas de Opini?on Sem?antica?
(TIN2012-38536-C03-03) and ?T?ecnicas de De-
construcci?on en la Tecnolog??as del Lenguaje Hu-
mano? (TIN2012-31224).
References
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy social
media text, how diffrnt social media sources. In Pro-
ceedings of the Sixth International Joint Conference
on Natural Language Processing, pages 356?364.
Steven Bird. 2006. Nltk: the natural language
toolkit. In Proceedings of the COLING/ACL on In-
teractive presentation sessions, COLING-ACL ?06,
pages 69?72, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram corpus version 1. Technical report, Google
Research.
5
Figure 2: Transformation trends by English-speaking countries.
Figure 3: Transformation trends by Spanish-speaking countries.
Fernando Cuetos, Maria Glez-Nosti, Anala Barbn, and
Marc Brysbaert. 2011. Subtlex-esp: Spanish word
frequencies based on film subtitles. Psicolgica,
32(2).
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model
for geographic lexical variation. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ?10, pages 1277?
1287, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jennifer Foster,
?
Ozlem C?etinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011.
#hardtoparse: Pos tagging and parsing the twitter-
verse. In Analyzing Microtext, volume WS-11-05 of
AAAI Workshops. AAAI.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: short papers - Volume 2,
HLT ?11, pages 42?47, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
S. Gouws, D. Hovy, and D. Metzler. 2011. Unsuper-
vised mining of lexical variants from noisy text. In
Proceedings of the First workshop on Unsupervised
Learning in NLP, page 82?90.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2012), pages 421?
432, Jeju Island, Korea.
Bo Han, Paul Cook, and Timothy Baldwin. 2013. Lex-
ical normalization for social media text. ACM Trans.
Intell. Syst. Technol., 4(1):5:1?5:27, February.
Hany Hassan and Arul Menezes. 2013. Social text
normalization using contextual graph random walks.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1577?1586, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Ama Herdagdelen. 2013. Twitter n-gram corpus with
demographic metadata. Language Resources and
Evaluation, 47(4):1127?1147.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011a. Insertion, deletion, or substitution?: Nor-
malizing text messages without pre-categorization
6
nor supervision. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: Short Pa-
pers - Volume 2, HLT ?11, pages 71?76, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011b. Recognizing named entities in tweets.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
359?367, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. J. Mach. Learn.
Res., 2:419?444, March.
Alejandro Mosquera and Paloma Moreda. 2012.
Tenor: A lexical normalisation tool for spanish web
2.0 texts. In Text, Speech and Dialogue - 15th Inter-
national Conference (TSD 2012). Springer.
Alejandro Mosquera, Elena Lloret, and Paloma
Moreda. 2012. Towards facilitating the accessibil-
ity of web 2.0 texts through text normalisation. In
Proceedings of the LREC workshop: Natural Lan-
guage Processing for Improving Textual Accessibil-
ity (NLP4ITA) ; Istanbul, Turkey., pages 9?14.
PearAnalytics. 2009. Twitter study. In Retrieved De-
cember 15, 2009 from http://pearanalytics.com/wp-
content/uploads/2009/08/Twitter-Study-August-
2009.pdf.
Luz Rello and Ricardo A Baeza-Yates. 2012. Social
media is not that bad! the lexical quality of social
media. In ICWSM.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: Real-time
event detection by social sensors. In Proceedings
of the 19th International Conference on World Wide
Web, WWW ?10, pages 851?860, New York, NY,
USA. ACM.
Robert Thomson and Tamar Murachver. 2001. Pre-
dicting gender from electronic discourse.
Thurlow and Brown. 2003. Generation txt? the soci-
olinguistics of young people?s text-messaging.
A. Tumasjan, T.O. Sprenger, P.G. Sandner, and I.M.
Welpe. 2010. Predicting elections with twitter:
What 140 characters reveal about political senti-
ment. In Proceedings of the Fourth International
AAAI Conference on Weblogs and Social Media,
pages 178?185.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Pidong Wang and Hwee Tou Ng. 2013. A beam-search
decoder for normalization of social media text with
application to machine translation. In HLT-NAACL,
pages 471?481.
7
