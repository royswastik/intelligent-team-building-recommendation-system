Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 176?186,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Microblogs as Parallel Corpora
Wang Ling123 Guang Xiang2 Chris Dyer2 Alan Black2 Isabel Trancoso 13
(1)L2F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
(2)Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
(3)Instituto Superior Te?cnico, Lisbon, Portugal
{lingwang,guangx,cdyer,awb}@cs.cmu.edu
isabel.trancoso@inesc-id.pt
Abstract
In the ever-expanding sea of microblog data, there
is a surprising amount of naturally occurring par-
allel text: some users create post multilingual mes-
sages targeting international audiences while oth-
ers ?retweet? translations. We present an efficient
method for detecting these messages and extract-
ing parallel segments from them. We have been
able to extract over 1M Chinese-English parallel
segments from Sina Weibo (the Chinese counter-
part of Twitter) using only their public APIs. As a
supplement to existing parallel training data, our
automatically extracted parallel data yields sub-
stantial translation quality improvements in trans-
lating microblog text and modest improvements
in translating edited news commentary. The re-
sources in described in this paper are available at
http://www.cs.cmu.edu/?lingwang/utopia.
1 Introduction
Microblogs such as Twitter and Facebook have
gained tremendous popularity in the past 10 years.
In addition to being an important form of commu-
nication for many people, they often contain ex-
tremely current, even breaking, information about
world events. However, the writing style of mi-
croblogs tends to be quite colloquial, with fre-
quent orthographic innovation (R U still with me
or what?) and nonstandard abbreviations (idk!
shm)?quite unlike the style found in more tra-
ditional, edited genres. This poses considerable
problems for traditional NLP tools, which were
developed with other domains in mind, which of-
ten make strong assumptions about orthographic
uniformity (i.e., there is just one way to spell you).
One approach to cope with this problem is to an-
notate in-domain data (Gimpel et al, 2011).
Machine translation suffers acutely from the
domain-mismatch problem caused by microblog
text. On one hand, standard models are probably
suboptimal since they (like many models) assume
orthographic uniformity in the input. However,
more acutely, the data used to develop these sys-
tems and train their models is drawn from formal
and carefully edited domains, such as parallel web
pages and translated legal documents. MT training
data seldom looks anything like microblog text.
This paper introduces a method for finding nat-
urally occurring parallel microblog text, which
helps address the domain-mismatch problem.
Our method is inspired by the perhaps surpris-
ing observation that a reasonable number of mi-
croblog users tweet ?in parallel? in two or more
languages. For instance, the American entertainer
Snoop Dogg regularly posts parallel messages on
Sina Weibo (Mainland China?s equivalent of Twit-
ter), for example, watup Kenny Mayne!! - Kenny
Mayne?????????, where an English
message and its Chinese translation are in the
same post, separated by a dash. Our method is able
to identify and extract such translations. Briefly,
this requires determining if a tweet contains more
than one language, if these multilingual utterances
contain translated material (or are due to some-
thing else, such as code switching), and what the
translated spans are.
The paper is organized as follows. Section 2
describes the related work in parallel data extrac-
tion. Section 3 presents our model to extract par-
allel data within the same document. Section 4
describes our extraction pipeline. Section 5 de-
scribes the data we gathered from both Sina Weibo
(Chinese-English) and Twitter (Chinese-English
and Arabic-English). We then present experiments
showing that our harvested data not only substan-
tially improves translations of microblog text with
176
existing (and arguably inappropriate) translation
models, but that it improves the translation of
more traditional MT genres, like newswire. We
conclude in Section 6.
2 Related Work
Automatic collection of parallel data is a well-
studied problem. Approaches to finding par-
allel web documents automatically have been
particularly important (Resnik and Smith, 2003;
Fukushima et al, 2006; Li and Liu, 2008; Uszko-
reit et al, 2010; Ture and Lin, 2012). These
broadly work by identifying promising candidates
using simple features, such as URL similarity or
?gist translations? and then identifying truly par-
allel segments with more expensive classifiers.
More specialized resources were developed using
manual procedures to leverage special features of
very large collections, such as Europarl (Koehn,
2005).
Mining parallel or comparable messages from
microblogs has mainly relied on Cross-Lingual In-
formation Retrieval techniques (CLIR). Jelh et al
(2012) attempt to find pairs of tweets in Twitter us-
ing Arabic tweets as search queries in a CLIR sys-
tem. Afterwards, the model described in (Xu et al,
2001) is applied to retrieve a set of ranked trans-
lation candidates for each Arabic tweet, which are
then used as parallel candidates.
The work on mining parenthetical transla-
tions (Lin et al, 2008), which attempts to find
translations within the same document, has some
similarities with our work, since parenthetical
translations are within the same document. How-
ever, parenthetical translations are generally used
to translate names or terms, which is more lim-
ited than our work which extracts whole sentence
translations.
Finally, crowd-sourcing techniques to obtain
translations have been previously studied and ap-
plied to build datasets for casual domains (Zbib
et al, 2012; Post et al, 2012). These approaches
require remunerated workers to translate the mes-
sages, and the amount of messages translated per
day is limited. We aim to propose a method that
acquires large amounts of parallel data for free.
The drawback is that there is a margin of error in
the parallel segment identification and alignment.
However, our system can be tuned for precision or
for recall.
3 Parallel Segment Retrieval
We will first abstract from the domain of Mi-
croblogs and focus on the task of retrieving par-
allel segments from single documents. Prior work
on finding parallel data attempts to reason about
the probability that pairs of documents (x, y) are
parallel. In contrast, we only consider one doc-
ument at a time, defined by x = x1, x2, . . . , xn,
and consisting of n tokens, and need to deter-
mine whether there is parallel data in x, and if
so, where are the parallel segments and their lan-
guages. For simplicity, we assume that there are
at most 2 continuous segments that are parallel.
As representation for the parallel seg-
ments within the document, we use the tuple
([p, q], l, [u, v], r, a). The word indexes [p, q] and
[u, v] are used to identify the left segment (from
p to q) and right segment (from u to v), which
are parallel. We shall refer [p, q] and [u, v] as the
spans of the left and right segments. To avoid
overlaps, we set the constraint p ? q < u ? v.
Then, we use l and r to identify the language of
the left and right segments, respectively. Finally, a
represents the word alignment between the words
in the left and the right segments.
The main problem we address is to find the
parallel data when the boundaries of the parallel
segments are not defined explicitly. If we knew
the indexes [p, q] and [u, v], we could simply run
a language detector for these segments to find l
and r. Then, we would use an word alignment
model (Brown et al, 1993; Vogel et al, 1996),
with source s = xp, . . . , xq, target t = xu, . . . , xv
and lexical table ?l,r to calculate the Viterbi align-
ment a. Finally, from the probability of the word
alignments, we can determine whether the seg-
ments are parallel.
Thus, our model will attempt to find the opti-
mal values for the segments [p, q][u, v], languages
l, r and word alignments a jointly. However, there
are two problems with this approach. Firstly, word
alignment models generally attribute higher prob-
abilities to smaller segments, since these are the
result of a smaller product chain of probabilities.
In fact, because our model can freely choose the
segments to align, choosing only one word as the
left segment that is well aligned to a word in the
right segment would be the best choice. This
is obviously not our goal, since we would not
obtain any useful sentence pairs. Secondly, in-
ference must be performed over the combination
of all latent variables, which is intractable using
177
a brute force algorithm. We shall describe our
model to solve the first problem in 3.1 and our
dynamic programming approach to make the in-
ference tractable in 3.2.
3.1 Model
We propose a simple (non-probabilistic) three-
factor model that models the spans of the parallel
segments, their languages, and word alignments
jointly. This model is defined as follows:
S([u, v], r, [p, q],l, a | x) =
S?S ([p, q], [u, v] | x)?
S?L(l, r | [p, q], [u, v], x)?
S?T (a | [p, q], l, [u, v], r, x)
Each of the components is weighted by the pa-
rameters ?, ? and ?. We set these values empiri-
cally ? = 0.3, ? = 0.3 and ? = 0.4, and leave the
optimization of these parameters as future work.
We discuss the components of this model in turn.
Span score SS . We define the score of hypothe-
sized pair of spans [p, q], [u, v] as:
SS([p, q], [u, v] | x) =
(q ? p+ 1) + (v ? u+ 1)?
0<p??q?<u??v??n(q? ? p? + 1) + (v? ? u? + 1)
?
?([p, q], [u, v], x)
The first factor is a distribution over all spans that
assigns higher probability to segmentations that
cover more words in the document. It is highest
for segmentations that cover all the words in the
document (this is desirable since there are many
sentence pairs that can be extracted but we want
to find the largest sentence pair in the document).
The function ? takes on values of 0 or 1 depend-
ing on whether certain constraints are violated,
these include: parenthetical constraints that en-
force that spans must not break text within par-
enthetical characters and language constraints that
ensure that we do break a sequence of Mandarin
characters, Arabic words or Latin words.
Language score SL. The language score
SL(l, r | [p, q], [u, v], x) indicates whether the lan-
guage labels l, r are appropriate to the document
contents:
SL(l, r | [p, q], [u, v], x) =?q
i=p L(l, xi) +
?v
i=u L(r, xi)
n
where L(l, x) is a language detection function that
yields 1 if the word xi is in language l, and 0 oth-
erwise. We build the function simply by consid-
ering all words that are composed of Latin char-
acters as English, Arabic characters as Arabic and
Han characters as Mandarin. This approach is not
perfect, but it is simple and works reasonably well
for our purposes.
Translation score ST . The translation score
ST (a | [p, q], l, [u, v], r) indicates whether [p, q]
is a reasonable translation of [u, v] with the align-
ment a. We rely on IBM Model 1 probabilities for
this score:
ST (a | [p, q], l, [u, v], r, x) =
1
(q ? p+ 1)v?u+2
v?
i=u
PM1(xi | xai).
The lexical tables PM1 for the various language
pairs are trained a priori using available parallel
corpora. While IBM Model 1 produces worse
alignments than other models, in our problem, we
need to efficiently consider all possible spans, lan-
guage pairs and word alignments, which makes
the problem intractable. We will show that dy-
namic programing can be used to make this prob-
lem tractable, using Model 1. Furthermore, IBM
Model 1 has shown good performance for sen-
tence alignment systems previously (Xu et al,
2005; Braune and Fraser, 2010).
3.2 Inference
Our goal is to find the spans, language pair and
alignments such that:
argmax
[p,q],l,[u,v],r,a
S([p, q], l, [u, v], r, a | x) (1)
A high score indicates that the predicted bispan is
likely to correspond to a valid parallel span, so we
set a constant threshold ? to determine whether a
document has parallel data, i.e., the value of z:
z? = max
[u,v],r,[p,q],l,a
S([u, v], r, [p, q], l, a | x) > ?
Naively maximizing Eq. 1 would require
O(|x|6) operations, which is too inefficient to be
practical on large datasets. To process millions
of documents, this process would need to be op-
timized.
The main bottleneck of the naive algorithm is
finding new Viterbi Model 1 word alignments ev-
ery time we change the spans. Thus, we propose
178
an iterative approach to compute the Viterbi word
alignments for IBM Model 1 using dynamic pro-
gramming.
Dynamic programming search. The insight we
use to improve the runtime is that the Viterbi
word alignment of a bispan can be reused to cal-
culate the Viterbi word alignments of larger bis-
pans. The algorithm operates on a 4-dimensional
chart of bispans. It starts with the minimal valid
span (i.e., [0, 0], [1, 1]) and progressively builds
larger spans from smaller ones. Let Ap,q,u,v rep-
resent the Viterbi alignment (under ST ) of the bis-
pan [p, q], [u, v]. The algorithm uses the follow-
ing recursions defined in terms of four operations
?{+v,+u,+p,+q} that manipulate a single dimension
of the bispan to construct larger spans:
? Ap,q,u,v+1 = ?+v(Ap,q,u,v) adds one token to
the end of the right span with index v + 1 and
find the viterbi alignment for that token. This
requires iterating over all the tokens in the left
span, [p, q] and possibly updating their align-
ments. See Fig. 1 for an illustration.
? Ap,q,u+1,v = ?+u(Ap,q,u,v) removes the first to-
ken of the right span with index u, so we only
need to remove the alignment from u, which can
be done in time O(1).
? Ap,q+1,u,v = ?+q(Ap,q,u,v) adds one token to
the end of the left span with index q + 1, we
need to check for each word in the right span, if
aligning to the word in index q+1 yields a better
translation probability. This update requires n?
q + 1 operations.
? Ap+1,q,u,v = ?+p(Ap,q,u,v) removes the first
token of the left span with index p. After re-
moving the token, we need to find new align-
ments for all tokens that were aligned to p.
Thus, the number of operations for this update
is K ? (q ? p + 1), where K is the number of
words that were aligned to p. In the best case, no
words are aligned to the token in p, and we can
simply remove it. In the worst case, if all target
words were aligned to p, this update will result
in the recalculation of all Viterbi Alignments.
The algorithm proceeds until all valid cells have
been computed. One important aspect is that the
update functions differ in complexity, so the se-
quence of updates we apply will impact the per-
formance of the system. Most spans are reach-
able using any of the four update functions. For
instance, the span A2,3,4,5 can be reached us-
ing ?+v(A2,3,4,4), ?+u(A2,3,3,5), ?+q(A2,2,4,5) or
?+p(A1,3,4,5). However, we want to use ?+u
a b - A B
a
b
-
A
B
a b - A B
p
qu v
p
qu v?+v
Figure 1: Illustration of the ?+v operator. The
light gray boxes show the parallel span and the
dark boxes show the span?s Viterbi alignment.
In this example, the parallel message contains a
?translation? of a b to A B.
whenever possible, since it only requires one op-
eration, although that is not always possible. For
instance, the state A2,2,2,4 cannot be reached us-
ing ?+u, since the state A2,2,1,4 is not valid, be-
cause the spans overlap. If this happens, incre-
mentally more expensive updates need to be used,
such as ?+v, then ?+q, which are in the same order
of complexity. Finally, we want to minimize the
use of ?+p, which is quadratic in the worst case.
Thus, we use the following recursive formulation
that guarantees the optimal outcome:
Ap,q,u,v =
?
????
????
?+u(Ap,q,u?1,v) if u > q + 1
?+v(Ap,q,u,v?1) else if v > q + 1
?+p(Ap?1,q,u,v) else if q = p+ 1
?+q(Ap,q?1,u,v) otherwise
This transition function applies the cheapest
possible update to reach state Ap,q,u,v.
Complexity analysis. We can see that ?+u
is only needed in the following the cases
[0, 1][2, 2], [1, 2][3, 3], ? ? ? , [n ? 2, n ? 1][n, n].
Since, this update is quadratic in the worst
case, the complexity of this operations is
O(n3). The update ?+q, is applied to the cases
[?, 1][2, 2], [?, 2][3, 3], ? ? ? , [?, n?1], [n, n], where
? denotes any number within the span constraints
but not present in previous updates. Since, the
update is linear and we need to iterate through
all tokens twice, this update takes O(n3) opera-
tions. The update ?+v is applied for the cases
[?, 1][2, ?], [?, 2][3, ?], ? ? ? , [?, n? 1], [n, ?]. Thus,
with three degrees of freedom and a linear update,
it runs in O(n4) time. Finally, update ?+u runs in
constant time, but is run for all remaining cases,
which constitute O(n4) space. By summing the
179
executions of all updates, we observe that the or-
der of magnitude of our exact inference process is
O(n4). Note that for exact inference, it is not pos-
sible to get a lower order of magnitude, since we
need to at least iterate through all possible span
values once, which takes O(n4) time.
4 Parallel Data Extraction
We will now describe our method to extract par-
allel data from Microblogs. The target domains
in this work are Twitter and Sina Weibo, and
the main language pair is Chinese-English. Fur-
thermore, we also run the system for the Arabic-
English language pair using the Twitter data.
For the Twitter domain, we use a previously
crawled dataset from the years 2008 to 2013,
where one million tweets are crawled every day.
In total, we processed 1.6 billion tweets.
Regarding Sina Weibo, we built a crawler that
continuously collects tweets from Weibo. We start
from one seed user and collect his posts, and then
we find the users he follows that we have not con-
sidered, and repeat. Due to the rate limiting es-
tablished by the Weibo API1, we are restricted in
terms of number of requests every hour, which
greatly limits the amount of messages we can col-
lect. Furthermore, each request can only fetch up
to 100 posts from a user, and subsequent pages of
100 posts require additional API calls. Thus, to
optimize the number of parallel posts we can col-
lect per request, we only crawl all messages from
users that have at least 10 parallel tweets in their
first 100 posts. The number of parallel messages
is estimated by running our alignment model, and
checking if ? > ?, where ? was set empirically
initially, and optimized after obtaining annotated
data, which will be detailed in 5.1. Using this
process, we crawled 65 million tweets from Sina
Weibo within 4 months.
In both cases, we first filter the collection of
tweets for messages containing at least one trigram
in each language of the target language pair, deter-
mined by their Unicode ranges. This means that
for the Chinese-English language pair, we only
keep tweets with more than 3 Mandarin charac-
ters and 3 latin words. Furthermore, based on the
work in (Jelh et al, 2012), if a tweet A is iden-
tified as a retweet, meaning that it references an-
other tweetB, we also consider the hypothesis that
these tweets may be mutual translations. Thus, if
A and B contain trigrams in different languages,
1http://open.weibo.com/wiki/API??/en
these are also considered for the extraction of par-
allel data. This is done by concatenating tweets A
and B, and adding the constraint that [p, q] must
be within A and [u, v] must be within B. Finally,
identical duplicate tweets are removed.
After filtering, we obtained 1124k ZH-EN
tweets from Sina Weibo, 868k ZH-EN and 136k
AR-EN tweets from Twitter. These language pairs
are not definite, since we simply check if there is
a trigram in each language.
Finally, we run our alignment model described
in section 3, and obtain the parallel segments and
their scores, which measure how likely those seg-
ments are parallel. In this process, lexical tables
for EN-ZH language pair used by Model 1 were
built using the FBIS dataset (LDC2003E14) for
both directions, a corpus of 300K sentence pairs
from the news domain. Likewise, for the EN-
AR language pair, we use a fraction of the NIST
dataset, by removing the data originated from UN,
which leads to approximately 1M sentence pairs.
5 Experiments
We evaluate our method in two ways. First, intrin-
sically, by observing how well our method identi-
fies tweets containing parallel data, the language
pair and what their spans are. Second, extrinsi-
cally, by looking at how well the data improves
a translation task. This methodology is similar to
that of Smith et al (2010).
5.1 Parallel Data Extraction
Data. Our method needs to determine if a given
tweet contains parallel data, and if so, what is
the language pair of the data, and what segments
are parallel. Thus, we had a native Mandarin
speaker, also fluent in English, to annotate 2000
tweets sampled from crawled Weibo tweets. One
important question of answer is what portion of
the Microblogs contains parallel data. Thus, we
also use the random sample Twitter and annotated
1200 samples, identifying whether each sample
contains parallel data, for the EN-ZH and AR-EN
filtered tweets.
Metrics. To test the accuracy of the score S, we
ordered all 2000 samples by score. Then, we cal-
culate the precision, recall and accuracy at increas-
ing intervals of 10% of the top samples. We count
as a true positive (tp) if we correctly identify a par-
allel tweet, and as a false positive (fp) spuriously
detect a parallel tweet. Finally, a true negative (tn)
occurs when we correctly detect a non-parallel
180
tweet, and a false negative (fn) if we miss a par-
allel tweet. Then, we set the precision as tptp+fp ,
recall as tptp+fn and accuracy as tp+tntp+fp+tn+fn . Forlanguage identification, we calculate the accuracy
based on the number of instances that were iden-
tified with the correct language pair. Finally, to
evaluate the segment alignment, we use the Word
Error Rate (WER) metric, without substitutions,
where we compare the left and right spans of our
system and the respective spans of the reference.
We count an insertion error (I) for each word in
our system?s spans that is not present in the refer-
ence span and a deletion error (D) for each word
in the reference span that is not present in our sys-
tem?s spans. Thus, we set WER = D+IN , where
N is the number of tokens in the tweet. To com-
pute this score for the whole test set, we compute
the average of the WER for each sample.
Results. The precision, recall and accuracy
curves are shown in Figure 2. The quality of the
parallel sentence detection did not vary signifi-
cantly with different setups, so we will only show
the results for the best setup, which is the baseline
model with span constraints.
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
0.7	 ?
0.8	 ?
0.9	 ?
1	 ?
10%	 ? 20%	 ? 30%	 ? 40%	 ? 50%	 ? 60%	 ? 70%	 ? 80%	 ? 90%	 ? 100%	 ?
Precision	 ?
Recall	 ?
Accuracy	 ?
Figure 2: Precision, recall and accuracy curves
for parallel data detection. The y-axis denotes the
scores for each metric, and the x-axis denotes the
percentage of the highest scoring sentence pairs
that are kept.
From the precision and recall curves, we ob-
serve that most of the parallel data can be found
at the top 30% of the filtered tweets, where 5 in 6
tweets are detected correctly as parallel, and only
1 in every 6 parallel sentences is lost. We will de-
note the score threshold at this point as ?, which is
a good threshold to estimate on whether the tweet
is parallel. However, this parameter can be tuned
for precision or recall. We also see that in total,
30% of the filtered tweets are parallel. If we gen-
eralize this ratio for the complete set with 1124k
tweets, we can expect approximately 337k paral-
lel sentences. Finally, since 65 million tweets were
extracted to generate the 337k tweets, we estimate
that approximately 1 parallel tweet can be found
for every 200 tweets we process using our tar-
geted approach. On the other hand, from the 1200
tweets from Twitter, we found that 27 had parallel
data in the ZH-EN pair, if we extrapolate for the
whole 868k filtered tweets, we expect that we can
find 19530. 19530 parallel sentences from 1.6 bil-
lion tweets crawled randomly, represents 0.001%
of the total corpora. For AR-EN, a similar re-
sult was obtained where we expect 12407 tweets
out of the 1.6 billion to be parallel. This shows
that targeted approaches can substantially reduce
the crawling effort required to find parallel tweets.
Still, considering that billions of tweets are posted
daily, this is a substantial source of parallel data.
The remainder of the tests will be performed on
the Weibo dataset, which contains more parallel
data. Tests on the Twitter data will be conducted
as future work, when we process Twitter data on a
larger scale to obtain more parallel sentences.
For the language identification task, we had an
accuracy of 99.9%, since distinguishing English
and Mandarin is trivial. The small percentage of
errors originated from other latin languages (Ex:
French) due to our naive language detector.
As for the segment alignment task. Our base-
line system with no constraints obtains a WER of
12.86%, and this can be improved to 11.66% by
adding constraints to possible spans. This shows
that, on average, approximately 1 in 9 words on
the parallel segments is incorrect. However, trans-
lation models are generally robust to such kinds of
errors and can learn good translations even in the
presence of imperfect sentence pairs.
Among the 578 tweets that are parallel, 496
were extracted within the same tweet and 82 were
extracted from retweets. Thus, we see that the ma-
jority of the parallel data comes from within the
same tweet.
Topic analysis. To give an intuition about the
contents of the parallel data we found, we looked
at the distribution over topics of the parallel
dataset inferred by LDA (Blei et al, 2003). Thus,
we grouped the Weibo filtered tweets by users,
and ran LDA over the predicted English segments,
with 12 topics. The 7 most interpretable topics are
shown in Table 1. We see that the data contains a
181
# Topic Most probable words in topic
1 (Dating) love time girl live mv back word night rt wanna
2 (Entertainment) news video follow pong image text great day today fans
3 (Music) cr day tour cn url amazon music full concert alive
4 (Religion) man god good love life heart would give make lord
5 (Nightlife) cn url beijing shanqi party adj club dj beijiner vt
6 (Chinese News) china chinese year people world beijing years passion country government
7 (Fashion) street fashion fall style photo men model vogue spring magazine
Table 1: Most probable words inferred using LDA in several topics from the parallel data extracted from
Weibo. Topic labels (in parentheses) were assigned manually for illustration purposes.
variety of topics, both formal (Chinese news, reli-
gion) and informal (entertainment, music).
Example sentence pairs. To gain some perspec-
tive on the type of sentence pairs we are extract-
ing, we will illustrate some sentence pairs we
crawled and aligned automatically. Table 2 con-
tains 5 English-Mandarin and 4 English-Arabic
sentence pairs that were extracted automatically.
These were chosen, since they contain some as-
pects that are characteristic of the text present in
Microblogs and Social Media. These are:
? Abbreviations - In most sentence pairs exam-
ples, we can witness the use of abbreviated
forms of English words, such as wanna, TMI,
4 and imma. These can be normalized as want
to, too much information, for and I am going
to, respectively. In sentence 5, we observe that
this phenomena also occurs in Mandarin. We
find that TMD is a popular way to write???
whose Pinyin rendering is ta? ma? de. The mean-
ing of this expression depends on the context it
is used, and can convey a similar connotation
as adding the intensifier the hell to an English
sentence.
? Jargon - Another common phenomena is the
appearance of words that are only used in sub-
communities. For instance, in sentence pair 4,
we the jargon word cday is used, which is a col-
loquial variant for birthday.
? Emoticons - In sentence 8, we observe the pres-
ence of the emoticon :), which is frequently
used in this media. We found that emoticons are
either translated as they are or simply removed,
in most cases.
? Syntax errors - In the domain of microblogs, it
is also common that users do not write strictly
syntactic sentences, for instance, in sentence
pair 7, the sentence onni this gift only 4 u, is
clearly not syntactically correct. Firstly, onni
is a named entity, yet it is not capitalized. Sec-
ondly, a comma should follow onni. Thirdly, the
verb is should be used after gift. Having exam-
ples of these sentences in the training set, with
common mistakes (intentional or not), might
become a key factor in training MT systems that
can be robust to such errors.
? Dialects - We can observe a much broader range
of dialects in our data, since there are no di-
alect standards in microblogs. For instance, in
sentence pair 6, we observe an arabic word (in
bold) used in the spoken Arabic dialect used in
some countries along the shores of the Persian
Gulf, which means means the next. In standard
Arabic, a significantly different form is used.
We can also see in sentence pair 9 that our
aligner does not alway make the correct choice
when determining spans. In this case, the segment
RT @MARYAMALKHAWAJA: was included in the
English segment spuriously, since it does not cor-
respond to anything in the Arabic counterpart.
5.2 Machine Translation Experiments
We report on machine translation experiments us-
ing our harvested data in two domains: edited
news and microblogs.
News translation. For the news test, we cre-
ated a new test set from a crawl of the Chinese-
English documents on the Project Syndicate web-
site2, which contains news commentary articles.
We chose to use this data set, rather than more
standard NIST test sets to ensure that we had re-
cent documents in the test set (the most recent
NIST test sets contain documents published in
2007, well before our microblog data was created).
We extracted 1386 parallel sentences for tuning
and another 1386 sentences for testing, from the
manually aligned segments. For this test set, we
used 8 million sentences from the full NIST par-
allel dataset as the language model training data.
We shall call this test set Syndicate.
2http://www.project-syndicate.org/
182
ENGLISH MANDARIN
1 i wanna live in a wes anderson world ??????Wes Anderson????
2 Chicken soup, corn never truly digests. TMI. ??????????????????.??
3 To DanielVeuleman yea iknw imma work on that ?DanielVeuleman?????????????????
4 msg 4 Warren G his cday is today 1 yr older. ????Warren G????????????????
5 Where the hell have you been all these years? ????TMD????
ENGLISH ARABIC
6 It?s gonna be a warm week! Qk ?


AJ
? @ ? ?J.?B@
7 onni this gift only 4 u ?? ?? 	? ?K
Y?? @ ? 	Y? ?

	
G?

@
8 sunset in aqaba :) (: ?J. ???@ ?

	
? ?? ??@ H. ?Q
	
?
9 RT @MARYAMALKHAWAJA: there is a call @Y 	? ??A 	J? ?Y? ?


	
? H@Q?A 	??? Z @Y 	K ?A 	J?for widespread protests in #bahrain tmrw
Table 2: Examples of English-Mandarin and English-Arabic sentence pairs. The English-Mandarin
sentences were extracted from Sina Weibo and the English-Arabic sentences were extracted from Twitter.
Some messages have been shorted to fit into the table. Some interesting aspects of these sentence pairs
are marked in bold.
Microblog translation. To carry out the mi-
croblog translation experiments, we need a high
quality parallel test set. Since we are not aware
of such a test set, we created one by manually se-
lecting parallel messages from Weibo. Our proce-
dure was as follows. We selected 2000 candidate
Weibo posts from users who have a high num-
ber of parallel tweets according to our automatic
method (at least 2 in every 5 tweets). To these, we
added another 2000 messages from our targeted
Weibo crawl, but these had no requirement on the
proportion of parallel tweets they had produced.
We identified 2374 parallel segments, of which we
used 1187 for development and 1187 for testing.
We refer to this test set as Weibo.3
Obviously, we removed the development and
test sets from our training data. Furthermore, to
ensure that our training data was not too similar to
the test set in the Weibo translation task, we fil-
tered the training data to remove near duplicates
by computing edit distance between each paral-
lel sentence in the heldout set and each training
instance. If either the source or the target sides
of the a training instance had an edit distance of
less than 10%, we removed it.4 As for the lan-
guage models, we collected a further 10M tweets
from Twitter for the English language model and
another 10M tweets from Weibo for the Chinese
language model.
3We acknowledge that self-translated messages are prob-
ably not a typically representative sample of all microblog
messages. However, we do not have the resources to produce
a carefully curated test set with a more broadly representative
distribution. Still, we believe these results are informative as
long as this is kept in mind.
4Approximately 150,000 training instances removed.
Syndicate Weibo
ZH-EN EN-ZH ZH-EN EN-ZH
FBIS 9.4 18.6 10.4 12.3
NIST 11.5 21.2 11.4 13.9
Weibo 8.75 15.9 15.7 17.2
FBIS+Weibo 11.7 19.2 16.5 17.8
NIST+Weibo 13.3 21.5 16.9 17.9
Table 3: BLEU scores for different datasets in dif-
ferent translation directions (left to right), broken
with different training corpora (top to bottom).
Baselines. We report results on these test sets us-
ing different training data. First, we use the FBIS
dataset which contains 300K high quality sentence
pairs, mostly in the broadcast news domain. Sec-
ond, we use the full 2012 NIST Chinese-English
dataset (approximately 8M sentence pairs, includ-
ing FBIS). Finally, we use our crawled data (re-
ferred as Weibo) by itself and also combined with
the two previous training sets.
Setup. We use the Moses phrase-based MT sys-
tem with standard features (Koehn et al, 2003).
For reordering, we use the MSD reordering
model (Axelrod et al, 2005). As the language
model, we use a 5-gram model with Kneser-
Ney smoothing. The weights were tuned using
MERT (Och, 2003). Results are presented with
BLEU-4 (Papineni et al, 2002).
Results. The BLEU scores for the different par-
allel corpora are shown in Table 3 and the top 10
out-of-vocabulary (OOV) words for each dataset
are shown in Table 4. We observe that for the
Syndicate test set, the NIST and FBIS datasets
183
Syndicate (test) Weibo (test)
FBIS NIST Weibo FBIS NIST Weibo
obama (83) barack (59) democracies (15) 2012 (24) showstudio (9) submissions (4)
barack (59) namo (6) imbalances (13) alanis (13) crue (9) ivillage (4)
princeton (40) mitt (6) mahmoud (12) crue (9) overexposed (8) scola (3)
ecb (8) guant (6) millennium (9) showstudio (9) tweetmeian (5) rbst (3)
bernanke (8) fairtrade (6) regimes (8) overexposed (8) tvd (5) curitiba (3)
romney (7) hollande (5) wolfowitz (7) itunes (8) iheartradio (5) zeman (2)
gaddafi (7) wikileaks (4) revolutions (7) havoc (8) xoxo (4) @yaptv (2)
merkel (7) wilders (3) qaddafi (7) sammy (6) snoop (4) witnessing (2)
fats (7) rant (3) geopolitical (7) obama (6) shinoda (4) whoohooo (2)
dialogue (7) esm (3) genome (7) lol (6) scrapbook (4) wbr (2)
Table 4: The most frequent out-of-vocabulary (OOV) words and their counts for the two English-source
test sets with three different training sets.
perform better than our extracted parallel data.
This is to be expected, since our dataset was ex-
tracted from an extremely different domain. How-
ever, by combining the Weibo parallel data with
this standard data, improvements in BLEU are ob-
tained. Error analysis indicates that one major fac-
tor is that names from current events, such as Rom-
ney and Wikileaks do not occur in the older NIST
and FBIS datasets, but they are represented in the
Weibo dataset. Furthermore, we also note that the
system built on the Weibo dataset does not per-
form substantially worse than the one trained on
the FBIS dataset, a further indication that harvest-
ing parallel microblog data yields a diverse collec-
tion of translated material.
For the Weibo test set, a significant improve-
ment over the news datasets can be achieved us-
ing our crawled parallel data. Once again newer
terms, such as iTunes, are one of the reasons older
datasets perform less well. However, in this case,
the top OOV words of the news domain datasets
are not the most accurate representation of cov-
erage problems in this domain. This is because
many frequent words in microblogs, e.g., nonstan-
dard abbreviations, like u and 4 are found in the
news domain as words, albeit with different mean-
ings. Thus, the OOV table gives an incomplete
picture of the translation problems when using
the news domain corpora to translate microblogs.
Also, some structural errors occur when training
with the news domain datasets, one such example
is shown in table 5, where the character ? is in-
correctly translated to said. This occurs because
this type of constructions is infrequent in news
datasets. Furthermore, we can see that compound
expressions, such as the translation from ???
? to party time are also learned.
Finally, we observe that combining the datasets
Source ?sam farrar??????
Reference to sam farrar , party time
FBIS farrar to sam said , in time
NIST to sam farrar said , the moment
WEIBO to sam farrar , party time
Table 5: Translation Examples using different
training sets.
yields another gain over individual datasets, both
in the Syndicate and in the Weibo test sets.
6 Conclusion
We presented a framework to crawl parallel data
from microblogs. We find parallel data from sin-
gle posts, with translations of the same sentence
in two languages. We show that a considerable
amount of parallel sentence pairs can be crawled
from microblogs and these can be used to improve
Machine Translation by updating our translation
tables with translations of newer terms. Further-
more, the in-domain data can substantially im-
prove the translation quality on microblog data.
The resources described in this paper and fur-
ther developments are available to the general pub-
lic at http://www.cs.cmu.edu/?lingwang/utopia.
Acknowledgements
The PhD thesis of Wang Ling is supported by FCT
grant SFRH/BD/51157/2010. The authors wish
to express their gratitude to thank William Cohen,
Noah Smith, Waleed Ammar, and the anonymous
reviewers for their insight and comments. We are
also extremely grateful to Brendan O?Connor for
providing the Twitter data and to Philipp Koehn
and Barry Haddow for providing the Project Syn-
dicate data.
184
References
[Axelrod et al2005] Amittai Axelrod, Ra Birch Mayne,
Chris Callison-burch, Miles Osborne, and David
Talbot. 2005. Edinburgh system description for the
2005 iwslt speech translation evaluation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT.
[Blei et al2003] David M. Blei, Andrew Y. Ng, and
Michael I. Jordan. 2003. Latent dirichlet alocation.
J. Mach. Learn. Res., 3:993?1022, March.
[Braune and Fraser2010] Fabienne Braune and Alexan-
der Fraser. 2010. Improved unsupervised sentence
alignment for symmetrical and asymmetrical paral-
lel corpora. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 81?89, Stroudsburg, PA, USA.
Association for Computational Linguistics.
[Brown et al1993] Peter F. Brown, Vincent J. Della
Pietra, Stephen A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical machine
translation: parameter estimation. Comput. Lin-
guist., 19:263?311, June.
[Fukushima et al2006] Ken?ichi Fukushima, Kenjiro
Taura, and Takashi Chikayama. 2006. A fast and
accurate method for detecting English-Japanese par-
allel texts. In Proceedings of the Workshop on Mul-
tilingual Language Resources and Interoperability,
pages 60?67, Sydney, Australia, July. Association
for Computational Linguistics.
[Gimpel et al2011] Kevin Gimpel, Nathan Schneider,
Brendan O?Connor, Dipanjan Das, Daniel Mills, Ja-
cob Eisenstein, Michael Heilman, Dani Yogatama,
Jeffrey Flanigan, and Noah A. Smith. 2011. Part-
of-speech tagging for twitter: annotation, features,
and experiments. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers - Volume 2, HLT ?11, pages 42?47, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
[Jelh et al2012] Laura Jelh, Felix Hiebel, and Stefan
Riezler. 2012. Twitter translation using translation-
based cross-lingual retrieval. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 410?421, Montre?al, Canada, June. Asso-
ciation for Computational Linguistics.
[Koehn et al2003] Philipp Koehn, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 48?54,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
[Koehn2005] Philipp Koehn. 2005. Europarl: A Par-
allel Corpus for Statistical Machine Translation. In
Proceedings of the tenth Machine Translation Sum-
mit, pages 79?86, Phuket, Thailand. AAMT, AAMT.
[Li and Liu2008] Bo Li and Juan Liu. 2008. Mining
Chinese-English parallel corpora from the web. In
Proceedings of the 3rd International Joint Confer-
ence on Natural Language Processing (IJCNLP).
[Lin et al2008] Dekang Lin, Shaojun Zhao, Benjamin
Van Durme, and Marius Pas?ca. 2008. Mining par-
enthetical translations from the web by word align-
ment. In Proceedings of ACL-08: HLT, pages 994?
1002, Columbus, Ohio, June. Association for Com-
putational Linguistics.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 160?167, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Papineni et al2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine trans-
lation. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 311?318, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
[Post et al2012] Matt Post, Chris Callison-Burch, and
Miles Osborne. 2012. Constructing parallel cor-
pora for six indian languages via crowdsourcing. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 401?409, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
[Resnik and Smith2003] Philip Resnik and Noah A.
Smith. 2003. The web as a parallel corpus. Compu-
tational Linguistics, 29:349?380.
[Smith et al2010] Jason R. Smith, Chris Quirk, and
Kristina Toutanova. 2010. Extracting parallel sen-
tences from comparable corpora using document
level alignment. In Proceedings of the 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics.
[Ture and Lin2012] Ferhan Ture and Jimmy Lin. 2012.
Why not grab a free lunch? mining large corpora for
parallel sentences to improve translation modeling.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 626?630, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
[Uszkoreit et al2010] Jakob Uszkoreit, Jay Ponte,
Ashok C. Popat, and Moshe Dubiner. 2010. Large
scale parallel document mining for machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 1101?
1109.
[Vogel et al1996] Stephan Vogel, Hermann Ney, and
Christoph Tillmann. 1996. Hmm-based word align-
ment in statistical translation. In Proceedings of the
16th conference on Computational linguistics - Vol-
ume 2, COLING ?96, pages 836?841, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
[Xu et al2001] Jinxi Xu, Ralph Weischedel, and Chanh
Nguyen. 2001. Evaluating a probabilistic model
185
for cross-lingual information retrieval. In Proceed-
ings of the 24th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?01, pages 105?110, New
York, NY, USA. ACM.
[Xu et al2005] Jia Xu, Richard Zens, and Hermann
Ney. 2005. Sentence segmentation using ibm word
alignment model 1. In Proceedings of EAMT 2005
(10th Annual Conference of the European Associa-
tion for Machine Translation, pages 280?287.
[Zbib et al2012] Rabih Zbib, Erika Malchiodi, Jacob
Devlin, David Stallard, Spyros Matsoukas, Richard
Schwarz, John Makhoul, Omar F. Zaidan, and Chris
Callison-Burch. 2012. Machine translation of Ara-
bic dialects. In Proceedings of the 2012 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies.
186
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 836?842,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Extracting Events with Informal Temporal References in Personal
Histories in Online Communities
Miaomiao Wen, Zeyu Zheng, Hyeju Jang, Guang Xiang, Carolyn Penstein Rose?
Language Technologies Institute, Carnegie Mellon University
{mwen,zeyuz,hyejuj,guangx,cprose}@cs.cmu.edu
Abstract
We present a system for extracting the
dates of illness events (year and month of
the event occurrence) from posting histo-
ries in the context of an online medical
support community. A temporal tagger re-
trieves and normalizes dates mentioned in-
formally in social media to actual month
and year referents. Building on this, an
event date extraction system learns to in-
tegrate the likelihood of candidate dates
extracted from time-rich sentences with
temporal constraints extracted from event-
related sentences. Our integrated model
achieves 89.7% of the maximum perfor-
mance given the performance of the tem-
poral expression retrieval step.
1 Introduction
In this paper we present a challenging new event
date extraction task. Our technical contribution
is a temporal tagger that outperforms previously
published baseline approaches in its ability to
identify informal temporal expressions (TE) and
that normalizes each of them to an actual month
and year (Chang and Manning, 2012; Strotgen
and Gertz, 2010). This temporal tagger then con-
tributes towards high performance at matching
event mentions with the month and year in which
they occurred based on the complete posting his-
tory of users. It does so with high accuracy on
informal event mentions in social media by learn-
ing to integrate the likelihood of multiple candi-
date dates extracted from event mentions in time-
rich sentences with temporal constraints extracted
from event-related sentences.
Despite considerable prior work in temporal in-
formation extraction, to date state-of-the-art re-
sources are designed for extracting temporally
scoped facts about public figures/organizations
from newswire or Wikipedia articles (Ji et al,
2011; McClosky and Manning, 2012; Garrido et
[11/15/2008] I have noticed some pulling recently and I 
won't start rads until March.
[11/20/2008] It is sloowwwly healing, so slowly, in fact, 
that she said she HOPES it will be healed by March, when 
I am supposed to start rads.
[1/13/2009] I still have one last chemo to go on the 19th 
and then start rads in 5 wks.
[1/31/2009] I go for my first meeting with the rad onc on 
 2/10 (my 50th birthday!).
[2/23/2009] I had my first rad today.
[3/31/2009] Tomorrow will be my last full rads
[4/2/2009] I started rads in Feb, just did #29 today.
[4/8/2009] The rad onc wants to see me again next week 
for a skin check as I have had cellulitis twice since August.
[6/21/2010] My friend Lisa had her port put in last week 
and will begin 2 weeks of radiation on Tuesday.
Figure 1: User posts containing keywords for the
start of Radiation. Event keywords are in bold and
temporal expressions are in italics.
al., 2012). When people are instead communi-
cating informally about their lives, they refer to
time more informally and frequently from their
personal frame of reference rather than from an
impersonal third person frame of reference. For
example, they may use their own birthday as a
time reference. The proportion of relative (e.g.,
?last week?, ?two days from now?), or personal
time references in our data is more than one and a
half times as high as in newswire and Wikipedia.
Therefore, it is not surprising that there would be
difficulty in applying a temporal tagger designed
for newswire to social media data (Strotgen and
Gertz, 2012; Kolomiyets et al, 2011). Recent be-
havioral studies (Choudhury et al, 2013; Park and
Choi, 2012; Wen et al, 2012) demonstrate that
user-focused event mentions extracted from social
media data can provide a useful timeline-like tool
for studying how behavior patterns change over
time in response to mentioned events. Our re-
search contributes towards automating this work.
2 Task
Our task is to extract personal illness events men-
tioned in the posting histories of online commu-
nity participants. The input to our system is
836
a candidate event and a posting history. The
output is the event date (month and year) for
the event if it occurred, or ?unknown? if it
did not occur. The process iterates through a
list of 10 cancer events (CEs). This list in-
cludes breast cancer Diagnosis, Metastasis, Re-
currence, Mastectomy, Lumpectomy, Reconstruc-
tion, Chemotherapy-Start, Chemotherapy-End,
Radiation-Start and Radiation-End. For each of
these target CEs, we manually designed an event
keyword set that includes the name of the event,
abbreviations, slang, aliases and related words.
For each of the 10 events, all sentences that
mention a related event keyword are extracted
from the user?s posting history. Figure 1 shows
sevaral sentences that were extracted for one user
for the start date of Radiation. The task is to de-
termine that the beginning of this user?s Radiation
therapy was 2/2009. Note that the user began to
post about Radiation before she started it. She first
reported planning to start Radiation in March, but
then rescheduled for February. Most of the TEs
are non-standard and need to be resolved to calen-
dar dates (year and month).
Once the full set of event mention sentences has
been extracted for a user, all the temporal expres-
sions (TEs) that appear in the same sentence with
an event mention are resolved to a set of candi-
date dates. Besides a standard event-time classi-
fier for within-sentence event-time anchoring, we
leverage a new source of temporal information to
train a constraint-based event-time classifier. Pre-
vious work only retrieves time-rich sentences that
include both the query and some TEs (Ji et al,
2011; McClosky and Manning, 2012; Garrido et
al., 2012). However, sentences that contain only
the event mention but no explicit TE can also be
informative. For example, the post time (usually
referred to as document creation time or DCT) of
the sentence ?metastasis was found in my bone?
might be labeled as being after the ?metastasis?
event date. These DCTs impose constraints on
the possible event dates, which can be integrated
with the event-time classifier, as a variant on re-
lated work(Chambers, 2012).
3 Related Work
Previous work on TE extraction has focused
mainly on newswire text (Strotgen and Gertz,
2010; Chang and Manning, 2012). This paper
presents a rule-based TE extractor that identifies
and resolves a higher percentage of nonstandard
TEs than earlier state-of-art temporal taggers.
Our task is closest to the temporal slot filling
track in the TAC-KBP 2011 shared task (Ji et al,
2011) and timelining task (McClosky and Man-
ning, 2012). Their goal was to extract the tempo-
ral bounds of event relations. Our task has two key
differences. First, they used newswire, Wikipedia
and blogs as data sources from which they extract
temporal bounds of facts found in Wikipedia in-
foboxes. Second, in the KBP task, the set of gold
event relations are provided as input, so that the
task is only to identify a date for an event that is
guaranteed to have been mentioned. In our task,
we provide a set of potential events. However,
most of the candidate events won?t have ever been
reported within a user?s posting history.
Temporal constraints have proven to be use-
ful for producing a globally consistent timeline.
In most temporal relation bound extraction sys-
tems, the constraints are included as input rather
than learned by the system (Talukdar et al, 2012;
Wang et al, 2011). A notable exception is Mc-
Closkyet al (2012) who developed an approach to
learning constraints such as that people cannot at-
tend school if they have not been born yet. A no-
table characteristic of our task is that constraints
are softer. Diseases may occur in very different
ways across patients. Recurring illnesses falsely
appear to have an unpredictable order. Thus, there
can be no universal logical constraints on the order
of cancer events.
Our approach to using temporal constraints is a
variant on previously published approaches. Gar-
rido et al (2012) made use of DCT (document cre-
ation time) as well, however, they have assumed
the DCT is within the time-range of the event
stated in the document, which is often not true
in our data. Chambers (2012) utilized the within-
sentence time-DCT relation to learn constrains for
predicting DCT. We learn the event-DCT relations
to produce constrains for the event date.
4 Corpus Annotation
We have scraped the posts, users, and profiles from
a large online cancer support community. From
this collection we extracted and then annotated
two separate corpora, one for evaluating our TE
retrieval and normalization, the other one for event
date extraction.
For creating the TE extraction corpus, we ran-
837
domly picked one post from each of 1,000 ran-
domly selected users. We used this sampling tech-
nique because each user tends to use a narrow
range of date expression forms. From these posts,
we manually extracted 601 TEs and resolved them
to a specific month and year or just year if the
month was not mentioned. Events not reported
to have occurred were annotated as ?unknown?.
Our corpus for event date extraction consists of
the complete posting history of 300 users that were
randomly drawn from our dataset. Three annota-
tors were provided with guidelines for how to in-
fer the date of the events (Wen et al, 2013). We
achieved .94 Kappa on identification of whether an
event has a reported event date in a user?s history
or not. In evaluation of agreement on extracted
dates, we achieved a .99 Cronbach?s alpha. From
this corpus, 509 events were annotated with occur-
rence dates (year and month). In our evaluation,
we use data from 250 users for training, and 50 for
testing.
5 Method
Now we explain on a more technical level how our
system works on our task. Given an event and a
user?s post history, the system searches for all of
the sentences that contain an event keyword (key-
word sentence) and all the sentences that contain
both a keyword and a TE (date sentence). The TEs
in the date sentences are resolved and then used as
candidate dates for the event. For selecting among
candidate dates, our model integrates two main
components. First, the Date Classifier is trained
from date sentences to predict how likely its can-
didate TE and the gold event date are to overlap.
Then, because constraints over event dates can be
informed by temporal relations between the event
date and the DCT, the Constraint-based Classifier
provides an indication of the plausibility of can-
didate dates. The integrated system combines the
predictions from both classifiers.
5.1 Temporal Tagger
We design a rule-based temporal tagger that is
built using regular expression patterns to recog-
nize informal TEs. Similar to SUTime (Chang and
Manning, 2012), we identify and resolve a wide
range of non-standard TE types such as ?Feb ?07
(2/2007)?. The additional types of TE we han-
dle include: 1)user-specific TEs: A user?s age,
cancer anniversary and survivorship can provide
temporal information about the user?s CEs. We
obtain the birth date of users from their personal
profile to resolve age date expressions such as ?at
the age of 57?. 2)non-whole numbers such as ?a
year and half? and ?1/2 weeks?. 3)abbreviations
of time units : e.g. ?wk? as the abbreviation of
?week?. 4)underspecified month mentions, we
resolve the year information according to the DCT
month, the mentioned month and the verb tense.
5.2 Date Classifier
We train a MaxEnt classifier to predict the tem-
poral relationship between the retrieved TE and
the event date as overlap or no-overlap, similar
to the within-sentence event-time anchoring task
in TempEval-2 (UzZaman and Allen, 2010). Fea-
tures for the classifier include many of those in
(McClosky and Manning, 2012; Yoshikawa et al,
2009): namely, event keyword and its dominant
verb, verb and preposition that dominate TE, de-
pendency path between TE and keyword and its
length, unigram and bigram word and POS fea-
tures. New features include the Event-Subject,
Negative and Modality features. In online sup-
port groups, users not only tell stories about them-
selves, they also share other patients? stories (as
shown in Figure 1). So we add subject fea-
tures to remove this kind of noise, which in-
cludes the governing subject of the event key-
word and its POS tag. Modality features include
the appearance of modals before the event key-
word (e.g., may, might). Negative features include
the presence/absence of negative words (e.g., no,
never). These two features indicate a hypothetical
or counter-factual expression of the event.
To calculate the likelihood of a candidate date
for an event, we need to aggregate the hard de-
cisions from the classifier. Let DSu be the set
of the user?s date sentences, let Du be the set of
dates resolved from each TE. We represent a Max-
Ent classifier by Prelation(R|t, ds) for a candidate
date t in date sentence ds and possible relation
R = {overlap, no-overlap}. We map the distri-
bution over relations to a distribution over dates
by defining PDateSentence(t|DSu):
PDateSentence(t|DSu) = (1)
1
Z(Du)
?
tj?Du
?tj (t)Prelation(overlap|tj , dsj)
?tj (t) =
{
1 if t = tj
0 otherwise
838
We refer to this model as the Date Classifier.
5.3 Constraint-based Classifier
Previous work only retrieves time-rich sentences
(i.e., date sentences) (Ling and Weld, 2010; Ji et
al., 2011; McClosky and Manning, 2012; Garrido
et al, 2012). However, keyword sentences can in-
form temporal constraints for events and therefore
should not be ignored. For example, ?Well, I?m
officially a Radiation grad!? indicates the user has
done radiation by the time of the post (DCT). ?Ra-
diation is not a choice for me.? indicates the user
probably never had radiation. The topic of the
sentence can also indicate the temporal relation.
For example, before chemotherapy, the users tend
to talk about choices of drug combinations. After
chemotherapy, they talk about side-effects.
This section departs from the above Date Clas-
sifier and instead predicts whether each keyword
sentence is posted before or overlap-or-after the
user?s event date. The goal is to automatically
learn time constraints for the event. This task is
similar to the sentence event-DCT ordering task
in TempEval-2 (UzZaman and Allen, 2010). We
create training examples by computing the tempo-
ral relation between the DCT and the user?s gold
event date. If the user has not reported an event
date, the label should be unknown.
We train a MaxEnt classifier on each event
mention paired with its corresponding DCT. All
the features used in the classifier component that
are not related to the TEs are included. Let
KSu be the set of the user?s keyword sentences,
let Du be the set of dates resolved from each
date sentence. We define a MaxEnt classifier by
Prelation(R|ks) for a keyword sentence ks and
possible relation R = {before, overlap-or-after,
unknown}. DCT is the post time of the keyword
sentence ks. The rel(DCT, t) function simply de-
termines if the DCT is before or overlap-or-after
the candidate date t. We map this distribution over
relations to a distribution over dates by defining
PKeywordSentence(t,KSu):
PKeywordSentence(t,KSu) = (2)
1
Z(Du)
?
ksj?KSu
Prelation(rel(dctj , t)|ksj)
rel(dct, t) =
{
before if dct < t
overlap-or-after if dct ? t
5.4 Integrated Model
Given the Date Classifier of Section 5.2 and the
Constraint-based Classifier of Section 5.3, we cre-
ate a Integrated Model combining the two with the
following linear interpolation as follows:
P (t|postsu) = ?PDateSentence(t|DSu)
+ (1? ?)PKeywordSentence(t|KSu)
where t is a candidate event date. The system will
output t that maximizes P (t|postsu) and unknown
if DSu is empty. ? was set to 0.7 by maximizing
accuracy using five-fold cross-validation over the
training set.
6 Evaluation Metric and Results
6.1 Temporal Expression Retrieval
We compare our temporal tagger?s performance
with SUTime (Chang and Manning, 2012) on the
601 manually extracted TEs. We exclude user-
specific TEs such as birthday references since SU-
Time cannot handle those. We first evaluate iden-
tification of the extent of a TE and then production
of the correctly resolved date for each recognized
expression. Table 1 shows that our tagger has sig-
nificantly higher precision and recall for both.
P R F1
Extents SUTime 97.5 75.4 85.0
Our tagger 97.9 91.8 94.8
Normalization SUTime 89.4 71.2 79.3
Our tagger 91.3 85.5 88.3
Table 1: Temporal expression retrieval results
6.2 Event-date Extraction
6.2.1 Evaluation metric
The extracted date is only considered correct if it
completely matches the gold date. For less than
4% of users, we have multiple dates for the same
event (e.g., a user had a mastectomy twice). Sim-
ilar to the evaluation metric in a previous study(Ji
et al, 2011), in these cases, we give the system the
benefit of the doubt and the extracted date is con-
sidered correct if it matches one of the gold dates.
In previous work (McClosky and Manning, 2012;
Ji et al, 2011), the evaluation metric score is de-
fined as 1/((1 + |d|)) where d is the difference
between the values in years. We choose a much
stricter evaluation metric because we need a pre-
cise event date to study user behavior changes.
6.2.2 Baselines and oracle
Based on our temporal tagger, we provide two
baselines to describe heuristic methods of ag-
gregating the hard decisions from the classifier
839
Baseline1 Baseline2 Date Integrated Oracle
CE count P R F1 P R F1 P R F1 P R F1 F1
Diagnosis 112 .64 .70 .67 .60 .66 .63 .68 .75 .71 .68 .75 .71 .80
Metastasis 7 .16 .58 .25 .12 .43 .19 .25 .86 .39 .25 .86 .39 .86
Recurrence 14 .14 .35 .20 .11 .29 .16 .13 .36 .19 .13 .36 .19 .47
Chemo-start 54 .49 .61 .54 .42 .52 .46 .52 .66 .58 .58 .74 .65 .76
Chemo-end 43 .44 .59 .50 .36 .49 .42 .47 .63 .54 .48 .66 .56 .84
Rad-start 38 .35 .47 .40 .30 .40 .34 .36 .47 .41 .40 .53 .46 .64
Rad-end 35 .48 .63 .54 .30 .39 .34 .50 .66 .57 .50 .66 .57 .84
Mastectomy 68 .58 .71 .64 .52 .62 .57 .62 .76 .68 .62 .76 .68 .77
Lumpectomy 33 .49 .71 .58 .43 .76 .46 .46 .79 .58 .46 .79 .62 .91
Reconstruction 43 .38 .57 .46 .29 .44 .35 .41 .63 .50 .43 .65 .52 .86
Table 2: Event-level five-fold cross-validation performance of models and baselines on training data.
learned in Section 5.3. The first baseline, Base-
line1, is to pick the date with the highest clas-
sifier?s prediction confidence. The second base-
line, Baseline2, is along the same lines as the
Combined Classifier used in (McClosky and Man-
ning, 2012). For example, if the candidate
date is ?6/2009? and we have retrieved two TEs
that are resolved to ?6/2009? and ?4/2008?, then
P (?6/2009?) = Prelation(overlap|?6/2009?) ?
Prelation(no-overlap|?4/2008?).
To set an upper bound on performance given our
TE retrieval system, we calculate the oracle score
by considering an extraction as correct if the gold
date is one of the retrieved candidate dates. The
oracle score can differ from a perfect score since
we can only use candidate temporal expressions
if (a)the relation is known and (b)mentions of the
event are retrievable, (c)the TE and event keyword
appear in the same sentence, and (d)our temporal
tagger is able to recognize and resolve it correctly.
6.2.3 Results
We present the performance of our models, base-
lines and the oracle in Table 2. Both the Date Clas-
sifier and Integrated model significantly outper-
form the baselines (p < 0.0001, McNemar?s test,
2-tailed). This shows the value of our approach to
leveraging redundancy of event date mentions. In-
corporating time constraints further improves the
F1 of the Date Classifier by 3%. The Integrated
model achieves 89.7% of the oracle result.
Model P R F1
Baseline1 46.1 63.7 53.5
Baseline2 39.3 54.4 45.6
Date Classifier 49.6 67.7 57.3
Integrated Model 51.0 69.3 58.8
Oracle 77.3 77.3 77.3
Table 3: Performance of systems on the test set.
Table 3 shows the performance of our systems
and baselines on individual event types. The Joint
Model derives most of its improvement from per-
formance related to the Chemotherapy/Radiation-
start date. This is mainly because Chemotherapy
and Radiation last for a period of time and there
are more event-related discussions containing the
event keyword. None of our systems improves on
cancer Metastasis and Recurrence. This is likely
due to the sparsity of these events.
7 Conclusion
We presented a novel event date extraction task
that requires extraction and resolution of non-
standard TEs, namely personal illness event dates,
from the posting histories of online community
participants. We constructed an evaluation corpus
and designed a temporal tagger for non-standard
TEs in social media. Using a much stricter stan-
dard correctness measure than in previous work,
our method achieves promising results that are sig-
nificantly better than two types of baseline. By
creating an analogous keyword set, our event date
extraction method could be easily adapted to other
datasets.
8 Acknowledgments
We want to thank Dong Nguyen and Yi-chia
Wang, who helped provide the data for this
project. The research reported here was sup-
ported by National Science Foundation grant IIS-
0968485.
840
References
Javier Artiles, Qi Li, Taylor Cassidy, Suzanne
Tamang, and Heng Ji. 2011. CUNY BLENDER
TACKBP2011 Temporal Slot Filling System De-
scription. In Proceedings of Text Analysis Confer-
ence (TAC).
Nathanael Chambers, Shan Wang, and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics.
Nathanael Chambers. 2012. Labeling documents with
timestamps: Learning from their time expressions.
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics.
Angel X. Chang and Christopher D. Manning. 2012.
SUTIME: A library for recognizing and normaliz-
ing time expressions. In 8th International Confer-
ence on Language Resources and Evaluation(LREC
2012).
De Choudhury, M., Counts, S., and Horvitz, E. 2013.
Major Life Changes and Behavioral Markers in So-
cial Media: Case of Childbirth. In Proc. CSCW
2013.
Guillermo Garrido, Anselmo Penas, Bernardo Ca-
baleiro, and Alvaro Rodrigo. 2012. Temporally An-
chored Relation Extraction. In Proceedings of the
50th annual meeting of the as-sociation for compu-
tational linguistics.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the TAC 2011 Knowledge Base Popu-
lation track. In Proceedings of Text Analysis Con-
ference (TAC).
Hyuckchul Jung, James Allen, Nate Blaylock, Will de
Beaumont, Lucian Galescu, and Mary Swift. 2011.
Building timelines from narrative clinical records:
initial results based-on deep natural language under-
standing. In Proceedings of BioNLP 2011.
Oleksandr Kolomiyets, Steven Bethard and Marie-
Francine Moens. 2011. Model-Portability Experi-
ments for Textual Temporal Analysis. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics.
Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2012. Extracting narrative time-
lines as temporal dependency structures. In Pro-
ceedings of the 50th annual meeting of the Associ-
ation for Computational Linguistics.
Xiao Ling and Daniel S Weld. 2010 Temporal infor-
mation extraction. Proceedings of the Twenty Fifth
National Conference on Artificial Intelligence.
David McClosky and Christopher D. Manning. 2012.
Learning Constraints for Consistent Timeline Ex-
traction. Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP2012).
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
47th annual meeting of the Association for Compu-
tational Linguistics.
Heekyong Park and Jinwook Choi 2012. V-model: a
new innovative model to chronologically visualize
narrative clinical texts. In Proceedings of the 2012
ACM annual conference on Human Factors in Com-
puting Systems. ACM.
Catherine Plaisant, Brett Milash, Anne Rose, Seth Wid-
off, and Ben Shneiderman. 1996. LifeLines: vi-
sualizing personal histories. In Proceedings of the
SIGCHI conference on Human factors in computing
systems.
James Pustejovsky, Jos M. Castao, Robert Ingria, Roser
Sauri, Robert J. Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir R. Radev. 2003. TimeML: Ro-
bust specification of event and temporal expressions
in text. TimeML: Robust specification of event and
temporal expressions in text. In New Directions in
Question Answering?03.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev. 2003. The Timebank corpus. In
Corpus Linguistics.
Preethi Raghavan, Eric Fosler-Lussier, and Albert M.
Lai. 2012. Learning to Temporally Order Medical
Events in Clinical Text. In Proceedings of the 50th
annual meeting of the Association for computational
Linguistics.
Jannik Strotgen and Michael Gertz. 2010. Heidel-
Time:High Quality Rule-Based Extraction and Nor-
malizationof Temporal Expressions. In SemEval
?10.
Jannik Strotgen and Michael Gertz. 2012. Temporal
Tagging on Different Domains: Challenges, Strate-
gies, and Gold Standards. In LREC2012.
Partha Pratim Talukdar, Derry Wijaya, and Tom
Mitchell. 2012. Coupled temporal scoping of re-
lational facts. In Proceedings of the fifth ACM inter-
national conference on Web search and data mining.
ACM.
Naushad UzZaman and James F. Allen. 2010. TRIPS
and TRIOS system for TempEval-2: Extracting tem-
poral information from text. In Proceedings of the
5th International Workshop on Semantic Evaluation.
Yafang Wang, Bing Yang, Lizhen Qu, Marc Spaniol,
and GerhardWeikum. 2011. Harvesting facts from
textual web sources by constrained label propaga-
tion. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Man-
agement.
841
Miaomiao Wen, Hyeju Jang, and Carolyn Rose?. 2013.
Coding Manual for Illness Event Date Extraction.
Carnegie Mellon University, School of Computer
Science, Language Technology Institute.
K.-Y. Wen, F. McTavish, G. Kreps, M. Wise, and D.
Gustafson. 2012. From diagnosis to death: A
case study of coping with breast cancer as seen
through online discussion group messages. Jour-
nal of Computer-Mediated Communication, 16:331-
361.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly identi-
fying temporal relations with markov logic. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP.
Li Zhou and George Hripcsak. 2007. Temporal rea-
soning with medical data?a review with emphasis
on medical natural language processing. Journal of
biomedical informatics 40.2 (2007): 183.
842
