Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 967?976,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Knowledge-Based Question Answering as Machine Translation
Junwei Bao
? ?
, Nan Duan
?
, Ming Zhou
?
, Tiejun Zhao
?
?
Harbin Institute of Technology
?
Microsoft Research
baojunwei001@gmail.com
{nanduan, mingzhou}@microsoft.com
tjzhao@hit.edu.cn
Abstract
A typical knowledge-based question an-
swering (KB-QA) system faces two chal-
lenges: one is to transform natural lan-
guage questions into their meaning repre-
sentations (MRs); the other is to retrieve
answers from knowledge bases (KBs) us-
ing generated MRs. Unlike previous meth-
ods which treat them in a cascaded man-
ner, we present a translation-based ap-
proach to solve these two tasks in one u-
nified framework. We translate questions
to answers based on CYK parsing. An-
swers as translations of the span covered
by each CYK cell are obtained by a ques-
tion translation method, which first gener-
ates formal triple queries as MRs for the
span based on question patterns and re-
lation expressions, and then retrieves an-
swers from a given KB based on triple
queries generated. A linear model is de-
fined over derivations, and minimum er-
ror rate training is used to tune feature
weights based on a set of question-answer
pairs. Compared to a KB-QA system us-
ing a state-of-the-art semantic parser, our
method achieves better results.
1 Introduction
Knowledge-based question answering (KB-QA)
computes answers to natural language (NL) ques-
tions based on existing knowledge bases (KBs).
Most previous systems tackle this task in a cas-
caded manner: First, the input question is trans-
formed into its meaning representation (MR) by
an independent semantic parser (Zettlemoyer and
Collins, 2005; Mooney, 2007; Artzi and Zettle-
moyer, 2011; Liang et al, 2011; Cai and Yates,
?
This work was finished while the author was visiting Mi-
crosoft Research Asia.
2013; Poon, 2013; Artzi et al, 2013; Kwiatkowski
et al, 2013; Berant et al, 2013); Then, the answer-
s are retrieved from existing KBs using generated
MRs as queries.
Unlike existing KB-QA systems which treat se-
mantic parsing and answer retrieval as two cas-
caded tasks, this paper presents a unified frame-
work that can integrate semantic parsing into the
question answering procedure directly. Borrow-
ing ideas from machine translation (MT), we treat
the QA task as a translation procedure. Like MT,
CYK parsing is used to parse each input question,
and answers of the span covered by each CYK cel-
l are considered the translations of that cell; un-
like MT, which uses offline-generated translation
tables to translate source phrases into target trans-
lations, a semantic parsing-based question trans-
lation method is used to translate each span into
its answers on-the-fly, based on question patterns
and relation expressions. The final answers can be
obtained from the root cell. Derivations generated
during such a translation procedure are modeled
by a linear model, and minimum error rate train-
ing (MERT) (Och, 2003) is used to tune feature
weights based on a set of question-answer pairs.
Figure 1 shows an example: the question direc-
tor of movie starred by Tom Hanks is translated to
one of its answers Robert Zemeckis by three main
steps: (i) translate director of to director of ; (ii)
translate movie starred by Tom Hanks to one of it-
s answers Forrest Gump; (iii) translate director of
Forrest Gump to a final answer Robert Zemeckis.
Note that the updated question covered by Cell[0,
6] is obtained by combining the answers to ques-
tion spans covered by Cell[0, 1] and Cell[2, 6].
The contributions of this work are two-fold: (1)
We propose a translation-based KB-QA method
that integrates semantic parsing and QA in one
unified framework. The benefit of our method
is that we don?t need to explicitly generate com-
plete semantic structures for input questions. Be-
967
Cell[0, 6] 
Cell[2, 6] 
Cell[0, 1] 
director of movie starred by Tom Hanks 
(ii) movie starred by Tom Hanks ? Forrest Gump 
(iii) director of Forrest Gump ? Robert Zemeckis 
(i) director of ? director of 
Figure 1: Translation-based KB-QA example
sides which, answers generated during the transla-
tion procedure help significantly with search space
pruning. (2) We propose a robust method to trans-
form single-relation questions into formal triple
queries as their MRs, which trades off between
transformation accuracy and recall using question
patterns and relation expressions respectively.
2 Translation-Based KB-QA
2.1 Overview
Formally, given a knowledge base KB and an N-
L question Q, our KB-QA method generates a set
of formal triples-answer pairs {?D,A?} as deriva-
tions, which are scored and ranked by the distribu-
tion P (?D,A?|KB,Q) defined as follows:
exp{
?
M
i=1
?
i
? h
i
(?D,A?,KB,Q)}
?
?D
?
,A
?
??H(Q)
exp{
?
M
i=1
?
i
? h
i
(?D
?
,A
?
?,KB,Q)}
? KB denotes a knowledge base
1
that stores a
set of assertions. Each assertion t ? KB is in
the form of {e
ID
sbj
, p, e
ID
obj
}, where p denotes
a predicate, e
ID
sbj
and e
ID
obj
denote the subject
and object entities of t, with unique IDs
2
.
? H(Q) denotes the search space {?D,A?}. D
is composed of a set of ordered formal triples
{t
1
, ..., t
n
}. Each triple t = {e
sbj
, p, e
obj
}
j
i
?
D denotes an assertion in KB, where i and
j denotes the beginning and end indexes of
the question span from which t is trans-
formed. The order of triples in D denotes
the order of translation steps from Q to A.
E.g., ?director of, Null, director of ?
1
0
, ?Tom
1
We use a large scale knowledge base in this paper, which
contains 2.3B entities, 5.5K predicates, and 18B assertions. A
16-machine cluster is used to host and serve the whole data.
2
Each KB entity has a unique ID. For the sake of conve-
nience, we omit the ID information in the rest of the paper.
Hanks, Film.Actor.Film, Forrest Gump?
6
2
and
?Forrest Gump, Film.Film.Director, Robert
Zemeckis?
6
0
are three ordered formal triples
corresponding to the three translation steps in
Figure 1. We define the task of transforming
question spans into formal triples as question
translation. A denotes one final answer ofQ.
? h
i
(?) denotes the i
th
feature function.
? ?
i
denotes the feature weight of h
i
(?).
According to the above description, our KB-
QA method can be decomposed into four tasks as:
(1) search space generation for H(Q); (2) ques-
tion translation for transforming question spans in-
to their corresponding formal triples; (3) feature
design for h
i
(?); and (4) feature weight tuning for
{?
i
}. We present details of these four tasks in the
following subsections one-by-one.
2.2 Search Space Generation
We first present our translation-based KB-QA
method in Algorithm 1, which is used to generate
H(Q) for each input NL question Q.
Algorithm 1: Translation-based KB-QA
1 for l = 1 to |Q| do
2 for all i, j s.t. j ? i = l do
3 H(Q
j
i
) = ?;
4 T = QTrans(Q
j
i
,KB);
5 foreach formal triple t ? T do
6 create a new derivation d;
7 d.A = t.e
obj
;
8 d.D = {t};
9 update the model score of d;
10 insert d toH(Q
j
i
);
11 end
12 end
13 end
14 for l = 1 to |Q| do
15 for all i, j s.t. j ? i = l do
16 for all m s.t. i ? m < j do
17 for d
l
? H(Q
m
i
) and d
r
? H(Q
j
m+1
) do
18 Q
update
= d
l
.A+ d
r
.A;
19 T = QTrans(Q
update
,KB);
20 foreach formal triple t ? T do
21 create a new derivation d;
22 d.A = t.e
obj
;
23 d.D = d
l
.D
?
d
r
.D
?
{t};
24 update the model score of d;
25 insert d toH(Q
j
i
);
26 end
27 end
28 end
29 end
30 end
31 returnH(Q).
968
The first half (from Line 1 to Line 13) gen-
erates a formal triple set T for each unary span
Q
j
i
? Q, using the question translation method
QTrans(Q
j
i
,KB) (Line 4), which takesQ
j
i
as the
input. Each triple t ? T returned is in the form of
{e
sbj
, p, e
obj
}, where e
sbj
?s mention occurs inQ
j
i
,
p is a predicate that denotes the meaning expressed
by the context of e
sbj
in Q
j
i
, e
obj
is an answer of
Q
j
i
based on e
sbj
, p and KB. We describe the im-
plementation detail of QTrans(?) in Section 2.3.
The second half (from Line 14 to Line 31) first
updates the content of each bigger spanQ
j
i
by con-
catenating the answers to its any two consecutive
smaller spans covered by Q
j
i
(Line 18). Then,
QTrans(Q
j
i
,KB) is called to generate triples for
the updated span (Line 19). The above operations
are equivalent to answering a simplified question,
which is obtained by replacing the answerable
spans in the original question with their corre-
sponding answers. The search spaceH(Q) for the
entire question Q is returned at last (Line 31).
2.3 Question Translation
The purpose of question translation is to translate
a span Q to a set of formal triples T . Each triple
t ? T is in the form of {e
sbj
, p, e
obj
}, where e
sbj
?s
mention
3
occurs inQ, p is a predicate that denotes
the meaning expressed by the context of e
sbj
in
Q, e
obj
is an answer to Q retrieved from KB us-
ing a triple query q = {e
sbj
, p, ?}. Note that if
no predicate p or answer e
obj
can be generated,
{Q, Null,Q} will be returned as a special triple,
which sets e
obj
to be Q itself, and p to be Null.
This makes sure the un-answerable spans can be
passed on to the higher-level operations.
Question translation assumes each span Q is a
single-relation question (Fader et al, 2013). Such
assumption simplifies the efforts of semantic pars-
ing to the minimum question units, while leaving
the capability of handling multiple-relation ques-
tions (Figure 1 gives one such example) to the out-
er CYK-parsing based translation procedure. Two
question translation methods are presented in the
rest of this subsection, which are based on ques-
tion patterns and relation expressions respectively.
2.3.1 Question Pattern-based Translation
A question pattern QP includes a pattern string
QP
pattern
, which is composed of words and a slot
3
For simplicity, a cleaned entity dictionary dumped from
the entire KB is used to detect entity mentions inQ.
Algorithm 2:QP-based Question Translation
1 T = ?;
2 foreach entity mention e
Q
? Q do
3 Q
pattern
= replace e
Q
inQ with [Slot];
4 foreach question patternQP do
5 ifQ
pattern
==QP
pattern
then
6 E = Disambiguate(e
Q
,QP
predicate
);
7 foreach e ? E do
8 create a new triple query q;
9 q = {e,QP
predicate
, ?};
10 {A
i
} = AnswerRetrieve(q,KB);
11 foreach A ? {A
i
} do
12 create a new formal triple t;
13 t = {q.e
sbj
, q.p,A};
14 t.score = 1.0;
15 insert t to T ;
16 end
17 end
18 end
19 end
20 end
21 return T .
symbol [Slot], and a KB predicate QP
predicate
,
which denotes the meaning expressed by the con-
text words in QP
pattern
.
Algorithm 2 shows how to generate formal
triples for a span Q based on question pattern-
s (QP-based question translation). For each en-
tity mention e
Q
? Q, we replace it with [Slot]
and obtain a pattern string Q
pattern
(Line 3). If
Q
pattern
can match one QP
pattern
, then we con-
struct a triple query q (Line 9) using QP
predicate
as its predicate and one of the KB entities re-
turned by Disambiguate(e
Q
,QP
predicate
) as it-
s subject entity (Line 6). Here, the objective of
Disambiguate(e
Q
,QP
predicate
) is to output a set
of disambiguated KB entities E in KB. The name
of each entity returned equals the input entity
mention e
Q
and occurs in some assertions where
QP
predicate
are the predicates. The underlying
idea is to use the context (predicate) information to
help entity disambiguation. The answers of q are
returned by AnswerRetrieve(q,KB) based on q
and KB (Line 10), each of which is used to con-
struct a formal triple and added to T for Q (from
Line 11 to Line 16). Figure 2 gives an example.
Question patterns are collected as follows: First,
5W queries, which begin with What, Where, Who,
When, or Which, are selected from a large scale
query log of a commercial search engine; Then, a
cleaned entity dictionary is used to annotate each
query by replacing all entity mentions it contains
with the symbol [Slot]. Only high-frequent query
patterns which contain one [Slot] are maintained;
969
?                    : who is the director of Forrest Gump 
?????????    : who is the director of [Slot] 
???????????: Film.Film.Director 
?                    : <Forrest Gump, Film.Film.Director, ?> 
?                     : <Forrest Gump, Film.Film.Director, Robert Zemeckis> 
KB 
Figure 2: QP-based question translation example
Lastly, annotators try to manually label the most-
frequent 50,000 query patterns with their corre-
sponding predicates, and 4,764 question patterns
with single labeled predicates are obtained.
From experiments (Table 3 in Section 4.3) we
can see that, question pattern based question trans-
lation can achieve high end-to-end accuracy. But
as human efforts are needed in the mining proce-
dure, this method cannot be extended to large scale
very easily. Besides, different users often type the
questions with the same meaning in different NL
expressions. For example, although the question
Forrest Gump was directed by which moviemaker
means the same as the question Q in Figure 2, no
question pattern can cover it. We need to find an
alternative way to alleviate such coverage issue.
2.3.2 Relation Expression-based Translation
Aiming to alleviate the coverage issue occurring in
QP-based method, an alternative relation expres-
sion (RE) -based method is proposed, and will be
used when the QP-based method fails.
We define RE
p
as a relation expression set for
a given KB predicate p ? KB. Each relation ex-
pressionRE ? RE
p
includes an expression string
RE
expression
, which must contain at least one con-
tent word, and a weight RE
weight
, which denotes
the confidence thatRE
expression
can represent p?s
meaning in NL. For example, is the director of
is one relation expression string for the predicate
Film.Film.Director, which means it is usually used
to express this relation (predicate) in NL.
Algorithm 3 shows how to generate triples for
a question Q based on relation expressions. For
each possible entity mention e
Q
? Q and a K-
B predicate p ? KB that is related to a KB enti-
ty e whose name equals e
Q
, Sim(e
Q
,Q,RE
p
) is
computed (Line 5) based on the similarity between
question context and RE
p
, which measures how
likely Q can be transformed into a triple query
Algorithm 3:RE-based Question Translation
1 T = ?;
2 foreach entity mention e
Q
? Q do
3 foreach e ? KB s.t. e.name==e
Q
do
4 foreach predicate p ? KB related to e do
5 score = Sim(e
Q
,Q,RE
p
);
6 if score > 0 then
7 create a new triple query q;
8 q = {e, p, ?};
9 {A
i
} = AnswerRetrieve(q,KB);
10 foreach A ? {A
i
} do
11 create a new formal triple t;
12 t = {q.e
sbj
, q.p,A};
13 t.score = score;
14 insert t to T ;
15 end
16 end
17 end
18 end
19 end
20 sort T based on the score of each t ? T ;
21 return T .
q = {e, p, ?}. If this score is larger than 0, which
means there are overlaps betweenQ?s context and
RE
p
, then q will be used as the triple query of Q,
and a set of formal triples will be generated based
on q andKB (from Line 7 to Line 15). The compu-
tation of Sim(e
Q
,Q,RE
p
) is defined as follows:
?
n
1
|Q| ? n+ 1
? {
?
?
n
?Q,?
n
?
e
Q
=?
P (?
n
|RE
p
)}
where n is the n-gram order which ranges from 1
to 5, ?
n
is an n-gram occurring inQ without over-
lapping with e
Q
and containing at least one con-
tent word, P (?
n
|RE
p
) is the posterior probability
which is computed by:
P (?
n
|RE
p
) =
Count(?
n
,RE
p
)
?
?
?
n
?RE
p
Count(?
?
n
,RE
p
)
Count(?,RE
p
) denotes the weighted sum of
times that ? occurs inRE
p
:
Count(?,RE
p
) =
?
RE?RE
p
{#
?
(RE) ? RE
weight
}
where #
?
(RE) denotes the number of times that
? occurs inRE
expression
, andRE
weight
is decided
by the relation expression extraction component.
Figure 3 gives an example, where n-grams with
rectangles are the ones that occur in bothQ?s con-
text and the relation expression set of a given pred-
icate p = Film.F ilm.Director. Unlike the QP-
based method which needs a perfect match, the
970
?                                 : Forrest Gump was directed by which moviemaker 
????????????????????: is directed by 
was directed and written by 
is the moviemaker of 
was famous as the director of 
? 
?                                  : <Forrest Gump, Film.Film.Director, ?> 
?                                   : <Forrest Gump, Film.Film.Director, Robert Zemeckis> 
KB 
Figure 3: RE-based question translation example
RE-based method allows fuzzy matching between
Q andRE
p
, and records this (Line 13) in generat-
ed triples, which is used as features later.
Relation expressions are mined as follows: Giv-
en a set of KB assertions with an identical predi-
cate p, we first extract all sentences from English
Wiki pages
4
, each of which contains at least one
pair of entities occurring in one assertion. Then,
we extract the shortest path between paired entities
in the dependency tree of each sentence as an RE
candidate for the given predicate. The intuition is
that any sentence containing such entity pairs oc-
cur in an assertion is likely to express the predi-
cate of that assertion in some way. Last, all rela-
tion expressions extracted are filtered by heuristic
rules, i.e., the frequency must be larger than 4, the
length must be shorter than 10, and then weighted
by the pattern scoring methods proposed in (Ger-
ber and Ngomo, 2011; Gerber and Ngomo, 2012).
For each predicate, we only keep the relation ex-
pressions whose pattern scores are larger than a
pre-defined threshold. Figure 4 gives one relation
expression extraction example. The statistics and
overall quality of the relation expressions are list-
ed in Section 4.1.
{ Forrest Gump , Robert Zemeckis }  
{ Titanic, James Cameron }  
{ The Dark Knight Rises , C hristopher  Nolan }  
Paired entity of a 
KB predicate  
??Film.Film.Director 
Passage retrieval  
from Wiki pages  
Relation expression 
weighting  
Robert Zemeckis  is the director of Forrest Gump  
James Cameron  is the moviemaker of Titanic 
The Dark Knight Rises is directed by C hristopher  Nolan  
is the director of           ||| 0.25  
is the moviemaker of   ||| 0.23  
is directed by                 ||| 0.20  
Figure 4: RE extraction example
4
http://en.wikipedia.org/wiki/Wikipedia:Database download
2.3.3 Question Decomposition
Sometimes, a question may provide multiple con-
straints to its answers. movie starred by Tom Han-
ks in 1994 is one such question. All the films as
the answers of this question should satisfy the fol-
lowing two constraints: (1) starred by Tom Hanks;
and (2) released in 1994. It is easy to see that such
questions cannot be translated to single triples.
We propose a dependency tree-based method to
handle such multiple-constraint questions by (i)
decomposing the original question into a set of
sub-questions using syntax-based patterns; and (ii)
intersecting the answers of all sub-questions as the
final answers of the original question. Note, ques-
tion decomposition only operates on the original
question and question spans covered by complete
dependency subtrees. Four syntax-based patterns
(Figure 5) are used for question decomposition. If
a question matches any one of these patterns, then
sub-questions are generated by collecting the path-
s between n
0
and each n
i
(i > 0) in the pattern,
where each n denotes a complete subtree with a
noun, number, or question word as its root node,
the symbol ? above prep
?
denotes this preposition
can be skipped in matching. For the question men-
tioned at the beginning, its two sub-questions gen-
erated are movie starred by Tom Hanks and movie
starred in 1994, as its dependency form matches
pattern (a). Similar ideas are used in IBM Wat-
son (Kalyanpur et al, 2012) as well.
???? 
?? 
????? 
?? 
????? 
?? 
? 
? 
???? 
?? ?? ???? 
(a) 
?? 
???? 
?? 
?? 
(c) 
and  ?? 
????? 
???? 
?? 
?? 
(d) 
????? and  ???? 
?? 
????? 
(b) 
Figure 5: Four syntax-based patterns for question
decomposition
As dependency parsing is not perfect, we gen-
erate single triples for such questions without con-
sidering constraints as well, and add them to the
search space for competition. h
syntax constraint
(?)
971
is used to boost triples that are converted from sub-
questions generated by question decomposition.
The more constraints an answer satisfies, the bet-
ter. Obviously, current patterns used can?t cover
all cases but most-common ones. We leave a more
general pattern mining method for future work.
2.4 Feature Design
The objective of our KB-QA system is to seek the
derivation ?
?
D,
?
A? that maximizes the probability
P (?D,A?|KB,Q) described in Section 2.1 as:
?
?
D,
?
A? = argmax
?D,A??H(Q)
P (?D,A?|KB,Q)
= argmax
?D,A??H(Q)
M
?
i=1
?
i
? h
i
(?D,A?,KB,Q)
We now introduce the feature sets {h
i
(?)} that are
used in the above linear model:
? h
question word
(?), which counts the number of
original question words occurring inA. It pe-
nalizes those partially answered questions.
? h
span
(?), which counts the number of spans
in Q that are converted to formal triples. It
controls the granularity of the spans used in
question translation.
? h
syntax subtree
(?), which counts the number
of spans inQ that are (1) converted to formal
triples, whose predicates are not Null, and
(2) covered by complete dependency subtrees
at the same time. The underlying intuition
is that, dependency subtrees of Q should be
treated as units for question translation.
? h
syntax constraint
(?), which counts the num-
ber of triples in D that are converted from
sub-questions generated by the question de-
composition component.
? h
triple
(?), which counts the number of triples
in D, whose predicates are not Null.
? h
triple
weight
(?), which sums the scores of all
triples {t
i
} in D as
?
t
i
?D
t
i
.score.
? h
QP
count
(?), which counts the number of
triples in D that are generated by QP-based
question translation method.
? h
RE
count
(?), which counts the number of
triples in D that are generated by RE-based
question translation method.
? h
staticrank
sbj
(?), which sums the static rank
scores of all subject entities in D?s triple set
as
?
t
i
?D
t
i
.e
sbj
.static rank.
? h
staticrank
obj
(?), which sums the static rank
scores of all object entities inD?s triple set as
?
t
i
?D
t
i
.e
obj
.static rank.
? h
confidence
obj
(?), which sums the confidence
scores of all object entities inD?s triple set as
?
t?D
t.e
obj
.confidence.
For each assertion {e
sbj
, p, e
obj
} stored in KB,
e
sbj
.static rank and e
obj
.static rank denote the
static rank scores
5
for e
sbj
and e
obj
respectively;
e
obj
.confidence rank represents the probability
p(e
obj
|e
sbj
, p). These three scores are used as fea-
tures to rank answers generated in QA procedure.
2.5 Feature Weight Tuning
Given a set of question-answer pairs {Q
i
,A
ref
i
}
as the development (dev) set, we use the minimum
error rate training (MERT) (Och, 2003) algorithm
to tune the feature weights ?
M
i
in our proposed
model. The training criterion is to seek the feature
weights that can minimize the accumulated errors
of the top-1 answer of questions in the dev set:
?
?
M
1
= argmin
?
M
1
N
?
i=1
Err(A
ref
i
,
?
A
i
;?
M
1
)
N is the number of questions in the dev set, A
ref
i
is the correct answers as references of the i
th
ques-
tion in the dev set,
?
A
i
is the top-1 answer candi-
date of the i
th
question in the dev set based on
feature weights ?
M
1
, Err(?) is the error function
which is defined as:
Err(A
ref
i
,
?
A
i
;?
M
1
) = 1? ?(A
ref
i
,
?
A
i
)
where ?(A
ref
i
,
?
A
i
) is an indicator function which
equals 1 when
?
A
i
is included in the reference set
A
ref
i
, and 0 otherwise.
3 Comparison with Previous Work
Our work intersects with two research directions:
semantic parsing and question answering.
Some previous works on semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Wong and Mooney, 2006; Zettle-
moyer and Collins, 2007; Wong and Mooney,
5
The static rank score of an entity represents a general
indicator of the overall quality of that entity.
972
2007; Kwiatkowski et al, 2010; Kwiatkowski
et al, 2011) require manually annotated logical
forms as supervision, and are hard to extend result-
ing parsers from limited domains, such as GEO,
JOBS and ATIS, to open domains. Recent work-
s (Clarke and Lapata, 2010; Liang et al, 2013)
have alleviated such issues using question-answer
pairs as weak supervision, but still with the short-
coming of using limited lexical triggers to link NL
phrases to predicates. Poon (2013) has proposed
an unsupervised method by adopting grounded-
learning to leverage the database for indirect su-
pervision. But transformation from NL questions
to MRs heavily depends on dependency parsing
results. Besides, the KB used (ATIS) is limited as
well. Kwiatkowski et al (2013) use Wiktionary
and a limited manual lexicon to map POS tags to
a set of predefined CCG lexical categories, which
aims to reduce the need for learning lexicon from
training data. But it still needs human efforts to de-
fine lexical categories, which usually can not cover
all the semantic phenomena.
Berant et al (2013) have not only enlarged the
KB used for Freebase (Google, 2013), but also
used a bigger lexicon trigger set extracted by the
open IE method (Lin et al, 2012) for NL phrases
to predicates linking. In comparison, our method
has further advantages: (1) Question answering
and semantic parsing are performed in an join-
t way under a unified framework; (2) A robust
method is proposed to map NL questions to their
formal triple queries, which trades off the mapping
quality by using question patterns and relation ex-
pressions in a cascaded way; and (3) We use do-
main independent feature set which allowing us to
use a relatively small number of question-answer
pairs to tune model parameters.
Fader et al (2013) map questions to formal
(triple) queries over a large scale, open-domain
database of facts extracted from a raw corpus by
ReVerb (Fader et al, 2011). Compared to their
work, our method gains an improvement in two
aspects: (1) Instead of using facts extracted us-
ing the open IE method, we leverage a large scale,
high-quality knowledge base; (2) We can han-
dle multiple-relation questions, instead of single-
relation queries only, based on our translation
based KB-QA framework.
Espana-Bonet and Comas (2012) have proposed
an MT-based method for factoid QA. But MT in
there work means to translate questions into n-
best translations, which are used for finding simi-
lar sentences in the document collection that prob-
ably contain answers. Echihabi and Marcu (2003)
have developed a noisy-channel model for QA,
which explains how a sentence containing an an-
swer to a given question can be rewritten into that
question through a sequence of stochastic opera-
tions. Compared to the above two MT-motivated
QA work, our method uses MT methodology to
translate questions to answers directly.
4 Experiment
4.1 Data Sets
Following Berant et al (2013), we use the same
subset of WEBQUESTIONS (3,778 questions) as
the development set (Dev) for weight tuning in
MERT, and use the other part of WEBQUES-
TIONS (2,032 questions) as the test set (Test). Ta-
ble 1 shows the statistics of this data set.
Data Set # Questions # Words
WEBQUESTIONS 5,810 6.7
Table 1: Statistics of evaluation set. # Questions is
the number of questions in a data set, # Words is
the averaged word count of a question.
Table 2 shows the statistics of question patterns
and relation expressions used in our KB-QA sys-
tem. As all question patterns are collected with hu-
man involvement as we discussed in Section 2.3.1,
the quality is very high (98%). We also sample
1,000 instances from the whole relation expression
set and manually label their quality. The accuracy
is around 89%. These two resources can cover 566
head predicates in our KB.
# Entries Accuracy
Question Patterns 4,764 98%
Relation Expressions 133,445 89%
Table 2: Statistics of question patterns and relation
expressions.
4.2 KB-QA Systems
Since Berant et al (2013) is one of the latest
work which has reported QA results based on a
large scale, general domain knowledge base (Free-
base), we consider their evaluation result on WE-
BQUESTIONS as our baseline.
Our KB-QA system generates the k-best deriva-
tions for each question span, where k is set to 20.
973
The answers with the highest model scores are
considered the best answers for evaluation. For
evaluation, we follow Berant et al (2013) to al-
low partial credit and score an answer using the F1
measure, comparing the predicted set of entities to
the annotated set of entities.
One difference between these two systems is the
KB used. Since Freebase is completely contained
by our KB, we disallow all entities which are not
included by Freebase. By doing so, our KB pro-
vides the same knowledge as Freebase does, which
means we do not gain any extra advantage by us-
ing a larger KB. But we still allow ourselves to
use the static rank scores and confidence scores of
entities as features, as we described in Section 2.4.
4.3 Evaluation Results
We first show the overall evaluation results of our
KB-QA system and compare them with baseline?s
results on Dev and Test. Note that we do not re-
implement the baseline system, but just list their
evaluation numbers reported in the paper. Com-
parison results are listed in Table 3.
Dev (Accuracy) Test (Accuracy)
Baseline 32.9% 31.4%
Our Method 42.5% (+9.6%) 37.5% (+6.1%)
Table 3: Accuracy on evaluation sets. Accuracy is
defined as the number of correctly answered ques-
tions divided by the total number of questions.
Table 3 shows our KB-QA method outperforms
baseline on both Dev and Test. We think the po-
tential reasons of this improvement include:
? Different methods are used to map NL phras-
es to KB predicates. Berant et al (2013)
have used a lexicon extracted from a subset
of ReVerb triples (Lin et al, 2012), which
is similar to the relation expression set used
in question translation. But as our relation
expressions are extracted by an in-house ex-
tractor, we can record their extraction-related
statistics as extra information, and use them
as features to measure the mapping quality.
Besides, as a portion of entities in our KB
are extracted from Wiki, we know the one-
to-one correspondence between such entities
and Wiki pages, and use this information in
relation expression extraction for entity dis-
ambiguation. A lower disambiguation error
rate results in better relation expressions.
? Question patterns are used to map NL context
to KB predicates. Context can be either con-
tinuous or discontinues phrases. Although
the size of this set is limited, they can actually
cover head questions/queries
6
very well. The
underlying intuition of using patterns is that
those high-frequent questions/queries should
and can be treated and solved in the QA task,
by involving human effort at a relative small
price but with very impressive accuracy.
In order to figure out the impacts of question
patterns and relation expressions, another exper-
iment (Table 4) is designed to evaluate their in-
dependent influences, where QP
only
and RE
only
denote the results of KB-QA systems which only
allow question patterns and relation expressions in
question translation respectively.
Settings Test (Accuracy) Test (Precision)
QP
only
11.8% 97.5%
RE
only
32.5% 73.2%
Table 4: Impacts of question patterns and relation
expressions. Precision is defined as the num-
ber of correctly answered questions divided by the
number of questions with non-empty answers gen-
erated by our KB-QA system.
From Table 4 we can see that the accuracy of
RE
only
on Test (32.5%) is slightly better than
baseline?s result (31.4%). We think this improve-
ment comes from two aspects: (1) The quality of
the relation expressions is better than the quality
of the lexicon entries used in the baseline; and
(2) We use the extraction-related statistics of re-
lation expressions as features, which brings more
information to measure the confidence of map-
ping between NL phrases and KB predicates, and
makes the model to be more flexible. Meanwhile,
QP
only
perform worse (11.8%) than RE
only
, due
to coverage issue. But by comparing the precision-
s of these two settings, we find QP
only
(97.5%)
outperforms RE
only
(73.2%) significantly, due to
its high quality. This means how to extract high-
quality question patterns is worth to be studied for
the question answering task.
As the performance of our KB-QA system re-
lies heavily on the k-best beam approximation, we
evaluate the impact of the beam size and list the
comparison results in Figure 6. We can see that as
6
Head questions/queries mean the questions/queries with
high frequency and clear patterns.
974
we increase k incrementally, the accuracy increase
at the same time. However, a larger k (e.g. 200)
cannot bring significant improvements comparing
to a smaller one (e.g., 20), but using a large k has
a tremendous impact on system efficiency. So we
choose k = 20 as the optimal value in above ex-
periments, which trades off between accuracy and
efficiency.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
5 20 50 100 200
Accuracy on Test 
Accuracy
Figure 6: Impacts of beam size on accuracy.
Actually, the size of our system?s search space
is much smaller than the one of the semantic parser
used in the baseline.This is due to the fact that, if
triple queries generated by the question translation
component cannot derive any answer from KB, we
will discard such triple queries directly during the
QA procedure. We can see that using a small k
can achieve better results than baseline, where the
beam size is set to be 200.
4.4 Error Analysis
4.4.1 Entity Detection
Since named entity recognizers trained on Penn
TreeBank usually perform poorly on web queries,
We instead use a simple string-match method to
detect entity mentions in the question using a
cleaned entity dictionary dumped from our KB.
One problem of doing so is the entity detection
issue. For example, in the question who was Es-
ther?s husband ?, we cannot detect Esther as an
entity, as it is just part of an entity name. We need
an ad-hoc entity detection component to handle
such issues, especially for a web scenario, where
users often type entity names in their partial or ab-
breviation forms.
4.4.2 Predicate Mapping
Some questions lack sufficient evidences to detec-
t predicates. where is Byron Nelson 2012 ? is an
example. Since each relation expression must con-
tain at least one content word, this question cannot
match any relation expression. Except for Byron
Nelson and 2012, all the others are non-content
words.
Besides, ambiguous entries contained in rela-
tion expression sets of different predicates can
bring mapping errors as well. For the follow-
ing question who did Steve Spurrier play pro
football for? as an example, since the unigram
play exists in both Film.Film.Actor and Ameri-
can Football.Player.Current Team ?s relation ex-
pression sets, we made a wrong prediction, which
led to wrong answers.
4.4.3 Specific Questions
Sometimes, we cannot give exact answers to
superlative questions like what is the first book
Sherlock Holmes appeared in?. For this example,
we can give all book names where Sherlock
Holmes appeared in, but we cannot rank them
based on their publication date , as we cannot
learn the alignment between the constraint word
first occurred in the question and the predicate
Book.Written Work.Date Of First Publication
from training data automatically. Although we
have followed some work (Poon, 2013; Liang
et al, 2013) to handle such special linguistic
phenomena by defining some specific operators,
it is still hard to cover all unseen cases. We leave
this to future work as an independent topic.
5 Conclusion and Future Work
This paper presents a translation-based KB-QA
method that integrates semantic parsing and QA
in one unified framework. Comparing to the base-
line system using an independent semantic parser
with state-of-the-art performance, we achieve bet-
ter results on a general domain evaluation set.
Several directions can be further explored in the
future: (i) We plan to design a method that can
extract question patterns automatically, using ex-
isting labeled question patterns and KB as weak
supervision. As we discussed in the experiment
part, how to mine high-quality question patterns is
worth further study for the QA task; (ii) We plan
to integrate an ad-hoc NER into our KB-QA sys-
tem to alleviate the entity detection issue; (iii) In
fact, our proposed QA framework can be general-
ized to other intelligence besides knowledge bases
as well. Any method that can generate answers to
questions, such as the Web-based QA approach,
can be integrated into this framework, by using
them in the question translation component.
975
References
Yoav Artzi and Luke S. Zettlemoyer. 2011. Boot-
strapping semantic parsers from conversations. In
EMNLP, pages 421?432.
Yoav Artzi, Nicholas FitzGerald, and Luke S. Zettle-
moyer. 2013. Semantic parsing with combinatory
categorial grammars. In ACL (Tutorial Abstracts),
page 2.
Jonathan Berant, Andrew Chou, Roy Frostig, and Per-
cy Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP, pages 1533?
1544.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In ACL, pages 423?433.
James Clarke and Mirella Lapata. 2010. Discourse
constraints for document compression. Computa-
tional Linguistics, 36(3):411?441.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
ACL.
Cristina Espana-Bonet and Pere R. Comas. 2012. Full
machine translation for factoid question answering.
In EACL, pages 20?29.
Anthony Fader, Stephen Soderland, and Oren Etzion-
i. 2011. Identifying relations for open information
extraction. In EMNLP, pages 1535?1545.
Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In ACL, pages 1608?1618.
Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2011.
Bootstrapping the linked data web. In ISWC.
Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2012.
Extracting multilingual natural-language patterns
for rdf predicates. In ESWC.
Google. 2013. Freebase. In http://www.freebase.com.
Aditya Kalyanpur, Siddharth Patwardhan, Branimir
Boguraev, Adam Lally, and Jennifer Chu-Carroll.
2012. Fact-based question decomposition in deep-
qa. IBM Journal of Research and Development,
56(3):13.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In EMNLP, pages 1223?1233.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2011. Lexical general-
ization in ccg grammar induction for semantic pars-
ing. In EMNLP, pages 1512?1523.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and
Luke S. Zettlemoyer. 2013. Scaling seman-
tic parsers with on-the-fly ontology matching. In
EMNLP, pages 1545?1556.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In ACL, pages 590?599.
Percy Liang, Michael I. Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389?446.
Thomas Lin, Mausam, and Oren Etzioni. 2012. Entity
linking at web scale. In AKBC-WEKEX, pages 84?
88.
Raymond J. Mooney. 2007. Learning for semantic
parsing. In CICLing, pages 311?324.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160?
167.
Hoifung Poon. 2013. Grounded unsupervised seman-
tic parsing. In ACL, pages 933?943.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In HLT-NAACL.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In ACL.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In AAAI/IAAI, Vol. 2, pages 1050?
1055.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI, pages 658?666.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for parsing to
logical form. In EMNLP-CoNLL, pages 678?687.
976
