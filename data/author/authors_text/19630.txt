Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 721?730, Dublin, Ireland, August 23-29 2014.
Towards Syntax-aware Compositional Distributional Semantic Models
Lorenzo Ferrone
Department of Enterprise Engineering
University of Rome ?Tor Vergata?
Via del Politecnico, 1 00173 Roma
lorenzo.ferrone@gmail.com
Fabio Massimo Zanzotto
Department of Enterprise Engineering
University of Rome ?Tor Vergata?
Via del Politecnico, 1 00173 Roma
fabio.massimo.zanzotto@uniroma2.it
Abstract
Compositional Distributional Semantics Models (CDSMs) are traditionally seen as an entire dif-
ferent world with respect to Tree Kernels (TKs). In this paper, we show that under a suitable
regime these two approaches can be regarded as the same and, thus, structural information and
distributional semantics can successfully cooperate in CSDMs for NLP tasks. Leveraging on
distributed trees, we present a novel class of CDSMs that encode both structure and distribu-
tional meaning: the distributed smoothed trees (DSTs). By using DSTs to compute the similarity
among sentences, we implicitly define the distributed smoothed tree kernels (DSTKs). Exper-
iment with our DSTs show that DSTKs approximate the corresponding smoothed tree kernels
(STKs). Thus, DSTs encode both structural and distributional semantics of text fragments as
STKs do. Experiments on RTE and STS show that distributional semantics encoded in DSTKs
increase performance over structure-only kernels.
1 Introduction
Compositional distributional semantics is a flourishing research area that leverages distributional seman-
tics (see Turney and Pantel (2010), Baroni and Lenci (2010)) to produce meaning of simple phrases
and full sentences (hereafter called text fragments). The aim is to scale up the success of word-level
relatedness detection to longer fragments of text. Determining similarity or relatedness among sentences
is useful for many applications, such as multi-document summarization, recognizing textual entailment
(Dagan et al., 2013), and semantic textual similarity detection (Agirre et al., 2013).
Compositional distributional semantics models (CDSMs) are functions mapping text fragments to
vectors (or higher-order tensors). Functions for simple phrases directly map distributional vectors of
words to distributional vectors for the phrases (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010;
Clark et al., 2008; Grefenstette and Sadrzadeh, 2011; Zanzotto et al., 2010). Functions for full sentences
are generally defined as recursive functions over the ones for phrases (Socher et al., 2011; Socher et al.,
2012; Kalchbrenner and Blunsom, 2013). Distributional vectors for text fragments are then used as inner
layers in neural networks, or to compute similarity among text fragments via dot product.
CDSMs generally exploit structured representations t
x
of text fragments x to derive their meaning
f(t
x
), but the structural information, although extremely important, is obfuscated in the final vectors.
Structure and meaning can interact in unexpected ways when computing cosine similarity (or dot prod-
uct) between vectors of two text fragments, as shown for full additive models in (Ferrone and Zanzotto,
2013). Smoothed tree kernels (STK) (Mehdad et al., 2010; Croce et al., 2011) instead realize a clearer
interaction between structural information and distributional meaning. STKs are specific realizations of
convolution kernels (Haussler, 1999) where the similarity function is recursively (and, thus, composition-
ally) computed. Distributional vectors are used to represent word meaning in computing the similarity
among nodes. STKs, however, are not considered part of the CDSMs family. As usual in kernel machines
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
721
(Cristianini and Shawe-Taylor, 2000), STKs directly compute the similarity between two text fragments
x and y over their tree representations t
x
and t
y
, that is, STK(t
x
, t
y
). The function f that maps trees
into vectors is only implicitly used, and, thus, STK(t
x
, t
y
) is not explicitly expressed as the dot product
or the cosine between f(t
x
) and f(t
y
). Such a function f , which is the underlying reproducing function
of the kernel (Aronszajn, 1950), is a CDSM since it maps trees to vectors by using distributional mean-
ing. However, the huge dimensionality of R
n
(since it has to represent the set of all possible subtrees)
prevents to actually compute the function f(t), which thus can only remain implicit.
Distributed tree kernels (DTK) (Zanzotto and Dell?Arciprete, 2012) partially solve the last problem.
DTKs approximate standard tree kernels (such as (Collins and Duffy, 2002)) by defining an explicit
function DT that maps trees to vectors in R
m
where m  n and R
n
is the explicit space for tree
kernels. DTKs approximate standard tree kernels (TK), that is, ?DT (t
x
), DT (t
y
)? ? TK(t
x
, t
y
), by
approximating the corresponding reproducing function (Aronszajn, 1950). Thus, these distributed trees
are small vectors that encode structural information. In DTKs tree nodes u and v (and then also words)
are represented by nearly orthonormal vectors, that is, vectors
?
u and
?
v such that ?
?
u,
?
v ? ? ?(
?
u,
?
v )
where ? is the Kroneker?s delta. This is in contrast with distributional semantics vectors where ?
?
u,
?
v ?
is allowed to be any value in [0, 1] according to the similarity between the words v and u. Thus, early
attempts to include distributional vectors in the DTs failed (Zanzotto and Dell?Arciprete, 2011).
In this paper, leveraging on distributed trees, we present a novel class of CDSMs that encode both
structure and distributional meaning: the distributed smoothed trees (DST). DSTs carry structure and dis-
tributional meaning on a 2-dimensional tensor (a matrix): one dimension encodes the structure and one
dimension encodes the meaning. By using DSTs to compute the similarity among sentences with a gen-
eralized dot product (or cosine), we implicitly define the distributed smoothed tree kernels (DSTK) which
approximate the corresponding STKs. We present two DSTs along with the two smoothed tree kernels
(STKs) that they approximate. We experiment with our DSTs to show that their generalized dot products
approximate STKs by directly comparing the produced similarities and by comparing their performances
on two tasks: recognizing textual entailment (RTE) and semantic similarity detection (STS). Both exper-
iments show that the dot product on DSTs approximates STKs and, thus, DSTs encode both structural
and distributional semantics of text fragments in tractable 2-dimensional tensors. Experiments on STS
and RTE show that distributional semantics encoded in DSTs increases performance over structure-only
kernels. DSTs are the first positive way of taking into account both structure and distributional meaning
in CDSMs.
The rest of the paper is organized as follows. Section 2 introduces the basic notation used in the paper.
Section 3 describe our distributed smoothed trees as compositional distributional semantic models that
can represent both structural and semantic information. Section 4 reports on the experiments. Finally,
Section 5 draws some conclusions.
2 Notation
Before describing the distributed smoothed trees (DST) we introduce a formal way to denote
constituency-based lexicalized parse trees, as DSTs exploit this kind of data structures.
Lexicalized trees are denoted with the letter t and N(t) denotes the set of non terminal nodes of tree
t. Each non-terminal node n ? N(t) has a label l
n
composed of two parts l
n
= (s
n
, w
n
): s
n
is the
syntactic label, while w
n
is the semantic headword of the tree headed by n, along with its part-of-speech
tag. For example, the root node of the tree in Fig.1 has the label S:booked::v where S is the syntactic
information and booked::v is the semantic head of the whole tree. Terminal nodes of trees are treated
differently, these nodes represent only wordsw
n
without any additional information, and their labels thus
only consist of the word itself (see Fig. 1). The structure of a tree is represented as follows: Given a tree
t, h(t) is its root node and s(t) is the tree formed from t but considering only the syntactic structure (that
is, only the s
n
part of the labels), c
i
(n) denotes i-th child of a node n. As usual for constituency-based
parse trees, pre-terminal nodes are nodes that have a single terminal node as child. Finally,
?
s
n
? R
m
and
?
w
n
? R
k
represent respectively distributed vectors for node labels s
n
and distributional vectors for
words w
n
, whereas T represents the matrix of a tree t encoding structure and distributional meaning.
The difference between distributed and distributional vectors is described in the next section.
722
S:booked::v
X
X
X
X
X





NP:we::p
PRP:we::p
We
VP:booked::v
P
P
P
P




V:booked::v
booked
NP:flight::n
H
H
H



DT:the::d
the
NN:flight::n
flight
Figure 1: A lexicalized trees
S(t) = {
S:booked::v
l
l
,
,
NP VP
,
VP:booked::v
@ 
V NP
,
NP:we::p
PRP
,
S:booked::v
@ 
NP
PRP
VP
,
S:booked::v
Q
Q


NP VP
@ 
V NP
, . . . ,
VP:booked::v
b
b
"
"
V
booked
NP
@ 
DT NN
, . . . }
Figure 2: Subtrees of the tree t in Figure 1 (a non-exhaustive list)
3 Distributed Smoothed Trees as Compositional Distributional Semantic Models
We define Distributed Smoothed Trees as recursive functions DST mapping lexicalized trees t to R
m?k
where matrices T = DST (t) encode both syntactic structures and distributional vectors. DSTs are
thus compositional distributional models, as they map lexicalized trees to matrices, and they are defined
recursively on distributed vectors for syntactic node labels and distributional vectors for words. In the
following we introduce DSTs: Section 3.1 gives a rough idea of the method, Section 3.2 describes how
to recursively encode structures in vectors by means of distributed trees (Zanzotto and Dell?Arciprete,
2012), and finally Section 3.3 merges distributed trees and distributional semantic vectors in matrices.
3.1 The method in a glance
We describe here the approach in a few sentences. In line with tree kernels over structures (Collins and
Duffy, 2002), we introduce the set S(t) of the subtrees t
i
of a given lexicalized tree t. A subtree t
i
is in
the set S(t) if s(t
i
) is a subtree of s(t) and, if n is a node in t
i
, all the siblings of n in t are in t
i
. For each
node of t
i
we only consider its syntactic label s
n
, except for the head h(t
i
) for which we also consider
its semantic component w
n
. Figure 2 reports a sample for the subtrees of the tree in Fig. 1 The recursive
functions DSTs we define compute the following:
T =
?
t
i
?S(t)
T
i
where T
i
is the matrix associated to each subtree t
i
. The similarity between two text fragments a and b
represented as lexicalized trees t
a
and t
b
can be computed using the Frobenius product between the two
matrices T
a
and T
b
, that is:
?T
a
,T
b
?
F
=
?
t
a
i
?S(t
a
)
t
b
j
?S(t
b
)
?T
a
i
,T
b
j
?
F
(1)
We want to obtain that the product ?T
a
i
,T
b
j
?
F
approximates the dot product between the distributional
vectors of the head words (?T
a
i
,T
b
j
?
F
? ?
?
h(t
a
i
),
?
h(t
b
j
)?) whenever the syntactic structure of the subtrees
is the same (that is s(t
a
i
) = s(t
b
j
)), and ?T
a
i
,T
b
j
?
F
? 0 otherwise. This property is expressed as:
?T
a
i
,T
b
j
?
F
? ?(s(t
a
i
), s(t
b
j
)) ? ?
?
h(t
a
i
),
?
h(t
b
j
)? (2)
723
3.2 Representing Syntactic Structures with Distributed Trees
Distributed trees (Zanzotto and Dell?Arciprete, 2012) recursively encode syntactic trees t in small vectors
by means of a recursive function DT . These DTs preserve structural information as the dot product
between the DTs of two trees approximates the classical tree kernels TK as defined by Collins and
Duffy (2002), that is, TK(t
a
, t
b
) ? ?DT (t
a
), DT (t
b
)?. To obtain this result, distributed trees DT (t) are
defined as follows:
DT (t) =
?
t
i
?S(t)
?
?
|N(t
i
)|
?
s(t
i
) (3)
where S(t) is again the set of the subtrees of t,
?
s(t
i
) are vectors in R
m
corresponding to tree fragment t
i
and
?
?
|N(t
i
)|
is the weight of subtree t
i
in the final feature space, with ? being the traditional parameter
used to penalize large subtrees and |N(t
i
)| being the number of nodes in t
i
. The approximation of tree
kernels is then given by the fact that ?
?
s(t
i
),
?
s(t
j
)? ? ?(s(t
i
), s(t
j
)). Vectors with this property are called
distributed vectors. A key feature of the distributed vectors of subtrees
?
s(t
i
) is that these vectors are built
compositionally from a setN of nearly orthonormal random vectors
?
s
n
, that are associated to each node
label. Given a subtree s(t
i
), the related vector is obtained as:
?
s(t
i
) =
?
s
n
1

?
s
n
2
 . . .
?
s
n
k
=
?
(s
n
,w
n
)?N(t
i
)
?
s
n
where node vectors
?
s
n
i
are ordered according to a depth-first visit of subtree t
i
and is a vector composi-
tion operation, specifically the shuffled circular convolution
1
. This function guarantees that two different
subtrees have nearly orthonormal vectors (see (Zanzotto and Dell?Arciprete, 2012) for more details). For
example, the fifth tree t
5
of set S(t) in Figure 2 is
?
s(t
5
) =
?
S  (
?
NP  (
?
V P  (
?
V 
?
NP ))). Thus, DTs
in Equation 3 can be recursively defined as:
DT (t) =
?
n?N(t)
?(n) (4)
where ?(n) is recursively defined as follows:
?(n) =
{
?
? (
?
s
n

?
w) if n is a pre-terminal node
?
?
?
s
n
 (
?
i
(
?
s
c
i
(n)
+ ?(c
i
(n)))) if n is an internal node
(5)
The vector ?(n) encodes all the subtrees that have root in n along with their penalizing weight
?
?
|N(t
i
)|
,
that is:
?(n) =
?
t
i
?S(t)?h(t
i
)=n
?
?
|N(t
i
)|
?
s(t
i
)
This is what we need in order to define our distributed smoothed trees.
3.3 Representing distributional meaning and distributed structure with matrices
We now move from distributed trees (encoded as small vectors) to distributed smoothed trees (DST)
represented as matrices. DST is a function that maps trees t to matrices T. In analogy with Equation 4,
DST is defined as:
DST (t) =
?
n?N(t)
S(n)
where S(n) is now defined as:
S(n) = ?(n)
?
w
n
>
1
The shuffled circular convolution  is defined as
?
a 
?
b = s
1
(
?
a ) ? s
2
(
?
b ) where ? is the circular convolution and s
1
and
s
2
are two different (but fixed) random permutations of vector elements.
724
where ?(n) is the one defined in Equation 5 and (?)
>
is vector transposition. By combining the two
equations, DST (t) is the sum of the matrices described in Equation 1:
DST (t) =
?
n?N(t)
?
t
i
?S(t)?h(t
i
)=n
?
?
|N(t
i
)|
?
s(t
i
)
?
w
n
>
=
?
t
i
?S(t)
?
s(t
i
)
?
w
n
>
where n is h(t
i
) and T
i
=
?
s(t
i
)
?
w
h(t
i
)
>
is the outer product between the distributed vector
?
s(t
i
) and
the distributional vector
?
w
h(t
i
)
. There is an important property of the outer product that applies to the
Frobenius product: ?
?
a
?
w
>
,
?
b
?
v
>
?
F
= ?
?
a ,
?
b ? ? ?
?
w,
?
v ?. Using this property, we have that Equation 2 is
satisfied as:
?T
i
,T
j
?
F
= ?
?
s(t
i
),
?
s(t
j
)? ? ?
?
w
h(t
i
)
,
?
w
h(t
j
)
? ? ?(s(t
i
), s(t
j
)) ? ?
?
w
h(t
i
)
,
?
w
h(t
j
)
?
We refer to the Frobenius product of two distributed smoothed trees as distributed smoothed tree kernel
(DSTK). These DSTKs are approximating the smoothed tree kernels described in the next section. We
propose two versions of our DSTKs according to how we produce distributional vectors for words. We
have a plain version DSTK
0
when we use distributional vectors
?
w
n
as they are, and a slightly modified
version DSTK
+1
when we use as distributional vectors
?
w
n
?
=
(
1
?
w
n
)
.
3.4 The Approximated Smoothed Tree Kernels
The two CDSMs we proposed, that is, the two distributed smoothed tree kernelsDSTK
0
andDSTK
+1
,
are approximating two specific tree kernels belonging to the smoothed tree kernels class (e.g., (Mehdad
et al., 2010; Croce et al., 2011)). These two specific smoothed tree kernels recursively compute (but, the
recursive formulation is not given here) the following general equation:
STK(t
a
, t
b
) =
?
t
i
?S(t
a
)
t
j
?S(t
b
)
?(t
i
, t
j
)
where ?(t
i
, t
j
) is the similarity weight between two subtrees t
i
and t
j
. DTSK
0
and DSTK
+1
approx-
imate respectively STK
0
and STK
+1
where the weights are defined as follows:
?
0
(t
i
, t
j
) = ?
?
w
h(t
i
)
,
?
w
h(t
j
)
? ? ?(s(t
i
), s(t
j
)) ?
?
?
|N(t
i
)|+|N(t
j
)|
?
+1
(t
i
, t
j
) = (?
?
w
h(t
i
)
,
?
w
h(t
j
)
?+ 1) ? ?(s(t
i
), s(t
j
)) ?
?
?
|N(t
i
)|+|N(t
j
)|
STK
+1
is actually computing a sum between STK
0
and the tree kernel (Collins and Duffy, 2002).
4 Experimental investigation
4.1 Experimental set-up
Generic settings We experimented with two datasets: the Recognizing Textual Entailment datasets
(RTE) (Dagan et al., 2006) and the the Semantic Textual Similarity 2013 datasets (STS) (Agirre et al.,
2013). The STS task consists of determining the degree of similarity (ranging from 0 to 5) between
two sentences. We used the data for core task of the 2013 challenge data. The STS datasets contains
5 datasets: headlines, OnWN, FNWN, SMT and MSRpar, which contains respectively 750, 561, 189,
750 and 1500 pairs. The first four datasets were used for testing, while all the training has been done
on the fifth. RTE is instead the task of deciding whether a long text T entails a shorter text, typically
a single sentence, called hypothesis H . It has been often seen as a classification task (see (Dagan et
al., 2013)). We used four datasets: RTE1, RTE2, RTE3, and RTE5, with the standard split between
training and testing. The dev/test distribution for RTE1-3, and RTE5 is respectively 567/800, 800/800,
800/800, and 600/600 T-H pairs. Distributional vectors are derived with DISSECT (Dinu et al., 2013)
from a corpus obtained by the concatenation of ukWaC (wacky.sslmit.unibo.it), a mid-2009 dump of
725
RTE1 RTE2 RTE3 RTE5 headl FNWN OnWN SMT
STK
0
vs DSTK
0
1024 0.86 0.84 0.90 0.84 0.87 0.65 0.95 0.77
2048 0.87 0.84 0.91 0.84 0.90 0.65 0.96 0.77
STK
+1
vs DSTK
+1
1024 0.81 0.77 0.83 0.72 0.88 0.53 0.93 0.66
2048 0.82 0.78 0.84 0.74 0.91 0.56 0.94 0.67
Table 1: Spearman?s correlation between Distributed Smoothed Tree Kernels and Smoothed Tree Kernels
the English Wikipedia (en.wikipedia.org) and the British National Corpus (www.natcorp.ox.ac.uk), for
a total of about 2.8 billion words. We collected a 35K-by-35K matrix by counting co-occurrence of the
30K most frequent content lemmas in the corpus (nouns, adjectives and verbs) and all the content lemmas
occurring in the datasets within a 3 word window. The raw count vectors were transformed into positive
Pointwise Mutual Information scores and reduced to 300 dimensions by Singular Value Decomposition.
This setup was picked without tuning, as we found it effective in previous, unrelated experiments. To
build our DTSKs and for the two baseline kernels TK and DTK, we used the implementation of the
distributed tree kernels
2
. We used: 1024 and 2048 as the dimension of the distributed vectors, the weight
? is set to 0.4 as it is a value generally considered optimal for many applications (see also (Zanzotto and
Dell?Arciprete, 2012)). The statistical significance, where reported, is computed according to the sign
test.
Direct correlation settings For the direct correlation experiments, we used the RTE data sets and the
testing sets of the STS dataset (that is, headlines, OnWN, FNWN, SMT). We computed the Spearman?s
correlation between values produced by our DSTK
0
and DSTK
+1
and produced by the standard ver-
sions of the smoothed tree kernel, that is, respectively, STK
0
and STK
+1
. We obtained text fragment
pairs by randomly sampling two text fragments in the selected set. For each set, we produced exactly the
number of examples in the set, e.g., we produced 567 pairs for RTE1 dev, etc..
Task-based settings For the task-based experiments, we compared systems using the standard evalua-
tion measure and the standard split in the respective challenges. As usual in RTE challenges the measure
used is the accuracy, as testing sets have the same number of entailment and non-entailment pairs. For
STS, we used MSRpar as training, and we used the 4 test sets as testing. We compared systems using
the Pearson?s correlation as the standard evaluation measure for the challenge
3
. Thus, results can be
compared with the results of the challenge.
As classifier and regression learner, we used the java version of LIBSVM (Chang and Lin, 2011). In
the two tasks we used in a different way our DSTs (and the related STKs) within the learners. In the
following, we refer to instances in RTE or STS as pairs p = (t
a
, t
b
) where t
a
and t
b
are the two parse
trees for the two sentences a and b for STS and for the text a and the hypothesis b in RTE.
We will indicate with K(p
1
, p
2
) the final kernel used in the learning algorithm, which takes as in-
put two training instances, while we will use ? to denote either any of our DSTK (that is, ?(x, y) =
?DST (x), DST (y)?) or any of the standard smoothed tree kernels (that is, ?(x, y) = STK(x, y)).
In STS, we encoded only similarity feature between the two sentences. Thus, we used two classes of
kernels: (1) the syntactic/semantic class (SS) with the final kernel defined as K(p
1
, p
2
) = (?(t
a
1
, t
b
1
) ?
?(t
a
2
, t
b
2
) + 1)
2
; and, (2) the SS class along with token-based similarity (SSTS) where the final kernel is
K(p
1
, p
2
) = (?(t
a
1
, t
b
1
) ? ?(t
a
2
, t
b
2
) + TS(a
1
, b
1
) ? TS(a
2
, b
2
) + 1)
2
where TS(a, b) counts the percent of
the common content tokens in a and b.
In RTE, we followed standard approaches (Dagan et al., 2013; Zanzotto et al., 2009), that is, we
exploited two models: a model with only a rewrite rule feature space (RR) and a model with the previous
space along with a token-level similarity feature (RRTWS). The two models use our DSTs and the
standard STKs in the following way as kernel functions: (1) RR(p
1
, p
2
) = ?(t
a
1
, t
a
2
) + ?(t
b
1
, t
b
2
); (2)
RRTS(p
1
, p
2
) = ?(t
a
1
, t
a
2
) + ?(t
b
1
, t
b
2
) + (TWS(a
1
, b
1
) ? TS(a
2
, b
2
) + 1)
2
where TWS is a weighted
token similarity as in Corley and Mihalcea (2005).
2
http://code.google.com/p/distributed-tree-kernels/
3
Correlations are obtained with the organizers? script
726
SS SSTS
headl FNWN OnWN SMT Average headl FNWN OnWN SMT Average
TS ? ? ? ? ? 0.701 0.311 0.515 0.323 0.462
Add ? ? ? ? ? 0.691 0.268 0.511 0.317 0.446
Mult ? ? ? ? ? 0.291 ?0.03 0.228 0.291 0.201
DTK 0.448 0.118 0.162 0.301 0.257 0.698 0.311 0.510 0.329 0.462
TK 0.456 0.145 0.158 0.303 0.265
?
0.699 0.316 0.511 0.329 0.463
?
DSTK
0
0.491 0.155 0.358 0.305 0.327
?
0.700 0.314 0.519 0.327 0.465
STK
0
0.490 0.159 0.349 0.305 0.325
?
0.700 0.314 0.519 0.327 0.465
?
DSTK
+1
0.475 0.138 0.266 0.304 0.295 0.700 0.314 0.519 0.327 0.465
STK
+1
0.478 0.156 0.259 0.305 0.299
?
0.700 0.314 0.519 0.327 0.465
?
Table 2: Task-based analysis: Correlation on Semantic Textual Similarity ( ? is different from DTK, TK,
DSTK
+1
, and STK
+1
with a stat.sig. of p > 0.1; ? the difference between the kernel and its distributed
version is not stat.sig.)
We also used two standard and simple CDSMs to compare with: the Additive model (Add) and the
Multiplicative model (Mult) as firstly discussed in Mitchell and Lapata (2008). The Additive Model
performs a sum of all the distributional vectors of the content words in the text fragment and the Multi-
plicative model performs an element-wise product among all the content vectors. These are used in the
above models as ?(a, b).
Finally, to investigate whether our DSTKs behave better than purely structural models, we experi-
mented with the classical tree kernel (TK) (Collins and Duffy, 2002) and the distributed tree kernel (DTK)
(Zanzotto and Dell?Arciprete, 2012). Again, these kernels are used in the above models as ?(t
a
, t
b
).
4.2 Results
Table 1 reports the results for the correlation experiments. We report the Spearman?s correlations over
the different sets (and different dimensions of distributed vectors) between our DSTK
0
and the STK
0
(first two rows) and between our DSTK
+1
and the corresponding STK
+1
(second two rows) . The
correlation is above 0.80 in average for both RTE and STS datasets in the case of DSTK
0
and the
STK
0
. The correlation between DSTK
+1
and the corresponding STK
+1
is instead a little bit lower.
This depends on the fact that DSTK
+1
is approximating the sum of two kernels the TK and the STK
0
(as STK
+1
is the sum of the two kernels). Then, the underlying feature space is bigger with respect to the
one of STK
0
and, thus, approximating it is more difficult. The approximation also depends on the size of
the distributed vectors. Higher dimensions yield to better approximation: if we increase the distributed
vectors dimension from 1024 to 2048 the correlation between DSTK
+1
and STK
+1
increases up to
0.80 on RTE and up to 0.77 on STS. This direct analysis of the correlation shows that our CDSM are
approximating the corresponding kernel function and there is room of improvement by increasing the size
of distributed vectors. Task-based experiments confirm the above trend. Table 2 and Table 3, respectively,
report the correlation of different systems on STS and the accuracies of the different systems on RTE.
Our CDSMs are compared against baseline systems (Add,Mult, TK, andDTK) in order to understand
whether in the specific tasks our more complex model is interesting, and against, again, the systems with
the corresponding smoothed tree kernels in order to explore whether our DSTKs approximate systems
based on STKs. For all this set of experiment we fixed the dimension of the distributed vectors to
1024. Table 2 is organized as follows: columns 2-6 report the correlation of the STS systems based
on syntactic/semantic similarity (SS) and columns 7-11 report the accuracies of SS systems along with
token-based similarity (SSTS). The first observation for this task is that baseline systems based only on
the token similarity (first row) behave extremely well. These results are above many models presented
in the 2013 Shared Task (see (Agirre et al., 2013)). This can be disappointing as we cannot appreciate
differences among methods in the columns SSTS. But, focusing on the results without this important
token-based similarity, we can better understand if our model is capturing both structural and semantic
information, that is, if DSTKs behave similarly to STKs. It is also useless to compare results of DSTKs
and STKs to the Add baseline model as Add is basically doing a weighted count of the common words
727
RR RRTWS
RTE1 RTE2 RTE3 RTE5 Average RTE1 RTE2 RTE3 RTE5 Average
Add 0.541 0.496 0.507 0.520 0.516 0.560 0.538 0.643 0.578 0.579
Mult 0.495 0.481 0.497 0.528 0.500 0.533 0.563 0.642 0.586 0.581
DTK 0.533 0.515 0.516 0.530 0.523 0.583 0.601 0.643 0.621 0.612
TK 0.561 0.552 0.531 0.54 0.546 0.608 0.627 0.648 0.630 0.628
DSTK
0
0.571 0.551 0.547 0.531 0.550
?
0.628 0.616 0.650 0.625 0.629
?
STK
0
0.586 0.563 0.538 0.545 0.558
?
0.638 0.618 0.648 0.636 0.635
?
DSTK
+1
0.588 0.562 0.555 0.541 0.561
?
0.638 0.621 0.646 0.652 0.639
?
STK
+1
0.586 0.562 0.542 0.546 0.559
?
0.638 0.618 0.650 0.636 0.635
?
Table 3: Task-based analysis: Accuracy on Recognizing Textual Entailment ( ? is different from DTK
and TK wiht a stat.sig. of p > 0.1; ? the difference between the kernel and its distributed counterpart is
not statistically significant.)
that is exactly what the token-based similarity is doing. Add slightly decreases the performance of
the token-based similarity. The Mult model instead behaves very poorly. Comparing rows in the SS
columns, we can discover that DSTK
0
and DSTK
+1
behave significantly better than DTK and that
DSTK
0
behave better than the standard TK. Thus, our DSTKs are positively exploitng distributional
semantic information along with structural information. Moreover, both DSTK
0
and DSTK
+1
behave
similarly to the corresponding models with standard kernels STKs. Results in this task confirm that
structural and semantic information are both captured by CDSMs based on DSTs.
Table 3 is organized as follows: columns 2-6 report the accuracy of the RTE systems based on rewrite
rules (RR) and columns 7-11 report the accuracies of RR systems along with token similarity (RRTS).
Results on RTE are extremely promising as all the models including structural information and distribu-
tional semantics have better results than the two baseline models with a statistical significance of 93.7%.
For RR models DTSK
0
, STK
0
, DSTK
+1
, and STK
+1
have an average accuracy 7.9% higher than
Add and 11.4% higher than Mult model. For RRTS, the same happens with an average accuracy 9.58%
higher than Add and 9.2% higher than the Mult. This task is more sensible to syntactic information
than STS. As expected (Mehdad et al., 2010), STKs behave also better than tree kernels exploiting only
syntactic information. But, more importantly, our CDSMs based on the DSTs are behaving similarly
to these smoothed tree kernels, in contrast to what reported in (Zanzotto and Dell?Arciprete, 2011). In
(Polajnar et al., 2013), it appears that results of the Zanzotto and Dell?Arciprete (2011)?s method are
comparable to the results of STKs for STS, but this is mainly due to the flattening of the performance
given by the lexical token similarity feature which is extremely relevant in STS. Even if distributed tree
kernels do not approximate well tree kernels with distributed vectors dimension of 1024, our smoothed
versions of the distributed tree kernels approximate correctly the corresponding smoothed tree kernels.
Their small difference is not statistically significant (less than 70%). The fact that our DSTKs behave
significantly better than baseline models in RTE and they approximate the corresponding STKs shows
that it is possible to positively exploit structural information in CDSMs.
5 Conclusions and Future Work
Distributed Smoothed Trees (DST) are a novel class of Compositional Distributional Semantics Mod-
els (CDSM) that effectively encode structural information and distributional semantics in tractable 2-
dimensional tensors, as experiments show. The paper shows that DSTs contribute to close the gap be-
tween two apparently different approaches: CDSMs and convolution kernels (Haussler, 1999). This
contribute to start a discussion on a deeper understanding of the representation power of structural infor-
mation of existing CDSMs.
References
[Agirre et al.2013] Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. 2013. *sem
2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational
728
Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual
Similarity, pages 32?43, Atlanta, Georgia, USA, June. Association for Computational Linguistics.
[Aronszajn1950] N. Aronszajn. 1950. Theory of reproducing kernels. Transactions of the American Mathematical
Society, 68(3):337?404.
[Baroni and Lenci2010] Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework
for corpus-based semantics. Comput. Linguist., 36(4):673?721, December.
[Baroni and Zamparelli2010] Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are ma-
trices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing, pages 1183?1193, Cambridge, MA, October. Association
for Computational Linguistics.
[Chang and Lin2011] Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector ma-
chines. ACM Transactions on Intelligent Systems and Technology, 2:27:1?27:27. Software available at
http://www.csie.ntu.edu.tw/
?
cjlin/libsvm.
[Clark et al.2008] Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh. 2008. A compositional distributional
model of meaning. Proceedings of the Second Symposium on Quantum Interaction (QI-2008), pages 133?140.
[Collins and Duffy2002] Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging:
Kernels over discrete structures, and the voted perceptron. In Proceedings of ACL02.
[Corley and Mihalcea2005] Courtney Corley and Rada Mihalcea. 2005. Measuring the semantic similarity of texts.
In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 13?18.
Association for Computational Linguistics, Ann Arbor, Michigan, June.
[Cristianini and Shawe-Taylor2000] Nello Cristianini and John Shawe-Taylor. 2000. An Introduction to Support
Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press, March.
[Croce et al.2011] Danilo Croce, Alessandro Moschitti, and Roberto Basili. 2011. Structured lexical similarity via
convolution kernels on dependency trees. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 1034?1046, Stroudsburg, PA, USA. Association for Computational
Linguistics.
[Dagan et al.2006] Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual
entailment challenge. In Quionero-Candela et al., editor, LNAI 3944: MLCW 2005, pages 177?190. Springer-
Verlag, Milan, Italy.
[Dagan et al.2013] Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing
Textual Entailment: Models and Applications. Synthesis Lectures on Human Language Technologies. Morgan
& Claypool Publishers.
[Dinu et al.2013] Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013. DISSECT: DIStributional SEman-
tics Composition Toolkit. In Proceedings of ACL (System Demonstrations), pages 31?36, Sofia, Bulgaria.
[Ferrone and Zanzotto2013] Lorenzo Ferrone and Fabio Massimo Zanzotto. 2013. Linear compositional distribu-
tional semantics and structural kernels. In Proceedings of the Joint Symposium of Semantic Processing (JSSP),
pages ?.
[Grefenstette and Sadrzadeh2011] Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support
for a categorical compositional distributional model of meaning. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP ?11, pages 1394?1404, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Haussler1999] David Haussler. 1999. Convolution kernels on discrete structures. Technical report, University of
California at Santa Cruz.
[Kalchbrenner and Blunsom2013] Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent convolutional neural net-
works for discourse compositionality. Proceedings of the 2013 Workshop on Continuous Vector Space Models
and their Compositionality.
[Mehdad et al.2010] Yashar Mehdad, Alessandro Moschitti, and Fabio Massimo Zanzotto. 2010. Syntac-
tic/semantic structures for textual entailment recognition. In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the Association for Computational Linguistics, HLT ?10, pages
1020?1028, Stroudsburg, PA, USA. Association for Computational Linguistics.
729
[Mitchell and Lapata2008] Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composi-
tion. In Proceedings of ACL-08: HLT, pages 236?244, Columbus, Ohio, June. Association for Computational
Linguistics.
[Polajnar et al.2013] Tamara Polajnar, Laura Rimell, and Douwe Kiela. 2013. Ucam-core: Incorporating struc-
tured distributional similarity into sts. In Second Joint Conference on Lexical and Computational Semantics
(*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,
pages 85?89, Atlanta, Georgia, USA, June. Association for Computational Linguistics.
[Socher et al.2011] Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Man-
ning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in
Neural Information Processing Systems 24.
[Socher et al.2012] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic
Compositionality Through Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Conference on Empir-
ical Methods in Natural Language Processing (EMNLP).
[Turney and Pantel2010] Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space
models of semantics. J. Artif. Intell. Res. (JAIR), 37:141?188.
[Zanzotto and Dell?Arciprete2011] Fabio Massimo Zanzotto and Lorenzo Dell?Arciprete. 2011. Distributed struc-
tures and distributional meaning. In Proceedings of the Workshop on Distributional Semantics and Composi-
tionality, pages 10?15, Portland, Oregon, USA, June. Association for Computational Linguistics.
[Zanzotto and Dell?Arciprete2012] F.M. Zanzotto and L. Dell?Arciprete. 2012. Distributed tree kernels. In Pro-
ceedings of International Conference on Machine Learning, pages 193?200.
[Zanzotto et al.2009] Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessandro Moschitti. 2009. A machine
learning approach to textual entailment recognition. NATURAL LANGUAGE ENGINEERING, 15-04:551?582.
[Zanzotto et al.2010] Fabio Massimo Zanzotto, Ioannis Korkontzelos, Francesca Fallucchi, and Suresh Manand-
har. 2010. Estimating linear models for compositional distributional semantics. In Proceedings of the 23rd
International Conference on Computational Linguistics (COLING), August,.
730
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 93?98,
Dublin, Ireland, August 23-24 2014.
Compositional Distributional Semantics Models
in Chunk-based Smoothed Tree Kernels
Nghia The Pham
University of Trento
thenghia.pham@unitn.it
Lorenzo Ferrone
University of Rome ?Tor Vergata?
lorenzo.ferrone@gmail.com
Fabio Massimo Zanzotto
University of Rome ?Tor Vergata?
fabio.massimo.zanzotto@uniroma2.it
Abstract
The field of compositional distributional
semantics has proposed very interesting
and reliable models for accounting the
distributional meaning of simple phrases.
These models however tend to disregard
the syntactic structures when they are ap-
plied to larger sentences. In this paper we
propose the chunk-based smoothed tree
kernels (CSTKs) as a way to exploit the
syntactic structures as well as the reliabil-
ity of these compositional models for sim-
ple phrases. We experiment with the rec-
ognizing textual entailment datasets. Our
experiments show that our CSTKs per-
form better than basic compositional dis-
tributional semantic models (CDSMs) re-
cursively applied at the sentence level, and
also better than syntactic tree kernels.
1 Introduction
A clear interaction between syntactic and semantic
interpretations for sentences is important for many
high-level NLP tasks, such as question-answering,
textual entailment recognition, and semantic tex-
tual similarity. Systems and models for these tasks
often use classifiers or regressors that exploit con-
volution kernels (Haussler, 1999) to model both
interpretations.
Convolution kernels are naturally defined on
spaces where there exists a similarity function be-
tween terminal nodes. This feature has been used
to integrate distributional semantics within tree
kernels. This class of kernels is often referred to as
smoothed tree kernels (Mehdad et al., 2010; Croce
et al., 2011), yet, these models only use distribu-
tional vectors for words.
Compositional distributional semantics models
(CDSMs) on the other hand are functions map-
ping text fragments to vectors (or higher-order ten-
sors) which then provide a distributional meaning
for simple phrases or sentences. Many CDSMs
have been proposed for simple phrases like non-
recursive noun phrases or verbal phrases (Mitchell
and Lapata, 2008; Baroni and Zamparelli, 2010;
Clark et al., 2008; Grefenstette and Sadrzadeh,
2011; Zanzotto et al., 2010). Non-recursive
phrases are often referred to as chunks (Abney,
1996), and thus, CDSMs are good and reliable
models for chunks.
In this paper, we present the chunk-based
smoothed tree kernels (CSTK) as a way to merge
the two approaches: the smoothed tree kernels
and the models for compositional distributional se-
mantics. Our approach overcomes the limitation
of the smoothed tree kernels which only use vec-
tors for words by exploiting reliable CDSMs over
chunks. CSTKs are defined over a chunk-based
syntactic subtrees where terminal nodes are words
or word sequences. We experimented with CSTKs
on data from the recognizing textual entailment
challenge (Dagan et al., 2006) and we compared
our CSTKs with other standard tree kernels and
standard recursive CDSMs. Experiments show
that our CSTKs perform better than basic compo-
sitional distributional semantic models (CDSMs)
recursively applied at the sentence level and better
than syntactic tree kernels.
The rest of the paper is organized as follows.
Section 2 describes the CSTKs. Section 3 re-
ports on the experimental setting and on the re-
sults. Finally, Section 4 draws the conclusions and
sketches the future work.
2 Chunk-based Smoothed Tree Kernels
This section describes the new class of kernels.
We first introduce the notion of the chunk-based
syntactic subtree. Then, we describe the recursive
formulation of the class of kernels. Finally, we in-
troduce the basic CDSMs we use and we introduce
two instances of the class of kernels.
93
2.1 Notation and preliminaries
S
h
h
h
h
h
(
(
(
(
(
NP
X
X
X



DT
the:d
NN
rock:n
NN
band:n
VP
X
X
X



VBZ
holds:v
NP
X
X
X
X




PRP
its:p
JJ
final:j
NN
concert:n
Figure 1: Sample Syntactic Tree
A Chunk-based Syntactic Sub-Tree is a subtree
of a syntactic tree where each non-terminal node
dominating a contiguous word sequence is col-
lapsed into a chunk and, as usual in chunks (Ab-
ney, 1996), the internal structure is disregarded.
For example, Figure 2 reports some chunk-based
syntactic subtrees of the tree in Figure 1. Chunks
are represented with a pre-terminal node dominat-
ing a triangle that covers a word sequence. The
first subtree represents the chunk covering the sec-
ond NP and the node dominates the word sequence
its:d final:n concert:n. The second subtree repre-
sents the structure of the whole sentence and one
chunk, that is the first NP dominating the word
sequence the:d rock:n band:n. The third subtree
again represents the structure of the whole sen-
tence split into two chunks without the verb.
NP
`
`
`
`
 
 
 
 
its:p final:j concert:n
S
X
X
X



NP
X
X
X
X




the:d rock:n band:n
VP
ZProceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 300?304,
Dublin, Ireland, August 23-24, 2014.
haLF: Comparing a Pure CDSM Approach with a Standard
Machine Learning System for RTE
Lorenzo Ferrone
University of Rome ?Tor Vergata?
Via del Politecnico 1
00133 Roma, Italy
lorenzo.ferrone@gmail.com
Fabio Massimo Zanzotto
University of Rome ?Tor Vergata?
Via del Politecnico 1
00133 Roma, Italy
fabio.massimo.zanzotto@uniroma2.it
Abstract
In this paper, we describe our sub-
mission to the Shared Task #1. We
tried to follow the underlying idea of
the task, that is, evaluating the gap
of full-fledged recognizing textual en-
tailment systems with respect to com-
positional distributional semantic mod-
els (CDSMs) applied to this task. We
thus submitted two runs: 1) a sys-
tem obtained with a machine learning
approach based on the feature spaces
of rules with variables and 2) a sys-
tem completely based on a CDSM that
mixes structural and syntactic infor-
mation by using distributed tree ker-
nels. Our analysis shows that, under
the same conditions, the fully CDSM
system is still far from being competi-
tive with more complex methods.
1 Introduction
Recognizing Textual Entailment is a largely
explored problem (Dagan et al., 2013). Past
challenges (Dagan et al., 2006; Bar-Haim et
al., 2006; Giampiccolo et al., 2007) explored
methods and models applied in complex and
natural texts. In this context, machine learn-
ing solutions show interesting results. The
Shared Task #1 of SemEval instead wants to
explore systems in a more controlled textual
environment where the phenomena to model
are clearer. The aim of the Shared Task is to
study how RTE systems built upon composi-
tional distributional semantic models behave
This work is licenced under a Creative Commons At-
tribution 4.0 International License. Page numbers and
proceedings footer are added by the organizers. License
details: http://creativecommons.org/licenses/by/
4.0/
with respect to the above tradition. We tried
to capture this underlying idea of the task.
In this paper, we describe our submission
to the Shared Task #1. We tried to fol-
low the underlying idea of the task, that is,
evaluating the gap of full-fledged recognizing
textual entailment systems with respect to
compositional distributional semantic models
(CDSMs) applied to this task. We thus sub-
mitted two runs: 1) a system obtained with a
machine learning approach based on the fea-
ture spaces of rules with variables (Zanzotto
et al., 2009) and 2) a system completely based
on a CDSM that mixes structural and syntac-
tic information by using distributed tree ker-
nels (Zanzotto and Dell?Arciprete, 2012). Our
analysis shows that, under the same condi-
tions, the fully CDSM system is still far from
being competitive with more complete meth-
ods.
The rest of the paper is organized as follows.
Section 2 describes the full-fledged recognizing
textual entailment system that is used for com-
parison. Section 3 introduces a novel composi-
tional distributional semantic model, namely,
the distributed smoothed tree kernels, and the
way this model is applied to the task of RTE.
Section 4 describes the results in the challenge
and it draws some preliminary conclusions.
2 A Standard full-fledged Machine
Learning Approach for RTE
For now on, the task of recognizing textual en-
tailment (RTE) is defined as the task to decide
if a pair p = (a, b) like:
(?Two children are lying in the snow and are
making snow angels?, ?Two angels are
making snow on the lying children?)
is in entailment, in contradiction, or neutral.
As in the tradition of applied machine learn-
300
ing models, the task is framed as a multi-
classification problem. The difficulty is to de-
termine the best feature space on which to
train the classifier.
A full-fledged RTE systems based on ma-
chine learning that has to deal with natural
occurring text is generally based on:
? some within-pair features that model the
similarity between the sentence a and the
sentence b
? some features representing more complex
information of the pair (a, b) such as rules
with variables that fire (Zanzotto and
Moschitti, 2006)
In the following, we describe the within-pair
feature and the syntactic rules with variable
features used in the full-fledged RTE system.
As the second space of features is generally
huge, the full feature space is generally used in
kernel machines where the final kernel between
two instances p
1
= (a
1
, b
1
) and p
2
= (a
2
, b
2
) is:
K(p
1
, p
2
) = FR(p
1
, p
2
) +
+ (WTS(a
1
, b
1
) ?WTS(a
2
, b
2
) + 1)
2
where FR counts how many rules are in com-
mon between p
1
and p
2
and WTS computes a
lexical similarity between a and b. In the fol-
lowing sections we describe the nature ofWTS
and of FR
2.1 Weighted Token Similarity (WTS)
This similarity model was first defined bt Cor-
ley and Mihalcea (2005) and since then has
been used by many RTE systems. The model
extends a classical bag-of-word model to a
Weighted-Bag-of-Word (wbow) by measuring
similarity between the two sentences of the
pair at the semantic level, instead of the lexical
level.
For example, consider the pair: ?Os-
cars forgot Farrah Fawcett?, ?Farrah Fawcett
snubbed at Academy Awards?. This pair is
redundant, and, hence, should be assigned
a very high similarity. Yet, a bag-of-word
model would assign a low score, since many
words are not shared across the two sen-
tences. wbow fixes this problem by match-
ing ?Oscar?-?Academy Awards? and ?forgot?-
?snubbed? at the semantic level. To provide
these matches, wbow relies on specific word
similarity measures over WordNet (Miller,
1995), that allow synonymy and hyperonymy
matches: in our experiments we specifically
use Jiang&Conrath similarity (Jiang and Con-
rath, 1997).
2.2 Rules with Variables as Features
The above model alone is not sufficient to
capture all interesting entailment features as
the relation of entailment is not only related
to the notion of similarity between a and b.
In the tradition of RTE, an interesting feature
space is the one where each feature represents
a rule with variables, i.e. a first order rule
that is activated by the pairs if the variables
are unified. This feature space has been
introduced in (Zanzotto and Moschitti, 2006)
and shown to improve over the one above.
Each feature ?fr
1
, fr
2
? is a pair of syntactic
tree fragments augmented with variables.
The feature is active for a pair (t
1
, t
2
) if the
syntactic interpretations of t
1
and t
2
can
be unified with < fr
1
, fr
2
>. For example,
consider the following feature:
?
S
P
P
P



NP
X
VP
H
H


VBP
bought
NP
Y
,
S
P
P


NP
X
VP
H
H


VBP
owns
NP
Y
?
This feature is active for the pair (?GM bought
Opel?,?GM owns Opel?), with the variable
unification
X
= ?GM ? and
Y
= ?Opel?. On
the contrary, this feature is not active for the
pair (?GM bought Opel?,?Opel owns GM ?) as
there is no possibility of unifying the two vari-
ables.
FR(p
1
, p
2
) is a kernel function that counts
the number of common rules with variables
between p
1
and p
2
. Efficient algorithms for
the computation of the related kernel func-
tions can be found in (Moschitti and Zanzotto,
2007; Zanzotto and Dell?Arciprete, 2009; Zan-
zotto et al., 2011).
301
S(t) = {
S:booked::v
Q
NP VP
,
VP:booked::v
Z