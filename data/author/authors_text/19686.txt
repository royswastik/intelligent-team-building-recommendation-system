Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 984?995, Dublin, Ireland, August 23-29 2014.
A Hybrid Approach to Features Representation for Fine-grained Arabic
Named Entity Recognition
Fahd Alotaibi
School of Computer Science
University of Birmingham, UK
fsa081@cs.bham.ac.uk
Faculty of Computing
King Abdulaziz University, KSA
fsalotaibi@kau.edu.sa
Mark Lee
School of Computer Science
University of Birmingham, UK
m.g.lee@cs.bham.ac.uk
Abstract
Despite considerable research on the topic of Arabic Named Entity Recognition (NER), almost
all efforts focus on a traditional set of semantic classes, features and token representations. In
this work, we advance previous research in a systematic manner and devise a novel method
to represent these features, relying on a dependency-based structure to capture further evidence
within the sentence. Moreover, the work also describes an evaluation of the method involving the
capture of global features and employing the clustering of unannotated textual data. To meet this
set of goals, we conducted a series of evaluations to evaluate different aspects that demonstrate
great improvement when compared with the baseline model.
1 Introduction
Traditionally, the focus of Arabic NER has been on a very limited number of semantic classes, i.e.
PERSON, ORGANISATION and LOCATION, utilising the newswire domain such as those described
by Benajiba and Rosso (2008), Benajiba et al. (2010) and Abdul-Hamid and Darwish (2010) . This limits
higher-level applications (such as question answering) from extracting in-depth knowledge and working
on a relatively open domains.
This paper addresses the issue of a fine-grained NER of 50 classes for Arabic and presents a com-
prehensive set of experiments that evaluate innovative means of representing the features set. Thus, the
contribution of this paper falls into different categories with unique outcomes, as follows:
1. A novel approach to representing the features is used, relying on dependency representation. This
representation overcomes the drawback of current window-based representations of features.
2. The representation of global evidence involves clustering unannotated textual data, employing hier-
atical clusters (Brown et al., 1992).
3. Due to the fact that there is no comparable work to use as a comparison in the task of Arabic fine-
grained NER, a baseline model was developed, based on Conditional Random Fields (CRF), using
the best features, as established and used elsewhere in the literature.
4. Development of publically available gold-standard fine-grained NER corpora
1
from two different
genres, i.e. Newswire and Wikipedia.
Each contribution is discussed in more detail during in the remainder of this paper.
2 Arabic Fine-grained Named Entity Corpora
The majority of Arabic NER approaches are supervised, ensuring that the machine learns from an an-
notated corpus and aims to predict unseen text. This approach requires a reasonable bank of labelled
data. This section examines the availability of such an annotated dataset at the fine-grained level, and the
creation of gold-standard corpora.
This work is licensed under a Creative Commons Attribution 4.0 International Licence.
1
Available at: http://sourceforge.net/projects/arabic-named-entity-corpora/ and
Mirror at: http://fsalotaibi.kau.edu.sa/Pages-Arabic-NE-Corpora.aspx
984
2.1 Available Corpora
One of the earliest corpora publically released was ANERcorp, developed by Benajiba et al. (2007). This
is a newswire based corpus and follows the CoNLL format. It annotates into four coarse-grained classes:
PERSON, ORGANISATION, LOCATION, and MISCELLANEOUS. This dataset has been extensively
used such as in (Benajiba et al., 2008b; Benajiba et al., 2010; Abdul-Hamid and Darwish, 2010).
Among corpora applying a fine-grained level of classes are those released by the Linguistic Data
Consortium
2
(LDC). They released two multilingual NE corpora including Arabic (Mitchell et al., 2005;
Walker et al., 2006). Both corpora were used in the Automatic Content Extraction (ACE) technology
evaluation, at the coarse-grained level only. However, these corpora are governed by a costly annual
license, which prevents the researcher from accessing and utilising them. At present, we are not aware
of a study tackling fine-grained Arabic NER using this dataset.
Alotaibi and Lee (2013) released fine-grained Arabic NE corpora - WikiFANE
Selective
and
WikiFANE
Whole
. These were built automatically using the Arabic version of Wikipedia and released
under the Creative Commons Attribution-ShareAlike 3.0 Unported Licence.
3
. These corpora apply a
similar annotation taxonomy to that of the ACE corpus, but deliver increased coverage through the in-
clusion of a new class, i.e. PRODUCT, which includes Books, Movies, Sound, Hardware, Software,
Food, Drugs and Other. Moreover, the corpora divide the PERSON class into 10 fine-classes, in order
to provide wider coverage (i.e. Politician, Athlete, Businessperson, Artist, Scientist, Police, Religious,
Engineer and Group). It is notable that this taxonomy can be easily mapped into CoNLL and ACE at
either the coarse or fine-grained levels.
2.2 Creating Gold-standard Fine-grained Named Entity Corpora
Since the aim of this paper is to conduct an in-depth experiment for fine-grained Arabic NE, we manually
created gold-standard fine-grained NE corpora for Arabic, drawing on two different genres. This gives a
critical benchmark for evaluation and comparison with the automatically constructed corpus.
The first corpus is newswire-based, using the same textual data appearing in ANERcorp. The complete
corpus was re-annotated to the fine-grained level. The second corpus is drawn from the Arabic version of
Wikipedia. The selection of articles was made using a random heuristic, i.e. selecting articles discussing
a named entity and maintaining a fair level of distribution among the classes. Moreover, the amount of
textual data drawn from the Wikipedia article was restricted by avoiding such elements as lists, headings,
and captions on images and tables.
2.3 Annotation Strategy and Quality Evaluation
For both corpora, the two-level taxonomy presented by Alotaibi and Lee (2013) was applied. This con-
sists of 8 coarse-grained classes and 50 fine-grained classes. An in-house tool to facilitate the annotation
process was developed. Two independent graduate-level Arabic native speakers were engaged to anno-
tate the entire corpora. They were provided with extended instructions to guide them in the annotation
process and a number of feedback sessions were conducted in the early stages of the process to ensure
that any difficulties could be resolved.
After its completion, the quality of the annotation was evaluated by calculating the inter-
annotator agreement between both annotators. The entity F-measure was used to evaluate the inter-
annotation agreement as in (Hripcsak and Rothschild, 2005; Zhang, 2013). The corpora were named
NewsFANE
Gold
and WikiFANE
Gold
, referring to News-based, and Wikipedia-based, Fine-grained Ara-
bic Named Entity Gold corpus, respectively. Micro-averaging was used while matching exact phrases, in
order to calculate the agreement. The size and the inter-annotator agreement of NewsFANE
Gold
is 170K
of tokens and 91% while WikiFANE
Gold
is 500K of tokens and 89% .
2
https://www.ldc.upenn.edu/
3
Available at: http://www.cs.bham.ac.uk/?fsa081/resources.html
Mirror at: http://sourceforge.net/projects/arabic-named-entity-corpora/
985
Corpus Token level Phrase level
NewsFANE
Gold
10.7 6.7
WikiFANE
Gold
13.1 7.4
WikiFANE
Selective
10.8 6.4
WikiFANE
Whole
7.08 4.9
Table 1: The density of NEs on token and phrase levels
Corpus
Length
1 2 3 4 5 6 7 8
NewsFANE
Gold
58.19 30.77 8 1.73 0.82 0.21 0.2 0.04
WikiFANE
Gold
51.75 31.55 10.88 3.48 1.34 0.46 0.21 0.12
WikiFANE
Selective
48.27 37.95 10.22 2.98 0.41 0.11 0.05 0.01
WikiFANE
Whole
66.22 25.85 6.02 1.58 0.05 0.02 0.01 0.01
Table 2: The distribution of NE phrases relative to length.
3 Corpus-based Evaluation and Comparison
It is important to closely evaluate and compare different corpora. The nature of the distribution of NE
phrases is expected to differ to some extent, affecting the performance of learning the probabilistic model.
Therefore, the coverage of NE phrases related to different aspects was studied, including the distribution
of density, length and semantic classes.
3.1 The Density of NE
The density represents the coverage of NEs at the level of tokens and phrases. As can be seen in Ta-
ble 1, WikiFANE
Gold
has the greater density at both levels. This demonstrates that the Wikipedia-
based gold corpus tends to represent more NE phrases in context than that of the newswire-based.
Although WikiFANE
Gold
is 0.7% denser than NewsFANE
Gold
in the phrase level, it reveals a notable
difference (2.4%) in the token level. This indicates that WikiFANE
Gold
possess a greater variety in the
length of NE phrases than the newswire-based corpus. However, the automatically developed corpus,
WikiFANE
Selective
, has a similar density of coverage as NewsFANE
Gold
whereas the WikiFANE
Whole
demonstrates a low level of density, due to its method of compilation.
3.2 The Distribution of the Length of Named Entity Phrases
It can be seen in Table 2, NewsFANE
Gold
and WikiFANE
Whole
tend to have more single-word NE
phrases than other corpora. When it comes to the newswire corpus, this is due to differences in the way
the NE phrases are written in a newswire domain. On the other hand, the boundaries of multi-word NE
phrases are difficult to detect, in Arabic, due to the fact that the language has a complex morphology.
This is demonstrated in the Wikipedia corpora, i.e. the gold and the selective - less than half the NE
phrases in WikiFANE
Selective
are single-word, with a slightly higher rate found in WikiFANE
Gold
.
3.3 The Distribution of the Fine-grained Classes
This demonstrates the distribution of NE phrases into fine-grained classes according to their annotation.
As shown in Figure 1, the majority of classes tend (to some extent) to have a relatively harmonic dis-
tribution. In general, the newswire-based corpus tends to include more NE phrases related to politics,
government, commerce, nations and cities, whereas the automatically-built corpora score a very high
frequency on NE types such as ?Nation? and ?Population-centre?. Moreover, WikiFANE
Gold
shows wide
distribution on most of the fine-grained classes of ?PERSON?, ?LOCATION?, ?FACILITY?, ?VEHICLE?
and ?PRODUCT?, compared to other corpora.
986
Figure 1: Distribution of Fine-grained Classes
4 The Baseline Model for Fine-grained Arabic NER
In order to prepare the baseline model and conduct successive experiments, the dataset for each corpus
was divided into training, development and test. It is important to emphasise that, due to the limitations
of computation power and the space allocated for the machine used, only a portion of WikiFANE
Selective
and WikiFANE
Whole
were selected with a size of ?500K tokens each. The following table shows each
corpus and its size.
Corpus Type Training Dev Test
NewsFANE
Gold
gold-standard 120K 25K 25K
WikiFANE
Gold
gold-standard 350K 75K 75K
WikiFANE
Selective
automatically-developed 354K 73K 73K
WikiFANE
Whole
automatically-developed 356K 72K 72K
Table 3: The size of the training, development and test for each corpus
Since there is no comparative work in the form of a fine-grained Arabic NER to use as a comparison,
a baseline model based on Conditional Random Fields (CRF) was developed. It was decided to use the
most successful features of the coarse-grained NER. For this purpose, the following features were ex-
tracted: Lexical and contextual features (current token, two tokens before and after the current token,
first and last three characters of the token, and length of the token); Morphological features (gender,
number and person); Syntactical features (part of speech and base phrase chunk); and External knowl-
edge (the presence of the token in the gazetteer developed by Alotaibi and Lee (2013)). It was decided to
use the BILOU scheme representation for the baseline model and successive experiments, as suggested
by Ratinov and Roth (2009). The performance of the baseline model is presented in Table 4.
Corpus
Development Test
P R F P R F
NewsFANE
Gold
79.58 57.87 67.01 73.07 53.34 61.67
WikiFANE
Gold
62.19 43.67 51.31 68.13 44.78 54.04
WikiFANE
Selective
89.01 68.92 77.69 88.69 60.37 71.84
WikiFANE
Whole
82.35 49.83 62.09 84.27 58.63 69.15
Table 4: The results of the baseline model by learning CRF classifier with traditional features
987
5 Dependency based Features Representation
The current representation of the sequence tagging classifier involves using a predefined window of to-
kens (e.g. with size 5, including the current token) in order to capture local evidence. This representation
has the following three drawbacks:
1. It is restricted to only capturing local evidence.
2. It fails to capture the relationship between dependent tokens, particularly for long sentences and
multiword NE phrases.
3. Since Arabic has a relatively free word order, the window-based feature representation cannot cap-
ture the order variation for different examples.
In this paper, a new approach has been devised to utilise further evidence within a sentence in the classifi-
cation process. The key idea informing this approach was to rely on the dependency-based representation
of sentences in order to extract valuable features.
The dependency structure is one of syntactical representations, where a sentence is analysed by con-
necting its words in a word-to-word relationship. These relationships specify the head and dependent
tokens in context, and assign a grammatical role for each token, e.g. subject, object and modifier.
To elaborate on the amount of knowledge that can be utilised based on the dependency structure,
consider the following sentences:
? (?A... dm yJ ??rJ yJ ?A?wO?  ?? Ty??F?  ??Am?   A  Hl? Hy?C ?A? /qAl
r?yys mjls AtHAd AlmHAkm AlAslAmy~ fy AlSwmAl ?syx ?sryf ?syx OHmd fy ...Alx/ ?The head of
the Council of the Islamic Courts Union, Sheikh Sharif Sheikh Ahmed, said in Somalia ...etc.?)
? (Cwy? ?w Ah ?A? ?t?  ?ry?  ?CA?z?  d` ?zyl??  ?FAys?  ??Cw? z?CAJ ?wq?
?A... Ay?AW?r ? CE? Hy?C /yqwl ?sArlz mwrfy AlsyAsy AlAnjlyzy b?d AlzyAr~ AlOxyr~ Alty
qAm bhA jwn myjwr r?yys wzrA? bryTAnyA ...Alx/ ?Charles Murphy, the English politician, said
after the recent visit by John Major, Britain?s prime minister ... etc.?)
? (2000 
 HWs?  ?? ?A?wOl? ?Asy?C 	t?  ?s  ?} ? r?@? /y?kr On SlAd Hsn Antxb
r?yysA?a llSwmAl fy A?sTs
?
Ab 2000/ ?It was mentioned that, Salad Hassan was elected as president
of Somalia in August 2000?)
The dependency representation and an English gloss of each example are shown in Figure 2. The parsed
output includes a new set of information, which can be utilised as features, as follows:
1. Head and Dependent Relation: The relationship between the head and the dependent is one
of the most important features to capture. Consider the token (yJ /?syx/ ?Shaikh?), in Figure 2a; the
head (Hy?C /r?yys/ ?the head of?) is located far away and cannot be captured in the local window-based
representation. Moreover, the vice versa relationship between the dependent and head is also useful.
Consider the example in Figure 2b: the token (?w /jwn/ ?John?) has two dependents (Cwy? /myjwr/
?Major?) and (Hy?C /r?yys/ ?Prime?)4 where the latter dependent (i.e. ?Hy?C?) gives a useful clue of the
way in which it has been used in political contexts. The sequence of heads or dependents can also be
utilised in the same way.
2. Sibling Relation: The sibling tokens are those dependent on the same head. Siblings can be
located near each other in context, or appear at a distance. For example: the sibling of the token (yJ
/?syx/ ?Shaikh?) is (Hl? /mjls/ ?council?), in Figure 2a, is expected to appear in a political context,
which gives a clue towards the target NE class. Meanwhile, the token (?? /fy/ ?in?) is also a sibling, and
can be avoided as it is a stop word. This is also the case in the example presented in Figure 2c, where
the token ( ?} /SlAd/ ?Salad?) is a sibling to the token (	t?  /Antxb/ ?elected?), which relates to the
political context.
4
Different contexts yield different English translation of the token ?Hy?C? as ?the head of? and ?Prime?
988
?A? Hy?C Hl?  A  ??Am?  Ty??F?  ?? ?A?wO?  yJ ??rJ yJ dm
11110010 1110000110 1110000111 111000010 1000 1101100 1111110 10100 1100100 1100100 1100100 1100100
qAl rYys mjls At.hAd Alm.hAkm AlAslAmyh fy Al.swmAl ?syx ?sryf ?syx A.hmd
said head council union courts islamic in Somalia Shaikh Sheriff Shaikh Ahmed
VRB NOM NOM NOM Al-NOM Al-NOM-p PRT-PREP NOM-PROP NOM-PROP NOM-PROP NOM-PROP NOM-PROP
O O B-Gov I-Gov I-Gov L-Gov O U-Nation B-Poli I-Poli I-Poli L-Poli
root
SBJ
IDF
IDF
IDF
MOD
MOD
OBJ
MOD
? ? ?
(a) The first example. (The second row represents the clusters according to the Brown algorithm)
?wq? z?CAJ ??Cw? ?FAys?  ?zyl??  d` ?CA?z?  ?ry?  ?t?  ?A? 
 A? ?w Cwy? Hy?C ? CE? Ay?AW?r
yqwl ?sArlz mwrfy AlsyAsy AlAnjlyzy bEd AlzyArp AlAxyrp Alty qAm b hA jwn myjwr rYys wzrA? bryTAnyA
says Charles Murphy politician English after visit recent which did for it John Major prime minister Britain
VRB NOM NOM-y Al-NOM-y Al-NOM-y NOM-PREP Al-NOM-p Al-NOM-p Al-NOM-y VRB PRT NOM-PRON NOM-PROP NOM-PROP NOM NOM NOM-PROP
O B-Poli L-Poli O O O O O O O O O B-Poli L-Poli O O B-Nation
root
SBJ
IDF
MOD
MOD
MOD
IDF
MOD
MOD
MOD
MOD
OBJ
SBJ
?
MOD
IDF
IDF
(b) The second example
r?@? ?  ?} ?s 	t?  ?Asy?C ? ?A?wO?  ?? HWs?  
 2000
mentioned that Salad Hasan elected president for Somalia in August August 2000
y*kr An SlAd Hsn Antxb rYysA l AlSwmAl fy AgsTs Ab 2000
mentioned that Salad Hasan elected president for Somalia in August August 2000
VRB PRT-An NOM NOM-PROP VRB-PASS NOM PRT-l NOM-PROP PRT-PREP NOM-PROP NOM-PROP NUM-NOM
O O B-Politician L-Politician O O B-Nation L-Nation O O O O
root
SBJ
SBJ
MOD
MOD
MOD
MOD
OBJ
MOD
OBJ
?
MOD
(c) The third example
Figure 2: The examples of a dependency structure. The rows show the Arabic token, Buckwalter translit-
eration, English gloss, POS and NE tag, respectively (the sentence is displayed left to right).
3. Syntactic Roles: The syntactical roles also benefit by being utilised to capture NE phrases in
context. Among those with concern for NER are:
a. SBJ and OBJ: defines which subject and object roles are assigned to the head token of the NE
phrase. For example, the tokens ( ?} /SlAd/ ?Salad?) and (z?CAJ /?sArlz/ ?Charles?) are tagged as
subjects.
b. IDF
5
: the Idafa chain is another important syntactical role, which helps to identify multiword NE
phrases. For example: the token (??Cw? /mwrfy/ ?Murphy?) is tagged as an IDF of its previous token
(z?CAJ /?sArlz/ ?Charles?), where this indicates a multiword NE phrase. This is also the case for the
example (Ty??F?  ??Am?   A  Hl? /mjls AtHAd AlmHAkm AlIslAmy~/ ?Council of the
Islamic Courts Union?) where all tokens are assigned an IDF role except the last token.
c. Flat relation (?): is a special role used by a CATiB pipeline parser for the sequence of proper
nouns. For example: NE phrases such as (dm yJ ??rJ yJ /?syx ?sryf ?syx OHmd/ ?Sheikh Sharif
Sheikh Ahmed?), in which all tokens are assigned a flat relation.
5
The naming of this abbreviation is used in CATiB to represent the syntactical role of idafa.
989
Corpus
Development Test
+|-
P R F P R F
NewsFANE
Gold
79.84 56.75 66.34 76.14 57.70 65.65 +3.98
WikiFANE
Gold
71.17 46.95 56.58 75.18 45.10 56.38 +2.34
WikiFANE
Selective
87.00 73.55 79.71 85.78 69.18 76.59 +4.75
WikiFANE
Whole
88.58 66.97 72.22 85.15 59.01 69.71 +0.56
Table 5: The results of the dependency-based features representation. (?+|-? represents the variation
compared with the previous experiment)
5.1 Dependency-based Features set
The representation of the dependency structure presents each token as a node. A particular token (T)
should have one node and only one head (H), except for the root, and zero or more dependents (D). A
token (T) can have zero or more siblings (S), where they are connected, (i.e. are dependent), to the same
head. Therefore, the following set of features has been extracted:
1. The current token T
2. POS of T
3. The presence of T in the Gazetteer
4. Syntactical role of T
5. Token of 1st, 2nd and 3rd Head H
6. Syntactical role of 1st, 2nd and 3rd H
7. POS of 1st, 2nd and 3rd H
8. Token of 1st, 2nd and 3rd Dependent D or ?NA? otherwise
9. Syntactical role of 1st, 2nd and 3rd D or ?NA? otherwise
10. POS of 1st, 2nd and 3rd D or ?NA? otherwise
11. Token of 1st, 2nd and 3rd Sibling S or ?NA? otherwise
12. Syntactical role of 1st, 2nd and 3rd S or ?NA? otherwise
13. POS of 1st, 2nd and 3rd S or ?NA? otherwise
The 1st, 2nd and 3rd ?H? represent the parent, grandparent and great grandparent heads; while the 1st,
2nd and 3rd ?S? represent the first three siblings (if any).
5.2 Evaluation
It was decided to use the CATiB pipeline tool
6
(produced by Marton et al. (2013)), to parse all corpora
and produce the set of features presented in the previous section. Since the POS tag set produced using
the CATiB pipeline tool is very limited, it was decided instead to rely on the output of the AMIRA to-
keniser and POS tagger produced by Diab (2009). The same classifier (CRF) was used, with a similar
encoding scheme. Two experiments were conducted: the first was intended to evaluate the dependency-
based representations. This was important in examining the effectiveness of the approach, compared
with the window-based representation of local evidence. This is shown in Table 5, where in all corpora
the performance of dependency-based representation alone outperforms that with window-based repre-
sentation. The recall metrics reveal improvement across corpora, suggesting that the dependency-base
representation has the ability to capture an increased number of NE phrases comparing to the traditional
window-based representation.
The second experiment is intended to evaluate the integration in the classification process of
dependency-based and window-based representations. This evaluation is expected to attain maximum
benefit from both approaches in one model. The results in Table 6 demonstrate that the classifier tends to
efficiently utilise both dependency-based and window-based representations in all corpora, apart from
WikiFANE
Whole
. The reason behind the degradation of the performance over the WikiFANE
Whole
dataset is due to the nature of the compiling of the corpus. Alotaibi and Lee (2013) state that this
version includes entire sentences from Wikipedia articles, with no further filtering, ensuring that it is
6
Not yet released to the public. We would like to thank the author for permission for its use.
990
Corpus
Development Test
+|-
P R F P R F
NewsFANE
Gold
82.08 57.77 67.81 80.21 61.58 69.68 +4.03
WikiFANE
Gold
89.31 49.11 63.37 83.34 50.48 62.88 +4.63
WikiFANE
Selective
87.03 73.29 79.57 87.31 76.17 77.81 +1.22
WikiFANE
Whole
82.44 57.91 68.03 75.88 52.45 62.03 -7.68
Table 6: The results of the hybrid approach using dependency-based and window-based features repre-
sentation
possible to have sentences including NE phrases that are mistakenly assigned to ?O? class when using an
automatic approach, as these NE phrases have no known destination in a Wikipedia article. This vari-
ety of mis-annotation is expected to propagate at this stage. It is worth noting that NewsFANE
Gold
and
WikiFANE
Gold
, as gold-standard corpora of different genres, reveal notable improvements of 4.03 and
4.63 F-measure respectively by using hybrid representation.
6 Further Exploiting of Global Evidences
Thus far, this study has examined the window-based and dependency-based representation of evidence,
in order to increase the performance of the classification process. However, there is still room for im-
provement. Both approaches focus only at the sentence level. This section will investigate the approach
to capturing global evidence. One means of achieving this is by utilising unannotated textual data, by
clustering tokens into semantic groups based on context similarity. The reasoning behind this approach
is that a NE token such as (??AW?  /AlTA?yf/ ?Taif?) (which is not seen in the training process) cannot be
correctly classified, as it contains neither window-based nor dependency-based evidence in the training
phase. Meanwhile, the token ???AW? ? is assigned to the same cluster of (?dn? /lndn/ ?London?) where
the classifier knows that ??dn?? is a location. In this way, the knowledge capacity of the classifier has
been broadened to a global level. A number of efforts have been undertaken for languages other than
Arabic that demonstrate the usefulness of injecting clustering into NLP tasks, e.g. PCFG parsing (Can-
dito and Crabb?e, 2009) and dependency parsing (Koo et al., 2008). Utilising unannotated textual data in
the supervised NER has already been variously studied with reference to English. The studies in (Turian
et al., 2009; Turian et al., 2010; Tkachenko et al., 2012; Ratinov and Roth, 2009; Miller et al., 2004)
reveal improvements when using the Brown clustering algorithm (Brown et al., 1992) to extract useful
features.
This paper focuses on extracting a useful set of features from unannotated Arabic textual data, by
relying on the Brown algorithm. We are not aware of any other study employing the Brown algorithm to
Arabic textual data and in an Arabic NER task.
6.1 Brown Clustering and NER
The Brown clustering algorithm works by maximising the mutual information of bigrams. It uses hier-
archical representation for the clusters. The hierarchal representation of the Brown clusters algorithm
allows inclusion of different semantic levels of granularity. The output from the clustering delivers valu-
able information, which can be utilised by NER. This information can be divided into three categories:
1. The cluster of tokens belongs to the named entity category. For example, (w?AkyJ /?sykA?w/
?Chicago?) and (wy?wV /Twkyw/ ?Tokyo?) belong to the same cluster, where both are NE type
?LOCATION?. In addition, (??d? /hdyl/ ?Hadeel?) and (