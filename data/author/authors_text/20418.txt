Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 76?84,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Using Minimal Recursion Semantics for Entailment Recognition
Elisabeth Lien
Department of Informatics, University of Oslo
elien@ifi.uio.no
Abstract
This paper describes work on using Mini-
mal Recursion Semantics (MRS) repre-
sentations for the task of recognising tex-
tual entailment. I use entailment data
from a SemEval-2010 shared task to de-
velop and evaluate an entailment recog-
nition heuristic. I compare my results to
the shared task winner, and discuss dif-
ferences in approaches. Finally, I run my
system with multiple MRS representations
per sentence, and show that this improves
the recognition results for positive entail-
ment sentence pairs.
1 Introduction
Since the first shared task on Recognising Text-
ual Entailment (RTE) (Dagan et al., 2005) was
organised in 2005, much research has been done
on how one can detect entailment between natural
language sentences. A range of methods within
statistical, rule based, and logical approaches have
been applied. The methods have exploited knowl-
edge on lexical relations, syntactic and semantic
knowledge, and logical representations.
In this paper, I examine the benefits and pos-
sible disadvantages of using rich semantic repre-
sentations as the basis for entailment recognition.
More specifically, I use Minimal Recursion Se-
mantics (MRS) (Copestake et al., 2005) represen-
tations as output by the English Resource Gram-
mar (ERG) (Flickinger, 2000). I want to investi-
gate how logical-form semantics compares to syn-
tactic analysis on the task of determining the en-
tailment relationship between two sentences. To
my knowledge, MRS representations have so far
not been extensively used for this task.
To this end, I revisit a SemEval shared task from
2010 that used entailment recognition as a means
to evaluate parser output. The shared task data
were constructed so as to require only syntactic
analysis to decide entailment for a sentence pair.
The MRSs should perform well on such data, as
they abstract over irrelevant syntactic variation, as
for example use of active vs. passive voice, or
meaning-preserving variation in constituent order,
and thus normalise at a highly suitable level of
?who did what to whom?. The core idea of my
approach is graph alignment over MRS represen-
tations, where successful alignment of MRS nodes
is treated as an indicator of entailment.
This work is part of an ongoing dissertation
project, where the larger goal is to look more
closely at correspondences between logical and
textual entailment, and the use of semantic repre-
sentations in entailment recognition.
Besides using MRS, one novel aspect of this
work is an investigation of using n-best lists of
parser outputs in deciding on entailment relations.
In principle, the top-ranked (i.e., most probable)
parser output should correspond to the intended
reading, but in practise this may not always be
the case. To increase robustness in our approach
to imperfect parse ranking, I generalise the sys-
tem to operate over n-best lists of MRSs. This
setup yields greatly improved system performance
and advances the state of the art on this task, i.e.,
makes my system retroactively the top performer
in this specific competition.
The rest of this paper is organised as follows: in
section 2, I describe the task of recognising text-
ual entailment. I also briefly describe MRS rep-
resentations, and mention previous work on RTE
using MRS. In section 3, I analyse the shared task
data, and implement an entailment decision com-
ponent which takes as input MRS representations
from the ERG. I then analyse the errors that the
component makes. Finally, I compare my results
to the actual winner of the 2010 shared task. In
section 4, I generalise my approach to 10-best lists
of MRSs.
76
2 Background
In the following, I briefly review the task of recog-
nising entailment between natural language sen-
tences. I also show an example of an MRS rep-
resentation, and mention some previous work on
entailment recognition that has used MRSs.
2.1 Recognising Textual Entailment
Research on automated reasoning has always been
a central topic in computer science, with much fo-
cus on logical approaches. Although there had
been research on reasoning expressed in natural
language, the PASCAL Recognising Textual En-
tailment (RTE) Challenge (Dagan et al., 2005)
spurred wide interest in the problem. In the task
proposed by the RTE Challenge, a system is re-
quired to recognise whether the meaning of one
text can be inferred from the meaning of another
text. Their definition of inference, or textual en-
tailment, is based on the everyday reasoning abili-
ties of humans rather than the logical properties of
language.
The RTE Challenge evolved from the relatively
simple task of making binary decisions about sen-
tence pairs into more complex variants with many
categories and multi-sentence texts. The data sets
issued by the organisers over the years provide
valuable research material. However, they con-
tain a wide range of inference phenomena, and re-
quire both ontological and world knowledge. The
data set that I have used for the present work, the
PETE data set, focusses on syntactic phenomena,
and does not require any knowledge about the state
of the world or ontological relations.
2.2 Minimal Recursion Semantics
Minimal Recursion Semantics (MRS) (Copestake
et al., 2005) is a framework for computational se-
mantics which can be used for both parsing and
generation. MRS representations are expressive,
have a clear interface with syntax, and are suitable
for processing. MRSs can be underspecified with
regard to scope in order to allow a semantically
ambiguous sentence to be represented with a sin-
gle MRS that captures every reading. MRS is inte-
grated with the HPSG English Resource Grammar
(ERG) (Flickinger, 2000).
An MRS representation contains a multiset of
relations, called elementary predications (EPs).
An EP usually corresponds to a single lexeme, but
can also represent general grammatical features.
Each EP has a predicate symbol which, in the case
of lexical predicates, encodes information about
lemma, part-of-speech, and sense distinctions. An
EP also has a label (also called handle) attached
to it. Each EP contains a list of numbered argu-
ments: ARG0, ARG1, etc. The value of an ar-
gument can be either a scopal variable (a handle
which refers to another EP?s label) or a non-scopal
variable (events or states, or entities).
The ARG0 position of the argument list has
the EP?s distinguished variable as its value. This
variable denotes either an event or state, or a
referential or abstract entity (e
i
or x
i
, respec-
tively). Each non-quantifier EP has its unique dis-
tinguished variable.
Finally, an MRS has a set of handle constraints
which describe how the scopal arguments of the
EPs can be equated with EP labels. A constraint
h
i
=
q
h
j
denotes equality modulo quantifier in-
sertion. In addition to the indirect linking through
handle constraints, EPs are directly linked by shar-
ing the same variable as argument values. The re-
sulting MRS forms a connected graph.
In figure 2, we see an MRS for the sentence
Somebody denies there are barriers from the
PETE development data (id 4116)
1
. The topmost
relation of the MRS is deny v to, which has
two non-empty arguments: x
5
and h
10
. x
5
is the
distinguished variable of the relations some q
and person, which represent the pronoun some-
body. A handle constraint equates the senten-
tial variable h
10
with h
11
, which is the label of
be v there. This last relation has x
13
as its
sole argument, which is the distinguished variable
of udef q and barrier n to, the representa-
tion of barriers.
2.3 Previous Work on RTE using MRS
To my knowledge, MRS has not been used exten-
sively in entailment decision systems. Notable ex-
amples of approaches that use MRSs are Wotzlaw
and Coote (2013), and Bergmair (2010).
In Wotzlaw and Coote (2013), the authors
present an entailment recognition system which
combines high-coverage syntactic and semantic
text analysis with logical inference supported by
relevant background knowledge. Their system
combines deep and shallow linguistic analysis,
and transforms the results into scope-resolved
1
The event and entity variables of the EPs often have
grammatical features attached to them. I have removed these
features from the MRS for the sake of readability.
77
?h
1
,
h
4
:proper q?0:5?(ARG0 x
6
, RSTR h
5
, BODY h
7
),
h
8
:named?0:5?(ARG0 x
6
, CARG Japan),
h
2
: deny v to?6:12?(ARG0 e
3
, ARG1 x
6
, ARG2 h
10
, ARG3 i
9
),
h
11
: be v there?19:22?(ARG0 e
12
, ARG1 x
13
),
h
14
:udef q?23:37?(ARG0 x
13
, RSTR h
15
, BODY h
16
),
h
17
: real a 1?23:27?(ARG0 e
18
, ARG1 x
13
),
h
17
: barrier n to?28:37?(ARG0 x
13
, ARG1 i
19
)
{h
15
=
q
h
17
, h
10
=
q
h
11
, h
5
=
q
h
8
, h
1
=
q
h
2
} ?
Figure 1: MRS for the sentence Japan denies there are real barriers.
?h
1
,
h
4
:person?0:8?(ARG0 x
5
),
h
6
: some q?0:8?(ARG0 x
5
, RSTR h
7
, BODY h
8
),
h
2
: deny v to?9:15?(ARG0 e
3
, ARG1 x
5
, ARG2 h
10
, ARG3 i
9
),
h
11
: be v there?22:25?(ARG0 e
12
, ARG1 x
13
),
h
14
:udef q?26:35?(ARG0 x
13
, RSTR h
15
, BODY h
16
),
h
17
: barrier n to?26:35?(ARG0 x
13
, ARG1 i
18
)
{h
15
=
q
h
17
, h
10
=
q
h
11
, h
7
=
q
h
4
, h
1
=
q
h
2
} ?
Figure 2: MRS for the sentence Somebody denies there are barriers.
MRS representations. The MRSs are in turn trans-
lated into another semantic representation for-
mat, which, enriched with background knowledge,
forms the basis for logical inference.
In Bergmair (2010), we find a theory-driven ap-
proach to textual entailment that uses MRS as an
intermediate format in constructing meaning rep-
resentations. The approach is based on the as-
sumptions that the syllogism is a good approx-
imation of natural language reasoning, and that
a many-valued logic provides a better model of
natural language semantics than bivalent logics do.
MRSs are used as a step in the translation of natu-
ral language sentences into logical formulae that
are suitable for processing. Input sentences are
parsed with the ERG, and the resulting MRSs are
translated into ProtoForms, which are fully recur-
sive meaning representations that are closely re-
lated to MRSs. These ProtoForms are then decom-
posed into syllogistic premises that can be pro-
cessed by an inference engine.
3 Recognising Syntactic Entailment
using MRSs
In this section, I briefly review the SemEval-2010
shared task that used entailment decision as a
means of evaluating parsers. I then describe the
entailment system I developed for the shared task
data, and compare its results to the winner of the
original task.
3.1 The PETE Shared Task
Parser Evaluation using Textual Entailments
(PETE) was a shared task in the SemEval-
2010 Evaluation Exercises on Semantic Evalua-
tion (Yuret et al., 2010). The task involved build-
ing an entailment system that could decide entail-
ment for sentence pairs based on the output of a
parser. The organisers proposed the task as an al-
ternative way of evaluating parsers. The parser
evaluation method that currently dominates the
field, PARSEVAL (Black et al., 1991), compares
the phrase-structure bracketing of a parser?s output
with the gold annotation of a treebank. This makes
the evaluation both formalism-dependent and vul-
nerable to inconsistencies in human annotations.
The PETE shared task proposes a different eval-
uation method. Instead of comparing parser output
directly to a gold standard, one can evaluate in-
directly by examining how well the parser output
supports the task of entailment recognition. This
strategy has several advantages: the evaluation is
formalism-independent, it is easier for annotators
to agree on entailment than on syntactic categories
and bracketing, and the task targets semantically
relevant phenomena in the parser output. The data
are constructed so that syntactic analysis of the
78
sentences is sufficient to determine the entailment
relationship. No background knowledge or rea-
soning ability is required to solve the task.
It is important to note that in the context of the
PETE shared task, entailment decision is not a
goal in itself, it is just a tool for parser evaluation.
The PETE organisers created two data sets for
the task: a development set of 66 sentence pairs,
and a test set of 301 pairs. The data sets were built
by taking a selection of sentences that contain syn-
tactic dependencies that are challenging for state-
of-the-art parsers, and constructing short entail-
ments that (in the case of positive entailment pairs)
reflect these dependencies. The resulting sentence
pairs were annotated with entailment judgements
by untrained annotators, and only sentence pairs
with a high degree of inter-annotator agreement
were kept.
20 systems from 7 teams participated in the
PETE task. The best scoring system was the Cam-
bridge system (Rimell and Clark, 2010), with an
accuracy of 72.4 %.
3.2 The System
My system consists of an entailment decision
component that processes MRS representations as
output by the ERG
2
. The entailment decision com-
ponent is a Python implementation I developed af-
ter analysing the PETE development data.
The core idea is based on graph alignment,
seeking to establish equivalence relations between
components of MRS graphs. In a nutshell, if all
nodes of the MRS corresponding to the hypothesis
can be aligned with nodes of the MRS of the text,
then we will call this relation MRS inclusion, and
treat it as an indicator for entailment.
3
Further-
more, the PETE data set employs a limited range
of ?robust? generalisations in hypothesis strings,
for example replacing complex noun phrases from
the text by an underspecified pronoun like some-
body. To accomodate such variation, my graph
alignment procedure supports a number of ?ro-
bust? equivalences, for example allowing an arbi-
trarily complex sub-graph to align with the graph
fragment corresponding to expressions like some-
body. These heuristic generalisations were de-
signed in response to an in-depth analysis of the
PETE development corpus, where I made the fol-
2
I used the 1212 release of the ERG, in combination with
the PET parser (Callmeier, 2000).
3
On this view, bidirectional inclusion indicates that the
two MRS graphs are isomorphic, i.e., logically equivalent.
lowing observations for the sentences of positive
entailment pairs (I use T
sent
to mean the text sen-
tence, and H
sent
to mean the hypothesis sentence):
? H
sent
is always shorter than T
sent
.
? In some cases, H
sent
is completely included
in T
sent
.
? Mostly, H
sent
is a substructure of T
sent
with
minor changes:
? T
sent
is an active sentence, while H
sent
is passive.
? A noun phrase in T
sent
has been re-
placed by somebody, someone or some-
thing in H
sent
.
? The whole of H
sent
corresponds to a
complex noun phrase in T
sent
.
In addition, I noted that the determiner or defi-
niteness of a noun phrase often changes from text
to hypothesis without making any difference for
the entailment. I also noted that, in accordance
with the PETE design principles, the context pro-
vided by the text sentence does not influence the
entailment relationship.
In the negative entailment pairs the hypothesis
is usually a combination of elements from the text
that does not match semantically with the text.
I examined treebanked MRS representations of
the PETE development data in order to develop
an entailment recognition heuristic. I found that
by taking the EPs that have an event variable as
their distinguished variable, I would capture the
semantically most important relations in the sen-
tence (the verbs). The heuristic picks out all EPs
whose ARG0 is an event variable from both the
text and hypothesis MRSs?let us call them event
relations. Then it tries to match all the event re-
lations of the hypothesis to event relations in the
text. In the following, T
mrs
means the MRS for
the text sentence, and H
mrs
the MRS for the hy-
pothesis. We say that two event relations match
if:
1. they are the same or similar relations. Two
event relations are the same or similar if they
share the same predicate symbol, or if their
predicate symbols contain the same lemma
and part-of-speech.
2. and all their arguments match. Two argu-
ments in the same argument position match
if:
79
? they are the same relation; or
? the argument in T
mrs
represents a noun
phrase and the argument in H
mrs
is
somebody/someone/something; or
? the argument in T
mrs
is either a scopal
relation or a conjunction relation, and
the argument in the hypothesis is an ar-
gument of this relation; or
? the argument in H
mrs
is not expressed.
Let us see how the heuristic works for the fol-
lowing sentence pair (PETE id 4116):
4116 T
sent
: The U.S. wants the removal of what
it perceives as barriers to investment; Japan
denies there are real barriers.
4116 H
sent
: Somebody denies there are barriers.
Figure 2 shows the MRS for 4116 H
sent
. Fig-
ure 1 shows an MRS for the part of 4116 T
sent
that entails 4116 H
sent
: Japan denies there are
real barriers. The heuristic picks out two rela-
tions in 4116 H
mrs
that have an event variable
as their distinguished variable: deny v to and
be v there. It then tries to find a match for
these relations in the set of event relations in
4116 T
mrs
:
? The relation deny v to also appears in
4116 T
mrs
, and all its argument variables can
be unified since their relations match accord-
ing to the heuristic:
? x
5
unifies with x
6
, since some q and
person (which represent somebody)
match proper q and named (which
represent Japan
4
)
? h
10
unifies with h
10
, since they both (via
the handle constraints) lead to the rela-
tion be v there.
? The variables i
9
and i
9
both represent
unexpressed arguments, and so are triv-
ially unified.
? The relation be v there matches the cor-
responding relation in 4116 T
mrs
, since their
single argument x
13
denotes the same rela-
tions: udef q and barrier n to.
4
According to the heuristic, any proper name matches the
pronoun somebody, so we do not have to consider the actual
proper name involved.
This strategy enables us to capture all the core
relations of the hypothesis. When examining the
data one can see that, contrary to the design prin-
ciples for the PETE data, some sentence pairs do
require reasoning. The heuristic will fail to cap-
ture such pairs.
The ERG is a precision grammar and does not
output analyses for sentences that are ungrammat-
ical. Some of the sentences in the PETE data sets
are arguably in a grammatical gray zone, and con-
sequently the ERG will not give us MRS represen-
tations for such sentences. In some cases, errors in
an MRS can also cause the MRS processing in the
system to fail. Therefore, my system must have
a fallback strategy for sentence pairs were MRSs
are lacking or processing fails. The system answer
NO in such cases, since it has no evidence for an
entailment relationship.
For the development process I used both tree-
banked and 1-best MRSs.
3.3 Error analysis
Tables 1 and 2 show the entailment decision re-
sults for 1-best MRSs for the PETE development
and test data. The ERG parsed 61 of the 66 pairs
in the development set, and 285 of the 301 pairs in
the test set. The five development set pairs that did
not get a parse were all negative entailments pairs.
Of the 16 test pairs that failed to parse, 10 were
negative entailment pairs. The system?s fallback
strategy labels these as NO.
gold YES: 38 gold NO: 28
sys YES 25 2
sys NO 13 26
Table 1: The results for 1-best MRSs for the PETE
development data.
gold YES: 156 gold NO: 145
sys YES 78 10
sys NO 78 135
Table 2: The results for 1-best MRSs for the PETE
test data.
The implementation of the heuristic is fine-
grained in its treatment of the transformations
from text to hypothesis that I found in the PETE
development sentences. Although I tried to antici-
pate possible variations in the test data set, it in-
evitably contained cases that were not covered by
80
the code. This meant that occasionally the system
was not able to recognise an entailment.
However, most of the incorrect judgements
were caused either by errors in the MRSs, or by
features of the MRSs or the PETE sentence pairs
that are outside the scope of my heuristic:
1. Recognising the entailment depends on infor-
mation about coreferring expressions, which
is not part of the MRS analyses.
2. The entailment (or non-entailment) relation-
ship depends on something other than syntac-
tic structure. Recognising the entailment re-
quires background knowledge and reasoning.
This means the entailment is really outside
the stated scope of the PETE task.
3. For some of the PETE sentence pairs, the
gold annotation can be discussed. The fol-
lowing pair (PETE id 2079) is labeled NO,
but is structurally similar to sentence pairs
in the data set that are labeled YES: Also,
traders are in better shape today than in 1987
to survive selling binges. ? Binges are sur-
vived.
3.4 Results and Comparison to Shared Task
Winner
At this point, we are ready to compare the results
with the winner of the PETE shared task. Of the 20
systems that took part in the shared task, the best
scoring participant was the Cambridge system, de-
veloped by Laura Rimell and Stephen Clark of
the University of Cambridge (Rimell and Clark,
2010). Their system had an overall accuracy of
72.4 %. My focus here is on comparing the perfor-
mance of the entailment systems, not the parsers.
The Cambridge system: The system consists of
a parser and an entailment system. Rimell and
Clark used the C&C parser, which can produce
output in the form of grammatical relations, that is,
labelled head-dependencies. They used the parser
with the Stanford Dependency scheme (de Marn-
effe et al., 2006), which defines a hierarchy of 48
grammatical relations.
The Cambridge entailment system was based on
the assumption that the hypothesis is a simplified
version of the text. In order to decide entailment,
one can then compare the grammatical relations?
the SDs?of the two sentences
5
. If the SDs of
the hypothesis are a subset of the SDs of the text,
then the text entails the hypothesis. However, be-
cause the hypotheses in the PETE data are often
not a direct substructure of the text, Rimell and
Clark used heuristics to deal with alterations be-
tween sentences (in the following, I use T
sd
and
H
sd
to mean the grammatical relations of text and
hypothesis sentences, respectively):
1. If a SD in the hypothesis contains a to-
ken which is not in the text, this SD is ig-
nored. This means that passive auxiliaries,
pronouns, determiners and expletive subjects
that are in H
sd
but not in T
sd
are ignored.
2. Passive subjects are equated with direct ob-
jects. This rule handles the PETE pairs where
the active verb of the text has become a pas-
sive in the hypothesis.
3. When checking whether the SDs in H
sd
are a
subset of the SDs in T
sd
, only subject and ob-
ject relations are considered (core relations).
4. The intersection of SDs in T
sd
and H
sd
has
to be non-empty (this is not restricted to sub-
jects and objects).
To sum up: if core(H
sd
)? core(T
sd
) and H
sd
?
T
sd
6= ?, then T
sent
entails H
sent
.
Results for 1-best (automatically generated)
test data: We can now compare the results from
the system for 1-best test data with those of Cam-
bridge.
In order to compare the test data results from
my system with those of Rimell & Clark, I have
to account for those sentence pairs that the ERG
could not parse (16) and the MRS pairs that my
system could not process (1). I use the same fall-
back strategy as Rimell & Clark, and let the en-
tailment decision be NO for those sentence pairs
the system cannot handle. For comparison, I also
include the results for SCHWA (University of Syd-
ney), the second highest scorer of the systems that
participated in the shared task.
From the results in table 3 we can see that my
system would have done well in the shared task.
An accuracy of 70.7 % places the system a little
5
In Rimell and Clark (2010), the authors used the abbre-
viation GR to mean the grammatical relations of the Stanford
Dependency scheme. I use SD instead, to avoid confusion
with the term GR as used by Carroll et al. (1999)
81
System A P R F1
Cambridge 72.4 79.6 62.8 70.2
My system 70.7 88.6 50.0 63.9
SCHWA 70.4 68.3 80.1 73.7
Table 3: The two top systems from the PETE
shared task (Yuret et al., 2010) compared to my
system. Accuracy (A) gives the percentage of cor-
rect answers for both YES and NO. Precision (P),
recall (R) and F1 are calculated for YES.
ahead of SCHWA, the second best system. We
also note that my system has a significantly higher
precision on the YES judgements than the other
two systems.
Resuls for gold/treebanked development data:
In order to evaluate their entailment system,
Rimell & Clark ran their system on manually an-
notated grammatical relations. Given a valid en-
tailment decision approach and correct SDs, the
system could in theory achieve 100 % accuracy.
Cambridge achieved 90.9 % accuracy on these
gold data. The authors noted that one incorrect
decision was due to a PETE pair requiring coref-
erence resolution, three errors were caused by cer-
tain transformations between text and hypothesis
that were not covered by their heuristic, and two
errors occured because the heuristic ignored some
SDs that were crucial for recognising the entail-
ments.
When I ran my system on treebanked MRSs for
the PETE development data, it achieved an accu-
racy of 92.4 %, which is slightly better than the
accuracy for Cambridge.
MRSs vs. grammatical relations: The infor-
mation that the Cambridge system uses is word
dependencies that are typed with grammatical re-
lations. More specifically, Cambridge uses subject
and object relations between words to decide en-
tailment. Because the relations are explicit?we
know exactly what type of grammatical relation
that holds between two words?it is easy to select
the relations in H
sd
that one wants to check.
The EPs of MRSs are a mixture of lexical re-
lations, and various syntactic and semantic re-
lations. A lot of the grammatical information
that is explicitly represented as SDs in the Stan-
ford scheme is implicitly represented in MRS EPs
as argument-value pairs. For example, the sub-
ject relation between he and the verb in he runs
is represented as (nsubj run he) in Stan-
ford notation. The corresponding representation
in an MRS is [ run v 1 LBL: h ARG0: e
ARG1: x ], where ARG1 denotes the proto-
agent of the verb. The assignment of semantic
roles to arguments in EPs is not affected by pas-
sivisation or dative shift, whereas such transforma-
tions can cause differences in SDs. For sentence
pairs where these phenomena occur, it is easier
to match EPs and their arguments than the corre-
sponding grammatical relations.
Cambridge heuristic vs. my heuristic: The
Cambridge system checks whether the subject and
object relations in H
sd
also appear in T
sd
. How-
ever, because their heuristic ignores tokens in the
hypothesis that are not in the text, the system in
certain cases does not check core relations that are
crucial to the entailment relationship.
My system checks whether the event relations
in H
mrs
also appear in T
mrs
, and whether their
arguments can be matched. Whereas the Cam-
bridge system ignores tokens in the hypothesis that
have no match in the text, my heuristic has ex-
plicit rules for matching arguments that are dif-
ferent. It makes my system more vulnerable to
unseen cases, but at the same time makes the pos-
itive entailment decisions more well-founded. It
leads my system to make fewer mistakes on the
NO entailments than both the Cambridge system
and SCHWA.
In their paper, Rimell & Clark do not provide
an error analysis for the PETE test set, so I can-
not do a comparative error analysis with my sys-
tem. However, they go into detail on some analy-
ses and mention some errors that the system made
on the development data (both automatically gen-
erated and gold-standard), and I can compare these
to my own results on the development data. (I will
only look at those analyses where there are signif-
icant differences between Cambridge and my sys-
tem.)
PETE id 5019: He would wake up in the mid-
dle of the night and fret about it. ? He would
wake up. The Cambridge system recognises this
correctly, but the decision is based only on the sin-
gle SD match (nsubj would he). The other
SDs are ignored, since they are non-core accord-
ing to the heuristic. In my system, the YES de-
cision is based on matching of both the relation
would v modal which has wake v up as its
scopal argument, and wake v up itself with its
82
pronoun argument.
PETE id 3081.N: Occasionally, the children
find steamed, whole-wheat grains for cereal which
they call ?buckshot?. ? Grains are steamed. The
transformation of steamed from an adjective in
T
sent
to a passive in H
sent
was not accounted for
in the Cambridge heuristic, and the system failed
to recognise the entailment. In the MRS analyses
for these sentences, steamed gets exactly the same
representation, and my entailment system can eas-
ily match the two.
The Cambridge paper mentions that two of the
errors the entailment system made were due to the
fact that a non-core relation or a pronoun in the
hypothesis, which Cambridge ignores, was crucial
for recognising an entailment. The paper does not
mention which sentences these were, but it seems
likely that they would not pose a problem to my
system.
4 Using 10-best MRSs
So far, I have used only one MRS per sentence
in the entailment decision process. The entail-
ment decisions were based on the best MRSs for a
sentence pair, either chosen manually (treebanked
MRSs) or automatically (1-best MRSs). In both
cases, it can happen that the MRS chosen for a
sentence is not actually the best interpretation, ei-
ther because of human error during treebanking,
or because the best MRS is not ranked as number
one.
I also noticed that many of the incorrect deci-
sions that the system made were caused either by
errors in the MRSs or by incompatible analyses
for a sentence pair. In both cases, the correct or
compatible MRS could possibly be found further
down the list of analyses produced by the ERG.
These shortcomings can perhaps be remedied by
examining more MRS analyses for each sentence
in a pair.
When doing n-best parsing on the PETE data
sets, we can expect a high number of analyses
for the text sentences, and fewer analyses for the
shorter hypotheses. By setting n to 10, I hope to
capture a sufficient number of the best analyses.
With 10-best parsing, I get on average 9 analyses
for the text sentences, and 3 analyses for the hy-
potheses.
I use a simple strategy for checking entailment
between a set of MRSs for the text and a set of
MRSs for the hypothesis: If I can find one case
of entailment between two MRSs, then I conclude
that the text entails the hypothesis.
In table 4, I compare my previous results with
those that I get with 10-best MRSs. As we can see,
the system manages to recognise a higher number
of positive entailment pairs, but the precision goes
down a little. Using 10-best MRSs ensures that we
do not miss out on positive entailment pairs where
an incorrect MRS is ranked as number one. How-
ever, it also increases the number of spurious en-
tailments caused by MRSs whose event relations
accidentally match. Variation of n allows trad-
ing off precision and recall, and n can possibly be
tuned separately for texts and hypotheses.
When we compare 10-best entailment checking
to the PETE shared task results, we see that my
results improve substantially over the previously
highest reported performance. My system scores
about 4 accuracy points higher than the system of
Rimell & Clark, and more than 5 points for F1.
System A P R F1
One MRS 70.7 88.6 50.0 63.9
10-best 76.4 81.4 70.5 75.5
Table 4: Here I compare system results for one
MRS and 10-best MRSs. Accuracy (A) gives the
percentage of correct answers for both YES and
NO. Precision (P), recall (R) and F1 are calculated
for YES.
5 Conclusions and Future Work
In this paper, I have demonstrated how to build
an entailment system from MRS graph alignment,
combined with heuristic ?robust? generalisations.
I compared my results to the winner of the 2010
PETE shared task, the Cambridge system, which
used grammatical relations as the basis for entail-
ment decision. I performed an in-depth compar-
ison of types and structure of information rele-
vant to entailment in syntactic dependencies vs.
logical-form meaning representations. The system
achieved competitive results to the state of the art.
Results on gold-standard parser output suggests
substantially better performance in my entailment
system than the PETE shared task winner.
I also generalised the approach to using n-
best lists of parser outputs. Using 1-best out-
put makes entailment decision vulnerable to in-
correct MRS analyses being ranked as number
one. Using n-best can counterbalance this prob-
83
lem. With 10-best MRSs, a significant improve-
ment was achieved in the performance of the en-
tailment decision system. The n-best setup offers
the flexibility of trading off precision and recall.
With the 10-best MRS lists, I used a simple
strategy for entailment decision: if one MRS pair
supports a YES decision, we say that we have en-
tailment. It would be interesting to explore more
complex strategies, such as testing all the MRS
combinations for a sentence pair for a certain n,
and decide for the majority vote. One could also
make use of the conditional probabilities on parser
outputs, for instance by multiplying the probabil-
ities for each MRS pair, summing up for YES vs.
NO decisions, and setting a threshold for the final
decision.
Acknowledgments
I am grateful to my supervisors Jan Tore L?nning
and Stephan Oepen for suggesting this task, and
for their valuable advice on my work. I also ap-
preciate the thorough comments made by the three
anonymous reviewers.
References
Richard Bergmair. 2010. Monte Carlo Semantics: Ro-
bust Inference and Logical Pattern Processing with
Natural Language Text. Ph.D. thesis, University of
Cambridge.
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A Pro-
cedure for Quantitatively Comparing the Syntactic
Coverage of English Grammars. In Speech and nat-
ural language: proceedings of a workshop, held at
Pacific Grove, California, February 19-22, 1991,
page 306. Morgan Kaufman Pub.
Ulrich Callmeier. 2000. PET. A platform for ex-
perimentation with efficient HPSG processing tech-
niques. Journal of Natural Language Engineering,
6(1):99108, March.
John A. Carroll, Guido Minnen, and Ted Briscoe.
1999. Corpus annotation for parser evaluation. Pro-
ceedings of the EACL workshop on Linguistically In-
terpreted Corpora (LINC).
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal Recursion Semantics:
An Introduction. Research on Language & Compu-
tation, 3(2):281?332.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL Recognising Textual Entail-
ment Challenge. In Joaquin Qui?nonero Candela, Ido
Dagan, Bernardo Magnini, and Florence d?Alch?e
Buc, editors, MLCW, volume 3944 of Lecture Notes
in Computer Science, pages 177?190. Springer.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of LREC, Genoa, Italy.
Dan Flickinger. 2000. On building a more effcient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28.
Laura Rimell and Stephen Clark. 2010. Cam-
bridge: Parser Evaluation using Textual Entailment
by Grammatical Relation Comparison. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, ACL 2010.
Andreas Wotzlaw and Ravi Coote. 2013. A Logic-
based Approach for Recognizing Textual Entailment
Supported by Ontological Background Knowledge.
CoRR, abs/1310.4938.
Deniz Yuret, Aydin Han, and Zehra Turgut. 2010.
SemEval-2010 Task 12: Parser Evaluation using
Textual Entailments. In Proceedings of the 5th
International Workshop on Semantic Evaluation,
pages 51?56. Association for Computational Lin-
guistics.
84
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 699?703,
Dublin, Ireland, August 23-24, 2014.
UIO-Lien: Entailment Recognition using Minimal Recursion Semantics
Elisabeth Lien
Department of Informatics
University of Oslo, Norway
elien@ifi.uio.no
Milen Kouylekov
Department of Informatics
University of Oslo, Norway
milen@ifi.uio.no
Abstract
In this paper we present our participa-
tion in the Semeval 2014 task ?Evalu-
ation of compositional distributional se-
mantic models on full sentences through
semantic relatedness and textual entail-
ment?. Our results demonstrate that us-
ing generic tools for semantic analysis is a
viable option for a system that recognizes
textual entailment. The invested effort in
developing such tools allows us to build
systems for reasoning that do not require
training.
1 Introduction
Recognizing textual entailment (RTE) has been a
popular area of research in the last years. It has
appeared in a variety of evaluation campaigns as
both monolingual and multilingual tasks. A wide
variety of techniques based on different levels of
text interpretation has been used, e.g., lexical dis-
tance, dependency parsing and semantic role la-
beling (Androutsopoulos and Malakasiotis, 2010).
Our approach uses a semantic representation
formalism called Minimal Recursion Semantics
(MRS), which, to our knowledge, has not been
used extensively in entailment decision systems.
Notable examples of systems that use MRS are
Wotzlaw and Coote (2013), and Bergmair (2010).
In Wotzlaw and Coote (2013), the authors present
an entailment recognition system which combines
high-coverage syntactic and semantic text analysis
with logical inference supported by relevant back-
ground knowledge. MRS is used as an interme-
diate format in transforming the results of the lin-
guistic analysis into representations used for log-
ical reasoning. The approach in Bergmair (2010)
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
uses the syllogism as an approximation of natural
language reasoning. MRS is used as a step in the
translation of natural language sentences into logi-
cal formulae that are suitable for processing. Both
works describe approaches that can be adapted
to RTE, but no empirical evaluation is included
to demonstrate the potential of the proposed ap-
proaches.
In contrast to these approaches, our system
bases entailment decision directly on the MRS
representations. Graph alignment over MRS rep-
resentations forms the basis for entailment recog-
nition. If key nodes in the hypothesis MRS can be
aligned to nodes in the text MRS, this is treated as
an indicator of entailment.
This paper represents our first attempt to evalu-
ate a system based on logical-form semantic rep-
resentations in a RTE competition. Using a state-
of-the-art semantic analysis component, we have
created a generic rule-based system for recogniz-
ing textual entailment that obtains competitive re-
sults on a real evaluation dataset. Our approach
does not require training. We confront it with
a strong baseline provided by the EDITS system
(Kouylekov et al., 2011).
In Section 2 we describe the computational se-
mantics framework that forms the basis of our ap-
proach. Section 3 details our entailment system,
and in Section 4 we analyze our results from the
task evaluation.
2 Minimal Recursion Semantics
Minimal Recursion Semantics (MRS) (Copestake
et al., 2005) is a framework for computational se-
mantics which provides expressive representations
with a clear interface with syntax. MRS allows
underspecification of scope, in order to capture the
different readings of a sentence with a single MRS
representation. We use the MRS analyses that are
produced by the HPSG English Resource Gram-
mar (ERG) (Flickinger, 2000).
699
The core of an MRS representation is a mul-
tiset of relations, called elementary predications
(EPs). An EP represents a single lexeme, or gen-
eral grammatical features. Each EP has a predi-
cate symbol, and a label (also called handle) that
identifies the EPs position within the MRS struc-
ture. Each EP contains a list of numbered argu-
ments: ARG0, ARG1, etc., whose values are scopal
or non-scopal variables. The ARG0 value is called
the EP?s distinguished variable, and denotes an
event or state, or an entity.
Finally, an MRS has a set of handle constraints
which describe how the scopal arguments of the
EPs can be equated with EP labels. A constraint
h
i
=
q
h
j
denotes equality modulo quantifier inser-
tion. EPs are directly and indirectly linked through
handle constraints and variable sharing, and the re-
sulting MRS forms a connected graph.
In Figure 1, we see an MRS for the sentence
A woman is cutting a potato. The topmost EP,
cut v 1, has a list of three argument-value pairs:
its distinguished variable e
3
denotes an event, and
the variables x
6
and x
9
refer to the entities filling
the agent and patient roles in the verb event. x
6
and x
9
are in turn the distinguished variables of
the EPs that represent a woman and a potato, re-
spectively.
3 System Description
In the following, T
sent
and H
sent
refer to the text
and hypothesis sentence, and T
mrs
and H
mrs
to
their MRS representations.
The core of our system is a rule based compo-
nent, which bases entailment decision on graph
alignment over MRS structures. An earlier ver-
sion of the system is described in Lien (2014).
The earlier version was developed on the data set
from the SemEval-2010 shared task Parser Eval-
uation using Textual Entailment (PETE) (Yuret et
al., 2010). Using no external linguistic resources,
the system output positive entailment decisions for
sentence pairs where core nodes of the H
mrs
could
be aligned to nodes in T
mrs
according to a set of
heuristic matching rules. The system we present
in this paper extends the earlier version by adding
support for contradiction recognition, and by us-
ing lexical relations from WordNet.
For our participation in the entailment recogni-
tion task, first, we did an analysis of the SICK trial
data. In the ENTAILMENT pairs, H
sent
is a para-
phrase over the whole or part of the text sentence.
The changes from T
sent
to H
sent
can be syntactic
(e.g., active-passive conversion), lexical (e.g., syn-
onymy, hyponymy-hypernymy, multiword expres-
sions replaced by single word), or T
sent
contains
some element that does not appear in H
sent
(e.g.,
T
sent
is a conjunction and H
sent
one of its con-
juncts, a modifier in T
sent
is left out of H
sent
). In
the CONTRADICTION category, the sentences of
a pair are also basically the same or paraphrases,
and a negation or a pair of antonymous expres-
sions create the contradiction. The NEUTRAL
pairs often have a high degree of word overlap, but
H
sent
cannot be inferred from T
sent
. Our system
accounts for many of these characteristics.
The system bases its decision on the results of
two procedures: a) an event relation match which
searches for an alignment between the MRSs, and
b) a contradiction cue check. After running these
procedures, the system outputs
1. ENTAILMENT, if the event relation match-
ing procedure found an alignment, and no
contradiction cues were found,
2. CONTRADICTION, if contradiction cues
were found,
3. NEUTRAL, if neither of the above condi-
tions are met.
The event relation matching procedure extends
the one developed in Lien (2014) to account for
the greater lexical variation in the SICK data. The
procedure selects all the EPs in T
mrs
and H
mrs
that have an event variable as their ARG0?we call
them event relations. These event relations mainly
represent verbs, verb conjunctions, adjectives, and
prepositions. For each event relation H
event
in the
hypothesis the procedure tries to find a matching
relation T
event
among the text event relations. We
say that H
event
matches T
event
if:
1. they represent the same lexeme with the
same part-of-speech, or if both are verbs and
H
event
is a synonym or hypernym of T
event
,
and
2. all their arguments match. Two event rela-
tion arguments in the same argument position
match if:
? they are the same or synonymous, or the
H
event
argument is a hypernym of the
T
event
argument, or
700
?h
1
,
h
4
: a q?0:1?(ARG0 x
6
, RSTR h
7
, BODY h
5
),
h
8
: woman n 1?2:7?(ARG0 x
6
),
h
2
: cut v 1?11:18?(ARG0 e
3
, ARG1 x
6
, ARG2 x
9
),
h
10
: a q?19:20?(ARG0 x
9
, RSTR h
12
, BODY h
11
),
h
13
: potato n 1?21:28?(ARG0 x
9
)
{h
12
=
q
h
13
, h
7
=
q
h
8
, h
1
=
q
h
2
} ?
Figure 1: MRS for A woman is cutting a potato (pair 4661, SICK trial data).
? the argument in T
event
represents a noun
phrase and the argument in H
event
is an
underspecified pronoun like somebody,
or
? the argument in T
event
is either a sco-
pal relation or a conjunction relation,
and one of its arguments matches that of
H
event
, or
? the argument in H
event
is not expressed
(i.e., it matches the T
event
argument by
default)
The matching procedure does not search for
more than one alignment between the event rela-
tions of H
mrs
and T
mrs
.
The contradiction cue procedure checks
whether the MRS pairs contain relations express-
ing negation. The quantifier no q rel negates
an entity (e.g., no man), whereas neg rel
denotes sentence negation. If a negation relation
appears in one but not the other MRS, we treat
this as an indicator of CONTRADICTION.
Example: Figure 1 shows the MRS analysis of
the hypothesis in the entailment pair A woman
is slicing a potato ? A woman is cutting a
potato. There is only one event relation in H
mrs
:
cut v 1. T
mrs
is an equivalent structure with
one event relation slice v 1. Using Word-
Net, the system finds that cut v 1 is a hyper-
nym of slice v 1. Then, the system compares
the ARG1 and ARG2 values of the event relations.
The arguments match since they are the same re-
lations. There are no contradiction cues in either
of the MRSs, so the system correctly outputs EN-
TAILMENT.
If we look at the rule based component?s output
(Table 1) for the 481 of the 500 SICK trial sen-
tence pairs for which the ERG produced MRSs,
we get a picture of how well it covers the phenom-
ena in the data set:
Of the 134 ENTAILMENT pairs, 59 were para-
phrases where the variation was relatively limited
gold ENT gold CON gold NEU
sys ENT 59 0 1
sys CON 0 51 14
sys NEU 75 22 259
Table 1: Output for the system on SICK trial data.
and could be captured by looking for synonyms,
hyponyms, and treating the hypothesis as a sub-
graph of the text. The simple contradiction cue
check, which looks for negation relations, covered
51 of 73 CONTRADICTION pairs.
75 ENTAILMENT and 22 CONTRADICTION
pairs were not captured by the matching and con-
tradiction cue procedures. Almost 30% of the
ENTAILMENT pairs had word pairs whose lex-
ical relationship was not recognized using Word-
Net (e.g.: playing a guitar? strumming a guitar).
In the other pairs there were alternations between
simple and more complex noun phrases (protec-
tive gear ? gear used for protection), change of
part-of-speech from T
sent
to H
sent
for the same
meaning entities (It is raining on a walking man?
A man is walking in the rain); some pairs required
reasoning, and in some cases H
sent
contained in-
formation not present in T
sent
. In some cases, en-
tailment recognition fails because the MRS analy-
sis is not correct (e.g., misrepresentation of passive
constructions).
The contradiction cue check did not look for
antonymous words and expressions, and this ac-
counts for almost half of the missing CONTRA-
DICTION pairs. The rest contained negation,
but were misclassified either because an incorrect
MRS analysis was chosen by the parser or because
synonymous words within the scope of the nega-
tion were not recognized.
EDITS We used a backoff-system for the pairs
when the rule-based system fails to produce re-
701
System 1 2 3 4 5
Rules Only Rules Only Combined Combined Edits
Training 76.13 75.4 76.62 76.62 74.78
Test 77.0 76.35 77.12 77.14 74.79
Table 2: Submitted system accuracy on training and test set.
sults. Our choice was EDITS
1
as it provides
a strong baseline system for recognizing textual
entailment (Kouylekov et al., 2011). EDITS
(Kouylekov and Negri, 2010) is an open source
package which offers a modular, flexible, and
adaptable working environment for experimenting
with the RTE task over different datasets. The
package allows to: i) create an entailment engine
by defining its basic components; ii) train this
entailment engine over an annotated RTE corpus
to learn a model and iii) use the entailment en-
gine and the model to assign an entailment judg-
ment and a confidence score to each pair of an un-
annotated test corpus.
We used two strategies for combining the rule-
based system with EDITS: Our first strategy was
to let the rule-based system classify those sentence
pairs for which the ERG could produce MRSs, and
use EDITS for the pairs were we did not have
MRSs (or processing failed due to errors in the
MRSs) . The second strategy was to mix the out-
put from both systems when they disagree. In this
case we took the ENTAILMENT decisions from
the rule-based, and EDITS contributes with CON-
TRADICTION and NEUTRAL.
4 Analysis
We have submitted the results obtained from five
system configurations. The first four used the rule-
based system as the core. The fifth was a system
obtained by training EDITS on the training set.
We use the fifth system as a strong baseline. In
the few cases in which the rule-based system did
not produce result (2% of the test set pairs) EDITS
judgments were used in the submission. In System
1 and System 2 we have used the first combination
strategy described in the end of section 3. In Sys-
tem 4 and System 5 the entailment decisions are a
combination of the results from the rule-based sys-
tem and EDITS as described in the second strategy
in the same section. The rule-based component
in System 1 and System 3 has more fine-grained
1
http://edits.sf.net
Precision Recall F-Measure
Contradiction 0.8422 0.7264 0.78
Entailment 0.9719 0.4158 0.5825
Neutral 0.7241 0.9595 0.8254
Table 3: Performance of System 1.
negation rules so that no q rel is not treated as
a contradiction cue in different contexts (e.g., No
woman runs does not contradict A woman sings).
Table 2 shows the results for the five submitted
systems.
The results demonstrate that the rule-based sys-
tem can be used as a general system for recogniz-
ing textual entailment. It surpasses with 3 points
of accuracy EDITS, which is an established strong
baseline system. We are quite content with the re-
sults obtained as we did not use the training dataset
to create the rules, but only the trial dataset. The
combination of the two systems brings a slight im-
provement.
Overall the rule-based system is quite precise
as demonstrated in Table 3. The numbers in the
table correspond to System 1 but are comparable
to the other rule-based systems 2, 3 and 4. The
system achieves an excellent precision on the en-
tailment and contradiction relations. It is almost
always correct when assigning the entailment rela-
tion. And it also obtains a decent recall, correctly
assigning almost half of the entailment pairs. On
the contradiction relation the system also obtained
a decent result, capturing most of the negation
cases.
5 Conclusions
Using a state-of-the-art semantic analysis compo-
nent, we have created a generic rule-based sys-
tem for recognizing textual entailment that obtains
competitive results on a real evaluation dataset.
An advantage of our approach is that it does not
require training. The precision of the approach
makes it an excellent candidate for a system that
uses textual entailment as the core of an intelligent
search engine.
702
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual Entail-
ment Methods. J. Artif. Intell. Res. (JAIR), 38:135?
187.
Richard Bergmair. 2010. Monte Carlo Semantics: Ro-
bust Inference and Logical Pattern Processing with
Natural Language Text. Ph.D. thesis, University of
Cambridge.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal Recursion Semantics:
An Introduction. Research on Language & Compu-
tation, 3(2):281?332.
Dan Flickinger. 2000. On Building a More Effcient
Grammar by Exploiting Types. Natural Language
Engineering, 6(1):15?28.
Milen Kouylekov and Matteo Negri. 2010. An
Open-Source Package for Recognizing Textual En-
tailment. In 48th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2010) ,Up-
psala, Sweden, pages 42?47.
Milen Kouylekov, Yashar Mehdad, and Matteo Negri.
2011. Is it Worth Submitting this Run? Assess your
RTE System with a Good Sparring Partner. In Pro-
ceedings of the TextInfer 2011 Workshop on Textual
Entailment, Edinburgh Scotland, pages 30?34.
Elisabeth Lien. 2014. Using Minimal Recursion Se-
mantics for Entailment Recognition. In Proceed-
ings of the Student Research Workshop at the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 76?84,
Gothenburg, Sweden, April.
Andreas Wotzlaw and Ravi Coote. 2013. A Logic-
based Approach for Recognizing Textual Entailment
Supported by Ontological Background Knowledge.
CoRR, abs/1310.4938.
Deniz Yuret, Aydin Han, and Zehra Turgut. 2010.
SemEval-2010 Task 12: Parser Evaluation using
Textual Entailments. In Proceedings of the 5th
International Workshop on Semantic Evaluation,
pages 51?56.
703
