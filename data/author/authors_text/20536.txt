Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 405?413,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Predicting and Characterising User Impact on Twitter
Vasileios Lampos
1
, Nikolaos Aletras
2
, Daniel Preot?iuc-Pietro
2
and Trevor Cohn
3
1
Department of Computer Science, University College London
2
Department of Computer Science, University of Sheffield
3
Computing and Information Systems, The University of Melbourne
v.lampos@ucl.ac.uk, {n.aletras,d.preotiuc}@dcs.shef.ac.uk, trevor.cohn@gmail.com
Abstract
The open structure of online social net-
works and their uncurated nature give rise
to problems of user credibility and influ-
ence. In this paper, we address the task of
predicting the impact of Twitter users based
only on features under their direct control,
such as usage statistics and the text posted
in their tweets. We approach the problem as
regression and apply linear as well as non-
linear learning methods to predict a user
impact score, estimated by combining the
numbers of the user?s followers, followees
and listings. The experimental results point
out that a strong prediction performance is
achieved, especially for models based on
the Gaussian Processes framework. Hence,
we can interpret various modelling com-
ponents, transforming them into indirect
?suggestions? for impact boosting.
1 Introduction
Online social networks have become a wide spread
medium for information dissemination and inter-
action between millions of users (Huberman et al.,
2009; Kwak et al., 2010), turning, at the same
time, into a popular subject for interdisciplinary
research, involving domains such as Computer Sci-
ence (Sakaki et al., 2010), Health (Lampos and
Cristianini, 2012) and Psychology (Boyd et al.,
2010). Open access along with the property of struc-
tured content retrieval for publicly posted data have
brought the microblogging platform of Twitter into
the spotlight.
Vast quantities of human-generated text from
a range of themes, including opinions, news and
everyday activities, spread over a social network.
Naturally, issues arise, like user credibility (Castillo
et al., 2011) and content attractiveness (Suh et al.,
2010), and quite often trustful or appealing informa-
tion transmitters are identified by an impact assess-
ment.
1
Intuitively, it is expected that user impact
cannot be defined by a single attribute, but depends
on multiple user actions, such as posting frequency
and quality, interaction strategies, and the text or
topics of the written communications.
In this paper, we start by predicting user impact
as a statistical learning task (regression). For that
purpose, we firstly define an impact score function
for Twitter users driven by basic account proper-
ties. Afterwards, from a set of accounts, we mea-
sure several publicly available attributes, such as
the quantity of posts or interaction figures. Textual
attributes are also modelled either by word frequen-
cies or, more generally, by clusters of related words
which quantify a topic-oriented participation. The
main hypothesis being tested is whether textual
and non textual attributes encapsulate patterns that
affect the impact of an account.
To model this data, we present a method based
on nonlinear regression using Gaussian Processes,
a Bayesian non-parametric class of methods (Ras-
mussen and Williams, 2006), proven more effec-
tive in capturing the multimodal user features. The
modelling choice of excluding components that
are not under an account?s direct control (e.g. re-
ceived retweets) combined with a significant user
impact prediction performance (r = .78) enabled
us to investigate further how specific aspects of a
user?s behaviour relate to impact, by examining the
parameters of the inferred model.
Among our findings, we identify relevant fea-
tures for this task and confirm that consistent ac-
tivity and broad interaction are deciding impact
factors. Informativeness, estimated by computing
a joint user-topic entropy, contributes well to the
separation between low and high impact accounts.
Use case scenarios based on combinations of fea-
tures are also explored, leading to findings such as
that engaging about ?serious? or more ?light? topics
may not register a differentiation in impact.
1
For example, the influence assessment metric of Klout ?
http://www.klout.com.
405
2 Data
For the experimental process of this paper, we
formed a Twitter data set (D1) of more than 48 mil-
lion tweets produced by |U | = 38, 020 users geolo-
cated in the UK in the period between 14/04/2011
and 12/04/2012 (both dates included, ?t = 365
days). D1 is a temporal subset of the data set used
for modelling UK voting intentions in (Lampos et
al., 2013). Geolocation of users was carried out
by matching the location field in their profile with
UK city names on DBpedia as well as by check-
ing that the user?s timezone is set to G.M.T. (Rout
et al., 2013). The use of a common greater geo-
graphical area (UK) was essential in order to derive
a data set with language and topic homogeneity.
A distinct Twitter data set (D2) consisting of ap-
prox. 400 million tweets was formed for learning
term clusters (Section 4.2). D2 was retrieved from
Twitter?s Gardenhose stream (a 10% sample of the
entire stream) from 02/01 to 28/02/2011. D1 and
D2 were processed using TrendMiner?s pipeline
(Preot?iuc-Pietro et al., 2012).
3 User Impact Definition
On the microblogging platform of Twitter, user ?
or, in general, account ? popularity is usually quan-
tified by the raw number of followers (?
in
? 0),
i.e. other users interested in this account. Likewise,
a user can follow others, which we denote as his set
of followees (?
out
? 0). It is expected that users
with high numbers of followers are also popular
in the real world, being well-known artists, politi-
cians, brands and so on. However, non popular
entities, the majority in the social network, can also
gain a great number of followers, by exploiting,
for example, a follow-back strategy.
2
Therefore,
using solely the number of followers to quantify
impact may lead to inaccurate outcomes (Cha et al.,
2010). A natural alternative, the ratio of ?
in
/?
out
is not a reliable metric, as it is invariant to scal-
ing, i.e. it cannot differentiate accounts of the type
{?
in
, ?
out
} = {m,n} and {? ? m, ? ? n}. We
resolve this problem by squaring the number of
followers
(
?
2
in
/?
out
)
; note that the previous expres-
sion is equal to (?
in
? ?
out
)? (?
in
/?
out
) +?
in
and
thus, it incorporates the ratio as well as the differ-
ence between followers and followees.
An additional impact indicator is the number of
times an account has been listed by others (?
?
? 0).
Lists provide a way to curate content on Twitter;
thus, users included in many lists are attractors of
2
An account follows other accounts randomly expecting
that they will follow back.
?5 0 5 10 15 20 25 300
0.05
0.1
0.15
Impact Score (S)
Prob
abilit
y De
nsity
@guardian
@David_Cameron
@PaulMasonNews
@lampos
@nikaletras
@spam?
Figure 1: Histogram of the user impact scores in
our data set. The solid black line represents a gen-
eralised extreme value probability distribution fit-
ted in our data, and the dashed line denotes the
mean impact score (= 6.776). User @spam? is a
sample account with ?
in
= 10, ?
out
= 1000 and
?
?
= 0; @lampos is a very active account, whereas
@nikaletras is a regular user.
interest. Indeed, Pearson?s correlation between ?
in
and ?
?
for all the accounts in our data set is equal
to .765 (p < .001); the two metrics are correlated,
but not entirely and on those grounds, it would be
reasonable to use both for quantifying impact.
Consequently, we have chosen to represent user
impact (S) as a log function of the number of fol-
lowers, followees and listings, given by
S(?
in
, ?
out
, ?
?
) = ln
(
(?
?
+ ?) (?
in
+ ?)
2
?
out
+ ?
)
,
(1)
where ? is a smoothing constant set equal to 1 so
that the natural logarithm is always applied on a
real positive number. Figure 1 shows the impact
score distribution for all the users in our sample,
including some pointers to less or more popular
Twitter accounts. The depicted user impact scores
form the response variable in the regression models
presented in the following sections.
4 User Account Features
This section presents the features used in the user
impact prediction task. They are divided into two
categories: non-textual and text-based. All features
have the joint characteristic of being under the
user?s direct control, something essential for char-
acterising impact based on the actions of a user.
Attributes such as the number of received retweets
or @-mentions (of a user in the tweets of others)
were not considered as they are not controlled by
the account itself.
406
a1
# of tweets
a
2
proportion of retweets
a
3
proportion of non-duplicate tweets
a
4
proportion of tweets with hashtags
a
5
hashtag-tokens ratio in tweets
a
6
proportion of tweets with @-mentions
a
7
# of unique @-mentions in tweets
a
8
proportion of tweets with @-replies
a
9
links ratio in tweets
a
10
# of favourites the account made
a
11
total # of tweets (entire history)
a
12
using default profile background (binary)
a
13
using default profile image (binary)
a
14
enabled geolocation (binary)
a
15
population of account?s location
a
16
account?s location latitude
a
17
account?s location longitude
a
18
proportion of days with nonzero tweets
Table 1: Non textual attributes for a Twitter account
used in the modelling process. All attributes refer
to a set of 365 days (?t) with the exception of a
11
,
the total number of tweets in the entire history of an
account. Attributes a
i
, i ? {2? 6, 8, 9} are ratios
of a
1
, whereas attribute a
18
is a proportion of ?t.
4.1 Non textual attributes
The non-textual attributes (a) are derived either
from general user behaviour statistics or directly
from the account?s profile. Table 1 presents the 18
attributes we extracted and used in our models.
4.2 Text features
We process the text in the tweets of D1 and com-
pute daily unigram frequencies. By discarding
terms that appear less than 100 times, we form
a vocabulary of size |V | = 71, 555. We then form
a user term-frequency matrix of size |U |?|V | with
the mean term frequencies per user during the time
interval ?t. All term frequencies are normalised
with the total number of tweets posted by the user.
Apart from single word frequencies, we are also
interested in deriving a more abstract representa-
tion for each user. To achieve this, we learn word
clusters from a distinct reference corpus (D2) that
could potentially represent specific domains of
discussion (or topics). From a multitude of pro-
posed techniques, we have chosen to apply spec-
tral clustering (Shi and Malik, 2000; Ng et al.,
2002), a hard-clustering method appropriate for
high-dimensional data and non-convex clusters
(von Luxburg, 2007). Spectral clustering performs
graph partitioning on the word-by-word similar-
ity matrix. In our case, tweet-term similarity is
reflected by the Normalised Pointwise Mutual In-
formation (NPMI), an information theoretic mea-
sure indicating which words co-occur in the same
context (Bouma, 2009). We use the random walk
graph Laplacian and only keep the largest compo-
nent of the resulting graph, eliminating most stop
words in the process. The number of clusters needs
to be specified in advance and each cluster?s most
representative words are identified by the following
metric of centrality:
C
w
(c) =
?
v?c
NPMI(w, v)
|c| ? 1
, (2)
where w is the target word and c the cluster it be-
longs (|c| denotes the cluster?s size). Examples of
extracted word clusters are illustrated in Table 4.
Other techniques were also applied, such as online
LDA (Hoffman et al., 2010), but we found that
the results were not satisfactory, perhaps due to
the short message length and the foreign terms co-
occuring within a tweet. After forming the clusters
using D2, we compute a topic score (? ) for each
user-topic pair in D1, representing a normalised
user-word frequency sum per topic.
5 Methods
This section presents the various modelling ap-
proaches for the underlying inference task, the im-
pact score (S) prediction of Twitter users based on
a set of their actions.
5.1 Learning functions for regression
We formulate this problem as a regression task,
i.e. we infer a real numbered value based on a set
of observed features. As a simple baseline, we ap-
ply Ridge Regression (RR) (Hoerl and Kennard,
1970), a reguralised version of the ordinary least
squares. Most importantly, we focus on nonlinear
methods for the impact score prediction task given
the multimodality of the feature space. Recently, it
was shown by Cohn and Specia (2013) that Sup-
port Vector Machines for Regression (SVR) (Vap-
nik, 1998; Smola and Sch?olkopf, 2004), commonly
considered the state-of-the-art for NLP regression
tasks, can be outperformed by Gaussian Processes
(GPs), a kernelised, probabilistic approach to learn-
ing (Rasmussen and Williams, 2006). Their setting
is close to ours, in that they had few (17) features
and were also aiming to predict a complex con-
tinuous phenomenon (human post-editing time).
The initial stages of our experimental process con-
firmed that GPs performed better than SVR; thus,
407
we based our modelling around them, including
RR for comparison.
In GP regression, for the inputs x ? R
d
we want
to learn a function f : R
d
? R that is drawn from
a GP prior
f(x) ? GP
(
m(x), k(x,x
?
)
)
, (3)
where m(x) and k(x,x
?
) denote the mean (set to
0 in our experiments) and covariance (or kernel)
functions respectively. The GP kernel function rep-
resents the covariance between pairs of input. We
wish to limit f to smooth functions over the inputs,
with different smoothness in each input dimension,
assuming that some features are more useful than
others. This can be accommodated by a squared ex-
ponential covariance function with Automatic Rele-
vance Determination (ARD) (Neal, 1996; Williams
and Rasmussen, 1996):
k
ard
(x,x
?
) = ?
2
exp
[
d
?
i
?
(x
i
? x
?
i
)
2
2`
2
i
]
, (4)
where ?
2
denotes the overall variance and `
i
is
the length-scale parameter for feature x
i
; all hy-
perparameters are learned from data during model
inference. Parameter `
i
is inversely proportional to
the feature?s relevancy in the model, i.e. high val-
ues of `
i
indicate a low degree of relevance for the
corresponding x
i
. By setting `
i
= ` in Eq. 4, we
learn a common length-scale for all the dimensions
? this is known as the isotropic squared exponen-
tial function (k
iso
) since it is based purely on the
difference |x ? x
?
|. k
iso
is a preferred choice when
the dimensionality of the input space is high. Hav-
ing set our covariance functions, predictions are
conducted using Bayesian integration
P(y
?
|x
?
,O) =
?
f
P(y
?
|x
?
, f)P(f |O), (5)
where y
?
is the response variable,O a labelled train-
ing set and x
?
the current observation. We learn the
hyperparameters of the model by maximising the
log marginal likelihood P(y|O) using gradient as-
cent. However, inference becomes intractable when
many training instances (n) are present as the num-
ber of computations needed is O(n
3
) (Qui?nonero-
Candela and Rasmussen, 2005). Since our training
samples are tens of thousands, we apply a sparse
approximation method (FITC), which bases param-
eter learning on a few inducing points in the train-
ing set (Qui?nonero-Candela and Rasmussen, 2005;
Snelson and Ghahramani, 2006).
5.2 Models
For predicting user impact on Twitter, we develop
three regression models that build on each other.
The first and simplest one (A) uses only the non-
textual attributes as features; the performance of A
is tested using RR,
3
SVR as well as a GP model.
For SVR we used an RBF kernel (equivalent to
k
iso
), whereas for the GP we applied the following
covariance function
k(a,a
?
) = k
ard
(a,a
?
) + k
noise
(a,a
?
) + ?, (6)
where k
noise
(a,a
?
) = ?
2
? ?(a,a
?
), ? is a Kro-
necker delta function and ? is the regression bias;
this function consists of (|a| + 3) hyperparame-
ters. Note that the sum of covariance functions is
also a valid covariance function (Rasmussen and
Williams, 2006).
The second model (AW) extends model A by
adding word-frequencies as features. The 500 most
frequent terms in D1 are discarded as stop words
and we use the following 2, 000 ones (denoted by
w). Setting x = {a,w}, the covariance function
becomes
k(x,x
?
) = k
ard
(a,a
?
) + k
iso
(w,w
?
)
+ k
noise
(x,x
?
) + ?,
(7)
where we apply k
iso
on the term-frequencies due to
their high dimensionality; the number of hyperpa-
rameters is (|a|+ 5). This is an intermediate model
aiming to evaluate whether the incorporation of
text improves prediction performance.
Finally, in the third model (AC) instead of rely-
ing on the high dimensional space of single words,
we use topic-oriented collections of terms extracted
by applying spectral clustering (see Section 4.2).
By denoting the set of different clusters or topics
as ? and the entire feature space as x = {a,? }, the
covariance function now becomes
k(x,x
?
) = k
ard
(x,x
?
) + k
noise
(x,x
?
) + ?. (8)
The number of hyperparameters is equal to (|a|+
|? |+ 3) and this model is applied for |? | = 50 and
100.
6 Experiments
Here we present the experimental results for the
user impact prediction task and then investigate the
factors that can affect it.
6.1 Predictive Accuracy
We evaluated the performance of the proposed
models via 10-fold cross-validation. Results are
presented in Table 2; Root Mean Squared Error
3
Given that the representation of attributes a
16
and a
17
(latitude, longitude) is ambiguous in a linear model, they were
not included in the RR-based models.
408
Linear (RR) Nonlinear (GP)
Model r RMSE r RMSE
A .667 2.642 .759 2.298
AW .712 2.529 .768 2.263
AC, |? | = 50 .703 2.518 .774 2.234
AC, |? | = 100 .714 2.480 .780 2.210
Table 2: Average performance (RMSE and Pear-
son?s r) derived from 10-fold cross-validation for
the task of user impact score prediction.
Model Top relevant features
A a

13
, a
11
, a
7
, a
1
, a
9
, a
8
, a
18
, a
4
, a
6
, a
3
AW a
7
, a
1
, a
11
, a

13
, a
9
, a
8
, a
18
, a
4
, a
6
, a
15
AC, ? = 50 a

13
, a
11
, a
7
, ?
?
1
, a
1
, a
9
, a
8
, ?
?
2
, a
6
, ?
?
3
AC, ? = 100 a

13
, a
11
, a
7
, a
1
, a
9
, ?
1
, ?
2
, ?
3
, a
18
, a
8
Table 3: The 10 most relevant features in descend-
ing relevance order for all GP models. ?
?
i
and ?
i
denote word clusters (may vary in each model).
6
(RMSE) and Pearson?s correlation (r) between pre-
dictions and responses were used as the perfor-
mance metrics. Overall, the best performance in
terms of both RMSE (2.21 impact points) and lin-
ear correlation (r = .78, p < .001) is achieved
by the GP model (AC) that combines non-textual
attributes with a 100 topic clusters; the difference
in performance with all other models is statistically
significant.
4
The linear baseline (RR) follows the
same pattern of improvement through the differ-
ent models, but never manages to reach the perfor-
mance of the nonlinear alternative. As mentioned
previously, we have also tried SVR with an RBF
kernel for model A (parameters were optimised on
a held-out development set) and the performance
(RMSE: 2.33, r = .75, p < .001) was significantly
worse than the one achieved by the GP model.
4
Notice that when word-based features are intro-
duced in model AW, performance improves. This
was one of the motivations for including text in the
modelling, apart from the notion that the posted
content should also affect general impact. Lastly,
turning this problem from regression to classifi-
cation by creating 3 impact score pseudo-classes
based on the .25 and the .9 quantiles of the re-
sponse variable (4.3 and 11.4 impact score points
respectively) and by using the outputs of model
AC (? = 100) in each phase of the 10-fold cross-
validation, we achieve a 75.86% classification ac-
curacy.
5
4
Indicated by performing a t-test (5% significance level).
5
Similar performance scores can be estimated for different
class threshold settings.
0
100
0
100
0
100
0
100
0 10 20 300
100
0 10 20 30
L H
L H
L H
L H
L H
Tweetszinzentirezhistoryz(?11)
Uniquez@-mentionsz(?7)
Linksz(?9)
@-repliesz(?8)
Dayszwithznonzeroztweetsz(?18)
Figure 2: User impact distribution (x-axis: impact
points, y-axis: # of user accounts) for users with a
low (L) or a high (H) participation in a selection
of relevant non-textual attributes. Dot-dashed lines
denote the respective mean impact score; the red
line is the mean of the entire sample (= 6.776).
6.2 Qualitative Analysis
Given the model?s strong performance, we now
conduct a more thorough analysis to identify and
characterise the properties that affect aspects of
the user impact. GP?s length-scale parameters (`
i
)
? which are inversely proportional to feature rele-
vancy ? are used for ranking feature importance.
Note that since our data set consists of UK users,
some results may be biased towards specific cul-
tural properties.
Non-textual attributes. Table 3 lists the 10 most
relevant attributes (or topics, where applicable) as
extracted in each GP model. Ranking is determined
by the mean value of the length-scale parameter for
each feature in the 10-fold cross-validation process.
We do not show feature ranking derived from the
RR models as we focus on the models with the best
performance. Despite this, it is worth mentioning
6
Length-scales are comparable for features of the same
variance (z-scored). Binary features (denoted by ) are not
z-scored, but for comparison purposes we have rescaled their
length-scale using the feature?s variance.
409
Label ?(`)? ?(`) Cluster?s words ranked by centrality |c|
?
1
: Weather 3.73? 1.80 mph, humidity, barometer, gust, winds, hpa, temperature, kt, #weather [...] 309
?
2
: Healthcare
Finance
Housing
5.44? 1.55 nursing, nurse, rn, registered, bedroom, clinical, #news, estate, #hospital,
rent, healthcare, therapist, condo, investment, furnished, medical, #nyc,
occupational, investors, #ny, litigation, tutors, spacious, foreclosure [...]
1281
?
3
: Politics 6.07? 2.86 senate, republican, gop, police, arrested, voters, robbery, democrats, presi-
dential, elections, charged, election, charges, #religion, arrest, repeal, dems,
#christian, reform, democratic, pleads, #jesus, #atheism [...]
950
?
4
: Showbiz
Movies
TV
7.36? 2.25 damon, potter, #tvd, harry, elena, kate, portman, pattinson, hermione, jen-
nifer, kristen, stefan, robert, catholic, stewart, katherine, lois, jackson, vam-
pire, natalie, #vampirediaries, tempah, tinie, weasley, turner, rowland [...]
1943
?
5
: Commerce 7.83? 2.77 chevrolet, inventory, coupon, toyota, mileage, sedan, nissan, adde, jeep, 4x4,
2002, #coupon, enhanced, #deal, dodge, gmc, 20%, suv, 15%, 2005, 2003,
2006, coupons, discount, hatchback, purchase, #ebay, 10% [...]
608
?
6
: Twitter
Hashtags
8.22? 2.98 #teamfollowback, #500aday, #tfb, #instantfollowback, #ifollowback, #in-
stantfollow, #followback, #teamautofollow, #autofollow, #mustfollow [...]
194
?
7
: Social
Unrest
8.37? 5.52 #egypt, #tunisia, #iran, #israel, #palestine, tunisia, arab, #jan25, iran, israel,
protests, egypt, #yemen, #iranelection, israeli, #jordan, regime, yemen,
#gaza, protesters, #lebanon, #syria, egyptian, #protest, #iraq [...]
321
?
8
: Non English 8.45? 3.80 yg, nak, gw, gue, kalo, itu, aku, aja, ini, gak, klo, sih, tak, mau, buat [...] 469
?
9
: Horoscope
Gambling
9.11? 3.07 horoscope, astrology, zodiac, aries, libra, aquarius, pisces, taurus, virgo,
capricorn, horoscopes, sagitarius, comprehensive, lottery, jackpot [...]
1354
?
10
: Religion
Sports
10.29? 6.27 #jesustweeters, psalm, christ, #nhl, proverbs, unto, salvation, psalms, lord,
kjv, righteousness, niv, bible, pastor, #mlb, romans, awards, nhl [...]
1610
Table 4: The 10 most relevant topics (for model AC, |? | = 100) in the prediction of a user?s impact score
together with their most central words. The topics are ranked by their mean length-scale, ?(`), in the
10-fold cross-validation process (?(`) is the respective standard deviation).
that RR?s outputs also followed similar ranking pat-
terns, e.g. the top 5 features in model A were a
18
,
a
7
, a
3
, a
11
and a
9
. Notice that across all models,
among the strongest features are the total number
of posts either in the entire account?s history (a
11
)
or within the 365-day interval of our experiment
(a
1
) and the number of unique @-mentions (a
7
),
good indicators of user activity and user interaction
respectively. Feature a
13
is also a very good predic-
tor, but is of limited utility for modelling our data
set because very few accounts maintain the default
profile photo (0.4%). Less relevant attributes (not
shown) are the ones related to the location of a
user (a
16
, a
17
) signalling that the whereabouts of a
user may not necessarily relate to impact. Another
low relevance attribute is the number of favourites
that an account did (a
10
), something reasonable, as
those weak endorsements are not affecting the main
stream of content updates in the social network.
In Figure 2, we present the distribution of user
impact for accounts with low (left-side) and high
(right-side) participation in a selection of non-
textual attributes. Low (L) and high (H) participa-
tions are defined by selecting the 500 accounts with
lowest and highest scores for this specific attribute.
The means of (L) and (H) are compared with the
mean impact score in our sample. As anticipated,
accounts with low activity (a
11
) are likely to be
assigned impact scores far below the mean, while
very active accounts may follow a quite opposite
pattern. Avoiding mentioning (a
7
) or replying (a
8
)
to others may not affect (on average) an impact
score positively or negatively; however, accounts
that do many unique @-mentions are distributed
around a clearly higher impact score. On the other
hand, users that overdo @-replies are distributed be-
low the mean impact score. Furthermore, accounts
that post irregularly with gaps longer than a day
(a
18
) or avoid using links in their tweets (a
9
) will
probably appear in the low impact score range.
Topics. Regarding prediction accuracy (Table 2),
performance improves when topics are included.
In turn, some of the topics replace non-textual at-
tributes in the relevancy ranking (Table 3). Table 4
presents the 10 most relevant topic word-clusters
based on their mean length-scale ?(`) in the 10-
fold cross-validation process for the best perform-
ing GP model (AC, |? | = 100). We see that clusters
with their most central words representing topics
such as ?Weather?, ?Healthcare/Finance?, ?Politics?
and ?Showbiz? come up on top.
Contrary to the non-textual attributes, accounts
with low participation in a topic (for the vast major-
ity of topics) were distributed along impact score
values lower than the mean. Based on the fact that
word clusters are not small in size, this is a rational
outcome indicating that accounts with small word-
frequency sums (i.e. the ones that do not tweet
much) will more likely be users with small impact
410
0100
0 10 20 300
100
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
?1 ?2 ?3 ?4 ?5
?6 ?7 ?8 ?9 ?10
Figure 3: User impact distribution (x-axis: impact points, y-axis: # of user accounts) for accounts with a
high participation in the 10 most relevant topics. Dot-dashed lines denote mean impact scores; the red line
is the mean of the entire sample (= 6.776).
Nu
mb
er o
f A
cco
unt
s
Impact Score (S)
0 10 20 300
50
100
All
Low Entropy
High Entropy
Figure 4: User impact distribution for accounts with
high (blue) and low (dark grey) topic entropy. Lines
denote the respective mean impact scores.
scores. Hence, in Figure 3 we only show the user
impact distribution for the 500 accounts with the
top participation in each topic. Informally, this is a
way to quantify the contribution of each domain or
topic of discussion in the impact score. Notice that
the topics which ?push? users towards the highest
impact scores fall into the domains of ?Politics? (?
3
)
and ?Showbiz? (?
4
). An equally interesting observa-
tion is that engaging a lot about a specific topic will
more likely result to a higher than average impact;
the only exception is ?
8
which does not deviate
from the mean, but ?
8
rather represents the use of a
non-English language (Indonesian) and therefore,
does not form an actual topic of discussion.
To further understand how participation in the
10 most relevant topics relates to impact, we also
computed the joint user-topic entropy defined by
H(u
i
, ?) = ?
M
?
j=1
P(u
i
, ?
j
)? log
2
P(u
i
, ?
j
), (9)
where u
i
is a user and M = 10 (Shannon, 2001).
This is a measure of user pseudo-informativeness,
meaning that users with high entropy are consid-
ered as more informative (without assessing the
quality of the information). Figure 4 shows the im-
pact score distributions for the 500 accounts with
the lowest and highest entropy. Low and high en-
tropies are separated, with the former being placed
clearly below the mean user impact score and the
latter above. This pictorial assessment suggests that
a connection between informativeness and impact
may exist, at least in their extremes (their correla-
tion in the entire sample is r = .35, p < .001).
Use case scenarios. Most of the previous analysis
focused on the properties of single features. How-
ever, the user impact prediction models we learn
depend on feature combinations. For that reason,
it is of interest to investigate use case scenarios
that bring various attributes together. To reduce
notation in this paragraph, we use x
+
i
(x is ei-
ther a non-textual attribute a or a topic ? ) to ex-
press x
i
> ?(x
i
), the set of users for which the
value of feature x
i
is above the mean; equivalently
x
?
i
: x
i
< ?(x
i
). We also use ?
?
A
to express the
more complex set {?
+
A
? ?
?
j
? ... ? ?
?
z
}, an inter-
section of users that are active in one topic (?
A
),
but not very active in the rest. Figure 5 depicts the
user impact distributions for five use case scenarios.
Scenario A compares interactive to non interac-
tive users, represented by P(a
+
1
, a
+
6
, a
+
7
, a
+
8
) and
P(a
+
1
, a
?
6
, a
?
7
, a
?
8
) respectively; interactivity, de-
fined by an intersection of accounts that tweet regu-
larly, do many @-mentions and @-replies, but also
411
0 10 20 300
150
300
450
600
750
900 IA
NIA
0 10 20 300
100
200
300
400 IAIAC
0 10 20 300
100
200
300
400
500 L
NL
0 10 20 300
100
200
300
400
500 TO
TF
0 10 20 300
50
100
150
200 LT
ST
A B C D E
Figure 5: User impact distribution (x-axis: impact points, y-axis: # of user accounts) for five Twitter
use scenarios based on subsets of the most relevant attributes and topics ? IA: Interactive, IAC: Clique
Interactive, L: Using many links, TO: Topic-Overall, TF: Topic-Focused, LT: ?Light? topics, ST: ?Serious?
topics. (N) denotes negation and lines the respective mean impact scores.
mention many different users, seems to be rewarded
on average with higher impact scores. Interactive
users gain more impact than clique-interactive ac-
counts represented by P(a
+
1
, a
+
6
, a
?
7
, a
+
8
), i.e. users
who interact, but do not mention many differ-
ent accounts, possibly because they are conduct-
ing discussions with a specific circle only (sce-
nario B). The use of links when writing about
the most prevalent topics (?Politics? and ?Show-
biz?) appears to be an important impact-wise fac-
tor (scenario C); the compared probability distri-
butions in that case were P
(
a
+
1
, (?
+
3
? ?
+
4
), a
+
9
)
against P
(
a
+
1
, (?
+
3
? ?
+
4
), a
?
9
)
. Surprisingly, when
links were replaced by hashtags in the previous
distributions, a clear class separation was not
achieved. In scenario D, topic-focused accounts,
i.e. users that write about one topic consistently,
represented by P
(
a
+
1
, (?
?
2
? ?
?
3
? ?
?
4
? ?
?
7
? ?
?
10
)
)
,
have on average slightly worse impact scores when
compared to accounts tweeting about many top-
ics, P(a
+
1
, ?
+
2
, ?
+
3
, ?
+
4
, ?
+
7
, ?
+
10
). Finally, scenario
E shows thats users engaging about more ?seri-
ous? topics, P
(
a
+
1
, ?
?
4
, ?
?
5
, ?
?
9
, (?
+
3
? ?
+
7
)
)
, were
not differentiated from the ones posting about more
?light? topics, P
(
a
+
1
, (?
+
4
? ?
+
5
? ?
+
9
), ?
?
3
, ?
?
7
)
.
7 Related Work
The task of user-impact prediction based on a ma-
chine learning approach that incorporates text fea-
tures is novel, to the best of our knowledge. De-
spite this fact, our work is partly related to research
approaches for quantifying and analysing user in-
fluence in online social networks. For example,
Cha et al. (2010) compared followers, retweets
and @-mentions received as measures of influ-
ence. Bakshy et al. (2011) aggregated all posts by
each user, computed an individual-level influence
and then tried to predict it by modelling user at-
tributes (# of followers, followees, tweets and date
of joining) together with past user influence. Their
method, based on classification and regression trees
(Breiman, 1984), achieved a modest performance
(r = .34). Furthermore, Romero et al. (2011) pro-
posed an algorithm for determining user influence
and passivity based on information-forwarding ac-
tivity, and Luo et al. (2013) exploited user attributes
to predict retweet occurrences. The primary differ-
ence with all the works described above is that we
aim to predict user impact by exploiting features
under the user?s direct control. Hence, our findings
can be used as indirect insights for strategies that in-
dividual users may follow to increase their impact
score. In addition, we incorporate the actual text
posted by the users in the entire modelling process.
8 Conclusions and Future Work
We have introduced the task of user impact pre-
diction on the microblogging platform of Twitter
based on user-controlled textual and non-textual
attributes. Nonlinear methods, in particular Gaus-
sian Processes, were more suitable than linear ap-
proaches for this problem, providing a strong per-
formance (r = .78). That result motivated the anal-
ysis of specific characteristics in the inferred model
to further define and understand the elements that
affect impact. In a nutshell, activity, non clique-
oriented interactivity and engagement on a diverse
set of topics are among the most decisive impact
factors. In future work, we plan to improve various
modelling components and gain a deeper under-
standing of the derived outcomes in collaboration
with domain experts. For more general conclusions,
the consideration of different cultures and media
sources is essential.
Acknowledgments
This research was supported by EU-FP7-ICT
project n.287863 (?TrendMiner?). Lampos also ac-
knowledges the support from EPSRC IRC project
EP/K031953/1.
412
References
Eytan Bakshy, Jake M. Hofman, Winter A. Mason, and Dun-
can J. Watts. 2011. Everyone?s an influencer: quantifying
influence on Twitter. In 4th International Conference on
Web Search and Data Mining, WSDM?11, pages 65?74.
Gerlof Bouma. 2009. Normalized (pointwise) mutual in-
formation in collocation extraction. In Biennial GSCL
Conference, pages 31?40.
Danah Boyd, Scott Golder, and Gilad Lotan. 2010. Tweet,
Tweet, Retweet: Conversational Aspects of Retweeting on
Twitter. In System Sciences, HICSS?10, pages 1?10.
Leo Breiman. 1984. Classification and regression trees.
Chapman & Hall.
Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011.
Information credibility on Twitter. In 20th International
Conference on World Wide Web, WWW?11, pages 675?
684.
Meeyoung Cha, Hamed Haddadi, Fabricio Benevenuto, and
Krishna P. Gummadi. 2010. Measuring User Influence in
Twitter: The Million Follower Fallacy. In 4th International
Conference on Weblogs and Social Media, ICWSM?10,
pages 10?17.
Trevor Cohn and Lucia Specia. 2013. Modelling Annotator
Bias with Multi-task Gaussian Processes: An Application
to Machine Translation Quality Estimation. In 51st Annual
Meeting of the Association for Computational Linguistics,
ACL?13, pages 32?42.
Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge Re-
gression: Biased Estimation for Nonorthogonal Problems.
Technometrics, 12(1):55?67.
Matthew Hoffman, David Blei, and Francis Bach. 2010. On-
line Learning for Latent Dirichlet Allocation. In Advances
in Neural Information Processing Systems, NIPS?10, pages
856?864.
Bernardo A. Huberman, Daniel M. Romero, and Fang Wu.
2009. Social Networks that Matter: Twitter Under the
Microscope. First Monday, 14(1).
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is Twitter, a social network or a news
media? In 19th International Conference on World Wide
Web, WWW?10, pages 591?600.
Vasileios Lampos and Nello Cristianini. 2012. Nowcast-
ing Events from the Social Web with Statistical Learning.
ACM Transactions on Intelligent Systems and Technology,
3(4):72:1?72:22.
Vasileios Lampos, Daniel Preot?iuc-Pietro, and Trevor Cohn.
2013. A user-centric model of voting intention from Social
Media. In 51st Annual Meeting of the Association for
Computational Linguistics, ACL?13, pages 993?1003.
Zhunchen Luo, Miles Osborne, Jintao Tang, and Ting Wang.
2013. Who will retweet me?: finding retweeters in Twit-
ter. In 36th International Conference on Research and
Development in Information Retrieval, SIGIR?13, pages
869?872.
Radford M. Neal. 1996. Bayesian Learning for Neural Net-
works. Springer.
Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. 2002. On
spectral clustering: Analysis and an algorithm. In Advances
in Neural Information Processing Systems, NIPS?02, pages
849?856.
Daniel Preot?iuc-Pietro, Sina Samangooei, Trevor Cohn,
Nicholas Gibbins, and Mahesan Niranjan. 2012. Trend-
miner: An Architecture for Real Time Analysis of Social
Media Text. In 6th International Conference on Weblogs
and Social Media, ICWSM?12, pages 38?42.
Joaquin Qui?nonero-Candela and Carl E. Rasmussen. 2005.
A unifying view of sparse approximate Gaussian Process
regression. Journal of Machine Learning Research, 6:1939?
1959.
Carl E. Rasmussen and Christopher K. I. Williams. 2006.
Gaussian Processes for Machine Learning. MIT Press.
Daniel M. Romero, Wojciech Galuba, Sitaram Asur, and
Bernardo A. Huberman. 2011. Influence and Passivity
in Social Media. In Machine Learning and Knowledge
Discovery in Databases, volume 6913, pages 18?33.
Dominic Rout, Daniel Preot?iuc-Pietro, Bontcheva Kalina, and
Trevor Cohn. 2013. Where?s @wally: A classification
approach to geolocating users based on their social ties. In
24th Conference on Hypertext and Social Media, HT?13,
pages 11?20.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010.
Earthquake shakes Twitter users: real-time event detection
by social sensors. In 19th International Conference on
World Wide Web, WWW?10, pages 851?860.
Claude E. Shannon. 2001. A mathematical theory of com-
munication. SIGMOBILE Mob. Comput. Commun. Rev.,
5(1):3?55 (reprint with corrections).
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and
image segmentation. Transactions on Pattern Analysis and
Machine Intelligence, 22(8):888?905.
Alex J. Smola and Bernhard Sch?olkopf. 2004. A tutorial
on support vector regression. Statistics and Computing,
14(3):199?222.
Edward Snelson and Zoubin Ghahramani. 2006. Sparse
Gaussian Processes using Pseudo-inputs. In Advances in
Neural Information Processing Systems, NIPS?06, pages
1257?1264.
Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H. Chi.
2010. Want to be Retweeted? Large Scale Analytics on
Factors Impacting Retweet in Twitter Network. In Social
Computing, SocialCom?10, pages 177?184.
Vladimir N. Vapnik. 1998. Statistical learning theory. Wiley.
Ulrike von Luxburg. 2007. A tutorial on spectral clustering.
Statistics and computing, 17(4):395?416.
Christopher K. I. Williams and Carl E. Rasmussen. 1996.
Gaussian Processes for Regression. In Advances in Neural
Information Processing Systems, NIPS?96, pages 514?520.
413
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 13?17,
Baltimore, Maryland, USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Extracting Socioeconomic Patterns from the News: Modelling Text and
Outlet Importance Jointly
Vasileios Lampos
1
, Daniel Preot?iuc-Pietro
2
, Sina Samangooei
3
,
Douwe Gelling
2
, and Trevor Cohn
4
1
Department of Computer Science, University College London ? v.lampos@ucl.ac.uk
2
Department of Computer Science, The University of Sheffield ? {d.preotiuc,d.gelling}@shef.ac.uk
3
Electronics and Computer Science, University of Southampton ? ss@ecs.soton.ac.uk
4
Computing and Information Systems, The University of Melbourne ? t.cohn@unimelb.edu.au
Abstract
Information from news articles can be used
to study correlations between textual dis-
course and socioeconomic patterns. This
work focuses on the task of understanding
how words contained in the news as well as
the news outlets themselves may relate to
a set of indicators, such as economic senti-
ment or unemployment rates. The bilinear
nature of the applied regression model fa-
cilitates learning jointly word and outlet
importance, supervised by these indicators.
By evaluating the predictive ability of the
extracted features, we can also assess their
relevance to the target socioeconomic phe-
nomena. Therefore, our approach can be
formulated as a potential NLP tool, partic-
ularly suitable to the computational social
science community, as it can be used to in-
terpret connections between vast amounts
of textual content and measurable society-
driven factors.
1 Introduction
Vast amounts of user-generated content on the Inter-
net as well as digitised textual resources allow us to
study text in connection to real world events across
large intervals of time. Over the last decade, there
has been a shift in user news consumption starting
with a move from offline to online sources (Lin
et al., 2005); in more recent years user-generated
news have also become prominent. However, tra-
ditional news outlets continue to be a central refer-
ence point (Nah and Chung, 2012) as they still have
the advantage of being professionally authored, al-
leviating the noisy nature of citizen journalism for-
mats.
Here, we present a framework for analysing so-
cioeconomic patterns in news articles. In contrast
to prior approaches, which primarily focus on the
textual contents, our analysis shows how Machine
Learning methods can be used to gain insights into
the interplay between text in news articles, the news
outlets and socioeconomic indicators. Our experi-
ments are performed on a set of EU-related news
summaries spanning over 8 years, with the inten-
tion to study two basic economic factors: EU?s
unemployment rate and Economic Sentiment Index
(ESI) (European Commision, 1997). To determine
connections between the news, the outlets and the
indicators of interest, we formulate our learning
task as bilinear text-based regression (Lampos et
al., 2013).
Approaches to learning the correlation of news,
or text in general, with real world indicators have
been performed in both unsupervised and super-
vised settings. For example, Flaounas et al. (2010)
uncover interesting patterns in EU?s Mediasphere,
whereas Schumaker and Chen (2009) demonstrate
that news articles can predict financial indicators.
Conversely, Bentley et al. (2014) show that emo-
tions in the textual content of books reflect back
on inflation and unemployment rates during the
20th century. Recently, Social Media text has been
intensively studied as a quicker, unobtrusive and
cheaper alternative to traditional surveys. Applica-
tion areas include politics (O?Connor et al., 2010),
finance (Bollen and Mao, 2011), health (Lampos
and Cristianini, 2012; Paul and Dredze, 2011) or
psychology (De Choudhury et al., 2013; Schwartz
et al., 2013).
In this paper, we apply a modified version of a
bilinear regularised regression model (BEN) pro-
posed for the task of voting intention inference
from Twitter content (Lampos et al., 2013). The
main characteristic of BEN is the ability of mod-
elling word frequencies as well as individual user
importance in a joint optimisation task. By apply-
ing it in the context of supervised news analysis,
we are able to visualise relevant discourse to a par-
ticular socioeconomic factor, identifying relevant
words together with important outlets.
13
2 Data
We compiled a data set by crawling summaries
on news articles written in English language, pub-
lished by the Open Europe Think Tank.
1
The press
summaries are daily aggregations of news items
about the EU or member countries with a focus
on politics; the news outlets used to compile each
summary are listed below the summary?s text. The
site is updated every weekday, with the major news
being covered in a couple of paragraphs, and other
less prevalent issues being mentioned in one para-
graph to as little as one sentence. The news sum-
maries were first published on February 2006; we
collected all of them up to mid-November 2013,
creating a data set with the temporal resolution of
1913 days (or 94 months).
The text was tokenised using the NLTK li-
brary (Bird et al., 2009). News outlets with fewer
than 5 mentions were removed, resulting in a total
of 435 sources. Each summary contains on average
14 news items, with an average of 3 news sources
per item; where multiple sources were present, the
summary was assigned to all the referenced news
outlets. After removing stop words, we ended up
with 8, 413 unigrams and 19, 045 bigrams; their
daily occurrences were normalised using the total
number of news items for that day.
For the purposes of our supervised analysis, we
use the response variables of ESI and unemploy-
ment rate across the EU. The monthly time series
of these socioeconomic indicators were retrieved
from Eurostat, EU?s statistical office (see the red
lines in Fig. 1a and 1b respectively). ESI is a com-
posite indicator often seen as an early predictor for
future economic developments (Gelper and Croux,
2010). It consists of five confidence indicators with
different weights: industrial (40%), services (30%),
consumer (20%), construction (5%) and retail trade
(5%). The unemployment rate is a seasonally ad-
justed ratio of the non employed persons over the
entire EU labour force.
2
3 Models
A common approach to regression arises through
the application of generalised linear models. These
models use a feature vector inputx and aim to build
a linear function of x for predicting a response
1
http://www.openeurope.org.uk/Page/
PressSummary/en/
2
http://epp.eurostat.ec.europa.
eu/statistics_explained/index.php/
Unemployment_statistics
variable y:
f(x) = x
T
w + ? where x,w ? R
m
. (1)
The objective is to find an f , which minimises a
model-dependent loss function (e.g. sum squared
error), optionally subject to a regularisation penalty
?; `
2
-norm regularisation (ridge regression) pe-
nalises high weights (Hoerl and Kennard, 1970),
while `
1
-norm regularisation (lasso) encourages
sparse solutions (Tibshirani, 1994). Sparsity is de-
sirable for avoiding overfitting, especially when
the dimensionality m is larger than the number of
training examples n (Hastie et al., 2009). Elastic
Net formulates a combination of `
1
and `
2
-norm
regularisation defined by the objective:
{w
?
, ?
?
} =argmin
w,?
n
?
i=1
(x
T
i
?w + ? ? y
i
)
2
+ ?
EN
(w, ?) ,
(2)
where ? denotes the regularisation parameters (Zou
and Hastie, 2005); we refer to this model as LEN
(Linear Elastic Net) in the remainder of the script.
In the context of voting intention inference from
Twitter content, Lampos et al. (2013) extended
LEN to a bilinear formulation, where a set of two
vector weights are learnt: one for words (w) and
one for users (u). This was motivated by the ob-
servation that only a sparse set of users may have
predictive value. The model now becomes:
f(X) = u
T
Xw + ? , (3)
where X is a matrix of word ? users frequencies.
The bilinear optimisation objective is formulated
as:
{w
?
,u
?
, ?
?
} =argmin
w,u,?
n
?
i=1
(
u
T
X
i
w + ? ? y
i
)
2
+ ?
EN
(w, ?
1
) + ?
EN
(u, ?
2
) ,
(4)
where X
i
is the word ? user frequency matrix, and
?
1
, ?
2
are the word and user regularisation param-
eters. This can be treated as a biconvex learning
task and be solved by iterating over two convex
processes: fixingw and learning u, and vice versa
(Lampos et al., 2013). Regularised regression on
both user and word spaces allows for an automatic
selection of the most important words and users,
performing at the same time an improved noise
filtering.
14
In our experiments, news outlets and socioeco-
nomic indicators replace users and voting intention
in the previous model formulation. To ease the in-
terpretation of the outputs, we further impose a
positivity constraint on the outlet weights u, i.e.
min(u) ? 0; this makes the model more restric-
tive, but, in our case, did not affect the prediction
performance. We refer to this model as BEN (Bi-
linear Elastic Net).
4 Experiments
Both models are applied to the news summaries
data set with the aim to predict EU?s ESI and rate
of unemployment. The predictive capability of the
derived models, assessed by their respective infer-
ence performance, is used as a metric for judging
the degree of relevance between the learnt model
parameters ? word and outlet weights ? and the
response variable. A strong predictive performance
increases confidence on the soundness of those pa-
rameters.
To match input with the monthly temporal reso-
lution of the response variables, we compute the
mean monthly term frequencies for each outlet.
Evaluation is performed via a 10-fold validation,
where each fold?s training set is based on a mov-
ing window of p = 64 contiguous months, and the
test set consists of the following q = 3 months;
formally, the training and test sets for fold i are
based on months {q(i? 1) + 1, ..., q(i? 1) + p}
and {q(i? 1) + p+ 1, ..., q(i? 1) + p+ q} re-
spectively. In this way, we emulate a scenario
where we always train on past and predict future
points.
Performance results for LEN and BEN are pre-
sented in Table 1; we show the average Root Mean
Squared Error (RMSE) as well as an error rate
(RMSE over ?(y)) across folds to allow for a bet-
ter interpretation. BEN outperforms LEN in both
tasks, with a clearer improvement when predict-
ing ESI. Predictions for all folds are depicted in
Fig. 1a and 1b together with the actual values. Note
that reformulating the problem into a multi-task
learning scenario, where ESI and unemployment
are modelled jointly did not improve inference per-
formance.
The relatively small average error rates (< 8.8%)
make meaningful a further analysis of the model?s
outputs. Due to space limitations, we choose to fo-
cus on the most recent results, depicting the models
derived in the 10th fold. Following the example of
Schwartz et al. (2013), we use a word cloud visu-
ESI Unemployment
LEN 9.253 (9.89%) 0.9275 (8.75%)
BEN 8.209 (8.77%) 0.9047 (8.52%)
Table 1: 10-fold validation average RMSEs (and
error rates) for LEN and BEN on ESI and unem-
ployment rates prediction.
2007 2008 2009 2010 2011 2012 20130
50
100
 
 
actualpredictions
(a) ESI
2007 2008 2009 2010 2011 2012 20130
5
10
 
 
actualpredictions
(b) Unemployment
Figure 1: Time series of ESI and unemployment
together with BEN predictions (smoothed using a
3-point moving average).
alisation, where the font size is proportional to the
derived weights by applying BEN, flipped terms de-
note negative weights and colours are determined
by the frequency of use in the corpus (Fig. 2). Word
clouds depict the top-60 positively and negatively
weighted n-grams (120 in total) together with the
top-30 outlets; bigrams are separated by ? ?.
5 Discussion and Future Work
Our visualisations (Fig. 2) present various inter-
esting insights into the news and socioeconomic
features being explored, serving as a demonstra-
tion of the potential power of the proposed mod-
elling. Firstly, we notice that in the word cloud,
the size of a feature (BEN?s weight) is not tightly
connected with its colour (frequency in the corpus).
Also, the word clouds suggest that mostly different
terms and outlets are selected for the two indicators.
For example, ?sky.it? is predominant for ESI but
not for unemployment, while the opposite is true
for ?hedgefundsreview.com?. Some of the words
selected for ESI reflect economical issues, such as
?stimulus? and ?spending?, whereas key politicians
15
(a) ESI
(b) Unemployment
Frequency
Word
Outlet
Weight
a
a
Polarity
Yes
Yes
+
-
Figure 2: Word clouds for words and outlets visualising the outputs of BEN.
like ?david cameron? and ?berlusconi?, are major
participants in the word cloud for unemployment.
In addition, the visualisations show a strong neg-
ative relationship between unemployment and the
terms ?food?, ?russia? and ?agriculture?, but no such
relationship with respect to ESI. The disparity of
these selections is evidence for our framework?s
capability to highlight features of lesser or greater
importance to a given socioeconomic time series.
The exact interpretation of the selected words and
outlets is, perhaps, context-dependent and beyond
the scope of this work.
In this paper, we presented a framework for per-
forming a supervised analysis on news. An impor-
tant factor for this process is that the bilinear nature
of the learning function allows for a joint selection
of important words and news outlets. Prediction
performance is used as a reference point for de-
termining whether the extracted outputs (i.e. the
model?s parameters) encapsulate relevant informa-
tion regarding to the given indicator. Experiments
were conducted on a set of EU-related news sum-
maries and the supervising socioeconomic factors
were the EU-wide ESI and unemployment. BEN
outperformed the linear alternative (LEN), produc-
ing error rates below 8.8%.
The performance of our framework motivates
several extensions to be explored in future work.
Firstly, the incorporation of additional textual fea-
tures may improve predictive capability and allow
for richer interpretations of the term weights. For
example, we could extend our term vocabulary us-
ing n-grams with n > 2, POS tags of words and
entities (people, companies, places, etc.). Further-
more, multi-task learning approaches as well as
models which incorporate the regularised learning
of weights for different countries might give us fur-
ther insights into the relationship between news,
geographic location and socioeconomic indicators.
Most importantly, we plan to gain a better under-
standing of the outputs by conducting a thorough
analysis in collaboration with domain experts.
16
Acknowledgements
VL acknowledges the support from the EPSRC
IRC project EP/K031953/1. DPP, SS, DG and TC
were supported by EU-FP7-ICT project n.287863
(?TrendMiner?).
References
R. Alexander Bentley, Alberto Acerbi, Paul Ormerod,
and Vasileios Lampos. 2014. Books average previ-
ous decade of economic misery. PLoS ONE, 9(1).
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python. O?Reilly
Media.
Johan Bollen and Huina Mao. 2011. Twitter mood as a
stock market predictor. IEEE Computer, 44(10):91?
94.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013. Social media as a measurement tool
of depression in populations. In Proceedings of ACM
WebSci?13, pages 47?56.
European Commision. 1997. The joint harmonised EU
programme of business and consumer surveys. Euro-
pean economy: Reports and studies.
Ilias Flaounas, Marco Turchi, Omar Ali, Nick Fyson,
Tijl De Bie, Nick Mosdell, Justin Lewis, and Nello
Cristianini. 2010. The Structure of the EU Medias-
phere. PLoS ONE, 5(12), 12.
Sarah Gelper and Christophe Croux. 2010. On the con-
struction of the European Economic Sentiment Indi-
cator. Oxford Bulletin of Economics and Statistics,
72(1):47?62.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2009. The Elements of Statistical Learning: Data
Mining, Inference, and Prediction. Springer.
Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge
regression: biased estimation for nonorthogonal prob-
lems. Technometrics, 12:55?67.
Vasileios Lampos and Nello Cristianini. 2012. Now-
casting events from the Social Web with statistical
learning. ACM TIST, 3(4):72:1?72:22.
Vasileios Lampos, Daniel Preot?iuc-Pietro, and Trevor
Cohn. 2013. A user-centric model of voting inten-
tion from Social Media. In Proceedings of ACL?13,
pages 993?1003.
Carolyn Lin, Michael B. Salwen, Bruce Garrison, and
Paul D. Driscoll. 2005. Online news as a functional
substitute for offline news. Online news and the pub-
lic, pages 237?255.
Seungahn Nah and Deborah S. Chung. 2012. When cit-
izens meet both professional and citizen journalists:
Social trust, media credibility, and perceived journal-
istic roles among online community news readers.
Journalism, 13(6):714?730.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: linking text sentiment to
public opinion time series. In Proceedings of AAAI
ICWSM?10, pages 122?129.
Michael J. Paul and Mark Dredze. 2011. You
Are What You Tweet: Analyzing Twitter for Public
Health. In Proceedings of AAAI ICWSM?11, pages
265?272.
Robert P. Schumaker and Hsinchun Chen. 2009. Tex-
tual analysis of stock market prediction using break-
ing financial news: the AZFin text system. ACM
TOIS, 27(2):12:1?12:19.
H. Andrew Schwartz, Johannes C. Eichstaedt, Mar-
garet L. Kern, Lukasz Dziurzynski, Stephanie M. Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David Stillwell, Martin E. P. Seligman, and
Lyle H. Ungar. 2013. Personality, Gender, and
Age in the Language of Social Media: The Open-
Vocabulary Approach. PLoS ONE, 8(9).
Robert Tibshirani. 1994. Regression shrinkage and
selection via the lasso. JRSS: Series B, 58:267?288.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. JRSS: Series B,
67(2):301?320.
17
