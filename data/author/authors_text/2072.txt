Back Transliteration from Japanese to English 
Using Target English Context  
Isao Goto?, Naoto Kato??, Terumasa Ehara???, and Hideki Tanaka? 
?NHK Science and Technical 
Research Laboratories  
1-11-10 Kinuta, Setagaya,  
Tokyo, 157-8510, Japan 
goto.i-es@nhk.or.jp 
tanaka.h-ja@nhk.or.jp 
??ATR Spoken Language Trans-
lation Research Laboratories 
2-2-2 Hikaridai, Keihanna  
Science City, Kyoto, 619-0288, 
Japan 
naoto.kato@atr.jp 
???Tokyo University of  
Science, Suwa  
5000-1, Toyohira, Chino,  
Nagano, 391-0292, Japan 
eharate@rs.suwa.tus.
ac.jp 
 
Abstract 
This paper proposes a method of automatic 
back transliteration of proper nouns, in which 
a Japanese transliterated-word is restored to 
the original English word. The English words 
are created from a sequence of letters; thus 
our method can create new English words that 
are not registered in dictionaries or English 
word lists. When a katakana character is con-
verted into English letters, there are various 
candidates of alphabetic characters. To ensure 
adequate conversion, the proposed method 
uses a target English context to calculate the 
probability of an English character or string 
corresponding to a Japanese katakana charac-
ter or string. We confirmed the effectiveness 
of using the target English context by an ex-
periment of personal-name back translitera-
tion.  
1 Introduction 
In transliteration, a word in one language is con-
verted into a character string of another language 
expressing how it is pronounced. In the case of 
transliteration into Japanese, special characters 
called katakana are used to show how a word is 
pronounced. For example, a personal name and 
its transliterated word are shown below.  
Cunningham         ?????
(ka ni n ga mu)
[Transliteration]
 
Here, the italic alphabets are romanized Japanese 
katakana characters.  
New transliterated words such as personal 
names or technical terms in katakana are not al-
ways listed in dictionaries. It would be useful for 
cross-language information retrieval if these 
words could be automatically restored to the 
original English words.  
Back transliteration is the process of restoring 
transliterated words to the original English words. 
Here is a problem of back transliteration.  
?                ?????????
(English word) (ku ra cchi fi ? ru do)
[Back transliteration]
 
There are many ambiguities to restoring a 
transliterated katakana word to its original Eng-
lish word. For example, should "a" in "ku ra cchi 
fi ? ru do" be converted into the English letter of 
"a" or "u" or some other letter or string? Trying 
to resolve the ambiguity is a difficult problem, 
which means that back transliteration to the cor-
rect English word is also difficult.  
Using the pronunciation of a dictionary or lim-
iting output English words to a particular English 
word list prepared in advance can simplify the 
problem of back transliteration. However, these 
methods cannot produce a new English word that 
is not registered in a dictionary or an English 
word list. Transliterated words are mainly proper 
nouns and technical terms, and such words are 
often not registered. Thus, a back transliteration 
framework for creating new words would be very 
useful.  
A number of back transliteration methods for 
selecting English words from an English pronun-
ciation dictionary have been proposed. They in-
clude Japanese-to-English (Knight and Graehl, 
1998) 1 , Arabic-to-English (Stalls and Knight, 
                                                          
1 Their English letter-to-sound WFST does not convert Eng-
lish words that are not registered in a pronunciation diction-
ary.  
1998), and Korean-to-English (Lin and Chen, 
2002).  
There are also methods that select English 
words from an English word list, e.g., Japanese-
to-English (Fujii and Ishikawa, 2001) and Chi-
nese-to-English (Chen et al, 1998).  
Moreover, there are back transliteration meth-
ods capable of generating new words, there are 
some methods for back transliteration from Ko-
rean to English (Jeong et al, 1999; Kang and 
Choi, 2000).  
These previous works did not take the target 
English context into account for calculating the 
plausibility of matching target characters with the 
source characters.  
This paper presents a method of taking the tar-
get English context into account to generate an 
English word from a Japanese katakana word. 
Our character-based method can produce new 
English words that are not listed in the learning 
corpus.  
This paper is organized as follows. Section 2 
describes our method. Section 3 describes the 
experimental set-up and results. Section 4 dis-
cusses the performance of our method based on 
the experimental results. Section 5 concludes our 
research. 
2 Proposed Method 
2.1 Advantage of using English context 
First we explain the difficulty of back translitera-
tion without a pronunciation dictionary. Next, we 
clarify the reason for the difficulty. Finally, we 
clarify the effect using English context in back 
transliteration.  
In back transliteration, an English letter or 
string is chosen to correspond to a katakana char-
acter or string. However, this decision is difficult. 
For example, there are cases that an English letter 
"u" corresponds to "a" of katakana, and there are 
cases that the same English letter "u" does not 
correspond to the same "a" of katakana. "u" in 
Cunningham corresponds to "a" in katakana and 
"u" in Bush does not correspond to "a" in kata-
kana. It is difficult to resolve this ambiguity 
without the pronunciation registered in a diction-
ary.  
The difference in correspondence mainly 
comes from the difference of the letters around 
the English letter "u." The correspondence of an 
English letter or string to a katakana character or 
string varies depending on the surrounding char-
acters, i.e., on its English context.  
Thus, our back transliteration method uses the 
target English context to calculate the probability 
of English letters corresponding to a katakana 
character or string.  
2.2 Notation and conversion-candidate 
lattice  
We formulate the word conversion process as a 
unit conversion process for treating new words. 
Here, the unit is one or more characters that form 
a part of characters of the word.  
A katakana word, K, is expressed by equation 
2.1 with "^" and "$" added to its start and end, 
respectively.  
1
0 0 1 1...
m
mk k k k
+
+= =K  (2.1) 
0 ^k = , 1 $mk + =  (2.2) 
where jk  is the j-th character in the katakana 
word, and m is the number of characters except 
for "^" and "$" and 10
mk +  is a character string 
from 0k  to 1mk + .  
We use katakana units constructed of one or 
more katakana characters. We denote a katakana 
unit as ku. For any ku, many English units, eu, 
could be corresponded as conversion-candidates. 
The ku's and eu's are generated using a learning 
corpus in which bilingual words are separated 
into units and every ku unit is related an eu unit.  
{ }EL  denotes the lattice of all eu's correspond-
ing to ku's covering a Japanese word. Every eu is 
a node of the lattice and each node is connected 
with next nodes. { }EL  has a lattice structure start-
ing from "^" and ending at "$." Figure 1 shows an 
example of { }EL  corresponding to a katakana 
word "?????????(ki ru shu shu ta i 
n)." In the figure, each circle represents one eu. 
A character string linking individual character 
units in the paths 1 2( , ,.., )d qp p p p?  between "^" 
and "$" in { }EL  becomes a conversion candidate, 
where q is the number of paths between "^" and 
"$" in { }EL . 
We get English word candidates by joining eu's 
from "^" to "$" in { }EL . We select a certain path, 
pd, in { }EL . The number of character units 
cchi
?(ki)
chi
ci
cki
cy
k
ke
khi
ki
kie
kii
ky
qui
ch
che
chou
chu
s
sc
sch
schu
sh
?(ru) ??(shu) ??(shu) ?(ta) ?(i) ?(n)
ta
tad
tag
te
ter
tha
ti
to
tta
tu
e
hi
i
ji
y
yeh
yi
m
mon
mp
n
ne
ng
ngh
nin
nn
nne
nt
nw
t
??(tai)
l
ld
le
les
lew
ll
lle
llu
lou
lu
r
rc
rd
re
rg
roo
rou
rr
rre
rt
lu
she
shu
su
sy
sz
ch
che
chou
chu
s
sc
sch
schu
sh
she
shu
su
sy
sz
taj
tay
tey
ti
tie
ty
tye? ? ?
? ? ?
? ? ? ? ? ?
? ? ?
? ? ?
? ? ?
? ? ?
^ $
 
Figure 1: Example of lattice { }EL  of conversion candidates units. 
 
except for "^" and "$" in pd is expressed as ( )dn p . 
The character units in pd are numbered from start 
to end.  
The English word, E, resulting from the con-
version of a katakana word, K, for pd is expressed 
as follows: 
1
0 0 1 1..
m
mk k k k
+
+= =K  
( ) 1
0 0 1 ( ) 1..
d
d
n p
n p
+
+= ku = ku ku ku , (2.3) 
( ) 1
0 0 1 ( ) 1..
d
d
l p
l pe e e e
+
+= =E  
( ) 1
0 0 1 ( ) 1..
d
d
n p
n p
+
+= eu = eu eu eu , (2.4) 
0 0 0 0 ^k e= = = =ku eu ,  
1 ( ) 1 ( ) 1 ( ) 1 $d d dm l p n p n pk e+ + + += = = =ku eu ,  (2.5) 
where ej is the j-th character in the English word. 
( )dl p  is the number of characters except for "^" 
and "$" in the English word. ( ) 10 d
n p +eu  for each pd 
in { }EL  in equation 2.4 becomes the candidate 
English word. ( ) 10 d
n p +ku  in equation 2.3 shows the 
sequence of katakana units. 
2.3 Probability models using target Eng-
lish context 
To determine the corresponding English word for 
a katakana word, the following equation 2.6 must 
be calculated: 
? arg max ( | )P
E
E = E K . (2.6) 
Here, E?  represents an output result. 
To use the English context for calculating the 
matching of an English unit with a katakana unit, 
the above equation is transformed into Equation 
2.7 by using Bayes? theorem. 
? arg max ( ) ( | )P P=
E
E E K E  (2.7) 
Equation 2.7 contains a translation model in 
which an English word is a condition and kata-
kana is a result.  
The word in the translation model ( | )P K E  in 
Equation 2.7 is broken down into character units 
by using equations 2.3 and 2.4. 
{
}
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1 ( ) 1
0 0 0
? arg max ( )
( , , | )
arg max ( )
( | , , )
( | , ) ( | )
d d
n p n pd d
d d
n p n pd d
d d d
n p n p
n p n p
n p n p n p
P
P
P
P
P P
+ +
+ +
+ +
+ +
+ + +
=
?
=
?
?
? ?
? ?
E
eu ku
E
eu ku
E E
K ku eu E
E
K ku eu E
ku eu E eu E
 (2.8) 
( ) 1
0
dn p +eu  includes information of E. K is only 
affected by ( ) 1
0
dn p +ku . Thus equation 2.8 can be 
rewritten as follows:  
( ) 1 ( ) 1
0 0
( ) 1
0
( ) 1 ( ) 1 ( ) 1
0 0 0 .
? argmax ( )
( | )
( | ) ( | )
d
n p n pd d
d d d
n p
n p n p n p
P
P
P P
+ +
+
+ + +
???
???
=
?
?
? ?
E
eu ku
E E
K ku
ku eu eu E
 (2.9) 
( ) 1
0( | )
dn pP +K ku  is 1 when the string of K and 
( ) 1
0
dn p +ku  is the same, and the strings of the 
( ) 1
0
dn p +ku  of all paths in the lattice and the string 
of the K is the same. Thus, ( ) 1
0( | )
dn pP +K ku  is al-
ways 1.  
We approximate the sum of paths by selecting 
the maximum path.  
( ) 1 ( ) 1
0 0
( ) 1
0
? arg max ( ) ( | )
( | )
d d
d
n p n p
n p
P P
P
+ +
+
?
?
E
E E ku eu
eu E
 
 (2.10) 
We show an instance of each probability 
model with a concrete value as follows: 
 
( )
(^Crutchfield$)
P
P
E , 
 
( ) 1
0( | )
(^ | ^ / )
( ) ( / / / / / )
dn pP
P
ku ra cchi fi ru do ku ra cchi fi ru do
+
? ?
K ku
?????????$ ?/?/??/???/?/?/$ , 
 
( ) 1 ( ) 1
0 0( | )
(^ / | ^ / C/ru/tch/fie/l/d / $)
( / / / / / )
d dn p n pP
P
ku ra cchi fi ru do
+ +
?
ku eu
?/?/??/???/?/?/$ , 
 
( ) 1
0( | )
(^ / C/ru/tch/fie/ld / $ | ^Crutchfield$)
dn pP
P
+eu E . 
 
We broke down the language model ( )P E  in 
equation 2.10 into letters.  
( ) 1
1
1
( | )( )
dl p
j
j j a
j
P e eP
+
?
?
=
? ?E  (2.11) 
Here, a is a constant. Equation 2.11 is an (a+1)-
gram model of English letters.  
Next, we approximate the translation model 
( ) 1 ( ) 1
0 0( | )
d dn p n pP + +ku eu  and the chunking model 
( ) 1
0( | )
dn pP +eu E . For this, we use our previously 
proposed approximation technique (Goto et al, 
2003). The outline of the technique is shown as 
follows.  
( ) 1 ( ) 1
0 0( | )
d dn p n pP + +ku eu  is approximated by reduc-
ing the condition.  
( ) 1 ( ) 1
0 0
( ) 1
( ) 11
0 0
1
( | )
( | , )
d d
d
d
n p n p
n p
n pi
i
i
P
P
+ +
+
+?
=
= ?
ku eu
ku ku eu
 
( ) 1
( ) 1
( ) ( ) 1
1
( | , , )
dn p
start i
i start i b i end i
i
P e e
+
?
? +
=
? ? ku eu
 (2.12)
 
where start(i) is the first position of the i-th char-
acter unit eui, while end(i) is the last position of 
the i-th character unit eui; and b is a constant. 
Equation 2.12 takes English context ( ) 1( )
start i
start i be
?
?  and 
( ) 1end ie +  into account.  
Next, the chunking model ( ) 10( | )d
n pP +eu E  is 
transformed. All chunking patterns of ( ) 10 d
l pe +=E  
into ( ) 10 d
n p +eu  are denoted by each l(pd)+1 point 
between l(pd)+2 characters that serve or do not 
serve as delimiters. eu0 and ( ) 1dn p +eu  are deter-
mined in advance. l(pd)-1 points remain ambigu-
ous. We represent the value that is delimiter or is 
non-delimiter between ej and ej+1 by zj. We call 
the zj delimiter distinction.  { delimiternon-delimiterjz =  (2.13) 
Here, we show an example of English units us-
ing zj.  
(e1 e2 e3 e4  e5  e6 e7 e8 e9  e10 e11)
C r u t c h f i e l d
(z1  z2  z3  z4  z5  z6 z7  z8  z9  z10)
/ / / /
1  0  1  0  0  1  0  0  1  1
English:
Values of zj:
/
 
 
In this example, a delimiter of zj is represented by 
1 and a non-delimiter is represented by 0.  
The chunking model is transformed into a 
processing per character by using zj. And we re-
duce the condition.  
( ) 1
0
( ) 1 ( ) 1
0 0
( ) 1
( ) 11
0 0
1
( | )
( | )
( | , )
d
d d
d
d
n p
l p l p
l p
l pj
j
j
P
P z e
P z z e
+
? +
?
+?
=
=
= ?
eu E
 
( ) 1
1 1
1
1
( | , )
dl p
j j
j j c j c
j
P z z e
?
? +
? ? ?
=
? ?  (2.14) 
The conditional information of the English 
1j
j ce
+
?  is as many as c characters and 1 character 
before and after zj, respectively. The conditional 
information of 1 1
j
j cz
?
? ?  is as many as c+1 delimiter 
distinctions before zj.  
By using equation 2.11, 2.12, and 2.14, equa-
tion 2.10 becomes as follows:  
( ) 1
1
1
( )
( ) 1
( ) ( ) 1
1
( ) 1
1 1
1
1
? arg max ( | )
( | , , )
( | , ).
d
d
d
l p
j
j j a
j
n p
start i
i start i b i end i
i
l p
j j
j j c j c
j
P e e
P e e
P z z e
+
?
?
=
?
? +
=
?
? +
? ? ?
=
?
?
?
?
?
?
E
E
ku eu
 (2.15) 
Equation 2.15 is the equation of our back 
transliteration method.  
2.4 Beam search solution for context 
sensitive grammar 
Equation 2.15 includes context-sensitive gram-
mar. As such, it can not be carried out efficiently. 
In decoding from the head of a word to the tail, 
eend(i)+1 in equation 2.15 becomes context-
sensitive. Thus we try to get approximate results 
by using a beam search solution. To get the re-
sults, we use dynamic programming. Every node 
of eu in the lattice keeps the N-best results evalu-
ated by using a letter of eend(i)+1 that gives the 
maximum probability in the next letters. When 
the results of next node are evaluated for select-
ing the N-best, the accurate probabilities from the 
previous nodes are used.  
2.5 Learning probability models based 
on the maximum entropy method 
The probability models are learned based on the 
maximum entropy method. This makes it possi-
ble to prevent data sparseness relating to the 
model as well as to efficiently utilize many con-
ditions, such as context, simultaneously. We use 
the Gaussian Prior (Chen and Rosenfeld, 1999) 
smoothing method for the language model. We 
use one Gaussian variance. We use the value of 
the Gaussian variance that minimizes the test 
set's perplexity.  
The feature functions of the models based on 
the maximum entropy method are defined as 
combinations of letters. In addition, we use 
vowel, consonant, and semi-vowel classes for the 
translation model. We manually define the com-
binations of the letter positions such as ej and ej-1. 
The feature functions consist of the letter combi-
nations that meet the combinations of the letter 
positions and are observed at least once in the 
learning data.  
2.6 Corpus for learning 
A Japanese-English word list aligned by unit was 
used for learning the translation model and the 
chunking model and for generating the lattice of 
conversion candidates. The alignment was done 
by semi-automatically. A romanized katakana 
character usually corresponds to one or several 
English letters or strings. For example, a roman-
ized katakana character "k" usually corresponds 
to an English letter "c," "k," "ch," or "q." With 
such heuristic rules, the Japanese-English word 
corpus could be aligned by unit and the align-
ment errors were corrected manually.  
3 Experiment 
3.1 Learning data and test data 
We conducted an experiment on back translitera-
tion using English personal names. The learning 
data used in the experiment are described below. 
The Dictionary of Western Names of 80,000 
People2 was used as the source of the Japanese-
English word corpus. We chose the names in al-
phabet from A to Z and their corresponding kata-
kana. The number of distinct words was 39,830 
for English words and 39,562 for katakana words. 
The number of English-katakana pairs was 
83,0573. We related the alphabet and katakana 
character units in those words by using the 
method described in section 2.6. We then used 
the corpus to make the translation and the chunk-
ing models and to generate a lattice of conversion 
candidates. 
The learning of the language model was car-
ried out using a word list that was created by 
merging two word lists: an American personal-
                                                          
2 Published by Nichigai Associates in Japan in 1994.  
3  This corpus includes many identical English-katakana 
word pairs.  
name list4, and English head words of the Dic-
tionary of Western Names of 80,000 people. The 
American name list contains frequency informa-
tion for each name; we also used the frequency 
data for the learning of the language model. A 
test set for evaluating the value of the Gaussian 
variance was created using the American name 
list. The list was split 9:1, and we used the larger 
data for learning and the smaller data for evaluat-
ing the parameter value.  
The test data is as follows. The test data con-
tained 333 katakana name words of American 
Cabinet officials, and other high-ranking officials, 
as well as high-ranking governmental officials of 
Canada, the United Kingdom, Australia, and 
New Zealand (listed in the World Yearbook 2002 
published by Kyodo News in Japan). The English 
name words that were listed along with the corre-
sponding katakana names were used as answer 
words. Words that included characters other than 
the letters A to Z were excluded from the test 
data. Family names and First names were not 
distinguished.  
3.2 Experimental models 
We used the following methods to test the indi-
vidual effects of each factor of our method.  
? Method A 
Used a model that did not take English context 
into account. The plausibility is expressed as fol-
lows: 
( )
1
? arg max ( | )
dn p
i i
i
P
=
= ?
E
E eu ku . (3.1) 
? Method B 
Used our language model and a translation model 
that did not consider English context. The con-
stant a = 3 in the language model. The plausibil-
ity is expressed as follows: 
( ) 1 ( )
1
3
1 1
? arg max ( | ) ( | )
d dl p n p
j
j j i i
j i
P e e P
+
?
?
= =
= ? ?
E
E ku eu . 
 (3.2) 
? Method C 
Applied our chunking model to method B, with c 
= 3 in the chunking model. The plausibility is 
expressed as follows: 
                                                          
4  Prepared from the 1990 Census conducted by the U.S. 
Department of Commerce. Available at 
http://www.census.gov/genealogy/names/ . The list includes 
91,910 distinct words.  
( ) 1 ( )
1
3
1 1
? arg max ( | ) ( | )
d dl p n p
j
j j i i
j i
P e e P
+
?
?
= =
= ? ?
E
E ku eu  
( ) 1
1 4
4 3
1
( | , ).
dl p
j j
j j j
j
P z z e
?
? +
? ?
=
? ?  (3.3) 
? Method D 
Used our translation model that considered Eng-
lish context, but not the chunking model. b = 3 in 
the translation model. The plausibility is ex-
pressed as follows: 
( )
( ) 1
1
3
1
( )
( ) 1
( ) 3 ( ) 1
1
? arg max ( | )
| , , .
d
d
l p
j
j j
j
n p
start i
i start i i end i
i
P e e
P e e
+
?
?
=
?
? +
=
=
?
?
?
E
E
ku eu
 (3.4) 
? Method E 
Used our language model, translation model, and 
chunking model. The plausibility is expressed as 
follows: 
( )
( ) 1
1
3
1
( )
( ) 1
( ) 3 ( ) 1
1
( ) 1
1 4
4 3
1
? arg max ( | )
| , ,
( | , ).
d
d
d
l p
j
j j
j
n p
start i
i start i i end i
i
l p
j j
j j j
j
P e e
P e e
P z z e
+
?
?
=
?
? +
=
?
? +
? ?
=
=
?
?
?
?
?
E
E
ku eu  
 (3.5) 
3.3 Results 
Table 1 shows the results of the experiment5 on 
back transliteration from Japanese katakana to 
English. The conversion was determined to be 
successful if the generated English word agreed 
perfectly with the English word in the test data. 
Table 2 shows examples of back transliterated 
words.  
Method A B C D E 
Top 1 23.7 57.4 61.6 63.1 66.4
Top 2 34.8 69.1 72.4 71.8 74.2
Top 3 42.9 73.6 76.6 75.4 79.3
Top 5 54.1 77.5 79.9 80.8 83.5
Top 10 63.4 82.0 85.3 86.5 87.7
Table 1: Ratio (%) of including the answer word 
in high-ranking words.  
                                                          
5 For model D and E, we used N=50 for the beam search 
solution. In addition, we kept paths that represented parts of 
words existing in the learning data.  
Japanese katakana 
(romanized katakana) 
Created English 
??????? 
(a shu ku ro fu to) 
Ashcroft 
????????? 
(ki ru shu shu ta i n) 
Kirschstein 
????? 
(su pe n sa -) 
Spencer 
 
???? 
(pa u e ru) 
Powell 
 
????? 
(pu ri n shi pi) 
Principi 
 
Table 2: Example of English words produced.  
4 Discussion 
The correct-match ratio of the method E for the 
first-ranked words was 66%. Its correct-match 
ratio for words up to the 10th rank was 87%.  
Regarding the top 1 ranked words, method B 
that used a language model increase the ratio 33-
points from method A that did not use a language 
model. This demonstrates the effectiveness of the 
language model. 
Also for the top 1 ranked words, method C 
which adopted the chunking model increase the 
ratio 4-points from method B that did not adopt 
the chunking model in the top 1 ranked words. 
This indicates the effectiveness of the chunking 
model. 
Method D that used a translation model taking 
English context into account had a ratio 5-points 
higher in top 1 ranked words than that of method 
B that used a translation model not taking Eng-
lish context into account. This demonstrates the 
effectiveness of the language model.  
Method E gave the best ratio. Its ratio for the 
top 1 ranked word was 42-points higher than 
method A's.  
These results demonstrate the effectiveness of 
using English context for back transliteration.  
5 Conclusion 
This paper described a method for Japanese to 
English back transliteration. Unlike conventional 
methods, our method uses a target English con-
text to calculate the plausibility of matching be-
tween English and katakana. Our method can 
treat English words that do not exist in learning 
data. We confirmed the effectiveness of our 
method in an experiment using personal names. 
We will apply this technique to cross-language 
information retrieval.  
References  
Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding, 
and Shih-Chung Tsai. 1998. Proper Name Transla-
tion in Cross-Language Information Retrieval. 36th 
Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference 
on Computational Linguistics, pp.232-236. 
Stanley F. Chen, Ronald Rosenfeld. 1999. A Gaussian 
Prior for Smoothing Maximum Entropy Models. 
Technical Report CMU-CS-99-108, Carnegie Mel-
lon University.  
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating Names and Technical Terms in Arabic Text. 
COLING/ACL Workshop on Computational Ap-
proaches to Semitic Languages. 
Isao Goto, Naoto Kato, Noriyoshi Uratani, and Teru-
masa Ehara. 2003. Transliteration Considering 
Context Information based on the Maximum En-
tropy Method. Machine Translation Summit IX, 
pp.125-132. 
Kil Soon Jeong, Sung Hyun Myaeng, Jae Sung Lee, 
and Key-Sun Choi. 1999. Automatic Identification 
and Back-Transliteration of Foreign Words for In-
formation Retrieval. Information Processing and 
Management, Vol.35, No.4, pp.523-540. 
Byung-Ju Kang and Key-Sun Choi. 2000. Automatic 
Transliteration and Back-Transliteration by Deci-
sion Tree Learning. International Conference on 
Language Resources and Evaluation. pp.1135-1411. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics, Vol.24, 
No.4, pp.599-612. 
Wei-Hao Lin and Hsin-Hsi Chen. 2002. Backward 
Machine Transliteration by Learning Phonetic 
Similarity. 6th Conference on Natural Language 
Learning, pp.139-145.  
Atsushi Fujii and Tetsuya Ishikawa. 2001. Japa-
nese/English Cross-Language Information Re-
trieval: Exploration of Query Translation and 
Transliteration. Computers and the Humanities, 
Vol.35, No.4, pp.389-420. 
