Proceedings of the ACL 2007 Demo and Poster Sessions, pages 121?124,
Prague, June 2007. c?2007 Association for Computational Linguistics
Predicting Evidence of Understanding by Monitoring User?s Task 
Manipulation in Multimodal Conversations 
Yukiko I. Nakano? 
Yoshiko Arimoto?? 
?Tokyo University of Agri-
culture and Technology 
2-24-16 Nakacho, Koganei-
shi, Tokyo 184-8588, Japan 
{nakano, kmurata, meno-
moto}@cc.tuat.ac.jp 
Kazuyoshi Murata? 
Yasuhiro Asa??? 
??Tokyo University of 
Technology 
1404-1 Katakura, Hachioji, 
Tokyo 192-0981, Japan 
ar@mf.teu.ac.jp 
Mika Enomoto? 
Hirohiko Sagawa??? 
???Central Research Laboratory, 
Hitachi, Ltd. 
1-280, Higashi-koigakubo Kokub-
unji-shi, Tokyo 185-8601, Japan 
{yasuhiro.asa.mk, hiro-
hiko.sagawa.cu}@hitachi.com 
 
 
 
Abstract 
The aim of this paper is to develop ani-
mated agents that can control multimodal 
instruction dialogues by monitoring user?s 
behaviors. First, this paper reports on our 
Wizard-of-Oz experiments, and then, using 
the collected corpus, proposes a probabilis-
tic model of fine-grained timing dependen-
cies among multimodal communication 
behaviors: speech, gestures, and mouse 
manipulations. A preliminary evaluation 
revealed that our model can predict a in-
structor?s grounding judgment and a lis-
tener?s successful mouse manipulation 
quite accurately, suggesting that the model 
is useful in estimating the user?s under-
standing, and can be applied to determining 
the agent?s next action.  
1 Introduction 
In face-to-face conversation, speakers adjust their 
utterances in progress according to the listener?s 
feedback expressed in multimodal manners, such 
as speech, facial expression, and eye-gaze. In task-
manipulation situations where the listener manipu-
lates objects by following the speaker?s instruc-
tions, correct task manipulation by the listener 
serves as more direct evidence of understanding 
(Brennan 2000, Clark and Krych 2004), and affects 
the speaker?s dialogue control strategies.  
Figure 1 shows an example of a software in-
struction dialogue in a video-mediated situation 
(originally in Japanese). While the learner says 
nothing, the instructor gives the instruction in 
small pieces, simultaneously modifying her ges-
tures and utterances according to the learner?s 
mouse movements. 
To accomplish such interaction between human 
users and animated help agents, and to assist the 
user through natural conversational interaction, this 
paper proposes a probabilistic model that computes 
timing dependencies among different types of be-
haviors in different modalities: speech, gestures, 
and mouse events. The model predicts (a) whether 
the instructor?s current utterance will be success-
fully understood by the learner and grounded 
(Clark and Schaefer 1989), and (b) whether the 
learner will successfully manipulate the object in 
the near future. These predictions can be used as 
constraints in determining agent actions. For ex-
ample, if the current utterance will not be grounded, 
then the help agent must add more information. 
In the following sections, first, we collect hu-
man-agent conversations by employing a Wizard-
of-Oz method, and annotate verbal and nonverbal 
behaviors. The annotated corpus is used to build a 
Bayesian network model for the multimodal in-
struction dialogues. Finally, we will evaluate how 
?That? (204ms pause)
Pointing gesture <preparation>
<stroke>
Mouse move
Instructor:
Learner:
?at the most? (395ms pause)
?left-hand side?
Instructor:
Learner:
Instructor:
Mouse move  
Figure 1: Example of task manipulation dialogue 
121
accurately the model can predict the events in (a) 
and (b) mentioned above. 
2 Related work 
In their psychological study, Clark and Krych 
(2004) showed that speakers alter their utterances 
midcourse while monitoring not only the listener?s 
vocal signals, but also the listener?s gestural sig-
nals as well as through other mutually visible 
events. Such a bilateral process functions as a joint 
activity to ground the presented information, and 
task manipulation as a mutually visible event con-
tributes to the grounding process (Brennan 2000, 
Whittaker 2003). Dillenbourg, Traum, et al (1996) 
also discussed cross-modality in grounding: ver-
bally presented information is grounded by an ac-
tion in the task environment.  
Studies on interface agents have presented com-
putational models of multimodal interaction 
(Cassell, Bickmore, et al 2000). Paek and Horvitz 
(1999) focused on uncertainty in speech-based in-
teraction, and employed a Bayesian network to 
understand the user?s speech input. For user moni-
toring, Nakano, Reinstein, et al (2003) used a head 
tracker to build a conversational agent which can 
monitor the user?s eye-gaze and head nods as non-
verbal signals in grounding. 
These previous studies provide psychological 
evidence about the speaker?s monitoring behaviors 
as well as conversation modeling techniques in 
computational linguistics. However, little has been 
studied about how systems (agents) should monitor 
the user?s task manipulation, which gives direct 
evidence of understanding to estimate the user?s 
understanding, and exploits the predicted evidence 
as constraints in selecting the agent?s next action. 
Based on these previous attempts, this study pro-
poses a multimodal interaction model by focusing 
on task manipulation, and predicts conversation 
states using probabilistic reasoning. 
3 Data collection 
A data collection experiment was conducted using 
a Wizard-of-Oz agent assisting a user in learning a 
PCTV application, a system for watching and re-
cording TV programs on a PC.  
The output of the PC operated by the user was 
displayed on a 23-inch monitor in front of the user, 
and also projected on a 120-inch big screen, in 
front of which a human instructor was standing 
(Figure 2 (a)). Therefore, the participants shared 
visual events output from the PC (Figure 2 (b)) 
while sitting in different rooms. In addition, a rab-
bit-like animated agent was controlled through the 
instructor?s motion data captured by motion sen-
sors. The instructor?s voice was changed through a 
voice transformation system to make it sound like 
a rabbit agent. 
4 Corpus  
We collected 20 conversations from 10 pairs, and 
annotated 11 conversations of 6 pairs using the 
Anvil video annotating tool (Kipp 2004).   
Agent?s verbal behaviors: The agent?s (actually, 
instructor?s) speech data was split by pauses longer 
than 200ms. For each inter pausal unit (IPU), utter-
ance content type defined as follows was assigned.  
? Identification (id): identification of a target 
object for the next operation 
? Operation (op): request to execute a mouse 
click or a similar primitive action on the target 
? Identification + operation (idop): referring to 
identification and operation in one IPU 
In addition to these main categories, we also 
used:  State (referring to a state before/after an op-
eration), Function (explaining a function of the 
system), Goal (referring to a task goal to be ac-
complished), and Acknowledgment. The inter-
coder agreement for this coding scheme is very 
high K=0.89 (Cohen?s Kappa), suggesting that the 
assigned tags are reliable.  
Agent?s nonverbal behaviors: As the most salient 
instructor?s nonverbal behaviors in the collected 
data, we annotated agent pointing gestures: 
? Agent movement: agent?s position  movement 
? Agent touching target (att): agent?s touching 
the target object as a stroke of a pointing ges-
ture  
          (a) Instructor                          (b) PC output 
Figure 2: Wizard-of-Oz agent controlled by instructor 
122
User?s nonverbal behaviors: We annotated three 
types of mouse manipulation for the user?s task 
manipulation as follows:   
? Mouse movement: movement of the mouse 
cursor 
? Mouse-on-target: the mouse cursor is on the 
target object  
? Click target: click on the target object 
4.1 Example of collected data 
 An example of an annotated corpus is shown in 
Figure 3. The upper two tracks illustrate the 
agent?s verbal and nonverbal behaviors, and the 
other two illustrate the user?s behaviors. The agent 
was pointing at the target (att) and giving a se-
quence of identification descriptions [a1-3]. Since 
the user?s mouse did not move at all, the agent 
added another identification IPU [a4] accompanied 
by another pointing gesture. Immediately after that, 
the user?s mouse cursor started moving towards the 
target object. After finishing the next IPU, the 
agent finally requested the user to click the object 
in [a6]. Note that the collected Wizard-of-Oz con-
versations are very similar to the human-human 
instruction dialogues shown in Figure 1. While 
carefully monitoring the user?s mouse actions, the 
Wizard-of-Oz agent provided information in small 
pieces. If it was uncertain that the user was follow-
ing the instruction, the agent added more explana-
tion without continuing. 
5 Probabilistic model of user-agent mul-
timodal interaction 
5.1 Building a Bayesian network model 
To consider multiple factors for verbal and non-
verbal behaviors in probabilistic reasoning, we 
employed a Bayesian network technique, which 
can infer the likelihood of the occurrence of a tar-
get event based on the dependencies among multi-
ple kinds of evidence. We extracted the conversa-
tional data from the beginning of an instructor's 
identification utterance for a new target object to 
the point that the user clicks on the object. Each 
IPU was split at 500ms intervals, and 1395 inter-
vals were obtained. As shown in Figure 4, the net-
work consists of 9 properties concerning verbal 
and nonverbal behaviors for past, current, and fu-
ture interval(s).   
5.2 Predicting evidence of understanding 
As a preliminary evaluation, we tested how ac-
curately our Bayesian network model can predict 
an instructor?s grounding judgment, and the user?s 
mouse click. The following five kinds of evidence 
were given to the network to predict future states. 
As evidence for the previous three intervals (1.5 
sec), we used (1) the percentage of time the agent 
touched the target (att), (2) the number of the 
user?s mouse movements. Evidence for the current 
interval is (3) current IPU?s content type, (4) 
whether the end of the current interval will be the 
end of the IPU (i.e. whether a pause will follow 
after the current interval), and (5) whether the 
mouse is on the target object. 
Well, 
Yes View 
the TV right of 
Yes 
Beside the DVD There is a button 
starts with ?V?
Ah, yes Er, yes 
Press it 
This 
User
Agent
Speech
Gesture
Mouser actions
id id id id id+op
Mouse move
click
att att att
Mouse on 
target
[a2] [a3] [a4] [a5] [a6][a1]
ack ack ack ack
Speech
Off
On
 
Figure 3: Example dialogue between Wizard-of-Oz agent and user 
 
Figure 4: Bayesian network model 
123
(a) Predicting grounding judgment: We tested 
how accurately the model can predict whether the 
instructor will go on to the next leg of the instruc-
tion or will give additional explanations using the 
same utterance content type (the current message 
will not be grounded). 
The results of 5-fold cross-validation are shown 
in Table 1. Since 83% of the data are ?same con-
tent? cases, prediction for ?same content? is very 
accurate (F-measure is 0.90). However, it is not 
very easy to find ?content change? case because of 
its less frequency (F-measure is 0.68). It would be 
better to test the model using more balanced data.  
(b) Predicting user?s mouse click: As a measure 
of the smoothness of task manipulation, the net-
work predicted whether the user?s mouse click 
would be successfully performed within the next 5 
intervals (2.5sec). If a mouse click is predicted, the 
agent should just wait without annoying the user 
by unnecessary explanation. Since randomized 
data is not appropriate to test mouse click predic-
tion, we used 299 sequences of utterances that w-
ere not used for training. Our model predicted 84% 
of the user?s mouse clicks: 80% of them were pre-
dicted 3-5 intervals before the actual occurrence of 
the mouse click, and 20% were predicted 1 interval 
before. However, the model frequently generates 
wrong predictions. Improving precision rate is 
necessary.  
6 Discussion and Future Work 
We employed a Bayesian network technique to our 
goal of developing conversational agents that can 
generate fine-grained multimodal instruction dia-
logues, and we proposed a probabilistic model for 
predicting grounding judgment and user?s success-
ful mouse click. The results of preliminary evalua-
tion suggest that separate models of each modality 
for each conversational participant cannot properly 
describe the complex process of on-going multi-
modal interaction, but modeling the interaction as 
dyadic activities with multiple tracks of modalities 
is a promising approach. 
The advantage of employing the Bayesian net-
work technique is that, by considering the cost of 
misclassification and the benefit of correct classifi-
cation, the model can be easily adjusted according 
to the purpose of the system or the user?s skill level. 
For example, we can make the model more cau-
tious or incautious. Thus, our next step is to im-
plement the proposed model into a conversational 
agent, and evaluate our model not only in its accu-
racy, but also in its effectiveness by testing the 
model with various utility values. 
References 
Brennan, S. 2000. Processes that shape conversation and 
their implications for computational linguistics. In 
Proceedings of 38th Annual Meeting of the ACL. 
Cassell, J., Bickmore, T., Campbell, L., Vilhjalmsson, H. 
and Yan, H. (2000). Human Conversation as a Sys-
tem Framework: Designing Embodied Conversa-
tional Agents. Embodied Conversational Agents. J. 
Cassell, J. Sullivan, S. Prevost and E. Churchill. 
Cambridge, MA, MIT Press: 29-63. 
Clark, H. H. and Schaefer, E. F. 1989. Contributing to 
discourse. Cognitive Science 13: 259-294. 
Clark, H. H. and Krych, M. A. 2004. Speaking while 
monitoring addressees for understanding. Journal of 
Memory and Language 50(1): 62-81. 
Dillenbourg, P., Traum, D. R. and Schneider, D. 1996. 
Grounding in Multi-modal Task Oriented Collabora-
tion. In Proceedings of EuroAI&Education Confer-
ence: 415-425. 
Kipp, M. 2004. Gesture Generation by Imitation - From 
Human Behavior to Computer Character Animation, 
Boca Raton, Florida: Dissertation.com. 
Nakano, Y. I., Reinstein, G., Stocky, T. and Cassell, J. 
2003. Towards a Model of Face-to-Face Grounding. 
In Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics: 553-561. 
Paek, T. and Horvitz, E. (1999). Uncertainty, Utility, 
and Misunderstanding: A Decision-Theoretic Per-
spective on Grounding in Conversational Systems. 
Working Papers of the AAAI Fall Symposium on 
Psychological Models of Communication in Collabo-
rative Systems. S. E. Brennan, A. Giboin and D. 
Traum: 85-92. 
Whittaker, S. (2003). Theories and Methods in Medi-
ated Communication. The Handbook of Discourse 
Processes. A. Graesser, MIT Press. 
 
Table 1: Preliminary evaluation results 
 Precision Recall F-measure
Content  
change  0.53 0.99 0.68 
Same  
content 1.00 0.81 0.90 
124
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 100?103,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Implicit Proposal Filtering
in Multi-Party Consensus-Building Conversations
Yasuhiro Katagiri
Future University ? Hakodate
katagiri@fun.ac.jp
Yosuke Matsusaka
National Institute of Advanced
Industrial Science and Technology
yosuke.matsusaka@aist.go.jp
Yasuharu Den
Chiba University
den@cogsci.l.chiba-u.ac.jp
Mika Enomoto
Tokyo University of Technology
menomoto@media.teu.ac.jp
Masato Ishizaki
The University of Tokyo
ishizaki@iii.u-tokyo.ac.jp
Katsuya Takanashi
Kyoto University
takanasi@ar.media.kyoto-u.ac.jp
Abstract
An attempt was made to statistically estimate
proposals which survived the discussion to
be incorporated in the final agreement in an
instance of a Japanese design conversation.
Low level speech and vision features of hearer
behaviors corresponding to aiduti, noddings
and gaze were found to be a positive pre-
dictor of survival. The result suggests that
non-linguistic hearer responses work as im-
plicit proposal filters in consensus building,
and could provide promising candidate fea-
tures for the purpose of recognition and sum-
marization of meeting events.
1 Introduction
Non-verbal signals, such as gaze, head nods, fa-
cial expressions and bodily gestures, play signif-
icant roles in the conversation organization func-
tions. Several projects have been collecting multi-
modal conversation data (Carletta et al, 2006) for
multi-party dialogues in order to develop techniques
for meeting event recognitions from non-verbal as
well as verbal signals. We investigate, in this paper,
hearer response functions in multi-party consensus-
building conversations. We focus particularly on the
evaluative aspect of verbal and non-verbal hearer re-
sponses. During the course of a consensus-building
discussion meeting, a series of proposals are put
on the table, examined, evaluated and accepted or
rejected. The examinations of proposals can take
the form of explicit verbal exchanges, but they can
also be implicit through accumulations of hearer
responses. Hearers would express, mostly uncon-
sciously for non-verbal signals, their interest and
positive appraisals toward a proposal when it is
introduced and is being discussed, and that these
hearer responses would collectively contribute to the
determination of final consensus making. The ques-
tion we address is whether and in what degree it is
possible and effective to filter proposals and estimate
agreement by using verbal and non-verbal hearer re-
sponses in consensus-building discussion meetings.
2 Multi-Party Design Conversation Data
2.1 Data collection
We chose multi-party design conversations for the
domain of our investigation. Different from a fixed
problem solving task with a ?correct? solution, par-
ticipants are given partially specified design goals
and engage in a discussion to come up with an agree-
ment on the final design plan. The condition of our
data collection was as follows:
Number of participants: six for each session
Arrangement: face-to-face conversation
Task: Proposal for a new mobile phone business
Role: No pre-determined role was imposed
A compact meeting archiver equipment, AIST-
MARC (Asano and Ogata, 2006), which can cap-
ture panoramic video and speaker-separated speech
streams, was used to record conversations (Fig. 1).
The data we examined consist of one 30 minutes
conversation conducted by 5 males and 1 female.
Even though we did not assign any roles, a chairper-
son and a clerk were spontaneously elected by the
participants at the beginning of the session.
100
Figure 1: AIST-MARC and a recording scene
2.2 Data Annotation
2.2.1 Clause units
In order to provide a clause level segmentation
of a multi-channel speech stream, we extended the
notion of ?clause units (CUs)?, originally developed
for analyzing spoken monologues in the Corpus of
Spontaneous Japanese (Takanashi et al, 2003), to
include reactive tokens (Clancy et al, 1996) and
other responses in spoken conversations. Two of the
authors who worked on the Corpus of Spontaneous
Japanese independently worked on the data and re-
solved the differences, which created 1403 CUs con-
sisting of 469 complete utterances, 857 reactive to-
kens, and 77 incomplete or fragmental utterances.
2.2.2 Proposal units
We developed a simple classification scheme of
discourse segments for multi-party consensus build-
ing conversations based on the idea of ?interaction
process analysis? (Bales, 1950).
Proposal: Presentation of new ideas and their eval-
uation. Substructure are often realized through
elaboration and clarification.
Summary: Sum up multiple proposals possibly
with their assessment
Orientation: Lay out a topic to be discussed and
signal a transition of conversation phases, initi-
ated mostly by the facilitator of the discussion
Miscellaneous: Other categories including opening
and closing segments
The connectivity between clause units, the content
of the discussion, interactional roles, relationship
with adjacent segments and discourse markers were
considered in the identification of proposal units.
Two of the authors, one worked on the Corpus of
Spontaneous Japanese and the other worked for the
Figure 2: Image processing algorithm
project of standardization of discourse tagging, in-
dependently worked on the data and resolved the
differences, which resulted in 19 proposals, 8 sum-
maries, 19 orientations and 2 miscellaneouses.
2.3 Core clause units and survived proposal
units
Core clause units (CUs) were selected, out of all the
clause units, based on whether the CUs have sub-
stantial content as a proposal. A CU was judged
as a core CU, when the annotator would find it ap-
propriate to express, upon hearing the CU, either an
approval or a disapproval to its content if she were
in the position of a participant of the conversation.
Three of the authors worked on the text data exclud-
ing the reactive tokens, and the final selection was
settled by majority decision. 35 core CUs were se-
lected from 235 CUs in the total of 19 proposal PUs.
Cohen?s kappa agreement rate was 0.894.
Survived proposal units (PUs) were similarly se-
lected, out of all the proposal units, based on
whether the PUs were incorporated in the final
agreement among all the participants. 9 survived
PUs were selected from 19 proposal PUs.
3 Feature Extraction of Hearer?s Behavior
For each clause unit (CU), verbal and non-verbal
features concerning hearer?s behavior were ex-
tracted from the audio and the video data.
3.1 Non-Verbal Features
We focused on nodding and gaze, which were ap-
proximated by vertical and horizontal head move-
ments of participants.
An image processing algorithm (Figure 2) was ap-
plied to estimate head directions and motions (Mat-
susaka, 2005). Figure 3 shows a sample scene and
the results of applying head direction estimation al-
gorithm.
101
Figure 3: Sample scene with image processing results.
The circles represent detected face areas, and the lines in
the circles represent head directions.
For each CU, the vertical and horizontal compo-
nents of head movements of 5 hearers were calcu-
lated for two regions, the region inside the CU and
the 1-sec region immediately after the CU. For each
of the two regions, the mean and the peak values and
the relative location, in the region, of the peak were
computed. These 12 non-verbal features were used
for the statistical modeling.
3.2 Verbal Features
Verbal features were extracted from the audio data.
For each CU, power values of 5 hearers were ex-
tracted for two regions, ?within? and ?after? CU, and
for each of the two regions, the mean and the peak
values and the relative location, in the region, of
the peak were computed. In addition to these ver-
bal features, we also used aiduti features of reactive
tokens (RTs). The percentage of the total duration
of RTs, the total number of RTs, and the number of
participants who produced an RT were computed in
?within? and ?after? regions for each of the CUs. A
total of 12 CU verbal features were used for the sta-
tistical modeling.
4 Experiments
4.1 Overview of the Algorithm
Statistical modeling was employed to see if it is pos-
sible to identify the proposal units (PUs) that are sur-
vived in the participants? final consensus. To this
end, we, first, find the dominant clause unit (CU) in
each PU, and, then, based on the verbal and non-
verbal features of these CUs, we classify PUs into
?survived? and ?non-survived.?
Table 1: The optimal model for finding core-CUs
Estimate
(Intercept) ?1.72
within/speech power/mean ?11.54
after/vertical motion/peak loc. ?4.25
after/speech power/mean 3.91
after/aiduti/percent 3.02
Table 2: Confusion matrix of core-CU prediction experi-
ment (precision = 0.50, recall = 0.086)
Predicted
Observed Non-core Core
Non-core 431 3
Core 32 3
4.2 Finding Dominant CUs
A logistic regression model was used to model the
coreness of CUs. A total of 24 verbal and non-verbal
features were used as explanatory variables. Since
the number of non-core CUs was much larger than
that of core CUs, down-sampling of negative in-
stances was performed. To obtain a reliable estima-
tion, a sort of Monte Carlo simulation was adopted.
A model selection by using AIC was applied for
the 35 core CUs and another 35 non-core CUs that
were re-sampled from among the set of 434 com-
plete and non-core CUs. This process was repeated
100 times, and the features frequently selected in
this simulation were used to construct the optimal
model. Table 1 shows the estimated coefficient for
the optimal model, and Table 2 shows the accu-
racy based on a leave-1-out cross validation. The
dominant CU in each PU was identified as the CU
600 700 800 900 1000 1100 1200
0.0
0.2
0.4
0.6
0.8
1.0
time[sec]
core
?CU 
likelih
ood
O P S S O O P S S S P P O S P OO0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0(0) (0) (1) (0) (0)(0)(0) (0) (0) (0) (0) (0) (1) (0) (0) (0) (0)(0)
Figure 4: The predicted coreness of CUs. Dominant CUs
were defined to be CUs with the highest coreness in each
of the PUs. Black and white dots are CUs labeled as core
and non-core.
102
Table 3: The optimal model for finding survived-PUs
Estimate
within/vertical motion/peak val. 3.96
within/speech power/mean ?27.76
after/speech power/peak val. 1.49
Table 4: Result of the survived-PU prediction (precision
= 0.83, recall = 0.44)
Predicted
Observed Non-survived Survived
Non-survived 37 1
Survived 4 5
with the highest predicted value in that PU. Figure 4
shows the predicted values for coreness.
4.3 Finding Survived PUs
The verbal and non-verbal features of the dominant
CUs of each of the PUs were used for the modeling
of the survived-PU prediction. Discriminant analy-
sis was utilized and a model selection was applied
for the 47 PUs. Table 3 shows the estimated coeffi-
cient for the optimal model, and Table 4 shows the
accuracy based on a leave-1-out cross validation.
5 Discussions
The results of our estimation experiments indicate
that the final agreement outcome of the discus-
sion can be approximately estimated at the proposal
level. Though it may not be easy to identify actual
utterances contributing to the agreement (core-CUs),
the dominant CUs in PUs were found to be effective
in the identification of survived-PUs. The prediction
accuracy of survived-PUs was about 89%, with the
chance level of 69%, whereas that of core-CUs was
about 92%, with the chance level of 86%.
In terms of hearer response features, intensity
of verbal responses (within/speech power/mean, af-
ter/speech power/mean), and immediate nodding re-
sponses (after/vertical motion/peak loc.) were the
most common contributing features in core-CU es-
timation. In contrast, occurrence of a strong aiduti
immediately after, rather than within, the core-
CU (after/speech power/peak val.), and a strong
nodding within the core-CU (within/vertical mo-
tion/peak val.) appear to be signaling support from
hearers to the proposal. It should be noted that iden-
tification of target hearer behaviors must be vali-
dated against manual annotations before these gen-
eralizations are established. Nevertheless, the re-
sults are mostly coherent with our intuitions on the
workings of hearer responses in conversations.
6 Conclusions
We have shown that approximate identification of
the proposal units incorporated into the final agree-
ment can be obtained through the use of statistical
pattern recognition techniques on low level speech
and vision features of hearer behaviors. The result
provides a support for the idea that hearer responses
convey information on hearers? affective and evalu-
ative attitudes toward conversation topics, which ef-
fectively functions as implicit filters for the propos-
als in the consensus building process.
Acknowledgments
The work reported in this paper was supported by Japan
Society for the Promotion of Science Grants-in-aid for
Scientific Research (B) 18300052.
References
F. Asano and J. Ogata. 2006. Detection and separation
of speech events in meeting recordings. In Proc. Inter-
speech, pages 2586?2589.
R. F. Bales. 1950. A set of categories for the analysis
of small group interaction. American Sociological Re-
view, 15:257?263.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M.
Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I.
McCowan, W. Post, D. Reidsma, and P.Wellner. 2006.
The AMI meeting corpus: A pre-announcement. In
Machine Learning for Multimodal Interaction, pages
28?39.
P. M. Clancy, S. A. Thompson, R. Suzuki, and H. Tao.
1996. The conversational use of reactive tokens in En-
glish, Japanese and Mandarin. Journal of Pragmatics,
26:355?387.
Y. Matsusaka. 2005. Recognition of 3 party conversation
using prosody and gaze. In Proc. Interspeech, pages
1205?1208.
K. Takanashi, T. Maruyama, K. Uchimoto, and H.
Isahara. 2003. Identification of ?sentence? in.
spontaneous Japanese: detection and modification of
clause boundaries. In Proc. ISCA & IEEE Workshop
on Spontaneous Speech Processing and Recognition,
pages 183?186.
103
