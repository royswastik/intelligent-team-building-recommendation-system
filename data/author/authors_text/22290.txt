Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 363?373,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Multi-Predicate Semantic Role Labeling
Haitong Yang and Chengqing Zong
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{htyang, cqzong}@nlpr.ia.ac.cn
Abstract
The current approaches to Semantic Role
Labeling (SRL) usually perform role clas-
sification for each predicate separately and
the interaction among individual predi-
cate?s role labeling is ignored if there is
more than one predicate in a sentence. In
this paper, we prove that different predi-
cates in a sentence could help each other
during SRL. In multi-predicate role label-
ing, there are mainly two key points: argu-
ment identification and role labeling of the
arguments shared by multiple predicates.
To address these issues, in the stage of
argument identification, we propose nov-
el predicate-related features which help re-
move many argument identification errors;
in the stage of argument classification, we
adopt a discriminative reranking approach
to perform role classification of the shared
arguments, in which a large set of glob-
al features are proposed. We conducted
experiments on two standard benchmarks:
Chinese PropBank and English PropBank.
The experimental results show that our
approach can significantly improve SRL
performance, especially in Chinese Prop-
Bank.
1 Introduction
Semantic Role Labeling (SRL) is a kind of shal-
low semantic parsing task and its goal is to rec-
ognize some related phrases and assign a joint
structure (WHO did WHAT to WHOM, WHEN,
WHERE, WHY, HOW) to each predicate of a sen-
tence (Gildea and Jurafsky, 2002). Because of
the ability of encoding semantic information, SR-
L has been applied in many tasks of NLP, such as
question and answering (Narayanan and Haraba-
gir, 2004), information extraction (Surdeanu et
The justices will be forced to reconsider  the questions.
[      A1      ] [  Pred  ]
[      A0      ] [    Pred    ] [      A1      ]
Figure 1: A sentence from English PropBank,
with an argument shared by multiple predicates
al., 2003; Christensen et al., 2005), and machine
translation (Wu and Fung, 2009; Liu and Gildea,
2010; Xiong et al., 2012; Zhai et al., 2012).
Currently, an SRL system works as follows:
first identify argument candidates and then per-
form classification for each argument candidate.
However, this process only focuses on one inde-
pendent predicate without considering the internal
relations of multiple predicates in a sentence. Ac-
cording to our statistics, more than 80% sentences
in Propbank carry multiple predicates. One exam-
ple is shown in Figure 1, in which there are two
predicates ?Force? and ?Reconsider?. Moreover,
the constituent ?the justices? is shared by the two
predicates and is labeled as A1 for ?Force? but as
A0 for ?Reconsider?. We call this phenomenon of
the shared arguments Role Transition . Intuitive-
ly, all predicates in a sentence are closely related to
each other and the internal relations between them
would be helpful for SRL.
This paper has made deep investigation on
multi-predicate semantic role labeling. We think
there are mainly two key points: argument identi-
fication and role labeling of the arguments shared
by multiple predicates. We adopt different strate-
gies to address these two issues.
During argument identification, there are a large
number of identification errors caused by the poor
performance of auto syntax trees. However, many
of these errors can be removed, if we take other
predicates into consideration. To achieve this pur-
pose, we propose novel predicates-related features
which have been proved to be effective to recog-
363
nize many identification errors. After these fea-
tures added, the precision of argument identifica-
tion improves significantly by 1.6 points and 0.9
points in experiments on Chinese PropBank and
English PropBank respectively, with a slight loss
in recall.
Role labeling of the shared arguments is anoth-
er key point. The predicates and their shared argu-
ment could be considered as a joint structure, with
strong dependencies between the shared argumen-
t?s roles. If we consider linguistic basis for joint
modeling of the shared argument?s roles, there are
at least two types of information to be captured.
The first type of information is the compatibility
of Role Transition among the shared argument?s
roles. A noun phrase may be labeled as A0 for a
predicate and at the same time, it can be labeled
as A1 for another predicate. However, there are
few cases that a noun phrase is labeled as A0 for a
predicate and as AM-ADV for another predicate
at the same time. Secondly, joint modeling the
shared arguments could explore global informa-
tion. For example, in ?The columbia mall is ex-
pected to open?, there are two predicates ?expect?
and ?open? and a shared argument ?the columbi-
a mall?. Because this shared argument is before
?open? and the predicate ?open? is in active voice,
a base classifier often incorrectly label this argu-
ment A0 for ?open?. But if we observe that the ar-
gument is also an argument of ?expect?, it should
be labeled as A1 for ?expect? and ?open?.
Motivated by the above observations, we at-
tempt to jointly model the shared arguments? roles.
Specifically, we utilize the discriminative rerank-
ing approach that has been successfully employed
in many NLP tasks. Typically, this method first
creates a list of n-best candidates from a base sys-
tem, and then reranks them with arbitrary features
(both local and global), which are either not com-
putable or are computationally intractable within
the base model.
We conducted experiments on Chinese Prop-
Bank and English PropBank. Results show that
compared with a state-of-the-art base model, the
accuracy of our joint model improves significant-
ly by 2.4 points and 1.5 points on Chinese Prop-
Bank and English PropBank respectively, which
suggests that there are substantial gains to be made
by jointly modeling the shared arguments of mul-
tiple predicates.
Our contributions can be summarized as fol-
lows:
? To the best of our knowledge, this is the first
work to investigate the mutual effect of mul-
tiple predicates? semantic role labeling.
? We present a rich set of features for argument
identification and shared arguments? classifi-
cation that yield promising performance.
? We evaluate our method on two standard
benchmarks: Chinese PropBank and English
PropBank. Our approach performs well in
both, which suggests its good universality.
The remainder of this paper is organized as fol-
lows. Section 2 gives an overview of our approach.
We discuss the mutual effect of multi-predicate?
argument identification and argument classifica-
tion in Section 3 and Section 4 respectively. The
experiments and results are presented in Section
5. Some discussion and analysis can be found in
Section 6. Section 7 discusses the related work-
s. Finally, the conclusion and future work are in
Section 8.
2 Approach Overview
As illustrated in Figure 2, our approach follows the
standard separation of the task of semantic role la-
beling into two phases: Argument Identification
and Argument Classification . We investigate the
effect of multiple predicates in Argument Identi-
fication and Argument Classification respectively.
Specifically, in the stage of Argument Identifica-
tion, we introduce new features related to predi-
cates which are effective to recognize many argu-
ment identification errors. In the stage of Argu-
ment Classification, we concentrate on the classi-
fication of the arguments shared by multiple pred-
icates. We first use a base model to generate n-
best candidates for the shared arguments and then
construct a joint model to rerank the n-best list, in
which a rich set of global features are proposed.
3 Argument Identification
In this section, we investigate multi-predicate? mu-
tual effects in Argument Identification. Argument
Identification is to recognize the arguments from
all candidates of each predicate. Here, we use
the Maximum Entropy (ME) classifier to perform
binary classification. As a discriminative model,
ME can easily incorporate arbitrary features and
364
C andidates
Phase 1: 
Argument Identification Base 
Features
 New 
Features
Cl assifier
Refined Argument 
C andidates
Phase 2 : 
Argument Classification
Joint Model
B ase Model
N - Best list
Final Results
Is - Shared
Figure 2: The overview of our approach
achieve good performance. The model is formu-
lated as follows:
p(y|x) =
1
Z(x)
exp(
?
i
?
i
f
i
(x, y)) (1)
in which x is the input sample, y(0 or 1) is the out-
put label, f(x, y) are feature functions and Z(x)
is a normalization term as follows:
Z(x) =
?
y
exp(
?
i
?
i
f
i
(x, y))
3.1 Base Features
Xue (2008) took a critical look at the features used
in SRL and achieved good performance. So, we
use the same features in Xue (2008) as the base
features:
? Predicate lemma
? Path from node to predicate
? Head word
? Head word?s part-of-speech
? Verb class (Xue, 2008)
? Predicate and Head word combination
? Predicate and Phrase type combination
? Verb class and Head word combination
? Verb class and Phrase type combination
3.2 Additional Features
In the SRL community, it is widely recognized
that the overall performance of a system is large-
ly determined by the quality of syntactic parsers
(Gildea and Palmer, 2002), which is particularly
notable in the identification stage. Unfortunate-
ly, the state-of-the-art auto parsers fall short of the
demands of applications. Moreover, when there
are multiple predicates, or even multiple clauses
in a sentence, the problem of syntactic ambiguity
increases drastically (Kim et al., 2000). For ex-
ample, in Figure 3, there is a sentence with two
consecutive predicates ?/? (is) and ? ?? (devel-
op). Compared with the gold tree, the auto tree is
less preferable, which makes the classifier easily
mistake ??Q? (building) as an argument of ? 
?? (develop) with base features. But this identifi-
cation error can be removed if we note that there
is another predicate ?/? (is) before ? ?? (devel-
IP
NP VP
V V NP
DNP NP
VC VP
N N D EG
IP
NP VP
VC NP
CP NP
IP DEC
V V NP
??
?
?? ??
?
????
??
??
?
?? ?
????
( a ) ( b)
?? ? ?? ?? ? ????
Building is an economic activity of developing Pudong .
Figure 3: An example from Chinese PropBank.
Tree (a) is the gold syntax tree and (b) is parsed by
a state of-the-art parser Berkeley parser. On tree
(b), ??Q? (building) is mistaken as an argument
of ? ?? (develop) with base features.
365
op). Similar examples with the pattern ?NP +/ +
VV? can be found in PropBank, in which the sub-
ject NP of the sentence is usually not an argument
of the latter predicate. Thus, ?/? (is) is an effec-
tive clue to detect this kind of identification error.
It is challenging to obtain a fully correct syntax
tree for a complex sentence with multiple predi-
cates. Therefore, base features that heavily rely
on syntax trees often fail in discriminating argu-
ments from candidates as demonstrated in Figure
3. However, by considering the elements of neigh-
boring predicates, we could capture useful clues
like in the above example and remove many iden-
tification errors. Below, we define novel predi-
caterelated features to encode these ?clues? to re-
fine candidates.
There are mainly five kinds of features as fol-
lows.
? Is the given predicate the nearest one?
This is a binary feature that indicates whether
the predicate is the nearest one to the candi-
date.
? Local adjunct
This is a binary feature designed for adjective
and adverbial phrases. Some adjunct phras-
es, such as ??? (only), have a limited sphere
of influence. If the candidate is ?local? but
the given predicate is not the nearest one, the
candidate is often not an argument for the
given predicate. To collect local adjuncts, we
traverse the whole training set to get the ini-
tial lexicon of adjuncts and refine it manually.
? Cut-Clause
This type of feature is a binary feature de-
signed to distinguish identification errors of
noun phrase candidates. If a noun phrase can-
didate is separated from the given predicate
by a clause consisting of a NP and VP, the
candidate is usually not the argument of the
given predicate.
? Different Relative Positions with Conjunc-
tions
This is a binary feature that describes whether
the candidate and the predicate are located in
different positions as separated by conjunc-
tions such as ?F/? (but). Conjunctions are
often used to concatenate two clauses, but the
first clause commonly describes one proposi-
tion and the second clause describes anoth-
er one. Thus, if the candidate and the given
predicate have different positions relative to
the conjunctions, the candidate is usually not
the argument of the given predicate.
? Consecutive Predicates Sequence
When multiple predicates appear in a sen-
tence consecutively, parse errors frequently
occurs due to the problems of syntactic am-
biguity as demonstrated in Figure 2. To in-
dicate such errors, sequence features of the
candidates and consecutive predicates are de-
fined specifically. For instance, for the candi-
date ??Q? (building) of ? ?? (develop),
the features are ?cand-/- ?? and ?cand-
/-VV?, in which we use ?cand? to represent
the position of the candidate.
4 Argument Classification
In this section, we investigate multi-predicate? mu-
tual effects in Argument Classification. Argument
Classification is to assign a label to each argumen-
t candidate recognized by the phase of Argument
Identification.
4.1 Base Model
A conventional method in Argument Classifica-
tion is to assign a label to each argument candidate
by a classifier independently. We call this kind of
method Base Model. In the base model, we still
adopt ME (1) as our classifier; all base features of
Argument Identification are contained (shown in
subsection 3.1). In addition, there are some other
features:
? Position: the relative position of the candi-
date argument compared to the predicate
? Subcat frame: the syntactic rule that expands
the parent of the verb
? The first and the last word of the candidate
? Phrase type: the syntactic tag of the candidate
argument
? Subcat frame+: the frame that consists of the
NPs (Xue, 2008).
366
4.2 Joint Model
As discussed briefly in Section 1, there are many
dependencies between the shared arguments? la-
beling for different predicates, but the base model
completely ignores such useful information. To
incorporate these dependencies, we employ the
discriminative reranking method. Here, we first
establish a unified framework for reranking. For
an input x, the generic reranker selects the best
output y
?
among the set of candidates GEN(x)
according to the scoring function:
y
?
= argmax
y?GEN(x)
score(y) (2)
In our task, GEN(x) is a set of the n-best can-
didates generated from the base model. As usual,
we calculate the score of a candidate by the dot
product between a high dimensional feature and a
weight W:
score(y) = W ? f(y) (3)
We estimate the weight W using the aver-
aged perceptron algorithm (Collins, 2002a) which
is well known for its fast speed and good per-
formance in similar large-parameter NLP tasks
(Huang, 2008). The training algorithm of the
generic averaged perceptron is shown in Table 1.
In line 5, the algorithm updates W with the differ-
ence (if any) between the feature representations
of the best scoring candidate and the gold candi-
date. We also use a refinement called ?averaged
parameters? that the final weight vector W is the
average of weight vectors over T iterations and N
samples. This averaging effect has been shown to
reduce overfitting and produces more stable results
(Collins, 2002a).
Pseudocode: Averaged Structured Perceptron
1: Input: training data(x
t
, y
?
t
) for t = 1, ..., T ;
2: w?
(0)
? 0; v ? 0; i? 0
3: for n in 1,...,N do
4: for t in 1, ..., T do
5: w?
(i+1)
? update w?
(i)
according to (x
t
, y
?
t
)
6: v ? v + w?
i+1
7: i? i+ 1
8: w? ? v//(N ? T )
9: return w?
Table 1: The perceptron training algorithm
4.3 Features for Joint Model
Here, we introduce features used in the joint mod-
el. For clear illustration, we describe these fea-
tures in the context of the example in Figure 1.
Role Transition (RT): a binary feature to in-
dicate whether the transitions among roles of the
candidate are reasonable. Because all roles are as-
signed to the same candidate, all role transitions
should be compatible. For instance, if an argu-
ment is labeled as AM-TMP for one predicate, it
cannot be labeled as AM-LOC for another pred-
icate. This feature is constructed by traversing
the training data to ascertain whether transitions
between all roles are reasonable. In Table 2, we
list some role transitions which are obtained from
the training data of experiments on Chinese Prop-
Bank.
Roles and Predicates? Sequence (RPS): a
joint feature template that concatenates roles and
the given predicates. For the gold candidate
?Arg1, Arg0?, the feature is ?Arg1-force, Arg0-
reconsider?.
Roles and Predicates? Sequence with Rela-
tive Orders (RPSWR): the template is similar to
the above one except that relative orders between
roles and predicates are added. If the shared argu-
ment is before the given predicate, the feature is
described as ?Role-Predicate?; otherwise, the fea-
ture is ?Predicate-Role?. And, if the predicate?s
voice is passive, the order is reversed. Thus, for
the gold candidate ?Arg1, Arg0?, this feature is
?force-Arg1, Arg0-reconsider?.
Roles and Phrase Type Sequence (RPTS)
Roles and Head Word Sequence (RHWS)
Roles and Head Word?s POS Sequence
(RHWPS)
These three features are utilized to explore the
shared argument?s relations with roles.
Time and Location Class (TLC): We find
there are much confusions between AM-TMP and
AM-LOC in the base model. To fix these errors,
we add two features: Time and Location Class.
For these features, we just collect phrases labeled
as AM-TMP and AM-LOC from the training da-
ta. When the argument belongs to Time or Loca-
tion Class, we add a sequence template consisting
of ?Role-Time? for Time Class or ?Role-Location?
for Location Class. For the gold candidate ?Arg1,
Arg0?, the feature is ?Arg1-none, Arg0-none? be-
cause ?the justices? belongs neither to Time Class
nor to Location Class.
367
Role Arg0 Arg1 Arg2 AM-LOC AM-TMP AM-ADV AM-MNR AM-TPC
Arg0 + + + + + + + +
Arg1 + + + + - + + +
Arg2 + + + + - - - +
AM-LOC + + + + - + - +
AM-TMP + - - - + + - -
AM-ADV + - + - + + - -
AM-MNR + + - - - - + -
AM-TPC + + + + + + - +
Table 2: Some role transitons from Chinese PropBank. ?+? means reasonable role transition and ?-?
means illegal.
5 Experiments
5.1 Experimental Setting
To evaluate the performance of our approach, we
have conducted on two standard benchmarks: Chi-
nese PropBank and English PropBank. The exper-
imental setting is as follows:
Chinese:
We use Chinese Proposition Bank 1.0. All data
are divided into three parts. 648 files (from cht-
b 081.fid to chtb 899.fid) are used as the training
set. 40 files (from chtb 041.fid to chtb 080.fid)
constitutes the development set. The test set con-
sists of 72 files (chtb 001.fid to chtb 040.fid and
chtb 900.fid to chtb 931.fid). This data setting is
the same as in (Xue, 2008; Sun et al., 2009). We
adopt Berkeley Parser
1
to carry out auto parsing
for SRL and the parser is retrained on the training
set. We used n =10 joint assignments for training
the joint model and testing.
English:
We choose English Propbank as the evaluation
corpus. According to the traditional partition, the
training set consists of the annotations in Sections
2 to 21, the development set is Section 24, and
the test set is Section 23. This data setting is the
same as in (Xue and Palmer, 2004; Toutanova et
al., 2005). We adopt Charniak Parser
2
to carry out
auto parsing for SRL and the parser is retrained on
the training set. We used n =10 joint assignments
for training the joint model and testing.
5.2 Experiment on Argument Identification
We first investigate the performance of our ap-
proach in Argument Identification.
For the task of Argument Identification (AI), we
1
http://code.google.com/p/berkeleyparser/
2
https://github.com/BLLIP/bllip-parser
adopt auto parser to produce auto parsing trees for
SRL. The results are shown in Table 3. We can
see that in the experiment of Chinese, the F1 score
reaches to 78.79% with base features. While after
additional predicates-related features are added,
the precision has improved by 1.6 points with s-
light loss in recall, which leads to the improve-
ment of 0.6 points in F1. The similar effect oc-
curred in the experiment of English. After addi-
tional features added in the identification module,
the precision is improved by about 0.9 points with
a slight loss in recall, leading to an improvement
of 0.3 points in F1. However, the improvemen-
t in English is slight smaller than in Chinese. We
think the main reason is that there are less parse er-
rors in English than in Chinese. All results demon-
strate that the novel predicted-related features are
effective in recognizing many identification errors
which are difficult to discriminate with base fea-
tures.
P(%) R(%) F
1
(%)
Ch
Base 84.36 73.90 78.79
+Additional 85.97 73.72 79.38*
En
Base 82.86 76.83 79.73
+Additional 83.75 76.69 80.06
Table 3: Comparison with Base Features in Ar-
gument Identification. Scores marked by ?*? are
significantly better (p < 0.05) than base features.
5.3 Experiment on Argument Classification
5.3.1 Results
Errors produced in AI will influenced the evalu-
ation of Argument Classification (AC). So, to e-
valuate fairly we assume that the argument con-
stituents of a predicate are already known, and the
368
Num Acc(%)
Ch
Shared 2060 91.36
All 8462 92.77
En
Shared 2015 93.85
All 14061 92.30
Table 4: Performance of the Base Model in Argu-
ment Classification
Methods Acc(%)
Ch
Base 91.36
Joint 93.74*
En
Base 93.85
Joint 95.33*
Table 5: Comparison with Base Model on shared
arguments. Scores marked by ?*? are significantly
better (p < 0.05) than base model.
task is only to assign the correct labels to the con-
stituents. The evaluation criterion is Accuracy.
The results of the base model are shown in Ta-
ble 4. We first note that in testing set, there are a
large number of shared arguments, which weigh-
s about one quarter of all arguments in Chinese
and 14% in English. Therefore, the fine process-
ing of these arguments is essential for argumen-
t classification. However, the base model cannot
handle these shared arguments so well in Chinese
that the accuracy of the shared arguments is lower
by about 1.4 points than the average value of all
arguments. Nevertheless, from Table 5 we can see
that our joint model?s accuracy on the shared argu-
ments reaches 93.74%, 2.4 points higher than the
base model in Chinese. Although the base mod-
el obtain good performance on shared arguments
of English, our joint model?s performance reach-
es 95.33%, 1.5 points higher than the base mod-
el. This indicates that even though the base model
is optimized to utilize a large set of features and
achieves the state-of-the-art performance, it is still
advantageous to model the joint information of the
shared arguments.
Another point to note is that our joint model in
resolving English SRL task is not so good as in
Chinese SRL. There are mainly two reasons. The
first reason is that the shared arguments occur less
in English than in Chinese so that training sam-
ples are insufficient for our discriminative model.
The second reason is the annotation of some in-
transitive verbs. In English PropBank, there is a
class of intransitive verbs such as ?land? (known
as verbs of variable behavior), for which the ar-
gument can be tagged as either ARG0 or ARG1.
Here, we take examples from the guideline
3
of En-
glish PropBank to explain.
?A bullet (ARG1) landed at his feet?
?He (ARG0) landed?
In the above examples, the two arguments and
the predicate ?land? have the same relative order
and voice but the arguments have different label-
s for their respective predicates. In fact, accord-
ing to the intention of the annotator, ARG0 and
ARG1 are both correct. Unfortunately, in English
PropBank, there is only one gold label for each ar-
gument, which leads to much noise for our joint
model. Moreover, such situations are not rare in
the corpus.
5.3.2 Feature Performance
We investigate effects of the features of joint mod-
el to the performance and results are shown in Ta-
ble 6. Each row shows the improvement over the
baseline when that feature is used in the joint mod-
el. We can see that features proposed are beneficial
to the performance of the joint model. But some
features like ?RPS? and ?RPSRO? play a more im-
portant role.
Features Chinese English
base 91.36 93.85
RT 91.70 94.10
RPS 92.30 94.70
RPSRO 92.24 94.50
RPTS 91.80 94.18
RHWS 91.63 93.95
RHWPS 91.43 94.23
TCL 91.93 94.23
All 93.74 95.33
Table 6: Features performance in the Joint Model.
We use first letter of words to represent features.
5.4 SRL Results
We also conducted the complete experiment on the
auto parse trees. The results are shown in Table
7. In experiments on Chinese PropBank, we can
see that after novel predicate-related features are
added in the stage of Argument Identification, our
model outperforms the base model by 0.5 points
3
http://verbs.colorado.edu/propbank/EPB-
AnnotationGuidelines.pdf
369
F1
(%)
Chinese
Base 74.04
Base + AI 74.50
Base + AI + AC 75.31
English
Base 76.44
Base + AI 76.70
Base + AI + AC 77.00
Table 7: Results on auto parse trees. Base mean-
s the baseline system, +AI meaning predcates-
related features added in AI, + AC meaning joint
module added.
Methods F
1
(%)
Chinese
Xue(2008) 71.90
Sun et al.(2009) 74.12
Ours 75.31
English
Surdeanu and Turmo(2005) 76.46
Ours 77.00
Table 8: Comparison with Other Methods
in F1. Furthermore, after incorporating the joint
module, the performance goes up to 75.31%, 1.3
points higher than the base model. We obtain sim-
ilar observations in experiments on English Prop-
Bank, but due to reasons illustrated in Subsection
5.3, the performance of our method is slight better
than the base model.
We compare our method with others and the re-
sults are shown in Table 8. In Chinese, Xue (2008)
and Sun et al. (2009) are pioneer works in Chinese
SRL. Our approach outperforms these approaches
by about 3.4 and 1.9 F1 points respectively. In
English SRL, we compare out method with Sur-
deanu and Turmo (2005) which is best result ob-
tained with single parse tree as the input in CON-
LL 2005 SRL evaluation. Our approach is better
than their approach which ignores the relation of
multiple predicates? SRL.
6 Discussion and Analysis
In this section, we discuss some case studies that
illustrate the advantages of our model. Some ex-
amples from our experiments are shown in Table
9. In example (1), the argument is a preposition-
al phrase ?( n? ] t I? Y? ? ??
(at the same time of compulsory education) and
shared by two predicates ??0? (witness) and ?i
'? (expand). In the corpus, a prepositional phrase
is commonly labeled as ARGM-LOC and ARGM-
TMP. Thus, the base model labeled the argument
into these classes but one as ARGM-LOC, another
as ARGM-TMP. Unfortunately, ARGM-LOC for
??0? (witness) is wrong while our joint model
outputs both correct answers, which benefits from
the role transition feature. From Table 1, we can
see that the role transition between ARGM-TMP
and ARGM-LOC is impossible, which lowers the
score of candidates containing both ARGM-LOC
and ARGM-TMP in the joint model. Thus, the
joint model is more likely to output the gold can-
didate.
In example (2), the argument is ?w? ?:
:? (Hailar Airport) and shared by two predicates
?i?? (expand) and ?:? (become). Because of
the high similarity of the features in the base mod-
el, the argument for both predicates is classified
into the same class ARG0, but the label for ?i??
(expand) is wrong. Nevertheless, our joint mod-
el obtains both correct labels, which benefits from
the global features. After searching the training
data, we find some similar examples to this one,
such as ?0? ?L ? ? i? ?  l
?? (The railway operation mileage is expanded to
120 kilometers), in which ?0? ?L ?? (the
railway operation mileage) is labeled as ARG1 for
?i?? (expand) but ARG0 for ??? (to). We think
these samples provide evidence for our joint mod-
el while these information has not been captured
by the base model.
In example (3), the argument is ??? ?
?? ? ? ' ??? (a large group with high
reputation) and shared by predicates ??U? (de-
velop) and ?:? (become). Different from the
above cases in which only one label is wrong in
the base model, both labels for ??U? (develop)
and ?:? (become) are misclassified by the base
model. However, our method still gets correct an-
swers for both predicates, which also benefits from
the global features.
7 Related work
Our work is related to semantic role labeling
and discriminative reranking. In this section, we
briefly review these two types of work.
On Semantic Role Labeling
Gildea and Jurafsky (2002) first presented a sys-
tem based on a statistical classifier which is trained
on a hand-annotated corpora FrameNet. In their
pioneering work, they used a gold or autoparsed
syntax tree as the input and then extracted vari-
ous lexical and syntactic features to identify the
370
Examples Base Ours
1. (n?]tI?Y???-I
L