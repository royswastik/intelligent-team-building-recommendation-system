Proceedings of NAACL HLT 2007, pages 572?579,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Computing Semantic Similarity between Skill Statements for Approxi-
mate Matching 
 
 
Feng Pan Robert G. Farrell 
USC Information Sciences Institute IBM T. J. Watson Research Center 
Marina del Rey, CA 90292 Hawthorne, NY 10532 
pan@isi.edu robfarr@us.ibm.com 
 
 
 
 
Abstract 
This paper explores the problem of com-
puting text similarity between verb 
phrases describing skilled human behav-
ior for the purpose of finding approximate 
matches. Four parsers are evaluated on a 
large corpus of skill statements extracted 
from an enterprise-wide expertise taxon-
omy. A similarity measure utilizing com-
mon semantic role features extracted from 
parse trees was found superior to an in-
formation-theoretic measure of similarity 
and comparable to the level of human 
agreement. 
1 Introduction 
Knowledge-intensive industries need to become 
more efficient at deploying the right expertise as 
quickly and smoothly as possible, thus it is desired 
to have systems that can quickly match and deploy 
skilled individuals to meet customer needs. The 
searches in most of the current matching systems 
are based on exact matches between skill state-
ments. However, exact matching is very likely to 
miss individuals who are very good matches to the 
job but didn?t select the exact skills that appeared 
in the open job description.  
It is always hard for individuals to find the per-
fect skills to describe their skill sets. For example, 
an individual might not know whether to choose a 
skill stating that refers to ?maintaining? a given 
product or ?supporting? it or whether to choose a 
skill about maintaining a ?database? or about 
maintaining ?DB2?. Thus, it is desirable for the job 
search system to be able to find approximate 
matches, instead of only exact matches, between 
available individuals and open job positions. More 
specifically, a skill similarity computation is 
needed to allow searches to be expanded to related 
skills, and return more potential matches.  
In this paper, we present our work on develop-
ing a skill similarity computation based upon se-
mantic commonalities between skill statements. 
Although there has been much work on text simi-
larity metrics (Lin, 1998a; Corley and Mihalcea, 
2005), most approaches treat texts as a bag of 
words and try to find shared words with certain 
statistical properties based on corpus frequencies. 
As a result, the structural information in the text is 
ignored in these approaches. We will describe a 
new semantic approach that takes the structural 
information of the text into consideration and 
matches skill statements on corresponding seman-
tic roles. We will demonstrate that it can outper-
form standard statistical text similarity techniques, 
and reach the level of human agreement.   
In Section 2, we first describe the skill state-
ments we extracted from an enterprise-wide exper-
tise taxonomy. In Section 3, we describe the 
performance of a standard statistical approach on 
this task. This motivates our semantic approach of 
matching skill statements on corresponding seman-
tic roles. We also compare and evaluate the per-
formance of four natural language parsers (the 
Charniak parser, the Stanford parser, the ESG 
parser, and MINIPAR) for the purpose of our task. 
An inter-rater agreement study and evaluation of 
572
our approach will be presented in Section 4. We 
end with a discussion and conclusion.  
2 Skill Statements 
An expertise taxonomy is a standardized, enter-
prise-wide language and structure to describe job 
role requirements and people capabilities (skill 
sets) across a corporation. In the taxonomy we util-
ize for this study, skills are associated with job 
roles. The taxonomy has 10667 skills. Each skill 
has a title, for example, ?Advise BAAN eBusiness 
ASP.? We refer to this title as the skill statement.  
The official taxonomy update policies require 
that skill statements be verb phrases using one of 
18 valid skill verbs (e.g., Advise, Architect, Code, 
Design, Implement, Sell, and Support).  
3 Computing Semantic Similarities be-
tween Skill Statements 
In this section, we first explain a statistical infor-
mation-theoretic approach we used as a baseline, 
and show examples of how it performs for our 
task. The error analysis of this approach motivates 
our semantic approach that takes the structural in-
formation of the text into consideration. In the re-
mainder of this section, we describe how we 
extract semantic role information from the syntac-
tic parse trees of the skill statements. Four natural 
language parsers are compared and evaluated for 
the purpose of our task. 
3.1 Statistical Approach 
In order to compute semantic similarities between 
skill statements, we first adopted one of the stan-
dard statistical approaches to the problem of com-
puting text similarities based on Lin?s information-
theoretic similarity measure (Lin 1998a). Lin de-
fined the commonality between A and B as  
)),(( BAcommonI  
where common(A, B) is a proportion that states the 
commonalities between A and B and where the 
amount of information in proposition s is 
)(log)( sPsI ?=  
The similarity between A and B is then defined as 
the ratio between the amount of information 
needed to state the commonality of A and B and 
the information needed to fully describe A and B: 
)),((log
)),((log
),(
BAndescriptioP
BAcommonP
BASim =   
In order to compute common(A,B) and descrip-
tion(A,B), we use standard bag-of-words features, 
i.e., unigram features -- the frequency of words 
computed from the entire corpus of the skill state-
ments. Thus common(A,B) is the unigrams that 
both skill statements share, and description(A,B) is 
the union of the unigrams from both skill state-
ments.  
The words are stemmed first so that the words 
with the same root (e.g., managing & manage-
ment) can be found as commonalities between two 
skill statements. A stop-word list is also used so 
that the commonly used words in most of the docu-
ments (e.g., the, a) are not used as features. A for-
mal evaluation of this approach will be presented 
in Section 4 where the similarity between 75 pairs 
of skill statements will be evaluated against human 
judgments, but we discuss some examples here.  
In order to see how to improve Lin?s statistical 
similarity measure, we examine sample skill state-
ment pairs which achieve high similarity scores 
from Lin?s measure but were rated consistently as 
dissimilar by human subjects in our evaluation. 
Here are two examples:  
1. Advise Business Knowledge of CAD function-
ality for FEM 
Advise on Business Knowledge of Process for 
FEM 
2. Advise on Money Market 
Advise on Money Center Banking 
In these two examples, although many words are 
shared between the two pairs of skill statements 
(Advise Business Knowledge of ... for FEM for the 
first pair; Advise on Money for the second pair), 
they are not similar to human judges. We conjec-
ture that this judgment of dissimilarity is due to the 
differences between the key components of the 
skill statements (CAD functionality vs. Process in 
the first pair; Money Market vs. Money Center 
Banking in the second pair). 
This kind of error is common for most statistical 
approaches to the problem, where common infor-
mation is computed without considering the struc-
tural information in the text. From the above 
examples, we can see that the similarity computa-
tion would be more accurate if the verb phrases 
match on corresponding semantic roles, instead of 
573
matching words from any location in the skill 
statements. By identifying semantic roles, we can 
provide more weights to those semantic roles criti-
cal for our task, i.e., the key components of the 
skill statements. 
3.2 Identifying and Assigning Semantic 
Roles 
The following example shows the kind of semantic 
roles we want to be able to identify and assign.  
[action Apply] [theme Knowledge of [concept IBM E-
business Middleware]] to [purpose PLM Solu-
tions] 
In this example, ?Apply? is the ?action? of the 
skill; ?Knowledge of IBM E-business Middle-
ware? is the ?theme? of the skill, where the ?con-
cept? semantic role (IBM E-business Middleware) 
specifies the key component of the skill require-
ment and is the most important role for skill 
matching; ?PLM Solutions? is the ?purpose? of the 
skill. 
Our goal was to extract all such semantic role 
patterns for all the skill statements, and match on 
corresponding semantic roles. Although there ex-
ists some automatic semantic role taggers (Gildea 
and Jurafsky, 2002; Giuglea and Moschitti, 2006), 
most of them were trained on PropBank (Palmer 
et. al., 2005) and/or FrameNet (Johnson et. al., 
2003), and perform much worse in other corpora 
(Pradhan et. al., 2004). Our corpus is from a very 
different domain (information technology) and 
there are many domain-specific terms in the skill 
statements, such as product names, company 
names, and company-specific nomenclature for 
product offerings. Given this, we would expect 
poor performance from these automatic semantic 
role taggers. Moreover, the semantic role informa-
tion we need to extract is more detailed and deeper 
than most of the automatic semantic role taggers 
can identify and extract (e.g., the ?concept? role 
embedded within the ?theme? role).  
We developed a specialized parser that extracts 
semantic role patterns from each of the 18 skill 
verbs. This semantic role parser can achieve a 
much higher performance than the general-purpose 
semantic role taggers. The inputs needed for the 
semantic role parser are syntactic parse trees gen-
erated by a natural language parse of the original 
skill statements.  
3.3 Preprocessing for Parsing 
We first used the Charniak parser (2000) to parse 
the original skill statements. However, among all 
the 10667 skill statements, 1217 were not parsed as 
verb phrases, leading to very poor performance. 
After examining the error cases, we found that ab-
breviations are used widely in the skill statements. 
For example, 
Advise Solns Supp Bus Proc Reeng for E&E 
Eng Procs 
These abbreviations made the system unable to 
determine the part of speech of some words, result-
ing in incorrect parses. Thus, the first step of the 
preprocessing was to expand abbreviations.  
There were 225 valid abbreviations already 
identified by the expertise taxonomy team. How-
ever, we found many abbreviations that appeared 
in the skill statements but were not listed there. 
Since most abbreviations are not words found in a 
dictionary, in order to find the abbreviations that 
appear frequently in the skill statements, we first 
found all the words in the skill statements that 
were not in WordNet (Miller, 1990). We then 
ranked them based on their frequencies, and manu-
ally identified high frequency abbreviations. Using 
this approach, we added another 187 abbreviations 
to the list (a total of 412).  
From the error cases, we also found that many 
words were mistagged as proper nouns, For exam-
ple, ?Technically? in  
Advise Technically for Simulation 
was parsed as a proper noun. We realized the rea-
son for this error was that all the words, except for 
prepositions, are capitalized in the original state-
ments and the parser tends to tag them as proper 
nouns. To solve this problem, we changed all the 
capitalized words to lower case, except for the first 
word and the acronyms (words that have all letters 
capitalized, e.g., IBM). After applying these two 
steps of preprocessing, we parsed the skill state-
ments again. This time, more than 200 additional 
skill statements were parsed as verb phrases after 
the preprocessing. 
When we examined the error cases more 
closely, we found the errors occur mostly when the 
skill verbs can be both a noun and a verb (e.g., de-
sign, plan). In those cases, the parser may parse the 
entire statement as one noun phrase, instead of a 
verb phrase. In order to disambiguate such cases, 
574
we added a subject (?Employees?) to all the skill 
statements to convert them into full sentences. Af-
ter applying this additional step of preprocessing, 
we parsed the skill statements again. This time, 
only 28 skill statements were not parsed as sen-
tences containing verb phrases, a significant im-
provement. The remaining errors were due to the 
use of some words as skill verbs, e.g., ?architect?1, 
not recognized as verbs by the parser. 
3.4 Parser Evaluation and Comparison 
While the Charniak parser performed well in our 
initial verb phrase (VP) test, we decided to com-
pare the Charniak parser?s performance with other 
parsers. For this evaluation, we compared it with 
the Stanford parser, the ESG parser, and 
MINIPAR.   
The Stanford parser (Klein and Manning, 
2003) is an unlexicalized statistical syntactic parser 
that was trained on the same corpus as the 
Charniak parser (the Penn TreeBank). Its parse tree 
has the same structure as the Charniak parser. 
The ESG (English Slot Grammar) parser 
(McCord, 1980) is a rule-based parser based on the 
slot grammar where each phrase has a head and 
dependent elements, and is also marked with a syn-
tactic role.  
MINIPAR (Lin, 1998b), as a dependency 
parser, is very similar to the ESG parser in terms of 
its output. It represents sentence structures as a set 
of dependency relationships between head words. 
Since our purpose is to use the syntactic parses 
as inputs to extract semantic role patterns, the cor-
rectness of the bracketing of the parses and the 
syntactic labels of the phrases (e.g., NP, VP, and 
PP) are the most important information for our pur-
poses, whereas the POS (Part-Of-Speech) labels of 
individual words (e.g., nouns vs. proper nouns) are 
not that important (also, there are too many do-
main-specific terms in our data). Thus, our evalua-
tion of the parses is only on the correctness of the 
bracketing and the syntactic labels of the phrases, 
not the correctness of the entire parse. For our task, 
the correctness of the prepositional phrase attach-
ment is especially important for extracting accurate 
semantic role patterns (Gildea and Jurafsky, 2002). 
For example, for the sentence 
                                                          
1 ?Architect? has no verb sense in WordNet and many other 
dictionaries, but it does have a verb sense in the Oxford Eng-
lish Dictionary (http://dictionary.oed.com/). 
Apply Knowledge of IBM E-business Middle-
ware to PLM Solutions. 
the correct bracketing should be 
Apply [Knowledge [of [IBM E-business Mid-
dleware]]] [to [PLM Solutions]].  
Thus the parser needs to correctly attach ?of IBM 
E-business Middleware? to ?Knowledge? and at-
tach ?to PLM Solutions? to ?Apply?, not ?Knowl-
edge?. 
To evaluate the performance of the parsers, we 
randomly picked 100 skill statements from our cor-
pus, preprocessed them, and then parsed them us-
ing the four different parsers. We then evaluated 
the parses using the above evaluation measures. 
The parses were rated as correct or incorrect. No 
partial score was given. Figure 1 shows the evalua-
tion results. The error analysis reveals four major 
sources of error for all the parsers, most of which 
are specific to the domain we are working on: 
(1) Many domain specific terms and acronyms. 
For example, ?SAP? in ?Employees advise on 
SAP R/3 logistics basic data.? was always 
tagged as a verb by the parsers.  
(2) Many long noun phrases. For example, ?Em-
ployees perform JD edwards foundation suite 
address book.?  
(3) Some specialized use of punctuation. For ex-
ample, ?Employees perform business transpor-
tation consultant-logistics.sys.?  
(4) Prepositional phrase attachment can be diffi-
cult. For example, in ?Employees apply IBM 
infrastructure knowledge for IDBS?, ?for 
IDBS? should attach to ?apply?, but many 
parsers mistakenly attach it to ?IBM infrastruc-
ture knowledge?. 
0%
10%
20%
30%
40%
50%
60%
70%
80%
Charniak Stanford ESG MINIPAR
 
Figure 1. An Evaluation of Four Parsers on the 
Task of Parsing Human Skill-related Verb Phrases  
We noticed that MINIPAR performed much 
worse compared with the other parsers. The main 
575
reason is that it always parses the phrase ?VERB 
knowledge of Y? (e.g., ?Employees apply knowl-
edge of web technologies.?) incorrectly -- the parse 
result always mistakenly attaches ?of Y? (e.g., of 
web technologies) to the VERB (e.g., apply), not 
?knowledge?. Since there were so many of phrases 
in the test set and in the corpus, this kind of error 
significantly reduced the performance for our task. 
These kinds of errors on prepositional phrase at-
tachment in MINIPAR were also mentioned in 
(Pantel and Lin, 2000).   
From the evaluation and comparison results we 
can see that the Charniak parser performs the best 
for our task among all the four parsers. This result 
is consistent with a more thorough evaluation 
(Swanson and Gordon, 2006) on a different corpus 
with a set of different target verbs, which showed 
the Charniak parser performed the best among 
three parsers (including the Stanford parser and 
MINPAR) for labeling semantic roles. We note 
that although the ESG parser performed a little 
worse than the Charniak parser, its parses contain 
much richer syntactic (e.g., subject, object) and 
semantic (e.g., word senses) slot-filling informa-
tion, which can be very useful to many natural lan-
guage applications.  
3.5 Extracted Semantic Role Patterns 
From the parse trees generated by the Charniak 
parser, we first automatically extracted patterns for 
each of the 18 skill verbs (e.g., ?Advise on NP for 
NP?), and then we manually identified the seman-
tic roles. For example, the semantic role patterns 
identified for the skill verb ?Advise? are: 
? Advise [Theme] (for [Purpose]) 
? Advise (technically) on/about [Theme] (for 
[Purpose]) 
? Advise clients/customers/employees/users 
on/regarding [Theme]  
The corpus also contains embedded sub-semantic-
role patterns, for example, for the ?Theme? role we 
extracted the following sub-patterns: 
? (application) knowledge of/for [Concept] 
? sales of [Concept] 
? (technical) implementation of [Concept]   
We have extracted and identified a total of 74 such 
semantic role patterns from the skill statements. 
4 Evaluation 
In order to evaluate the two approaches (semantic 
role parsing and statistical) to computing semantic 
similarity of skill statements in our domain, we 
first conducted an experiment to evaluate how hu-
mans agree on this task, which also provides us 
with an upper bound accuracy for the task. 
4.1 Inter-Rater Agreement and Upper 
Bound Accuracy 
To assess inter-rater agreement, we randomly se-
lected 75 skill pairs from the expertise taxonomy. 
Since random pairs of verbs would have little or no 
similarity, we selected skill pairs that share the 
same job role, or same secondary or primary job 
category, or from across the entire expertise taxon-
omy. 
These 75 skill pairs are then given to three raters 
to independently judge their similarities on a 5 
point scale from 1 as very similar to 5 as very dis-
similar. Since this 5 point scale is very fine-
grained, we also converted the judgments to a 
more coarse-grained measure -- binary judgment: 1 
and 2 count as similar; 3-5 as not similar. 
The metric we used is the kappa statistic (Car-
letta, 1996), which factors out the agreement that is 
expected by chance: 
)(1
)()(
EP
EPAP
?
?=? 
 
where P(A) is the observed agreement among the 
raters, and P(E) is the expected agreement, i.e., the 
probability that the raters agree by chance. 
Since the judgment on the 5 point scale is ordi-
nal data, the weighted kappa statistic is used to 
take the distance of disagreement into considera-
tion (e.g., the disagreement between 1 and 2 is 
smaller than that between 1 and 5). 
The inter-rater agreement results for both the 
fine-grained and coarse-grained judgments are 
shown in Table 1. In general, a kappa value above 
0.80 represents perfect agreement, 0.60-0.80 repre-
sents significant agreement, 0.40-0.60 represents 
moderate agreement, and 0.20-0.40 is fair agree-
ment (Chklovski and Mihalcea, 2003). We can see 
that the agreement on the fine-grained judgment is 
moderate, whereas the agreement on the coarse-
grained (binary) judgment is significant. 
 Fine-Grained  Coarse-Grained 
Kappa 0.412 0.602 
Table 1. Inter-Rater Agreement Results. 
576
From the inter-rater agreement evaluation, we 
can also get an upper bound accuracy for our task, 
i.e., human agreement without factoring out the 
agreement expected by chance (i.e., P(A) in the 
kappa statistic). The average P(A) for the coarse-
grained (binary) judgment is 0.81, and that consti-
tutes the upper bound accuracy for our task.   
4.2 Evaluation of the Statistical Approach 
We use the 75 skill pairs as test data to evaluate 
our semantic similarity approach against human 
judgments. Considering the reliability of the data, 
only the coarse-grained (binary) judgments are 
used. The gold standard is obtained by majority 
voting from the three raters, i.e., for a given skill 
pair, if two or more raters judge it as similar, then 
the gold standard answer is ?similar?, otherwise it 
is ?not similar?.  
We first evaluated Lin?s statistical approach de-
scribed in Section 3.1. Among 75 skill pairs, 53 of 
them were rated correctly according to the human 
judgments, that is, 70.67% accuracy. The error 
analysis shows that many of the errors can be cor-
rected if the skills are matched on their correspond-
ing semantic roles. We then evaluated the utility of 
the extracted semantic role information to see 
whether it can outperform the statistical approach. 
4.3 Evaluation of Semantic Role Matching 
Approach 
For simplicity, we will only report on evaluating 
semantic role matching on the "concept" role that 
specifies the key component of the skills, as intro-
duced in Section 3.2. 
There are at least two straightforward ways of 
performing semantic role matching for the skill 
similarity computation: 1) match on the entire se-
mantic role; 2) match on the head nouns only. But 
both have their drawbacks: the first approach is too 
strict and will miss many similar skill statements; 
the second approach may not only miss the similar 
skill statements, e.g., 
Perform [Web Services Planning]2   
Perform [Web Services Assessment]    
but also misclassify dissimilar ones as similar, e.g., 
                                                          
2 The ?concept? role is identified with brackets, and the head 
nouns are italic. 
Advise about [Async Transfer Mode (ATM) 
Solutions]   
Advise about [CTI Solutions] 
In order to solve these problems, we used a simple 
matching criterion from Tversky (1977). The simi-
larity of two texts t1 and t2 is determined by: 
      Similarity(t1, t2) =  
         
21
21
 tand in t features  total#
 ) tand between t featurescommon  (#  2?
 
This equation states that two texts are similar if 
shared features are a large percentage of the total 
features. We set a threshold of 0.5, requiring that at 
least 50% of the features be shared. We apply this 
criterion to the text contained in the ?concept? role.  
The words in the calculation are preprocessed 
first: abbreviations are expanded, stop-words are 
excluded (e.g., the and of don't count as shared 
words), and the remaining words are stemmed 
(e.g., manager and management are counted as 
shared words), as was done in our previous infor-
mation-theoretic approach. Words connected by 
punctuation (e.g., e-business, software/hardware) 
are treated as separate words. For example, 
Advise on [Field/Force Management] for Tele-
com 
Apply Knowledge of [Basic Field Force Auto-
mation]         
The shared words between the two ?concept? roles 
(bracketed) are ?Field? and ?Force?, and their 
shared percentage is (2*2)/7 = 57.14% > 50%, so 
they are similar. 
We have also evaluated this approach on our test 
set with the 75 skill pairs. Among 75 skill pairs, 60 
of them were rated correctly (i.e., 80% accuracy), 
which significantly outperforms the statistical ap-
proach, and is very close to the upper bound accu-
racy, i.e., human agreement (81%), as shown in 
Figure 2. 
64.00%
66.00%
68.00%
70.00%
72.00%
74.00%
76.00%
78.00%
80.00%
82.00%
Lin's Information-Theoretic Metric Semantic Role Matching Human Agreement
 
Figure 2. Evaluation on Semantic Similarity be-
tween Skill Statements 
577
The difference between this approach and Lin?s 
information content approach is that this computa-
tion is local -- no corpus statistics is used. Also, 
using this approach, it is easier to set an intuitive 
threshold (e.g., 50%) for a classification problem 
(e.g., similar or not for our task). With this ap-
proach, however, there are also cases that are 
mistagged as similar, for example, 
Apply Knowledge of [Basic Field Force Auto-
mation]  
Advise on [Sales Force Automation] 
Although ?Field Force Automation? and ?Sales 
Force Automation? seem similar on their surface 
form, they are two quite different concepts. Deeper 
domain knowledge (such as an ontology) is needed 
to distinguish such cases. 
5 Discussion  
We have also investigated several approaches to 
improving the semantic role text similarity meas-
ure we described. One approach is to also consider 
similarities between skill verbs. In this example:  
Implement Domino Mail Manager 
Develop for Domino Mail Manager 
although the key components of the skill state-
ments (Domino Mail Manager) are the same, their 
skill verbs are different (implement vs. develop 
for). The skills required for ?implementing? a sys-
tem or software product are usually different from 
those required for ?developing for? the same sys-
tem or software product. This example shows that 
a semantic similarity computation between skill 
verbs is required to distinguishing such cases. 
Many approaches to the problem of 
word/concept similarities are based on taxonomies, 
e.g., WordNet. The simplest approach is to count 
the number of nodes on the shortest path between 
two concepts in the taxonomy (Quillian, 1972). 
The fewer nodes on the path, the more similar the 
two concepts are. The assumption for this shortest 
path approach is that the links in the taxonomy rep-
resent uniform distances. However, in most tax-
onomies, sibling concepts deep in the taxonomy 
are usually more closely related than those higher 
up. Different approaches have been proposed to 
discount the depth of the concepts to overcome the 
problem. Budanitsky and Hirst (2006) thoroughly 
evaluated six of the approaches (Hirst and St-
Onge, Leacock and Chodorow, Jiang and Conrath, 
Lin, Resnik, Wu and Palmer), and found that Jiang 
and Conrath (1997) was superior to the other ap-
proaches based on their evaluation experiments. 
For our task, we compared two approaches to 
computing skill verb similarities: shortest path vs. 
Jiang and Conrath. Since the words are compared 
based on their specific senses, we first manually 
assigned one most appropriate sense for each of the 
18 skill verbs from WordNet. We then used the 
library developed by Pedersen et al (2004) to 
compute their similarity scores. 
Table 2 shows the top nine pairs of skill verbs 
with the highest similarity scores from the two ap-
proaches. We can see that the two approaches 
agree on the top four pairs, but disagree on the rest 
in the list. One intuitive example is the pair ?Lead? 
and ?Manage? which is ranked the 5th by the Jiang 
and Conrath approach but ranked the 46th by the 
shortest path approach. It seems that the Jiang and 
Conrath approach matches better with our human 
intuition for this example. While we didn?t com-
pare these results with human performance, in gen-
eral most of the similar skill verb pairs listed in the 
table don?t look very similar for our domain. This 
may be due to the fact that WordNet is a general-
purpose taxonomy -- although we have already 
selected the most appropriate sense for each verb, 
their relationship represented in the taxonomy may 
still be quite different from the relationship in our 
domain. A domain-specific taxonomy for skill 
verbs may improve the performance. The other 
reason may be due to the structure of WordNet?s 
verb taxonomy, as mentioned in (Resnik and Diab, 
2000), which is considerably wider and shallower 
than WordNet?s noun taxonomy. A different verb 
lexicon, e.g., VerbNet (Kipper et al, 2000), can be 
explored. 
  
Shortest Path Jiang and Conrath 
Apply Use Apply Use 
Design Plan Design Plan 
Apply Implement Apply Implement 
Implement Use Implement Use 
Analyze  Apply Lead Manage 
Analyze Perform Apply Support 
Analyze Support Support Use 
Analyze Use Apply Sell 
Perform Support Sell Use 
? ? ? ? 
Table 2. Top Similar Skill Verb Pairs 
578
6 Conclusion 
In this paper, we have presented our work on a se-
mantic similarity computation for skill statements 
in natural language. We compared and evaluated 
four different natural language parsers for our task, 
and matched skills on their corresponding semantic 
roles extracted from the parse trees generated by 
one of these parsers. The evaluation results showed 
that the skill similarity computation based on se-
mantic role matching can outperform a standard 
statistical approach and reach the level of human 
agreement.  
The extracted semantic role information can also 
be incorporated into the standard statistical ap-
proaches as additional features. One way is to give 
higher weights to those semantic role features 
deemed most important. This approach has 
achieved a high performance for a text categoriza-
tion task when combining extracted keywords with 
the full text (Hulth and Megyesi, 2006). 
We have shown that good results can be 
achieved for a domain-specific text matching task 
by performing a simple word-based feature com-
parison on corresponding structural elements of 
texts.  We have shown that the structural elements 
of importance can be identified by domain-specific 
pattern analysis of corresponding parse trees. We 
believe this approach can generalize to other do-
mains where phrases, sentences, or other short 
texts need to be compared. 
Acknowledgements  
The majority of this work was performed while the 
first author was a summer intern at IBM T. J. Wat-
son Research Center in Hawthorne, NY. Thanks to 
Yael Ravin and Jennifer Lai for supporting this 
work, Brian White for his help on the software, 
Michael McCord for assistance with the IBM ESG 
parser, and the IBM Expertise Taxonomy team for 
letting us use their data.    
References  
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based Measures of Lexical Semantic Relatedness. 
Computational Linguistics. 32(1):13-47. 
J. Carletta. 1996. Assessing agreement on classification 
tasks: the kappa statistic. Computational Linguistics, 
22(2):249?254. 
E. Charniak. 2000. A maximum-entropy-inspired 
parser. In Proceedings of NAACL. 
T. Chklovski and R. Mihalcea. 2003. Exploiting Agree-
ment and Disagreement of Human Annotators for 
Word Sense Disambiguation. In Proceedings of 
RANLP. 
D. Gildea and D. Jurafsky. 2002. Automatic labeling of 
semantic roles. Computational Linguistics, 28(3): 
245 ? 288. 
A. Giuglea and A. Moschitti. 2006. Semantic Role La-
beling via FrameNet, VerbNet and PropBank. In 
Proceedings of COLING-ACL. 
A. Hulth and B. B. Megyesi. 2006. A Study on Auto-
matically Extracted Keywords in Text Categoriza-
tion. In Proceedings of COLING-ACL. 
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity 
based on corpus statistics and lexical taxonomy. In 
Proceedings of ROCLING X. 
C. Johnson, M. Petruck, C. Baker, M. Ellsworth, J. Rup-
penhofer, and C. Fillmore. 2003. Framenet: Theory 
and practice. Berkeley, California. 
K. Kipper, H. T. Dang, M. Palmer. 2000. Class-Based 
Construction of a Verb Lexicon. In Proceedings of 
AAAI. 
D. Klein and C. D. Manning. 2003. Accurate Unlexical-
ized Parsing. In Proceedings of ACL. 
D. Lin. 1998a. An information-theoretic definition of 
similarity. In Proceedings of ICML. 
D. Lin. 1998b. Dependency-based evaluation of 
MINIPAR. In Proceedings of the Workshop at LREC 
on The Evaluation of Parsing Systems. 
M. C. McCord. 1980. Slot grammars. Computational 
Linguistics, 6: 31-43. 
G. A. Miller. 1990. WordNet: an On-line Lexical Data-
base. International Journal of Lexicography 3(4). 
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The 
proposition bank: An annotated corpus of semantic 
roles. Computational Linguistics, 31(1). 
P. Pantel and D. Lin. 2000. An unsupervised approach 
to prepositional phrase attachment using contextually 
similar words. In Proceedings of ACL. 
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. 
Wordnet::similarity - measuring the relatedness of 
concepts. In Proceedings of AAAI, Intelligent Sys-
tems Demonstration. 
S. Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. 
Jurafsky. 2004. Shallow Semantic Parsing using 
Support Vector Machines. In Proceedings of 
HLT/NAACL. 
M. R. Quillian. 1972. Semantic Memory, Semantic In-
formation Processing. Semantic information process-
ing, Cambridge. 
P. Resnik and M. Diab. 2000. Measuring verb similar-
ity. In Proceedings of COGSCI. 
R. Swanson and A. S. Gordon. 2006. A Comparison of 
Alternative Parse Tree Paths for Labeling Semantic 
Roles. In Proceedings of COLING/ACL. 
A. Tversky. 1977. Features of Similarity, Psychological 
Review, vol. 84, no. 4, pages 327-352. 
579
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 301?306,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
A Robotic World Model Framework Designed to Facilitate  Human-robot Communication   Meghann Lomas, E. Vincent Cross II, Jonathan Darvill, R. Christopher Garrett, Michael Kopack, and Kenneth Whitebread Lockheed Martin Advanced Technology Laboratories 3 Executive Campus, Suite 600, Cherry Hill, NJ  08002 1 856.792.9681 {mlomas, ecross, jdarvill, rgarrett, mkopack, kwhitebr}@atl.lmco.com       Abstract We describe a novel world model frame-work designed to support situated human-robot communication through improved mutual knowledge about the physical world. This work focuses on enabling a  robot to store and use semantic information from a human located in the same envi-ronment as the robot and respond using human-understandable terminology. This facilitates information sharing between a robot and a human and subsequently pro-motes team-based operations. Herein, we present motivation for our world model, an overview of the world model, a discussion of proof-of-concept simulations, and future work. 
1 Introduction As robots become more ubiquitous, their interac-tions with humans must become more natural and intuitive for humans. One of the main challenges to natural human-robot interaction is the ?language barrier? between humans and robots. While a con-siderable amount of work has gone into making robot dialogue more human-like (Fong et al, 2005), the content of the conversation is frequently highly scripted.  An essential precondition to intuitive human-robot dialogue is the establishment of a common 
ground of understanding between humans and ro-bots (Kiesler, 2005). Operators expect information to be presented in a way such that they can connect it with their own world information. This implies a need for robots to be capable of expressing infor-mation in human-understandable terms. By shifting some responsibility for establishing common ground to robots, interactions between humans and robots become considerably more natural for  humans by reducing the need for humans to ?trans-late? the robot?s information. Ultimately, the robot?s world model is a key contributor to the ?language barrier.? Because  humans and robots view and think about the world differently (having different ?sensors? and ?pro-cessing algorithms?), they subsequently have different world representations (Figure 1). Humans tend to think of the world as objects in space, while robotic representations vary based on sensors, but are typically coordinate-based representations of  
  Figure 1. Humans and robots think and subse-quently communicate about the world using different terminology. 
301
free and occupied space. This presents a consider-able challenge when humans want to communicate naturally with robots. For robots to become active partners for humans, they must be better able to share the information they have gathered about the world. To that end, we have begun to address the ?language barrier? by focusing on how information is stored by the robot. We have developed a novel world model repre-sentation that will enable a robot to merge information communicated by its human team-mates with its own situational awareness data and use the resulting ?operating picture? to drive plan-ning and decision-making for navigation in unfamiliar environments. The ultimate aim of this research is to enable robots to communicate with humans and maintain an ?actionable awareness? of the environment. This provides a number of bene-fits: ? Increased robot situational awareness. The robots will be able to learn about, store, and recall envi-ronmental information obtained from humans (or other robots). This can include information the  robot would be incapable of getting on its own,  either because it has not visited that region of the environment or because it is not capable of sens-ing that information. ? Increased human situational awareness. Humans will be able to receive information from robots in human-understandable terms. ? Reduced workload and training for human-robot interaction. Because robots will be able to com-municate in human-understandable terms, people will be able to interact with robots in ways that are more natural to humans. As a result, people will need fewer specialized interfaces to interact with robots and subsequently less training. ? Improved collaboration. Because people and ro-bots will be able to share information, the team will be able to operate more efficiently. Each team member will be able to contribute to team knowledge, which will allow for better planning. 2 World Model Overview Our world model framework was designed using several key principles: that information must be stored in both human-understandable terms and in a format usable by the robot; that information must be capable of being added, deleted, or modified during operations; and that the world model framework should be capable of integrating with a 
wide variety of external systems including pre-existing perception and planning systems.  To meet these principles, we have developed a layered framework that has internal functions for managing the world model and can integrate with external systems that use the world model, such as systems that populate it (perception systems) or use it to govern robotic actions (planning systems) (Figure 2).   
  Figure 2. We have developed a two-layer world model that integrates with external functions via translation functions to support the use of a variety of robotic capabilities.  Layered world models have shown promise for both robot navigation (Kuipers and Byun, 1991; Mataric, 1990) and for communication with  humans (Kennedy et al, 2007; Zender et al, 2008). Additionally, work in symbol grounding has supported robotic actions based on natural lan-guage interactions (Jacobsson et al, 2008, Hsiao et al, 2008). We leverage this research and extend it with the aim of supporting human-robot infor-mation sharing, robot navigation, and use by external systems. The bottom layer stores a spatiotemporal de-scription of the environment expressed in metrical terms. While there are several different possibili-ties for how this location-based information could be stored, we use a grid-based representation be-cause it is commonly used by existing planners (e.g., a cost map-based planner) and it allows for flexibility of information storage. While our framework supports the inclusion of an arbitrary number of grids, our experimental prototype uses three: an occupancy grid that stores free and occu-pied space, an ?object? grid, and a ?terrain? grid. The object grid stores the types of objects in each   
302
cell in ascending order of vertical position (e.g., ?table, plate, apple?). The terrain grid stores terrain type in each cell and may also have multiple  entries per cell (e.g., ?sand, boulders? or ?grass?). The top layer stores a relational description of the situation in semantic terms compatible with typical human descriptions of the physical envi-ronment. We use node-attribute structures in which objects (e.g., chairs, keys, trees, people, buildings) are represented as nodes that have a list of corre-sponding attributes (e.g., type, color, GPS coordinates, last time sensed, source of infor-mation, etc.). The nodes are connected by their relationships, which are human-understandable concepts (e.g., ?near? or ?above?). The graph form of the semantic layer supports the many, varied types of relationships between objects. There are many ways to express the physical relationships between objects, and humans often use ambiguous terms (Crangle et al, 1987). By establishing the semantic layer as a connected graph, we aim to support these ambiguous terms and ultimately pro-vide a way for the robot to process their meaning. In the top layer of the world model, we use an ontological representation to model the world, and include both an ?upper ontology? that provides a template for what information can be included in the world as well as an instantiated world built from experience. In addition to providing a frame-work that stores the list of all objects that could be present in the world, their associated attributes, and the possible relationship between the objects, this upper layer includes other information such as the robot?s goals and current high level plans and addi-tional information the robot has about itself or the world (e.g., domain theory or object affordances). An additional benefit of an ontology-based repre-sentation is that it supports the inclusion of objects despite uncertainty. If a perception algorithm can-not confidently identify an object but can classify it, this class of object can be stored in the semantic layer of the world model and refined as more  information is made available. To support a consistent, complete view of the world, translation functions translate the infor-mation between the layers and assimilation functions merge information within layers. These translation functions support symbol grounding and enable the robot to use both semantically-described information along with sensed data. The translation functions are a set of functions, each of 
which translates an attribute, for example, a color translation function that translates between RGB values and a semantic label. More interesting are the location-based translation functions, for exam-ple ?near A? translates to ?within 2 meters of A?s position.? This introduces uncertainty into the po-sition of the object and so we use a probabilistic approach for placing any unsensed (but described) object in the bottom layer. The location of the  object is updated once the object is sensed by the robot. The assimilation algorithms, which are also still in development, are built upon data fusion ideas because they merge data from multiple sources. Because a considerable amount of existing work has been done on integrating (assimilating) infor-mation at the sensor level, to date we have focused on assimilation in the semantic layer of our world model. We have developed heuristic-based algo-rithms that compare information stored in the world model with actively sensed information  (essentially creating a temporary world model of the area currently being sensed by the robot). Dur-ing operation, the robot?s sensor detects an object and outputs a vector of possible object classifica-tions. Each object classification has an associated confidence along with attributes of the object  including size, color, etc. The assimilation compo-nent pulls all objects within a prescribed radius of the newly sensed object?s location from the world model to compare them with the newly sensed  object. The assimilation algorithm starts with the object closest in position to the newly sensed ob-ject and stops comparing objects if an object is determined to be ?same as? the newly sensed  object or if all objects with the prescribed radius are compared and none match.  To compare our newly sensed object with one of the objects already in the world model, the  assimilation algorithm compares the object vectors, which contain the list and confidence in each ob-ject type and object attributes such as color, size, and location. Some attributes (like source of  information) are ignored in this calculation. To compare two objects, we compute the distance  between the object vectors. This distance is com-puted through a pairwise comparison of attributes in the vector lists. These distances are then weighted according to ?importance? in assimila-tion process, for example objects with similar type should be more likely to be merged than objects 
303
that only have similar color. We then sum the weighted distances; if sum is less than a prescribed threshold, we assume the objects are the same and then merge them. If not the same, the algorithm checks this object against the other objects within the radius and if none are found, adds the object as a new object. To merge objects, the algorithm merges the attribute vectors of the temporary ob-ject and the original object. Some parts of the vectors are averaged (e.g., color), some amalga-mated (e.g., data source), and some pick one of the values (e.g., pick most recent time). Additionally, because it is stored in the world model, we can  incorporate logic about the world to facilitate  assimilation (e.g., ?this object is immovable so it must not have changed position?). While this algo-rithm has served as an initial assimilation algorithm, we will continue researching and  designing assimilation algorithms to better support the uncertainty present in the sensing outputs (e.g., false positives). One of the key requirements of our world mod-el is that it be able to integrate with external robotic systems. To accomplish this, the world model layers integrate with external functions that serve as translators to existing (or future) func-tions. These external translation functions pull relevant information from the world model and present it in a form usable by a planner. For exam-ple, we have created a planning translator that takes the grids from the physical layer and produc-es a cost map for a ground robot (with set parameters), which can then be used by any cost map-based planner. 3 Proof-of-Concept Simulations To evaluate the feasibility of our world model framework, we performed several proof-of-concept simulations designed to both demonstrate and test the capabilities of our world model and subse-quently to help the design process. We created different environments using Player/Stage and ran the robot through two scenarios. In both scenarios, humans needed robotic assistance to escape from a burning building and communicated with the robot using natural language. In the first scenario, a  mobile robot was asked by a group of trapped peo-ple to unlock a door and alert them when the door was open. In the second scenario, two mobile  robots were tasked with searching for trapped  
people and coordinating with first responders. Be-cause the focus of the simulations was on evaluating the world model itself, we made the  assumption that the robot had both camera and LIDAR sensors and had processing algorithms  capable of outputting an object classification and a confusion matrix. We assumed the robot had both a speech processing and synthesis mechanism with which it could communicate verbally with people in the environment. We assumed the robot had a common A* planner that used a cost map represen-tation for planning.  The first scenario highlighted the ability for the robot to understand and use human-communicated information by adding a human-described object to its world model and planning based on this assimi-lated information. At the beginning of the scenario, a human described the location of a key (?near the desk in the room with one table and one desk?) and told the robot to open the locked east door. The human did not tell the robot to use the key to  unlock the door, instead the robot used object  affordances stored in its world model to establish a high-level plan of getting the key, then unlocking the door. When the human told the robot about the location of the key, the robot stored this location in the top layer and translated the object?s position down to the bottom layer using a probabilistic translation algorithm that placed the key in the bot-tom layer at the most likely position within a certain region (whose size and position corre-sponded to ?nearness?). The robot used a simple cost map-based planner to plan its movements and so the system created a cost map from all the rele-vant bottom layer information in a format used by a classic A* planner. As a result, this scenario showed that our world model enabled the robot to use information gathered by a human teammate and expressed in semantic terminology without a specially designed planner.  The second scenario illustrated the merits of our world model for responding to humans. In this scenario, once the robot had searched the environ-ment, it was asked a series of questions by a first responder including: ?How many people did you find?? and ?How do I get to the fire extinguisher?? The latter question was particularly interesting  because it forced the robot to describe a path in semantic terminology (as opposed to a list of way-points). The robot used information from its top layer to describe the path from the first responder?s 
304
current position to the fire extinguisher. This sce-nario highlighted the ability for the robot to produce human-understandable and useful infor-mation despite having gathered the information using its low-level sensors and planner. In both of the scenarios, the robot was given both instructions and information verbally from one or more of the people in the robot?s environ-ment. The robot stored this described information in the world model and merged it with the infor-mation the robot had gathered with its own sensors to form a cohesive view of the world. The robot then used both the described and sensed infor-mation to formulate a plan to accomplish its goals. At the end of the mission, the robot was asked questions about the environment and was able to answer using human understandable terminology.  In these simulations we were able to show the robot formulating a plan based on information it had not sensed by itself. Because the robot had on-ly a simple cost map-based planner, it was essential that the semantic information be translat-ed to the grid representations in the bottom layer. This allowed the planning translator to produce a cost map in the form expected by the planner.  We used these simulations to inform key design decisions including the need to have multiple grids in the bottom layer of the world model and to  incorporate object affordances in the semantic lay-er. Another key insight was that uncertainty must be included in the semantic layer and that it is an important element in semantic layer assimilation. 4 Conclusions and Future Work We have designed and developed a world model framework that supports situated information shar-ing between robots and humans. By integrating semantic and sensor-based terminology, we have enabled a robot to integrate information described in natural human terms with its own sensed infor-mation. In addition, we have shown how a robot with a standard A* planning algorithm can thereby plan and respond appropriately using information obtained in semantic terms.  Because this world model framework was  designed to support a variety of robotic operations and capabilities, there are many areas of potential future work. These include facilitating robotic dia-logue systems, developing reasoning systems that can use the semantic level information to predict 
certain aspects of the world model (such as how an event will affect the physical layout of the world or where an object will be in a certain amount of time), and enabling semantic-level planners that can perform high-level planning. To further improve the functionality supported by this world model framework, there are a num-ber of areas of future work within the framework itself. We are exploring the design changes needed to support modeling of dynamic objects and the types of assimilation algorithms that exist or need to be developed to truly integrate tracks generated by external perception systems into our world model. We are also looking into how to better  reason about spatial relationships, particularly those that are only true when described from a spe-cific vantage point. Additionally, we would like to improve the translation algorithms by exploring additional scenarios and determining what mecha-nisms are needed. In the area of multi-robot coordination, we want to explore physical layer as-similation, which includes the ability to align reference frame for heterogeneous robots. Finally, we would also like to apply our world model on multiple real robots with speech systems and eval-uate the world model in a series of real-world operations. References  Terrence W. Fong, Illah Nourbakhsh, Robert Ambrose, Reid Simmons, Alan Schultz, and Jean Scholtz. The peer-to-peer human-robot interaction project. AIAA Space, 2005.  S. Kiesler. Fostering common ground in human-robot interaction. Robot and Human Interactive Communi-cation Proceedings. ROMAN 2005. The 14th IEEE International Workshop. Nashville, TE. Aug 2005. Benjamin Kuipers and Yung-Tai Byun. A robot explo-ration and mapping strategy based on a semantic hierarchy of spatial representation. Journal of Robot-ics and Autonomous Systems, 8:47?63, 1991. Maja Mataric. A distributed model for mobile robot  environment-learning and navigation. Technical Re-port, MIT Artificial Intelligence Laboratory, 1990. William G. Kennedy, Magdalena D. Bugajska, Matthew Marge, William Adams, Benjamin R. Fransen,  Dennis Perzanowski, Alan C. Schultz, and J. Gregory Trafton. Spatial representation and reasoning for  human-robot collaboration. In Proceedings of the 
305
Twenty-Second Conference on Artificial Intelli-gence, 2007. C. Crangle, P. Suppes, and S. Michalowski. Types of verbal interaction with instructable robots. In Pro-ceedings of the Workshop on Space Telerobotics, Vol 2, 1987.  H. Zender, O. Martinez Mozos, P. Jenselt, G.-J. M. Kruijff, and W. Burgard. Conceptual Spatial Repre-sentations for Indoor Mobile Robots. Robotics and Autonomous Systems, Special Issue ?From Sensors to Human Spatial Concepts.? Vol. 56, Issue 6. pp. 493-502. Elsevier. June 2008. 
H. Jacobsson, N. Hawes, G-J. Kruijff, J. Wyatt, Cross-modal Content Binding in Information-Processing Architectures. Proceedings of the 3rd ACM/IEEE  International Conference on Human-Robot Interac-tion (HRI). March 2008. Amsterdam, The Netherlands. Kai-yuh Hsiao, Soroush Vosoughi, Stefanie Tellex, Rony Kubat, Deb Roy. (2008). Object Schemas for Responsive Robotic Language Use. Proceedings of the 3rd ACM/IEEE International Conference on  Human-Robot Interaction, pages 233-240. 
306
