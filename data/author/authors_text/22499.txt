Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1779?1785,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Cross-Lingual Part-of-Speech Tagging through Ambiguous Learning
Guillaume Wisniewski Nicolas P?cheux Souhir Gahbiche-Braham Fran?ois Yvon
Universit? Paris Sud
LIMSI-CNRS
91 403 ORSAY CEDEX, France
{wisniews,pecheux,souhir,yvon}@limsi.fr
Abstract
When Part-of-Speech annotated data is
scarce, e.g. for under-resourced lan-
guages, one can turn to cross-lingual trans-
fer and crawled dictionaries to collect par-
tially supervised data. We cast this prob-
lem in the framework of ambiguous learn-
ing and show how to learn an accurate
history-based model. Experiments on ten
languages show significant improvements
over prior state of the art performance.
1 Introduction
In the past two decades, supervised Machine
Learning techniques have established new perfor-
mance standards for many NLP tasks. Their suc-
cess however crucially depends on the availability
of annotated in-domain data, a not so common sit-
uation. This means that for many application do-
mains and/or less-resourced languages, alternative
ML techniques need to be designed to accommo-
date unannotated or partially annotated data.
Several attempts have recently been made to
mitigate the lack of annotated corpora using par-
allel data pairing a (source) text in a resource-rich
language with its counterpart in a less-resourced
language. By transferring labels from the source
to the target, it becomes possible to obtain noisy,
yet useful, annotations that can be used to train a
model for the target language in a weakly super-
vised manner. This research trend was initiated
by Yarowsky et al. (2001), who consider the trans-
fer of POS and other syntactic information, and
further developed in (Hwa et al., 2005; Ganchev
et al., 2009) for syntactic dependencies, in (Pad?
and Lapata, 2009; Kozhevnikov and Titov, 2013;
van der Plas et al., 2014) for semantic role la-
beling and in (Kim et al., 2012) for named-entity
recognition, to name a few.
Assuming that labels can actually be projected
across languages, these techniques face the issue
of extending standard supervised techniques with
partial and/or uncertain labels in the presence of
alignment noise. In comparison to the early ap-
proach of Yarowsky et al. (2001) in which POS
are directly transferred, subject to heuristic fil-
tering rules, recent works consider the integra-
tion of softer constraints using expectation regu-
larization techniques (Wang and Manning, 2014),
the combination of alignment-based POS transfer
with additional information sources such as dic-
tionaries (Li et al., 2012; T?ckstr?m et al., 2013)
(Section 2), or even the simultaneous use of both
techniques (Ganchev and Das, 2013).
In this paper, we reproduce the weakly super-
vised setting of T?ckstr?m et al. (2013). By re-
casting this setting in the framework of ambiguous
learning (Bordes et al., 2010; Cour et al., 2011)
(Section 3), we propose an alternative learning
methodology and show that it improves the state of
the art performance on a large array of languages
(Section 4). Our analysis of the remaining errors
suggests that in cross-lingual settings, improve-
ments of error rates can have multiple causes and
should be looked at with great care (Section 4.2).
All tools and resources used in this study
are available at http://perso.limsi.fr/
wisniews/ambiguous.
2 Projecting Labels across Aligned
Corpora
Projecting POS information across languages re-
lies on a rather strong assumption that morpho-
syntactic categories in the source language can
be directly related to the categories in the tar-
get language, which might not always be war-
ranted (Evans and Levinson, 2009; Broschart,
2009). The universal reduced POS tagset pro-
posed by Petrov et al. (2012) defines an opera-
tional, albeit rather empirical, ground to perform
this mapping. It is made of the following 12 cat-
egories: NOUN (nouns), VERB (verbs), ADJ (ad-
1779
ar cs de el es fi fr id it sv
% of test covered tokens (type) 83.2 93.2 95.6 97.4 96.7 83.0 98.3 90.5 95.8 95.3
% of test correctly covered token (type) 72.9 94.2 93.7 92.9 93.8 93.6 92.1 89.6 93.6 94.1
avg. number of labels per token (type) 2.1 1.3 1.3 1.3 1.3 1.4 1.3 1.2 1.3 1.3
avg. number of labels per token (type+token) 1.7 1.1 1.1 1.1 1.1 1.2 1.2 1.1 1.1 1.1
% of aligned tokens 53.0 77.8 66.7 69.3 74.0 73.1 64.7 81.6 72.2 79.9
% of token const. violating type const. 2.5 16.0 15.8 21.4 16.9 14.3 16.1 19.3 17.5 13.6
% informative token const. 79.7 27.5 15.7 29.8 21.3 36.0 25.5 16.2 28.2 26.4
Table 1: Interplay between token and type constraints on our training parallel corpora. ?Informative?
token constraints correspond to tokens for which (a) a POS is actually transfered and (b) type constraints
do not disambiguate the label, but type+token constraints do.
jectives), ADV (adverbs), PRON (pronouns), DET
(determiners and articles), ADP (prepositions and
postpositions), NUM (numerals), CONJ (conjunc-
tions), PRT (particles), ?.? (punctuation marks)
and X (a catch-all for other categories). These
labels have been chosen for their stability across
languages and for their usefulness in various mul-
tilingual applications. In the rest of this work, all
annotations are mapped to this universal tagset.
Transfer-based methods have shown to be very
effective, even if projected labels only deliver a
noisy supervision, due to tagging (of the source
language) and other alignment errors (Yarowsky
et al., 2001). While this uncertainty can be ad-
dressed in several ways, recent works have pro-
posed to combine projected labels with monolin-
gual information in order to filter out invalid la-
bel sequences (Das and Petrov, 2011; T?ckstr?m
et al., 2013). In this work we follow T?ckstr?m et
al. (2013) and use two families of constraints:
Token constraints rely on word alignments to
project labels of source words to target words
through alignment links. Table 1 shows that, de-
pendening on the language, only 50?80% of the
target tokens would benefit from label transfer.
Type constraints rely on a tag dictionary to
define the set of possible tags for each word
type. Type constraints reduce the possible la-
bels for a given word and help filtering out cross-
lingual transfer errors (up to 20%, as shown in Ta-
ble 1). As in (T?ckstr?m et al., 2013), we con-
sider two different dictionaries. The first one is
extracted automatically from Wiktionary,
1
us-
ing the method of (Li et al., 2012). The second
tag dictionary is built by using for each word the
two most frequently projected POS labels from
the training data.
2
In contrast to T?ckstr?m et al.
1
http://www.wiktionary.org/
2
This heuristic is similar to the way T?ckstr?m et al.
(2013) we use the intersection
3
of the two type
constraints instead of their union. Table 1 shows
the precision and recall of the resulting constraints
on the test data.
These two information sources are merged ac-
cording to the rules of T?ckstr?m et al. (2013).
These rules assume that type constraints are more
reliable than token constraints and should take
precedence: by default, a given word is associated
to the set of possible tags licensed type constraints;
additionally, when a POS tag can be projected
through alignment and also satisfies the type con-
straints, then it is actually projected, thereby pro-
viding a full (yet noisy) supervision.
As shown in Table 1, token and type con-
straints complement each other effectively and
greatly reduce label ambiguity. However, the
transfer method sketched above associates each
target word with a set of possible labels, of which
only one is true. This situation is less favorable
than standard supervised learning in which one
unique gold label is available for each occurrence.
We describe in the following section how to learn
from this ambiguous supervision information.
3 Modeling Sequences under Ambiguous
Supervision
We use a history-based model (Black et al., 1992)
with a LaSO-like training method (Daum? and
Marcu, 2005). History-based models reduce struc-
tured prediction to a sequence of multi-class clas-
sification problems. The prediction of a complex
structure (here, a sequence of POS tags) is thus
modeled as a sequential decision problem: at each
(2013) filter the tag distribution with a threshold to build the
projected type constraints.
3
If the intersection is empty we use the constraints
from Wiktionary first, if also empty, the projected con-
straints then, and by default the whole tag set.
1780
position in the sequence, a multiclass classifier
is used to make a decision, using features that
describe both the input structure and the history
of past decisions (i.e. the partially annotated se-
quence).
Let x = (x
i
)
n
i=1
denote the observed sequence
and Y be the set of possible labels (in our case
the 12 universal POS tags). Inference consists in
predicting labels one after the other using, for in-
stance, a linear model:
y
?
i
= argmax
y?Y
?w|?(x, i, y, h
i
)? (1)
where ??|?? is the standard dot product operation,
y
?
i
the predicted label for position i, w the weight
vector, h
i
= y
?
1
, ..., y
?
i?1
the history of past de-
cisions and ? a joint feature map. Inference can
therefore be seen as a greedy search in the space
of the # {Y}
n
possible labelings of the input se-
quence. Trading off the global optimality of in-
ference for additional flexibility in the design of
features and long range dependencies between la-
bels has proved useful for many sequence labeling
tasks in NLP (Tsuruoka et al., 2011).
The training procedure, sketched in Algo-
rithm 1, consists in performing inference on each
input sentence and correcting the weight vector
each time a wrong decision is made. Impor-
tantly (Ross and Bagnell, 2010), the history used
during training has to be made of the previous pre-
dicted labels so that the training samples reflect the
fact that the history will be imperfectly known at
test time.
This reduction of sequence labeling to multi-
class classification allows us to learn a sequence
model in an ambiguous setting by building on the
theoretical results of Bordes et al. (2010) and Cour
et al. (2011). The decision about the correctness of
a prediction and the weight updates can be adapted
to the amount of supervision information that is
available.
Full Supervision In a fully supervised setting,
the correct label is known for each word token: a
decision is thus considered wrong when this gold
label is not predicted. In this case, a standard per-
ceptron update is performed:
w
t+1
? w
t
?? (x, i, y
?
i
, h
i
)+? (x, i, y?
i
, h
i
) (2)
where y
?
i
and y?
i
are the predicted and the gold la-
bel, respectively. This update is a stochastic gra-
dient step that increases the score of the gold label
while decreasing the score of the predicted label.
Ambiguous Supervision During training, each
observation i is now associated with a set of possi-
ble labels, denoted by
?
Y
i
. In this case, a decision is
considered wrong when the predicted label is not
in
?
Y
i
and the weight vector is updated as follows:
w
t+1
? w
t
?? (x, i, y
?
i
, h
i
)+
?
y?
i
?
?
Y
i
? (x, i, y?
i
, h
i
)
(3)
Compared to (2), this rule uniformly increases the
scores of all the labels in
?
Y
i
.
It can be shown (Bordes et al., 2010; Cour et
al., 2011), under mild assumptions (namely that
two labels never systematically co-occur in the
supervision information), that the update rule (3)
enables to learn a classifier in an ambiguous set-
ting, as if the gold labels were known. Intuitively,
as long as two labels are not systematically co-
occurring in
?
Y , updates will reinforce the correct
labels more often than the spurious ones; at the
end of training, the highest scoring label should
therefore be the correct one.
Algorithm 1 Training algorithm. In the ambigu-
ous setting,
?
Y
i
contains all possible labels; in the
supervised setting, it only contains the gold label.
w
0
? 0
for t ? J1, T K do
Randomly pick example x,
?
y
h? empty list
for i ? J1, nK do
y
?
i
= argmax
y?Y
?w
t
|?(x, i, y, h
i
)?
if y
?
i
/?
?
Y
i
then
w
t+1
? update(w
t
,x, i,
?
Y
i
, y
?
i
, h
i
)
end if
push(y
?
i
, h)
end for
end for
return
1
T
?
T
t=1
w
t
4 Empirical Study
Datasets Our approach is evaluated on 10 lan-
guages that present very different characteristics
and cover several language families.
4
In all our ex-
periments we use English as the source language.
Parallel sentences
5
are aligned with the standard
4
Resources considered in the related works are not freely
available, which prevents us from presenting a more complete
comparison.
5
All resources and features used in our experiments are
thoroughly documented in the supplementary material.
1781
ar cs de el es fi fr id it sv
HBAL 27.9 10.4 8.8 8.1 8.2 13.3 10.2 11.3 9.1 10.1
Partially observed CRF 33.9 11.6 12.2 10.9 10.7 12.9 11.6 16.3 10.4 11.6
HBSL ? 1.5 5.0 ? 2.4 5.9 3.5 4.8 2.8 3.8
HBAL + matched POS 24.1 7.6 8.0 7.3 7.4 12.2 7.4 9.8 8.3 8.8
(Ganchev and Das, 2013) 49.9 19.3 9.6 9.4 12.8 ? 12.5 ? 10.1 10.8
(T?ckstr?m et al., 2013) ? 18.9 9.5 10.5 10.9 ? 11.6 ? 10.2 11.1
(Li et al., 2012) ? ? 14.2 20.8 13.6 ? ? ? 13.5 13.9
Table 2: Error rate (in %) achieved by the method described in Sec. 3 trained in an ambiguous (HBAL)
or in a supervised setting (HBSL), a partially observed CRF and different state-of-the-art results.
MOSES pipeline, using the intersection heuristic
that only retains the most reliable alignment links.
The English side of the bitext is tagged using a
standard linear CRF trained on the Penn Treebank.
Tags are then transferred to the target language us-
ing the procedure described in Section 2. For each
language, we train a tagger using the method de-
scribed in Section 3 with T = 100 000 iterations
6
using a feature set similar to the one of Li et al.
(2012) and T?ckstr?m et al. (2013). The baseline
system is our reimplementation of the partially
observed CRF model of T?ckstr?m et al. (2013).
Evaluation is carried out on the test sets of tree-
banks for which manual gold tags are known. For
Czech and Greek, we use the CoNLL?07 Shared
Task on Dependency Parsing; for Arabic, the Ara-
bic Treebank; and otherwise the data of the Uni-
versal Dependency Treebank Project (McDonald
et al., 2013). Tagging performance is evaluated
with the standard error rate.
4.1 Results
Table 2 summarizes the performance achieved
by our method trained in the ambiguous setting
(HBAL) and by our re-implementation of the
partially supervised CRF baseline. As an upper
bound, we also report the score of our method
when trained in a supervised (HBSL) settings
considering the training part of the various tree-
banks, when it is available.
7
For the sake of com-
parison, we also list the best scores of previous
studies. Note, however, that a direct comparison
with these results is not completely fair as these
6
Preliminary experiments showed that increasing the
number of iterations T in Algorithm 1 has no significant
impact.
7
In this setting, HBSL implements an averaged percep-
tron, and achieves results that are similar to those obtained
with standard linear CRF.
systems were not trained and evaluated with the
same exact resources (corpora,
8
type constraints,
alignments, etc). Also note that the state-of-the-
art scores have been achieved by different models,
which have been selected based on their scores on
the test set and not on a validation set.
9
Experimental results show that HBAL signif-
icantly outperforms, on all considered languages
but one, the partially observed CRF that was
trained and tested in the same setting.
4.2 Discussion
The performance of our new method still falls
short of the performance of a fully supervised POS
tagger: for instance, in Spanish, full supervision
reduces the error rate by a factor of 4. A fine-
grained error analysis shows that many errors of
HBAL directly result from the fact that, contrary
to the fully supervised learner HBSL, our am-
biguous setting suffers from a train/test mismatch,
which has two main consequences. First, the train
and test sets do not follow exactly the same nor-
malization and tokenization conventions, which is
an obvious source of mistakes. Second, and more
importantly, many errors are caused by systematic
differences between the test tags and the super-
vised tags (i.e. the English side of the bitext and
Wiktionary). While some of these differences
are linguistically well-justified and reflect funda-
mental differences in the language structure and
usage, others seem to be merely due to arbitrary
annotation conventions.
For instance, in Greek, proper names are labeled
8
The test sets are only the same for Czech, Greek and
Swedish.
9
The partially observed CRF is the best model in (T?ck-
str?m et al., 2013) only for German (de), Greek (el) and
Swedish (sv), and uses only type constraints extracted from
Wiktionary.
1782
either as X (when they refer to a foreigner and are
not transliterated) or as NOUN (in all other cases),
while they are always labeled as NOUN in English.
In French and in Greek, contractions of a prepo-
sition and a determiner such as ????? (??? ???,
meaning ?to the?) or ?aux? (?? les? also meaning
?to the?) are labeled as ADP in the Universal De-
pendency Treebank but as DET in Wiktionary
and are usually aligned with a determiner in the
parallel corpora. In the Penn Treebank, quanti-
fiers like ?few? or ?little? are generally used in con-
junction with a determiner (?a few years?, ?a little
parable?, ...) and labeled as ADJ; the correspond-
ing Spanish constructions lack an article (?mucho
tempio?, ?pocos a?os?, ...) and the quantifiers are
therefore labeled as DET. Capturing such subtle
differences is hardly possible without prior knowl-
edge and specifically tailored features.
This annotation mismatch problem is all the
more important in settings like ours, that rely
on several, independently designed, information
sources, which follow contradictory annotation
conventions and for which the mapping to the uni-
versal tagset is actually error-prone (Zhang et al.,
2012). To illustrate this point, we ran three ad-
ditional experiments to assess the impact of the
train/test mismatch.
We first designed a control experiment in which
the type constraints were manually completed
with the gold labels of the most frequent errors of
HBAL. These errors generally concern function
words and can be assumed to result from system-
atic differences in the annotations rather than pre-
diction errors. For instance, for French the type
constraints for ?du?, ?des?, ?au? and ?aux? were cor-
rected from DET to ADP. The resulting model,
denoted ?HBAL + matched POS? in Table 2, sig-
nificantly outperforms HBAL, stressing the diver-
gence in the different annotation conventions.
Additionally, in order to approximate the am-
biguous setting train/test mismatch, we learn two
fully supervised Spanish taggers on the same train-
ing data as HBAL, using two different strategies
to obtain labeled data. We first use HBSL (which
was trained on the treebank) to automatically la-
bel the target side of the parallel corpus. In this
setting, the POS tagger is trained with data from
a different domain, but labeled with the same an-
notation scheme as a the test set. Learning with
this fully supervised data yields an error rate of
4.2% for Spanish, almost twice as much as HBSL,
bringing into light the impact of domain shift. We
then use a generic tagger, FREELING,
10
to label
the training data, this time with possible addi-
tional inconsistent annotations. The correspond-
ing error rate for Spanish was 6.1%, to be com-
pared with the 8.2% achieved by HBAL. The last
two control experiments show that many of the re-
maining labeling errors seem to be due to domain
and convention mismatches rather to the trans-
fer/ambiguous setting, as supervised models also
suffer from very similar conditions.
These observations show that the evaluation of
transfer-based methods suffer from several biases.
Their results must therefore be interpreted with
great care.
5 Conclusion
In this paper, we have presented a novel learning
methodology to learn from ambiguous supervision
information, and used it to train several POS tag-
gers. Using this method, we have been able to
achieve performance that surpasses the best re-
ported results, sometimes by a wide margin. Fur-
ther work will attempt to better analyse these re-
sults, which could be caused by several subtle
differences between HBAL and the baseline sys-
tem. Nonetheless, these experiments confirm that
cross-lingual projection of annotations have the
potential to help in building very efficient POS
taggers with very little monolingual supervision
data. Our analysis of these results also suggests
that, for this task, additional gains might be more
easily obtained by fixing systematic biases intro-
duced by conflicting mappings between tags or
by train/test domain mismatch than by designing
more sophisticated weakly supervised learners.
Acknowledgments
We wish to thank Thomas Lavergne and Alexan-
dre Allauzen for early feedback and for providing
us with the partially observed CRF implementa-
tion. We also thank the anonymous reviewers for
their helpful comments.
References
Ezra Black, Fred Jelinek, John Lafferty, David M.
Magerman, Robert Mercer, and Salim Roukos.
1992. Towards history-based grammars: Using
10
http://nlp.lsi.upc.edu/freeling/
1783
richer models for probabilistic parsing. In Proceed-
ings of the Workshop on Speech and Natural Lan-
guage, HLT ?91, pages 134?139, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Antoine Bordes, Nicolas Usunier, and Jason Weston.
2010. Label ranking under ambiguous supervision
for learning semantic correspondences. In ICML,
pages 103?110.
J?rgen Broschart. 2009. Why Tongan does it differ-
ently: Categorial distinctions in a language without
nouns and verbs. Linguistic Typology, 1:123?166,
10.
Timothee Cour, Ben Sapp, and Ben Taskar. 2011.
Learning from partial labels. Journal of Machine
Learning Research, 12:1501?1536, July.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ?11, pages 600?609, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Hal Daum?, III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In Proceedings
of the 22Nd International Conference on Machine
Learning, ICML ?05, pages 169?176, New York,
NY, USA. ACM.
Nicholas Evans and Stephen C. Levinson. 2009. The
myth of language universals: Language diversity
and its importance for cognitive science. Behavioral
and Brain Sciences, 32:429?448, 10.
Kuzman Ganchev and Dipanjan Das. 2013. Cross-
lingual discriminative learning of sequence models
with posterior regularization. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1996?2006, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP:
Volume 1 - Volume 1, ACL ?09, pages 369?377,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3):311?325, September.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.
2012. Multilingual named entity recognition using
parallel data and metadata from wikipedia. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Long Papers
- Volume 1, ACL ?12, pages 694?702, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Mikhail Kozhevnikov and Ivan Titov. 2013. Cross-
lingual transfer of semantic role labeling models.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1190?1200, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Shen Li, Jo?o V. Gra?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1389?1398, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ckstr?m, Claudia Bedini, N?ria
Bertomeu Castell?, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 92?97, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Sebastian Pad? and Mirella Lapata. 2009. Cross-
lingual annotation projection of semantic roles. J.
Artif. Int. Res., 36(1):307?340, September.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Nicoletta Cal-
zolari (Conference Chair), Khalid Choukri, Thierry
Declerck, Mehmet U?gur Do?gan, Bente Maegaard,
Joseph Mariani, Jan Odijk, and Stelios Piperidis, ed-
itors, Proceedings of the Eight International Con-
ference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey, may. European Lan-
guage Resources Association (ELRA).
St?phane Ross and Drew Bagnell. 2010. Efficient re-
ductions for imitation learning. In AISTATS, pages
661?668.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with lookahead: Can
history-based models rival globally optimized mod-
els? In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning, pages
238?246, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Oscar T?ckstr?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1?12.
1784
Lonneke van der Plas, Marianna Apidianaki, and Chen-
hua Chen. 2014. Global methods for cross-lingual
semantic role and predicate labelling. In Proceed-
ings of COLING 2014, the 25th International Con-
ference on Computational Linguistics: Technical
Papers, pages 1279?1290, Dublin, Ireland, August.
Dublin City University and Association for Compu-
tational Linguistics.
Mengqiu Wang and Christopher D. Manning. 2014.
Cross-lingual projected expectation regularization
for weakly supervised learning. Transactions of the
ACL, 2:55?66, February.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analy-
sis tools via robust projection across aligned cor-
pora. In Proceedings of the First International Con-
ference on Human Language Technology Research,
HLT ?01, pages 1?8, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Yuan Zhang, Roi Reichart, Regina Barzilay, and Amir
Globerson. 2012. Learning to map into a univer-
sal pos tagset. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, EMNLP-CoNLL ?12, pages 1368?
1378, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
1785
Two Ways to Use a Noisy Parallel News corpus for improving Statistical
Machine Translation
Souhir Gahbiche-Braham He?le`ne Bonneau-Maynard
Universite? Paris-Sud 11
LIMSI-CNRS
91403 Orsay, France
{souhir,hbm,yvon}@limsi.fr
Franc?ois Yvon
Abstract
In this paper, we present two methods to use
a noisy parallel news corpus to improve sta-
tistical machine translation (SMT) systems.
Taking full advantage of the characteristics of
our corpus and of existing resources, we use
a bootstrapping strategy, whereby an existing
SMT engine is used both to detect parallel sen-
tences in comparable data and to provide an
adaptation corpus for translation models. MT
experiments demonstrate the benefits of vari-
ous combinations of these strategies.
1 Introduction
In Statistical Machine Translation (SMT), systems
are created from parallel corpora consisting of a set
of source language texts aligned with its translation
in the target language. Such corpora however only
exist (at least are publicly documented and avail-
able) for a limited number of domains, genres, reg-
isters, and language pairs. In fact, there are a few
language pairs for which parallel corpora can be ac-
cessed, except for very narrow domains such as po-
litical debates or international regulatory texts. An-
other very valuable resource for SMT studies, espe-
cially for under-resource languages, are comparable
corpora, made of pairs of monolingual corpora that
contain texts of similar genres, from similar periods,
and/or about similar topics.
The potential of comparable corpora has long
been established as a useful source from which to
extract bilingual word dictionaries (see eg. (Rapp,
1995; Fung and Yee, 1998)) or to learn multilingual
terms (see e.g. (Lange?, 1995; Smadja et al, 1996)).
More recently, the relative corpus has caused the
usefulness of comparable corpora be reevaluated as
a potential source of parallel fragments, be they
paragraphs, sentences, phrases, terms, chunks, or
isolated words. This tendency is illustrated by the
work of e.g. (Resnik and Smith, 2003; Munteanu
and Marcu, 2005), which combines Information Re-
trieval techniques (to identify parallel documents)
and sentence similarity detection to detect parallel
sentences.
There are many other ways to improve SMT mod-
els with comparable or monolingual data. For in-
stance, the work reported in (Schwenk, 2008) draws
inspiration from recent advances in unsupervised
training of acoustic models for speech recognition
and proposes to use self-training on in-domain data
to adapt and improve a baseline system trained
mostly with out-of-domain data.
As discussed e.g. in (Fung and Cheung, 2004),
comparable corpora are of various nature: there ex-
ists a continuum between truly parallel and com-
pletely unrelated texts. Algorithms for exploiting
comparable corpora should thus be tailored to the
peculiarities of the data on which they are applied.
In this paper, we report on experiments aimed at
using a noisy parallel corpus made out of news sto-
ries in French and Arabic in two different ways: first,
to extract new, in-domain, parallel sentences; sec-
ond, to adapt our translation and language models.
This approach is made possible due to the specifici-
ties of our corpus. In fact, our work is part of a
project aiming at developing a platform for process-
ing multimedia news documents (texts, interviews,
images and videos) in Arabic, so as to streamline the
44
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 44?51,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
work of a major international news agency. As part
as the standard daily work flow, a significant por-
tion of the French news are translated (or adapted)
in Arabic by journalists. Having access to one full
year of the French and Arabic corpus (consisting, to
date, of approximately one million stories (150 mil-
lion words)), we have in our hands an ideal compa-
rable resource to perform large scale experiments.
These experiments aim at comparing various
ways to build an accurate machine translation sys-
tem for the news domain using (i) a baseline system
trained mostly with out-of-domain data (ii) the com-
parable dataset. As will be discussed, given the very
large number of parallel news in the data, our best
option seems to reconstruct an in-domain training
corpus of automatically detected parallel sentences.
The rest of this paper is organized as follows.
In Section 2, we relate our work to some exist-
ing approaches for using comparable corpora. Sec-
tion 3 presents our methodology for extracting par-
allel sentences, while our phrase-table adaptation
strategies are described in Section 4. In Section 5,
we describe our experiments and contrast the results
obtained with several adaptation strategies. Finally,
Section 6 concludes the paper.
2 Related work
From a bird?s eye view, attempts to use comparable
corpora in SMT fall into two main categories: first,
approaches aimed at extracting parallel fragments;
second, approaches aimed at adapting existing re-
sources to a new domain.
2.1 Extracting parallel fragments
Most attempts at automatically extracting parallel
fragments use a two step process (see (Tillmann and
Xu, 2009) for a counter-example): a set of candidate
parallel texts is first identified; within this short list
of possibly paired texts, parallel sentences are then
identified based on some similarity score.
The work reported in (Zhao and Vogel, 2002) con-
centrates on finding parallel sentences in a set of
comparable stories pairs in Chinese/English. Sen-
tence similarity derives from a probabilistic align-
ment model for documents, which enables to recog-
nize parallel sentences based on their length ratio,
as well as on the IBM 1 model score of their word-
to-word alignment. To account for various levels of
parallelism, the model allows some sentences in the
source or target language to remain unaligned.
The work of (Resnik and Smith, 2003) considers
mining a much larger ?corpora? consisting of docu-
ments collected on the Internet. Matched documents
and sentences are primarily detected based on sur-
face and/or formal similarity of the web addresses
or of the page internal structure.
This line of work is developed notably in
(Munteanu and Marcu, 2005): candidate parallel
texts are found using Cross-Lingual Information Re-
trieval (CLIR) techniques; sentence similarity is in-
directly computed using a logistic regression model
aimed at detecting parallel sentences. This formal-
ism allows to enrich baseline features such as the
length ratio, the word-to-word (IBM 1) alignment
scores with supplementary scores aimed at reward-
ing sentences containing identical words, etc. More
recently, (Smith et al, 2010) reported significant im-
provements mining parallel Wikipedia articles us-
ing more sophisticated indicators of sentence par-
allelism, incorporating a richer set of features and
cross-sentence dependencies within a Conditional
Random Fields (CRFs) model. For lack of find
enough parallel sentences, (Munteanu and Marcu,
2006; Kumano and Tokunaga, 2007) consider the
more difficult issue of mining parallel phrases.
In (Abdul-Rauf and Schwenk, 2009), the authors,
rather than computing a similarity score between a
source and a target sentence, propose to use an ex-
isting translation engine to process the source side
of the corpus, thus enabling sentence comparison to
be performed in the target language, using the edit
distance or variants thereof (WER or TER). This
approach is generalized to much larger collections
in (Uszkoreit et al, 2010), which draw advantage
of working in one language to adopt efficient paral-
lelism detection techniques (Broder, 2000).
2.2 Comparable corpora for adaptation
Another very productive use of comparable corpora
is to adapt or specialize existing resources (dictio-
naries, translation models, language models) to spe-
cific domains and/or genres. We will only focus here
on adapting the translation model; a review of the
literature on language model adaptation is in (Bella-
garda, 2001) and the references cited therein.
45
Figure 1: Extraction of parallel corpora
The work in (Snover et al, 2008) is a first step
towards augmenting the translation model with new
translation rules: these rules associate, with a tiny
probability, every phrase in a source document with
the most frequent target phrases found in a compa-
rable corpus specifically built for this document.
The study in (Schwenk, 2008) considers self-
training, which allows to adapt an existing system
to new domains using monolingual (source) data.
The idea is to automatically translate the source side
of an in-domain corpus using a reference translation
system. Then, according to some confidence score,
the best translations are selected to form an adap-
tation corpus, which can serve to retrain the trans-
lation model. The authors of (Cettolo et al, 2010)
follow similar goals with different means: here, the
baseline translation model is used to obtain a phrase
alignment between source and target sentences in
a comparable corpus. These phrase alignments are
further refined, before new phrases not in the origi-
nal phrase-table, can be collected.
The approaches developed below borrow from
both traditions: given (i) the supposed high degree
of parallelism in our data and (ii) the size of the
available comparable data, we are in a position to
apply any of the above described technique. This
is all the easier to do as all stories are timestamped,
which enables to easily spot candidate parallel texts.
In both cases, we will apply a bootstrapping strat-
egy using as baseline a system trained with out-of-
domain data.
3 Extracting Parallel Corpora
This section presents our approach for extracting a
parallel corpus from a comparable in-domain cor-
pora so as to adapt a SMT system to a specific do-
main. Our methodology assumes that both a base-
line out-of-domain translation system and a compa-
rable in-domain corpus are available, two require-
ments that are often met in practice.
As shown in Figure 1, our approach for extracting
an in-domain parallel corpus from the in-domain
comparable corpus consists in 3 steps and closely
follows (Abdul-Rauf and Schwenk, 2009):
translation: translating the source side of the
comparable corpora;
document pairs selection : selecting, in the com-
parable corpus, documents that are similar to the
translated output;
sentence pairs selection : selecting parallel sen-
tences among the selected documents.
The main intuition is that computing document
similarities in one language enables to use simple
and effective comparison procedures, instead of hav-
ing to define ad hoc similarities measures based on
complex underlying alignment models.
The translation step consists here in translating
the source (Arabic) side of the comparable corpus
using a baseline out-of-domain system, which has
been trained on parallel out-of-domain data.
The document selection step consists in trying
to match the automatic translations (source:target)
with the original documents in the target language.
For each (source:target) document, a similarity score
with all the target documents is computed. We con-
tend here with a simple association score, namely
the Dice coefficient, computed as the number of
words in common in both documents, normalized by
the length of the (source:target) document.
A priori knowledge, such as the publication dates
46
of the documents, are used to limit the number of
document pairs to be compared. For each source
document, the target document that has the best
score is then selected as a potential parallel docu-
ment. The resulting pairs of documents are then fil-
tered depending on a threshold Td, so as to avoid
false matches (in the experiments described below,
the threshold has been set so as to favor precision
over recall).
At the end of this step, a set of similar source
and target document pairs has been selected. These
pairs may consist in documents that are exact trans-
lations of each other. In most cases, the documents
are noisy translation and only a subset of their sen-
tences are mutual translation.
The sentence selection step then consists in per-
forming a sentence level alignment of each pair of
documents to select a set of parallel sentences. Sen-
tence alignment is then performed with the hunalign
sentence alignment tool (Varga et al, 2005), which
also provides alignment confidence measures. As
for the document selection step, only sentence pairs
that obtain an alignment score greater than a prede-
fined threshold Ts are selected, where Ts is again
chosen to favor prevision of alignments of recall.
From these, 1 : 1 alignments are retained, yielding
a small, adapted, parallel corpus. This method is
quite different from (Munteanu and Marcu, 2005)?s
work where the sentence selection step is done by a
Maximum Entropy classifier.
4 Domain Adaptation
In the course of mining our comparable corpus, we
have produced a translation into French for all the
source language news stories. This means that we
have three parallel corpora at our disposal:
? The baseline training corpus, which is large
(a hundred million words), delivering a reason-
able translation performance quality of transla-
tion, but out-of-domain;
? The extracted in-domain corpus, which is
much smaller, and potentially noisy;
? The translated in-domain corpus, which is of
medium-size, and much worse in quality than
the others.
Considering these three corpora, different adapta-
tion methods of the translation models are explored.
The first approach is to concatenate the baseline and
in-domain training data (either extracted or trans-
lated) to train a new translation model. Given the
difference in size between the two corpus, this ap-
proach may introduce a bias in the translation model
in favor of out-of-domain.
The second approach is to train separate transla-
tion models with baseline on the one hand, and with
in-domain on the other data and to weight their com-
bination with MERT (Och, 2003). This alleviates
the former problem but increases the number of fea-
tures that need to be trained, running the risk to make
MERT less stable.
A last approach is also considered, which consists
in using only the in-domain data to train the trans-
lation model. In that case, the question is the small
size of the in-domain data.
The comparative experiments on the three ap-
proaches, using the three corpora are described in
next section.
5 Experiments and results
5.1 Context and data
The experiments have been carried out in the con-
text of the Cap Digital SAMAR1 project which aims
at developping a platform for processing multimedia
news in Arabic. Every day, about 250 news in Ara-
bic, 800 in French and in English2 are produced and
accumulated on our disks. News collected from De-
cember 2009 to December 2010 constitute the com-
parable corpora, containing a set of 75,975 news for
the Arabic part and 288,934 news for the French part
(about 1M sentences for Arabic and 5M sentences
for French).
The specificity of this comparable corpus is that
many Arabic stories are known to be translation of
news that were first written in French. The transla-
tions may not be entirely faithful: when translating
a story, the journalist is in fact free to rearrange the
structure, and to some extend, the content of a doc-
ument (see example Figure 2).
In our experiments, the in-domain comparable
corpus then consists in a set of Arabic and French
1http://www.samar.fr
2The English news have not been used in this study.
47
Arabic:
	
?A
	
J

J

?@ 	?? A
	
JK
Y? ?
	
KA? B ?A?g ?


	
? 	?m