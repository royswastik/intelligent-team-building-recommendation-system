Two Step Chinese Named Entity Recognition Based on Conditional 
Random Fields Models 
Yuanyong Feng*          Ruihong Huang*         Le Sun? 
*Institute of Software, Graduate University 
Chinese Academy of Sciences 
Beijing, China, 100080 
{comerfeng,ruihong2}@iscas.cn 
?Institute of Software 
Chinese Academy of Sciences 
Beijing, China, 100080 
sunle@iscas.cn 
 
 
Abstract 
This paper mainly describes a Chinese 
named entity recognition (NER) system 
NER@ISCAS, which integrates text, part-
of-speech and a small-vocabulary-
character-lists feature and heristic post-
process rules for MSRA NER open track 
under the framework of Conditional Ran-
dom Fields (CRFs) model. 
1  Introduction 
The system NER@ISCAS is designed under the 
Conditional Random Fields (CRFs. Lafferty et al, 
2001) framework. It integrates multiple features 
based on single Chinese character or space sepa-
rated ASCII words. The early designed system 
(Feng et al, 2006) is used for the MSRA NER 
open track this year. The output of an external 
part-of-speech tagging tool and some carefully 
collected small-scale-character-lists are used as 
open knowledge. Some post process steps are also 
applied to complement the local limitation in 
model?s feature engineering. 
The remaining of this paper is organized as fol-
lows. Section 2 introduces Conditional Random 
Fields model. Section 3 presents the details of our 
system on Chinese NER integrating multiple fea-
tures. Section 4 describes the post-processings 
based on some heuristic rules. Section 5 gives the 
evaluation results. We end our paper with some 
conclusions and future works. 
2  Conditional Random Fields Model 
Conditional random fields are undirected graphical 
models for calculating the conditional probability 
for output vertices based on input ones. While 
sharing the same exponential form with maximum 
entropy models, they have more efficient proce-
dures for complete, non-greedy finite-state infer-
ence and training.  
Given an observation sequence o=<o1, o2, ..., 
oT>, linear-chain CRFs model based on the as-
sumption of first order Markov chains defines the 
corresponding state sequence s? probability as fol-
lows (Lafferty et al, 2001): 
1
1
1
( | ) exp( ( , , , ))
T
k k t t
t k
p f
Z
?? ?
=
= ??
o
s o os s t
 
(1)
Where ? is the model parameter set, Zo is the nor-
malization factor over all state sequences, fk is an 
arbitrary feature function, and ?k is the learned fea-
ture weight. A feature function defines its value to 
be 0 in most cases, and to be 1 in some designated 
cases. For example, the value of a feature named 
?MAYBE-SURNAME? is 1 if and only if st-1 is 
OTHER, st is PER, and the t-th character in o is a 
common-surname. 
The inference and training procedures of CRFs 
can be derived directly from those equivalences in 
HMM. For instance, the forward variable ?t(si) 
defines the probability that state at time t being si 
at time t given the observation sequence o. As-
sumed that we know the probabilities of each 
possible value si for the beginning state  ?0(si), 
then we have 
1( ) ( )exp( ( , , , )t i t k k i
s k
s s f s s t? ? ?+
?
? ?=? ? o (2)
120
Sixth SIGHAN Workshop on Chinese Language Processing
In similar ways, we can obtain the backward 
variables and Baum-Welch algorithm. 
3  Chinese NER Using CRFs Model Inte-
grating Multiple Features 
Besides the text feature(TXT), simplified part-of-
speech (POS) feature, and small-vocabulary-
character-lists (SVCL) feature, which use in the 
early system (Feng et al, 2006), some new fea-
tures such as word boundary, adjoining state bi-
gram ? observation and early NE output are also 
combined under the unified CRFs framework. 
The text feature includes single Chinese charac-
ter, some continuous digits or letters. 
POS feature is an important feature which car-
ries some syntactic information. Unlike those in 
the earyly system, the POS tag set are merged into 
9 categories from the criterion of modern Chinese 
corpora construction (Yu, 1999), which contains 
39 tags.  
The third type of features are derived from the 
small-vocabulary-character lists which are essen-
tially same as the ones used in last year except 
with some additional items. Some examples of this 
list are given in Table 1. 
Value Description Examples 
digit Arabic digit(s) 1,2,3 
letter Letter(s) A,B,C,...,a, b, c 
Continuous digits and/or letters (The sequence is 
regarded as a single token) 
chseq Chinese order 1 ? ? ? ?, , ,  
chdigit Chinese digit ? ? ?, ,  
tianseq Chinese order 2 ? ? ? ?, , ,  
chsurn Surname ? ? ? ?, , ,  
notname Not name ? ? ? ? ? ?, , , , , 
loctch LOC tail charac-ter 
? ? ? ? ?, , , , , 
? ?,  
orgtch ORG tail charac- ? ? ? ? ?, , , , , 
ter ?, ? 
other Other case ? ?, ?, ,   ?, ? 
Table 1.  Some Examples of SVCL Feature 
The fourth type of feature is word boundary. We 
use the B, I, E, U, and O to indicate Begining, In-
ner, Ending, and Uniq part of, or outside of a word 
given a word segmentation. The O case occurs 
when a token, for example the charactor ?&?, is 
ignored by the segmentator. We do not combine 
the boundary information with other features be-
cause we argue it is very limited and may cause 
errors. 
The last type of features is bigram state com-
bined with observations. We argue that observa-
toin (mainly is of named entity derived by early 
system or character text itself) and state transition 
are not conditionally independent and entails dedi-
cate considerings. 
Each token is presented by its feature vector, 
which is combined by these features we just dis-
cussed. Once all token feature (Maybe including 
context features) values are determined, an obser-
vation sequence is feed into the model. 
Each token state is a combination of the type of 
the named entity it belongs to and the boundary 
type it locates within. The entity types are person 
name (PER), location name (LOC), organization 
name (ORG), date expression (DAT), time expres-
sion (TIM), numeric expression (NUM), and not 
named entity (OTH). The boundary types are sim-
ply Beginning, Inside, and Outside (BIO).  
All above types of features are extracted from a 
varying length window. The main criteria is that 
wider window with smaller feature space and nar-
row window when the observation features are in a 
large range. 
The main feature set is shown the following. 
Character Texts(TXT): 
TXT-2, TXT-1, TXT0, TXT1, TXT2, 
TXT-1TXT0,  TXT1TXT0, TXT1TXT2 
simplified part-of-speech (POS): 
unigram: POS-4 ~ POS4 
121
Sixth SIGHAN Workshop on Chinese Language Processing
small-vocabulary-character-lists (SVCL): 
unigram: SVCL-2 ~ SVCL7 
bigram:SVCL0SVCL1, SVCL1SVCL2 
Word Boundary (WB): 
WB-1,WB0,WB1 
Named Entity (NE): 
unigram: NE-4 ~ NE4 
bigram:NE-2NE-1, NE-1NE0, NE0NE1, NE1NE2 
State Bigram (B) ? Observation: 
B, B-TXT0, B-NE-1, B-NE0, B-NE1 
Table 2.  The Main Feature Set 
4  Post Processing on Heuristic Rules 
Observing from the evaluation, our model has 
worse performance on ORG and PER than LOC. 
Furthermore, the analysis of the errors tells us that 
they are hard to be tackled with the improvement 
of the model itself. Therefore, we decided to do 
some post-process to correct certain types of tag-
ging errors of the unified model mainly concerning 
the two kinds of entities, ORG and PER.  
At the training phrase, we compare the tagging 
output of the model with the correct tags and col-
lect the falsely tagged instances. To identify the 
rules used in the post-process, we categorize the 
errors into several types, discriminate the types 
and encode them into the rules according to two 
principles: 
1) the rules are applied on the tagged sequences 
output by the unified model.  
2) The rules applied shouldn?t introduce more 
other errors. 
As a result, we have extracted eight rules, seven 
for ORG, one for PER. Generally, the rules work 
only on the local context of the examined tags, 
they correct some type of error by changing some 
tags when seeing certain pattern of context before 
or after the current tags in a limited distance. We 
want to give one rule as one example to explain 
the way they function. 
Example: {<LOC>}+<ORG> ==> <ORG>,  
After this rule is applied, one or more locations 
followed by a organization name will be tagged 
ORG. This is the case where there are a location 
name in a organization name. Besides, we can see 
since the location and latter part of the organiza-
tion name are tagged separately in the unified 
model, we may only resort to the post-process to 
get the right government boundary.  
5  Evaluation 
5.1  Results 
The evaluations in training phrase tell us the 
post-process can improve the performance by one 
percent. We are satisfied since we just applied 
eight rules. 
The formal eveluation results of our system are 
shown in Table 3. 
 R P F 
Overall 86.74 90.03 88.36 
PER 90.83 92.16 91.49 
LOC 89.89 91.66 90.77 
ORG 77.99 85.16 81.41 
Table 3. Formal Results on MSRA NER Open 
5.2  Errors  from NER Track 
The NER errors in our system are mainly of as 
follows: 
z Abbreviations 
Abbreviations are very common among the er-
rors. Among them, a significant part of abbrevia-
tions are mentioned before their corresponding full 
names. Some common abbreviations has no corre-
sponding full names appeared in document. Here 
are some examples: 
R1: ??[???? LOC]????? 
K: [?????? LOC]????? 
R: [? ? LOC]?? 
K: [? LOC][? LOC]?? 
In current system, the recognition is fully de-
pended on the linear-chain CRFs model, which is 
heavily based on local window observation fea-
tures; no abbreviation list or special abbreviation 
                                                 
1 R stands for system response, K for key. 
122
Sixth SIGHAN Workshop on Chinese Language Processing
recognition involved. Because lack of constraint 
checking on distant entity mentions, the system 
fails to catch the interaction among similar text 
fragments cross sentences. 
z Concatenated Names 
For many reasons, Chinese names in titles and 
some sentences, especially in news, are not sepa-
rated. The system often fails to judge the right 
boundaries and the reasonable type classification. 
For example: 
R:?[???? LOC]?[???? PER]
?? 
K:?[???? PER]?[???? PER]
?? 
z Hints 
Though it helps to recognize an entity at most 
cases, the small-vocabulary-list hint feature may 
recommend a wrong decision sometimes. For in-
stance, common surname character ??? in the 
following sentence is wrongly labeled when no 
word segmentation information given: 
R:[?? LOC]?[? ???????
? PER] 
K:[?? LOC]? ?[???????
? PER] 
Other errors of this type may result from failing 
to identify verbs and prepositions, such as: 
R:??????????? ????? 
K:[??????????? ORG]????
? 
6  Conclusions and Future Work 
We mainly described a Chinese named entity rec-
ognition system NER@ISCAS, which integrates 
text, part-of-speech and a small-vocabulary-
character-lists feature for MSRA NER open track 
under the framework of Conditional Random 
Fields (CRFs) model. Although it provides a uni-
fied framework to integrate multiple flexible fea-
tures, and to achieve global optimization on input 
text sequence, the popular linear chained Condi-
tional Random Fields model often fails to catch 
semantic relations among reoccurred mentions and 
adjoining entities in a catenation structure. 
The situations containing exact reoccurrence 
and shortened occurrence enlighten us to take 
more effort on feature engineering or post process-
ing on abbreviations / recurrence recognition. 
Another effort may be poured on the common 
patterns, such as paraphrase, counting, and con-
straints on Chinese person name lengths. 
From current point of view, enriching the hint 
lists is also desirable. 
Acknowledgements 
This work is partially supported by National Natural Science 
Foundation of China under grant #60773027, #60736044 and 
by ?863? Key Projects #2006AA010108.  
References 
Chinese 863 program. 2005. Results on Named 
Entity Recognition. The 2004HTRDP Chinese 
Information Processing and Intelligent Human-
Machine Interface Technology Evaluation. 
Yuanyong Feng, Le Sun, Yuanhua Lv. 2006. 
Chinese Word Segmentation and Named Entity 
Recognition Based on Conditional Random 
Fields Models. Proceedings of SIGHAN-2006, 
Fifth SIGHAN Workshop on Chinese Language 
Processing, Sydney, Australia,. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional Random Fields: 
Probabilistic Models for Segmenting and Label-
ing Sequence Data. ICML. 
Shiwen Yu. 1999. Manual on Modern Chinese 
Corpora Construction. Institute of Computa-
tional Language, Peking Unversity. Beijing.  
123
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 181?184,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Word Segmentation and Named Entity Recognition Based on 
Conditional Random Fields Models 
 
 
Yuanyong Feng Le Sun Yuanhua Lv 
Institute of Software, Chinese Academy of Sciences, Beijing, 100080, China 
{yuanyong02, sunle, yuanhua04}@ios.cn 
 
 
Abstract 
This paper mainly describes a Chinese 
named entity recognition (NER) system 
NER@ISCAS, which integrates text, 
part-of-speech and a small-vocabulary-
character-lists feature for MSRA NER 
open track under the framework of Con-
ditional Random Fields (CRFs) model. 
The techniques used for the close NER 
and word segmentation tracks are also 
presented. 
1 Introduction 
The system NER@ISCAS is designed under the 
Conditional Random Fields (CRFs. Lafferty et 
al., 2001) framework. It integrates multiple fea-
tures based on single Chinese character or space 
separated ASCII words. The early designed sys-
tem (Feng et al, 2005) is used for the MSRA 
NER open track this year. The output of an ex-
ternal part-of-speech tagging tool and some care-
fully collected small-scale-character-lists are 
used as outer knowledge. 
The close word segmentation and named en-
tity recognition tracks are also based on this sys-
tem by some adjustments.  
The remaining of this paper is organized as 
follows. Section 2 introduces Conditional Ran-
dom Fields model. Section 3 presents the details 
of our system on Chinese NER integrating mul-
tiple features. Section 4 describes the features 
extraction for close track. Section 5 gives the 
evaluation results. We end our paper with some 
conclusions and future works. 
2 Conditional Random Fields Model 
Conditional random fields are undirected graphi-
cal models for calculating the conditional prob-
ability for output vertices based on input ones. 
While sharing the same exponential form with 
maximum entropy models, they have more effi-
cient procedures for complete, non-greedy finite-
state inference and training.  
Given an observation sequence o=<o1, o2, ..., 
oT>, linear-chain CRFs model based on the as-
sumption of first order Markov chains defines 
the corresponding state sequence s? probability as 
follows (Lafferty et al, 2001): 
1
1
1
( | ) exp( ( , , , ))
T
k k t t
t k
p f s s t
Z
?? ?
=
= ??
o
s o o
 
(1)
Where ? is the model parameter set, Zo is the 
normalization factor over all state sequences, fk is 
an arbitrary feature function, and ?k is the learned 
feature weight. A feature function defines its 
value to be 0 in most cases, and to be 1 in some 
designated cases. For example, the value of a 
feature named ?MAYBE-SURNAME? is 1 if 
and only if st-1 is OTHER, st is PER, and the t-th 
character in o is a common-surname. 
The inference and training procedures of 
CRFs can be derived directly from those equiva-
lences in HMM. For instance, the forward vari-
able ?t(si) defines the probability that state at 
time t being si at time t given the observation 
sequence o. Assumed that we know the proba-
bilities of each possible value si for the beginning 
state  ?0(si), then we have 
1( ) ( ) exp( ( , , , )t i t k k i
s k
s s f s s t? ? ?+
?
? ?= ? ? o (2)
In similar ways, we can obtain the backward 
variables and Baum-Welch algorithm. 
3 Chinese NER Using CRFs Model Inte-
grating Multiple Features for Open  
Track 
In our system the text feature, part-of-speech 
(POS) feature, and small-vocabulary-character-
lists (SVCL) feature are combined under a uni-
fied CRFs framework. 
181
The text feature includes single Chinese char-
acter, some continuous digits or letters. 
POS feature is an important feature which car-
ries some syntactic information. Our POS tag set 
follows the criterion of modern Chinese corpora 
construction (Yu, 1999), which contains 39 tags.  
The last feature is based on lists. We first list 
all digits and English letters in Chinese. Then 
most frequently used character feature in Chinese 
NER are collected, including 100 single charac-
ter surnames, 100 location tail characters, and 40 
organization tail characters. The total number of 
these items in our lists is less than 600. The lists 
altogether make up a list feature (SVCL). Some 
examples of this list are given in Table 1. 
 
 
Each token is presented by its feature vector, 
which is combined by these features we just dis-
cussed. Once all token feature (Maybe including 
context features) values are determined, an ob-
servation sequence is feed into the model. 
Each token state is a combination of the type 
of the named entity it belongs to and the bound-
ary type it locates within. The entity types are 
person name (PER), location name (LOC), or-
ganization name (ORG), date expression (DAT), 
time expression (TIM), numeric expression 
(NUM), and not named entity (OTH). The 
boundary types are simply Beginning, Inside, 
and Outside (BIO).  
4 Feature Extraction for Close Tracks 
In close tracks, only character and word list fea-
tures which are extracted from training data are 
applied for word segmentation. In NER track we 
also include a named entity list extracted from 
the training data.  
To extract the list feature, we simply search 
each text string among the list items in maximum 
length forward way. 
Taking the word segmentation task for in-
stance, when a text string c1c2?cn is given, we 
tag each character into a BIO-WL style. If 
cici+1?cj matches an item I of length j-i+1 and no 
other item I? of length k (k>j-i+1) in the list 
matches cici+1?cj?ck+i-1, then the characters are 
tagged as follows: 
 
ci ci+1 ? cj 
B-WL I-WL ? I-WL 
 
If no item in the list matches head subpart of 
the string, then ci is tagged as 0.  
The tagging operation iterates on the 
remaining part until all characters are tagged. 
5 Evaluation 
5.1 Results 
The system for our MSRA NER open track 
submission has some bugs and was trained on a 
much smaller training data set than the full set 
the organizer provided. The results are very low, 
see Table 2:  
 
Accuracy 96.28% 
Precision 83.20% 
Recall 67.03% 
FB1 74.24% 
Table 2. MSRA NER Open 
 
When we fixed the bug and retrained on the 
full training corpus, the result comes out to be as 
follows: 
 
Accuracy 98.24% 
Precision 89.38% 
Recall 83.07% 
FB1 86.11% 
Table 3. MSRA NER Open (retrained) 
 
All the submissions on close tracks are trained 
on 80% of the training corpora, the remaining 
20% parts are used for development. The results 
are shown in Table 4 and Table 5: 
 
Value Description Examples 
digit Arabic digit(s) 1,2,3 
letter Letter(s) A,B,C,...,a, b, c 
Continuous digits and/or letters (The sequence is
regarded as a single token) 
chseq Chinese order 1 ? ? ? ?, , ,  
chdigit Chinese digit ? ? ?, ,  
tianseq Chinese order 2 ? ? ? ?, , ,  
chsurn Surname ? ? ? ?, , ,  
notname Not name ? ? ? ? ? ?, , , , , 
loctch LOC tail char-acter 
? ? ? ? ?, , , , , 
? ?,  
orgtch ORG tail char-acter 
? ? ? ? ?, , , , , 
?, ? 
other Other case ? ?, ?, ,   ?, ? 
Table 1.  Some Examples of SVCL Feature 
182
Corpus Measure 
UPUC  CityU  CKIP MSRA
Recall 0.922 0.952 0.939 0.933
Precision 0.912 0.954 0.929 0.942
FB1 0.917 0.953 0.934 0.937
OOV  
Recall 0.680 0.747 0.606 0.640
IV Recall 0.945 0.960 0.954 0.943
Table 4. WS Close 
 
Measure MSRA CityU LDC 
Accuracy 92.44 97.80 93.82 
Precision 81.64 92.76 81.43 
Recall 31.24 81.81 59.53 
FB1 45.19 86.94 68.78 
Table 5. NER Close 
 
The reason for low measure on MSRA NER 
track exists in that we chose a much smaller 
training data file encoded in CP936 (about 7% of 
the full data set). This file may be an incomplete 
output when the organizer transfers from another 
encoding scheme. 
5.2 Errors  from NER Track 
The NER errors in our system are mainly as fol-
lows: 
? Abbreviations 
Abbreviations are very common among the er-
rors. Among them, a significant part of abbrevia-
tions are mentioned before their corresponding 
full names. Some common abbreviations has no 
corresponding full names appeared in document. 
Here are some examples: 
R1:?????????? ? ? ?
??[???????????? ORG] 
[?? GPE]?[?? GPE]?????
???? 
K:?????????? [? GPE] 
[? GPE]???[?????????
??? ORG][?? GPE]?[?? 
GPE]????????? 
R: ??[???? LOC]????? 
K: [?????? LOC]????? 
R: [? ? LOC]?? 
K: [? LOC][? LOC]?? 
In current system, the recognition is fully de-
pended on the linear-chain CRFs model, which is 
heavily based on local window observation fea-
tures; no abbreviation list or special abbreviation 
                                                 
1 R stands for system response, K for key. 
recognition involved. Because lack of constraint 
checking on distant entity mentions, the system 
fails to catch the interaction among similar text 
fragments cross sentences. 
? Concatenated Names 
For many reasons, Chinese names in titles and 
some sentences, especially in news, are not sepa-
rated. The system often fails to judge the right 
boundaries and the reasonable type classification. 
For example: 
R:????[?? ?? PER]??[?
? PER] ???? 
K:????[?? PER][?? 
PER][?? PER][?? PER] ???
? 
R:?[???? LOC]?[???? 
PER]?? 
K:?[???? PER]?[???? 
PER]?? 
? Hints 
Though it helps to recognize an entity at most 
cases, the small-vocabulary-list hint feature may 
recommend a wrong decision sometimes. For 
instance, common surname character ??? in the 
following sentence is wrongly labeled when no 
word segmentation information given: 
R:[?? LOC]?[? ???????
? PER] 
K:[?? LOC]? ?[???????
? PER] 
Other errors of this type may result from fail-
ing to identify verbs and prepositions, such as: 
R:[???? ? ???????? 
ORG]??????[??? ORG]??
???? 
K:[???? ORG]?[??????
?? ORG]??????[??? ORG]
?????? 
R:??????????? ????? 
K:[??????????? ORG]???
?? 
R:?? ?? 
K:[?? PER] ?? 
? Other Types: 
R:???? ??? ??????? 
K:????[??? PER]?????
?? 
R:????? ? ? ??? 
183
K:?????[? LOC][? LOC]?
?? 
6 Conclusions and Future Work 
We mainly described a Chinese named entity 
recognition system NER@ISCAS, which inte-
grates text, part-of-speech and a small-
vocabulary-character-lists feature for MSRA 
NER open track under the framework of Condi-
tional Random Fields (CRFs) model. Although it 
provides a unified framework to integrate multi-
ple flexible features, and to achieve global opti-
mization on input text sequence, the popular lin-
ear chained Conditional Random Fields model 
often fails to catch semantic relations among re-
occurred mentions and adjoining entities in a 
catenation structure. 
The situations containing exact reoccurrence 
and shortened occurrence enlighten us to take 
more effort on feature engineering or post proc-
essing on abbreviations / recurrence recognition. 
Another effort may be poured on the common 
patterns, such as paraphrase, counting, and con-
straints on Chinese person name lengths. 
From current point of view, enriching the hint 
lists is also desirable. 
Acknowledgment 
This work is supported by the National Science 
Fund of China under contract 60203007. 
References 
Chinese 863 program. 2005. Results on Named 
Entity Recognition. The 2004HTRDP Chinese 
Information Processing and Intelligent Hu-
man-Machine Interface Technology Evalua-
tion. 
Yuanyong Feng, Le Sun and Junlin Zhang. 2005. 
Early Results for Chinese Named Entity Rec-
ognition Using Conditional Random Fields 
Model, HMM and Maximum Entropy. IEEE 
Natural Language Processing & Knowledge 
Engineering. Beijing: Publishing House, 
BUPT. pp. 549~552. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional Random Fields: 
Probabilistic Models for Segmenting and La-
beling Sequence Data. ICML. 
Shiwen Yu. 1999. Manual on Modern Chinese 
Corpora Construction. Institute of Computa-
tional Language, Peking Unversity. Beijing.  
184
