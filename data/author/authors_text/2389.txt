Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 219?222,
New York, June 2006. c?2006 Association for Computational Linguistics
Can the Internet help improve Machine Translation? 
 
Ariadna Font Llitj?s 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA, 15213. USA 
aria@cs.cmu.edu
 
 
 
 
Abstract 
This paper summarizes a largely automated 
method that uses online post-editing feed-
back to automatically improve translation 
rules. As a starting point, bilingual speak-
ers? local fixes are collected through an 
online Translation Correction Tool. Next, 
the Rule Refinement Module attacks the 
problem at its core and uses the local fixes 
to detect incorrect rules that need to be re-
fined. Once the grammar and lexicon have 
been refined, the Machine Translation sys-
tem not only produces the correct transla-
tion as fixed by the bilingual speaker, but is 
also able to generalize and correctly trans-
lates similar sentences. Thus, this work 
constitutes a novel approach to improving 
translation quality. Enhanced by the reach-
ing power of the Internet, our approach be-
comes even more relevant to address the 
problem of how to automatically improve 
the quality of Machine Translation output. 
1 Introduction 
Achieving high translation quality remains the big-
gest challenge Machine Translation (MT) systems 
currently face. Researchers have explored a variety 
of methods to include user feedback in the MT 
loop. Similar to our approach, Phaholphinyo and 
colleagues (2005) proposed adding post-editing 
rules to their English-Thai MT system with the use 
of a post-editing tool. However, they use context 
sensitive pattern-matching rules, which make it 
impossible to fix errors involving missing words. 
Unlike our approach, in their system, the rules are 
created by experienced linguists and their approach 
requires a large corpus. They mention an experi-
ment with 6,000 bilingual sentences but report no 
results due to data sparseness. 
In general, most MT systems have failed to in-
corporate post-editing efforts beyond the addition 
of corrected translations to the parallel training 
data for SMT and EBMT or to a translation mem-
ory database.1 Therefore, a largely automated 
method that uses online post-editing information to 
automatically improve translation rules constitutes 
a great advance in the field.  
If an MT-produced translation is incorrect, a bi-
lingual speaker can diagnose the presence of an 
error reliably using the online Translation Correc-
tion Tool (Font Llitj?s and Carbonell, 2004). An 
example of an English-Spanish sentence pair gen-
erated by our MT system is ?Gaud? was a great 
artist - Gaud? era un artista grande?. Using the 
online tool, bilingual speakers modified the incor-
rect translation to obtain a correct one: ?Gaud? era 
un gran artista?.  
Bilingual speakers, however, cannot be expected 
to diagnose which complex translation rules pro-
duced the error, and even less, determine how to 
improve those rules. One of the main goals of this 
research is to automate the Rule Refinement proc-
ess based on just error-locus and possibly some 
error-type information from the bilingual speaker, 
relying on rule blame assignment and on regres-
sion testing to evaluate and measure the conse-
quent improvement in MT accuracy. In this case, 
our Automatic Rule Refinement system can add 
the missing sense to the lexicon (great?gran) as 
                                                          
1 For a more detailed discussion, see Font Llitj?s and colleagues (2005a) 
219
well as the special case rule for Spanish pre-
nominal adjectives to the grammar.  
With this system in place, we envision a modi-
fied version of the Translation Correction Tool as a 
game with a purpose, available online through a 
major web portal. This would allow bilingual 
speakers to correct MT input and get rewards for 
making good corrections, and compare their scores 
and speed with other users. For the MT community 
this means having a free and easy way to get MT 
output feedback and potentially improve their sys-
tems based on such feedback.  Furthermore, a fully 
interactive system would be a great opportunity to 
show users that their corrections have a visible im-
pact on technology, since they would see the ef-
fects their corrections have on other sentences. 
Last but not least, this new method is also expected 
to be particularly useful in resource-poor scenarios, 
such as the ones the Avenue project is devoted to 
(Font Llitj?s et al, 2005b), where statistical sys-
tems are not an option and where there might be no 
experts with knowledge of the resource-poor lan-
guage (Figure 1). 
 
 
Figure 1. Simplified Avenue Architecture  
2 Online Elicitation of MT Errors 
The main challenge of the error elicitation part of 
this work is how to elicit minimal post-editing in-
formation from non-expert bilingual speakers. The 
Translation Correction Tool (TCTool) is a user-
friendly online tool that allows users to add, delete 
and modify words and alignments, as well as to 
drag words around to change word order. A set of 
user studies was conducted to discover the right 
amount of error information that bilingual speakers 
can detect reliably when using the TCTool. These 
studies showed that simple error information can 
be elicited much more reliably (F1 0.89) than error 
type information (F1 0.72) (Font Llitj?s and Car-
bonell, 2004). Most importantly, it became appar-
ent that for our Rule Refinement purposes, the list 
of correction action(s) with information about error 
and correction words is sufficient.   
Building on the example introduced above, Fig-
ure 2 shows the initial state of the TCTool, once 
the user has decided that the translation produced 
by the MT system is not correct.   
 
 
Figure 2. TCTool snapshot with initial translation pair 
 
In this case, the bilingual speaker changed 
?grande? to ?gran? and dragged ?gran(de)? in front 
of ?artista?, effectively flipping the order of these 
two words. Figure 3 shows the state of the TCTool 
after the user corrections. 
 
 
Figure 3. TCTool snapshot after user has corrected the 
translation 
3 Extracting Error Information 
User correction actions are registered into a log 
file. The Automatic Rule Refinement (RR) module 
extracts all the relevant information from the 
Learning  
Module 
Learned 
Tr. Rules 
Lexical 
Resources 
Transfer 
System 
Decoder 
Online 
Translation  
Correction  
Tool 
Word-
Aligned 
Parallel 
Corpus 
Elicitation 
Rule  
Learning 
Run-Time 
System 
Rule Refinement 
 
Rule  
Refinement  
Module 
 
Elicitation 
Tool 
Elicitation 
Corpus 
Manual  
Rules 
INPUT 
OUTPUT 
220
TCTool log files and stores it into a Correction 
Instance. See Figure 4 for an example. 
 
SL: Gaud? was a great artist  
TL: Gaud? era un artista grande 
 AL: ((1,1),(2,2),(3,3),(4,5),(5,4))  
 
 Action 1: edit (grande? gran) 
 Temp CTL: Gaudi era un artista gran  
 
Action 2: change word order  
(gran artista) 
 
CTL: Gaud? era un gran artista 
 AL: ((1,1),(2,2),(3,3),(4,4),(5,5)) 
 
Figure 4. A Correction Instance stores the source lan-
guage sentence (SL), the target language sentence (TL) 
and the initial alignments (AL), as well as all the correc-
tion actions done by the user. It also provides the cor-
rected translation (CTL) and final alignments.  
 
The Rule Refinement (RR) module processes 
one action at a time. So in this approach, the order 
in which users correct a sentence does have an im-
pact on the order in which refinements apply. 
4 Lexical Refinements 
After having stored all the relevant information 
from the log file, the Rule Refinement module 
starts processing the Correction Instance. In the 
example above, it first goes into the lexicon and, 
after double checking that there is no lexical entry 
for [great?gran], it proceeds to add one by dupli-
cating the lexical entry for [great?grande]. Since 
these two lexical entries are identical at the feature 
level, the RR module postulates a new binary fea-
ture, say feat12, which serves the purpose of distin-
guishing between two words that are otherwise 
identical (according to our lexicon):  
          
                                                          
2 A more mnemonic name for feat1 would be pre-nominal. 
5 Rule Refinements 
Now the RR module moves on to process the next 
action in the Correction Instance and the first step 
is to look at the parse trace output by the MT sys-
tem, so that the grammar rule responsible for the 
error can be identified: 
 
 
At this point, the system extracts the relevant 
rule (NP,8) from the grammar, and has two op-
tions, either to make the required changes directly 
onto the original rule (REFINE) or to make a copy 
of the original rule and modify the copy (BIFUR-
CATE). If the system has correctly applied the rule 
in the past (perhaps because users have evaluated 
the translation pair ?She saw a dangerous man ? 
Ella vio un hombre peligroso? as correct), then the 
RR module opts for the BIFURCATE operation. In 
this case, the RR module makes a copy of the 
original rule (NP,8) and then modifies the copy 
(NP,8?) by flipping the order of the noun and ad-
jective constituents, as indicated by the user. This 
rule needs to unify with ?gran? but not with 
?grande?, and so the RR module proceeds to add 
the constraint that the Spanish adjective (now y2) 
needs to have the feat1 with value +:  
          
 
These two refinements result in the MT system 
generating the desired translation, namely ?Gaud? 
era un gran artista? and not the previous incorrect 
translation. But can the system also eliminate other 
incorrect translations automatically? In addition to 
generating the correct translation, we would also 
like the RR module to produce a refined grammar 
that is as tight as possible, given the data that is 
available. Since the system already has the infor-
mation that ?un artista gran? is not a correct se-
221
quence in Spanish, the grammar can be further re-
fined to also rule out this incorrect translation. This 
can be done by restricting the application of the 
general rule (NP,8) to just post-nominal adjectives, 
like ?grande?, which in this example are marked in 
the lexicon with (feat1 = ? ). 
6 Generalization power 
The difference between this approach and mere 
post-editing is that the resulting refinements affect 
not only to the translation instance corrected by the 
user, but also to other similar sentences where the 
same error would manifest. After the refinements 
have been applied to the grammar in our example 
sentence, a sentence like ?Irina is a great friend? 
will now correctly be translated as ?Irina es una 
gran amiga?, instead of ?Irina es una amiga 
grande?.  
7 Evaluation  
We plan to evaluate the RR module on its ability to 
improve coverage and overall translation quality.  
This requires identifying sensible evaluation met-
rics. Initial experiments have shown that both 
BLEU [Papineni et al, 2001] and METEOR [La-
vie et al, 2004] can automatically distinguish be-
tween raw MT output and corrected MT output, 
even for a small set of sentences. In addition to the 
presence of the corrected translation in the lattice 
produced by the refined system, our evaluation 
metrics will also need to take into account whether 
the incorrect translation is now prevented from 
being generated and whether the lattice of alterna-
tive translations increased or decreased. A decrease 
of lattice size would mean that the refinement also 
made the grammar tighter, which is the desired 
effect.  
8 Technical Challenges and Future Work 
The Rule Refinement process is not invariable. It 
depends on the order in which refinement opera-
tions are applied. In batch mode, the RR module 
can rank Correction Instances (CI) in such a way 
as to maximize translation accuracy. Suppose that 
the first CI (CI1) triggers a bifurcation of a gram-
mar rule, like the one we see in the example de-
scribed in Section 5. After that, any CI that affects 
the same rule that got bifurcated, will only modify 
the original rule (NP,8) and not the copy (NP,8?). 
If the constraint that enforces determiner-noun 
agreement were missing from the original rule, 
say, the copy (NP,8?) would not have that con-
straint added to it, and so another example with the 
pre-nominal adjective exhibiting that agreement 
error would be required (CI2: *Irina es un gran 
amiga), before the system added the relevant con-
straint to NP,8?. However, if we can detect such 
rule dependencies before the refinement process, 
then we can try to find an optimal ranking, given 
the current set of CIs, which should result in higher 
translation accuracy, as measured on a test set.  
Another interesting future direction is enhancing 
the Rule Refinement system to allow for further 
user interaction. In an interactive mode, the system 
can use Active Learning to produce minimal pairs 
to further investigate which refinement operations 
are more robust, treating the bilingual speaker as 
an oracle. We hope to explore the space between 
batch mode and a fully interactive system to dis-
cover the optimal setting which allows the system 
to only ask the user for further interaction when it 
cannot determine the appropriate refinement opera-
tion or when it would be impossible to correctly 
refine the grammar and the lexicon automatically. 
References  
Alon Lavie, Kenji Sagae and Shyamsundar Jayaraman. 
2004. The Significance of Recall in Automatic Met-
rics for MT Evaluation. AMTA, Washington, DC. 
Ariadna Font Llitj?s, Jaime Carbonell and Alon Lavie. 
2005a. A Framework for Interactive and Automatic 
Refinement of Transfer-based Machine Translation. 
EAMT, Budapest, Hungary. 
Ariadna Font Llitj?s, Roberto Aranovich and Lori Levin 
2005b. Building Machine translation systems for in-
digenous languages. Second Conference on the In-
digenous Languages of Latin America (CILLA II), 
Texas, USA.  
 Ariadna Font Llitj?s and Jaime Carbonell. 2004. The 
Translation Correction Tool: English-Spanish user 
studies. LREC 04, Lisbon, Portugal. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2001. BLEU: A Method for Automatic 
Evaluation of Machine Translation. IBM Research 
Report RC22176 (W0109-022). 
Sitthaa Phaholphinyo, Teerapong Modhiran, Nattapol 
Kritsuthikul and Thepchai Supnithi. 2005. A Practi-
cal of Memory-based Approach for Improving Accu-
racy of MT. MT Summit X. Phuket Island, Thailand. 
222
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 72?79,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
A Walk on the Other Side:  
Adding Statistical Components to a Transfer-Based Translation System 
Ariadna Font Llitj?s 
Carnegie Mellon University 
5000 Forbes Ave. 
 Pittsburgh, PA, 15213 
aria@cs.cmu.edu 
Stephan Vogel 
Carnegie Mellon University 
5000 Forbes Ave. 
 Pittsburgh, PA, 15213 
vogel+@cs.cmu.edu 
  
Abstract 
This paper seeks to complement the cur-
rent trend of adding more structure to Sta-
tistical Machine Translation systems, by 
exploring the opposite direction: adding 
statistical components to a Transfer-Based 
MT system. Initial results on the BTEC 
data show significant improvement ac-
cording to three automatic evaluation 
metrics (BLEU, NIST and METEOR). 
1 Introduction 
In recent years the machine translation research 
community has seen a remarkable paradigm shift.  
It is not the first one, but it has been a very dra-
matic one: statistical machine translation has taken 
the center stage. Conferences like ACL or HLT are 
virtually flooded with papers on various flavors of 
SMT.  In international machine translation evalua-
tion like NIST (NIST MT Evaluation), TC-Star 
(TC-STAR Evaluation) or IWSLT (IWSLT 2006) 
evaluations, most participating systems are SMT 
systems, with a few Example-Based systems sprin-
kled in. Rule-Based systems seem to have for the 
most part disappeared. There may be many reasons 
for this paradigm shift. One obvious reason is the 
comparable ease, which with data-driven systems 
can be built once some parallel data is available. 
Another reason is that the performance of statisti-
cal translation systems has dramatically improved 
over the last 5 to 10 years. 
Does this mean that work on grammar-based 
systems should be stopped?  Should all the insight 
into the structure of languages be neglected?  This 
might be too drastic a reaction. Actually, now that 
SMT has reached some maturity, we see several 
attempts to integrate more structure into these sys-
tems, ranging from simple hierarchical alignment 
models (Wu 1997, Chiang 2005) to syntax-based 
statistical systems (Yamada and Knight 2001, 
Zollmann and Venugopal 2006). What can tradi-
tional Rule-Based translation systems learn from 
these approaches? And would it not make sense to 
work from both sides towards that common goal:  
structurally rich statistical translation models.  In 
this paper we study some enhancements for a 
Transfer-Based translation system, using tech-
niques and even components developed for statisti-
cal machine translation.  While the core engine 
remains virtually untouched, additional features are 
added to re-score the n-best list generated by the 
transfer engine. Statistical alignment techniques 
are used to lower the burden in building a lexicon 
for a new domain. Minimum error rate training is 
used to optimize the system. We show that this 
leads to significant improvements in performance. 
2 A Transfer-Based Translation System 
2.1 The Lexicon and Grammar 
In our Rule-Based MT (RBMT) system, translation 
rules include parsing, transfer, and generation in-
formation, similar to the modified transfer ap-
proach used in the early Metal system (Hutchins 
and Somers, 1992).  
The initial lexicon (479 entries) and grammar 
(40 rules) used in our experiments were manually 
written to cover the syntactic structures and the 
vocabulary of the first 400 sentences of the 
AVENUE Elicitation Corpus (Probst et al2001). 
The Elicitation Corpus contains sets of minimal 
pairs in English and it was designed to cover a va-
riety of linguistic phenomena. Building these two 
language-dependent components took a computa-
tional linguist 2-3 months. Figures 1 and 2 show 
72
examples of a translation rules in the grammar and 
the lexicon. 
 
{S,4} 
S::S : [NP VP] -> [NP VP] 
( (X1::Y1)  (X2::Y2) 
  (x0 = x2) 
  ((y2 subj) = -) 
  ((y1 case) = nom) 
  ((y1 agr) = (x1 agr)) 
  ((y2 tense) = (x2 tense)) 
  ((y2 agr pers) = (y1 agr pers)) 
  ((y2 agr num) = (y1 agr num)) ) 
 
Figure 1: English?Spanish translation rule with 
agreement constraints for subject (NP) and verb 
(VP). 
 
V::V |: ["prefer"] -> ["prefiero"] 
((X1::Y1) 
((x0 form) = prefer) 
((x0 tense) = pres) 
((y0 agr pers) = 1) 
((y0 agr num) = sg)) 
 
Figure 2: English?Spanish lexical entry for the 
verb ?prefer?. 
2.2 Refined MT System 
The original grammar and lexicon were automati-
cally improved with an Automatic Rule Refiner, 
guided by a few bilingual speaker corrections 
(Font Llitj?s & Ridmann 2007). In this approach, 
automatic refinements only affect the target lan-
guage side of translation rules, namely transfer and 
generation information. 
The refined MT system used in our experiments 
is the result of adding 30 agreement constraints to 
the grammar rules, which makes the grammar 
tighter (leading to an increase in precision), as well 
as adding three new rules to cover new syntactic 
structures and five lexical entries for new senses 
and forms of existing words (leading to an increase 
in recall). 
2.3 The Transfer Engine 
The Transfer Engine, or Xfer engine for short, 
combines the translation grammar and lexicon in 
order to produce translations of a source language 
sentence into a target language. The Xfer engine 
incorporates the three main processes involved in 
Transfer-based MT: parsing of the source language 
input, transfer of the parsed constituents of the 
source sentence to their corresponding structured 
constituents on the target language side, and gen-
eration of the target sentence. 
The currently implemented algorithm is similar 
to bottom-up chart parsing as described for exam-
ple in Allen (1995). A chart is first populated with 
all constituent structures that were created in the 
course of parsing the source language sentence 
with the source-side portion of the transfer gram-
mar. Transfer and generation are applied to each 
constituent entry. The transfer rules associated 
with each entry in the chart are used in order to 
determine the corresponding constituent structure 
on the target language side. At the word level, lexi-
cal transfer rules are used in order to get the differ-
ent lexical choices. 
Often, no parse for the entire source sentence 
can be found. Partial parses are concatenated se-
quentially to generate complete translations. 
In the current version of the Xfer system, the 
output can be a first-best translation or a n-best list, 
which can be used for additional n-best list rescor-
ing. The alternatives arise from lexical ambiguity 
and multiple synonymous choices for lexical items 
in the dictionary, but also from syntactic ambiguity 
and multiple competing hypotheses from the 
grammar. 
For our experiments, we used version 3 of the 
Xfer engine. An older version of the Xfer engine is 
described in detail in Peterson (2002).  
2.4 Ranking Translations 
The Xfer engine can generate multiple translations.  
This requires a quality score to be assigned to all 
the alternatives. Based on these scores, the 1-best 
translation will be selected by the system. 
Fragmentation Penalty 
In the original Xfer system the only score used to 
rank translation alternatives was a heuristic frag-
mentation penalty. The fragmentation penalty is 
essentially the number of different chunks (rules or 
lexical entries not embedded in another rule) that 
span the whole translation. The intuition behind 
this score is that the more partial parses are neces-
sary to span the entire sentence the less likely the 
resulting translation will be a good one. 
N-gram LM 
The fragmentation feature is rather weak. It does 
not distinguish between words which are more 
likely to be seen in the target language and words 
which are less likely to be used.  To generate sen-
73
tences which are not only grammatically correct, 
but also use words and word sequences that are 
more natural and more common, data-driven ma-
chine translation systems use a n-gram language 
model.  To get the same benefit in the Xfer system, 
an n-gram LM has been integrated with the engine.   
This has the advantage that in the case of prun-
ing, the LM score can be used to avoid pruning 
good hypotheses, in addition to re re-rank the final 
translations. 
For our experiments, a suffix array language 
model based on the SALM toolkit (Zhang & Vo-
gel, 2006) is used. 
Length Model 
To adjust for the length of the translations gener-
ated by the system, the difference between the 
number of words generated and the expected num-
ber of words is added as a very simple feature. The 
expected length is calculated by multiplying the 
source sentence length by the ratio of the number 
of target and source words in the training corpus. 
The effect of this feature is to balance globally the 
length of the translations. 
2.5 Pruning 
To deal with the combinatorial explosion during 
the parsing/translation process, pruning has to be 
applied. Only the n top-ranking hypotheses are 
kept in each cell of the chart. The ranking of these 
partial translations is based on their language 
model score, which at this time is only an ap-
proximation, as the true history has not been seen 
and cannot be taken into account. 
3 Building a Xfer System for a New Do-
main 
A major bottleneck in developing a RBMT system 
for a new translation task (a new language pair or a 
new domain) is writing the grammar and building 
the lexicon. Automatic grammar induction using 
statistical alignments has been studied in (Probst 
2005).   
Here, we start with an existing grammar and 
augment the baseline lexicon with entries to cover 
the new domain. We explore semi-automatic lexi-
con generation for fast adaptation to the travel do-
main (Section 3.2). 
3.1 Test Data: BTEC Corpus 
For initial evaluation on unseen data, we selected 
the Basic Travel Expression Corpus (BTEC) 
(Takezawa et al 2002), which has been used in the 
evaluation campaigns in connection with the Inter-
national Workshop on Spoken Language Transla-
tion (IWSLT 2006). Besides still being currently 
used to build real systems (Shimizu et al 2006; 
Nakamura, et al 2006), this corpus contains rela-
tively simple sentences that are comparable to the 
ones initially corrected by users, and which are 
covered by the baseline manual grammar. 
As our test set, we used 506 English sentences 
for which two sets of Spanish reference transla-
tions were available. Table 1 shows corpus statis-
tics for the BTEC data. 
 
Data  English 
Sentences Pairs 123,416 
Sentence Length   7.3 
Word Tokens  903,525 
 
Train 
Word Types  12,578 
Sentence Pairs 506 
Word Tokens 3,764 
Word Types 776 
 
 
BTEC 
 
 
 
 
 
Test  
Coverage Test 756 (97%) 
Table 1: Corpus Statistics for the BTEC corpus 
3.2 Semi-Automatic Generation of the 
Transfer Lexicon 
The Transfer-Based system relies on a lexicon that 
contains POS, gender and number agreement, 
among other linguistic features. To adjust the sys-
tem quickly to a new task, we decided to leverage 
from statistical alignment models to generate word 
and phrase alignments as candidates for the trans-
fer lexicon. 
In the first step, we trained statistical lexicons 
using the well-known IBM1 word alignment 
model: one for the directions Spanish to English, 
and one for the direction English to Spanish. As 
multi-word entries, are often needed ([valuables] 
? [objetos de valor], [reception desk] ?[recep-
ci?n], [air conditioner]?[aire acondicionado]), we 
used phrase alignment techniques to create transla-
tion candidates for words and 2-word phrases. The 
phrase alignment also generates multi-word trans-
lations for single source words. With reasonably 
tight pruning, a manageable phrase translation ta-
74
ble was generated. This first step took about 5 
hours. 
The next step, manually cleaning the translation 
table, annotating them with parts-of-speech, and 
with agreement and tense constraints, was initially 
restricted to those items that overlapped with the 
vocabulary of our development test set, and took 
two days. 
The statistically generated lexicon comprises 
1,248 lexical entries, whereas the initial manual 
lexicon contained 479 lexical entries. For our 
BTEC experiments, we combined both lexicons. 
3.3 Xfer Results with No Ranking 
To determine how the Xfer system would perform 
only on the basis of the lexicon and grammar, we 
ran one translation experiment in which no lan-
guage model was used. This experiment was also 
intended to see if the refined grammar would lead 
to better translations. We took the first-best transla-
tion output by the system without using any statis-
tical components to rank alternative translations. 
 
System METEOR BLEU NIST
Baseline 0.5666 0.2745 5.88 
Refined 0.5676 0.2559 5.62 
Table 2: Automatic metric scores for a purely 
Rule-Based MT System. 
 
Table 2 shows that, in this crude setting, differ-
ent automatic metrics do not agree on the transla-
tion accuracy of both systems. On one hand, 
METEOR (Lavie et al 2004), which has been 
shown to correlate well with human judgments 
(Snover et al 2006), indicates that the refined sys-
tem outperforms the baseline system (as measured 
by the latest version v0.5.1,).  On the other hand, 
both BLEU (Papineni et al, 2002) and NIST 
(Doddington 2002) scores are higher for the base-
line system (mteval-v11b.pl).  
However, human inspection revealed that the re-
fined grammar is able to augment the n-best list 
with correct translations that the baseline system 
was not able to generate. This suggests that these 
results reflect poor re-ranking and not n-best list 
quality. In the next section, we describe an oracle 
experiment to measure n-best list quality of both 
systems.  
3.4 Oracle Experiment 
Oracle scores provide an upper-bound in perform-
ance. For the BTEC test set, we approximated a 
human oracle by calculating automatic metric 
scores for METEOR and for BLEU and NIST. 
Given 100-best lists for each source language 
sentence, we selected the best translation hypothe-
sis for each automatic metric separately. 
These scores reflect the fact that automatic re-
finements are able to feed the n-best list with better 
translations, as evulated by comparison against 
human reference translations. Even with a small set 
of independent user corrections, the refined system 
shows potential improved translation quality as 
indicated by higher scores for all three automatic 
evaluation metrics in Table 3. 
 
System METEOR BLEU NIST
Baseline 0.6863 0.4068 7.42 
Refined 0.6954 0.4215 7.51 
Table 3: Automatic metric oracle scores based on a 
100-best list 
 
Moreover, oracle scores provide the margin that 
we can gain when improving on the re-ranking of 
the n-best list produced by the Xfer engine. 
3.5 Xfer Results with Initial Ranking 
As expected, when the Xfer system is run in com-
bination with a LM1 as well as the fragmentation 
penalty, automatic metric scores for the 1-best hy-
pothesis are significantly higher (Table 4), than 
when just using the first translation output by the 
Xfer system alone (Table 2). 
 
System METEOR BLEU NIST
Baseline 0.6176 0.3425 6.53 
Refined 0.6222 0.3513 6.56 
Table 4: Automatic metric scores for 1-best de-
coder hypothesis. 
 
These results are lower than the oracle scores for 
both the baseline and the refined system (Table 3), 
which is also to be expected. However, the impor-
tant thing to notice from these results is that, like in 
the oracle case, the refined system consistently 
outperforms the baseline MT system for all three 
automatic metrics. 
                                                     
1 The Suffix Array Language Model (SALM) was built using 
the 123,416 Spanish sentences from the training data. 
75
The difference between the baseline and the re-
fined system in terms of 1-best scores is slightly 
smaller than the difference between oracle scores, 
which means that the decoder can not fully lever-
age the improvements made in the grammar. This 
indicates that the decoder fails to select the best 
translation in most cases. 
4 Adding Statistical Components to a Re-
Ranker 
The information used in the Xfer system to rank 
alternative translations is limited.  Essentially, it is 
the n-gram LM, which is the most important com-
ponent, a simple sentence length model, and the 
fragmentation score, which measures if a com-
pletely spanning parse could be found or if the 
translation is glued together from partial parses. 
Given an n-best list of translations for each source 
sentence, we can apply additional models to re-
rank these n-best list, hopefully pushing more good 
translations into the first rank. We studied the ef-
fect of adding different features to the n-best lists: 
lexical features and rule (type) probability features. 
4.1 Word-To-Word Probabilities 
In SMT systems, rescoring with an IBM1 model-
like word alignment score has become a standard 
feature. We use two word-to-word lexicons (Eng-
lish?Spanish and Spanish?English) to calculate 
sentence translation probabilities, based on word-
to-word probabilities: 
??= )|(1)|( jiI sepJseP       Eq.1 
and: 
??= )|(1)|( ijJ espIesP       Eq.2 
 
Here, we denote the English words with e, the 
Spanish words with s, the sentence lengths are 
given by I and J.  In the IBM1 alignment model, 
the position alignment is a uniform distribution p( i 
| j ) = 1/I for Spanish to English and p( j | i ) = 1/J 
for English to Spanish.  For Spanish to English, we 
have the additional factor of (1/I)J, i.e. longer 
translations get a smaller probability, and for En-
Sp we have (1/J)I, which again gives a bias to-
wards shorter translations.  To compensate for this 
bias, we use probabilities normalized to the sen-
tence length. Table 5 shows that adding the lexical 
probabilities improves the 1-best translation score.  
However, there is no significant difference when 
using different normalization of the lexicon prob-
abilities. The length bias introduced by different 
lexicon features can be balanced by the decoder?s 
length feature. 
 
 BLEU NIST 
Refined 0.3513 6.56 
+Lex Prob 0.3755 6.88 
Table 5: Comparing 1-best scores with scores 
result of rescoring the n-best list with lexical fea-
tures. 
4.2 Rule Probabilities 
The Xfer MT system can display the derivation 
tree showing the rules applied during translation. 
This allows rescoring the translations with rule 
probabilities. However, there is no annotated cor-
pus from which the rule probabilities could be es-
timated. As an approximation to such a training 
corpus, we decided to run the Xfer system over the 
training data and to generate n-best lists with trans-
lations and translation trees. Overall, about 6 mil-
lion parse trees were generated.  Using this data to 
estimate rule probabilities is definitely not ideal, as 
the translation on the training data are far from per-
fect, especially as not all the vocabulary has so far 
been added to the Xfer lexicon.  By averaging over 
all n-best translations a reasonable smoothing is to 
be expected. 
We used this information in three ways.  We es-
timated conditional probabilities rule r given rule-
type R, i.e. the distribution over different VP rules 
or NP rules. For each derivation D the overall 
probability was then calculated as: 
?= )|()( RrpDP                             Eq. 3 
As an alternative, we just build n-gram language 
models, one on the rule level and on the rule type 
level: 
? ??= )...|()( 1rrrpDP n                      Eq. 4 
? ??= )...|()( 1RRRpDP n                   Eq. 5 
 
Overall, 1,685 different rules and 19 rule types 
were seen in the training data. For models 2 and 3, 
we used the suffix array LM once again to allow 
for arbitrary long histories. Even though it often 
backs-off to 3-gram, 2-gram or even unigram prob-
abilities. 
76
In Table 6, we can see the effect of adding these 
LMs as additional features to the system and run-
ning MER training. 
 
 BLEU NIST
Refined 0.3513 6.56 
Lex. Prob. 0.3755 6.88 
Cond. Prob. 0.3728 6.81 
Rule LM 0.3717 6.74 
Rule Type LM 0.3736 6.78 
Table 6: BLEU scores when rescoring the n-
best list with different rule probability features (as 
well as the n-gram LM). 
5 MER Training 
Like in SMT systems, in the Xfer engine transla-
tions are ranked to their total cost, which is a 
weighted linear combination of the individual 
costs. When adding more features to the translation 
system, a careful balancing of the individual con-
tributions can make a significant difference. How-
ever, with each feature added, manually tuning the 
system becomes less and less practical, and auto-
matic optimization becomes necessary. 
Different optimization techniques are available, 
like the Simplex algorithm or the special Minimum 
Error Training as described in (Och 2003). In 
Minimum Error Rate (MER) training, the n-best 
list generated by the translation system is used to 
find feature weight, thereby re-ranking the n-best 
list. This improves the match between the 1-best 
translation and given reference translations. Opti-
mization can use any metric as objective function.  
Typically, systems are tuned towards high BLEU 
or high NIST scores, more recently also towards 
METEOR or TER (Snover et al 2006). 
We used a MER training module (Venugopal), 
originally developed for an SMT system, to run 
MER training on the n-best lists generated by the 
Xfer system. This implementation allows for opti-
mization towards BLEU and NIST mteval metrics. 
5.1 Results 
In Table 7, we summarize some of the results from 
different n-best list rescoring experiments.  Using 
only the Xfer engine, without language model, 
gives a very low score, as the selection is based 
only on the fragmentation score. 
Adding the n-gram language model gives a huge 
improvement. Adding additional features leads to 
more then 2 BLEU points improvement. However, 
there is not much difference when using different 
feature combinations. It seems that the rather small 
size of the n-best list is a limiting factor.  
When setting the optimal weights in the Xfer 
engine for the LM and fragmentation penalty 
scores obtained from MER training, both the base-
line and the refined system get higher scores, not 
only according to BLEU, which was used as the 
objective function, but also according to METEOR 
and NIST automatic evaluation metrics (Table 8). 
 
 System + Statistical Components 1-best 
Rule Based Xfer 0.2559 
+ Stat. Comp. Xfer + LM + Frag 0.3513 
POS LM 0.3180 
Rule Probabilities (Prob.) 0.2593 
LM + Rule Type LM 0.3736 
LM + Frag/Len + Rule Type LM 0.3737 
LM + POS + Rule LM 0.3744 
LM + Frag + Rule Type LM + Cond. Rule Prob. 0.3743 
LM + Len + Rule Type LM + Cond. Rule Prob. 0.3745 
LM + POS + Rule LM + Cond. Rule Prob. 0.3741 
LM + Frag + Len + Rule Type LM + Rule Prob. 0.3746 
 
 
 
Optimizing 
weights 
with 
MER training 
LM + Frag + Len + POS + Rule LM + Rule Prob. 0.3741 
Table 7:  BLEU scores for the Refined MT System as the weights for the different statistical components 
described in Section 2.4 and 4 are optimized with MER Training. 
 
77
Moreover, the difference between the Baseline 
and the Refined system after MER training is sta-
tistically significant2, whereas this was not the case 
for the initial ranking results (Table 4). 
 
 
 
Table 8: Automatic metric scores for 1-best de-
coder hypothesis, after LM and Fragmentation 
weights have been optimized. 
 
Table 9 shows a few examples from the BTEC cor-
pus with 1-best translations output by the Refined 
MT system before (No Optimization) and after 
(With Optimization) MER training, given LM and 
Fragmentation penalty scores. From these exam-
ples, it can be observed that re-ranking improves 
after optimizing the LM and fragmentation 
weights. In particular, order issues get resolved 
(examples 1, 2 and 4), which result in correct de-
terminer agreement (1 and 2); determiner insertion 
(3); correct verb form (5 and 7) and omission of 
incorrect pronouns (6 and 7).   
6 Conclusion 
Starting from a Transfer-Based translation system, 
we explored techniques currently used in statistical 
translation systems to rapidly adapt to a new do-
main and to improve its performance.  Using word 
and phrase alignment techniques allowed us to 
quickly augment the transfer lexicon. Adding a 
statistical language model is crucial in selecting 
good translations from the n-best lists generated by 
the Xfer engine. Adding additional features, such 
as word-to-word probabilities and rule (type) prob-
abilities, further improves performance. 
While this information would ideally be used in 
the parsing and transfer steps of the translation sys-
tem, our initial experiments were targeted at using 
this in an n-best list rescoring setup. As rule prob-
abilities were estimated from noisy training data, 
these models are far from optimal.   
To facilitate the experiments with the Xfer sys-
tem, especially when adding more and more fea-
tures, we added a Minimum Error Rate training 
                                                     
2 According to the standard paired two-tailed t-Test, the de-
coder METEOR scores with optimized weights are statisti-
cally significant, with a p value of 0.0051. 
component. Having such a component will defi-
nitely boost the development of the Xfer engine. 
We see statistically significant improvements 
over the baseline system when using optimized 
weights for the word-level language model and the 
fragmentation score.  
System METEOR BLEU NIST
Baseline 0.6184 0.3609 6.68 
Refined 0.6231 0.3780  6.79  1 Source: where is the boarding gate ?   
   NO: d?nde est? el embarque puerta ? 
   WO: d?nde est? la puerta embarque ? 
2 Src: where is the bus stop for city hall ? 
  NO: d?nde est? el autob?s parada para ayuntamiento ? 
  WO: d?nde est? la parada autob?s para ayuntamiento ? 
3 Src: i would like a twin room with a bath please . 
   NO: me gustar?a habitaci?n una cama doble con un 
           ba?o por favor . 
   WO: me gustar?a una habitaci?n cama doble con un 
            ba?o por favor . 
4 Src:  i would like to buy some duty-free items . 
  NO: me gustar?a  comprar algunos duty-free productos. 
  WO: me gustar?a  comprar algunos art?culos duty-free . 
5 Src: does he speak japanese ? 
   NO: ?l hablar a japon?s ? 
   WO: habla japon?s ? 
6 Src: it is just round the corner . 
   NO: lo es simplemente a la vuelta de la esquina . 
   WO: es simplemente a la vuelta de la esquina . 
7  Src: do you sell duty-free items ?   
    NO: te venden art?culos duty-free ? 
    WO: vend?is art?culos duty-free ? 
Table 9: 1-best translations from the BTEC test set 
output by the Refined MT system before and after 
MER training. NO stands for No Optimization of 
LM and Fragmentation weights, and WO stands 
for With Optimization of weights. 
7 Future Work 
Using rule probabilities has shown to be a promis-
ing extension to the current Xfer system.  We plan 
to improve these models by selecting the oracle 
best translations from the n-best list generated on 
the training data. This will reduce the noise in the 
training stage. Ultimately, the rule probabilities 
should be applied not as an n-best list rescoring 
step, but directly in the Xfer engine decoder. 
Analyzing the translation results, one important 
shortcoming became obvious. Currently the trans-
lation lexicon only covers about 88% of the words 
that appear in the reference translations. This se-
verely limits as to what kind of BLEU score we 
can achieve. When we generated the phrasal lexi-
con from the BTEC training data, we deliberately 
78
chose to only include few alternatives, mainly to 
limit the manual labor when adding POS and con-
straint. We expect that the Xfer system will sig-
nificantly benefit from further expanding the 
lexicon. 
References 
Allen, J. 1995. Natural Language Understanding. Sec-
ond Edition ed. Benjamin Cummings. 
 Chiang, D. 2005. A hierarchical phrase-based model 
for statistical machine translation. In Proceedings of 
the 43rd Annual Meeting of the Association for 
Computational Linguistics (ACL), Ann Arbor, USA. 
Doddington G. 2002. Automatic evaluation of machine 
translation quality using n-gram co-occurrence sta-
tistics. In Proc. of the HLT 2002, San Diego, USA. 
Hutchins, W. J., and H. L. Somers. 1992. An Introduc-
tion to Machine Translation. London: Academic 
Press. 
Font Llitj?s, A. and W. Ridmann. 2007 The Inner 
Works of an Automatic Rule Refiner for Machine 
Translation. METIS-II Workshop, Leuven, Belgium. 
IWSLT 2006: http://www.slt.atr.jp/IWSLT2006/
Lavie, A., K. Sagae and S. Jayaraman. 2004. The Sig-
nificance of Recall in Automatic Metrics for MT 
Evaluation. AMTA, Washington DC, USA. 
Nakamura, S., K. Markov, H. Nakaiwa, G. Kikui, H. 
Kawai, T. Jitsuhiro, J. Zhang, H. Yamamoto, E. 
Sumita, and S. Yamamoto. 2006. The ATR multilin-
gual speech-to-speech translation system. IEEE 
Trans. on Audio, Speech, and Language Processing, 
14, No.2:365?376. 
NIST MT Evaluations: 
http://www.nist.gov/speech/tests/mt/
Och, F. J. 2003. Minimum error rate training in statisti-
cal machine translation.  In Proc. of the 41st Annual 
Meeting of the Association for Computational Lin-
guistics (ACL), Sapporo, Japan. 
Papineni, K, S. Roukos, T. Ward, and W. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. In Proc. of the 40th ACL, Phila-
delphia, USA. 
Peterson, E. 2002. Adapting a transfer engine for rapid 
machine translation development. M.S. Thesis, 
Georgetown University. 
Probst, K., Brown, R., Carbonell, J., Lavie, A. Levin, 
and L., Peterson, E., 2001. Design and Implementa-
tion of Controlled Elicitation for Machine Transla-
tion of Low density Languages. Proceedings of the 
MT2001 workshop at MT Summit, Santiago de 
Compostela, Spain. 
SALM Toolkit: 
http://projectile.is.cs.cmu.edu/research/public/tools/s
alm/salm.htm 
Shimizu T., Y. Ashikari, E. Sumita, H. Kashioka and  S. 
Nakamura. 2006. Development of client-server 
speech translation system on a multi-lingual speech 
communication platform. IWSLT, Kyoto, Japan. 
Snover, M; B. Dorr, R. Schwartz, L. Micciulla, 2006. 
Targeted Human Annotation. AMTA, Boston, USA.  
Takezawa, T, E. Sumita, F. Sugaya, H. Yamamoto, and 
S. Yamamoto, 2002. Toward a Broad-Coverage Bi-
lingual Corpus for Speech Translation of Travel 
Conversations in the Real World. In Proceedings of 
3rd LREC, Las Palmas, Spain. 
TC-STAR Evaluations: http://www.tc-star.org/
Venugopal, A.:  MER Training Toolkit. 
http://www.cs.cmu.edu/~ashishv/mer.html
Wu, D. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23:377?404. 
Yamada, Kenji and Kevin Knight. 2001. A syntax-based 
statistical translation model. In Proceedings of the 
39th Annual Meeting of the ACL, Toulouse, France. 
Zhang, Y and S. Vogel. 2006. Suffix Array and its Ap-
plications in Empirical Natural Language Process-
ing,. Technical Report CMU-LTI-06-010, Pittsburgh 
PA, USA. 
Zhang, Y, A. S. Hildebrand and S. Vogel. 2006.  Dis-
tributed Language Modeling for N-best List Re-
ranking. Empirical Methods in Natural Language 
Processing (EMNLP), Sydney, Australia.  
Zollmann A. and A. Venugopal. 2006. Syntax Aug-
mented Machine Translation via Chart Parsing. In 
Proc. of NAACL 2006 - Workshop on Statistical 
Machine Translation, New York, USA. 
79
