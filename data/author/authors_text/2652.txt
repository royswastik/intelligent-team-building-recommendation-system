Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 41?48,
New York City, June 2006. c?2006 Association for Computational Linguistics
Catching Metaphors
Matt Gedigian, John Bryant, Srini Narayanan, and Branimir Ciric
International Computer Science Institute
1947 Center Street. Suite 600
Berkeley, CA 94704, USA
{gedigian, jbryant, snarayan}@icsi.berkeley.edu
Abstract
Metaphors are ubiquitous in language and
developing methods to identify and deal
with metaphors is an open problem in
Natural Language Processing (NLP). In
this paper we describe results from us-
ing a maximum entropy (ME) classifier
to identify metaphors. Using the Wall
Street Journal (WSJ) corpus, we anno-
tated all the verbal targets associated with
a set of frames which includes frames of
spatial motion, manipulation, and health.
One surprising finding was that over 90%
of annotated targets from these frames
are used metaphorically, underscoring the
importance of processing figurative lan-
guage. We then used this labeled data and
each verbal target?s PropBank annotation
to train a maximum entropy classifier to
make this literal vs. metaphoric distinc-
tion. Using the classifier, we reduce the
final error in the test set by 5% over the
verb-specific majority class baseline and
31% over the corpus-wide majority class
baseline.
1 Introduction
To move beyond ?factoid? style questions, question
answering systems must rely on inferential mecha-
nisms. To answer such commonplace questions as
Which train should I take to get to the airport? re-
quires justifications, predictions and recommenda-
tions that can only be produced through inference.
One such question answering system (Narayanan
and Harabagiu, 2004) takes PropBank/FrameNet an-
notations as input, uses the PropBank targets to in-
dicate which actions are being described with which
arguments and produces an answer using probabilis-
tic models of actions as the tools of inference. Initi-
ating these action models is called simulation.
Such action models provide deep inferential capa-
bilities for embodied domains. They can also, when
provided with appropriate metaphoric mappings, be
extended to cover metaphoric language (Narayanan,
1997). Exploiting the inferential capabilities of such
action models over the broadest domain requires a
system to determine whether a verb is being used lit-
erally or metaphorically. Such a system could then
activate the necessary metaphoric mappings and ini-
tiate the appropriate simulation.
2 Metaphor
Work in Cognitive Semantics (Lakoff and Johnson,
1980; Johnson, 1987; Langacker, 1987; Lakoff,
1994) suggests that the structure of abstract actions
(such as states, causes, purposes, and means) are
characterized cognitively in terms of image schemas
which are schematized recurring patterns from the
embodied domains of force, motion, and space.
Consider our conceptualization of events as ex-
emplified in the mapping called the Event Structure
Metaphor.
? States are locations (bounded regions in space).
? Changes are movements (into or out of
bounded regions).
41
? Causes are forces.
? Actions are self-propelled movements.
? Purposes are destinations.
? Difficulties are impediments to motion.
This mapping generalizes over an extremely wide
range of expressions for one or more aspects of event
structure. For example, take states and changes. We
speak of being in or out of a state, of entering or
leaving it, of getting to a state or emerging from it.
This is a rich and complex metaphor whose parts
interact in complex ways. To get an idea of how
it works, consider the submapping Difficulties are
impediments to motion. In the metaphor, purpose-
ful action is self-propelled motion toward a destina-
tion. A difficulty is something that impedes such
motion. Metaphorical difficulties of this sort come
in five types: blockages; features of the terrain; bur-
dens; counterforces; lack of an energy source. Here
are examples of each: Blockages: He?s trying to get
around the regulations. We?ve got him boxed into
a corner. Features of the terrain: It?s been uphill all
the way. We?ve been hacking our way through a jun-
gle of regulations. Burdens: He?s carrying quite a
load. Get off my back! Counterforces: Quit pushing
me around. She?s leading him around by the nose.
Lack of an energy source: I?m out of gas. We?re run-
ning out of steam.
In summary, these metaphors are ontological
mappings across conceptual domains, from the
source domain of motion and forces to the target do-
main of abstract actions. The mapping is conven-
tional, that is, it is a fixed part of our conceptual sys-
tem, one of our conventional ways of conceptualiz-
ing actions. Conventional metaphors capture gener-
alizations governing polysemy, over inference pat-
terns, and governing novel metaphorical language
(Lakoff and Turner, 1989).
2.1 Metaphors vs. Different Word Senses
Presumably, one could treat the metaphoric usage of
run as a different sense, much in the same way that
move forward on a business plan is treated as a dif-
ferent sense from literal move forward. From a pars-
ing/information extraction point of view, these two
approaches are equivalent in terms of their represen-
tational requirements.
The benefit of employing the metaphor-based ap-
proach, as suggested in the introduction, comes
when performing inference. As shown by
(Narayanan, 1997), a metaphorical usage and a lit-
eral usage share inferential structure. For example,
the aspectual structure of run is the same in either
domain whether it is literal or metaphorical usage.
Further, this sharing of inferential structure between
the source and target domains simplifies the repre-
sentational mechanisms used for inference making
it easier to build the world models necessary for
knowledge-intensive tasks like question answering
(Sinha and Narayanan, 2005).
3 Objective
While this work in Cognitive Semantics is sugges-
tive, without a corpus-based analysis, it is hard to
accurately estimate the importance of metaphoric in-
formation for Natural Language Processing (NLP)
tasks such as Question Answering or Information
Distillation. Our work is a first step to remedy this
situation. We start with our computational defini-
tion of metaphor as a mapping from concrete to ab-
stract domains. We then investigate the Wall Street
Journal (WSJ) corpus, selecting a subset of its ver-
bal targets and labeling them as either metaphoric
or literal. While we had anticipated the pervasive-
ness of metaphor, we could not anticipate just how
pervasive with over 90% of the labeled data being
metaphoric.
Provided with labeled training data, our task is to
automatically classify the verbal targets of unseen
utterances as either metaphoric or literal. Motivated
by the intuition that the types of a target?s arguments
are important for making this determination, we ex-
tracted information about the arguments from the
PropBank (Kingsbury et al, 2002) annotation for
each sentence, using WordNet (Fellbaum, 1998) as
the type hierarchy.
3.1 Using Verbal Arguments
A metaphor is a structured mapping between the
roles of two frames that makes it possible to describe
a (usually) more abstract concept in terms of a more
concrete one (Lakoff and Johnson, 1980). The more
abstract concept is referred to as the target domain
while the more concrete concept is referred to as the
42
1. MET : Texas Air has {run} into difficulty...
2. LIT : ?I was doing the laundry and nearly
broke my neck {running} upstairs to see ...
Figure 1: Examples taken from the WSJ Corpus.
MET indicates a metaphoric use of the target verb
and LIT indicates a literal use.
source domain. More precisely, the metaphor maps
roles of the target frame onto the source frame.
Figure 1 shows some example sentences with a
particular verbal target run in curly braces. Example
1 is a metaphoric usage (marked by MET) of run
where the destination role is filled by the state of
difficulty. Example 2 is a literal usage (marked by
LIT) of run.
The arguments of a verb are an important fac-
tor for determining whether that verb is being used
metaphorically. If they come from the source do-
main frame, then the likelihood is high that the verb
is being used literally. In the example literal sen-
tence from Figure 1, the theme is a person, which is
a physical object and thus part of the source domain.
If, on the other hand, the arguments come from
the target domain, then it is likely that the verb is
being used metaphorically. Consider the metaphor-
ical run from Figure 1. In that case, both the theme
and the goal of the action are from the target domain.
Thus any approach that tries to classify sentences as
literal or metaphoric must somehow incorporate in-
formation about verbal arguments.
4 Data
Because no available corpus is labeled for the
metaphoric/literal distinction, we labeled a subset
of the WSJ corpus for our experiments. To focus
the task, we concentrated on motion-related frames
that act as the source domain for the Event Structure
Metaphor and some additional non-motion based
frames including Cure and Placing. Figure 2 shows
the selected frames along with example lexical units
from each frame.
To identify relevant sentences we first obtained
from FrameNet a list of lexical units that evoke
the selected source frames. Since WSJ is labeled
with PropBank word senses, we then had to deter-
mine which PropBank senses correspond to these
Frame Example LUs
Motion float, glide, go, soar
Motion-directional drop, fall, plummet
Self-motion amble, crawl, hobble
Cause-motion catapult, haul, throw, yank
Cotheme accompany, escort, pursue
Placing cram, heap, pocket, tuck
Cure cure, ease, heal, treat
Figure 2: The frames selected for annotation and
some of the lexical units that evoke them.
Cure Frame LU PropBank Sense
alleviate alleviate.01
cure cure.01
ease ease.02
heal heal.01
rehabilitate rehabilitate.01
resuscitate resuscitate.01
treat treat.03
Figure 3: The lexical units that evoke the Cure frame
and each unit?s associated PropBank sense2.
FrameNet lexical items. The lexical items that evoke
the Cure frame and the corresponding PropBank
senses are shown in Figure 3.
As anyone who has inspected both PropBank and
FrameNet can attest, these two important lexical
resources have chosen different ways to describe
verbal senses and thus in many cases, determining
which PropBank sense corresponds to a particular
FrameNet sense is not a straightforward process.
Verbs like slide have a single PropBank sense used
to describe both the slid in The book slid off the ta-
ble and the slid in I slid the book off the table. While
FrameNet puts slide both in the Motion frame and
in the Cause-motion frame, PropBank uses the argu-
ment labeling to distinguish these two senses.
Periodically, PropBank has two senses, one for
the literal interpretation and one for the metaphoric
interpretation, where FrameNet uses a single sense.
Consider the word hobble and its two senses in Prop-
Bank:
? hobble.01 ?walk as if feet tied together?
? hobble.02 ?tie the feet of, metaphorically ?hin-
der??
43
Frame #MET #LIT Total %MET
Cause-motion 461 44 505 91
Cotheme 926 8 934 99
Motion-directional 1087 21 1108 98
Placing 888 110 998 89
Self-motion 424 86 510 83
Cure 105 26 131 80
All Frames 3891 295 4186 93
Figure 4: The number of targets annotated
metaphoric or literal, broken down by frame.
Because we intended to classify both literal and
metaphoric language, both PropBank senses of hob-
ble were included. However most verbs do not have
distinct literal and metaphoric senses in PropBank.
The final step in obtaining the relevant portion of
theWSJ corpus is to use the lists of PropBank senses
that corresponding to the FrameNet frames and ex-
tract sentences with these targets. Because the Prop-
Bank annotations label which PropBank sense is be-
ing annotated, this process is straightforward.
Having obtained the WSJ sentences with items
that evoke the selected source frames, we labeled the
data using a three-way split:
? MET: indicating metaphoric use of the target
? LIT: indicating literal use of the target
? ? : indicating a target that the annotator was
unsure of
For our experiments, we concentrated only on those
cases where the label was MET or LIT and ignored
the unclear cases.
As is shown in Figure 4, the WSJ data is heav-
ily weighted towards metaphor over all the frames
that we annotated. This tremendous bias towards
metaphoric usage of motion/cause-motion lexical
items shows just how prevalent the Event Structure
Metaphor is, especially in the domain of economics
where it is used to describe market fluctuations and
policy decisions.
Figure 5 shows the breakdown for each lexical
item in the Cure frame. Note that most of the fre-
quently occurring verbs are strongly biased towards
either a literal or metaphoric usage. Ease, for ex-
ample, in all 81 of its uses describes the easing of an
Lexical Unit #MET #LIT
alleviate 8 0
cure 7 3
ease 81 0
heal 3 0
rehabilitate 1 0
resuscitate 2 0
treat 3 23
Figure 5: The lexical units that evoke the Cure frame
and each unit?s counts for metaphoric (#MET) and
literal (#LIT) usage.
economic condition and not the easing of pain. Treat
on the other hand, is overwhelmingly biased towards
the treating of physical and psychological disorders
and is only rarely used for an abstract disorder.
5 The Approach
As has been discussed in this paper, there are at
least two factors that are useful in determining
whether the verbal target of an utterance is being
used metaphorically:
1. The bias of the verb
2. The arguments of the verbal target in that utter-
ance
To determine whether the arguments suggest
a metaphoric or a literal interpretation, the sys-
tem needs access to information about which con-
stituents of the utterance correspond to the argu-
ments of the verbal target. The PropBank annota-
tions fill this role in our system. For each utterance
that is used for training or needs to be classified, the
gold standard PropBank annotation is used to deter-
mine the verbal target?s arguments.
For every verbal target in question, we used the
following method to extract the types of its argu-
ments:
1. Used PropBank to extract the target?s argu-
ments.
2. For each argument, we extracted its head using
rules closely based on (Collins, 1999).
44
Feature Schema Example Instantiation Comment
verb verb=treat The verbal target
ARG0 TYPE uninstantiated ARG0 (Doctor role) not present
ARG1 TYPE uninstantiated ARG1 (Patient role) not present
ARG2 TYPE ARG2 TYPE=anemia The WordNet type is anemia.
ARG3 TYPE ARG3 TYPE=drug The WordNet type is drug.
Figure 6: The feature schemas used for classification. The instantiated features are drawn from the sentence
The drug is being used primarily to {treat} anemias.
3. If the head is a pronoun, use the pronoun type
(without coreference resolution) as the type of
the argument.
4. If the head is a named entity, use the Identi-
finder tag as the type of the argument (BBN
Identifinder, 2004).
5. If neither, use the name of the head?s WordNet
synset as the type of the argument.
Consider the sentence The drug is being used pri-
marily to {treat} anemias. The PropBank annota-
tion of this sentence marks the drug as ARG3 and
anemias as ARG2. We turned this information into
features for the classifier as shown in Figure 6.
The verb feature is intended to capture the bias
of the verb. The ARGX TYPE feature captures the
type of the arguments directly. To measure the trade-
offs between various combinations of features, we
randomly partitioned the data set into a training set
(65% of the data), a validation set (15% of the data),
and a test set (20% of the data).
6 Results
6.1 Classifier Choice
Because of its ease of use and Java compatibility,
we used an updated version of the Stanford condi-
tional log linear (aka maxent) classifier written by
Dan Klein (Stanford Classifier, 2003). Maxent clas-
sifiers are designed to maximize the conditional log
likelihood of the training data where the conditional
likelihood of a particular class c on training example
i is computed as:
1
Z
exp(fi ? ?c)
Here Z is a normalizing factor, fi is the vector of
features associated with example i and ?c is the vec-
tor of weights associated with class c. Additionally,
the Stanford classifier uses by default a Gaussian
prior of 1 on the features, thus smoothing the fea-
ture weights and helping prevent overfitting.
6.2 Baselines
We use two different baselines to assess perfor-
mance. They correspond to selecting the major-
ity class of the training set overall or the major-
ity class of verb specifically. The strong bias to-
ward metaphor is reflected in the overall baseline of
93.80% for the validation set. The verb baseline is
higher, 95.50% for the validation set, due to the pres-
ence of words such as treat which are predominantly
literal.
6.3 Validation Set Results
Figure 7 shows the performance of the classifier on
the feature sets described in the previous section.
The overall and verb baselines are 605 and 616 out
of 645 total examples in the validation set.
The first feature set we experimented with was
just the verb. We then added each argument in turn;
trying ARG0 (Feature Set 2), ARG1 (Feature Set 3),
ARG2 (Feature Set 4) and ARG3 (Feature Set 5).
Adding ARG1 gave the best performance gain.
ARG1 corresponds to the semantic role of mover
in most of PropBank annotations for motion-related
verbs. For example, stocks is labeled as ARG1 in
both Stocks fell 10 points and Stocks were being
thrown out of windows3. Intuitively, the mover role
is highly informative in determining whether a mo-
tion verb is being used metaphorically, thus it makes
sense that adding ARG1 added the single biggest
3This is an actual sentence from the training set.
45
FSet Feature Schemas M L Total %Tot
1 verb 599/605 20/40 619/645 95.97
2 verb, ARG0 TYPE 601/605 17/40 618/645 95.81
3 verb, ARG1 TYPE 602/605 19/40 621/645 96.28
4 verb, ARG2 TYPE 600/605 19/40 619/645 95.97
5 verb, ARG3 TYPE 599/605 20/40 619/645 95.97
6 verb, ARG1 TYPE, ARG3 TYPE 602/605 19/40 621/645 96.28
7 verb, ARG1 TYPE, ARG2 TYPE, ARG3 TYPE 601/605 18/40 619/645 95.97
8 verb, ARG0 TYPE, ARG1 TYPE, ARG2 TYPE 602/605 18/40 620/645 96.12
9 verb, ARG0 TYPE, ARG1 TYPE, ARG2 TYPE, ARG3 TYPE 602/605 17/40 619/645 95.97
Figure 7: For each Feature Set, the feature schemas that define it, along with the ratio of correct to total
examples on the validation set for metaphor (M), literal (L) and total (Total) is shown.
jump in performance compared to the other argu-
ments.
Once we determined that ARG1 was the best ar-
gument to add, we also experimented with combin-
ing ARG1 with the other arguments. Validation re-
sults are shown for these other feature combinations
(Feature Sets 6,7, 8 and 9)
Using the best feature sets (Feature Sets 3,6), 621
targets are correctly labeled by the classifier. The
accuracy is 96.98%, reducing error on the validation
set by 40% and 17% over the baselines.
6.4 Test Set Results
We retrained the classifier using Feature Set 3 over
the training and validation sets, then tested it on the
test set. The overall and verb baselines are 800 and
817 out of 861 total examples, respectively. The
classifier correctly labeled 819 targets in the test set.
The results, broken down by frame, are shown in
Figure 8. The final accuracy of 95.12%, represents
a reduction of error by 31% and 5% over the base-
lines.
6.5 Discussion
A comprehensive assessment of the classifier?s
performance requires a measure of interannotator
agreement. Interannotator agreement represents a
ceiling on the performance that can be expected on
the classification task. Due to the very high base-
line, even rare disagreements by human annotators
affects the interpretation of the classifier?s perfor-
mance. Unfortunately, we did not have the resources
available to redundantly annotate the corpus.
We examined the 42 remaining errors and catego-
rized them into four types:
? 13 fixable errors
? 27 errors caused by verbal biases
? 2 errors caused by bias in the training set
The fixable errors are those that could be fixed
given more experimentation with the feature sets and
more data. Many of these errors are probably caused
by the verbal bias, but a verbal bias that should not
be insurmountable (for example, 2 or 3 metaphor to
each 1 literal).
The 27 errors caused by verbal biases are ones
where the verb is so strongly biased to a particu-
lar metaphoric class that it is unsurprising that a test
example of the opposite class was missed. Verbs
like treat (0 metaphoric to 20 literal) and lead (345
metaphoric to 0 literal) are in this category.
The two remaining errors are cases where the verb
was not present in the training data.
7 Related Work
Previous work on automated metaphor detection
includes Fass (1991), Martin (1990), and Mason
(2004). Whereas our aim is to classify unseen
sentences as literal or metaphorical, these projects
address the related but distinct task of identifying
metaphorical mappings. All three use the selectional
preferences of verbs to identify metaphors. In lit-
eral usage, the arguments that fill particular roles of
a verb are frequently of a common type. For in-
stance, in the MEDICAL domain, the object of the
46
Frame M L Total %Tot %OBL %VBL
Cause motion 78/78 1/10 79/88 89.77 88.64 88.64
Cotheme 179/179 0/2 179/181 98.90 98.90 98.90
Cure 26/30 3/3 29/33 87.88 90.91 90.91
Motion directional 242/242 0/2 242/244 99.18 99.18 99.18
Placing 176/181 13/25 189/206 91.75 87.86 91.26
Self motion 87/90 14/19 101/109 92.66 82.57 91.74
All Frames 788/800 31/61 819/861 95.12 92.92 94.89
Figure 8: The results of the classifier on the test set, using Feature Set 6. For each frame, the ratio of correct
to total examples for metaphor (M), literal (L) and total (Total) is shown. The total percent correct for the
frame (%Tot), the overall baseline percentage (%OBL), and the verb baseline percentage (%VBL) are also
shown. The cumulative performance over all frames is located in the bottom row of the table.
verb treat is usually a pathological state. In the FI-
NANCE domain, the object of treat is usually an
economic problem. This difference in selectional
preference suggests metaphorical usage. Further-
more, it suggests a metaphorical mapping between
health problems and economic problems.
The systems described by Fass and Martin exhibit
impressive reasoning capabilities such as identify-
ing novel metaphors, distinguishing metaphor from
metonymy, and interpreting some metaphorical sen-
tences. But they require hand-coded knowledge
bases and thus have limited coverage and are dif-
ficult to extend. More similar to our efforts, Ma-
son?s CorMet uses a corpus-based approach. In
CorMet, domains are characterized by certain key-
words which are used to compile domain-specific
corpora from the internet. Based on differences in
selectional preferences between domains, CorMet
seeks to identify metaphorical mappings between
concepts in those domains.
One shortcoming of using syntactic arguments
is reflected by CorMet?s mistaken identification of
a mapping between institutions and liquids. This
arises from sentences like The company dissolved
and The acid dissolved the compound. Such sen-
tences suggest a mapping between the subjects in
the target domain, institutions, and the subjects in
source domain, liquids. Using semantic roles avoids
this source of noise. This is not to suggest that the
syntactic features are unimportant, indeed the selec-
tional preferences determined by CorMet could be
used to select which arguments to use for features in
our classifier.
Our approach considers each sentence in isola-
tion. However the distribution of metaphorical us-
age is not uniform in the WSJ corpus (Martin,
1994),. It is therefore possible that the information
about surrounding sentences would be useful in de-
termining whether a usage is metaphorical. CorMet
incorporates context in a limited way, computing
a confidence rating, based in part upon whether a
metaphoric mapping co-occurs with others in a sys-
tematic way.
8 Conclusion
Metaphors are a ubiquitous phenomenon in lan-
guage, and our corpus analysis clearly bears this out.
It is somewhat gratifying that with a judicious com-
bination of the available wide-coverage resources
(WordNet, FrameNet, PropBank) we were able to
build classifiers that could outperform the baseline
even in the most skewed cases. Our results show the
utility of our approach and more generally the matu-
rity of the current NLP technology to make progress
in attacking the challenging and important problem
of interpreting figurative language.
However, this is only the first step. As with all
semantic extraction methods and technologies, the
proof of utility is not in how good the extractor is
but how much it helps in an actual task. As far
as we can tell, this problem remains open for the
entire semantic parsing/role labeling/extraction field
despite the flurry of activity in the last four years. In
the case of metaphor interpretation, we have some
initial encouragement from the results published by
(Narayanan, 1997) and others.
47
Our classifier relies on PropBank senses, so we
can use the high performance classifiers available
for PropBank. The price is that we have to con-
struct mappings from FrameNet frames to PropBank
senses. However, this is a one-time effort pursued
by many groups, so this should not present a prob-
lem to extending our approach to cover all frames
and metaphors. Additionally, we are in the process
of linking the metaphor detector to a metaphor infer-
ence system. We hope to have initial results to report
on by conference time.
References
BBN Identifinder. 2004.
http://www.bbn.com/for government customers/
data indexing and mining/identifinder.html.
Michael Collins. 1999. Head-Driven Statistical Models
of Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
Dan Fass. 1991. Met*: a method for discriminating
metonymy and metaphor by computer. Comput. Lin-
guist., 17(1):49?90.
Christine Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Mark Johnson. 1987. The Body in the Mind: The Bodily
Basis of Meaning, Imagination and Reason. Univer-
sity of Chicago Press.
Paul Kingsbury, Martha Palmer, and Mitchell Marcus.
2002. Adding semantic annotation to the penn tree-
bank. In Proceedings of the Human Language Tech-
nology Conference.
George Lakoff and Mark Johnson. 1980. Metaphors We
Live By. University of Chicago Press.
George Lakoff and Mark Turner. 1989. More Than Cool
Reason: A Field Guide to Poetic Metaphor. University
of Chicago Press.
George Lakoff. 1994. The contemporary theory of
metaphor. In Andrew Ortony, editor, Metaphor and
Thought. Cambridge University Press.
Ronald Langacker. 1987. Foundations of Cognitive
Grammar I: Theoretical Prerequisites. Stanford Uni-
versity Press.
James Martin. 1990. Computational Model of Metaphor
Interpretation. Academic Press.
J.H. Martin. 1994. A corpus-based analysis of context
effects on metaphor comprehension. Technical report,
Boulder: University of Colorado: Computer Science
Department.
Zachary J. Mason. 2004. Cormet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Comput. Linguist., 30(1):23?44.
Srini Narayanan and Sanda Harabagiu. 2004. Question
answering based on semantic structures. In Proceed-
ings of the International Conference on Computational
Linguistics.
Srini Narayanan. 1997. Knowledge-Based Action Rep-
resentations for Metaphor and Aspect. Ph.D. thesis,
University of California at Berkeley.
Steve Sinha and Srini Narayanan. 2005. Model-based
answer selection. In Proceedings of the AAAI Work-
shop on Inference for Textual Question Answering.
Stanford Classifier. 2003.
http://nlp.stanford.edu/software/classifier.shtml.
48
