Proceedings of NAACL HLT 2007, Companion Volume, pages 133?136,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
An integrated architecture for speech-input multi-target machine translation
Alicia Pe?rez, M. Ine?s Torres
Dep. of Electricity and Electronics
University of the Basque Country
manes@we.lc.ehu.es
M. Teresa Gonza?lez, Francisco Casacuberta
Dep. of Information Systems and Computation
Technical University of Valencia
fcn@dsic.upv.es
Abstract
The aim of this work is to show the abil-
ity of finite-state transducers to simultane-
ously translate speech into multiple lan-
guages. Our proposal deals with an ex-
tension of stochastic finite-state transduc-
ers that can produce more than one out-
put at the same time. These kind of de-
vices offer great versatility for the inte-
gration with other finite-state devices such
as acoustic models in order to produce a
speech translation system. This proposal
has been evaluated in a practical situation,
and its results have been compared with
those obtained using a standard mono-
target speech transducer.
1 Introduction
Finite-state models constitute an important frame-
work both in syntactic pattern recognition and in
language processing. Specifically, stochastic finite-
state transducers (SFSTs) have proved to be useful
for machine translation tasks within restricted do-
mains; they usually offer high speed during the de-
coding step and they provide competitive results in
terms of error rates (Mohri et al, 2002). Moreover,
SFSTs have proved to be versatile models, which
can be easily integrated with other finite-state mod-
els (Pereira and Riley, 1997).
The article (Casacuberta and Vidal, 2004) ex-
plored an automatic method to learn an SFST from a
bilingual set of samples for machine translation pur-
poses, the so-called GIATI (Grammar Inference and
Alignments for Transducers Inference). It described
how to learn both the structural and the probabilistic
components of an SFST making use of underlying
alignment models.
A multi-target SFST is a generalization of stan-
dard SFSTs, in such a way that every input string
in the source language results in a tuple of output
strings each being associated to a different target
language. An extension of GIATI that allowed to in-
fer a multi-target SFST from a multilingual corpus
was proposed in (Gonza?lez and Casacuberta, 2006).
A syntactic variant of this method (denoted as GI-
AMTI) has been used in this work in order to infer
the models from training samples as it is summa-
rized in section 3.
On the other hand, speech translation has been al-
ready carried out by integrating acoustic models into
a SFST (Casacuberta et al, 2004). Our main goal
in this work is to extend and assess these method-
ologies to accomplish spoken language multi-target
translation. Section 2 deals with this proposal by
presenting a new integrated architecture for speech-
input multi-target translation. Under this approach
spoken language can be simultaneously decoded and
translated into m languages using a unique network.
In section 4, the performance of the system has
been experimentally evaluated over a trilingual task
which aims to translate TVweather forecast into two
languages at the same time.
2 An integrated architecture for
speech-input multi-target translation
The classical architecture for spoken language
multi-target translation involves a speech recogni-
133
tion system in a serial architecture withm decoupled
text-to-text translators. Thus, the whole process in-
volves m + 1 searching stages, a first one for the
speech signal transcription into the source language
text string, and further m for the source language
translation into the m target languages. If we re-
placed the m translators by the multi-target SFST,
the problem would be reduced to 2 searching stages.
Nevertheless, in this paper we propose a natural way
for acoustic models to be integrated in the same net-
work. As a result, the input speech-signal can be
simultaneously decoded and translated into m target
languages just in a single searching stage.
Given the acoustic representation (x) of a speech
signal, the goal of multi-target speech translation
is to find the most likely m target strings (tm);
that is, one string (ti) per target language involved
(i ? {1, . . . ,m}). This approach is summarized
in eq. (1), where the hidden variable s can be in-
terpreted as the transcription of the speech signal:
t?m = argmax
tm
P (tm|x) = argmax
tm
?
s
P (tm, s|x)
(1)
Making use of Bayes? rule, the former expression
turns into:
t?m = argmax
tm
?
s
P (tm, s)P (x|tm, s) (2)
Empirically, there is no loss of generality if we as-
sume that the acoustic signal representation depends
only on the source string: i.e., that P (x|tm, s) is in-
dependent of tm. In this sense, eq. (2) can be rewrit-
ten as:
t?m = argmax
tm
?
s
P (tm, s)P (x|s) (3)
Equation (3) combines a standard acoustic model,
P (x|s), and a multi-target translation model,
P (tm, s), both of whom can be integrated on the fly
during the searching routine. Nevertheless, the outer
maximization is computationally very expensive to
search for the optimal tuple of target strings tm in
an effective way. Thus we make use of the so called
Viterbi approximation, which finds the best path.
3 Inference
Given a multilingual corpus, that is, a finite set of
multilingual samples (s, t1, . . . , tm) ? ?? ? ??1 ?
? ? ? ? ??m, where ti denotes the translation of the
source sentence s (formed by words of the input vo-
cabulary ?) into the i-th target language, which, in
its turn, has a vocabulary ?i, the GIAMTI method
can be outlined as follows:
1. Each multilingual sample is transformed into a
single string from an extended vocabulary (? ?
????1 ? ? ? ? ??
?
m) using a labelling function
(Lm). This transformation searches an ade-
quate monotonous segmentation for each of the
m source-target language pairs. A monotonous
segmentation copes with monotonous align-
ments, that is, j < k ? aj < ak following
the notation of (Brown et al, 1993). Each
source word is then joined with a target phrase
of each language as the corresponding segmen-
tation suggests. Each extended symbol consists
of a word from the source language plus zero
or more words from each target language.
2. Once the set of multilingual samples has been
converted into a set of single extended strings
(z ? ??), a stochastic regular grammar can be
inferred.
3. The extended symbols associated with the
transitions of the automaton are transformed
into one input word and m output phrases
(w/p?1/ . . . /p?m) by the inverse labeling func-
tion (L?m), leading to the required transducer.
In this work, the first step of the algorithm (as
described above), which is the one that handles
the alignment and segmentation routines, relies on
statistical alignments obtained with GIZA++ (Och,
2000). The second step was implemented us-
ing our own language modeling toolkit, which
learns stochastic k-testable in the string-sense gram-
mars (Torres and Varona, 2001), and allows for
back-off smoothing.
4 Experimental results
4.1 Task and corpus
We have implemented a highly practical application
that could be used to translate on-line TV weather
forecasts into several languages, taking the speech
of the presenter as the input and producing as output
text-strings, or sub-titles, in several languages. For
134
this purpose, we used the corpus METEUS (see Ta-
ble 1) which consists of a set of trilingual sentences,
in English, Spanish and Basque, as extracted from
weather forecast reports that had been published on
the Internet. Basque language is a minority lan-
guage, spoken in a small area of Europe and also
within some small American communities (such as
that in Boise, Idaho). In the Basque Country it has
an official status along with Spanish. However both
languages differs greatly in syntax and in semantics.
The differences in the size of the vocabulary (see
Table 1), for instance, are due to the agglutinative
nature of the Basque language.
With regard to the speech test, the input consisted
of the speech signal recorded by 36 speakers, each
one reading out 50 sentences from the test-set in Ta-
ble 1. That is, each sentence was read out by at least
three speakers. The input speech resulted in approx-
imately 3.50 hours of audio signal. Needless to say,
the application that we envisage has to be speaker-
independent if it is to be realistic.
Spanish Basque English
T
ra
in
in
g Sentences 14,615
Different Sent. 7,225 7,523 6,634
Words 191,156 187,462 195,627
Vocabulary 702 1,147 498
Average Length 13.0 12.8 13.3
Te
st
Different Sent. 500
Words 8,706 8,274 9,150
Average Length 17.4 16.5 18.3
Perplexity (3grams) 4.8 6.7 5.8
Table 1: Main features of the METEUS corpus.
4.2 System evaluation
The experimental setup was as follows: the multi-
target SFST was learned from the training set in Ta-
ble 1 using the GIAMTI algorithm described in sec-
tion 1; then, the speech test was translated, and the
output provided by the system in each language was
compared to the corresponding reference sentence.
Additionally, two mono-target SFST were inferred
from the same training set with their outputs for the
aforementioned test to be taken as baseline.
4.2.1 Computational cost
The expected searching time and the amount of
memory that needs to be allocated for a given model
are two key parameters to bear in mind in speech-
input machine translation applications. These values
can be objectively measured based on the size and on
the average branching factor of the model displayed
in Table 2.
multi-target mono-targetS2B S2E
Nodes 52,074 35,034 20,148
Edges 163,146 115,526 69,690
Braching factor 3.30 3.13 3.46
Table 2: Features of multi-target model and the two
decoupled mono-target models (one for Spanish to
Basque translation, referred to as S2B, and the sec-
ond for Spanish to English, S2E).
Adding the states and the edges up for the two
mono-target SFSTs that take part in the decoupled
architecture (see Table 2), we conclude that the de-
coupled model needs a total of 185, 216 edges to be
allocated in memory, which represents an increment
of 13% in memory-space with respect to the multi-
target model.
On the other hand, the multi-target approach of-
fers a slightly smaller branching factor than each
mono-target approach. As a result, fewer paths have
to be explored with the multi-target approach than
with the decoupled one, which means that searching
for a translation can be faster. In fact, experimental
results in Table 3 show that the mono-target archi-
tecture works%11more slowly than the multi-target
one.
multi-target mono-targetS2B S2E S2B+S2E
Time (s) 30,514 24,398 9,501 33,899
Table 3: Time needed to translate the speech-test
into two languages.
Summarizing, in terms of computational cost
(space and time), a multi-target SFST performs bet-
ter than the mono-target decoupled system.
4.2.2 Performance
So far, the capability of the systems have been as-
sessed in terms of time and spatial costs. However,
the quality of the translations they provide is, doubt-
less, the most relevant evaluation criterion. In order
to assess the performance of the system in a quan-
titative manner, the following evaluation parameters
135
were computed for each scenario: bilingual evalua-
tion under study (BLEU), position independent er-
ror rate (PER) and word error rate (WER).
As can be derived from the Speech-input trans-
lation results shown in Table 4, slightly better re-
sults are obtained with the classical mono-target SF-
STs, compared with the multi-target approach. From
Spanish into English the improvement is around
3.4% but from Spanish into Basque, multi-target ap-
proach works better with an improvement of a 0.8%.
multi-target mono-target
S2B S2E S2B S2E
BLEU 39.5 59.0 39.2 61.1
PER 42.2 25.3 41.5 23.6
WER 51.5 33.9 50.5 31.9
Table 4: Speech-input translation results for Spanish
into Basque (S2B) and Spanish into English (S2E)
using a multi-target SFST or two mono-target SF-
STs.
The process of speech signal decoding is itself
introducing some errors. In an attempt to measure
these errors, the text transcription of the recognized
input signal was extracted and compared to the input
reference in terms of WER as shown in Table 5.
multi-target mono-targetS2B S2E
WER 10.7 9.3 9.1
Table 5: Spanish speech decoding results for the
multi-target SFST and the two mono target SFSTs.
5 Concluding remarks and further work
A fully embedded architecture that integrates the
acoustic model into the multi-target translation
model for multiple speech translation has been pro-
posed. Due to the finite-state nature of this model,
the speech translation engine is based on a Viterbi-
like algorithm. The most significant feature of this
approach is its ability to carry out both the recogni-
tion and the translation into multiple languages inte-
grated in a unique model.
In contrast to the classical decoupled systems,
multi-target SFSTs enable the translation from one
source language simultaneously into several target
languages with lower computational costs (in terms
of space and time) and comparable qualitative re-
sults.
In future work we intend to make a deeper study
on the performance of the multi-target system as the
amount of targets increase, since the amount of pa-
rameters to be estimated also increases.
Acknowledgements
This work has been partially supported by the Uni-
versity of the Basque Country and by the Spanish
CICYT under grants 9/UPV 00224.310-15900/2004
and TIC2003-08681-C02-02 respectively.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Francisco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(2):205?
225.
F. Casacuberta, H. Ney, F. J. Och, E. Vidal, J. M.
Vilar, S. Barrachina, I. Garc??a-Varea, D. Llorens,
C. Mart??nez, S. Molau, F. Nevado, M. Pastor, D. Pico?,
A. Sanchis, and C. Tillmann. 2004. Some approaches
to statistical and finite-state speech-to-speech transla-
tion. Computer Speech and Language, 18:25?47, Jan-
uary.
M. Teresa Gonza?lez and Francisco Casacuberta. 2006.
Multi-Target Machine Translation using Finite-State
Transducers. In Proceedings of TC-Star Speech to
Speech Translation Workshop, pages 105?110.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69?88, January.
Franz J. Och. 2000. GIZA++: Training of statistical
translation models.
Fernando C.N. Pereira and Michael D. Riley. 1997.
Speech Recognition by Composition of Weighted Fi-
nite Automata. In Emmanuel Roche and Yves Sch-
abes, editors, Finite-State Language Processing, Lan-
guage, Speech and Communication series, pages 431?
453. The MIT Press, Cambridge, Massachusetts.
M. Ine?s Torres and Amparo Varona. 2001. k-tss lan-
guage models in speech recognition systems. Com-
puter Speech and Language, 15(2):127?149.
136
Proceedings of the Second Workshop on Statistical Machine Translation, pages 56?63,
Prague, June 2007. c?2007 Association for Computational Linguistics
Speech-input multi-target machine translation
Alicia Pe?rez, M. Ine?s Torres
Dep. of Electricity and Electronics
University of the Basque Country
manes@we.lc.ehu.es
M. Teresa Gonza?lez, Francisco Casacuberta
Dep. of Information Systems and Computation
Technical University of Valencia
fcn@dsic.upv.es
Abstract
In order to simultaneously translate speech
into multiple languages an extension of
stochastic finite-state transducers is pro-
posed. In this approach the speech trans-
lation model consists of a single network
where acoustic models (in the input) and the
multilingual model (in the output) are em-
bedded.
The multi-target model has been evaluated
in a practical situation, and the results have
been compared with those obtained using
several mono-target models. Experimental
results show that the multi-target one re-
quires less amount of memory. In addition, a
single decoding is enough to get the speech
translated into multiple languages.
1 Introduction
In this work we deal with finite-state models which
constitute an important framework in syntactic pat-
tern recognition for language and speech processing
applications (Mohri et al, 2002; Pereira and Riley,
1997). One of their outstanding characteristics is the
availability of efficient algorithms for both optimiza-
tion and decoding purposes.
Specifically, stochastic finite-state transducers
(SFSTs) have proved to be useful for machine trans-
lation tasks within restricted domains. There are
several approaches implemented over SFSTs which
range from word-based systems (Knight and Al-
Onaizan, 1998) to phrase-based systems (Pe?rez et
al., 2007). SFSTs usually offer high speed during
the decoding step and they provide competitive re-
sults in terms of error rates. In addition, SFSTs have
proved to be versatile models, which can be easily
integrated with other finite-state models, such as a
speech recognition system for speech-input transla-
tion purposes (Vidal, 1997). In fact, the integrated
architecture has proved to work better than the de-
coupled one. Our main goal is, hence, to extend
and assess these methodologies to accomplish spo-
ken language multi-target translation.
As far as multilingual translation is concerned,
there are two main trends in machine translation de-
voted to translate an input string simultaneously into
m languages (Hutchins and Somers, 1992): inter-
lingua and parallel transfer. The former has his-
torically been a knowledge-based technique that re-
quires a deep-analysis effort, and the latter consists
on m decoupled translators in a parallel architec-
ture. These translators can be either knowledge or
example-based. On the other hand, in (Gonza?lez
and Casacuberta, 2006) an example based technique
consisting of a single SFST that cope with multiple
target languages was presented. In that approach,
when translating an input sentence, only one search
through the multi-target SFST is required, instead of
the m independent decoding processes required by
the mono-target translators.
The classical layout for speech-input multi-target
translation includes a speech recognition system in
a serial architecture with m decoupled text-to-text
translators. Thus, this architecture entails a decod-
ing stage of the speech signal into the source lan-
guage text, and m further decoding stages to trans-
late the source text into each of the m target lan-
56
guages. If we supplant the m translators with the
multi-target SFST, the problem would be reduced to
2 searching stages. Nevertheless, in this paper we
propose a natural way for acoustic models to be in-
tegrated in the multilingual network itself, in such
a way that the input speech signal can be simulta-
neously decoded and translated into m target lan-
guages. As a result, due to the fact that there is just
a single searching stage, this novel approach entails
less computational cost.
The remainder of the present paper is structured
as follows: section 2 describes both multi-target SF-
STs and the inference algorithm from training ex-
amples; in section 3 a novel integrated architecture
for speech-input multi-target translation is proposed;
section 4 presents a practical application of these
methods, including the experimental setup and the
results they produced; finally, section 5 summarizes
the main conclusions of this work.
2 Multi-target stochastic finite-state
transducers
A multi-target SFST is a generalization of standard
SFSTs, in such a way that every input string in the
source language results in a tuple of output strings
each being associated to a different target language.
2.1 Definition
A multi-target stochastic finite-state transducer is a
tuple T = ??,?1 . . .?m, Q, q0, R, F, P ?, where:
? is a finite set of input symbols (source vocabu-
lary);
?1 . . .?m are m finite sets of output symbols (tar-
get vocabularies);
Q is a finite set of states;
q0 ? Q is the initial state;
R ? Q?????1 . . .?
?
m?Q is a set of transitions
such as (q, w, p?1, . . . , p?m, q?), which is a tran-
sition from the state q to the state q?, with the
source symbol w and producing the substrings
(p?1, . . . , p?m);
P : R ? [0, 1] is the transition probability distri-
bution;
F : Q ? [0, 1] is the final state probability distri-
bution;
The probability distributions satisfy the stochastic
constraint:
?q ? Q (1)
F (q)+
?
w,p?1,...,p?m,q?
P (q, w, p?1, . . . , p?m, q?) = 1
2.2 Training the multilingual translation model
Both topology and parameters of an SFST can
be learned fully automatically from bilingual ex-
amples making use of underlying alignment mod-
els (Casacuberta and Vidal, 2004). Furthermore,
a multi-target SFST can be inferred from a multi-
lingual set of samples (Gonza?lez and Casacuberta,
2006). Even though in realistic situations multilin-
gual corpora are too scarce, recent works (Popovic?
et al, 2005) show that bilingual corpora covering the
same domain are sufficient to obtain generalized cor-
pora based on which one can subsequently create the
required collections of aligned tuples.
The inference algorithm, GIAMTI (grammatical
inference and alignments for multi-target transducer
inference), requires a multilingual corpus, that is, a
finite set of multilingual samples (s, t1, . . . , tm) ?
?????1?? ? ???
?
m, where ti denotes the translation
of the source sentence s into the i-th target language;
? denotes the source language vocabulary, and ?i
the i-th target language vocabulary; the algorithm
can be outlined as follows:
1. Each multilingual sample is transformed into
a single string from an extended vocabulary
(? ? ? ? ??1 ? ? ? ? ? ?
?
m) using a labeling
function (Lm). This transformation searches an
adequate monotonic segmentation for each of
the m source-target language pairs on the basis
of bilingual alignments such as those given by
GIZA++ (Och, 2000). A monotonic segmen-
tation copes with monotonic alignments, that
is, j < k ? aj < ak following the notation
of (Brown et al, 1993). Each source token,
which can be either a word or a phrase (Pe?rez
et al, 2007), is then joined with a target phrase
of each language as the corresponding segmen-
tation suggests. Each extended symbol consists
of a token from the source language plus zero
57
Alignment #0
0:tenperatura
1:minimoa
2:jeitsiko
3:da
0:
te
mp
er
at
ur
as
1:
mi
ni
ma
s
2:
en
3:
de
sc
en
so
(a) Spanish-Basque
Alignment #0
0:low
1:temperatures
2:falling
0:
te
mp
er
at
ur
as
1:
mi
ni
ma
s
2:
en
3:
de
sc
en
so
(b) Spanish-English
0 1temperaturas | temperatura | NIL 2maximas | maximoak | high temperaturesminimas | minimoak | low temperatures 3en | NIL | NIL 5descenso | jaitsiko da | fallingascenso | igoko da | rising
(c) Multi-target SFST from Spanish into English and Basque.
Figure 1: Example of a trilingual alignment over a trilingual sentence extracted from the task under consid-
eration;the related multi-target SFST (with Spanish as input, and English and Basque as output).
or more words from each target language in
their turn.
2. Once the set of multilingual samples has been
converted into a set of single extended strings
(z ? ??), a stochastic regular grammar can be
inferred. Specifically, in this work we deal with
k-testable in the string-sense grammars (Garc??a
and Vidal, 1990), which are considered to be
a syntactic approach of the n-gram models. In
addition, they allow the integration of several
order models in a single smoothed automa-
ton (Torres and Varona, 2001).
3. The extended symbols associated with the
transitions of the automaton are transformed
into one input token and m output phrases
(w/p?1| . . . |p?m) by the inverse labeling function
(L?m), leading to the required transducer.
Example An illustration of the inference of the
multi-target SFST can be shown over a couple of
simple trilingual sentences from the corpus (where
?B? stands for Basque, ?S? for Spanish and ?E? for
English):
1-B tenperatura maximoa jaitsiko da
1-S temperaturas ma?ximas en descenso
1-E high temperatures falling
2-B tenperatura minimoa igoko da
2-S temperaturas m??nimas en ascenso
2-E low temperatures rising
From the alignments, depicted in Figures 1(a)
and 1(b), an input-language-synchronized
monotonous segmentation can be built (bear in
mind that we are considering Spanish as the input
language). The corresponding extended strings with
the following constituents for the first and second
samples respectively are the following ones:
1 temperaturas|tenperatura|?
m??nimas|minimoa|low temperatures
en|?|?
descenso|jaitsiko da|falling
58
2 temperaturas|tenperatura|?
ma?ximas|maximoa|high temperatures
en|?|?
ascenso|igoko da|rising
Finally, from this representation of the data, the
multi-target SFST can be built as shown in Fig-
ure 1(c).
2.3 Decoding
Given an input string s (a sentence in the source lan-
guage), the decoding module has to search the opti-
mal m output strings tm ? ??1 ? ? ? ? ??
?
m (a sen-
tence in each of the target language) according to the
underlying translation model (T ):
t?m = arg max
tm???1??????
?
m
PT (s, t
m) (2)
Solving equation (2) is a hard computational prob-
lem, however, it can be efficiently computed under
the so called maximum approach as follows:
PT (s, t
m) ? max
?(s,tm)
PT (?(s, t
m)) (3)
where ?(s, tm) is a translation form, that is, a se-
quence of transitions in the multi-target SFST com-
patible with both the input and the m output strings.
?(s, tm) : (q0, w1, p?m1 , q1) ? ? ? (qJ?1, wJ , p?
m
J , qJ)
The input string (s) is a sequence of J input sym-
bols, s = wJ1 , and each of the m output strings
consists of J phrases in its corresponding language
tm = (t1, ? ? ? , tm) = (p?1)J1 , ? ? ? , (p?m)
J
1 . Thus, the
probability supplied by the multi-target SFST to the
translation form is given by:
PT (?(s, t
m)) = F (qJ)
J?
j=1
P (qj?1, wj , p?
m
j , qj)
(4)
In this context, the Viterbi algorithm can be used
to obtain the optimal sequence of states through the
multi-target SFST for a given input string. As a
result, the established m translations are built con-
catenating the (J) output phrases for each language
through the optimal path.
3 An embedded architecture for
speech-input multi-target translation
3.1 Statistical framework
Given the acoustic representation (x) of a speech
signal, the goal of multi-target speech translation
is to find the most likely m target strings (tm);
that is, one string (ti) per target language involved
(i ? {1, . . . ,m}). This approach is summarized
in eq. (5), where the hidden variable s can be in-
terpreted as the transcription of the speech signal:
t?m = arg max
tm
P (tm|x) = arg max
tm
?
s
P (tm, s|x)
(5)
Making use of Bayes? rule, the former expression
turns into:
t?m = arg max
tm
?
s
P (tm, s)P (x|tm, s) (6)
Empirically, there is no loss of generality if we as-
sume that the acoustic signal representation depends
only on the source string, i.e. P (x|tm, s) is inde-
pendent of tm. In this sense, eq. (6) can be rewritten
as:
t?m = arg max
tm
?
s
P (tm, s)P (x|s) (7)
Equation (7) combines a standard acoustic model,
P (x|s), and a multi-target translation model,
P (tm, s), both of whom can be integrated on the fly
during the searching routine as shown in Figure 2.
That is, each acoustic sub-network is only expanded
at decoding time when it is required.
The outer sum is computationally very expensive
to search for the optimal tuple of target strings tm
in an effective way. Thus we make use of the so
called Viterbi approximation, which finds the best
path over the whole transducer.
3.2 Practical issues
The underlying recognizer used in this work is our
own continuous-speech recognition system, which
implements stochastic finite-state models at all lev-
els: acoustic-phonetic, lexical and syntactic, and
which allows to infer them based on samples.
The signal analysis was carried out in a stan-
dard way, based on the classical Mel-cepstrum
parametrization. Each phone-like unit was modeled
59
1 /e/ | NIL | NIL 2/n/ | NIL | NIL
Figure 2: Integration on the fly of acoustic models in one edge of the SFST shown in Figure 1(c)
by a typical left to right hidden Markov model. A
phonetically-balanced Spanish database, called Al-
bayzin (Moreno et al, 1993), was used to train these
models.
The lexical model consisted of the extended to-
kens of the multi-target SFST instead of running
words. The acoustic transcription for each extended
token was automatically obtained on the basis of the
input projection of each unit, that is, the Spanish vo-
cabulary in this case.
Instead of the usual language model, we make use
of the multi-target SFST itself, which had the syn-
tactic structure provided by a k-testable in the strict
sense model, with k=3, and Witten-Bell smoothing.
Note that the SFST implicitly involves both input
and output language models.
4 Experimental results
4.1 Task and corpus
The described general methodology has been put
into practice in a highly practical application that
aims to translate on-line TV weather forecasts into
several languages, taking the speech of the presen-
ter as the input and producing as output text-strings,
or sub-titles, in several languages. For this purpose,
we used the corpus METEUS which consists of a
set of trilingual sentences, in English, Spanish and
Basque, as extracted from weather forecast reports
that had been published on the Internet. Let us no-
tice that it is a real trilingual corpus, which they are
usually quite scarce.
Basque is a pre-Indoeuropean language of still
unknown origin. It is a minority language, spo-
ken in a small area of Europe and also within some
small American communities (such as that in Reno,
Nevada). In the Basque Country (located in the
north of Spain) it has an official status along with
Spanish. However, despite having coexisted for cen-
turies in the same area, they differ greatly both in
syntax and in semantics. Hence, efforts are being
devoted nowadays to machine translation tools in-
volving these two languages (Alegria et al, 2004),
although they are still scarce. With regard to the or-
der of the phrases within a sentence, the most com-
mon one in Basque is Subject plus Objects plus Verb
(even though some alternative structures are also ac-
cepted), whereas in Spanish and English other con-
structions such as Subject plus Verb plus Objects are
more frequent (see Figures 1(a) and 1(b)). Another
difference between Basque and Spanish or English
is that Basque is an extremely inflected language.
In this experiment we intend to translate Span-
ish speech simultaneously into both Basque and En-
glish. Just by having a look at the main features of
the corpus in Table 1, we can realize that there are
substantial differences among these three languages,
in terms both of the size of the vocabulary and of the
amount of running words. These figures reveal the
agglutinant nature of the Basque language in com-
parison with English or Spanish.
Spanish Basque English
T
ra
in
in
g Total sentences 14,615
Different sentences 7,225 7,523 6,634
Words 191,156 187,462 195,627
Vocabulary 702 1,147 498
Average Length 13.0 12.8 13.3
Te
st
Sentences 500
Words 8,706 8,274 9,150
Average Length 17.4 16.5 18.3
Perplexity (3grams) 4.8 6.7 5.8
Table 1: Main features of the METEUS corpus.
With regard to the speech test, the input consisted
of the speech signal recorded by 36 speakers, each
one reading out 50 sentences from the test-set in Ta-
ble 1. That is, each sentence was read out by at least
three speakers. The input speech resulted in approx-
imately 3.50 hours of audio signal. Needless to say,
the application that we envisage has to be speaker-
60
independent if it is to be realistic.
4.2 System evaluation
The performance obtained by the acoustic integra-
tion has been experimentally tested for both multi-
target and mono-target devices. As a matter of com-
parison, text-input translation results are also re-
ported.
The multi-target SFST was learned from the train-
ing set described in Table 1 using the previously de-
scribed GIAMTI algorithm. The 500 test sentences
were then translated by the multi-target SFST. The
translation provided by the system in each language
was compared to the corresponding reference sen-
tence. Additionally, two mono-target SFSTs were
inferred with their outputs for the aforementioned
test to be taken as baseline. The evaluation includes
both computational cost and performance of the sys-
tem.
4.2.1 Computational cost
The expected searching time and the amount of
memory that needs to be allocated for a given model
are two key parameters to bear in mind in speech-
input machine translation applications. These val-
ues can be objectively measured in terms of the size
and on the average branching factor of the model
displayed in Table 2.
multi-target mono-targetS2B S2E
Nodes 52,074 35,034 20,148
Edges 163,146 115,526 69,690
Branching factor 3.30 3.13 3.46
Table 2: Features of multi-target model and the two
decoupled mono-target models (one for Spanish to
Basque translation, referred to as S2B, and the sec-
ond for Spanish to English, S2E).
Adding the edges up for the two mono-target SF-
STs that take part in the decoupled architecture (see
Table 2), we conclude that the decoupled model
needs a total of 185, 216 edges to be allocated in
memory, which represents an increment of 13%
in memory-space with respect to the multi-target
model.
On the other hand, the multi-target approach of-
fers a slightly smaller branching factor than each
mono-target approach. As a result, fewer paths have
to be explored with the multi-target approach than
with the decoupled one, which suggests that search-
ing for a translation might be faster. As a matter of
fact, experimental results in Table 3 show that the
mono-target architecture works 11% more slowly
than the multi-target one for speech-input machine
translation and decoding, and 30% for text to text
translation.
Time (s)
multi-target mono-targetS2B+S2E
Text-input 0.36 0.47
Speech-input 16.9 18.9
Table 3: Average time needed to translate each input
sentence into two languages.
Summarizing, in terms of computational cost
(space and time), a multi-target SFST performs bet-
ter than the mono-target decoupled system.
4.2.2 Performance
So far, the capability of the systems has been as-
sessed in terms of time and spatial costs. However,
the quality of the translations they provide is, doubt-
less, the most relevant evaluation criterion. In or-
der to determine the performance of the system in
a quantitative manner, the following evaluation pa-
rameters were computed for each scenario: bilingual
evaluation under study (BLEU), position indepen-
dent error rate (PER) and word error rate (WER).
Both text and speech-input translation results pro-
vided by the multi-target and the mono-target mod-
els respectively are shown in Table 4.
As can be derived from the translation results,
for text-input translation the classical approach per-
forms slightly better than the multi-target one, but
for speech-input translation from Spanish into En-
glish is the other way around. In any case, the dif-
ferences in performance are marginal.
Comparing the text-input with the speech-input
results we realize that, as could be expected, the pro-
cess of speech signal decoding is itself introducing
some errors. In an attempt to measure these errors,
the text transcription of the recognized input signal
was extracted and compared to the input reference
in terms of WER as shown in the last row of the Ta-
ble 4. Note that even though the input sentences are
the same the three results differ due to the fact that
61
we are making use of different SFST models that de-
code and translate at the same time.
multi-target mono-target
S2B S2E S2B S2E
Te
xt
BLEU 42.7 66.7 43.4 67.8
PER 39.9 19.9 38.2 19.0
WER 48.0 27.5 46.2 26.6
Sp
ee
ch
BLEU 39.5 59.0 39.2 61.1
PER 42.2 25.3 41.5 23.6
WER 51.5 33.9 50.5 31.9
recognition WER 10.7 9.3 9.1
Table 4: Text-input and speech-input translation re-
sults for Spanish into Basque (S2B) and Spanish into
English (S2E) using a multi-target SFST (columns
on the left) or two mono-target SFSTs (columns on
the right). The last row shows Spanish speech de-
coding results using each of the three devices.
In these series of experiments the same task has
been compared with two extremely different lan-
guage pairs under the same conditions. There is a
noticeable difference in terms of quality between the
English and the Basque translations. The underlying
reason might be due to the fact that SFST models
do not capture properly the rich morphology of the
Basque as they have to face long-distance reordering
issues. These differences in the performance of the
system when translating into English or into Basque
have been previously detected in other works (Or-
tiz et al, 2003). In our case, a manual review of the
models and the obtained translations encourage us to
make use of reordering models in future work, since
they have proved to report good results in a similar
framework (Kanthak et al, 2005).
5 Concluding remarks and further work
The main contribution of this paper is the proposal
of a fully embedded architecture for multiple speech
translation. Thus, acoustic models are integrated on
the fly into a multi-target translation model. The
most significant feature of this approach is its abil-
ity to carry out both the recognition and the transla-
tion into multiple languages integrated in a unique
model. Due to the finite-state nature of this model,
the speech translation engine is based on a Viterbi-
like algorithm.
In contrast to the mono-target systems, multi-
target SFSTs enable the translation from one source
language simultaneously into several target lan-
guages with lower computational costs (in terms
of space and time) and comparable qualitative re-
sults. Moreover, the integration of several languages
and acoustic models is straightforward on means of
finite-state devices.
Nevertheless, the integrated architecture needs
more parameters to be estimated. In fact, as the
amount of targets increase the data sparseness might
become a difficult problem to cope with. In future
work we intend to make a deeper study on the per-
formance of the multi-target system with regard to
the amount of parameters to be estimated. In ad-
dition, as the first step of the learning algorithm is
decisive, we are planning to make use of reordering
models in an attempt to face up to with long dis-
tance reordering and in order to homogenize all the
languages involved.
Acknowledgments
This work has been partially supported by the Uni-
versity of the Basque Country and by Spanish CI-
CYT under grants 9/UPV 00224.310-15900/2004,
TIC2003-08681-C02-02, and CICYT es TIN2005-
08660-C04-03 respectively.
References
In?aki Alegria, Olatz Ansa, Xabier Artola, Nerea Ezeiza,
Koldo Gojenola, and Ruben Urizar. 2004. Repre-
sentation and treatment of multiword expressions in
basque. In Takaaki Tanaka, Aline Villavicencio, Fran-
cis Bond, and Anna Korhonen, editors, Second ACL
Workshop on Multiword Expressions: Integrating Pro-
cessing, pages 48?55, Barcelona, Spain, July. Associ-
ation for Computational Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Francisco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(2):205?
225.
P. Garc??a and E. Vidal. 1990. Inference of k-testable
languages in the strict sense and application to syntac-
tic pattern recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 12(9):920?925.
62
M.T. Gonza?lez and F. Casacuberta. 2006. Multi-Target
Machine Translation using Finite-State Transducers.
In Proceedings of TC-Star Speech to Speech Transla-
tion Workshop, pages 105?110.
John Hutchins and Harold L. Somers. 1992. An In-
troduction to Machine Translation. Academic Press,
Cambridge, MA.
Stephan Kanthak, David Vilar, Evgeny Matusov, Richard
Zens, and Hermann Ney. 2005. Novel reordering ap-
proaches in phrase-based statistical machine transla-
tion. In Proceedings of the ACL Workshop on Building
and Using Parallel Texts, pages 167?174, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
K. Knight and Y. Al-Onaizan. 1998. Translation with
finite-state devices. In 4th AMTA (Association for Ma-
chine Translation in the Americas).
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69?88, January.
A. Moreno, D. Poch, A. Bonafonte, E. Lleida, J. Llisterri,
J. B. Mario, and C. Nadeu. 1993. Albayzin speech
database: Design of the phonetic corpus. In Proc. of
the European Conference on Speech Communications
and Technology (EUROSPEECH), Berl??n, Germany.
Franz J. Och. 2000. GIZA++: Train-
ing of statistical translation models.
http://www.fjoch.com/GIZA++.html.
Daniel Ortiz, Ismael Garc??a-Varea, Francisco Casacu-
berta, Antonio Lagarda, and Jorge Gonza?lez. 2003.
On the use of statistical machine translation techniques
within a memory-based translation system (AME-
TRA). In Proc. of Machine Translation Summit IX,
pages 115?120, New Orleans, USA, September.
Fernando C.N. Pereira and Michael D. Riley. 1997.
Speech Recognition by Composition of Weighted Fi-
nite Automata. In Emmanuel Roche and Yves Sch-
abes, editors, Finite-State Language Processing, Lan-
guage, Speech and Communication series, pages 431?
453. The MIT Press, Cambridge, Massachusetts.
Alicia Pe?rez, M. Ine?s Torres, and Francisco Casacuberta.
2007. Speech translation with phrase based stochas-
tic finite-state transducers. In Proceedings of the 32nd
International Conference on Acoustics, Speech, and
Signal Processing (ICASSP 2007), Honolulu, Hawaii
USA, April 15-20. IEEE.
Maja Popovic?, David Vilar, Hermann Ney, Slobodan
Jovic?ic?, and Zoran S?aric?. 2005. Augmenting a small
parallel text with morpho-syntactic language. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts, pages 41?48, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
M. Ine?s Torres and Amparo Varona. 2001. k-tss lan-
guage models in speech recognition systems. Com-
puter Speech and Language, 15(2):127?149.
Enrique Vidal. 1997. Finite-state speech-to-speech
translation. In Proc. IEEE International Conference
on Acoustics, Speech, and Signal Processing, vol-
ume 1, pages 111?114, Munich, Germany, April.
63
