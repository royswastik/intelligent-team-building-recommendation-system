Analysis and Detection of Reading Miscues for Interactive Literacy Tutors 
Katherine Lee, Andreas Hagen, Nicholas Romanyshyn, Sean Martin, Bryan Pellom 
Center for Spoken Language Research  
University of Colorado at Boulder 
pellom@cslr.colorado.edu 
 
Abstract 
The Colorado Literacy Tutor (CLT) is a 
technology-based literacy program, designed 
on the basis of cognitive theory and 
scientifically motivated reading research, 
which aims to improve literacy and student 
achievement in public schools. One of the 
critical components of the CLT is a speech 
recognition system which is used to track the 
child?s progress during oral reading and to 
provide sufficient information to detect 
reading miscues.  In this paper, we extend on 
prior work by examining a novel labeling of 
children?s oral reading audio data in order to 
better understand the factors that contribute 
most significantly to speech recognition 
errors.  While these events make up nearly 8% 
of the data, they are shown to account for 
approximately 30% of the word errors in a 
state-of-the-art speech recognizer.  Next, we 
consider the problem of detecting miscues 
during oral reading. Using features derived 
from the speech recognizer, we demonstrate 
that 67% of reading miscues can be detected at 
a false alarm rate of 3%. 
1 Introduction 
  Pioneering research by MIT and CMU as well as 
more recent work by the IBM Watch-me-Read 
project has demonstrated human language 
technologies can play an effective role in systems 
designed to improve children?s reading abilities 
(McCandless, 1992; Mostow et al, 1994; Zue et 
al., 1996). In CMU?s Project LISTEN, for example, 
the tutor operates by prompting children to read 
individual sentences out loud.  The tutor listens to 
the child using speech recognition and extracts 
features that can be used to detect oral reading 
miscues (Mostow et al, 2002; Tam et al 2003).  
The most common miscues that children make 
while reading out loud are word substitutions, 
repetitions, and self-corrections with word 
omissions and insertions being less frequent 
(Fogarty et al 2001). 
  Upon detecting such reading errors, the tutor must 
provide appropriate feedback to the child.  While 
the type of feedback and level of feedback is the 
current subject of much debate within the research 
community, recent results have shown that 
automated reading tutors can improve student 
achievement (Mostow et al, 2003).  In fact, 
providing real time feedback by simply 
highlighting words as they are read out loud is the 
basis of at least one commercial product today1.  
Cole et al (2003) and Wise et al (in press) 
describe a new scientifically-based literacy 
program, Foundations to Fluency, in which a 
virtual tutor?a lifelike 3D computer model?
interacts with children in multimodal learning tasks 
to teach them to read. A key component of this 
program is the Interactive Book, which combines 
real-time multilingual speech recognition, facial 
animation, and natural language understanding 
capabilities to teach children to read and 
comprehend text.  Within the context of this 
reading program, Hagen et al (2003) demonstrated 
an initial speech recognition system that provides 
real-time reading tracking for children.  This work 
was later extended by Hagen et al (2004) to 
incorporate improved acoustic and language 
modeling strategies.  When tested on 106 children 
(ages 9-11) who were asked to read one of a 
number of short age-appropriate stories, a final 
system word error rate of 8.0% was demonstrated.   
While reporting raw word error rate is useful for 
comparison purposes to prior research, we point 
out that it does not provide any diagnostic 
information which can be used to understand 
factors that contribute to speech recognition error 
within such children?s literacy tutor programs.  
Therefore, this paper extends our earlier work in 
two important ways.  First, in order to understand 
where future improvements can be obtained, we 
provide a novel ?event? labeling of our children?s 
speech corpus and examine the performance of the 
current speech recognition system under each 
labeled event condition.  Second, we describe the 
construction of an automated classifier which can 
detect reading miscues in children?s speech.     
This paper is organized as follows.  First, 
Section 2 provides an introduction and overview of 
the Colorado Literacy Tutor project.   Section 3 
describes the audio corpus used in the experiments 
                                                     
1 http://www.soliloquy.com 
provided in this paper and Section 4 describes our 
baseline speech recognition system.  Next, Section 
5 describes the event labeling methodology and 
word error analysis under each labeled event 
condition.  Finally Section 6 describes our initial 
work towards developing a system to detect 
reading miscues based on the output of our 
baseline speech recognition system.  Conclusions 
and future work are outlined in Section 7. 
2 The Colorado Literacy Tutor 
The Colorado Literacy Tutor (CLT)2 is a 
technology-based literacy program, designed on 
the basis of cognitive theory and scientifically 
motivated reading research, which aims to improve 
literacy and student achievement in public schools. 
The goal of the Colorado Literacy Tutor is to 
provide computer-based learning tools that will 
improve student achievement in any subject area 
by helping students learn to read fluently, to 
acquire new knowledge through deep 
understanding of what they read, to make 
connections to other knowledge and experiences, 
and to express their ideas concisely and creatively 
through writing. A second goal is to scale up the 
program to both state and national levels in the 
U.S. by providing accessible, inexpensive and 
effective computer-based learning tools.    
   The CLT project consists of four tightly 
integrated components: Managed Learning 
Environment, Foundational Reading Skills Tutors, 
Interactive Books, and Latent-Semantic Analysis 
(LSA)-based comprehension training (Steinhart 
2001; Deerwester et al, 1990; Landauer and 
Dumais, 1997).   A key feature of the project is the 
use of leading edge human communication 
technologies in learning tasks. The project has 
become a test bed for research and development of 
perceptive animated agents that integrate auditory 
and visual behaviors during face-to-face 
conversational interaction with human learners. 
The project enables us to evaluate component 
technologies with real users?students in 
classrooms?and to evaluate how the technology 
integration affects learning using standardized 
assessment tools.  
Within the CLT, Interactive Books are the main 
platform for research and development of natural 
language technologies and perceptive animated 
agents. Figure 1 shows a page of an Interactive 
Book. Interactive Books incorporate speech 
recognition, spoken dialogue, natural language 
processing, and computer animation technologies 
to enable natural face-to-face conversational 
                                                     
2 http://www.colit.org 
interaction with users. The integration of these 
technologies is performed using a client-server 
architecture that provides a platform-independent 
user interface for Web-based delivery of 
multimedia learning tools.  Interactive Book 
authoring tools are designed for easy use by project 
staff, teachers and students to enable authors to 
design and format books by combining text, 
images, videos and animated characters. Once text 
and illustrations have been imported or input into 
the authoring environment, authors can orchestrate 
interactions between users, animated characters 
and media objects. Developers can populate 
illustrations (digital images) with animated 
characters, and cause them to converse with each 
other, with the user, or speak their parts in the 
stories using naturally recorded or synthetic 
speech. A mark up language enables authors to 
control characters? facial expressions and gestures 
while speaking. The authoring tools also enable 
authors to pre-record sentences and/or individual 
words in the text as well as utterances to be 
produced by animated characters during 
conversations.    
 
 
Figure 1: An example interactive book 
 
Interactive Books enable a wide range of user 
and system behaviors. These include having the 
story narrated by animated characters, having 
conversations with animated characters in 
structured or mixed-initiative dialogues, having the 
student read out loud while words are highlighted, 
enabling the student to click on words to have 
them spoken by the agent or to have the agent 
interact with the student to sound out the word, 
having the student respond to questions posed by 
the agent either by clicking on objects in images or 
saying or typing responses, and having the student 
produce typed or spoken story summaries which 
can be analyzed for content using natural language 
processing techniques.  
3 CU Children?s Read Story Corpus 
Within the context of the CLT project, we have 
collected a corpus of audio data consisting of read 
stories spoken by children. Known as the CU 
Children?s Read Story Corpus3, the data currently 
contains speech and associated word-level 
transcriptions from 106 children who were asked 
to read a short age-appropriate story and to provide 
a spontaneous spoken summary of the material.  In 
addition, each child was prompted to read 25 
phonetically balanced sentences for future use in 
exploring strategies for speaker adaptation.   
The data were collected from native English 
speaking children in the Boulder Valley School 
District (Boulder, Colorado, USA).  We have 
initially collected and transcribed stories from 
children in grades 3, 4, and 5 (grade 3: 17 
speakers, grade 4: 28 speakers, grade 5: 61 
speakers).  The data were originally collected in a 
quiet room using a commonly available head-
mounted microphone.  The current 16 kHz 
sampled corpus consists of 10 different stories.  
Each story contains an average of 1054 words (min 
532 words / max 1926 words) with an average of 
413 unique words per story.    Note that while each 
story is accompanied by a spontaneous summary 
produced by the child, we do not consider those 
data for this paper. 
4 Baseline Speech Recognition System 
The CLT uses the SONIC speech recognition 
system as a basis for providing real-time 
recognition of children?s speech (Pellom, 2001; 
Pellom and Hacioglu, 2003; Hagen et al 2004)4.  
The recognizer implements an efficient time-
synchronous, beam-pruned Viterbi token-passing 
search through a static re-entrant lexical prefix tree 
while utilizing continuous density mixture 
Gaussian Hidden Markov Models (HMMs).  The 
recognizer uses PMVDR cepstral coefficients 
(Yapanel and Hansen, 2003) as its feature 
representation. Children?s acoustic models were 
estimated from 46 hours of audio from the CU 
Read and Prompted Children?s Speech Corpus 
(Hagen et al, 2003)5 and the OGI Kids? speech 
corpus (Shobaki et al, 2000). 
During oral reading, the speech recognizer 
models the story text using statistical n-gram 
language models.  This approach gives the 
recognizer flexibility to insert/delete/substitute 
                                                     
3 The CU Children?s Read Story Corpus is made 
available for research purposes (http://cslr.colorado.edu) 
4 SONIC is freely downloadable for research use 
from (http://cslr.colorado.edu) 
5 This corpus differs from the test corpus in Section 3. 
words based on acoustics and to provide accurate 
confidence information from the word-lattice.  The 
recognizer receives packets of audio and 
automatically detects voice activity.  When the 
child speaks, the partial hypotheses are sent to a 
reading tracking module.  The reading tracking 
module determines the current reading location by 
aligning each partial hypothesis with the story text 
using a Dynamic Programming search.  In order to 
allow for skipping of words or even skipping to a 
different place within the text, the search finds 
words that when strung together minimize a 
weighted cost function of adjacent word-proximity 
and distance from the reader's last active reading 
location. The Dynamic Programming search 
additionally incorporates constraints to account for 
boundary effects at the ends of each partial phrase. 
Hagen et al (2004) describes more recent 
advances made to both acoustic and language 
modeling for oral-reading recognition of children?s 
speech.  Specifically, that work describes the use 
of cross-utterance word history modeling, position-
sensitive dynamic n-gram language modeling, as 
well as vocal tract length normalization, speaker-
adaptive training, and iterative unsupervised 
speaker adaptation for improved recognition.  The 
final system was shown to have an overall word 
error rate of 8.0% on the speech corpus described 
in Section 3.  This system serves as the baseline for 
our experiments in the remainder of the paper. 
5 Event-based Word Error Analysis 
While our earlier work in Hagen et al (2003) 
and Hagen et al (2004) described consistent 
improvements in speech recognition accuracy on 
children?s speech, the use of raw word error rate 
does not reveal much information in terms of 
where future improvements in system performance 
may be obtained.  Because of this, we annotated 
the CU Children?s Read Story Corpus in terms of a 
set of event labels which we feel might have most 
relation to speech recognition error rate.  Next, in 
Section 5.1, we describe the event labeling 
methodology and then provide a detailed error 
analysis of our baseline system in Section 5.2. 
5.1 Event Labeling Methodology 
The event labels for this project were chosen 
based on the most common types of errors children 
make when reading aloud. Also included in the 
labels are other acoustic events that occur 
frequently, such as breaths and pauses, which may 
contribute to an error made by the speech 
recognizer.  The event labels for this study are 
summarized in Table 1. Common errors as stated 
before are word repetitions, omissions, 
substitutions, insertions, and self-corrections.  
Although pauses (PS) are natural in speech, too 
many can disrupt the fluency of the read story. If a 
pause is extended, the recognizer may potentially 
insert a word (during the silence region). Similarly, 
we marked breath placements (BR) if they were 
audible.  We hypothesize that words may be 
inserted during periods of breath if not properly 
accounted for by the speech recognizer. 
Mispronunciations (MP) tend to occur when a 
child is faced with word he/she is not familiar with 
and makes an attempt at either sounding it out (or 
speak fluently with an inappropriate phonetic 
realization). The use of wrong words (WW) is 
commonly a result of fast reading. The child may 
only read the first part of the word and guess on 
the rest replacing the word with one that is 
phonologically similar. An interjection (IJ) is any 
word inserted into a sentence that is not in the 
original text (e.g., ?um? or ?ah?).  Repetitions 
(REP) occur when the child realizes he/she has 
made a mistake and self corrects him/herself 
usually by repeating the misread word or by 
beginning the sentence over again. In some cases 
the child catches his/her error before finishing the 
word and thus creating a partial word, however, 
since it is a conscious act by the child the word is 
marked as a repetition assuming he/she did repeat 
it to self correct.  
Other important factors to be tracked by the 
recognizer are over-articulations (OA), hesitations 
(HS), non-speech segments (NS) and background 
noises (BN). An over-articulation is considered to 
be a deliberate sounding out of the word where 
each sound may be heard separately. A child may 
additionally hesitate on a word while looking 
ahead at the next word causing parts of the word to 
be elongated (e.g., stretched vowels). The non-
speech sound and background noise labels are 
meant to indicate any noise outside of the child?s 
reading such as a cough or a door closing.   We 
also considered including a label for head-colds 
(HC), but later removed this label due to 
inconsistencies and subjective assessments needed. 
These labels were applied to 106 read stories 
from the audio corpus described in Section 3.  
Each file was analyzed by one of three listeners 
and marked using these labels. Reliability between 
the listeners was checked by overlapping the files 
analyzed and comparing mark ups.   The event 
labeling and word-level transcription of the audio 
corpus were conducted using the freely available 
Transcriber software6. 
 
                                                     
6 http://www.etca.fr/CTA/gip/Projets/Transcriber/ 
Event Label  
& Event Description 
Total 
Words 
(%) 
Word 
Error 
(%) 
None No Labeled Event 92.26 5.7
REP Word Repetition 2.46 22.4
BR Breath 1.44 26.1
PW Partial Word 0.70 49.6
PS Pause 0.70 40.5
HS Hesitation/Elongation 0.67 13.8
WW Wrong Word 0.60 48.1
MP Mispronunciation 0.36 36.2
BN Background Noise 0.30 15.5
IJ Interjection / Insertion 0.28 61.3
NS Non-Speech Sound 0.27 58.8
OA Over-articulation 0.10 38.3
Table 1: Event labels used in speech recognition 
error analysis on the CU Children?s Read Story 
Corpus.  Total words aligned to each condition are 
shown (in %) along with the average word error 
rate of the baseline system under each condition.  
The baseline system has a word error rate of 8.0%. 
5.2 Speech Recognition Error Analysis 
Using the NIST Speech Recognition Scoring 
Toolkit (SCTK)7 we obtained the alignments of the 
reference word-level transcription with the 
hypothesized string from our baseline speech 
recognition system. By using the associated timing 
information, each word was then marked as 
belonging to one of the event classes shown in 
Table 1 (or possibly no class marking).  Each word 
was further marked as correctly or incorrectly 
recognized by the speech recognizer using the 
scoring software.  Based upon this analysis we are 
able to deduce the percentage of words that are 
output from the speech recognizer and associated 
with each event condition (column 2 of Table 1).  
We also can determine the average word-error rate 
for each labeled event type (column 3 of Table 1). 
What is most striking from Table 1 is that the 
average system word error rate during non-event 
labeled conditions is 5.7% while the average word 
error rate for words associated with the labeled 
event conditions is 31.5%.  While the speech 
recognizer output during the labeled events is 
small (approximately 7.7% of the words), the 
events contribute to nearly 30% of the word error 
rate of the system.  Most troubling are instances of 
repeated words and breaths made by the child 
during read-aloud.  We suggest that future progress 
can be made by focusing on (1) flexible n-gram 
language modeling which may take into account 
the problem of word-repetition, and (2) more 
accurate acoustic modeling and rejection of breath 
events during oral reading. 
                                                     
7 http://www.nist.gov/speech/tools/ 
6 Automatic Detection of Reading Miscues 
An important aspect in an automated reading 
tutor is the capability of detecting reading miscues 
and utilizing this knowledge to provide appropriate 
feedback. The level of detail present in the 
feedback strongly depends on the event detection 
accuracy, which is investigated in this paper.  We 
leave the problem of determining what feedback to 
provide as an area of future work.  First, we define 
our miscue detection problem and then provide a 
description of the features and classifier utilized.  
Finally, we evaluate our miscue detection system 
using the baseline speech recognition system 
described in Section 4.  
6.1 Problem Formulation 
Our main criterion for detecting events in our 
system is based on word alignments which 
compare the reference transcription of the child?s 
speech to the reference story text.  Similarly to 
Tam et al (2003), in order to detect reading 
miscues the speech recognizer's hypothesized 
output is aligned against the target story text using 
the Viterbi algorithm (i.e., hypothesis-target 
alignment). Furthermore the alignment of the 
human-based transcription against the story text is 
needed in the classification / evaluation process to 
determine where reading miscues actually occur 
(i.e., transcription-target algnment).  
We define a reading miscue event as any 
instance in which the child inserts, deletes or 
substitutes a word during oral reading.  Therefore 
each word spoken by the child is associated with 
an event label (insertion, deletion or substitution) 
or non-event (i.e., correct word). 
Given this word-level miscue labeling of the data 
we can propose a detection problem.  Here, each 
recognized word is submitted to a classifier.  This 
classifier labels each output word as correct or 
incorrect (i.e., a miscue event).  By thresholding 
the classifier output we can determine a detection 
rate for a given false alarm rate and therefore 
describe a Receiver Operating Characteristic 
(ROC).  The detection rate is defined as the 
number of times the hypothesis-target and 
transcription-target algnments show miscues at the 
same position divided by the number of 
transcription-target miscues. The false alarm rate is 
defined as the number of times the hypothesis-
target algnment shows a miscue at a position 
where the transcription-target algnment does not.  
We stress that we are not interested in the exact 
reading miscue (wrong word, correct word but 
pronounced incorrectly, partial word, etc.) that 
occurred, which would request too specific 
information for a current state of the art system to 
give reliable feedback. Rather, we wish to design 
an indicator that can accurately report the detection 
of a miscue event whenever the text was not read 
correctly.  
In order to be able to map one alignment to the 
other, the two alignments need to be synchronized. 
Our approach synchronizes the two alignments 
over the target words in the actual story text. 
Therefore each target word represents a unique 
position within both alignments. If one or more 
insertions occurred before a certain word in the 
target sentence this event is noted in a data 
structure attached to the specific target word 
stating the number of inserted words before the 
actual spoken word. If the word was replaced with 
another word in the hypothesis or transcription, the 
wrong word will be aligned with the actual target 
word, if a word is left out, no word from the 
hypothesis or transcription will be aligned with the 
specific deleted target word. Therefore the number 
of tokens with additional information about 
substitutions, deletions and insertions in the 
hypothesis-target algnment and transcription will 
be the same for both alignments and therefore 
word-based synchronization is ensured. To 
illustrate the process a short example is given. The 
target sentence,  
it was the first day of summer vacation  
might be spoken by the child (and transcribed) as,  
it was  it was the third day of summer vacation  
and the recognition hypothesis might state, 
it it was the first day vacation 
Therefore this transcription would have two 
insertions and one substitution events. The 
hypothesis would have one insertion and two 
deletions (?of summer?). The alignments along 
with the attached information are shown in Table 
2. The miscue columns indicate an event occurring 
at a specific position in the target text or right 
before it in the case of one or more insertions 
before a certain word.  
Story 
(Target)
Trans. 
(Ref.) 
Actual 
Miscue 
Recognizer
(Hyp.) 
Hyp. 
Miscue
it it  0-0-0 it  0-0-0 
was was 0-0-0 was 0-0-1 
the the 0-0-2 the 0-0-0 
first third 1-0-0 first 0-0-0 
day day 0-0-0 day  0-0-0 
of of 0-0-0 <no_word> 0-1-0 
summer summer 0-0-0 <no_word> 0-1-0 
vacation vacation 0-0-0 vacation 0-0-0 
Table 2: Transcription-target algnment and 
hypothesis-target algnment with substitution-
deletion-insertion (s-d-i) miscue annotation. 
This setup enables us to compute the detection 
and false alarm rates based on the synchronized 
alignments. Within the Viterbi-alignment process a 
soft decision is made whether to classify a word as 
a substitution or not. If the phonemes of the 
hypothesized word match the phonemes of the 
target word by at least 75% (determined by 
phoneme alignment) the word is accepted as 
correct. This softer decision overcomes less 
important events like misses of an ?s/z? sound at 
the end of a word (e.g., ?piano? vs. ?pianos?). 
6.2 Features for Miscue Detection 
The alignment based miscue detection is only 
capable of providing a single operating point 
(detection rate / false alarm rate). We next 
introduce additional features which allow us to 
threshold the classifier output and allow the system 
to operate at any point along the ROC curve. 
In order to be able to operate the detector at 
different levels of sensitivity additional features to 
the alignment used in a classifier are a useful 
extension. The features we chose are, 
 
? the word alignment  
(either 1 if the hypothesized word aligns to the 
target story word or 0 otherwise) 
? the speech recognizer language model score  
(computed per word) 
? the speech recognizer acoustic score  
(per word, normalized by frame count) 
? the length of the pause in seconds before the 
current word (0 if no pause exists) 
? the number of phonemes in the current word 
 
The alignment is obtained as discussed in 
Section 6.1. The language model score and the 
normalized acoustic score are indicators for the 
quality of the match between the hypothesized 
word?s model and the observed features. The 
length of the pause before a word indicates a 
hesitation that might be a hint for a reading 
irregularity. The number of phonemes should 
reflect the assumption that longer words are 
generally harder to read, especially for bad readers.  
6.3 Classifier Formulation 
We trained a linear classifier based on the 
features discussed above. The use of a linear 
classifier was motivated by earlier work of Hazen 
et al (2001) which demonstrated that such a 
classifier can generate acceptable performance for 
speech recognizer confidence estimation given that 
the decision surface is relatively simple. The 
classifier can be expressed as, 
 
 
 
fpr T
vv=  
 
where pv  is the trained classification vector and 
f
v
is the feature vector described above. The final 
classification is based on a threshold value. If r is 
greater than the threshold value, the instance under 
investigation is classified as a miscue, otherwise as 
a non-event. By varying r over a certain range the 
receiver operating characteristic (ROC) curve can 
be obtained. 
6.4 Evaluation 
The data set used to train the classifier consists 
of 50% of the CU Children?s Read Story Corpus 
randomly chosen such that age and grade levels are 
distributed similarly to the entire corpus. The 
training examples represent both miscue and non-
miscue events. The miscues are those examples 
that represent substitutions, deletions, or insertions 
within the transcription-target algnment. The 
negative examples are chosen from the non-miscue 
examples. There are 4,875 miscue and 8,715 non-
miscue examples used to train the classifier. 
We tested the classifier on the remaining 50% of 
the corpus. There are approximately 5,000 miscues 
in the test set. The ROC curve resulting from the 
classification system applied to the test set is 
shown in Figure 2.  It can be seen that the overall 
performance has a relatively high detection rate of 
67% with a false alarm rate of less than 3.0%. With 
the detection rate adjusted to 70% and higher the 
false alarm rate increases rapidly. 
 
 
 
 
DT (%) 55.0 60.0 65.0 70.0 75.0 80.0 
FA (%) 2.6 2.7 2.9 5.1 19.8 36.4 
Figure 2: Detection rate vs. false alarm rate ROC 
for the CU Children?s Read Story corpus.  
Example operating points are shown below. 
7 Conclusions 
In this paper we have described the Colorado 
Literacy Tutor (CLT) which aims to improve 
literacy and student achievement in public schools. 
We extended on our previous work in several 
novel aspects.  First, we have collected and 
annotated a children?s speech corpus in terms of a 
set of labeled event conditions which we believe 
strongly correlate to speech recognition error.  In 
fact while these events make up nearly 8% of the 
data, they were shown to account for 
approximately 30% of the word errors in a state-of-
the-art speech recognition system.  To our 
knowledge, previous work has not considered such 
a detailed word error analysis on a children?s 
speech corpus.  We then provided our initial 
framework for detecting oral reading miscues.  
Using a simple linear classifier and using features 
derived from a speech recognizer, we 
demonstrated that 67% of reading miscues can be 
detected at a false alarm rate of 3%.  While this 
system appears to outperform the previous results 
presented in Tam et al (2003), we point out that 
there is currently no standardized test set available 
to directly compare those systems.  Therefore, the 
audio corpus and event labeling presented in this 
paper will be made available to researchers to 
promote community-wide benchmarking.  In the 
future we plan to correlate the miscue detection 
performance with the event labeling strategy 
outlined in Section 5 of the paper.  We expect that 
such an error analysis will continue to provide 
insight to areas for system development. 
8 Acknowledgements 
   This work was supported by grants from the 
National Science Foundation's ITR and IERI 
Programs under grants NSF/ITR: REC-0115419, 
NSF/IERI: EIA-0121201, NSF/ITR: IIS-0086107, 
NSF/IERI: 1R01HD-44276.01; and the Coleman 
Institute for Cognitive Disabilities. The views 
expressed in this paper do not necessarily represent 
the views of the NSF. 
References  
R. Cole, S. van Vuuren, B. Pellom, K. Hacioglu, J. Ma, 
J. Movellan, S. Schwartz, D. Wade-Stein, W. Ward, J. 
Yan. 2003. Perceptive Animated Interfaces: First 
Steps Toward a New Paradigm for Human Computer 
Interaction. Proceedings of the IEEE, Vol. 91, No. 9, 
pp. 1391-1405. 
S. Deerwester, S. Dumais, T. Landauer, G. Furnas, and 
R. Harshman. 1990. Indexing by Latent Semantic 
Analysis. Journal of the Society for Information 
Science, vol. 41, no. 6, pp. 391-407. 
J. Fogarty, L. Dabbish, D. Steck, and J. Mostow.  2001. 
Mining a Database of Reading Mistakes: For What 
should an Automated Reading Tutor Listen? In J. D. 
Moore, C. L. Redfield, and W. L. Johnson (Eds.), 
Artificial Intelligence in Education:  AI-ED in the 
Wired and Wireless Future, pp. 422-433. 
A. Hagen, B. Pellom, and R. Cole. 2003. Children?s 
Speech Recognition with Application to Interactive 
Books and Tutors. ASRU-2003, St. Thomas, USA. 
A. Hagen, B. Pellom, S. Van Vuuren, R. Cole. 2004.   
Advances in Children?s Speech Recognition within an 
Interactive Literacy Tutor. HLT-NAACL, Boston 
Massachusetts, USA. 
T. Hazen, S. Seneff, and J. Polifroni. 2002. Recognition 
Confidence Scoring and its Use in Speech 
Understanding Systems. Computer Speech and 
Language, Vol. 16,No. 1, pp. 49-67. 
E. Kintsch, D. Steinhart, G. Stahl, C. Matthews, R. 
Lamb, and LRG. 2000. Developing Summarization 
Skills through the Use of LSA-based Feedback. 
Interactive Learning Environments, Vol. 8, pp. 87-
109. 
T. Landauer and S. Dumais. 1997. A Solution to Plato's 
Problem: The Latent Semantic Analysis Theory of 
Acquisition, Induction and Representation of 
Knowledge. Psych. Review, Vol. 104, pp. 211-240. 
M. McCandless. 1992. Word Rejection for a Literacy 
Tutor. Bachelor of Science Thesis, MIT. 
J. Mostow, G. Aist, P. Burkhead, A. Corbett, A. Cuneo, 
S. Eitelman, C. Huang, B. Junker, M. B. Sklar, and B. 
Tobin. 2003. Evaluation of an Automated Reading 
Tutor that Listens:  Comparison to Human Tutoring 
and Classroom Instruction. Journal of Educational 
Computing Research, 29(1), 61-117. 
J. Mostow, J. Beck, S. Winter, S. Wang, and B. Tobin. 
2002. Predicting Oral Reading Miscues. ICSLP-02, 
Denver, Colorado. 
J. Mostow, S. Roth, A. G. Hauptmann, and M. Kane. 
1994. A Prototype Reading Coach that Listens. 
AAAI-94, Seattle, WA, pp. 785-792.  
B. Pellom. 2001. SONIC: The University of Colorado 
Continuous Speech Recognizer. Technical Report TR-
CSLR-2001-01, University of Colorado. 
B. Pellom, K. Hacioglu. 2003. Recent Improvements in 
the CU SONIC ASR System for Noisy Speech: The 
SPINE Task. Proc. ICASSP, Hong Kong. 
K. Shobaki, J.-P. Hosom, and R. Cole. 2000. The OGI 
Kids' Speech Corpus and Recognizers. Proc. ICSLP-
2000, Beijing, China. 
D. Steinhart. 2001. Summary Street: An Intelligent 
Tutoring System for Improving Student Writing 
through the Use of Latent Semantic Analysis. Ph.D. 
Dissertation, Dept. Psychology, Univ. of Colorado, 
Boulder, CO. 
Y-C. Tam, J. Mostow, J. Beck, and S. Banerjee. 2003. 
Training a Confidence Measure for a Reading Tutor 
that Listens. Proc. Eurospeech, Geneva, Switzerland, 
3161-3164. 
U. Yapanel, J. H.L. Hansen. 2003. A New Perspective 
on Feature Extraction for Robust In-vehicle Speech 
Recognition. Proc. Eurospeech, Geneva, Switzerland. 
V. Zue, S. Seneff, J. Polifroni, H. Meng, J. Glass. 1996. 
Multilingual Human-Computer Interactions: From 
Information Access to Language Learning. ICSLP-96, 
Philadelphia, PA. 
Advances in Children?s Speech Recognition  
within an Interactive Literacy Tutor 
 
Andreas Hagen, Bryan Pellom, Sarel Van Vuuren, and Ronald Cole 
Center for Spoken Language Research 
University of Colorado at Boulder 
http://cslr.colorado.edu 
 
 
 
 
Abstract1 
In this paper we present recent advances in 
acoustic and language modeling that improve 
recognition performance when children read 
out loud within digital books. First we extend 
previous work by incorporating cross-
utterance word history information and dy-
namic n-gram language modeling. By addi-
tionally incorporating Vocal Tract Length 
Normalization (VTLN), Speaker-Adaptive 
Training (SAT) and iterative unsupervised 
structural maximum a posteriori linear regres-
sion (SMAPLR) adaptation we demonstrate a 
54% reduction in word error rate.  Next, we 
show how data from children?s read-aloud 
sessions can be utilized to improve accuracy 
in a spontaneous story summarization task.  
An error reduction of 15% over previous pub-
lished results is shown.  Finally we describe a 
novel real-time implementation of our re-
search system that incorporates time-adaptive 
acoustic and language modeling. 
1 Introduction 
Pioneering research by MIT and CMU as well as more 
recent work by the IBM Watch-me-Read Project have 
demonstrated that speech recognition can play an effec-
tive role in systems designed to improve children?s 
reading abilities (Mostow et al, 1994; Zue et al, 1996). 
In CMU?s Project LISTEN, for example, the tutor oper-
ates by prompting children to read individual sentences 
out loud.  The tutor listens to the child using speech 
recognition and extracts features that can be used to 
detect oral reading miscues (Mostow et al, 2002; Tam 
et al 2003).   Upon detecting reading miscues, the tutor 
provides appropriate feedback to the child.  Recent re-
                                                        
1
 This work was supported in part by grants from the National Science 
Foundation's Information Technology Research (ITR) Program and 
the Interagency Educational Research Initiative (IERI) under grants 
NSF/ITR: REC-0115419, NSF/IERI: EIA-0121201, NSF/ITR: IIS-
0086107, NSF/IERI: 1R01HD-44276.01, NSF: INT-0206207; and the 
Coleman Institute for Cognitive Disabilities. The views expressed in 
this paper do not necessarily represent the views of the NSF. 
sults show that such automated reading tutors can im-
prove student achievement (Mostow et al 2003). Pro-
viding real time feedback by highlighting words as the 
are read out loud is the basis of at least one commercial 
product today (http://www.soliloquy.com).  
Cole et al (2003) and Wise et al (in press) describe 
a new scientifically-based literacy program, Founda-
tions to Fluency, in which a virtual tutor?a lifelike 3D 
computer model?interacts with children in multimodal 
learning tasks to teach them to read. A key component 
of this program is the Interactive Book, which combines 
real-time speech recognition, facial animation, and natu-
ral language understanding capabilities to teach children 
to read and comprehend text.  Interactive Books are 
designed to improve student achievement by helping 
students to learn to read fluently, to acquire new knowl-
edge through deep understanding of what they read, to 
make connections to other knowledge, and to express 
their ideas concisely through spoken or written summa-
ries. Transcribed spoken summaries can be graded 
automatically to provide feedback to the student about 
their comprehension.  
During reading out loud activities in Interactive 
Books, the goal is to design a computer interface and 
speech recognizer that combine to teach the student to 
read fluently and naturally.  Here, speech recognition is 
used to track a child?s position within the text during 
read-aloud sessions in addition to providing timing and 
confidence information which can be used for reading 
assessment. The speech recognizer must follow the stu-
dents verbal behaviors accurately and quickly, so the 
cursor (or highlighted word) appears at the right place 
and right time when the student is reading fluently, and 
pauses when the student hesitates to sound out a word. 
The recognizer must also score mispronounced words 
accurately so that the student can revisit these words 
and receive feedback about their pronunciation after 
completing a paragraph or page (since highlighting hy-
pothesized mispronounced words when reading out loud 
may disrupt fluent reading behavior).   
In this paper we focus on the problem of speech rec-
ognition to track and provide feedback during reading 
out loud and to transcribe spoken summaries of text. 
Specifically, we describe several new methods for in-
corporating language modeling knowledge into the read 
aloud task.  In addition, through use of speaker adapta-
tion, we also demonstrate the potential for significant 
gains in recognition accuracy.  Finally, we leverage 
improvements in speech recognition for read aloud 
tracking to improve performance for spoken story sum-
marization.  Work reported here extends previous work 
in several important ways: by integrating the research 
advances into a real time system, and by including time-
adaptive language modeling and time-adaptive acoustic 
modeling of the child?s voice into the system. 
The paper is organized as follows. Sect. 2 describes 
our baseline speech recognition system and reading 
tracking method. Sect. 3 presents our rationale for using 
word-error-rate as a measure of performance.  Sect. 4 
describes the read aloud and story summarization cor-
pora used in this work. Sect. 5 describes and evaluates 
proposed improvements in a read aloud speech recogni-
tion task. Sect. 6 describes how these improvements 
translate to improved recognition of story summaries 
produced by a child. Sect. 7 details our real-time system 
implementation. 
2 Baseline System 
For this work we use the SONIC speech recognition 
system (Pellom, 2001; Pellom and Hacioglu, 2003).  
The recognizer implements an efficient time-
synchronous, beam-pruned Viterbi token-passing search 
through a static re-entrant lexical prefix tree while 
utilizing continuous density mixture Gaussian HMMs.  
For children?s speech, the recognizer has been trained 
on 46 hours of data from children in grades K through 9 
extracted from the CU Read and Prompted speech 
corpus (Hagen et al, 2003) and the OGI Kids? speech 
corpus (Shobaki et al, 2000).  Further, the baseline 
system utilizes PMVDR cepstral coefficients (Yapanel 
and Hansen, 2003) for improved noise robustness. 
During read-aloud operation, the speech recognizer 
models the story text using statistical n-gram language 
models.  This approach gives the recognizer flexibility 
to insert/delete/substitute words based on acoustics and 
to provide accurate confidence information from the 
word-lattice.  The recognizer receives packets of audio 
and automatically detects voice activity.  When the 
child speaks, the partial hypotheses are sent to a reading 
tracking module.  The reading tracking module deter-
mines the current reading location by aligning each par-
tial hypothesis with the book text using a Dynamic 
Programming search.  In order to allow for skipping of 
words or even skipping to a different place within the 
text, the search finds words that when strung together 
minimize a weighted cost function of adjacent word-
proximity and distance from the reader's last active 
reading location. The Dynamic Programming search 
additionally incorporates constraints to account for 
boundary effects at the ends of each partial phrase. 
3 Evaluation Methodology 
There are many different ways in which speech recogni-
tion can be used to serve children. In computer-based 
literacy tutors, speech recognition can be used to meas-
ure children's ability to read fluently and pronounce 
words while reading out loud, to engage in spoken dia-
logues with an animated agent to assess and train com-
prehension, or to transcribe spoken summaries of stories 
that can be graded automatically.  Because of the variety 
of ways of using speech recognition systems, it is criti-
cally important to establish common metrics that are 
used by the research community so that progress can be 
measured both within and across systems. 
For this reason, we argue that word error rate calcu-
lations using the widely accepted NIST scoring software 
provides the most widely accepted, easy to use and 
highly valid metric.  In this scoring procedure, word 
error rate is computed strictly by comparing the speech 
recognizer output against a known human transcription 
(or the text in a book).  Of course, authors are free to 
define and report other measures, such as detection/false 
alarm curves for useful events such as reading miscues.  
However, such analyses should always supplement re-
ports of word error rates using a single standardized 
measure. Adopting this strategy enables fair and bal-
anced comparisons within and across systems for any 
speech data given a known word-level transcription. 
4 Experimental Data 
For all experiments in this paper we use speech data and 
associated transcriptions from 106 children (grade 3: 17 
speakers, grade 4: 28 speakers, and grade 5: 61 speak-
ers) who were asked to read one of ten stories and to 
provide a spoken story summary.  The 16 kHz audio 
data contains an average of 1054 words (min 532 
words; max 1926 words) with an average of 413 unique 
words per story.  The resulting summaries spoken by 
children contain an average of 168 words. 
5 Improved Read-Aloud Recognition 
Baseline: Our baseline read-aloud system utilizes a 
trigram language model constructed from a normalized 
version of the story text. Text normalization consists 
primarily of punctuation removal and determination of 
sentence-like units.  For example,  
 
It was the first day of summer vacation.  Sue and Billy were 
eating breakfast.  ?What can we do today?? Billy asked. 
 
is normalized as: 
 
<s> IT WAS THE FIRST DAY OF SUMMERVACATION</s> 
<s> SUE AND BILLY WERE EATING BREAKFAST</s> 
<s> WHAT CAN WE DO TODAY </s> 
<s> BILLY ASKED </s> 
 
The resulting text is used to estimate a back-off trigram 
language model. We stress that only the story text is 
used to construct the language model. Details on the 
story texts are provided in Hagen et al (2003). Note that 
the sentence markers (<s> and </s>) are used to repre-
sent positions of expected speaker pause.  This baseline 
system is shown in Table 1(A) to produce a 17.4% word 
error rate. 
Improved Sentence Context Modeling: It is impor-
tant in the context of this research to note that children 
do not pause between each estimated sentence bound-
ary.  Instead, many children read fluently across phrases 
and sentences, where more experienced readers would 
pause. For this reason, we improved upon our baseline 
system by estimating language model parameters using 
a combined text material that is generated both with and 
without the contextual sentence markers (<s> and </s>).  
Results of this modification are shown in Table 1(B) 
and show a reduction in error from 17.4% to 13.5%. 
Improved Word History Modeling:  Most speech 
recognition systems operate on the utterance as a pri-
mary unit of recognition.  Word history information 
typically is not maintained across segmented utterances.  
However, in our text example, the words ?do today? 
should provide useful information to the recognizer that 
?Billy asked? may follow.  We therefore modify the 
recognizer to incorporate knowledge of previous utter-
ance word history. During token-passing search, the 
initial word-history tokens are modified to account for 
the fact that the incoming sentence may be either the 
beginning of a new sentence or a direct extension of the 
previous utterance?s word-end history.  Incorporating 
this constraint lowers the word error rate from 13.5% to 
12.7% as shown in Table 1(C). 
Dynamic n-gram Language Modeling:  During story 
reading we can anticipate words that are likely to be 
spoken next based upon the words in the text that are 
currently being read aloud.  To account for this knowl-
edge, we estimate a series of position-sensitive n-gram 
language models by partitioning the story into overlap-
ping regions containing at most 150 words (i.e., each 
region is centered on 50 words of text with 50 words 
before and 50 words after).  For each partition, we con-
struct an n-gram language model by using the entire 
normalized story text in addition to a 10x weighting of 
text within the partition.  Each position-sensitive lan-
guage model therefore contains the entire story vocabu-
lary.  We also compute a general language model 
estimated solely from the entire story text (similar to 
Table 1(C)).   At run-time, the recognizer implements a 
word-history buffer containing the most recent 15 rec-
ognized words.  After decoding each utterance, the 
probability of the text within the word history buffer is 
computed using each of the position-sensitive language 
models.  The language model with the highest probabil-
ity is selected for the first-pass decoding of the subse-
quent utterance.  This modification decreases the word 
error rate from 12.7% to 10.7% (Table 1(D)). 
Vocal Tract Normalization and Acoustic Adaptation:  
We further extend on our baseline system by incorporat-
ing the Vocal Tract Length Normalization (VTLN) 
method described in Welling et al (1999).  Based on 
results shown in Table 1(E), we see that VTLN provides 
only a marginal gain (0.1% absolute).  Our final set of 
acoustic models for the read aloud task are both VTLN 
normalized and estimated using Speaker Adaptive 
Training (SAT).  The SAT models are determined by 
estimating a single linear feature space transform for 
each training speaker (Gales, 1997).  The means and 
variances of the VTLN/SAT models are then iteratively 
adapted using the SMAPLR algorithm (Siohan, 2002) to 
yield a final recognition error rate of 8.0% absolute (Ta-
ble 1(G)).  By combining all of these techniques, we 
achieved a 54% reduction in word error rate relative to 
the baseline system.    
 
Word Error Rate (%) Experimental Configuration MFCC PMVDR 
(A) Baseline: single n-gram 
language model 17.7% 17.4% 
(B) (A) + Begin/End Sentence 
Context Modeling 14.0% 13.5% 
(C) (B) + between utterance 
word history modeling 13.0% 12.7% 
(D) (C) + dynamic  
n-gram language model 11.0% 10.7% 
(E) (D) + VTLN 10.9% 10.6% 
(F) (E) + VTLN/SAT + 
SMAPLR (iteration 1) 8.2% 8.2% 
(G) (E) + VTLN/SAT + 
SMAPLR (iteration 2) 8.0% 8.0% 
Table 1: Recognition of children?s read out-loud data. 
6 Improved Story Summary Recognition 
One of the unique and powerful features of our interac-
tive books is the notion of assessing and training com-
prehension by providing feedback to the student about a 
typed summary of text that the student has just read 
(Cole et al, 2003). Verbal input is especially important 
for younger children who often can not type well. Util-
izing summaries from the children?s speech corpus, 
Hagen et al (2003) showed that an error rate of 42.6% 
could be achieved.  The previous work, however, did 
not consider utilizing the read story material to provide 
improved initial acoustic models for the summarization 
task.  In Table 2 we demonstrate several findings using 
a language model trained on story text and example 
summaries produced by children (leaving out data from 
the child under test).  Without any adaptation the error 
rate is 47.1%.  However, utilizing adapted models from 
the read stories (see Table 1(G)) provides an initial per-
formance gain of nearly 10% absolute.  Further use 
SMAPLR adaptation reduces the error rate to 36.1%. 
 Word Error Rate (%) Experimental Configuration MFCC PMVDR 
(A) Baseline / no adaptation 47.0% 47.1% 
(B) Read-aloud adapted models 
(VTLN/SAT) 37.2% 38.0% 
(C) (B) + SMAPLR  
adaptation iteration #1 36.0% 36.6% 
(D) (C) + SMAPLR 
adaptation iteration #2 35.1% 36.1% 
Table 2:  Recognition of spontaneous story summaries 
7 Practical Real-Time Implementation 
The research systems described in Sect. 5 and 6 do not 
operate in real-time since multiple adaptation passes 
over the data are required.  To address this issue, we 
have implemented a real-time system that operates on 
small pipelined audio segments (250ms on average).  
When evaluated on the read-aloud task (Sect. 5), the 
initial baseline system achieves an error rate of 19.5%.  
This system has a real-time factor of 0.56 on a 2.4 GHz 
Intel Pentium 4 PC with 512MB of RAM. When inte-
grated, the proposed methods show the error rate can be 
reduced from 19.5% to 12.7% (compare with 10.7% 
error research system in Table 1(D)).  The revised sys-
tem which incorporates dynamic language modeling 
operates 35% faster than the single language model 
method while also reducing the variance in real-time 
factor for each processed chunk of audio.  Further gains 
are possible by incorporating adaptation in an incre-
mental manner.  For example, in Table 3(C) a real-time 
system that incorporates incremental unsupervised 
maximum likelihood linear regression (MLLR) adapta-
tion of the Gaussian means is shown.  This final real-
time system simultaneously adapts both language and 
acoustic model parameters during system use. The sys-
tem is now being refined for deployment in classrooms 
within the CLT project. We were able to further im-
prove the system after the submission deadline. The 
current WER on the story read aloud task improved to 
7.6%; while a WER of 32.2% was achieved on the 
summary recognition task. The improvements are due to 
the inclusion of a breath model and the additional use of 
audio data from 103 second graders for more accurate 
acoustic modeling. 
 
PMVDR Front-End System Description WER (%) RTF 
(A) Baseline: single LM 19.5% 0.56  (  2=0.11) 
(B) Proposed System 12.7% 0.36 (  2=0.06) 
(C) (B) + Incremental 
MLLR adaptation 11.5% 
0.80 
(  2=0.33) 
Table 3:  Evaluation of real-time read out-loud system. 
References 
 
V. Zue, S. Seneff, J. Polifroni, H. Meng, J. Glass 
(1996). ?Multilingual Human-Computer Interactions: 
From Information Acess to Language Learning,? 
ICSLP-96, Philadelphia, PA 
J. Mostow, S. Roth, A. G. Hauptmann, and M. Kane 
(1994). "A Prototype Reading Coach that Listens", 
AAAI-94, Seattle, WA, pp. 785-792.  
Y-C. Tam, J. Mostow, J. Beck, and S. Banerjee (2003). 
?Training a Confidence Measure for a Reading Tutor 
that Listens?. Eurospeech, Geneva, Switzerland, 
3161-3164. 
J. Mostow, J. Beck, S. Winter, S. Wang, and B. Tobin 
(2002). ?Predicting oral reading miscues? ICSLP-02, 
Denver, Colorado. 
J. Mostow, G. Aist, P. Burkhead, A. Corbett, A. Cuneo, 
S. Eitelman, C. Huang, B. Junker, M. B. Sklar, and 
B. Tobin (2003). ?Evaluation of an automated Read-
ing Tutor that listens:  Comparison to human tutoring 
and classroom instruction?. Journal of Educational 
Computing Research, 29(1), 61-117 
R. Cole, S. van Vuuren, B. Pellom, K. Hacioglu, J. Ma, 
J. Movellan, S. Schwartz, D. Wade-Stein, W. Ward, 
J. Yan (2003). ?Perceptive Animated Interfaces: First 
Steps Toward a New Paradigm for Human Computer 
Interaction,? Proceedings of the IEEE, Vol. 91, No. 
9, pp. 1391-1405 
A. Hagen, B. Pellom, and R. Cole (2003). "Children?s 
Speech Recognition with Application to Interactive 
Books and Tutors", ASRU-2003, St. Thomas, USA 
B. Pellom (2001). "SONIC: The University of Colorado 
Continuous Speech Recognizer", Technical Report 
TR-CSLR-2001-01, University of Colorado. 
B. Pellom and K. Hacioglu (2003). "Recent Improve-
ments in the CU Sonic ASR System for Noisy 
Speech: The SPINE Task", ICASSP-2003, Hong 
Kong, China. 
U. Yapanel, J. H.L. Hansen (2003). "A New Perspective    
on Feature Extraction for Robust In-vehicle Speech  
Recognition" Eurospeech, Geneva, Switzerland. 
K. Shobaki, J.-P. Hosom, and R. Cole (2000). "The OGI 
Kids' Speech Corpus and Recognizers", Proc. 
ICSLP-2000, Beijing, China. 
L. Welling, S. Kanthak, and H. Ney. (1999) "Improved 
Methods for Vocal Tract Length Normalization", 
ICASSP, Phoenix, Arizona. 
M. Gales (1997). Maximum Likelihood Linear Trans-
formations for HMM-Based Speech Recognition", 
Tech. Report, CUED/F-INFENG/TR291, Cambridge 
University. 
O. Siohan, T. Myrvoll, and C.-H. Lee (2002) "Structural 
Maximum a Posteriori Linear Regression for Fast 
HMM Adaptation", Computer, Speech and Lan-
guage, 16, pp. 5-24.  
Proceedings of NAACL HLT 2009: Short Papers, pages 77?80,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Generating Synthetic Children's Acoustic Models from Adult Models 
 
Andreas Hagen, Bryan Pellom, and Kadri Hacioglu 
Rosetta Stone Labs  
{ahagen, bpellom, khacioglu}@rosettastone.com 
 
 
Abstract 
This work focuses on generating children?s 
HMM-based acoustic models for speech rec-
ognition from adult acoustic models. Collect-
ing children?s speech data is more costly 
compared to adult?s speech. The patent-
pending method developed in this work re-
quires only adult data to estimate synthetic 
children?s acoustic models in any language 
and works as follows: For a new language 
where only adult data is available, an adult 
male and an adult female model is trained. A 
linear transformation from each male HMM 
mean vector to its closest female mean vector 
is estimated. This transform is then scaled to a 
certain power and applied to the female model 
to obtain a synthetic children?s model. In a 
pronunciation verification task the method 
yields 19% and 3.7% relative improvement on 
native English and Spanish children?s data, re-
spectively, compared to the best adult model. 
For Spanish data, the new model outperforms 
the available real children?s data based model 
by 13% relative. 
1 Introduction 
Language learning is becoming more and more 
important in the age of globalization. Depending 
on their work or cultural situation some people are 
confronted with various different languages on a 
daily basis. While it is very desirable to learn lan-
guages at any age, language learning, among other 
learning experiences, is comparably simpler for 
children than for adults and should therefore be 
encouraged at early ages. 
Even though the children?s language learning mar-
ket is highly important, comprising effective 
speech recognition tools for pronunciation assess-
ment is relatively hard due to the special characte-
ristics of children?s speech and the limited 
availability of children?s speech data in many lan-
guages in the speech research community. Adult 
speech data is usually easier to obtain. By under-
standing the characteristics of children?s speech the 
unconditional need for children?s speech data can 
be lessened by altering adult acoustic models such 
that they are suitable for children?s speech. 
Children?s speech has higher pitch and formants 
than female speech. Further, female speech has 
higher pitch and formants than male speech. Child-
ren?s speech is more variable than female speech, 
and, as research has shown, female speech is more 
variable than male speech (Lee et al, 1999). Given 
this transitive chain of argumentation, the trans-
formation from a male to a female acoustic model 
can be estimated for a language and applied (at a 
certain adjustable degree) to the female model. 
This process results in a synthetic children?s 
speech model designed on the basis of the female 
model. Therefore, for a new language an effective 
synthetic children?s acoustic model can be derived 
without the need of children?s data (Hagen et al, 
2008). 
2 Related Work  
Extensive research has been done in the field of 
children?s speech analysis and recognition in the 
past few years. A detailed overview of children?s 
speech characteristics can be found in (Lee et al, 
1999). The paper presents research results showing 
the higher variability in speech characteristics 
among children compared to adult speech. The 
properties of children?s speech that were re-
searched were duration of vowels and sentences, 
pitch, and formant locations. 
When designing acoustic models specially suited 
for children, properties as the formant locations 
and higher variability of children?s speech need to 
be accounted for. The best solution for building 
children?s speech models is to collect children?s 
speech data and to train models from scratch (Ha-
77
gen et al, 2003, Cosi et al 2005). Researchers 
have also tried to apply adult acoustic models us-
ing speaker normalization techniques to recognize 
children?s speech (Elenius et al, 2005, Potamianos 
et al 1997). Adult acoustic models were adapted 
towards children?s speech. A limited amount of 
children?s speech data was available for adapta-
tion. In (Gustafson et al, 2002) children?s voices 
were transformed before being sent to the recog-
nizer using adult acoustic models. In (Claes et al, 
1997) children?s acoustic models were built based 
on a VTL adaptation of cepstral parameters based 
on the third formant frequency. The method 
showed to be effective for building children?s 
speech models. 
3 Building Synthetic Children?s Models 
from Adult Models 
As mentioned in Section 1, research has shown 
that pitch and formants of children?s speech are 
higher than for female speech. Female speech has 
higher pitch and formants than male speech. In 
order to exploit these research results a transforma-
tion from a male acoustic model to a female acous-
tic model can be derived. This transformation will 
map a male model as close as possible to a female 
model. The transformation can be adjusted and 
applied to the female model. The resulting synthet-
ic model can be tested on children?s data. 
Parameters that are subject to transformation in 
this process are the mean vectors of the HMM 
states. The transformation can be represented as a 
square matrix in the dimension of the mean vec-
tors. The transformation chosen in this approach is 
therefore linear and is for example capable of 
representing a vocal tract length adaptation as it 
was shown in (Pitz et al, 2005). Linear transfor-
mations (i.e. matrices) are also chosen in adapta-
tion approaches as MAPLR and MLLR, whose 
benefit has been shown to be additive to the benefit 
of VTLN in speaker adaptation applications. A 
linear transform in the form of a matrix is therefore 
well suited due to its expressive power as well as 
its mathematical manageability. 
3.1 Transformation Matrix 
The transformation matrix used in this approach is 
estimated by mapping the male to the female 
acoustic model, such that each HMM state mean 
vector in the male model is assigned a correspond-
ing mean vector in the female model. Information 
used in the mapping process is the basic phoneme 
and context. The resulting mean vector pairs are 
used as source and target features in the training 
process of the transformation matrix. During train-
ing the matrix is initialized as the identity matrix 
and the estimate of the mapping is refined by gra-
dient descent. In a typical acoustic model there are 
several hundred, sometimes thousands, of these 
mean vector pairs to train the transformation ma-
trix. The expression that needs to be minimized is: 
2
),(
)(minarg yAxT
pairsyxA
?= ?   
where T is the error-minimizing transformation 
matrix; x is a male model?s source vector and y it 
corresponding female model?s target vector.  
In this optimization process the Matrix A is initia-
lized as the identity matrix. Each matrix entry ija is 
updated (to the new value 'ija ) in the following way 
by gradient descent: 
( ) jiiijij xyxAkaa ?+='  
where iA  is the i-th line of matrix A and k deter-
mines the descent step size (k<0 and incorporates 
the factor of 2 resulting from the differentiation). 
The gradient descent needs to be run multiple 
times over all vector pairs (x,y) for the matrix to 
converge to an acceptable approximation which is 
called the transformation matrix T. 
3.2 Synthetic Children?s Model Creation 
The transformation matrix can be applied to the 
female model in order to create a new synthetic 
acoustic model which should suit children?s speech 
better than adult acoustic models. It is unlikely that 
the transformation applied ?as is? will result in the 
best model possible, therefore the transformation 
can be altered (amplified or weakened) in order to 
yield the best results. An intuitive way to alter the 
impact of the transformation is taking the matrix T 
to a certain power p. Synthetic models can be 
created by applying pT  to the female model1, for 
various values p. If children?s data is available for 
evaluation purposes, the best value of p can be de-
termined. The power p is claimed to be language 
independent. It might vary in nuances, but experi-
                                                          
1
 Taking a matrix to the power of p is meant in the sense 
TT
pp
=
/1
, IdentityT =0 , TT =1  
78
ments have shown that a value around 0.25 is a 
reasonable choice. 
3.3 Transformation Algorithm 
The previous section presented the theoretical 
means necessary for the synthetic children?s model 
creation process. The precise, patent-pending algo-
rithm to create a synthetic children?s model in a 
new language is as follows (Hagen et al, 2008): 
 
1. Train a male and a female acoustic model 
2. Estimate the transform T from the male 
to the female model 
3. Determine the power p by which the 
transform T should be adjusted 
4. Apply pT  to the female acoustic model 
to create the synthetic children?s model 
 
Step 3, the determination of the power p, can be 
done in two different ways. If children?s test data 
in the relevant language is available, various mod-
els based on different p-values can be evaluated 
and the best one chosen. If there is no children?s 
data available in a new language, p can be esti-
mated by evaluations in a language where there is 
enough male, female, and children?s speech data 
available. The claim here is that the power p is rel-
atively language independent and estimating p in a 
different language is superior to a simple guess. 
4 Experiments 
The algorithm was tested on two languages: US 
English and Spanish. For both languages sufficient 
male, female, and children?s speech data was 
available (more than 20 hours) in order to train 
valid acoustic models and to have reference child-
ren?s acoustic models available. For English test 
data we used a corpus of 22 native speakers in the 
age range of 5 to 14. The number of utterances is 
2,182. For Spanish test data the corpus is com-
prised of 19 speakers in the age range of 8 to 13 
years. The number of utterances is 2,598. 
The transform from the male to the female model 
was estimated in English. The power of p was 
gradually increased and the transformation matrix 
was adjusted. With this adjusted matrix pT  a syn-
thetic children?s model was built. This synthetic 
children?s model was evaluated on children?s test 
data and the results were compared to the reference 
children?s model?s and the female model?s perfor-
mance. 
When speech is evaluated in a language learning 
system, the first step is utterance verification, 
meaning the task of evaluating if the user actually 
tried to produce the desired utterance. The Equal 
Error Rate (EER) on the utterance level is a means 
of evaluating this performance. For each utterance 
an in- and out-of-grammar likelihood score is de-
termined. The EER operating points, determined 
by the cutting point of the two distributions (in-
grammar and out-of-grammar), are reported as an 
error metric. Figure 1 shows the EER values of the 
synthetic model applied to children?s data. 
 
 
 
Figure 1: Synthetic model?s EER performance de-
pending on the power p used for model creation. 
 
It can be seen that the best performance is reached 
at about p=0.25. The overview of the results is 
given in Table 1. 
 
 Equal Error Rate  
Real Children?s Model 1.90% 
Male Model 4.07% 
Female Model 2.92% 
Synthetic Model 2.36% 
 
Table 1: EER numbers when using a real children?s 
model compared to a male, female, and synthetic 
model for children?s data evaluation. 
 
The results show that the synthetic children?s mod-
el yields good classification results when applied 
to children?s data. The gold standard, the real 
children?s model application, results in the best 
EER performance. 
If the same evaluation scenario is applied to Span-
ish, a very similar picture evolves. Figure 2 shows 
the EER results versus transformation power p for 
Spanish children?s data. 
 
79
  
Figure 2: Spanish synthetic model?s EER perfor-
mance depending on the power p used for model 
creation. 
 
In Figure 2 it can be seen that the optimal setting 
for p is about 0.27. This value is very similar to the 
one found for US English, which supports, but cer-
tainly does not prove, the language independence 
claim. Results for Spanish are given in Table 2. 
 
 Equal Error Rate 
Real Children?s model 2.40% 
Male model 5.62% 
Female model 2.17% 
Synthetic model 2.09% 
 
Table 2: EER numbers for Spanish when using a 
real children?s model compared to a male, female, 
and synthetic model for Spanish children?s data 
evaluation. 
 
Similar to English, the Spanish synthetic model 
performs better than the female model on child-
ren?s speech. Interestingly, the acoustic model 
purely trained on children?s data performs worse 
than the female and the synthetic model. It is not 
clear why the children?s model does not outper-
form the female and the synthetic model; an expla-
nation could be diverse and variable training data 
that hurts classification performance. 
It can be seen that for US English and Spanish the 
power p used to adjust the transformation is about 
0.25. Therefore, for a new language where only 
adult data is available, the transformation from the 
male to the female model can be estimated and 
applied to the female model (after being adjusted 
by p=0.25). The resulting synthetic model will 
work reasonably well and could be refined as soon 
as children?s data becomes available. 
 
5 Conclusion 
This work presented a new technique to create 
children?s acoustic models from adult acoustic 
models without the need for children?s training 
data when applied to a new language. While it can 
be assumed that the availability of children?s data 
would improve the resulting acoustic models, the 
approach is effective if children?s data is not avail-
able. It will be interesting to see how performance 
of this technique compares to adapting adult mod-
els by adaptation techniques, i.e. MLLR, when li-
mited amounts of children?s data are available. 
Two scenarios are possible: With increasing 
amount of children?s data speaker adaptation will 
draw even and/or be superior. The other possibility 
is that the presented technique yields better results 
regardless how much real children?s data is availa-
ble, due to the higher variability and noise-
pollution of children?s data. 
References  
Claes, T., Dologlou, I, ten Bosch, L., Van Compernolle, 
D. 1997. New Transformations of Cepstral Parame-
ters for Automatic Vocal Tract Length Normalization 
in Speech Recognition, 5th Europ. Conf. on Speech 
Comm. and Technology, Vol. 3: 1363-1366. 
Cosi, P., Pellom, B. 2005. Italian children's speech rec-
ognition for advanced interactive literacy tutors. 
Proceedings Interspeech, Lisbon, Portugal. 
Elenius, D. and Blomberg, M. 2005. Adaptation and 
Normalization Experiments in Speech Recognition 
for 4 to 8 Year old Children. Proceedings Inters-
peech, Lisbon, Portugal. 
Gustafson, J., Sj?lander, K. 2002. Voice transformations 
for improving children?s speech recognition in a pub-
licly available dialogue system. ICSLP, Denver. 
Hagen, A., Pellom, B., and Cole, R. 2003. Children's 
Speech Recognition with Application to Interactive 
Books and Tutors. Proceedings ASRU, USA. 
Lee, S., Potamianos, A., and Narayanan, S. 1999. 
Acoustics of children's speech: Developmental 
changes of temporal and spectral parameter. J. 
Acoust. Soc. Am., Vol. 105(3):1455-1468. 
Pitz, M., Ney, H. 2005. Vocal Tract Normalization 
Equals Linear Transformation in Cepstral Space. 
IEEE Trans. Speech & Audio Proc., 13(5): 930-944. 
Potamianos, A., Narayanan, S., and Lee, S. 1997. Auto-
matic Speech Recognition for Children. Proceedings 
Eurospeech, Rhodes, Greece. 
Hagen, A., Pellom, B., and Hacioglu, K. 2008. Method 
for Creating a Speech Model. US Patent Pending.  
 
80
