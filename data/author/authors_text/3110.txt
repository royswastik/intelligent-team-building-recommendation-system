BioNLP 2007: Biological, translational, and clinical language processing, pages 161?162,
Prague, June 2007. c?2007 Association for Computational Linguistics
Exploring the Use of NLP in the Disclosure of Electronic Patient Records
David Hardcastle
Faculty of Mathematics and Computing
The Open University
d.w.hardcastle@open.ac.uk
Catalina Hallett
Faculty of Mathematics and Computing
The Open University
c.hallett@open.ac.uk
Abstract
This paper describes a preliminary analysis
of issues involved in the production of re-
ports aimed at patients from Electronic Pa-
tient Records. We present a system proto-
type and discuss the problems encountered.
1 Introduction
Allowing patient access to Electronic Patient
Records (EPR) in a comprehensive format is a le-
gal requirement in most European countries. Apart
from this legal aspect, research shows that the provi-
sion of clear information to patients is instrumental
in improving the quality of care (Detmer and Sin-
gleton, 2004). Current work on generating expla-
nations of EPRs to patients suffer from two major
drawbacks. Firstly, existing report generation sys-
tems have taken an intuitive approach to the gener-
ation of explanation: there is no principled way of
selecting the information that requires further expla-
nation. Secondly, most work on medical report gen-
eration systems has concentrated on explaining the
structured part of an EPR; there has been very lit-
tle work on providing automatic explanations of the
narratives (such as letters between health practition-
ers) which represent a considerable part of an EPR.
Attempting to rewrite narratives in a patient-friendly
way is in many ways more difficult than providing
suggestions for natural language generation systems
that take as input data records. In narratives, ambi-
guity can arise from a combination of aspects over
which NLG systems have full control, such as syn-
tax, discourse structure, sentence length, formatting
and readability.
This paper introduces a pilot project that attempts
to address this gap by addressing the following re-
search questions:
1. Given the text-based part of a patient record,
which segments require explanation before being re-
leased to patients?
2. Which types of explanation are appropriate for
various types of segment?
3. Which subparts of a segment require explanation?
The prototype system correctly selects the seg-
ments that require explanation, but we have yet to
solve the problem of accurately identifiying the fea-
tures that contribute to the ?expertness? of a doc-
ument. We discuss the underlying issues in more
detail in section 3 below.
2 Feature identification method
To identify a set of features that differentiate med-
ical expert and lay language, we compared a cor-
pus of expert text with a corpus of lay texts. We
then used the selected features on a corpus of nar-
ratives extracted from a repository of Electronic Pa-
tient Records to attempt to answer the three ques-
tions posed above. First, paragraphs that contain
features characteristic to expert documents are high-
lighted using a corpus of patient information leaflets
as a background reference. Second, we prioritise the
explanations required by decomposing the classifi-
cation data. Finally, we identify within those sec-
tions the features that contribute to the classification
of the section as belonging to the expert register, and
provide suggestions for text simplification.
2.1 Features
The feature identification was performed on two cor-
pora of about 200000 words each: (a) an expert
corpus, containing clinical case studies and med-
ical manuals produced for doctors and (b) a lay
corpus, containing patient testimonials and infor-
mational materials for patients. Both corpora were
161
sourced from a variety of online sources. In com-
paring the corpora we considered a variety of fea-
tures in the following categories: medical content,
syntactic structure, discourse structure, readability
and layout. The features that proved to be best dis-
criminators were the frequency of medical terms,
readability indices, average NP length and the rela-
tive frequency of loan words against English equiva-
lents1. The medical content analysis is based on the
MeSH terminology (Canese, 2003) and consists of
assessing: (a) the frequency of MeSH primary con-
cepts and alternative descriptions, (b) the frequency
of medical terms types and occurences and (c) the
frequency of MeSH terms in various top-level cate-
gories. The readability features consist of two stan-
dard readability indices (FOG and Flesch-Kincaid).
Although some discourse and layout features also
proved to have a high discriminatory power, they
are strongly dependent on the distribution medium
of the analysed materials, hence not suitable for our
analysis of EPR narratives.
2.2 Analysing EPR narratives
We performed our analysis on a corpus of 11000
narratives extracted from a large repository of Elec-
tronic Patient Records, totalling almost 2 million
words. Each segment of each narrative was then as-
sessed on the basis of the features described above,
such as Fog, sentence length, MeSH primary con-
cepts etc. We then smoothed all of the scores for
all segments for each feature forcing the minimum
to 0.0, the maximum to 1.0 and the reference corpus
score for that feature to 0.5. This made it possible to
compare scores with different gradients and scales
against a common baseline in a consistent way.
3 Evaluation and discussion
We evaluated our segment identification method on
a set of 10 narratives containing 27 paragraphs, ex-
tracted from the same repository of EPRs . The seg-
ment identification method proved succesful, with
26/27 (96.3%) segments marked correctly are re-
quiring/not requiring explanation. However, this
only addresses the first of the three questions set
out above, leaving the following research questions
1An in-depth analysis of unfamiliar terms in medical docu-
ments can be found in (Elhadad, 2006)
open to further analysis.
Quantitative vs qualitative analysis
Many of the measures that discriminate expert from
lay texts are based on indicative features; for exam-
ple complex words are indicative of text that is dif-
ficult to read. However, there is no guarantee that
individual words or phrases that are indicative are
also representative - in other words a given complex
word or long sentence will contribute to the readabil-
ity score of the segment, but may not itself be prob-
lematic. Similarly, frequency based measures, such
as a count of medical terminology, discriminate at a
segment level but do not entail that each occurrence
requires attention.
Terminology
We used the MeSH terminology to analyse med-
ical terms in patient records, however (as with prac-
tically all medical terminologies) it contains many
non-expert medical terms. We are currently investi-
gating the possibility of mining a list of expert terms
from MeSH or of making use of medical-lay aligned
ontologies.
Classification
Narratives in the EPR are written in a completely dif-
ferent style from both our training expert corpus and
the reference patient information leaflets corpus. It
is therefore very difficult to use the reference corpus
as a threshold for feature values which can produce
good results on the corpus of narratives, suggest-
ing that a statistical thresholding technique might be
more effective.
Feature dependencies
Most document features are not independent. There-
fore, the rewriting suggestions the system provides
may themselves have an unwanted impact on the
rewritten text, leading to a circular process for the
end-user.
References
Kathi Canese. 2003. New Entrez Database: MeSH.
NLM Technical Bulletin, March-April.
D. Detmer and P. Singleton. 2004. The informed pa-
tient. Technical Report TIP-2, Judge Institute of Man-
agement, University of Cambridge, Cambridge.
Noemi Elhadad. 2006. Comprehending technical texts:
Predicting and defining unfamiliar terms. In Proceed-
ing of AMIA?06, pages 239?243.
162
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 40?48,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Fast, Scalable and Reliable Generation of Controlled Natural Language
David Hardcastle
Faculty of Maths, Computing
and Technology
The Open University
Milton Keynes, UK
d.w.hardcastle@open.ac.uk
Richard Power
Faculty of Maths, Computing
and Technology
The Open University
Milton Keynes, UK
r.power@open.ac.uk
Abstract
In this paper we describe a natural language
generation system which takes as its input a
set of assertions encoded as a semantic graph
and outputs a data structure connecting the se-
mantic graph to a text which expresses those
assertions, encoded as a TAG syntactic tree.
The scope of the system is restricted to con-
trolled natural language, and this allows the
generator to work within a tightly restricted
domain of locality. We can exploit this fea-
ture of the system to ensure fast and efficient
generation, and also to make the generator re-
liable by providing a rapid algorithm which
can exhaustively test at compile time the com-
pleteness of the linguistic resources with re-
spect to the range of potential meanings. The
system can be exported for deployment with
a minimal build of the semantic and linguistic
resources that is verified to ensure that no run-
time errors will result from missing resources.
The framework is targeted at using natural lan-
guage generation technology to build semantic
web applications where machine-readable in-
formation can be automatically expressed in
natural language on demand.
1 Introduction
This paper describes a fast, reliable and scalable
framework for developing applications supporting
tactical generation ? by which we mean applications
which take as their input some semantic structure
that has already been organised at a high level, and
choose the syntactic structures and words required to
express it. The framework takes as input a semantic
graph representing a set of assertions in Description
Logic (DL) (Baader et al, 2003) and transforms it
into a tree which encodes the grammar rules, syn-
tactic subcategorisations, orderings and lexical an-
chors required to construct a textual representation
of the input data. The resulting text is conceptu-
ally aligned, by which we mean that each compo-
nent of the text structure (such as words, clauses or
sentences, for example) is linked back to the medi-
ating structure from which the text was generated,
and from there back to vertices and edges in the
semantic graph received as input. The target con-
text for the framework is the construction of se-
mantic web (Berners-Lee et al, 2001) resources us-
ing Natural Language Generation (NLG) technology
which extends the notion of semantic alignment de-
veloped in the WYSIWYM system (Power and Scott,
1998; Power et al, 2003). In this context the text is
ephemeral and is generated on demand, while the
document content is fully machine-readable, sup-
porting tasks such as automated consistency check-
ing, inferencing and semantic search/query. Since
the text is fully linked to the underlying semantic
representation it supports a rich user interface en-
compassing fast and reliable semantic search, inline
syntax or anaphora highlighting, knowledge editing,
and so on. Finally, the text could be generated in
many different natural languages making the infor-
mation content more widely available. We envisage
the technology supporting a range of different use
cases such as information feeds, technical instruc-
tions, medical orders or short, factual reports.
For such a system to be of practical value in an
enterprise system the NLG component must sup-
40
port standard aspects of software engineering qual-
ity such as modularity, reliability, speed and scala-
bility. The design of the framework relies on two key
simplifying assumptions and these limit the range of
information which can be represented and the flu-
ency of the text used to express it. Specifically, the
information is limited by the expressivity of DL ?
for example only limited quantification is possible ?
and the surface text is restricted to controlled natu-
ral language (Hartley and Paris, 2001). The upside
of this trade-off is that the domain of locality is very
restricted. This means that there is minimal search
during generation and so the algorithm is fast and
scalable. It also enables us to design the generator
so that it is predictable and can therefore be statically
tested for completeness, a notion which we define in
Section 3.
Our aim in this paper is to show how the simpli-
fying assumptions behind the design bring consider-
able engineering benefits. We discuss the theoreti-
cal background to our approach (Sections 2 and 3)
and then present the implementation details, focus-
ing on the features of the design that support speed
and scalability (Section 4) and reliability (Section
5), followed by an overview of the architectural con-
siderations (Section 6). Finally we present the re-
sults of tests evaluating the system?s performance
(Section 7).
2 Implementation Theory
The generation algorithm has its roots in the WYSI-
WYM system, which was originally developed as a
way of defining the input for multilingual NLG in
DRAFTER (Paris et al, 1995), one of a series of
projects in the KPML/Penman tradition (Bateman
et al, 1989). The system uses the standard seman-
tic representation employed in DL and the Semantic
Web: a Terminology Box, or Tbox, defining the con-
cepts and their interrelations and an Assertion Box,
or Abox, representing the information content that
forms the input (Baader et al, 2003). An Abox is a
set of assertions defining relations between instances
of the types defined in the Tbox. It can be depicted
by a connected graph (Figure 1) in which vertices
represent entities and edges represent relations, and
is represented in the input to the system by a set
of RDF subject-predicate-argument triples (Lassila
Figure 1: Sample Abox
and Swick, 1998), with one-place predications as-
signing types and two-place predications asserting
relationships. Assuming that the entities are being
mentioned for the first time, we might express this
Abox fragment in English by the sentence ?a woman
lost her bag?1. This sentence can be aligned with the
Abox by associating spans of the text with the enti-
ties expressed by the Abox, as follows:
Span Entity Context
a woman lost her bag e1 ROOT
a woman e2 AGENT
her bag e3 PATIENT
her e2 OWNER
Note that the same entity may be expressed in
multiple contexts (denoted by the incoming arcs
in the semantic graph). The relationships between
the entities are represented by syntactic dependen-
cies between the spans of the text. For instance,
AGENT(e1,e2) is realised by the clause-subject rela-
tion between ?a woman lost her bag? and its subspan
?a woman?. This direct linking of semantic and syn-
tactic dependencies has of course been noted many
times, for instance in Meaning-Text Theory (Can-
dito and Kahane, 1998).
The structure of the spans of text can be repre-
sented by a reconfiguration of the original Abox as
an ordered tree, which we will henceforth call an
Atree. Figure 2 shows an Atree that fits the exam-
ple Abox. Note that since this is a tree, the vertex
with two incoming edges (e2) has to be repeated,
and there are two spans referring to the woman.
1The system is able to generate a referring expression, ?her?,
for the second reference to the woman since it knows that the
entity has already been mentioned in the text. This informa-
tion is available because the Atree, see Figure 2, is an ordered
structure.
41
Figure 2: Sample Atree
This Atree is constructed using a set of bindings
which map instances of concepts from the Tbox in a
given context onto a subcategorisation frame, gram-
mar rule call and list of lexical anchors. As each ver-
tex of the Atree is constructed it is labelled with the
grammar rule and lexical anchors and linked back to
the vertex of the Abox which it expresses. Our cur-
rent model uses the Tree Adjoining Grammar (TAG)
formalism, see Joshi (1987), and the Atree acts as a
stand-in derivation tree from which the derived syn-
tactic tree can be computed. Each vertex of the de-
rived tree is linked back to the vertex of the Atree
from which it was generated, and so the output from
the system is a composite data structure compris-
ing the Tbox, Abox, Atree and derived tree with a
chain of references connecting each span of the sur-
face text via the Atree to the underlying semantic
representation. A detailed exposition of the process
through which the Atree and derivation tree are con-
structed is presented in a separate Technical Report
(Hardcastle and Power, 2008).
2.1 Simplifying assumptions
The design of the generator relies on two simplify-
ing assumptions. The key assumption for this paper
is that the text should adhere strictly to a controlled
language, so that a given local semantic configura-
tion is always realised by the same linguistic pattern.
The cost is that the text is likely to be repetitive and
may be awkward at times; however the trade-off is
that the domain of locality is tightly restricted and
this yields important benefits in speed, scalability,
reliability and verifiability that make the system suit-
able for deployment in an enterprise environment.
We also assume that the strategic element of the
NLG process, comprising content selection and doc-
ument structuring, has occurred prior to our system
receiving its input. Our framework is focused specif-
ically on tactical generation ? rendering the semantic
representation of the selected content as text.
3 Completeness
We can verify that the generator is complete, in the
sense that we can guarantee that it will produce a
derivation tree for any Abox valid under the Tbox.
We present the details of the verification algorithm
below, in Section 5. Note that we assume that the
system is equipped with the requisite morpholog-
ical and orthographic rules to realise the resulting
derivation tree. We also note that we cannot verify
that the generator is consistent, by which we mean
that it should produce different texts for different
Aboxes, nor that the syntactic frames and lexical an-
chors mapped to the concepts in the Tbox are appro-
priate. Checking the system for consistency remains
an open research question.
4 Speed and Scalability
In many NLG systems the choice of syntactic struc-
ture and lexical arguments depends on a large num-
ber of interdependent variables. This means that the
process of realizing the semantic input involves ex-
ploring a large search space, often with some back-
tracking. In contrast, the algorithm described in this
paper is monotonic and involves minimal search.
The system begins at the root of the Abox and uses
a set of mappings to construct the Atree one node
at a time. Because the same local semantic context
is always expressed in the same way the choice of
syntactic structure and lexical arguments can always
be made on the basis of a single mapping. Over
the course of this section we demonstrate this with a
simple example using resources that were automati-
cally inferred to construct the test harness described
in Section 8, which could be used to construct the
following simple sentence:
The nurse reminded the doctor that the pa-
tient was allergic to aspirin.
The Abox representing this sentence is rooted in
an instance of a Tbox concept representing an event
42
in which one person reminds another of a fact. Fig-
ure 3 shows the attributes defined for this Tbox con-
cept, 938B, namely an actor, actee and target. The
range of actor and actee is any concept in the Tbox
subsumed by the person concept, the range of tar-
get is any fact. There will therefore be three out-
going arcs from the root Abox node, labelled with
attributes actor, actee and target, pointing respec-
tively to nodes representing the nurse, doctor and
the fact about the patient?s allergy described in the
sample sentence above. Some of these nodes will
themselves have out-going arcs expressing their own
attributes, such as the subsidiary details of the fact
about the allergy.
938B
actor(person)
actee(person)
target(fact)
Figure 3: A Sample Tbox Node
To realize the first node in the Abox the system
searches for mappings for concept 938B. The con-
trolled language assumption allows the system to
search with a restricted domain of locality, and so
the only variables affecting the choice of frame will
be: the Tbox concept represented by the Abox node
to be realized (938B in this case), the syntactic con-
text (there is none at this stage since we are process-
ing the root node, so the system will default to clause
context), the incoming arc (there is none at this stage
so no constraint is applied), the out-going arcs (the
three attributes specified), and whether or not the in-
stance has already been expressed (in this case it has
not)2. The search parameters are used to locate a
2The last of these variables is used to determine whether or
not a referring expression (an anaphoric reference to an entity
which has already been mentioned) is required. Because the
Atree is ordered and is constructed in order, the system always
knows whether an instance is being mentioned for the first time.
We currently render subsequent mentions by pruning all of the
out-going arcs from the Abox node, which also allows us to
manage cycles in the semantic graph. Since the system knows
which nodes in the semantic graph have already been mentioned
it would also be possible to configure an external call to a GRE
system (Dale, 1989) - an application which infers the content of
a referring expression given the current semantic context.
mapping such as the one depicted in Figure 4 below.
<frame concept=?938B?
role=?any?
subcat=?CatClause-33?
bindings=?SUB,D OB,CL COM?>
<gr key=?TnxOVnx1s2?>
<anchor lemma=?remind? pos=?verb?/>
</gr>
</frame>
Figure 4: A Sample Mapping
This mapping tells the system which subcategori-
sation frame to use, which grammar rule to asso-
ciate with it, which lexical anchors to pass as argu-
ments to the grammar rule and also how to order the
subsidiary arguments of the subcategorisation frame
(the bindings attribute in the frame element). The
subcategorisation frame itself (shown in Figure 5) is
highly reusable as it only defines a coarse-grained
syntactic type and a list of arguments, each of which
consists of a free text label (such as SUB indicat-
ing the subject syntactic dependency) and a coarse-
grained syntactic constraint such as clause, nomi-
nal or modifier. In this example the first attribute
CatClause-33
type= CLAUSE
args= SUB/NOMINAL,
D OB/NOMINAL,
CL COM/CLAUSE
Figure 5: Sample Subcategorisation Frame
of the 938B node, namely the actor, is mapped to
the SUB (subject) argument, so it will become the
first child of the Atree node representing the remind
event. The nominal syntactic constraint will be car-
ried forward as the syntactic context for the nurse
node of the Abox, constraining the choice of map-
ping that the system can make to realise it. So, each
mapping enforces an ordering on the out-going arcs
of the Abox which is used to order the Atree and pro-
vides a syntactic context which is used to constrain
43
the mapping search for each child. The process of
locating and imposing mappings cascades through
the Abox from the root with minimal search and no
backtracking. If multiple mappings are defined for
a given context the first one is always chosen. If no
mapping is located then the system fails.
While the Atree is constructed, it is annotated
with the grammar rules and lexical anchors listed
in each mapping, allowing it to serve as a stand-in
TAG derivation tree from which a derived syntactic
tree can be constructed by instantiating and connect-
ing the elementary trees specified by each grammar
rule. Further details of this process are given in a
Technical Report (Hardcastle and Power, 2008).
So while the controlled language assumption that
we should always express the same local semantic
context in the same way limits expressivity, it also
limits algorithmic choice and prevents backtracking,
which means that the system can generate rapidly
and scale linearly. In the following section we show
how we can prove at compile time that no Abox can
be constructed which will result in a local semantic
context not accounted for in the mappings.
5 Reliability
In a real-world context the Tbox will evolve as the
underlying domain model is extended and enhanced.
As a result, some of the mappings described above
will become defunct and in some instances a re-
quired mapping will not be present. If the system
encounters an Abox which requires a mapping that
is not present it will not backtrack but will fail, mak-
ing the system fragile. To address this problem we
need to be able to run a static test in a short period
of time to determine if any mappings are unused or
missing.
Although the set of possible Abox graphs is an
infinite set, the tight domain of locality means that
there is a finite set of parameters which could be
passed to the generator for any given Tbox. As de-
scribed in the previous section the choice of map-
ping is based only on the following information:
the concept being realised, the syntactic context, the
number of attributes expressed by the concept, the
attribute used to select it, and whether or not this
Abox instance is being mentioned for the first time.
Given a starting TBox node and syntactic context
the system can crawl the TBox recursively using the
subcategorisation frames returned from each param-
eter set to derive a new list of parameter sets to be
tested. Each of these must be tested both as a first
and as a subsequent mention. The result is an algo-
rithm which proves the application?s completeness
(as defined in Section 3) with respect to a particular
domain (represented by the Tbox); if the test suc-
ceeds then it guarantees that the mappings defined
by the system can transform any Abox that is valid
under the given domain into an Atree annotated with
the information required to produce a derived syn-
tactic tree.
As above, the proving algorithm starts with a root
concept in the Tbox and an initial syntactic context
and uses these as the starting parameter set to find
the first mapping. Once a mapping is located it ex-
plores each of the attributes of the root concept using
the syntactic context to which the attribute is bound
by the mapping. Since there is no Abox it constructs
a list of parameter sets to check using every concept
in the range of the attribute.
For example, during the verification process the
prover will encounter the mapping shown above in
Figure 4 for the remind concept 938B in a clausal
context. The concept has three attributes: an actor,
an actee and a target. The first of these has as its
range all of the subconcepts of person defined in the
Tbox, and this introduces a new sub-problem. The
first attribute is bound to the SUB argument of the
subcategorisation frame used in the mapping, in Fig-
ure 4, by the bindings element, and this argument of
the subcategorisation frame imposes a nominal con-
straint. So the fact that concept 938B might need
to be expressed using this mapping means that any
subconcept of person might need to be expressed in
a nominal syntactic context with semantic role ac-
tor, and so the prover now checks each subconcept
with these parameters. If none of the subconcepts of
person define any attributes and a mapping is found
for each then no new sub-problems are introduced
and so this branch of the search bottoms out.
The prover then returns to 938B and processes the
actee and target attributes. The target attribute is
bound to the CL COM argument of the subcategori-
sation frame, and so the new sub-problem involves
checking that every subconcept of fact can be ex-
pressed as a clause with semantic role target. In
44
the ontology the subconcepts of fact include events,
each of which define a number of attributes, and so
this sub-problem branches out into many new sub-
problems before it bottoms out. One such event will
be the concept 938B, but since the mapping that we
have already encountered (Figure 4) is encoded for
any semantic role and the system has already pro-
cessed it the prover can break out and does not fall
into an infinite loop. This checking process contin-
ues recursively until the search space is exhausted,
with each parameter set tested being cached to re-
duce the size of the search space.
6 Relaxing the Simplifying Constraints
The simplifying assumptions described in Sec-
tion 2.1 deliver benefits in terms of performance
and reliability; however, they limit the expressiv-
ity of the language and reduce the scope of what
can be expressed. We can relax some of the con-
straints imposed by the simplifying assumptions and
still have a performant and reliable system, although
proving completeness becomes more complex and
some localised exponential complexity is introduced
into the generation algorithm. In this section we ex-
plore the ways in which relaxing the constraints to
allow quantification or underspecification impact on
the system.
The simplest scenario, which adheres to our sim-
plifying constraints, is that each node in the Abox
expresses exactly one of each of the attributes de-
fined by the Tbox concept which it instantiates. So,
using the remind example above, every instance of
remind must express an actor, an actee and a fact.
In practice the Tbox may allow an attribute not to be
expressed, to be expressed many times or to be ex-
pressed but not specified. We handle the first case by
allowing arguments in the subcategorisation frames
to be marked as optional; for example, a verb frame
may include an optional adverb slot. These optional
arguments increase the number of tests that must be
performed; if a frame has n optional slots then the
system will need to perform 2n checks to verify it,
and will have to consider 2n mapping combinations
during generation. This introduces localised expo-
nentiation into both the generation and the verifica-
tion algorithm, although it will only lead to tractabil-
ity problems if the number of optional slots on any
single frame is too high, since the exponent is only
applied to each frame and not across the whole
search space.
Where an attribute may remain unspecified the
system can be configured to respond in two different
ways. First, unspecified attributes can be included
in the text using the concept that represents the root
of the range. For example, if an event occurs at a
time which is not specified then the system can use
the concept that represents the root of the range (e.g.
timePeriod perhaps) and render it accordingly (?at
some time?). Alternatively the system can prune
all underspecified instances from the Abox before
the Atree is generated. Attributes which may not be
expressed (for either reason) must be flagged in the
TBox so that the proving algorithm knows to match
them to optional arguments in the subcategorisation
frames. This is implemented with a flag on each at-
tribute definition indicating whether its presence in
the Abox is optional.
Relaxing the constraints also impacts on our abil-
ity to verify the grammar rules which are associated
with each mapping. If we use TAG, then we can
easily verify that the syntactic type of the root of the
elementary tree defined by each mapping matches
the syntactic type of the subcategorisation frame to
which it is bound. However, if a mapping can be
accessed via an optional slot in another subcategori-
sation frame, then it must be bound to an auxiliary
tree, that is to an elementary tree which can be added
to the derived tree through adjunction, since any de-
rived tree with open substitution sites will be gram-
matically incomplete. For the system to support this
behaviour each mapping must declare not just the
concept which it realises but also the role (Tbox at-
tribute) which it fulfils, so that both the prover can
determine whether it may be left out, and this in-
creases the combinatorial complexity of the algo-
rithm.
7 Architecture
The design of the generator ensures that it can gen-
erate rapidly and that it can be verified at compile
time. A further feature is that it is implemented
with a component-based modular architecture. For
NLP applications it is particularly important that in-
dividual components can be independently verified
45
and reused, because linguistic resources are time-
consuming and expensive to build and curate. Fur-
thermore, because the mappings from concepts to
subcategorisation frames, grammar rules and lexi-
cal anchors are defined in a single file, the task of
building and maintaining the mappings is easier to
learn and easier to manage. It is also easier to boot-
strap the mappings through resource mining, as we
did ourselves in the construction of the test data set
discussed in Section 8.
The framework manages the graph and tree struc-
tures and the transformations between them, and it
defines the API for the domain and language specific
resources that will be required by the application. It
also defines the API of the linguistic resource man-
ager, leaving it to the application layer to provide
an appropriate implementer using dependency injec-
tion (Fowler, 2004). Rather than define a core ?in-
terlingual? feature structure that attempts to capture
all of the lexical features used by the grammar, the
framework provides a genericised interface to the
linguistic resource manager. This means that gram-
mars for different natural languages can use different
feature structures to define the lexical anchors used
by the application and to support tasks that are the
responsibility of the grammar, such as unification or
morphological inflection. For example, all verbs in
French should have a flag indicating whether avoir
or e?tre is used as a modal auxiliary for the passe?
compose?, but this flag need not be present for other
languages. The Tbox, the subcategorisation frames
and the mappings between them are all defined as
data sources and can be reused across applications
as appropriate. Although they are not defined in
code they can still be verified at compile time by
the prover discussed in the previous section, and this
allows the system to be flexible and modular with-
out introducing the risk of runtime failures caused
by faulty mapping data.
7.1 Export
A further feature of the system which arises from
the proving algorithm is that it supports export be-
haviour. In an enterprise context we want to be
able to reuse linguistic resource components, such
as a lexicon, a grammar, a morphological genera-
tor and so on, across many different applications.
These resources are large and complex and for a
given application much of the data may not be re-
quired. Because the proving algorithm is able to
compile a comprehensive list of the concepts, gram-
matical relations, subcategorisation frames and lexi-
cal anchors that will be required to realise any Abox,
given a starting concept and syntactic context, the
system can cut the Tbox, lexicon, grammar, subcat-
egorisation frame store and related resources to ex-
port a build for deployment, while guaranteeing that
the deployed application will never fail because of
a missing resource. This is of particular value if we
want to reuse large-scale, generic, curated resources
for a small domain and deploy where bandwidth is
an issue ? for example where language generation is
required in a client-heavy internet-based or mobile
application.
8 Testing and Results
We unit-tested the mechanics of the framework,
such as the graph and tree managers. We then built a
proof-of-concept application with a small ontology
representing the domain of patient treatment narra-
tives and handcrafted the subcategorisation frames,
lexical resources and TAG grammars for English,
French, Spanish and Italian. We used this applica-
tion to verify the independence of the framework,
domain and linguistic resources and verified that we
could develop linguistic resources offline and plug
them into the application effectively. The applica-
tion also served as a test harness to test the adaptibil-
ity of the framework to render the same semantic
context in different syntactic structures depending
on the target natural language. For example, we
included the examination of a body part belonging
to a person in the domain, and this was expressed
through a Saxon genitive in English but a preposi-
tional phrase (with the subsidiary NPs in the reverse
order) in the other languages.
To test our assumptions about efficiency and scal-
ability we inferred a larger Tbox, subcategorisation
frames and mappings using a pre-existing data set
of verb frames for English encoded using the COM-
LEX subcategorisation frame inventory (Grishman
et al, 1994). The linguistic resources for the appli-
cation comprised a generative TAG grammar based
on X-TAG (Doran et al, 1994) which we wrote our-
46
selves, the CUV+ lexicon3, and a pre-existing mor-
phological generator for English (Hardcastle, 2007).
To test the performance of the generation process
we used a set of randomly-generated Aboxes derived
from the Tbox to produce texts of increasing size.
For the purposes of testing we defined the size of an
Abox as the total number of nodes and edges in the
graph, which is the number of RDF triples required
to represent it. Table 1 shows the size of the out-
put text in sentences, the time taken to generate it in
milliseconds, averaged over 5 runs, and the ratio of
the time taken to the size of the output which shows
linear scaling4.
Size Timing Timing/Size
31 2 0.065
280 10 0.036
2,800 59 0.021
28,000 479 0.017
Table 1: The time, in milliseconds, taken to generate
Aboxes of increasing size and the ratio of time taken to
the size of the output.
To test the performance of the proving algorithm
we ran the algorithm on a set of Tboxes of differ-
ing sizes. The smallest Tbox in Table 2 is the hand-
crafted proof-of-concept Tbox, the largest is the in-
ferred Tbox described above, and the intermediate
ones were pruned from the large, inferred Tbox at
random cut points. The size of each Tbox is the
total number of attribute-concept pairs which it de-
fines. The table shows the time taken to run the
prover from the root node of the Tbox with no start-
ing syntactic context and the ratio of time taken to
size, which shows linear scaling.
We tested the mechanics of the implementation
of the prover through unit testing, and we tested
the the design with a test suite of sample data. We
performed white box tests by removing individual
bindings from a set of mappings which we judged
to be complete for the small handcrafted Tbox, and
checked to ensure that each was highlighted by the
prover. We performed black box tests by using a
3A publicly available lexicon for English available from the
Oxford Text Archive
4In fact scaling is slightly sub-linear for this test and the
test of the proving algorithm. In both cases that is because of
caching within the framework to improve performance.
Size Timing Timing/Size
125 10 0.08
86,766 432 0.005
2,054,020 8,217 0.004
9,267,444 21,526 0.002
Table 2: The time, in milliseconds, taken to prove relia-
bility for Tboxes of increasing size and the ratio of time
taken to size.
set of inferred mappings, judged by the prover to be
complete, to generate from a large number of ran-
domly structured Aboxes, drawn from our large in-
ferred Tbox, and checked that the generation process
never failed.
We chose not to undertake a formal evaluation
over and above the unit and sampling tests, because
the accuracy of the prover is a function of the re-
stricted domain of locality imposed by the system
and of the recursive algorithm which depends on it.
Instead we show that the prover is accurate by de-
scribing the parameters that guide search in gener-
ation and explaining why they can be exhaustively
tested (see Section 5).
9 Conclusion
In this paper we presented a tactical generator which
exploits a simplifying assumption that the output
text will be restricted to controlled natural language
to enforce a restricted domain of locality on search
in generation. As a result, the generation process is
fast and scales linearly, and furthermore the system
is reliable, since we are able to perform a compile-
time check of the data sources which drive the as-
signment of syntactic subcategorisations to the ex-
pression of each node in the input semantic graph.
The generator is most appropriate for applications
which need to present small chunks of structured
data as text on demand and in high volume. For ex-
ample, information feeds such as local weather fore-
casts, traffic information, and tourist information or
technical information that must be both machine-
readable (for example because it is safety critical
and requires consistency checking) and also human-
readable (for example for an operator to make use of
it) such as machine operator instructions, business
process/protocol descriptions and medical orders.
47
References
F. Baader, D. Calvanese, D. L. Mcguinness, D. Nardi, and
P. F. Patel-Schneider, editors. 2003. The Description
Logic Handbook : Theory, Implementation and Appli-
cations. Cambridge University Press.
J. Bateman, R. Kasper, J. Moore, and R. Whitney. 1989.
A general organization of knowledge for natural lan-
guage processing: The Penman Upper Model. Techni-
cal report, Information Sciences Institute, Marina del
Rey, California.
T. Berners-Lee, J. Hendler, and O. Lassila. 2001. The
Semantic Web. Scientific American, 284(5):34?43.
M. Candito and S. Kahane. 1998. Can the TAG deriva-
tion tree represent a semantic graph? An answer in
the light of the Meaning-Text Theory. In Proceedings
of the Fourth Workshop on Tree-Adjoining Grammars
and Related Frameworks, Philadephia, USA.
R. Dale. 1989. Cooking up referring expressions. In
Proceedings of the 27th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 68 ? 75,
Vancouver, Canada.
C. Doran, D. Egedia, B. Hockey, B. Srinivas, and
M. Zaidel. 1994. XTAG system - a wide coverage
grammar for English. In Proceedings of the 15th In-
ternational Conference on Computational Linguistics,
pages 922?928, Kyoto, Japan.
M. Fowler. 2004. Inversion of control containers and the
dependency injection pattern
http://www.martinfowler.com/
articles/injection.html.
R. Grishman, C. McLeod, and A. Myers. 1994. Com-
lex Syntax: Building a Computational Lexicon. In
Proceedings of the The 15th International Conference
on Computational Linguistics, pages 268?272, Kyoto,
Japan.
D. Hardcastle and R. Power. 2008. Generating Concep-
tually Aligned Texts. Technical Report 2008/06, The
Open University, Milton Keynes, UK.
D. Hardcastle. 2007. Riddle posed by computer (6): The
Computer Generation of Cryptic Crossword Clues.
PhD thesis, University of London.
A. Hartley and C. Paris. 2001. Translation, controlled
languages, generation. In E. Steiner and C. Yallop,
editors, Exploring Translation and Multilingual Text
production, pages 307?325. Mouton de Gruyter.
A. Joshi. 1987. The relevance of tree adjoining gram-
mar to generation. In G. Kempen, editor, Natural Lan-
guage Generation: New Directions in Artificial Intel-
ligence, Psychology, and Linguistics. Kluwer.
O. Lassila and R. Swick. 1998. Resource Descrip-
tion Framework (RDF) model and syntax specifica-
tion. W3C Working Draft WD-rdf-syntax-19981008.
C. Paris, K. Vander Linden, M. Fischer, A. Hartley,
L. Pemberton, R. Power, and D. Scott. 1995. A sup-
port tool for writing multilingual instructions. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence, pages 1398?1404, Montreal,
Canada.
R. Power and D. Scott. 1998. Multilingual authoring
using feedback texts. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics
and 36th Annual Meeting of the Association for Com-
putational Linguistics, pages 1053?1059, Montreal,
Canada.
R. Power, D. Scott, and N. Bouayad-Agha. 2003.
Document structure. Computational Linguistics,
29(4):211?260.
48
