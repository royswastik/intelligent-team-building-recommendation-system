Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 460?463,
Prague, June 2007. c?2007 Association for Computational Linguistics
UTD-SRL: A Pipeline Architecture for Extracting Frame
Semantic Structures
Cosmin Adrian Bejan and Chris Hathaway
Human Language Technology Research Institute
The University of Texas at Dallas
Richardson, TX 75083-0688, USA
{ady,chris}@hlt.utdallas.edu
Abstract
This paper describes our system for the task
of extracting frame semantic structures in
SemEval?2007. The system architecture
uses two types of learning models in each
part of the task: Support Vector Machines
(SVM) and Maximum Entropy (ME). De-
signed as a pipeline of classifiers, the seman-
tic parsing system obtained competitive pre-
cision scores on the test data.
1 Introduction
The SemEval?2007 task for extracting frame se-
mantic structures relies on the human annotated
data available in the FrameNet (FN) database. The
Berkeley FrameNet project (Baker et al, 1998) is
an ongoing effort of building a semantic lexicon for
English based on the theory of frame semantics. In
frame semantics, the meaning of words or word ex-
pressions, also called target words (TW), comprises
aspects of conceptual structures, or frames, that de-
scribe specific situations. The semantic roles, or
frame elements (FE), associated with a target word
are locally defined in the frame evoked by the tar-
get word. Currently, the FN lexicon includes more
than 135,000 sentences extracted from the British
National Corpus containing more than 6,100 target
words that evoke more than 825 semantic frames.
For this task, we extended our previous work at
Senseval-3 (Bejan et al, 2004) by (1) experiment-
ing with additional features, (2) adding new classifi-
cation sub-tasks to accomplish all the requirements,
and (3) integrating these sub-tasks into a pipeline ar-
chitecture.
2 System Description
Given a sentence, the frame semantic structure ex-
traction task consists of recognizing the word ex-
pressions that evoke semantic frames, assigning the
correct frame to them and, for each target word,
detecting and labeling the corresponding frame el-
ements properly. The task also requires the de-
termination of syntactic realizations associated to a
frame element, such as grammatical function (GF)
and phrase type (PT). The following illustrates a
sentence example annotated with frame elements to-
gether with their corresponding grammatical func-
tions and phrase types for the target word ?tie?:
FE = Content2
GF = Dep
PT = PP
FE = Content1
GF = Ext
PT = NP
AEOI?s activities and facilities  have been  tied   to several universities .
Frame = Make_Cognitive_Connection
evokes
To extract semantic structures similar to those il-
lustrated in the example we divide the SemEval?
2007 task into four sub-tasks: (1) target word frame
disambiguation (TWFD); (2) FE boundary detection
(FEBD); (3) GF label classification (GFLC) and (4)
FE label classification (FELC). The sub-tasks TWFD
and GFLC are natural extensions of the approach de-
scribed in (Bejan et al, 2004) for the task of se-
mantic role labeling at Senseval-03. We design ma-
chine learning classifiers specific for each of the four
sub-tasks and arrange them in a pipeline architecture
such that a classifier can use information predicted
by its previous classifiers. The system architecture
is illustrated in Figure 1. In the data processing step,
we parse each sentence into a syntactic tree using the
Collins parser and extract named entities using an in
460
Target word listTest data
Named Entity
Recognizer
Test Data
Target word
Frame
FE Boundary
GF
SVM model ME model SVM model ME model SVM model ME modelSVM model ME model
ME trainSVM train
Feature Extractor
ME train
one multi?class classifier
SVM train
Feature Extractor
ME trainSVM train
Feature Extractor
one binary classifier
ME trainSVM train
Feature Extractor
556 multi?class classifiers 489 multi?class classifiers
Test Data
Train Data
FeatureFeature
Test Data
Target word
Frame
FeatureFeature
Test Data
Target word
Frame
FE Boundary
FN Annotation
FE Boundary
Predictor
Frame
Predictor Predictor
FE Label
Predictor
Extractor Extractor Extractor Extractor
GF Label
Syntactic
Parser
GF Label Classification FE Label ClassificationFE Boundary DetectionFrame Disambiguation
FrameNet
Lexicon
Data Processing
Figure 1: System architecture.
house implementation of a named entity recognizer.
We also extract from the FN lexicon mappings of
target words and the semantic frames they evoke.
Various features corresponding to constituents
were extracted and passed to SVM and ME clas-
sifiers. For example, in Figure 2, the frame dis-
activities
NNS
AEOI
NNP POS
?s
NP
and
CC
facilities
NNS
NP
JJ
several
Inhibit_movement
Rope_manipulation
Attaching
Closure
Activity_finish
Finish_competition
Immobilization
Make_cognitive_connection
Knot_creation
Forming_relationships
Frame Disambiguation
positive negative
FE Boundary Detection
Head
NULL
Obj
Quant
Appositive
Dep
Ext
Gen
GF Classification
VBP
have
VP
VP
VBN
VBN
been
S
tied NPto
PP
NNS
universities
VP Concept_1
Concept_2
Evidence
Cognizer
Concepts
Time
Place
Circumstances
Frequency
FE Classif.
Figure 2: Classification examples for each sub-task.
ambiguation sub-task extracts features correspond-
ing to the constituent tied in order to predict the
right frame between the semantic frames that can be
evoked by this target word. In this figure, the correct
categories for each sub-task are shown in boldface.
The complete set of features extracted for all the
classification sub-tasks is illustrated in Figure 3.
These represent a subset of features used in previ-
ous works (Gildea and Jurafsky, 2002; Florian et al,
2002; Surdeanu et al, 2003; Xue and Palmer, 2004;
Bejan et al, 2004; Pradhan et al, 2005) for auto-
matic semantic role labeling and word sense disam-
biguation. Figure 3 also indicates whether or not a
feature is selected for a specific classification task.
In the remaining part of this section we describe
in detail each classification sub-task and the features
that have the most salient effect on improving the
corresponding classifiers.
2.1 Frame Disambiguation
In FrameNet, some target words can evoke multiple
semantic frames. In order to extract the semantic
structure of an ambiguous target word, the first step
is to assign the correct frame to the target word in
a given context. This task is similar with the word
sense disambiguation task.
We select from the FN lexicon 556 target words
that evoke at least two semantic frames and have at
least five sentences annotated for each frame, and
assemble a multi-class classifier for each ambiguous
target word. As described in Figure 3, for this task
we extract features used in word sense disambigua-
tion (Florian et al, 2002), lexical features of the tar-
get word, and NAMED ENTITY FLAGS associated
with the root node in a syntactic parse tree. For
the rest of the ambiguous target words that have less
than five sentences annotated we randomly choose a
frame as being the correct frame in a given context.
2.2 Frame Element Identification
The idea of splitting the automatic semantic role la-
beling task into FE boundary detection and FE label
classification was first proposed in (Gildea and Ju-
rafsky, 2002) and then adopted by other works in
this task. The problem of detecting the FE bound-
aries is cast as the problem of deciding whether or
not a constituent is a valid candidate for a FE.
461
TW
FD
G
FL
C
Feature DescriptionNO NO Feature Description
TW
FD
G
FL
C
FE
BD
FE
BD
FE
LC
FE
LC
CW: The content word of the constituent computed as described in
(Surdeanu et al, 2003);
v20
CW POS: The POS corresponding to the content word;v21
CW STEM: Stemmed content word;v22
GOVERNING CATEGORY: Test whether the noun phrase constituents arevv23
dominated by verbal phrases or sentence phrases;
SYNTACTIC DISTANCE: The length of the syntactic path;v24
PP FIRST WORD: If the constituent is a prepositional phrase, return the first
word in the phrase;
v25
HUMAN: Test whether the constituent phrase is either a personal pronoun
or a hyponym of first sense of PERSON synset in WordNet;
v26
CONSTITUENTS NUMBER: The number of candidate FEs;v27
CONSTITUENTS LIST: Constituents labels list of the candidate FEs;v28
SAME CLAUSE: Test whether the constituent is in the same clause withv29
the target word;
GF: The grammatical function of a candidate frame element;v30
GF LIST: The list of grammatical functions associated to the candidate FEs;v31
FRAME: The name of the semantic frame that is evoked by the target word;vvv32
NP SISTER: Determine whether the constituent has a noun phrase sister;v33
FIRST/LAST WORD: Return the first/last word of the constituent phrase;v34
FIRST/LAST POS: Return the first/last POS in the constituent;vv35
LEFT/RIGHT SISTER LABEL: Return the left/right sibling constituent label;v36
LEFT/RIGHT SISTER HEAD: Return the left/right sibling head word;v37
LEFT/RIGHT SISTER STEM HEAD: Return the left/right sibling stemmedv38
head word;
LEFT/RIGHT SISTER POS HEAD: Return the left/right sibling head POS;v39
HW POS: The syntactic head POS of the constituent;
HW STEM: The stem word of the constituent?s head word;
v v
v
18
19
TW STEM & HW STEM: Join of TW STEM and HW STEM;
TW STEM & PHRASE TYPE: Join of TW STEM and PHRASE TYPE;
v
v
40
41
VOICE & POSITION: Join of VOICE and POSITION.v42
TW UNIGRAMS: The words, stem words and part of speech (POS) unigramsv01
that are adjacent to target word expressions;
TW BIGRAMS: The words, stem words and POS bigrams that are adjacent to02
target word expressions;
TW WORD: The target word expression;03
TW STEM: The stem word(s) of the target word expression;v v04
v
TW POS: The POS of the target word;v
TW CLASS: The lexical class of the target word, e.g. verb, noun, adjective;vv06
05
NAMED ENTITY FLAGS: Set of binary features indicating whether a consti?vv07
tuent contains, is contained or exactly identifies a named entity;
VERB WSD: If the target word is a verb, extract the head noun of the direct
object and the prepositional object included in the verbal phrase;
v08
v
NOUN WSD: If the target word is a noun, extract the head word of the verbal
phrase that is in a verb?subject or verb?object relation with the noun;
09 v
ADJECTIVE WSD: If the target word is an adjective, extract the head noun
that is modified by the adjective;
10 v
PHRASE TYPE: The syntactic category of the constituent;vv11
DIRECTED PATH: Path in the syntactic parse tree between the constituent
and the target word preserving the movement direction;
vvv12
UNDIRECTED PATH: Same syntactic path as DIRECTED PATH without13 v
preserving the movement direction;
PARTIAL PATH: Path from the constituent to the earlier common ancestor of
the target word and the constituent;
v14
POSITION: Test whether the constituent contains the target word, or appears
before or after the target word;
vv v15
VOICE: Test if the verbal target word has active or passive construction;vv16
HW: The head word of the constituent;v vv17
Figure 3: Feature set for extracting frame semantic structures.
We consider a binary classifier over the entire FN
data and extract features for each constituent from a
syntactic parse tree. Because this experimental setup
allows training the binary classifier on a large set of
examples, the best feature combination consists of
a restrained number of features. Most of these fea-
tures are from the set proposed by (Gildea and Juraf-
sky, 2002). Another feature that improved the pre-
diction of FE boundaries in every feature selection
experiment is the FRAME feature. Since the frame
disambiguation is executed before the FE boundary
detection in the pipeline architecture, we can use the
FRAME feature at this step. This feature helps the
binary classifier distinguish between frame element
structures from different semantic frames.
2.3 Grammatical Function Classification
Once we identify the candidate boundaries for frame
elements, the next step is to assign the grammat-
ical functions to these boundaries. In FrameNet,
the grammatical functions represent the manner in
which the frame elements satisfy grammatical con-
straints with respect to the target word.
For this task we train a multi-class classifier over
the entire lexicon to predict seven categories of GFs
that exist in FN. In addition, we assign the NULL
category for those FEs that double as target words.
The features are extracted only for the constituents
that are identified as FEs in the previous FE bound-
ary identification sub-task. The best feature set in
this phase includes the features proposed by (Gildea
and Jurafsky, 2002) and the FRAME feature.
2.4 Frame Element Classification
The task of FE classification is to assign FE labels to
every constituent identified as FE. In order to predict
the frame elements, which are locally defined for
each semantic frame, we built 489 multi-class clas-
sifiers, where each classifier corresponds to a frame
in FrameNet. This partitioning of the FN lexicon has
the advantage of increasing the overall classification
performance and efficiently learning the frame ele-
ments labels. On the other hand, this approach suf-
fers from the lack of annotated data in some frames
and hence it requires using a large set of features.
The advantage of designing the classifiers in a
pipeline architecture is best illustrated in this sub-
task. Some of the most effective features for FE
classification are extracted using information from
previous sub-tasks: FRAME feature is made avail-
able by the TWFD sub-task, CONSTITUENTS NUM-
BER and CONSTITUENTS LIST are made available
by the FEBD sub-task, and GF and GF LIST are
made available by the GFLC sub-task.
462
3 Experimental Results
We report experimental results on all four classi-
fication sub-tasks. In our experiments we trained
two types of classification models for each sub-task:
SVM and ME. In order to optimize the performance
measure of each sub-task and to find the best config-
uration of classification models we used 20% of the
sub-tasks training data as validation data. Table 1
lists the best configuration of classification models
as well as the best sub-task results when running
the experiments on the validation data. For frame
disambiguation, we obtained 76.71% accuracy com-
pared to a baseline of 60.72% accuracy that always
predicts the most annotated frame for each of the
556 target words. The results for GFLC and FELC
sub-tasks listed in Table 1 were achieved by using
gold FE boundaries.
Frame Disambiguation
GF Label Classification
FE Label Classification
FE Boundary Detection
Task
SVM
SVM
76.71
Best Model
96.00
88.93
ME
ME
Accuracy
F1?measureRecall
73.65
Precision
87.08 79.80
Table 1: Task results on the validation set.
The SemEval?2007 organizers provided fully an-
notated training files, a scorer to evaluate these
training files, and testing files containing flat sen-
tences. In the evaluation process, a semantic depen-
dency graph corresponding to a fully system anno-
tated sentence is created and then matched with its
gold dependency graph. The matching process not
only evaluates every semantic structure of a target
word, but also considers frame-to-frame and FE-to-
FE graph relations between the semantic structures.
In addition, various scoring options were consid-
ered: exact or partial frame matching, partial credit
for evaluating the named entities, evaluation of the
flat frame elements labels, and an option for match-
ing only the frames in evaluation. The evaluation for
flat frame elements labels is similar with the evalu-
ation performed at Senseval-3. The only difference
is that for this scorer the FE boundaries must match
exactly.
In Table 2, we present the averaged precision,
recall and F1 measures for evaluating the seman-
tic dependency graphs and detecting the semantic
frames on the testing files. The ?Options? col-
umn represents the configuration parameters of the
scorer: (E)xact/(P)artial frame matching, seman-
tic (D)ependency or (L)abels only evaluation, and
(Y)es/(N)o named entity evaluation.
E D N
P D N
E L Y
P L Y
E D Y
P D Y
E L N
P L N
Semantic Dependency Evaluation
F1?measureRecallPrecision
51.10
50.29
54.78
51.85
51.38
56.13
55.56
56.59
27.74
27.05
29.48
27.59
26.95
29.45
30.19
30.14
35.88
35.11
38.26
35.94
35.29
38.57
39.04
39.25
69.16
71.69
80.35
69.16
71.69
80.35
77.82
77.82
42.73
44.43
49.79
42.73
44.43
49.79
48.09
48.09
52.71
54.74
61.35
52.71
54.74
61.35
59.32
59.32
Precision Recall F1?measure
Options Frame Detection Evaluation
Table 2: System results on the test set.
Although the system achieved good precision
scores on the test data, the recall values caused the
system to obtain unsatisfactory F1-measure values.
We expect that the recall will increase by consid-
ering various heuristics for a better mapping of the
frame elements to constituents in parse trees.
4 Conclusions
We described a system that participated in SemEval?
2007 for the task of extracting frame semantic struc-
tures. We showed that a pipeline architecture of the
SVM and ME classifiers as well as an adequate se-
lection of the classification models can improve the
performance measures of each sub-task.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998.
The Berkeley FrameNet project. In Proceedings of the
COLING-ACL, Montreal, Canada.
Cosmin Adrian Bejan, Alessandro Moschitti, Paul Mora?rescu,
Gabriel Nicolae, and Sanda Harabagiu. 2004. Semantic
Parsing Based on FrameNet. In Senseval-3: Workshop on
the Evaluation of Systems for the Semantic Analysis of Text.
Radu Florian, Silviu Cucerzan, Charles Schafer, and David
Yarowsky. 2002. Combining classifiers for word sense dis-
ambiguation. Natural Language Engineering.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling
of Semantic Roles. Computational Linguistic.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Support vec-
tor learning for semantic argument classification. Journal of
Machine Learning Research.
Mihai Surdeanu, Sanda M. Harabagiu, John Williams, and Paul
Aarseth. 2003. Using predicate-argument structures for in-
formation extraction. In Proceedings of ACL.
Nianwen Xue and Marta Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of EMNLP.
463
