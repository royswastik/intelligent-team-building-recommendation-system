Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 581?589, Prague, June 2007. c?2007 Association for Computational Linguistics
Lexical Semantic Relatedness with Random Graph Walks
Thad Hughes and Daniel Ramage
Computer Science Department
Stanford University
Stanford, CA 94305
{thughes, dramage}@cs.stanford.edu
Abstract
Many systems for tasks such as question answer-
ing, multi-document summarization, and infor-
mation retrieval need robust numerical measures
of lexical relatedness. Standard thesaurus-based
measures of word pair similarity are based on
only a single path between those words in the
thesaurus graph. By contrast, we propose a new
model of lexical semantic relatedness that incor-
porates information from every explicit or im-
plicit path connecting the two words in the en-
tire graph. Our model uses a random walk over
nodes and edges derived from WordNet links
and corpus statistics. We treat the graph as a
Markov chain and compute a word-specific sta-
tionary distribution via a generalized PageRank
algorithm. Semantic relatedness of a word pair is
scored by a novel divergence measure, ZKL, that
outperforms existing measures on certain classes
of distributions. In our experiments, the resulting
relatedness measure is the WordNet-based mea-
sure most highly correlated with human similar-
ity judgments by rank ordering at ? = .90.
1 Introduction
Several kinds of Natural Language Processing systems
need measures of semantic relatedness for arbitrary word
pairs. For example, document summarization and ques-
tion answering systems often use similarity scores to
evaluate candidate sentence alignments, and information
retrieval systems use relatedness scores for query expan-
sion. Several popular algorithms calculate scores from
information contained in WordNet (Fellbaum, 1998), an
electronic dictionary where word senses are explicitly
connected by zero or more semantic relationships. The
central challenge of these algorithms is to compute rea-
sonable relatedness scores for arbitrary word pairs given
that few pairs are directly connected.
Most pairs in WordNet share no direct semantic
link, and for some the shortest connecting path can be
surprising?even pairs that seem intuitively related, such
?furnace? and ?stove? share a lowest common ancestor
in the hypernymy taxonomy (is-a links) all the way up
at ?artifact? (a man-made object). Several existing algo-
rithms compute relatedness only by traversing the hyper-
nymy taxonomy and find that ?furnace? and ?stove? are
relatively unrelated. However, WordNet provides other
types of semantic links in addition to hypernymy, such
as meronymy (part/whole relationships), antonymy, and
verb entailment, as well as implicit links defined by over-
lap in the text of definitional glosses. These links can
provide valuable relatedness information. If we assume
that relatedness is transitive across a wide variety of such
links, then it is natural to follow paths such as furnace?
crematory?gas oven?oven?kitchen appliance?stove and
find a higher degree of relatedness between ?furnace?
and ?stove.?
This paper presents the application of random walk
Markov chain theory to measuring lexical semantic re-
latedness. A graph of words and concepts is constructed
from WordNet. The random walk model posits the exis-
tence of a particle that roams this graph by stochastically
following local semantic relational links. The particle is
biased toward exploring the neighborhood around a target
word, and is allowed to roam until the proportion of time
it visits each node in the limit converges to a stationary
distribution. In this way we can compute distinct, word-
specific probability distributions over how often a particle
visits all other nodes in the graph when ?starting? from a
specific word. We compute the relatedness of two words
as the similarity of their stationary distributions.
The random walk brings with it two distinct advan-
tages. First, it enables the similarity measure to have
a principled means of combination of multiple types of
edges from WordNet. Second, by traversing all links, the
walk aggregates local similarity statistics across the en-
tire graph. The similarity scores produced by our method
are, to our knowledge, the WordNet-based scores most
highly correlated with human judgments.
581
2 Related work
Budanitsky and Hirst (2006) provide a survey of many
WordNet-based measures of lexical similarity based on
paths in the hypernym taxonomy. As an example, one
of the best performing is the measure proposed by Jiang
and Conrath (1997) (similar to the one proposed by (Lin,
1991)), which finds the shortest path in the taxonomic hi-
erarchy between two candidate words before computing
similarity as a function of the information content of the
two words and their lowest common subsumer in the hi-
erarchy. We note the distinction between word similarity
and word relatedness. Similarity is a special case of relat-
edness in that related words such as ?cat? and ?fur? share
some semantic relationships (such as meronymy), but do
not express the same likeness of form as would similar
words such as ?cat? and ?lion.? The Jiang-Conrath mea-
sure and most other measures that primarily make use of
of hypernymy (is-a links) in the WordNet graph are better
categorized as measures of similarity than of relatedness.
Other measures have been proposed that utilize the text
in WordNet?s definitional glosses, such as Extended Lesk
(Banerjee and Pedersen, 2003) and later the Gloss Vec-
tors (Patwardhan and Pedersen, 2006) method. These ap-
proaches are primarily based on comparing the ?bag of
words? of two synsets? gloss text concatenated with the
text of neighboring words? glosses in the taxonomy. As
a result, these gloss-based methods measure relatedness.
Our model captures some of this relatedness information
by including weighted links based on gloss text.
A variety of other measures of semantic relatedness
have been proposed, including distributional similarity
measures based on co-occurrence in a body of text?
see (Weeds and Weir, 2005) for a survey. Other mea-
sures make use of alternative structured information re-
sources than WordNet, such as Roget?s thesaurus (Jar-
masz and Szpakowicz, 2003). More recently, measures
incorporating information from Wikipedia (Gabrilovich
and Markovitch, 2007; Strube and Ponzetto, 2006) have
reported stronger results on some tasks than have been
achieved by existing measures based on shallower lexical
resources. The results of our algorithm are competitive
with some Wikipedia algorithms while using only Word-
Net 2.1 as the underlying lexical resource. The approach
presented here is generalizable to construction from any
underlying semantic resource.
PageRank is the most well-known example of a ran-
dom walk Markov chain?see (Berkhin, 2005) for a sur-
vey. It uses the local hyperlink structure of the web to
define a graph which it walks to aggregate popularity
information for different pages. Recent work has ap-
plied random walks to NLP tasks such as PP attachment
(Toutanova et al, 2004), word sense disambiguation (Mi-
halcea, 2005; Tarau et al, 2005), and query expansion
(Collins-Thompson and Callan, 2005). However, to our
knowledge, the literature in NLP has only considered us-
ing one stationary distribution per specially-constructed
graph as a probability estimator. In this paper, we in-
troduce a measure of semantic relatedness based on the
divergence of the distinct stationary distributions result-
ing from random walks centered at different positions in
the word graph. We believe we are the first to define such
a measure.
3 Random walks on WordNet
Our model is based on a random walk of a particle
through a simple directed graphG = (V,E)whose nodes
V and edges E are extracted from WordNet version 2.1.
Formally, we define the probability n(t)i of finding the
particle at node ni ? V at time t as the sum of all ways in
which the particle could have reached ni from any other
node at the previous time-step:
n(t)i =
?
nj?V
n(t?1)j P (ni | nj)
where P (ni | nj) is the conditional probability of mov-
ing to ni given that the particle is at nj . In partic-
ular, we construct the transition distribution such that
P (ni | nj) > 0 whenever WordNet specifies a local link
relationship of the form j ? i. Note that this random
walk is a Markov chain because the transition probabili-
ties at time t are independent of the particle?s past trajec-
tory.
The subsections that follow present the construction of
the graph for our random walk from WordNet and the
mathematics of computing the stationary distribution for
a given word.
3.1 Graph Construction
WordNet is itself a graph over synsets. A synset is best
thought of as a concept evoked by one sense of one or
more words. For instance, different senses of the word
?bank? take part in different synsets (e.g. a river bank
versus a financial institution), and a single synset can
be represented by multiple synonymous words, such as
?middle? and ?center.? WordNet explicitly marks seman-
tic relationships between synsets, but we are additionally
interested in representing relatedness between words. We
therefore extract the following types of nodes fromWord-
Net:
Synset Each WordNet synset has a corresponding node.
For example, one node corresponds to the synset re-
ferred to by ?dog#n#3,? the third sense of dog as
noun, whose meaning is ?an informal term for a
man.? There are 117,597 Synset nodes.
582
TokenPOS One node is allocated to every word cou-
pled with a part of speech, such as ?dog#n? mean-
ing dog as a noun. These nodes link to all the
synsets they participate in, so that ?dog#n? links
the Synset nodes for canine, hound, hot dog, etc.
Collocations?multi-word expressions such as ?hot
dog??that take part in a synsets are also represented
by these nodes. There are 156,588 TokenPOS nodes.
Token Every TokenPOS is connected to a Token node
corresponding to the word when no part of speech
information is present. For example, ?dog? links to
?dog#n? and ?dog#v? (meaning ?to chase?). There
are 148,646 Token nodes.
Synset nodes are connected with edges correspond-
ing to many of the relationship types in Word-
Net. We use these WordNet relationships to form
edges: hypernym/hyponym, instance/instance of, all
holonym/meronym links, antonym, entails/entailed by,
adjective satellite, causes/caused by, participle, pertains
to, derives/derived from, attribute/has attribute, and top-
ical (but not regional or usage) domain links. By con-
struction, each edge created from a WordNet relationship
is guaranteed to have a corresponding edge in the oppo-
site direction.
Edges that connect a TokenPOS to the Synsets using it
are weighted based on a Bayesian estimate drawn from
the SemCor frequency counts included in WordNet but
with a non-uniform Dirichlet prior. Our edge weights are
the SemCor frequency counts for each target Synset, with
pseudo-counts of .1 for all Synsets, 1 for first sense of
each word, and .1 for the first word in each Synset. Intu-
itively, this causes the particle to have a higher probabil-
ity of moving to more common senses of a TokenPOS; for
example, the edges from ?dog#n? to ?dog#n#1? (canine)
and ?dog#n#5? (hotdog) have un-normalized weights of
43.2 and 0.1, respectively. The edges connecting a To-
ken to the TokenPOS nodes in which it can occur are also
weighted by the sum of the weights of the outgoing To-
kenPOS?Synset links. Hence a walk starting at a com-
mon word like ?cat? is far more likely to follow a link to
?cat#n? than to rarities like ?cat#v? (to vomit). These
edges are uni-directional; no edges are created from a
Synset to a TokenPOS that can represent the Synset.
In order for our graph construction to incorporate
textual gloss-based information, we also create uni-
directional edges from Synset nodes to the TokenPOS
nodes for the words and collocations used in that synset?s
gloss definition. This requires part-of-speech tagging the
glosses, for which we use the Stanford maximum entropy
tagger (Toutanova et al, 2003). It is important to cor-
rectly weight these edges, because high-frequency stop-
words such as ?by? and ?he? do not convey much in-
formation and might serve only to smear the probability
mass across the whole graph. Gloss-based links to these
nodes should therefore be down-weighted or removed.
On the other hand, up-weighting extremely rare words
such as by tf-idf scoring might also be inappropriate
because such rare words would get extremely high scores,
which is an undesirable trait in similarity search. (Haveli-
wala et al, 2002) and others have shown that a ?non-
monotonic document frequency? (NMDF) weighting can
be more effective in such a setting. Because the frequency
of words in the glosses is distributed by a power-law, we
weight each word by its distance from the mean word
count in log space. Formally, the weight wi for a word
appearing ri times is
wi = exp
(
?
(log(ri)? ?)2
2?2
)
where ? and ? are the mean and standard deviation of
the logs of all word counts. This is a smooth approxima-
tion to the high and low frequency stop lists used effec-
tively by other measures such as (Patwardhan and Ped-
ersen, 2006). We believe that because non-monotonic
frequency scaling has no parameters and is data-driven,
it could stand to be more widely adopted among gloss-
based lexical similarity measures.
We also add bi-directional edges between Synsets
whose word senses overlap with a common TokenPOS.
These edges have raw weights given by the number of
TokenPOS nodes shared by the Synsets. The intuition be-
hind adding these edges is that WordNet often divides the
meanings of words into fine-grained senses with similar
meanings, so there is likely to be some semantic relation-
ship between Synsets sharing a common TokenPOS.
The final graph has 422,831 nodes and 5,133,281
edges. This graph is very sparse; fewer than 1 in 10,000
node pairs are directly connected. When only the un-
weighted WordNet relationship edges are considered,
the largest degree of any node is ?city#n#1? with 667
edges (mostly connecting to particular cities), followed
by ?law#n#2? with 602 edges (mostly connecting to a
large number of domain terms such as ?dissenting opin-
ion? and ?freedom of speech?), and each node is on aver-
age connected to 1.7 other nodes. When the gloss-based
edges are considered separately, the highest degree nodes
are those with the longest definitions; the maximum out-
degree is 56 and the average out-degree is 6.2. For the
edges linking TokenPOS nodes to the Synsets in which
they participate, TokenPOS nodes with many senses are
the most connected; ?break#v? with 59 outgoing edges
and ?make#v? with 49 outgoing edges have the highest
out-degrees, with the average out-degree being 1.3.
3.2 Computing the stationary distribution
Each of the K edge types presented above can be repre-
sented as separate transition matrix Ek ? RN?N where
583
N is the total number of nodes. For each matrix, col-
umn j contains contains a normalized outgoing proba-
bility distribution,1 so the weight in cell (i, j) contains
PK(ni | nj), the conditional probability of moving from
node nj to node ni in edge type K. For many of the edge
types, this is either 0 or 1, but for the weighted edges,
these are real valued. The full transition matrixM is then
the column normalized sum of all of the edge types:
M? =
?
k
Ek
M =
(?
?
?M?
?
?
?
?
)?1
? M?
M is a distillation of relevant relatedness information
about all nodes extracted from WordNet and is not tai-
lored for computing a stationary distribution for any spe-
cific word. In order to compute the stationary distribu-
tion vdog#n for a walk centered around the TokenPOS
?dog#n,? we first define an initial distribution v(0)dog#n that
places all the probability mass in the single vector entry
corresponding to ?dog#n.? Then at every step of the walk,
we will return to v(0) with probability ?. Intuitively, this
return probability captures the notion that nodes close to
?dog#n? should be given higher weight, and also guaran-
tees that the stationary distribution exists and is unique
(Bremaud, 1999). The stationary distribution v is com-
puted via an iterative update algorithm:
v(t) = ?v(0) + (1? ?)Mv(t?1)
Because the walk may return to the initial distribution
v(0) at any step with probability ?, we found that v(t)
converges to its unique stationary distribution v(?) in a
number of steps roughly proportional to ??1. We experi-
mented with a range of return probabilities and found that
our results were relatively insensitive to this parameter.
Our convergence criteria was
?
?v(t?1) ? v(t)
?
?
1
< 10?10,
which, for our graph with a return probability of ? = .1,
was met after about two dozen iterations. This computa-
tion takes under two seconds on a modern desktop ma-
chine.
Note that because M is sparse, each iteration of the
above computation is linear in the total number of non-
zero entries in P , i.e. linear in the total number of edges.
Introducing an edge type that is dense would dramatically
increase running time.
3.3 Model variants
For this paper, we consider three model variants that dif-
fer based on which subset of the edge types are included
1The frequency-count derived edges are normalized by the
largest column sum. This effectively preserves relative term fre-
quency information across the graph and causes some columns
to sum to less than one. We interpret this lost mass as a link to
?nowhere.?
in the transition matrix M .
MarkovLink This variant includes the explicit WordNet
relations such as hypernymy and the edges repre-
senting overlap between the TokenPOS nodes con-
tained in Synsets. A particle walking through this
graph reaches only Synset nodes and can step from
one Synset to another whenever WordNet specifies a
relationship between the Synsets or when the Synsets
share a common word. There is a single connected
component in this model variant. This model is
loosely analogous to a smoothed version of the path-
based WordNet measures surveyed in (Budanitsky
and Hirst, 2006) but differs in that it integrates mul-
tiple link types and aggregates relatedness informa-
tion across all paths in the graph.
MarkovGloss This variant includes only the weighted
uni-directional edges linking Synsets to the Token-
POS nodes contained in their gloss definitions, and
the edges from a TokenPOS node to the Synsets con-
taining it. The intuition behind this model variant is
that the particle can move as if it were recursively
looking up words in a dictionary, stepping from
Synsets to the Synsets used to define them. Because
WordNet?s gloss definitions are not sense-tagged,
the particle must make an intermediate step to a To-
kenPOS contained in the gloss definition and then
to a Synset representing a particular sense of that
TokenPOS. The availability of sense-tagged glosses
would eliminate the noise introduced by this inter-
mediate step. The particle can reach both Synsets
and TokenPOS nodes in this variant, but some parts
of the graph are not reachable from other parts. This
model incorporates much of the same information
as the gloss-based WordNet measures (Banerjee and
Pedersen, 2003; Patwardhan and Pedersen, 2006)
but differs in that it considers many more glosses
than just those in the immediate neighborhoods of
the candidate words.
MarkovJoined This variant is the natural combination
of the above two; we construct the graph containing
WordNet relation edges, Synset overlap edges, and
gloss-based Synset to TokenPOS edges.
Many of the characteristics of the model variants can
be understood in terms of how much probability mass
they assign to each node for a particular word-specific
stationary distribution. Table 1 shows the highest scoring
nodes in the word-specific stationary distributions cen-
tered around the Token node for ?wizard,? as computed by
the MarkovLink and MarkovGloss variants. In both vari-
ants, the ?wizard? Token?s only neighbors are the ?wiz-
ard#n? and ?wizard#a? TokenPOS nodes, and ?wizard#n?
584
MarkovLink MarkovGloss
Node Probability Node Probability
wizard 1.0E-1 wizard 1.3E-01
wizard#n 2.5E-3 wizard#n 2.9E-02
wizard#a 7.8E-5 wizard#a 9.1E-04
ace#n#3 4.2E-5 ace#n#3 1.1E-06
sorcerer#n#1 2.2E-6 sorcerer#n#1 5.8E-07
charming#a#2 2.2E-6 dazzlingly#r 2.4E-08
expert#n#1 1.1E-6 charming#a#2 1.6E-09
track star#n#1 1.1E-6 sorcery#n 2.6E-10
occultist#n#1 5.7E-7 magic#n 6.8E-12
Cagliostro#n#1 5.7E-7 magic#a 6.8E-12
star#v#2 5.5E-7 dazzlingly#r#1 4.3E-14
breeze_through#v#1 5.4E-7 dazzle#n 9.4E-16
magic#n#1 2.1E-8 beholder#n 9.4E-16
sorcery#n#1 2.1E-7 dazzle#v 9.4E-16
magician#n#1 1.9E-7 magic#n#1 5.1E-16
Table 1: Highest scoring nodes in the stationary distri-
butions for ?wizard#n? as generated by the MarkovLink
model and the MarkovGloss model with return probabil-
ity 0.1.
has a higher probability mass because of its higher Sem-
Cor usage counts. Likewise, the only possible steps per-
mitted in either variant from ?wizard#n? and ?wizard#a?
are to the Synsets that can be expressed with those nodes:
?ace#n#3,? ?sorcerer#n#1,? and ?charming#a#1.? Again,
the amount of mass given to these nodes depends on the
strength of these edge weights, which is determined by
the SemCor usage counts.
The highest probability nodes in the table are common
because both model variants share the same initial links.
However, the orders of the remaining nodes in the station-
ary distributions are different. In the MarkovLink variant,
the random walk can only proceed to other Synsets using
WordNet relationship edges; ?track star#n#1? and ?ex-
pert#n#1? are first reached by following hyponym and
hypernym edges from ?ace#n#1,? and ?occultist#n#1?
and ?Cagliostro#n#1? are first reached with hypernym
and instance edges from ?sorcerer#n#1.? The node
?breeze through#v#1? is reached through a path follow-
ing derivational links with ?ace#n? and ?ace#v.?
The MarkovGloss variant in table 1 shows how infor-
mation can be extracted solely from the textual glosses.
Once the random walk reaches the first Synset nodes, it
can step to the TokenPOS nodes in their glosses; for ex-
ample, ?ace#n#1? has the gloss ?someone who is daz-
zlingly skilled in any field.? Links to TokenPOS nodes
that are very common in glosses are down-weighted with
NMDF weighting, so ?someone#n? receives little mass
while ?dazzlingly#r? receives more. From there, the
random walk can step to another Synset such as ?daz-
MarkovLink model MarkovGloss model
Figure 1: Example stationary distributions plotted against
each other for similar (top) and dissimilar (bottom) word
pairs, using the MarkovLink (left) and MarkovGloss
(right) model variants.
zlingly#r#1,? and then on to other TokenPOS nodes used
in its definition: ?in a manner or to a degree that dazzles
the beholder.?
Figure 1 demonstrates how two word-specific station-
ary distributions are more highly correlated if the words
are related. In both model variants, random walks for
related words are more likely to visit the same parts of
the graph, and so assign higher probability to the same
nodes. Figure 1 also shows that the MarkovGloss variant
produces distributions with a much wider range of proba-
bilities than the MarkovLink, which might be a source of
difficulty in integrating the two model variants.
Figure 2 shows the correlation between the stationary
distributions produced by the two model variants for the
same word. The log-log scale makes it possible to see the
entire range of probabilities on the same axes, and shows
that distributions produced by these two model variants
share many of the same highest-probability words.
A noteworthy property of the constructed graphs is that
word relatedness can be computed directly by compar-
ing walks that start at Token nodes. By contrast, existing
WordNet-based measures require independent similarity
judgments for all word senses relevant to a target word
pair (of which the maximum relatedness value is usu-
ally taken). Our algorithm lends itself to comparisons
between walks centered at a Synset node, or a Token-
POS node, or a Token node, or any mixed distribution
thereof. And because the Synset nodes are strongly con-
nected, the model also admits direct comparison across
parts of speech.
585
Figure 2: Correlation of the stationary distributions for
?wizard#n,? produced by the MarkovLink variant (x-axis)
and the MarkovGloss variant (y-axis).
4 Similarity judgments
We have shown how to compute the word-specific sta-
tionary distribution from any starting distribution in the
graph. Now consider the task of deciding similarity be-
tween two words. Intuitively, if the random walk starting
at the first word?s node and the random walk starting at
the second word?s node tend to visit the same nodes, we
would like to consider them semantically related. For-
mally, we measure the divergence of their respective sta-
tionary distributions, p and q.
A wide literature exists on similarity measures between
probability distributions. One standard choice is to con-
sider p and q to be vectors and measure the cosine of
the angle between them, which is rank equivalent to Eu-
clidean distance.
simcos(p, q) =
?
i piqi
?p? ?q?
Because p and q are probability distributions, we would
also expect a strong contender from the information-
theoretic measures based on Kullback-Leibler diver-
gence, defined as:
DKL(p ? q) =
?
i
pi log
pi
qi
Unfortunately, KL divergence is undefined if any qi is
zero because those terms in the sum will have infinite
weight. Several modifications to avoid this issue have
been proposed in the literature. One is Jensen-Shannon
divergence (Lin, 1991), a symmetric measure based on
KL-divergence defined as the average of the KL diver-
gences of each distribution to their average distribution.
Jensen-Shannon is well defined for all distributions be-
cause the average of pi and qi is non-zero whenever either
number is.
These measures and others are surveyed in (Lee,
2001), who finds that Jensen-Shannon is outperformed
by the Skew divergence measure introduced by Lee in
(1999). The skew divergence2 accounts for zeros in q by
mixing in a small amount of p.
s?(p, q) = D(p ? ?q + (1? ?)p)
=
?
i pi log
pi
?qi+(1??)pi
Lee found that as ? ? 1, the performance of skew di-
vergence on natural language tasks improves. In partic-
ular, it outperforms most other models and even beats
pure KL divergence modified to avoid zeros with sophis-
ticated smoothing models. In exploring the performance
of divergence measures on our model?s stationary distri-
butions, we observed the same phenomenon. Note that
in the limit as ? ? 1, alpha skew is identically KL-
divergence.
4.1 Zero-KL Divergence
In this section we introduce a novel measure of distribu-
tional divergence based on a reinterpretation of the skew
divergence. Skew divergence avoids zeros in q by mixing
in some of p, but its performance on many natural lan-
guage tasks improves as it better approximates KL diver-
gence. We propose an alternative approximation to KL
divergence called Zero-KL divergence, or ZKL. When
qi is non-zero, we use exactly the term from KL diver-
gence. When qi = 0, we have a problem?in the limit as
? ? 1, the corresponding term approaches infinity. We
let ZKL use the Skew divergence value for these terms:
pi log
pi
?qi+(1??)pi
. Because qi = 0 this simplifies to
pi log
pi
(1??)pi
= pi log 11?? .
Lee showed skew divergence?s best performance was
for ? near to 1, so we formalize this intuition by choosing
? exponentially near to 1, i.e. we can choose our ? as
1?2?? for some ? ? R+. Zero terms in the sum can now
be written as pi log 12?? = pi log 2
? = pi ?. Note here an
analogy to the case with qj > 0 and where pj is exactly
one order of magnitude greater than qj , i.e. pj = 2 ? qj .
For such a term in the standard KL divergence, we would
get pj log
pj
qj
= pj log(2) = pj . Therefore, the ? term
in skew divergence implicitly defines a parameter stating
how many orders of magnitude smaller than pj to count
qj if qj = 0.
We define the Zero-KL divergence with respect to
2In Lee?s (1999) original presentation, skew divergence is
defined not as s?(p, q) but rather as s?(q, p). We reverse the ar-
gument order for consistency with the other measures discussed
here.
586
gamma:
ZKL?(p, q) =
?
i
pi
{
log piqi qi 6= 0
? qi = 0
Note that this is exactly KL-divergence when KL-
divergence is defined and, like skew divergence, approx-
imates KL divergence in the limit as ? ? ?.
A similar analysis of the skew divergence terms for
when 0 < qi  pi (and in particular with qi less than pi
by more than a factor of 2??) shows that such a term in
the skew divergence sum is again approximated by ? pi.
ZKL does not have this property. Because ZKL is a better
approximation to KL divergence and because they have
the same behavior in the limit, we expect ZKL?s perfor-
mance to dominate that of skew divergence in many dis-
tributions. However, if there is a wide range in the ex-
ponent of noisy terms, the maximum possible penalty to
such terms ascribed by skew divergence may be benefi-
cial.
Figure 3 shows the relative performance of ZKL versus
Jensen-Shannon, skew divergence, cosine similarity, and
the Jaccard score (a measure from information retrieval)
for correlations with human judgment on the MarkovLink
model. ZKL consistently outperforms the other measures
on distributions resulting from this model, but ZKL is not
optimal on distributions generated by our other models.
The next section explores this topic in more detail.
5 Evaluation
Traditionally, there have been two primary types of eval-
uation for measures of semantic relatedness: one is cor-
relation to human judgment, the other is the relative per-
formance gains of a task-driven system when it uses the
measure. The evaluation here focuses on correlation with
human judgments of relatedness. For consistency with
previous literature, we use rank correlation (Spearman?s
? coefficient) rather than linear correlation when compar-
ing sets of relatedness judgments because the rank corre-
lation captures information about the relative ordering of
the scores. However, it is worth noting that many applica-
tions that make use of lexical relatedness scores (e.g. as
features to a machine learning algorithm) would better be
served by scores on a linear scale with human judgments.
Rubenstein and Goodenough (1965) solicited human
judgments of semantic similarity for 65 pairs of com-
mon nouns on a scale of zero to four. Miller and Charles
(1991) repeated their experiment on a subset of 29 noun
pairs (out of 30 total) and found that although indi-
viduals varied among their judgments, in aggregate the
scores were highly correlated with those found by Ruben-
stein and Goodenough (at ? = .944 by our calculation).
Resnik (1999) replicated the Miller and Charles experi-
ment and reported that the average per-subject linear cor-
relation on the dataset was around r = 0.90, providing
a rough upper bound on any system?s linear correlation
performance with respect to the Miller and Charles data.
Figure 3 shows that the ZKL measure on the MarkovLink
model has linear correlation coefficient r = .903?at the
limit of human inter-annotator agreement.
Recently, a larger set of word relatedness judg-
ments was obtained by (Finkelstein et al, 2002) in the
WordSimilarity-353 (WS-353) collection. Despite the
collection?s name, the study instructed participants to
score word pairs for relatedness (on a scale of 0 to
10), which is in contrast to the similarity judgments re-
quested of the Miller and Charles (MC) and Rubenstein
and Goodenough (RG) participants. For this reason, the
WordSimilarity-353 data contains many pairs that are not
semantically similar but still receive high scores, such as
?computer-software? at 8.81. WS-353 contains pairs that
include non-nouns, such as ?eat-drink,? one proper noun
not appearing in WordNet (?Maradona-football?), and
some pairs potentially subject to political bias. Again,
the aggregate human judgments correlate well with ear-
lier data sets where they overlap?the 30 judgments that
WordSimilarity-353 shares with the Miller and Charles
data have ? = .939 and the 29 shared with Rubenstein
and Goodenough have ? = .904 (by our calculations).
We generated similarity scores for word pairs in all
three data sets using the three variants of our walk
model (MarkovLink, MarkovGloss, MarkovJoined) and
with multiple distributional distance measures. We used
the WordNet::Similarity package (Pedersen et al, 2004)
to compute baseline scores for several existing measures,
noting that one word pair was not processed in WS-353
because one of the words was missing from WordNet.
The results are summarized in Table 2. These num-
bers differ slightly from previously reported scores due to
variations in the exact experimental setup, WordNet ver-
sion, and the method of breaking ties when computing
? (here we break ties using the product-moment formu-
lation of Spearman?s rank correlation coefficient). It is
worth noting that in their experiments, (Patwardhan and
Pedersen, 2006) report that the Vector method has rank
correlation coefficients of .91 and .90 for MC and RG,
respectively, which are also top performing values.
In our experiments, the MarkovLink model with ZKL
distance measure was the best performing model over-
all. MarkovGloss and MarkovJoined were also strong
contenders but with the cosine measure instead of ZKL.
One reason for this distinction is that the stationary dis-
tributions resulting from the MarkovLink model are non-
zero for all but the initial word nodes (i.e. non-zero
for all Synset nodes). Consequently, ZKL?s re-estimate
for the zero terms adds little information. By contrast,
theMarkovGloss andMarkovJoined models include links
that traverse from Synset nodes to TokenPOS nodes, re-
587
Figure 3: Correlation with the Miller & Charles data sets by linear correlation (left) and rank correlation (right) for the
MarkovLink model. All data points were based on one set of stationary distributions over the graph; only the divergence
measure between those distributions is varied. Note that ZKL? dominates both graphs but skew divergence does well
for increasing ? (computed as 1? 2?). Gamma is swept over the range 0 to 1, then 1 through 20, then 20 through 40
at equal resolutions.
Model MC Rank RG Rank WS-353 Rank
MarkovLink (ZKL) .904 .817 .552
MarkovGloss (cosine) .841 .762 .467
MarkovJoined (cosine) .841 .838 .547
Gloss Vectors .888 .789 .445
Extended Lesk .869 .829 .511
Jiang-Conrath .653 .584 .195
Lin .625 .599 .216
Table 2: Spearman?s ? rank correlation coefficients with
human judgments using ? = 2.0 for ZKL. Note that fig-
ure 3 demonstrates ZKL?s insensitivity with regard to the
parameter setting for the MarkovLink model.
sulting in a final stationary distribution with more (and
more meaningful) zero/non-zero pairs. Hence the proper
setting of gamma (or alpha for skew divergence) is of
greater importance. ZKL?s performance improves with
tuning of gamma, but cosine similarity remained the more
robust measure for these distributions.
6 Conclusion
In this paper, we have introduced a new measure of
lexical relatedness based on the divergence of the sta-
tionary distributions computed from random walks over
graphs extracted WordNet. We have explored the struc-
tural properties of extracted semantic graphs and charac-
terized the distinctly different types of stationary distribu-
tions that result. We explored several distance measures
on these distributions, including ZKL, a novel variant of
KL-divergence. Our best relatedness measure is at the
limit of human inter-annotator agreement and is one of
the strongest measures of semantic relatedness that uses
only WordNet as its underlying lexical resource.
In future work, we hope to integrate other lexical re-
sources such as Wikipedia into the walk. Incorporat-
ing more types of links from more resources will un-
derline the importance of determining appropriate rela-
tive weights for all of the types of edges in the walk?s
matrix. Even for WordNet, we believe that certain link
types, such as antonyms, may be more or less appropriate
for certain tasks and should weighted accordingly. And
while our measure of lexical relatedness correlates well
with human judgments, we hope to show performance
gains in a real-word task from the use of our measure.
Acknowledgments
Thanks to Christopher D. Manning and Dan Jurafsky for
their helpful comments and suggestions. We are also
grateful to Siddharth Patwardhan and Ted Pedersen for
assistance in comparing against their system. Thanks to
Sushant Prakash, Rion Snow, and Varun Ganapathi for
their advice on pursuing some of the ideas in this pa-
per, and to our anonymous reviewers for their helpful cri-
tiques. Daniel Ramage was funded in part by an NDSEG
fellowship. This work was also supported in part by the
DTO AQUAINT Program, the DARPA GALE Program,
and the ONR (MURI award N000140510388).
588
References
S. Banerjee and T. Pedersen. 2003. Extended gloss over-
laps as a measure of semantic relatedness. In Proceed-
ings of the Eighteenth International Joint Conference
on Artificial Intelligence, Acapulco, pages 805?810.
P. Berkhin. 2005. A survey on pagerank computing. In-
ternet Mathematics, 2(1):73?120.
P. Bremaud. 1999. Markov chains: Gibbs fields, monte
carlo simulation, and queues. Springer-Verlag.
A. Budanitsky and G. Hirst. 2006. Evaluating wordnet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32(1):13?47.
K. Collins-Thompson and J. Callan. 2005. Query expan-
sion using random walk models. In CIKM ?05: Pro-
ceedings of the 14th ACM international conference on
Information and knowledge management, pages 704?
711, New York, NY, USA. ACM Press.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. MIT Press.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The concept
revisited. ACM Transactions on Information Systems,
20(1):116?131.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In IJCAI.
T. Haveliwala, A. Gionis, D. Klein, and P. Indyk. 2002.
Evaluating strategies for similarity search on the web.
In WWW2002.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
thesaurus and semantic similarity. In Proceedings of
RANLP-03, pages 212?219.
J. J. Jiang and D. W. Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on Re-
search in Computational Linguistics (ROCLING X),
pages 19?33.
Lillian Lee. 1999. Measures of distributional similarity.
In 37th Annual Meeting of the Association for Compu-
tational Linguistics, pages 25?32.
Lillian Lee. 2001. On the effectiveness of the skew di-
vergence for statistical language analysis. In Artificial
Intelligence and Statistics 2001, pages 65?72.
Jianhua Lin. 1991. Divergence measures based on the
shannon entropy. In IEEE Transactions on Informa-
tion Theory, volume 37(1), pages 145?151.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In HLT ?05: Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 411?418, Morristown, NJ, USA.
Association for Computational Linguistics.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6:1?28.
S. Patwardhan and T. Pedersen. 2006. Using wordnet-
based context vectors to estimate the semantic related-
ness of concepts. In Proceedings of the EACL 2006
Workshop Making Sense of Sense - Bringing Com-
putational Linguistics and Pyscholinguistics Together,
pages 1?8.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the relat-
edness of concepts. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, (11):95?130.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Computational Linguistics,
8:627?633.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In Proceedings of the 21st National Con-
ference on Artificial Intelligence, pages 1419?1424.
Paul Tarau, Rada Mihalcea, and Elizabeth Figa. 2005.
Semantic document engineering with wordnet and
pagerank. In SAC ?05: Proceedings of the 2005
ACM symposium on Applied computing, pages 782?
786, New York, NY, USA. ACM Press.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
Kristina Toutanova, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk models
for inducing word dependency distributions. In ICML
?04: Proceedings of the twenty-first international con-
ference on Machine learning, New York, NY, USA.
ACM Press.
Julie Weeds and David Weir. 2005. Co-occurrence re-
trieval: A flexible framework for lexical distributional
similarity. Comput. Linguist., 31(4):439?475.
589
