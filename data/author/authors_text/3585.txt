53
54
55
56
57
58
Orthographic Disambiguation Incorporating Transliterated Probability
Eiji ARAMAKI Takeshi IMAI Kengo Miyo Kazuhiko Ohe
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8655, Japan
aramaki@hcc.h.u-tokyo.ac.jp
Abstract
Orthographic variance is a fundamental
problem for many natural language process-
ing applications. The Japanese language, in
particular, contains many orthographic vari-
ants for two main reasons: (1) transliterated
words allow many possible spelling varia-
tions, and (2) many characters in Japanese
nouns can be omitted or substituted. Pre-
vious studies have mainly focused on the
former problem; in contrast, this study has
addressed both problems using the same
framework. First, we automatically col-
lected both positive examples (sets of equiv-
alent term pairs) and negative examples (sets
of inequivalent term pairs). Then, by using
both sets of examples, a support vector ma-
chine based classifier determined whether
two terms (t1 and t2) were equivalent. To
boost accuracy, we added a transliterated
probability P (t1|s)P (t2|s), which is the
probability that both terms (t1 and t2) were
transliterated from the same source term (s),
to the machine learning features. Exper-
imental results yielded high levels of ac-
curacy, demonstrating the feasibility of the
proposed approach.
1 Introduction
Spelling variations, such as ?center? and ?centre?,
which have different spellings but identical mean-
ings, are problematic for many NLP applications
including information extraction (IE), question an-
swering (QA), and machine transliteration (MT). In
Table 1: Examples of Orthographic Variants.
spaghetti Thompson operation
* ?? indicates a pronunciation. () indicates a translation.
this paper, these variations can be termed ortho-
graphic variants.
The Japanese language, in particular, contains
many orthographic variants, for two main reasons:
1. It imports many words from other languages
using transliteration, resulting in many possible
spelling variations. For example, Masuyama et
al. (2004) found at least six different spellings
for? spaghetti?in newspaper articles (Table 1
Left).
2. Many characters in Japanese nouns can be
omitted or substituted, leading to tons of in-
sertion variations (Daille et al, 1996) (Table 1
Right).
To address these problems, this study developed a
support vector machine (SVM) based classifier that
48
can determine whether two terms are equivalent. Be-
cause a SVM-based approach requires positive and
negative examples, we also developed a method to
automatically generate both examples.
Our proposed method differs from previously de-
veloped methods in two ways.
1. Previous studies have focused solely on the for-
mer problem (transliteration); our target scope
is wider. We addressed both transliteration
and character omissions/substitutions using the
same framework.
2. Most previous studies have focused on back-
transliteration (Knight and Graehl, 1998; Goto
et al, 2004), which has the goal of generating a
source word (s) for a Japanese term (t). In con-
trast, we employed a discriminative approach,
which has the goal of determining whether two
terms (t1 and t2) are equivalent. These two
goals are related. For example, if two terms (t1
and t2) were transliterated from the same word
(s), they should be orthographic variants. To
incorporate this information, we incorporated
a transliterated-probability (P (s|t1)?P (s|t2))
into the SVM features.
Although we investigated performance using
medical terms, our proposed method does not de-
pend on a target domain1.
2 Orthographic Variance in Dictionary
Entries
Before developing our methodology, we examined
problems related to orthographic variance.
First, we investigated the amount of orthographic
variance between two dictionaries? entries (DIC1
(Ito et al, 2003), totaling 69,604 entries, and DIC2
(Nanzando, 2001), totaling 27,971 entries).
Exact matches between entries only occurred for
10,577 terms (15.1% of DIC1, and 37.8% of DIC2).
From other entries, we extracted orthographic vari-
ance as follows.
STEP 1: Extracting Term Pairs with Similar
Spelling
1The domain could affect the performance, because most of
medical terms are imported from other languages, leading to
many orthographic variants.



Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 464?467,
Prague, June 2007. c?2007 Association for Computational Linguistics
UTH: Semantic Relation Classification using Physical Sizes
Eiji ARAMAKI Takeshi IMAI Kengo MIYO Kazuhiko OHE
The University of Tokyo Hospital department
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
aramaki@hcc.h.u-tokyo.ac.jp
Abstract
Although researchers have shown increas-
ing interest in extracting/classifying seman-
tic relations, most previous studies have ba-
sically relied on lexical patterns between
terms. This paper proposes a novel way to
accomplish the task: a system that captures
a physical size of an entity. Experimental
results revealed that our proposed method is
feasible and prevents the problems inherent
in other methods.
1 Introduction
Classification of semantic relations is important to
NLP as it would benefit many NLP applications,
such as machine translation and information re-
trieval.
Researchers have already proposed various
schemes. For example, Hearst (1992) manually de-
signed lexico-syntactic patterns for extracting is-a
relations. Berland and Charniak (1999) proposed a
similar method for part-whole relations. Brin (1998)
employed a bootstrapping algorithm for more spe-
cific relations (author-book relations). Kim and
Baldwin (2006) and Moldovan et al(2004) focused
on nominal relations in compound nouns. Turney
(2005) measured relation similarity between two
words. While these methods differ, they all utilize
lexical patterns between two entities.
Within this context, our goal was to utilize infor-
mation specific to an entity. Although entities con-
tain many types of information, we focused on the
physical size of an entity. Here, physical size refers
to the typical width/height of an entity. For example,
we consider book to have a physical size of 20?25
cm, and book to have a size of 10?10 m, etc.
We chose to use physical size for the following
reasons:
1. Most entities (except abstract entities) have a
physical size.
2. Several semantic relations are sensitive to phys-
ical size. For example, a content-container rela-
tion (e1 content-container e2) naturally means
that e1 has a smaller size than e2. A book is
also smaller than its container, library. A part-
whole relation has a similar constraint.
Our next problem was how to determine physi-
cal sizes. First, we used Google to conduct Web
searches using queries such as ?book (*cm x*cm)?
and ?library (*m x*m)?. Next, we extracted numeric
expressions from the search results and used the av-
erage value as the physical size.
Experimental results revealed that our proposed
approach is feasible and prevents the problems in-
herent in other methods.
2 Corpus
We used a corpus provided by SemEval2007 Task
#4 training set. This corpus consisted of 980 anno-
tated sentences (140 sentences?7 relations). Table
1 presents an example.
Although the corpus contained a large quantity of
information such as WordNet sense keys, comments,
etc., we used only the most pertinent information:
entity1 (e1), entity2 (e2), and its relation (true/false)
464
The <e1>library</e1> contained <e2>books
</e2> of guidance on the processes.
WordNet(e1) = "library\%1:14:00::",
WordNet(e2) = "book\%1:10:00::",
Content-Container(e2, e1) = "true",
Query = "the * contained books"
Table 1: An Example of Task#4 Corpus.
Figure 1: Three types of Features.
1
. For example, we extracted a triple example (li-
brary, book, true from Table 1.
3 Method
We applied support vector machine (SVM)-based
learning (Vapnik, 1999) using three types of fea-
tures: (1) basic pattern features (Section 3.1), (2) se-
lected pattern features (Section 3.2), and (3) physical
size features (Section 3.3). Figure 1 presents some
examples of these features.
3.1 Basic Pattern Features
First, the system finds lexical patterns that co-occur
with semantic relations between two entities (e1 and
e2). It does so by conducting searches using two
queries ?e1 * e2? and ?e2 * e1?. For example, two
queries, ?library * book? and ?book * library?, are
generated from Table 1.
Then, the system extracts the word (or word se-
quences) between two entities from the snippets in
the top 1,000 search results. We considered the ex-
tracted word sequences to be basic patterns. For ex-
ample, given ?...library contains the book...?, the ba-
sic pattern is ?(e1) contains the (e2)? 2.
1Our system is classified as an A4 system, and therefore
does not use WordNet or Query.
2This operation does not handle any stop-words. Therefore,
We gathered basic patterns for each relation, and
identified if each pattern had been obtained as a
SVM feature or not (1 or 0). We refer to these fea-
tures as basic pattern features.
3.2 Selected Pattern Features
Because basic pattern features are generated only
from snippets, precise co-occurrence statistics are
not available. Therefore, the system searches again
with more specific queries, such as ?library contains
the book?. However, this second search is a heavy
burden for a search engine, requiring huge numbers
of queries (# of samples ? # of basic patterns).
We thus selected the most informative n patterns
(STEP1) and conducted specific searches (# of sam-
ples ? n basic patterns)(STEP2) as follows:
STEP1: To select the most informative patterns,
we applied a decision tree (C4.5)(Quinlan,
1987) and selected the basic patterns located in
the top n branches 3.
STEP2: Then, the system searched again us-
ing the selected patterns. We considered log
weighted hits (log
10
|hits|) to be selected pat-
tern features. For example, if ?library contains
the book? produced 120,000 hits in Google, it
yields the value log
10
(12, 000) = 5.
3.3 Physical Size Features
As noted in Section 1, we theorized that an entity?s
size could be a strong clue for some semantic rela-
tions.
We estimated entity size using the following
queries:
1. ?< entity > (* cm x * cm)?,
2. ?< entity > (* x * cm)?,
3. ?< entity > (* m x * m)?,
4. ?< entity > (* x * m)?.
In these queries, < entity > indicates a slot for
each entity, such as ?book?, ?library?, etc. Then, the
system examines the search results for the numerous
expressions located in ?*? and considers the average
value to be the size.
?(e1) contains THE (e2)? and ?(e1) contains (e2)? are different
patterns.
3In the experiments in Section 4, we set n = 10.
465
Precision Recall F
?=1
PROPOSED 0.57 (=284/497) 0.60 (=284/471) 0.58
+SEL 0.56 (=281/496) 0.59 (=281/471) 0.57
+SIZE 0.53 (=269/507) 0.57 (=269/471) 0.54
BASELINE 0.53 (=259/487) 0.54 (=259/471) 0.53
Table 2: Results.
When results of size expressions were insufficient
(numbers < 10), we considered the entity to be non-
physical, i.e., to have no size.
By applying the obtained sizes, the system gener-
ates a size feature, consisting of six flags:
1. LARGE-e1: (e1?s X > e2?s X) and (e1?s Y > e2?s Y)
2. LARGE-e2: (e1?s X < e2?s X) and (e1?s Y < e2?s Y)
3. NOSIZE-e1: only e1 has no size.
4. NOSIZE-e2: only e2 has no size.
5. NOSIZE-BOTH: Both e1 and e2 have no size.
6. OTHER: Other.
4 Experiments
4.1 Experimental Set-up
To evaluate the performance of our system, we
used a SemEval-Task No#4 training set. We com-
pared the following methods using a ten-fold cross-
validation test:
1. BASELINE: with only basic pattern features.
2. +SIZE: BASELINE with size features.
3. +SEL: BASELINE with selected pattern features.
4. PROPOSED: BASELINE with both size and selected
pattern features.
For SVM learning, we used TinySVM with a lin-
ear kernel4.
4.2 Results
Table 2 presents the results. PROPOSED was the
most accurate, demonstrating the basic feasibility of
our approach.
Table 3 presents more detailed results. +SIZE
made a contribution to some relations (REL2 and
REL4). Particularly for REL4, +SIZE significantly
boosted accuracy (using McNemar tests (Gillick and
4http://chasen.org/ taku/software/TinySVM/
Figure 2: The Size of a ?Car?.
Cox, 1989); p = 0.05). However, contrary to our ex-
pectations, size features were disappointing for part-
whole relations (REL6) and content-container rela-
tions (REL7).
The reason for this was mainly the difficulty in es-
timating size. Table 4 lists the sizes of several enti-
ties, revealing some strange results, such as a library
sized 12.1 ? 8.4 cm, a house sized 53 ? 38 cm, and
a car sized 39 ? 25 cm. These sizes are unusually
small for the following reasons:
1. Some entities (e.g.?car?) rarely appear with
their size,
2. In contrast, entities such as ?toy car? or ?mini
car? frequently appear with a size.
Figure 2 presents the size distribution of ?car.?
Few instances appeared of real cars sized approxi-
mately 500 ? 400 cm, while very small cars smaller
than 100 ? 100 cm appeared frequently. Our current
method of calculating average size is ineffective un-
der this type of situation.
In the future, using physical size as a clue for de-
termining a semantic relation will require resolving
this problem.
5 Conclusion
We briefly presented a method for obtaining the size
of an entity and proposed a method for classifying
semantic relations using entity size. Experimental
results revealed that the proposed approach yielded
slightly higher performance than a baseline, demon-
strating its feasibility. If we are able to estimate en-
466
Relation PROPOSED +SEL +SIZE BASELINE
Precision 0.60 (=50/83) 0.56 (=53/93) 0.54 (=53/98) 0.50 (=53/106)
REL1 Recall 0.68 (=50/73) 0.72 (=53/73) 0.72 (=53/73) 0.72 (=53/73)
(Cause-Effect) F
?=1
0.64 0.63 0.59 0.61
Precision 0.59 (=43/72) 0.60 (=44/73) 0.56 (=45/79) 0.55 (=44/79)
REL2 Recall 0.60 (=43/71) 0.61 (=44/71) 0.63 (=45/71) 0.61 (=44/71)
(Instrument-Agency) F
?=1
0.60 0.61 0.59 0.58
Precision 0.70 (=56/80) 0.73 (=55/75) 0.65 (=54/82) 0.68 (=51/74)
REL3 Recall 0.65 (=56/85) 0.64 (=55/85) 0.63 (=54/85) 0.60 (=51/85)
(Product-Producer) F
?=1
0.67 0.68 0.64 0.64
Precision 0.41 (=23/56) 0.35 (=18/51) 0.48 (=24/49) 0.52 (=13/25)
REL4 Recall 0.42 (=23/54) 0.33 (=18/54) 0.44 (=24/54) 0.24 (=13/54)
(Origin-Entity) F
?=1
0.41 0.34 0.46 0.32
Precision 0.62 (=40/64) 0.61 (=40/65) 0.56 (=28/50) 0.56 (=29/51)
REL5 Recall 0.68 (=40/58) 0.68 (=40/58) 0.48 (=28/58) 0.50 (=29/58)
(Theme-Tool) F
?=1
0.65 0.65 0.51 0.53
Precision 0.45 (=46/101) 0.46 (=46/100) 0.41 (=49/118) 0.43 (=53/123)
REL6 Recall 0.70 (=46/65) 0.70 (=46/65) 0.75 (=49/65) 0.81 (=53/65)
(Part-Whole) F
?=1
0.55 0.55 0.53 0.56
Precision 0.63 (26/41) 0.64 (=25/39) 0.51 (=16/31) 0.55 (=16/29)
REL7 Recall 0.40 (26/65) 0.38 (=25/65) 0.24 (=16/65) 0.24 (=16/65)
(Content-Container) F
?=1
0.49 0.48 0.33 0.34
Table 3: Detailed Results.
entity # size
library 51 12.1?8.4 m
room 204 5.4?3.5 m
man 75 1.5?0.5 m
benches 33 93?42 cm
granite 68 76?48 cm
sink 34 57?25 cm
house 86 53?38 cm
books 50 46?24 cm
car 91 39?25 cm
turtles 15 38?23 cm
food 38 35?26 cm
oats 16 24?13 cm
tumor shrinkage 6 -
habitat degradation 5 -
Table 4: Some Examples of Entity Sizes.
?#? indicates the number of obtained size expressions.
?-? indicates a ?NO-SIZE? entity.
tity sizes more precisely in the future, the system
will become much more accurate.
References
Matthew Berland and Eugene Charniak. 1999. Finding parts
in very large corpora. In Proceedings of the Annual Con-
ference of the Association for Computational Linguistics
(ACL1999), pages 57?64.
Sergey Brin. 1998. Extracting patterns and relations from the
world wide web. In WebDB Workshop at 6th International
Conference on Extending Database Technology, EDBT?98,
pages 172?183.
L. Gillick and SJ Cox. 1989. Some statistical issues in the com-
parison of speech recognition algorithms. In Proceedings of
IEEE International Conference on Acoustics, Speech, and
Signal Processing, pages 532?535.
M. Hearst. 1992. Automatic acquisition of hyponyms from
large text corpora. In Proceedings of International Confer-
ence on Computational Linguistics (COLING1992), pages
539?545.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting seman-
tic relations in noun compounds via verb semantics. In Pro-
ceedings of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 491?498.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju.
2004. Models for the semantic classification of noun
phrases. Proceedings of HLT/NAACL-2004 Workshop on
Computational Lexical Semantics.
J.R. Quinlan. 1987. Simplifying decision trees. International
Journal of Man-Machine Studies, 27(1):221?234.
Peter D. Turney. 2005. Measuring semantic similarity by latent
relational analysis. In Proceedings of the Nineteenth Inter-
national Joint Conference on Artificial Intelligence (IJCAI-
05), pages 1136?1141.
Vladimir Vapnik. 1999. The Nature of Statistical Learning
Theory. Springer-Verlag.
467
