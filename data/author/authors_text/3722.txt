TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 33?36,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Unigram Language Models using Diffusion Smoothing over Graphs
Bruno Jedynak
Dept. of Appl. Mathematics and Statistics
Center for Imaging Sciences
Johns Hopkins University
Baltimore, MD 21218-2686
bruno.jedynak@jhu.edu
Damianos Karakos
Dept. of Electrical and Computer Engineering
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218-2686
damianos@jhu.edu
Abstract
We propose to use graph-based diffusion
techniques with data-dependent kernels
to build unigram language models. Our
approach entails building graphs, where
each vertex corresponds uniquely to a
word from a closed vocabulary, and the
existence of an edge (with an appropri-
ate weight) between two words indicates
some form of similarity between them. In
one of our constructions, we place an edge
between two words if the number of times
these words were seen in a training set
differs by at most one count. This graph
construction results in a similarity ma-
trix with small intrinsic dimension, since
words with the same counts have the same
neighbors. Experimental results from a
benchmark task from language modeling
show that our method is competitive with
the Good-Turing estimator.
1 Diffusion over Graphs
1.1 Notation
Let G = (V,E) be an undirected graph, where V
is a finite set of vertices, and E ? V ? V is the
set of edges. Also, let V be a vocabulary of words,
whose probabilities we want to estimate. Each ver-
tex corresponds uniquely to a word, i.e., there is a
one-to-one mapping between V and V . Without loss
of generality, we will use V to denote both the set
of words and the set of vertices. Moreover, to sim-
plify notation, we assume that the letters x, y, z will
always denote vertices of G.
The existence of an edge between x, y will be
denoted by x ? y. We assume that the graph
is strongly connected (i.e., there is a path between
any two vertices). Furthermore, we define a non-
negative real valued function w over V ? V , which
plays the role of the similarity between two words
(the higher the value of w(x, y), the more similar
words x, y are). In the experimental results section,
we will compare different measures of similarity be-
tween words which will result in different smooth-
ing algorithms. The degree of a vertex is defined as
d(x) =
?
y?V :x?y
w(x, y). (1)
We assume that for any vertex x, d(x) > 0; that is,
every word is similar to at least some other word.
1.2 Smoothing by Normalized Diffusion
The setting described here was introduced in (Szlam
et al, 2006). First, we define a Markov chain {Xt},
which corresponds to a random walk over the graph
G. Its initial value is equal to X0, which has dis-
tribution pi0. (Although pi0 can be chosen arbitrar-
ily, we assume in this paper that it is equal to the
empirical, unsmoothed, distribution of words over a
training set.) We then define the transition matrix as
follows:
T (x, y) = P (X1 = y|X0 = x) = d
?1(x)w(x, y).
(2)
This transition matrix, together with pi0, induces a
distribution over V , which is equal to the distribu-
33
tion pi1 of X1:
pi1(y) =
?
x?V
T (x, y)pi0(x). (3)
This distribution can be construed as a smoothed
version of pi0, since the pi1 probability of an un-
seen word will always be non-zero, if it has a non-
zero similarity to a seen word. In the same way, a
whole sequence of distributions pi2, pi3, . . . can be
computed; we only consider pi1 as our smoothed es-
timate in this paper. (One may wonder whether the
stationary distribution of this Markov chain, i.e., the
limiting distribution of Xt, as t ? ?, has any sig-
nificance; we do not address this question here, as
this limiting distribution may have very little depen-
dence on pi0 in the Markov chain cases under con-
sideration.)
1.3 Smoothing by Kernel Diffusion
We assume here that for any vertex x, w(x, x) = 0
and that w is symmetric. Following (Kondor and
Lafferty, 2002), we define the following matrix over
V ? V
H(x, y) = w(x, y)?(x ? y)? d(x)?(x = y), (4)
where ?(u) is the delta function which takes the
value 1 if property u is true, and 0 otherwise. The
negative of the matrix H is called the Laplacian of
the graph and plays a central role in spectral graph
theory (Chung, 1997). We further define the heat
equation over the graph G as
?
?t
Kt = HKt, t > 0, (5)
with initial condition K0 = I , where Kt is a time-
dependent square matrix of same dimension as H ,
and I is the identity matrix. Kt(x, y) can be inter-
preted as the amount of heat that reaches vertex x
at time t, when starting with a unit amount of heat
concentrated at y. Using (1) and (4), the right hand
side of (5) expands to
HKt(x, y) =
?
z:z?x
w(x, z) (Kt(z, y)?Kt(x, y)) .
(6)
From this equation, we see that the amount of heat
at x will increase (resp. decrease) if the current
amount of heat at x (namely Kt(x, y)) is smaller
(resp. larger) than the weighted average amount of
heat at the neighbors of x, thus causing the system
to reach a steady state.
The heat equation (5) has a unique solution which
is the matrix exponential Kt = exp(tH), (see (Kon-
dor and Lafferty, 2002)) and which can be defined
equivalently as
etH = lim
n?+?
(
I +
tH
n
)n
(7)
or as
etH = I + tH +
t2
2!
H2 +
t3
3!
H3 + ? ? ? (8)
Moreover, if the initial condition is replaced by
K0(x, y) = pi0(x)?(x = y) then the solution of
the heat equation is given by the matrix product
pi1 = Ktpi0. In the following, pi0 will be the em-
pirical distribution over the training set and t will be
chosen by trial and error. As before, pi1 will provide
a smoothed version of pi0.
2 Unigram Language Models
Let Tr be a training set of n tokens, and T a sepa-
rate test set of m tokens. We denote by n(x),m(x)
the number of times the word x has been seen in
the training and test set, respectively. We assume a
closed vocabulary V containing K words. A uni-
gram model is a probability distribution pi over the
vocabulary V . We measure its performace using
the average code length (Cover and Thomas, 1991)
measured on the test set:
l(pi) = ?
1
|T |
?
x?V
m(x) log2 pi(x). (9)
The empirical distribution over the training set is
pi0(x) =
n(x)
n
. (10)
This estimate assigns a probability 0 to all unseen
words, which is undesirable, as it leads to zero prob-
ability of word sequences which can actually be ob-
served in practice. A simple way to smooth such
estimates is to add a small, not necessarily integer,
count to each word leading to the so-called add-?
estimate pi? , defined as
pi?(x) =
n(x) + ?
n + ?K
. (11)
34
One may observe that
pi?(x) = (1??)pi0(x)+?
1
K
, with ? = ?K
n + ?K
.
(12)
Hence add-? estimators perform a linear interpola-
tion between pi0 and the uniform distribution over
the entire vocabulary.
In practice, a much more efficient smoothing
method is the so-called Good-Turing (Orlitsky et al,
2003; McAllester and Schapire, 2000). The Good-
Turing estimate is defined as
piGT (x) =
rn(x)+1(n(x) + 1)
nrn(x)
, if n(x) < M
= ?pi0(x), otherwise,
where rj is the number of distinct words seen j times
in the training set, and ? is such that piGT sums up
to 1 over the vocabulary. The threshold M is em-
pirically chosen, and usually lies between 5 and 10.
(Choosing a much larger M decreases the perfor-
mance considerably.)
The Good-Turing estimator is used frequently in
practice, and we will compare our results against it.
The add-? will provide a baseline, as well as an idea
of the variation between different smoothers.
3 Graphs over sets of words
Our objective, in this section, is to show how to de-
sign various graphs on words; different choices for
the edges and for the weight function w lead to dif-
ferent smoothings.
3.1 Full Graph and add-? Smoothers
The simplest possible choice is the complete graph,
where all vertices are pair-wise connected. In the
case of normalized diffusion, choosing
w(x, y) = ??(x = y) + 1, (13)
with ? 6= 0 leads to the add-? smoother with param-
eter ? = ??1n.
In the case of kernel smoothing with the complete
graph and w ? 1, one can show, see (Kondor and
Lafferty, 2002) that
Kt(x, y) = K
?1
(
1 + (K ? 1)e?Kt
)
if x = y
= K?1
(
1? e?Kt
)
if x 6= y.
This leads to another add-? smoother.
3.2 Graphs based on counts
A more interesting way of designing the word graph
is through a similarity function which is based on
the training set. For the normalized diffusion case,
we propose the following
w(x, y) = ?(|n(x)? n(y)| ? 1). (14)
That is, 2 words are ?similar? if they have been seen
a number of times which differs by at most one. The
obtained estimator is denoted by piND. After some
algebraic manipulations, we obtain
piND(y) =
1
n
n(y)+1?
j=n(y)?1
jrj
rj?1 + rj + rj+1
. (15)
This estimator has a Good-Turing ?flavor?. For ex-
ample, the total mass associated with the unseen
words is
?
y;n(y)=0
pi1(y) =
1
n
r1
1 + r1r0 +
r2
r0
. (16)
Note that the estimate of the unseen mass, in the case
of the Good-Turing estimator, is equal to n?1r1,
which is very close to the above when the vocabu-
lary is large compared to the size of the training set
(as is usually the case in practice).
Similarly, in the case of kernel diffusion, we
choose w ? 1 and
x ? y ?? |n(x)? n(y)| ? 1 (17)
The time t is chosen to be |V |?1. The smoother can-
not be computed in closed form. We used the for-
mula (7) with n = 3 in the experiments. Larger
values of n did not improve the results.
4 Experimental Results
In our experiments, we used Sections 00-22 (con-
sisting of ? 106 words) of the UPenn Treebank cor-
pus for training, and Sections 23-24 (consisting of
? 105 words) for testing. We split the training set
into 10 subsets, leading to 10 datasets of size ? 105
tokens each. The first of these sets was further split
in subsets of size ? 104 tokens each. Averaged re-
sults are presented in the tables below for various
choices of the training set size. We show the mean
code-length, as well as the standard deviation (when
35
mean code length std
pi?, ? = 1 12.94 0.05
piGT 11.40 0.08
piND 11.42 0.08
piKD 11.51 0.08
Table 1: Results with training set of size ? 104.
mean code length std
pi?, ? = 1 11.10 0.03
piGT 10.68 0.06
piND 10.69 0.06
piKD 10.74 0.08
Table 2: Results with training set of size ? 105.
available). In all cases, we chose K = 105 as the
fixed size of our vocabulary.
The results show that piND, the estimate ob-
tained with the Normalized Diffusion, is competi-
tive with the Good-Turing piGT . We performed a
Kolmogorov-Smirnov test in order to determine if
the code-lengths obtained with piND and piGT in Ta-
ble 1 differ significantly. The result is negative (P-
value = .65), and the same holds for the larger train-
ing set in Table 2 (P-value=.95). On the other hand,
piKD (obtained with Kernel Diffusion) is not as effi-
cient, but still better than add-? with ? = 1.
5 Concluding Remarks
We showed that diffusions on graphs can be useful
for language modeling. They yield naturally smooth
estimates, and, under a particular choice of the ?sim-
ilarity? function between words, they are competi-
tive with the Good-Turing estimator, which is con-
sidered to be the state-of-the-art in unigram lan-
guage modeling. We plan to perform more exper-
mean code length
pi?, ? = 1 10.34
piGT 10.30
piND 10.30
piKD 10.31
Table 3: Results with training set of size ? 106.
iments with other definitions of similarity between
words. For example, we expect similarities based
on co-occurence in documents, or based on notions
of semantic closeness (computed, for instance, using
the WordNet hierarchy) to yield significant improve-
ments over estimators which are only based on word
counts.
References
F. Chung. 1997. Spectral Graph Theory. Number 92
in CBMS Regional Conference Series in Mathematics.
American Mathematical Society.
Thomas M. Cover and Joy A. Thomas. 1991. Elements
of Information Theory. John Wiley & Sons, Inc.
Risi Imre Kondor and John Lafferty. 2002. Diffusion
kernels on graphs and other discrete input spaces. In
ICML ?02: Proceedings of the Nineteenth Interna-
tional Conference on Machine Learning, pages 315?
322.
David McAllester and Robert E. Schapire. 2000. On the
convergence rate of Good-Turing estimators. In Proc.
13th Annu. Conference on Comput. Learning Theory.
Alon Orlitsky, Narayana P. Santhanam, and Junan Zhang.
2003. Always Good Turing: Asymptotically optimal
probability estimation. In FOCS ?03: Proceedings of
the 44th Annual IEEE Symposium on Foundations of
Computer Science.
Arthur D. Szlam, Mauro Maggioni, and Ronald R. Coif-
man. 2006. A general framework for adaptive regular-
ization based on diffusion processes on graphs. Tech-
nical report, YALE/DCS/TR1365.
36
