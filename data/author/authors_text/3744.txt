Chinese Word Segmentation based on Mixing Model 
Wei Jiang          Jian Zhao          Yi Guan          Zhiming Xu  
ITNLP, Harbin Institute of Technology,  Heilongjiang Province, 150001 China 
jiangwei@insun.hit.edu.cn
Abstract
This paper presents our recent work for 
participation in the Second Interna-
tional Chinese Word Segmentation 
Bakeoff. According to difficulties, we 
divide word segmentation into several 
sub-tasks, which are solved by mixed 
language models, so as to take advan-
tage of each approach in addressing 
special problems. The experiment indi-
cated that this system achieved 96.7% 
and 97.2% in F-measure in PKU and 
MSR open test respectively. 
1 Introduction 
Word is a logical semantic and syntactic unit in 
natural language. So word segmentation is the 
foundation of most Chinese NLP tasks. Though 
much progress has been made in the last two 
decades, there is no existing model that can 
solve all the problems perfectly at present. So 
we try to apply different language models to 
solve each special sub-task, due to ?No Free 
Lunch Theorem? and ?Ugly Duckling Theorem?. 
Our system participated in the Second Inter-
national Chinese Word Segmentation Bakeoff 
(henceforce, the bakeoff) held in 2005. Recently, 
we have done more work in dealing with three 
main sub-tasks: (1) Segmentation disambigua-
tion; (2) Named entities recognition; (3) New 
words1 detection. We apply different approachs 
to solve above three problems, and all the mod-
ules are integrated into a pragmatic system 
(ELUS). Due to the limitation of available re-
source, some kinds of features, e.g. POS, have 
been erased in our participation system. This 
segmenter will be briefly describled in this paper. 
                                                          
1 New words refer to this kind of out-of ?vocabulary words 
that are neither recognized named entities or factoid words 
nor morphological words. 
2 ELUS Segmenter 
All the words are categorized into five types: 
Lexicon words (LW), Factoid words (FT), Mor-
phologically derived words (MDW), Named 
entities (NE), and New words (NW). Accord-
ingly, four main modules are included to iden-
tify each kind of words, as shown in Figure 1.  
Class-based trigram model (Gao 2004) is 
adopted in the Basic Segmentation to convert 
the sentence into a word sequence. Let w = w1
w2 ?wn be a word class sequence, then the most 
likely word class sequence w* in trigram is: 
?
 
 
n
i
iii
www
wwwP
n 1
12 )|(maxarg*
21 
w ,
where let P(w0|w-2 w-1) be P(w0) and let 
P(w1|w-1 w0) be P(w1|w0). And wi represents LW 
or a type of FT or MDW. Viterbi algorithm is 
used to search the best candidate. Absolute 
smoothing algorithm is applied to overcome the 
data sparseness. Here, LW, FT and MDW are 
idendified (Zhao Yan 2005). All the Factoid 
words can be represented as regular expressions. 
As a result, the detection of factoid words can be 
archieved by Finite State Machines. 
Figure 1 ELUS Segmenter 
Factoid Detect
Morphology Word
Lexicon words
String
Result
NE Recognization
Sentence
Basic Segmentation
NW Detection
Disambiguation
180
Four kinds of Named entities are detected, i.e. 
Chinese person name, foreign person name, lo-
cation name and orgnization name. This is the 
most complicated module in ELUS.  
Three kinds of models are applied here. 
HMM model (one order) is described as: 
?
 
 
n
i
iiii
TTT
TTPTWP
n 1
1
# )|()|(maxarg
21 
T ,
where iT  represents the tag of current word,  
Viterbi algorithm is used to search the best path. 
Another model is Maximum Entropy (Zhao Jian 
2005, Hai Leong Chieu 2002). Take Chinese 
person name as example. Firstly, we combine 
HMM and Maximum Entropy (ME) model to 
lable the person name tag, e.g. ??/CPB ?/CPI  
?/CPI? (Tongmei Yao); Secondly, the tagged  
name is merged by combining ME Model and 
Support Vector Machine (SVM) and some aided 
rules, e.g. merged into ??/??? in PKU test.
Some complex features are added into ME 
model (described in Zhao Jian 2005), in addition, 
we also collect more than 110,000 person names, 
and acquire the statistic about common name 
characters, these kinds of features are also fused 
into the ME model to detect NE. The other kinds 
of NE recognition adopt similar method, except 
for individual features. 
New Words is another important kind of 
OOV words, especially in closed test. Take PKU 
test as example, we collect NW suffixes, such as 
???(city),???(lamp). Those usually construct 
new words, e.g. ?????(sighting lamp). 
A variance-based method is used to collect 
suffixes. And three points need to be consid-
ered:(1) It is tail of many words;(2) It has large 
variance in constructing word;(3) It is seldom 
used alone. We acquire about 25 common suf-
fixes in PKU training corpus by above method. 
We use Local Maximum Entropy model, e.g. 
??? /1 ? /1?(Huanggang city), i.e. only the 
nearer characters are judged before the suffix 
??? (city). By our approach, the training corpus 
can be generated via given PKU corpus in the 
bakeoff. The features come from the nearer con-
text, besides, common single words and punc-
tuations are not regarded as a part of New Word. 
The last module is Word Disambiugation. 
Word segmentation ambiguities are usually clas-
sified into two classes: overlapping ambiguity 
and combination ambiguity. By evaluating 
ELUS, the most segmentation errors are one 
segmentation errors (about 95%). i.e. the two 
words on both sides of current segmentation 
errors are right. These include LW ambiguities 
and FT ambiguities etc. Here, we adopt Maxi-
mum Entropy model. The same as other mod-
ules, it is defined over HhT in segmentation 
disambiguation, where H is the set of possible 
contexts around target word that will be tagged, 
and T is the set of allowable tags. Then the 
model?s conditional probability is defined as 
? ? Tt thp
thphtp
'
)',(
),()|(
, where 
?
 
 
k
j
thf
j
jthp
1
),(),( DSP
where h is current context and t is one of the 
possible tags. The ambiguous words are mainly 
collected by evaluating our system.  
In NE module and Word Disambiguation 
module, we introduce rough rule features, which 
are extracted by Rough Set (Wang Xiaolong 
2004), e.g. ???????(display ability), ??
???/??(only? can just), ???+person+?
?? (the reporter+person+report). Previous ex-
periment had indicated word disambiguation 
could achieve better performance by applying 
Rough Set. 
3 Performance and analysis 
The performance of ELUS in the bakeoff is pre-
sented in Table 1 and Table 2 respectively, in 
terms of recall(R), precision(P) and F score in 
percentages.
Table 1 Closed test, in percentages (%) 
Closed R P F OOV Roov Riv
PKU 95.4 92.7 94.1 5.8 51.8 98.1
MSR 97.3 94.5 95.9 2.6 32.3 99.1
CITYU 93.4 86.5 89.8 7.4 24.8 98.9
AS 94.3 89.5 91.8 4.3 13.7 97.9
Table 2 Open test, in percentages (%) 
Open R P F OOV Roov Riv
PKU 96.8 96.6 96.7 5.8 82.6 97.7
MSR 98.0 96.5 97.2 2.6 59.0 99.0
CITYU 94.6 89.8 92.2 7.4 41.7 98.9
AS 95.2 92.0 93.6 4.3 35.4 97.9
Our system has good performance in terms 
of F-measure in simplified Chinese open test, 
including PKU and MSR open test. In addition, 
181
its IV word identification performance is re-
markable, ranging from 97.7% to 99.1%, stands 
at the top or nearer the top in all the tests in 
which we have participated. This good perform-
ance owes to class-based trigram, absolute 
smoothing and word disambiguation module and 
rough rules. 
There is almost the same IV performance be-
tween open test and closed test in MSR, CITYU 
and AS respectively, because we adopt the same 
Lexicon between open test and closed test re-
spectively. While in open test of PKU, we adopt 
another Lexicon that comes from six-month 
corpora of Peoples? Daily (China) in 1998, 
which were also annotated by Peking University. 
The OOV word identification performance 
seems uneven, compared with PKU, the other 
tests seem lower, due to the following reasons: 
 (1) Because of our resource limitation, NE 
training resource is six-month corpora of Peo-
ples? Daily (China) in 1998, which came from 
Peking University, and some newspapers and 
web pages annotated by our laboratory;  
(2) We have no traditional Chinese corpus, 
so the NE training resource for CITYU and AS 
is acquired via converting above corpora. Since 
these corpora are converted from simplified 
Chinese, they are not well suitable to traditional 
Chinese corpora;
(3) The different corpora have different crite-
rions in NE detection, especially in location 
name and organization name, e.g. ????/??
/??? (Cuicun Town Xiangtang Hogpen) in 
PKU and ????????? in MSR criterion. 
Even if our system recognizes the ????/?/
?/??? as a orgnization name, we are not eas-
ily to regard ??? ? as one word in PKU, 
since ???? isn?t a lexical word. However in 
MSR, that is easy, because its criterion regard 
the whole Orgnization as a word;
(4) We need do more to comply with the 
segmentation criterion, e.g. ?????(outlier) in 
CITYU come from ???? + ???, while this 
kind of false segment is due to our bad under-
standing to CITYU criterion. 
Though there are above problems, our sys-
tem does well in regonization precision, since 
we adopt two steps in recognizing NE, especial 
in recognizing Chinese person name. And from 
the result of evalution in the bakeoff, we need to 
improve the NE recall in the future.  
In order to make our New words comply 
with the criterion, we conservatively use New 
Word Detection module, in order to avoid hav-
ing bad recognition result, since each corpus has 
its own New Word criterion.
4 Conclusion and Future work 
We have briefly describled our system based on 
mixed models. Different approachs are adopted 
to solve each special sub-task, since there is ?No 
Free Lunch Theorem?. And mixed models are 
used in NE detection. This sytem has a good 
performance in the simplified Chinese in the 
bakeoff.
The future work is mainly concentrating on 
two directions: finding effective features and 
delicately adjusting internal relations among 
different modules, in order to improve segmen-
tation performance. 
References
Fu Fuohong. 2000. Research on Statistical Methods 
of Chinese Syntactic Disambiguation. Ph.D. The-
sis. Harbin Institute of Technology, China. 
Hai Leong Chieu, Hwee Tou Ng. Named Entity. 
Recognition: A Maximum Entropy Approach Us-
ing Global. Information. Proceedings of the 19th 
International Conference. on Computational Lin-
guistics, 2002. 
Hua-Ping Zhang, Qun Liu etc. 2003. Chinese Lexical 
Analysis Using Hierarchical Hidden Markov 
Model, Second SIGHAN workshop affiliated with 
4th ACL, Sapporo Japan, pp.63-70, July 2003. 
Jianfeng Gao, Mu Li et al 2004. Chinese Word 
Segmentation: A Pragmatic Approach. MSR-TR-
2004-123, November 2004. 
Wang Xiaolong, Chen Qingcai, and Daniel S.Yeung. 
2004. Mining PinYin-to-Character Conversion 
Rules From Large-Scale Corpus: A Rough Set Ap-
proach, IEEE TRANSACTION ON SYSTEMS, 
MAN. AND CYBERNETICS-PART 
B:CYBERNETICS. VOL. 34, NO.2, APRIL. 
Zhao Jian, Wang Xiao-long et al 2005. Comparing 
Features Combination with Features Fusion in 
Chinese Named Entity Recognition. Computer 
Application. China. 
Zhao Yan. 2005. Research on Chinese Morpheme 
Analysis Based on Statistic Language Model. 
Ph.D. Thesis. Harbin Institute of Technology, 
China. 
182
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 189?192,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Pragmatic Chinese Word Segmentation System 
Wei Jiang, Yi Guan, Xiao-Long Wang 
School of Computer Science and Technology, Harbin Institute of Technology,  
Heilongjiang Province, 150001, P.R.China 
jiangwei@insun.hit.edu.cn 
 
Abstract 
This paper presents our work for partici-
pation in the Third International Chinese 
Word Segmentation Bakeoff. We apply 
several processing approaches according 
to the corresponding sub-tasks, which are 
exhibited in real natural language. In our 
system, Trigram model with smoothing 
algorithm is the core module in word 
segmentation, and Maximum Entropy 
model is the basic model in Named En-
tity Recognition task. The experiment 
indicates that this system achieves F-
measure 96.8% in MSRA open test in the 
third SIGHAN-2006 bakeoff. 
1 Introduction 
Word is a logical semantic and syntactic unit in 
natural language. Unlike English, there is no de-
limiter to mark word boundaries in Chinese lan-
guage, so in most Chinese NLP tasks, word seg-
mentation is a foundation task, which transforms 
Chinese character string into word sequence. It is 
prerequisite to POS tagger, parser or further ap-
plications, such as Information Extraction, Ques-
tion Answer system. 
Our system participated in the Third Interna-
tional Chinese Word Segmentation Bakeoff, 
which held in 2006. Compared with our system 
in the last bakeoff (Jiang 2005A), the system in 
the third bakeoff is adjusted intending to have a 
better pragmatic performance. This paper mainly 
focuses on describing two sub-tasks: (1) The ba-
sic Word Segmentation; (2) Named entities rec-
ognition. We apply different approaches to solve 
above two tasks, and all the modules are inte-
grated into a pragmatic system (ELUS). 
2 System Description 
All the words in our system are categorized into 
five types: Lexicon words (LW), Factoid words 
(FT), Morphologically derived words (MDW), 
Named entities (NE), and New words (NW). 
Figure 1 demonstrates our system structure.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The input character sequence is converted into 
one or several sentences, which is the basic deal-
ing unit. The ?Basic Segmentation? is used to 
identify the LW, FT, MDW words, and ?Named 
Entity Recognition? is used to detect NW words. 
We don?t adopt the New Word detection algo-
rithm in our system in this bakeoff. The ?Disam-
biguation? module performs to classify compli-
cated ambiguous words, and all the above results 
are connected into the final result, which is de-
noted by XML format. 
2.1 Trigram and Smoothing Algorithm 
We apply the trigram model to the word segmen-
tation task (Jiang 2005A), and make use of Ab-
solute Smoothing algorithm to overcome the 
sparse data problem. 
Trigram model is used to convert the sentence 
into a word sequence. Let w = w1 w2 ?wn be a 
word sequence, then the most likely word se-
quence w* in trigram is: 
?
=
??=
n
i
iii
www
wwwP
n 1
12 )|(maxarg*
21 L
w                   (1) 
where let P(w0|w-2 w-1) be P(w0) and let P(w1|w-1 
w0) be P(w1|w0), and wi represents LW or a type 
of FT or MDW. In order to search the best seg-
mentation way, all the word candidates are filled 
in the word lattice (Zhao 2005). And the Viterbi 
Basic 
Segmentation
Disambiguation
Sentence
Named Entity 
Recognition
Word 
Sequence
Factoid Detect
Lexicon Words
Morphological 
Word
Input 
Sequence
Figure 1 ELUS Segmenter and NER 
189
algorithm is used to search the best word seg-
mentation path.  
FT and MDW need to be detected when con-
structing word lattice (detailed in section 2.2). 
The data structure of lexicon can affect the effi-
ciency of word segmentation, so we represent 
lexicon words as a set of TRIEs, which is a tree-
like structure. Words starting with the same 
character are represented as a TRIE, where the 
root represents the first Chinese character, and 
the children of the root represent the second 
characters, and so on (Gao 2004). 
When searching a word lattice, there is the 
zero-probability phenomenon, due to the sparse 
data problem. For instance, if there is no cooc-
curence pair ???/?/???(we eat bananas) in 
the training corpus, then P(??|????) = 0. 
According to formula (1), the probability of the 
whole candidate path, which includes ???/?/
??? is zero, as a result of the local zero prob-
ability. In order to overcome the sparse data 
problem, our system has applied Absolute Dis-
counting Smoothing algorithm (Chen, 1999). 
|}0)(:{|)( 1 1
1
11 >=? ? +?? +?+ ii niii ni wwcwwN             (2) 
The notation N1+ is meant to evoke the number 
of words that have one or more counts, and the ?  
is meant to evoke a free variable that is summed 
over. The function ()c  represents the count of 
one word or the cooccurence count of multi-
words. In this case, the smoothing probability 
? +?+?? +?
?=
iw
i
ni
i
nii
nii
wc
Dwc
wwp
)(
}0,)(max{
)|(
1
11
1  
)|()1( 1 2
? +??+ i nii wwp?          (3)    
where, 
??
?
?
?
??
?
?
?
?=? ? +?+
+?? )()(1 1 111 i niw i ni wNwc
D
i
?     (4) 
Because we use trigram model, so the maxi-
mum n may be 3. A fixed discount D (0 ?D ? 1) 
can be set through the deleted estimation on the 
training data. They arrive at the estimate 
21
1
2nn
nD +=                                              (5) 
where n1 and n2 are the total number of n-grams 
with exactly one and two counts, respectively. 
After the basic segmentation, some compli-
cated ambiguous segmentation can be further 
disambiguated. In trigram model, only the previ-
ous two words are considered as context features, 
while in disambiguation processing, we can use 
the Maximum Entropy model fused more fea-
tures (Jiang 2005B) or rule based method. 
2.2 Factoid and Morphological words 
All the Factoid words can be represented as regu-
lar expressions. So the detection of factoid words 
can be achieved by Finite State Automaton(FSA). 
In our system, the following categories of factoid 
words can be detected, as shown in table 1. 
Table 1 Factoid word categories 
FT type Factoid word Example 
Number
Integer, real, 
percent  etc. 
2910, 46.12%, ??
?, ?????? 
Date Date 2005? 5? 12? 
Time Time 8:00, ????? 
English English word, How, are, you 
www Website, IP address  
http://www.hit.edu.cn
192.168.140.133 
email Email elus@google.com 
phone Phone, fax 0451-86413322 
Deterministic FSA (DFA) is efficient because 
a unique ?next state? is determined, when given 
an input symbol and the current state. While it is 
common for a linguist to write rule, which can be 
represented directly as a non-deterministic FSA 
(NFA), i.e. which allows several ?next states? to 
follow a given input and state.  
Since every NFA has an equivalent DFA, we 
build a FT rule compiler to convert all the FT 
generative rules into a DFA. e.g.  
z ?< digit > -> [0..9]; 
z < year > ::= < digit >{< digit >+}??; 
z < integer > ::= {< digit >+}; 
where ?->? is a temporary generative rule, and 
?::=? is a real generative rule. 
As for the morphological words, we erase the 
dealing module, because the word segmentation 
definition of our system adopts the PKU standard. 
3 Named Entity Recognition 
We adopt Maximum Entropy model to perform 
the Named Entity Recognition. The extensive 
evaluation on NER systems in recent years (such 
as CoNLL-2002 and CoNLL-2003) indicates the 
best statistical systems are typically achieved by 
using a linear (or log-linear) classification algo-
rithm, such as Maximum Entropy model, to-
gether with a vast amount of carefully designed 
linguistic features. And this seems still true at 
present in terms of statistics based methods. 
Maximum Entropy model (ME) is defined 
over H? T in segmentation disambiguation, 
where H is the set of possible contexts around 
target word that will be tagged, and T is the set of 
allowable tags, such as B-PER, I-PER, B-LOC, 
I-LOC etc. in our NER task. Then the model?s 
conditional probability is defined as 
190
? ?= Tt thp
thphtp
'
)',(
),()|(                                (6) 
where ?
=
=
k
j
thf
j
jthp
1
),(),( ???                              (7) 
where h is the current context and t is one of the 
possible tags.  
The several typical kinds of features can be 
used in the NER system. They usually include 
the context feature, the entity feature, and the 
total resource or some additional resources.  
Table 2 shows the context feature templates. 
Table2 NER feature template1 
Type Feature Template 
One order feature wi-2, wi-1, wi, wi+1, wi+2 
Two order feature wi-1:i, wi:i+1 
NER tag feature t i-1 
While, we only point out the local feature 
template, some other feature templates, such as 
long distance dependency templates, are also 
helpful to NER performance. These trigger fea-
tures can be collected by Average Mutual Infor-
mation or Information Gain algorithm etc. 
Besides context features, entity features is an-
other important factor, such as the suffix of Lo-
cation or Organization. The following 8 kinds of 
dictionaries are usually useful (Zhao 2006): 
Table 3 NER resource dictionary2  
List Type Lexicon Example 
Place lexicon ??, ??, ???Word list 
Chinese surname ?, ?, ?, ?? 
Prefix of PER ?, ?, ? 
Suffix of PLA ?, ?, ?, ?, ? String list 
Suffix of ORG ?, ??, ??, ? 
Character for CPER ?,?, ?, ?, ? 
Character for FPER ?, ?, ?, ?, ? Character list 
Rare character ?, ?, ? 
In addition, some external resources may im-
prove the NER performance too, e.g. we collect a 
lot of entities for Chinese Daily Newspaper in 
2000, and total some entity features. 
However, our system is based on Peking Uni-
versity (PKU) word segmentation definition and 
PKU NER definition, so we only used the basic 
features in table 2 in this bakeoff.  Another effect 
is the corpus: our system is training by the Chi-
nese Peoples? Daily Newspaper corpora in 1998, 
which conforms to PKU NER definition. In the 
section 4, we will give our system performance 
with the basic features in Chinese Peoples? Daily 
Newspaper corpora. 
                                                 
1 wi ? current word, wi-1 ? previous word, ti ? current tag. 
2 Partial translation: ?? BeiJing,?? New york;? Zhang, 
?Wang; ? Old;? mountain,? lake;? bureau. 
4 Performance and analysis 
4.1 The Evaluation in Word Segmentation 
The performance of our system in the third bake-
off is presented in table 4 in terms of recall(R), 
precision(P) and F score in percentages. The 
score software is standard and open by SIGHAN. 
Table 4 MSRA test in SIGHAN2006 (%) 
MSRA R P F OOV Roov Riv
Close 96.3 91.8 94.0 3.4 17.5 99.1
Open 97.7 96.0 96.8 3.4 62.4 98.9
Our system has good performance in terms of 
Riv measure. The Riv measure in close test and in 
open test are 99.1% and 98.9% respectively. This 
good performance owes to class-based trigram 
with the absolute smoothing and word disam-
biguation algorithm. 
In our system, it is the following reasons that 
the open test has a better performance than the 
close test: 
(1) Named Entity Recognition module is 
added into the open test system. And Named En-
tities, including PER, LOC, ORG, occupy the 
most of the out-of-vocabulary words. 
(2) The system of close test can only use the 
dictionary that is collected from the given train-
ing corpus, while the system of open test can use 
a better dictionary, which includes the words that 
exist in MSRA training corpus in SIGHAN2005. 
And we know, the dictionary is the one of impor-
tant factors that affects the performance, because 
the LW candidates in the word lattice are gener-
ated from the dictionary. 
As for the dictionary, we compare the two col-
lections in SIGHAN2005 and SIGHAN2006, and 
evaluating in SIGHAN2005 MSRA close test. 
There are less training sentence in SIGHAN2006, 
as a result, there is at least 1.2% performance 
decrease. So this result indicates that the diction-
ary can bring an important impact in our system. 
Table 5 gives our system performance in the 
second bakeoff. We?ll make brief comparison. 
Table 5 MSRA test in SIGHAN 2005 (%) 
MSRA R P F OOV Roov Riv
Close 97.3 94.5 95.9 2.6 32.3 99.1
Open 98.0 96.5 97.2 2.6 59.0 99.0
Comparing table 4 with table 5, we find that 
the OOV is 3.4 in third bakeoff, which is higher 
than the value in the last bakeoff. Obviously, it is 
one of reasons that affect our performance. 
In addition, based on pragmatic consideration, 
our system has been made some simplifier, for 
instance, we erase the new word detection algo-
rithm and the is no morphological word detection. 
191
4.2 Named Entity Recognition 
In MSRA NER open test, our NER system is 
training in prior six-month corpora of Chinese 
Peoples? Daily Newspaper in 1998, which were 
annotated by Peking University. Table 6 shows 
the NER performance in the MSRA open test. 
Table 6 The NER performance in MSRA Open test 
MSRA NER Precision Recall  F Score 
PER 93.68% 86.37% 89.87 
LOC 85.50% 59.67% 70.29 
ORG 75.87% 47.48% 58.41 
Overall 86.97% 65.56% 74.76 
As a result of insufficiency in preparing bake-
off, our system is only trained in Chinese Peo-
ples? Daily Newspaper, in which the NER is de-
fined according to PKU standard. However, the 
NER definition of MSRA is different from that 
of PKU, e.g, ???/LOC ???, ??/PER ?
/PER??? in MSRA, are not entities in PKU. 
So the training corpus becomes a main handicap 
to decrease the performance of our system, and it 
also explains that there is much difference be-
tween the recall rate and the precision in table 6. 
Table 7 gives the evaluation of our NER sys-
tem in Chinese Peoples? Daily Newspaper, train-
ing in prior five-month corpora and testing in the 
sixth month corpus. We also use the feature tem-
plates in table 2, in order to make comparison 
with table 6. 
Table 7 The NER test in Chinese Peoples? Daily 
MSRA NER Precision Recall  F Score 
CPN 93.56 90.96 92.24 
FPN 90.42 86.47 88.40 
LOC 91.94 90.52 91.22 
ORG 88.38 84.52 86.40 
Overall 91.35 88.85 90.08 
This experiment indicates that our system can 
have a good performance, if the test corpus and 
the training corpora conform to the condition of 
independent identically distributed attribution. 
4.3 Analysis and Discussion 
Some points need to be further considered: 
(1) The dictionary can bring a big impact to 
the performance, as the LW candidates come 
from the dictionary. However a big dictionary 
can be easily acquired in the real application. 
(2) Due to our technical and insufficiently 
preparing problem, we use the PKU NER defini-
tion, however they seem not unified with the 
MSRA definition. 
(3) Our NER system is a word-based model, 
and we have find out that the word segmentation 
with two different dictionaries can bring a big 
impact to the NER performance. 
(4) We erase the new word recognition algo-
rithm in our system. While, we should explore 
the real annotated corpora, and add new word 
detection algorithm, if it has positive effect. e.g. 
???  ??(lotus prize) can be recognized as one 
word by the conditional random fields model. 
5 Conclusion 
We have briefly described our word segmenta-
tion system and NER system. We use word-
based features in the whole processing. Our sys-
tem has a good performance in terms of Riv 
measure, so this means that the trigram model 
with the smoothing algorithm can deal with the 
basic segmentation task well. However, the result 
in the bakeoff indicates that detecting out-of-
vocabulary word seems to be a harder task than 
dealing with the segmentation-ambiguity task. 
The work in the future will concentrate on two 
sides: improving the NER performance and add-
ing New Word Detection Algorithm. 
References 
HuaPing Zhang, Qun Liu etc. 2003. Chinese Lexical 
Analysis Using Hierarchical Hidden Markov 
Model, Second SIGHAN workshop affiliated with 
4th ACL, Sapporo Japan, pp.63-70. 
Jianfeng Gao, Mu Li et al 2004. Chinese Word Seg-
mentation: A Pragmatic Approach. MSR-TR-2004-
123, November 2004. 
Peng Fuchun, Fangfang Feng and Andrew McCallum. 
Chinese segmentation and new word detection us-
ing conditional random fields. In:COLING 2004. 
Stanley F.Chen and J. Goodman. 1999. An empirical 
study of smoothing techniques for language model-
ing. Computer Speech and Language. 13:369-394. 
Wei Jiang, Jian Zhao et al 2005A.Chinese Word 
Segmentation based on Mixing Model. 4th 
SIGHAN Workshop. pp. 180-182.  
Wei Jiang, Xiao-Long Wang, Yi Guan et al 2005B. 
applying rough sets in word segmentation disam-
biguation based on maximum entropy model. Jour-
nal of Harbin Institute of Technology (New Series). 
13(1): 94-98. 
Zhao Jian. 2006. Research on Conditional Probabilis-
tic Model and Its Application in Chinese Named 
Entity Recognition. Ph.D. Thesis. Harbin Institute 
of Technology, China. 
Zhao Yan. 2005. Research on Chinese Morpheme 
Analysis Based on Statistic Language Model. Ph.D. 
Thesis. Harbin Institute of Technology, China. 
192
