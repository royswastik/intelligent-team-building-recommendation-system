Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 151?158,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Compositional Approach toward Dynamic Phrasal Thesaurus
Atsushi Fujita Shuhei Kato Naoki Kato Satoshi Sato
Graduate School of Engineering, Nagoya University
{fujita,ssato}@nuee.nagoya-u.ac.jp
{shuhei,naoki}@sslab.nuee.nagoya-u.ac.jp
Abstract
To enhance the technology for computing
semantic equivalence, we introduce the no-
tion of phrasal thesaurus which is a natural
extension of conventional word-based the-
saurus. Among a variety of phrases that
conveys the same meaning, i.e., paraphrases,
we focus on syntactic variants that are com-
positionally explainable using a small num-
ber of atomic knowledge, and develop a sys-
tem which dynamically generates such vari-
ants. This paper describes the proposed sys-
tem and three sorts of knowledge developed
for dynamic phrasal thesaurus in Japanese:
(i) transformation pattern, (ii) generation
function, and (iii) lexical function.
1 Introduction
Linguistic expressions that convey the same mean-
ing are called paraphrases. Handling paraphrases
is one of the key issues in a broad range of nat-
ural language processing tasks, including machine
translation, information retrieval, information ex-
traction, question answering, summarization, text
mining, and natural language generation.
Conventional approaches to computing semantic
equivalence between two expressions are five-fold.
The first approximates it based on the similarities
between their constituent words. If two words be-
long to closer nodes in a thesaurus or semantic net-
work, they are considered more likely to be similar.
The second uses the family of tree kernels (Collins
and Duffy, 2001; Takahashi, 2005). The degree of
equivalence of two trees (sentences) is defined as
the number of common subtrees included in both
trees. The third estimates the equivalence based on
word alignment composed using templates or trans-
lation probabilities derived from a set of parallel
text (Barzilay and Lee, 2003; Brockett and Dolan,
2005). The fourth espouses the distributional hy-
pothesis (Harris, 1968): given two words are likely
to be equivalent if distributions of their surrounding
words are similar (Lin and Pantel, 2001; Weeds et
al., 2005). The final regards two expressions equiva-
lent if they can be associated by using a set of lexico-
syntactic paraphrase patterns (Mel?c?uk, 1996; Dras,
1999; Yoshikane et al, 1999; Takahashi, 2005).
Despite the results previous work has achieved,
no system that robustly recognizes and generates
paraphrases is established. We are not convinced of
a hypothesis underlying the word-based approaches
because the structure of words also conveys some
meaning. Even tree kernels, which take structures
into account, do not have a mechanism for iden-
tifying typical equivalents: e.g., dative alternation
and passivization, and abilities to generate para-
phrases. Contrary to the theoretical basis, the two
lines of corpus-based approaches have problems in
practice, i.e., data sparseness and computation cost.
The pattern-based approaches seem steadiest. Yet
no complete resource or methodology for handling
a wide variety of paraphrases has been developed.
On the basis of this recognition, we introduce the
notion of phrasal thesaurus to directly compute se-
mantic equivalence of phrases such as follows.
(1) a. be in our favor / be favorable for us
b. its reproducibility / if it is reproducible
c. decrease sharply / show a sharp decrease
d. investigate the cause of a fire /
investigate why there was a fire /
investigate what started a fire /
make an investigation into the cause of a fire
151
Phrasal thesaurus is a natural extension of conven-
tional word-based thesaurus. It is thus promised that
it will bring us the following benefits:
Enhancement of NLP applications: As conven-
tional thesauri, phrasal thesaurus must be
useful to handle paraphrases having different
structures in a wide range of NLP applications.
Reading and writing aids: Showing more appro-
priate alternative phrases must be a power-
ful aid at certain situations such as writing
text. Controlling readability of text by altering
phrases must also be beneficial to readers.
Our aim is to develop resources and mechanisms
for computing semantic equivalence on the working
hypothesis that phrase is the appropriate unit for that
purpose. This paper describes the first version of our
paraphrase generation system and reports on our on-
going work on constructing resources for realizing
phrasal thesaurus.
The following sections describe the range of phe-
nomena we treat (Section 2), the overall architec-
ture of our paraphrase generation system which
functions as phrasal thesaurus (Section 3), the im-
plementation of knowledge bases (Section 4) fol-
lowed by discussion (Section 5), and conclusion
(Section 6).
2 Dynamic phrasal thesaurus
2.1 Issue
Toward realizing phrasal thesaurus, the following
two issues should be discussed.
? What sorts of phrases should be treated
? How to cope with a variety of expressions
Although technologies of shallow parsing have
been dramatically improved in the last decade, it
is still difficult to represent arbitrary expression in
logical form. We therefore think it is reasonable to
define the range relying on lexico-syntactic struc-
ture instead of using particular semantic representa-
tion. According to the work of (Chklovski and Pan-
tel, 2004; Torisawa, 2006), predicate phrase (sim-
ple sentence) is a reasonable unit because it approx-
imately corresponds to the meaning of single event.
Combination of words and a variety of construc-
tion coerce us into handling an enormous number
of expressions than word-based approaches. One
may think taking phrase is like treading a thorny
path because one of the arguments in Section 1 is
about coverage. On this issue, we speculate that
one of the feasible approach to realize a robust sys-
tem is to divide phenomena into compositional and
non-compositional (idiosyncratic) ones1, and sepa-
rately develop resources to handle them as described
in (Fujita and Inui, 2005).
To compute semantic equivalence of idiosyncratic
paraphrases, pairs or groups of paraphrases have to
be statically compiled into a dictionary as word-
based thesaurus. The corpus-based approach is valu-
able for that purpose, although they are not guaran-
teed to collect all idiosyncratic paraphrases. On the
other hand, compositional paraphrases can be cap-
tured by a relatively small number of rules. Thus it
seems tolerable approach to generate them dynam-
ically by applying such rules. Our work is targeted
at compositional paraphrases and the system can be
called dynamic phrasal thesaurus. Hereafter, we
refer to paraphrases that are likely to be explained
compositionally as syntactic variants.
2.2 Target language: Japanese
While the discussion above does not depend on par-
ticular language, our implementation of dynamic
phrasal thesaurus is targeted at Japanese. Sev-
eral methods for paraphrasing Japanese predicate
phrases have been proposed (Kondo et al, 1999;
Kondo et al, 2001; Kaji et al, 2002; Fujita et al,
2005). The range they treat is, however, relatively
narrow because they tend to focus on particular para-
phrase phenomena or to rely on existing resources.
On the other hand, we define the range of phenom-
ena from a top-down viewpoint. As a concrete defi-
nition of predicate phrase in Japanese,
noun phrase + case marker + predicate
is employed which is hereafter referred to ?phrase.?
Noun phrase and predicate in Japanese them-
selves subcategorize various syntactic variants as
shown in Figure 1 and paraphrase phenomena for
above phrase also involve those focused on their in-
teraction. Thus the range of phenomena is not so
narrow, and intriguing ones, such as shown in exam-
ples2 (2) and (3), are included.
1We regard lexical paraphrases (e.g., ?scope? ? ?range?)
and idiomatic paraphrases (e.g., ?get the sack?? ?be dismissed
from employment?) as idiosyncratic.
2In each example, ?s? and ?t? denote an original sentence
and its paraphrase, respectively. SMALLCAPS strings indicate
the syntactic role of their corresponding Japanese expressions.
[N] indicates a nominalizer.
152
(2) Head switching
s. kakunin-o isogu.
checking-ACC to hurry-PRES
We hurry checking it.
t. isoide kakunin-suru.
in a hurry to check-PRES
We check it in a hurry.
(3) Noun phrase ? sub-clause
s. kekka-no saigensei-o kenshou-suru.
result-GEN reproducibility-ACC to validate-PRES
We validate its reproducibility.
t. [ kekka-o saigen-dekiru ]
result-ACC to reproduce-to be able
ka-douka-o kenshou-suru.
[N]-whether-ACC to validate-PRES
We validate whether it is reproducible.
We focus on syntactic variants at least one side of
which is subcategorized into the definition of phrase
above. For the sake of simplicity, we hereafter rep-
resent those expressions using part-of-speech (POS)
patterns. For instance, (2s) is called N : C : V type,
and (3s) is N
1
: no : N
2
: C : V type.
3 Paraphrase generation system
Given a phrase, the proposed system generates its
syntactic variants in the following four steps:
1. Morphological analysis
2. Syntactic transformation
3. Surface generation with lexical choice
4. SLM-based filtering
where no particular domain, occasion, and media is
assumed3. Candidates of syntactic variants are first
over-generated in step 2 and then anomalies among
them are filtered out in steps 3 and 4 using rule-based
lexical choice and statistical language model.
The rest of this section elaborates on each compo-
nent in turn.
3.1 Morphological analysis
Technologies of morphological analysis in Japanese
have matured by introducing machine learning tech-
niques and large-scale annotated corpus, and there
are freely available tools. Since the structure of input
phrase is assumed to be quite simple, employment of
dependency analyzer was put off. We simply use a
morphological analyzer MeCab4.
3This corresponds to the linguistic transformation layer of
KURA (Takahashi et al, 2001).
4http://mecab.sourceforge.net/
noun phrase
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:
formal noun
8
<
:
?koto?
?mono?
?no?
content
8
>
>
>
>
<
>
>
>
>
:
single word
compound
j
N
1
N
2
N + suffixes
modified
8
>
<
>
:
N
1
+ ?no? +N
2
Adj+N
Adjectival verb+N
clause+N
predicate
8
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
:
verb phrase
8
>
>
>
>
>
<
>
>
>
>
>
:
single word
8
>
>
<
>
>
:
original verb
Sino-Japanese verb
lexical compound
light verb
Adv+ ?suru?
compound
8
>
<
>
:
original + original
Sino + original
Sino + Sino
N + Sino
Adj
j
single word
compound
Adjectival verb+ ?da?
Adv+ ?da?
Copula
Figure 1: Classification of syntactic variants of noun
phrase and predicate in Japanese.
Our system has a post-analysis processing. If ei-
ther of Sino-Japanese verbal nouns (e.g., ?kenshou
(validation)? and ?kandou (impression)?) or translit-
eration of verbs in foreign language (e.g., ?doraibu
(to drive)? and ?shifuto (to shift)?) is immediately
followed by ?suru (to do)? or ?dekiru (to be able),?
these adjacent two morphemes are joined into a sin-
gle morpheme to avoid incorrect transformation.
3.2 Syntactic transformation
The second step over-generates syntactic variants
using the following three sorts of knowledge:
(i) Transformation pattern: It gives skeletons of
syntactic variants. Each variant is represented
by POS symbols designating the input con-
stituents and triggers of the generation function
and lexical function below.
(ii) Generation function: It enumerates different
expressions that are constituted with the same
set of words and subcategorized into the re-
quired syntactic category. Some of generation
functions handle base phrases, while the rest
generates functional words. Base phrases the
former generates are smaller than that transfor-
mation patterns treat. Since some functional
words are disjunctive, the latter generates all
candidates with a separator ?/? and leaves the
selection to the following step.
153
Table 1: Grammar in Backus-Naur form, example, and instantiation for each knowledge.
Knowledge type Grammar / Example / Instantiation
(i) Transformation <transformation pattern> ::= <left pattern> ? <right pattern>
pattern <left pattern> ::= (<POS symbol>|<word form>)+
<POS symbol> ::= (N |C|V |Adj|Adv)
<word form> ::= (<hiragana>|<katakana>|<kanji>)+
<right pattern> ::=
(<POS symbol>|<word form>|<function definition>|<lexical function>)+
(a) N : C : V ? adv(V ) : vp(N)
(b) N
1
: no : N
2
: C : V ?N
1
: genCase() : vp(N
2
) : ka-douka : C : V
(a) kakunin : o : isogu ? adv (isogu) : vp(kakunin)
checking ACC to hurry adv(to hurry) vp(checking)
(b) kekka : no : saigensei : o : kenshou-suru
result GEN reproducibility ACC to validate-PRES
? kekka : genCase() : vp(saigensei) : ka-douka : o : kenshou-suru
result case marker vp(reproducibility) [N]-whether ACC to validate-PRES
(ii) Generation <generation function> ::= <function definition> ? ?{?<right pattern>+?}?
function <function definition> ::= <syntactic category>?(?<POS symbol>*?)?
<syntactic category> ::= (np | vp | lvc)
(a) vp(N) ? {v(N) : genVoice() : genTense()}
(b) np(N
1
, N
2
) ? {N
1
, N
2
, N
1
: N
2
, N
1
: no : N
2
, vp(N
1
) : N
2
,wh(N
2
) : vp(N
1
) : ka, . . .}
(a) vp(kakunin) ? { v(kakunin) : genVoice() : genTense() }
vp(verification) v(verification) verbal suffix for voice verbal suffix for tense
(b) np(shukka, gen-in)
np(starting fire, reason)
? { shukka , gen-in , shukka : gen-in , shukka : no : gen-in ,
starting fire reason starting fire reason starting fire GEN reason
vp(shukka) : gen-in , wh(gen-in) : vp(shukka) : ka , . . . }
vp(starting fire) reason wh(reason) vp(starting fire) [N]
(iii) Lexical <lexical function> ::= <relation>?(?<POS symbol>?)?
function <relation> ::= (n | v | adj | adjv | adv | wh)
(a) adv(V )
(b) wh(N)
(a) adv (isogu)
adv(to hurry)
?
? isoide
in a hurry
(given by a verb?adverb dictionary)
?
(b) wh(gen-in)
wh(reason)
?
? { naze , doushite }
why why
(given by a noun?interrogative dictionary)
?
(iii) Lexical function: It generates different lexi-
cal items in certain semantic relations, such
as derivative form, from a given lexical item.
The back-end of this knowledge is a set
of pre-compiled dictionaries as described in
Section 4.2.
Table 1 gives a summary of grammar in Backus-
Naur form, examples, and instantiations of each
knowledge. Figure 2 illustrates an example of
knowledge application flow for transforming (4s)
into (4t), where ?:? denotes delimiter of con-
stituents.
(4) s. ?kakunin:o:isogu?
t. ?isoide:{kakunin-suru:
{?, reru/rareru, seru/saseru}:{?, ta/da}}?
First, transformation patterns that match to the given
input are applied. Then, the skeletons of syntactic
variants given by the pattern are lexicalized by con-
secutively invoking generation functions and lexical
functions. Plural number of expressions that gen-
eration function and lexical function generate are
enumerated within curly brackets. Transformation
is ended when the skeletons are fully lexicalized.
In fact, knowledge design for realizing the trans-
formation is not really new, because we have been
inspired by the previous pattern-based approaches.
Transformation pattern is thus alike that in the
Meaning-Text Theory (MTT) (Mel?c?uk, 1996), Syn-
chronous Tree Adjoining Grammar (STAG) (Dras,
1999), meta-rule for Fastr (Yoshikane et al, 1999),
154
{v(kakunin) : genVoice() : genTense()}
okakunin
N
:
C
: isogu
V
Trans. Pat.
N:C:V? adv(V):vp(N)
adv(isogu) : vp(kakunin)
Gen. Func.
vp(N)
kakunin-suru
Lex. Func.
v(N)
Gen. Func.
genVoice()
Gen. Func.
genTense()
isoide
Lex. Func.
adv(V)
{?, reru/rareru, seru/saseru} {?, ta/da}
isoide : {kakunin-suru : {?, reru/rareru, seru/saseru} : {?, ta/da}}
Figure 2: Syntactic transformation (for (2)).
and transfer pattern for KURA (Takahashi et al,
2001). Lexical function is also alike that in MTT.
However, our aim in this research is beyond the
design. In other words, as described in Section 1,
we are aiming at the following two: to develop re-
sources for handling syntactic variants in Japanese,
and to confirm if phrasal thesaurus really contribute
to computing semantic equivalence.
3.3 Surface generation with lexical choice
The input of the third component is a bunch of candi-
date phrases such as shown in (4t). This component
does the following three processes in turn:
Step 1. Unfolding: All word sequences are gener-
ated by removing curly brackets one by one.
Step 2. Lexical choice: Disjunctive words are con-
catenated with ?/? (e.g., ?reru/rareru? in (4t)).
One of them is selected based on POS and con-
jugation types of the preceding word.
Step 3. Conjugation: In the transformation step,
conjugative words are moved to different po-
sitions and some of them are newly generated.
Inappropriate conjugation forms are corrected.
3.4 SLM-based filtering
In the final step, we assess the correctness of each
candidate of syntactic variants using a statistical lan-
guage model. Our model simply rejects candidate
phrases that never appear in a large size of raw text
corpus consisting of 15 years of newspaper articles
(Mainichi 1991?2005, approximately 1.8GB). Al-
though it is said that Japanese language has a degree
N:C:V
N1:N2:C:V+N
N:C:V1:V2
+V
N:C:Adv:V+Adv
Adj:N:C:V
+Adj
N:C:Adj
switch V with Adj
Figure 3: Derivations of phrase types.
of freedom in word ordering, current implementa-
tion does not yet employ structured language models
because phrases we handle are simple.
4 Knowledge implementation
4.1 Transformation patterns and generation
functions
An issue of developing resources is how to ensure
their coverage. Our approach to this issue is to de-
scribe transformation patterns by extending those for
simpler phrases. We first described following three
patterns for N : C : V type phrases which we con-
sider the simplest according to Figure 1.
(5) a. N : C : V ? vp(N)
b. N : C : V ? N : genCase() : lvc(V )
c. N : C : V ? adv(V ) : vp(N)
While the pattern (5c) is induced from example (2),
the patterns (5a-b) are derived from examples (6)
and (7), respectively.
(6) s. shigeki-o ukeru
inspiration-ACC to receive
to receive an inspiration
t. shigeki-sareru
to inspire-PASS
to be inspired
(7) s. hada-o shigeki-suru
skin-ACC to stimulate
to stimulate skin
t. hada-ni shigeki-o ataeru
skin-DAT stimulus-ACC to give
to give skin a stimulus
Regarding the patterns in (8) as the entire set of
compositional paraphrases for N : C : V type
phrases, we then extended them to a bit more com-
plex phrases as in Figure 3. For instance, 10 patterns
155
Table 2: Transformation patterns.
Target phrase # of patterns
N : C : V 3
N
1
: N
2
: C : V 10
N : C : V
1
: V
2
10
N : C : Adv : V 7
Adj : N : C : V 4
N : C : Adj 3
Total 37
Table 3: Generation functions.
Definition Syntactic category # of returned value
np(N
1
, N
2
) noun phrase 9
vp(N) verb phrase 1
vp(N
1
, N
2
) verb phrase 2
vp(V
1
, V
2
) verb phrase 3
lvc(V ) light verb construction 1
genCase() case marker 4
genVoice() verbal suffix for voice 3
genTense() verbal suffix for tense 2
genAspect () verbal suffix for aspect 2
for N
1
: N
2
: C : V type phrases shown in (8) have
been described based on patterns in (5), mainly fo-
cusing on interactions between newly introduced N
1
and other constituents.
(8) a. N
1
: N
2
: C : V ? vp(N
1
, N
2
) (5a)
b. N
1
: N
2
: C : V ?
N
1
: genCase() : vp(N
2
) (5a)
c. N
1
: N
2
: C : V ?
N
2
: genCase() : vp(N
1
) (5a)
d. N
1
: N
2
: C : V ?
np(N
1
, N
2
) : genCase() : lvc(V ) (5b)
e. N
1
: N
2
: C : V ? N
1
: genCase() :
N
2
: genCase() : lvc(V ) (5b)
f. N
1
: N
2
: C : V ? N
2
: genCase() :
N
1
: genCase() : lvc(V ) (5b)
g. N
1
: N
2
: C : V ?
adv (V ) : vp(N
1
, N
2
) (5c)
h. N
1
: N
2
: C : V ?
adv (V ) : N
1
: genCase() : vp(N
2
) (5c)
i. N
1
: N
2
: C : V ?
adv (V ) : N
2
: genCase() : vp(N
1
) (5c)
j. N
1
: N
2
: C : V ?
np(N
1
, N
2
) : C : V (new)
The number of transformation patterns we have so
far developed is shown in Table 2.
Generation functions shown in Table 3 are devel-
oped along with creating transformation patterns.
Although this is the heart of the proposed model,
two problems are remained: (i) the granularity of
each generation function is determined according to
Table 4: Dictionaries for lexical functions.
ID POS-pair |D| |C| |D ? C| |J |
(a) noun?verb 3,431 - 3,431 3,431
(b) noun?adjective 308 667 906 475 ?
(c) noun?adjectival verb 1,579 - 1,579 1,579
(d) noun?adverb 271 - 271 271
(e) verb?adjective 252 - 252 192 ?
(f) verb?adjectival verb 74 - 74 68 ?
(g) verb?adverb 74 - 74 64 ?
(h) adjective?adjectival verb 66 95 159 146 ?
(i) adjective?adverb 33 - 33 26 ?
(j) adjectival verb?adverb 70 - 70 70
Total 6,158 762 6,849 6,322
our linguistic intuition, and (ii) they do not ensure of
generating all possible phrases. Therefore, we have
to establish the methodology to create this knowl-
edge more precisely.
4.2 Lexical functions
Except wh(N), which generates interrogatives as
shown in the bottom line of Table 1, the relations
we have so far implemented are lexical derivations.
These roughly correspond to S, V, A, and Adv in
MTT. The back-end of these lexical functions is a
set of dictionaries built by the following two steps:
Step 1. Automatic candidate collection: Most
derivatives in Japanese share the beginning
of words and are characterized by the corre-
spondences of their suffixes. For example,
?amai (be sweet)? and ?amami (sweetness)?
has a typical suffix correspondence ??-i:?-mi?
of adjective?noun derivation. Using this clue,
candidates are collected by two methods.
? From dictionary: Retrieve all word pairs from
the given set of words those satisfying the
following four conditions: (i) beginning with
kanji character, (ii) having different POSs,
(iii) sharing at least the first character and the
first sound, and (iv) having a suffix pattern
which corresponds to at least two pairs.
? Using dictionary and corpus: Generate candi-
dates from a set of words by applying a set of
typical suffix patterns, and then check if each
candidate is an actual word using corpus. This
is based on (Langkilde and Knight, 1998).
Step 2. Manual selection: The set of word pairs
collected in the previous step includes those do
not have particular semantic relationship. This
step involves human to discard noises.
156
Table 4 shows the size of 10 dictionaries, where
each column denotes the number of word pairs re-
trieved from IPADIC5 (|D|), those using IPADIC,
seven patterns and the same corpus as in Section 3.4
(|C|), their union (|D ? C|), and those manu-
ally judged correct (|J |), respectively. The sets of
word pairs J are used as bi-directional lexical func-
tions, although manual screening for four dictionar-
ies without dagger (?) are still in process.
5 Discussion
5.1 Unit of processing
The working hypothesis underlying our work is that
phrase is the appropriate unit for computing seman-
tic equivalence. In addition to the arguments in
Section 1, the hypothesis is supported by what is
done in practice. Let us see two related fields.
The first is the task of word sense disambigua-
tion (WSD). State-of-the-art WSD techniques refer
to context as a clue. However, the range of context
is usually not so wide: words and their POSs within
small window centered the target word and content
words within the same sentence of the target word.
The task therefore can be viewed as determining the
meaning of phrase based on its constituent words
and surrounding content words.
Statistical language model (SLM) is another field.
SLMs usually deal with various things within word
sequence (or structure) at the same time. How-
ever, relations within a phrase should be differen-
tiated from that between phrases, because checking
the former is for grammaticality, while the latter for
cohesion. We think SLMs should take the phrase to
determine boundaries for assessing the correctness
of generated expressions more accurately.
5.2 Compositionality
We examined how large part of manually created
paraphrases could be generated in our compositional
approach. First, a set of paraphrase examples were
created in the following procedure:
Step 1. Most frequent 400 phrases typed N
1
: N
2
:
C : V were sampled from one year of newspa-
per articles (Mainichi 1991).
Step 2. An annotator produced paraphrases for each
phrase. We allowed to record more than one
5http://mecab.sourceforge.jp/
paraphrase for a given phrase and to give up
producing paraphrases. As a result, we ob-
tained 211 paraphrases for 170 input phrases.
Manual classification revealed that 42% (88 / 211)
of paraphrases could be compositionally explain-
able, and the (theoretical) coverage increases to 86%
(182 / 211) if we have a synonym dictionary. This
ratio is enough high to give these phenomena pref-
erence as the research target, although we cannot re-
ject a possibility that data has been biased.
5.3 Sufficient condition of equivalence
In our system, transformation patterns and genera-
tion functions offer necessary conditions for gener-
ating syntactic variants for given input. However,
we have no sufficient condition to control the appli-
cation of such a knowledge.
It has not been thoroughly clarified what clue can
be sufficient condition to ensure semantic equiva-
lence, even in a number of previous work. Though,
at least, roles of participants in the event have to be
preserved by some means, such as the way presented
in (Pantel et al, 2007). Kaji et al (2002) introduced
a method of case frame alignment in paraphrase gen-
eration. In the model, arguments of main verb in the
source are taken over by that of the target according
to the similarities between arguments of the source
and target. Fujita et al (2005) employed a semantic
representation of verb to realize the alignment of the
role of participants governed by the source and tar-
get verbs. According to an empirical experiment in
(Fujita et al, 2005), statistical language models do
not contribute to calculating semantic equivalence,
but to filtering out anomalies. We therefore plan to
incorporate above alignment-based models into our
system, for example, within or after the syntactic
transformation step (Figure 2).
5.4 Ideas for improvement
The knowledge and system presented in Section 3
are quite simple. Thus the following features should
be incorporated to improve the system in addition to
the one described in Section 5.3.
? Dependency structure: To enable flexible
matching, such as Adv : N : C : V type input
and transformation pattern for N : C : Adv :
V type phrases.
? Sophisticated SLM: The generation phase
should also take the structure into account to
157
evaluate generated expressions flexibly.
? Knowledge development: Although we have
not done intrinsic evaluation of knowledge, we
are aware of its incompleteness. We are con-
tinuing manual screening for the dictionaries
and planning to enhance the methodology of
knowledge development.
6 Conclusion
To enhance the technology for computing seman-
tic equivalence, we have introduced the notion of
phrasal thesaurus, which is a natural extension of
conventional word-based thesaurus. Plausibility of
taking phrase as the unit of processing has been dis-
cussed from several viewpoints. On the basis of
that, we have been developing a system to dynam-
ically generate syntactic variants in Japanese predi-
cate phrases which utilizes three sorts of knowledge
that are inspired by MTT, STAG, Fastr, and KURA.
Future work includes implementing more precise
features and larger resources to compute semantic
equivalence. We also plan to conduct an empirical
evaluation of the resources and the overall system.
Acknowledgments
This work was supported in part by MEXT Grants-
in-Aid for Young Scientists (B) 18700143, and for
Scientific Research (A) 16200009, Japan.
References
Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase:
an unsupervised approach using multiple-sequence align-
ment. In Proceedings of the 2003 Human Language Tech-
nology Conference and the North American Chapter of the
Association for Computational Linguistics (HLT-NAACL),
pages 16?23.
Chris Brockett and William B. Dolan. 2005. Support Vector
Machines for paraphrase identification and corpus construc-
tion. In Proceedings of the 3rd International Workshop on
Paraphrasing (IWP), pages 1?8.
Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: min-
ing the Web for fine-grained semantic verb relations. In Pro-
ceedings of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 33?40.
Michael Collins and Nigel Duffy. 2001. Convolution kernels
for natural language. In Advances in Neural Information
Processing Systems 14: Proceedings of the 2001 Confer-
ence, pages 625?632.
Mark Dras. 1999. Tree adjoining grammar and the reluctant
paraphrasing of text. Ph.D. thesis, Division of Information
and Communication Science, Macquarie University.
Atsushi Fujita, Kentaro Inui, and Yuji Matsumoto. 2005. Ex-
ploiting Lexical Conceptual Structure for paraphrase gener-
ation. In Proceedings of the 2nd International Joint Con-
ference on Natural Language Processing (IJCNLP), pages
908?919.
Atsushi Fujita and Kentaro Inui. 2005. A class-oriented ap-
proach to building a paraphrase corpus. In Proceedings
of the 3rd International Workshop on Paraphrasing (IWP),
pages 25?32.
Zellig Harris. 1968. Mathematical structures of language.
New York: Interscience Publishers.
Nobuhiro Kaji, Daisuke Kawahara, Sadao Kurohashi, and
Satoshi Sato. 2002. Verb paraphrase based on case frame
alignment. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL), pages
215?222.
Keiko Kondo, Satoshi Sato, and Manabu Okumura. 1999.
Paraphrasing of ?sahen-noun + suru?. IPSJ Journal,
40(11):4064?4074. (in Japanese).
Keiko Kondo, Satoshi Sato, and Manabu Okumura. 2001.
Paraphrasing by case alternation. IPSJ Journal, 42(3):465?
477. (in Japanese).
Irene Langkilde and Kevin Knight. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In Proceedings of
the 36th Annual Meeting of the Association for Computa-
tional Linguistics and the 17th International Conference on
Computational Linguistics (COLING-ACL), pages 704?710.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question answering. Natural Language Engineer-
ing, 7(4):343?360.
Igor Mel?c?uk. 1996. Lexical functions: a tool for the descrip-
tion of lexical relations in a lexicon. In Leo Wanner, editor,
Lexical Functions in Lexicography and Natural Language
Processing, pages 37?102. John Benjamin Publishing Com-
pany.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy
Chklovski, and Eduard Hovy. 2007. Isp: Learning infer-
ential selectional preferences. In Proceedings of Human
Language Technologies 2007: The Conference of the North
American Chapter of the Association for Computational Lin-
guistics (NAACL-HLT), pages 564?571.
Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, Atsushi Fujita,
and Kentaro Inui. 2001. KURA: a transfer-based lexico-
structural paraphrasing engine. In Proceedings of the 6th
Natural Language Processing Pacific Rim Symposium (NL-
PRS) Workshop on Automatic Paraphrasing: Theories and
Applications, pages 37?46.
Tetsuro Takahashi. 2005. Computation of semantic equiva-
lence for question answering. Ph.D. thesis, Graduate School
of Information Science, Nara Institute of Science and Tech-
nology.
Kentaro Torisawa. 2006. Acquiring inference rules with tem-
poral constraints by using Japanese coordinated sentences
and noun-verb co-occurrences. In Proceedings of the Hu-
man Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Linguis-
tics (HLT-NAACL), pages 57?64.
Julie Weeds, David Weir, and Bill Keller. 2005. The distribu-
tional similarity of sub-parses. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equivalence
and Entailment, pages 7?12.
Fuyuki Yoshikane, Keita Tsuji, Kyo Kageura, and Christian
Jacquemin. 1999. Detecting Japanese term variation in tex-
tual corpus. In Proceedings of the 4th International Work-
shop on Information Retrieval with Asian Languages, pages
97?108.
158
