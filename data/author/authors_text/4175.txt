Pseudo Relevance Feedback Method Based on Taylor Expansion of Re-
trieval Function in NTCIR-3 Patent Retrieval Task 
 Kazuaki KISHIDA 
Faculty of Cultural Information Resources 
Surugadai University 
698 Azu, Hanno, Saitama 357-8555 JAPAN 
kishida@surugadai.ac.jp 
 
 
Abstract 
Pseudo relevance feedback is empirically 
known as a useful method for enhancing 
retrieval performance. For example, we 
can apply the Rocchio method, which is 
well-known relevance feedback method, 
to the results of an initial search by as-
suming that the top-ranked documents are 
relevant. In this paper, for searching the 
NTCIR-3 patent test collection through 
pseudo feedback, we employ two rele-
vance feedback mechanism; (1) the Roc-
chio method, and (2) a new method that is 
based on Taylor formula of linear search 
functions. The test collection consists of 
near 700,000 records including full text of 
Japanese patent materials. Unfortunately, 
effectiveness of our pseudo feedback 
methods was not empirically observed at 
all in the experiment.  
1 Introduction 
Relevance feedback is widely recognized as an 
effective method for improving retrieval effective-
ness in the context of interactive IR. As often 
pointed out, it is difficult for users to represent 
their own information needs into a well-defined set 
of search terms or statements. The resulting short 
or poor queries would bring them unsatisfactory 
results. However, if a few relevant documents hap-
pen to be found by the search, we could automati-
cally or manually extract some useful terms from 
the documents, and add them to the initial search 
expression. It is obviously expected that search 
effectiveness of the second search using the ex-
tended query will be improved significantly. This 
is a basic idea of relevance feedback. 
Inevitably, for executing automatic relevance 
feedback, the system has to obtain relevance in-
formation, i.e., relevant or irrelevant documents, 
from the users interactively. However, some re-
searchers have tried to employ relevance feedback 
techniques with no relevance information. The ob-
jective is to enhance search performance of re-
trieval models such as vector space model, 
probabilistic model and so on, without interaction 
on relevance information between system and us-
ers. The technique is usually called pseudo rele-
vance feedback, in which a standard feedback 
method (e.g., the Rocchio method) is applied by 
assuming that top-ranked documents searched by 
the initial search are relevant. 
The purpose of this paper is to report results of 
retrieval experiments for examining effectiveness 
of pseudo relevance feedback in the case of search-
ing a patent collection. In particular, we attempt to 
compare search performance of the traditional 
Rocchio method with that of an alternative method, 
which is based on Taylor approximation of re-
trieval function proposed by Kishida[1]. This re-
port is based on two experiments using the 
NTCIR-1 test collection and the NTCIR-3 patent 
test collection, respectively. As to the latter, the 
results were obtained at the time of NTCIR-3 
Workshop held in October 2002 [2]. 
The rest of this paper is organized as follows. In 
Section 2, the Rocchio method and an alternative 
method proposed by Kishida[1] will be introduced. 
In Section 3 a preliminary experiment for confirm-
ing how well the alternative method works in a 
normal relevance feedback situation will be de-
scribed. The NTCIR-1 test collection with rele-
vance judgment information is used for the 
preliminary experiment. In Section 4, results of an 
experiment on pseudo relevance feedback method 
using the NTCIR-3 patent test collection will be 
shown.  
2 
2.1 
Relevance Feedback Methods 
Rocchio Method 
The most typical approach to relevance feedback 
would be so-called the Rocchio method [3]. A ba-
sic idea of the method is to add an average weight 
of each term within a set of relevant documents to 
the original query vector, and to subtract an aver-
age weight within a set of irrelevant ones from the 
vector. 
We denote a document vector and a query vec-
tor by d  and q , where 
 is a weight of a term within a document and  
is a weight of a term within the query (
i i iM
Tw w= ( ,..., )1 = ( ,..., )w wq qM T1
wij wqj
M  is the 
total number of distinct terms in the database, and 
T  indicates transposition). 
A modified query vector is obtained by a for-
mula, 
??
??
?+=
Ddi
i
Ddi
i
ii DD ::
~ ddqq ??? ,                           (1) 
where D  is the set of relevant documents, D  is the 
set of irrelevant documents, and ? ? ??and ?  
are constants. 
It has been empirically shown that the perform-
ance of the Rocchio method is very good [4], and 
in recent, many researchers have examined the 
method directly or indirectly [5-8]. Also, due to its 
effectiveness and simplicity, the Rocchio method 
has been widely applied in other research areas, for 
example, image retrieval [0] or text categorization 
[10]. 
2.2 Feedback Method Using Taylor Formula 
of Retrieval Function 
Kishida[1] has proposed an alternative relevance 
feedback method, which is suitable for the situa-
tion that the degree of relevance is given as a nu-
merical value, not dichotomous value (i.e., 
relevance or not), from actual users. In this section, 
according to Kishida[1], the method will be ex-
plained. 
In vector space model [10], typical formulas for 
determining term weights are as follows:  
w xij ij= +log .10                                                 (2) 
( )w x Nqj qj j= +(log . ) log10 n                              (3) 
where 
xij : frequency of a term  in a document , t j di
xqj : frequency of a term t  in the query, j
n j  : the number of documents including t , j
N  : the total number of documents in the data-
base. 
For calculating the degree of similarity between 
a document vector d  and the query vector q , a 
cosine formula is normally used: 
i
s w w wi ij qjj
M
ij qjj
M
j
M= = =? ?1 2 11 w=? 2                      (4) 
where  is a numerical score indicating similarity 
of the document given a query vector. 
si
On the other hand, a well-known formula based 
on probabilistic model derived from an assumption 
of two-Poisson distribution [12] is 
? = ???
?
++=
M
j
iji
ij
i xll
x
s
1 )5.15.0(
0.3
???
?
+
+???
5.0
5.0
log
j
j
qj n
nN
x  
(5) 
where 
l xi ij
M= =? 1 j , and l N , liiN= ? =?1 1
i.e., the former is a document length, and the latter 
is an average of the length over documents within 
the database. The formula (5) is a version of so-
called Okapi weighting [12] under a particular set-
ting of its parameters. 
We can represent concisely the two important 
retrieval models as a linear function of vector, 
s b Ab= =f ( ) ,                                               (6) 
where  is a s N  dimensional vector of document 
scores, s ,  is a linear function of vec-
tor ( ), and 
= ( , )sN T1
M N?
...,s
R?1
f
1f R: ? A  is a N M?  matrix of 
which each element is 
? = ++= Mj ijijij xxa 1 2)0.1(log)0.1(log ,           (7) 
in the case of vector space model (see (2) and (4)), 
or 
iji
ij
ij xll
x
a ++= )5.15.0(
0.3                                     (8) 
in the case of the Okapi formula (see (5)). 
Also,  is a b M  dimensional vector of which 
each element is defined as 
b w wj qj qj
M= =? 21 j                                          (9) 
where (w x Nqj qj j= +(log . ) log10 )n  in the case of 
vector space model (see (3)), or 
An approach to estimating ~b  is to pay our at-
tention to a difference between initial score  
and secondary score 
f X ( )b
f X (
~)b , and to apply so-called 
Taylor approximation for obtaining a vector func-
tion f X (
~)b , i.e., 
b x
N n
nj qj
j
j
= ? ++log
.
.
0 5
0 5
                                   (10) 
in the case of the Okapi formula (see (5)). 
The most important thing is that both of two 
well-known formulas for estimating scores to rank 
documents are able to be represented by a simple 
form (6). f f
f KX X X T(
~) ( ) ( ) (~ )b b b
b
b b= + ? +??
,               (13) 
For making ranked output, documents have to 
be sorted in the decreasing order of scores, s  
( i
i
N= 1,..., ). This means that each score is assumed 
to indicate the degree of relevance. In other words, 
the score is expected to be an estimate of ?true? 
degree of relevance . ri
where K  is a residual term (see [13]). If we em-
ploy (11) and assume that 
rX  = , )~(bXf
according to a target condition (12), we can obtain 
that 
~ (b b A r s= + ??X X X1 ) ,                                     (14) 
Let be a vector representing ?true? 
relevance degrees. By using this notation, we can 
describe operationally a purpose of retrieval sys-
tem as ?to estimate a vector s  that is the closest to 
vector r  when a search request is given.? 
r = ( ,..., )r rN T1 (see Appendix for detail calculation). It should be 
noted that 0=K  due to the linearity of Equation 
(11). This means (14) is not an approximation but 
an exact relation. 
The Equation (14) contains an abnormal inverse 
matrix , which is an A X?1 M n?  matrix and 
 where  is aA AX X? =1 I M I M M M?  matrix of which 
all diagonal elements are 1 and others are 0. Using 
singular value decomposition (SVD), transpose 
matrix of  can be represented as A X
Of course, r  is unknown in real situations, but 
it is possible to obtain information on a part of r  
through the process of relevance feedback. For ex-
ample, if a user replies a set of scores indicating 
the degrees of relevance for top-ranked n  docu-
ments searched by the initial query, the scores al-
low us to estimate the part of r  corresponding to 
the n  documents 
A U VX
T T= ? , 
where 
U : an M n?  orthogonal matrix, We denote a set of the top-ranked n  documents 
by X  and the part of r  corresponding to the set X  
by , which is actually n  dimensional vector re-
constructed by extracting n  elements of the docu-
ments from the original vector r . According to (6), 
we can write that 
rX
? : an n n?  diagonal matrix, and  
V : an n n?  orthogonal matrix. 
By employing the decomposition, we can finally 
represent (14) such as 
~ (b b U V r s= + ??? 1 T X X )
3 
3.1 
                              (15) 
(see Appendix for details). This is a final formula 
of our relevance feedback algorithm. For conven-
ience, we call the algorithm ?the Taylor formula 
based method? in this paper. 
s b AX X Xf= =( ) b ,                                       (11) 
where 
A X : an n M?  matrix, 
s X : an n  dimensional vector,  and 
f R RX
M n: ? ??1 1 . Preliminary Experiment with Relevance 
Information Both of the matrix and the vector are constructed 
by the same way with . rX
If we establish a distance measure ?  between 
 and s , the objective of relevance feedback can 
be formally described as follows: the relevance 
feedback aims at estimating a modified query vec-
tor such that 
rX X
Purpose and Test Data 
Before applying pseudo relevance feedback 
based on the Equation (15) to the patent test collec-
tion, we try checking retrieval performance of the 
Taylor formula based method by using other test 
collection with relevance judgment information. 
To do this, we employ a well-known Japanese Test 
Collection NTCIR-1 (NII/NACSIS Test Collection 
~ arg min ( , )b r
b
= ? X Xs = arg min ( , ( ))
b
r b? X Xf .     (12) 
Then we can use ~b  for the secondary search. 
for Information Retrieval - 1)1, which consists of 
about 330,000 bibliographic records of proceed-
ings at conferences held in Japan. It should be 
noted that, in the preliminary experiment, rele-
vance judgment information was used (i.e., not 
pseudo feedback). 
- In the case of runs based on the Taylor 
formula, TYLVEC and TYLPRB, the 
linear function (6) was used for matching 
operation. 
Fifty-three topics of NTCIR-1 were employed 
for the experiment (from No.31 to No.83). The 
format of these topics is also very similar with that 
of TREC, i.e., a record of each topic consists of 
fields of <title>, <description>, <narrative> and so 
on. For realistic situation in which feedback meth-
ods are used, it would be more reasonable to as-
sume that original search statements are short. 
Thus we employed only <title> and <description> 
fields for representing each topic. This means that 
a kind of ?short query? was used for the experiment. 
3.2 Procedure and type of runs 
Procedure of the preliminary experiment is as fol-
lows: 
(a) Initial search: two initial search runs were car-
ried out, i.e., the first is based on vector space 
model from (2) to (4) and the second is 
probabilistic model (5). We denote the initial 
search runs as ORGVEC and ORGPRB, re-
spectively. 
(b) Query modification through relevance feed-
back: initial queries were modified by using 
relevance judgment information on top-ranked 
 documents of each initial run. In this paper, 
we set 10 and 20 as the value of n , 
n
- In the case of vector space model, we can 
attempt two modification methods, i.e., 
the Rocchio method (1) (where ? = 8 , 
? = 16 and? = 4 ) and the Taylor formula 
based method (7), (9) and (15). The run 
using the Rocchio method is denoted as 
ROCCHI, and the run by the Taylor for-
mula based method as TYLVEC. 
- In the case of probabilistic model, only 
the Taylor formula based method was 
applied using (8), (10) and (15). We de-
note this run as TYLPRB. 
(c) Secondary search: each modified query was 
used for second run 
- In the case of ROCCHI, modified queries 
were matched with document vectors by 
cosine formula (4). 
                                                          
3.3 Conversion of binary judgment into con-
tinuous value 
One of the advantages of the Taylor formula based 
method (15) is to allow us for making use of con-
tinuous values representing the degree to which 
each document is relevant. Unfortunately, in the 
experiment, such values were not available be-
cause only results of binary judgments are offi-
cially provided as relevance information. 
Therefore, in order to testify the Taylor formula 
based method, we need to develop a special algo-
rithm for converting each binary judgment to a 
continuous score. An easy way for converting a 
value of binary judgment into a continuous degree 
of relevance is to predict the degree from a docu-
ment score in initial search by using a simple 
regression, r As Bi i= + . 
It would be straightforward that the constants A  
and B  are determined based on maximum and 
minimum values of s  and  for relevant and ir-
relevant documents independently. That is, we use 
a set of eight values for parameter estimation as 
follows.  
i ri
- s  and s : maximum and minimum values of 
 for ?relevant? documents in top-ranked n  
documents, 
max
1
si
1
min
- s  and s : maximum and minimum values of 
 for ?irrelevant? documents in top-ranked n  
documents, 
max
0
si
min
0
- r  and r : maximum and minimum values of 
 for ?relevant? documents in top-ranked n  
documents, 
max
1
ri
min
1
-  r  and r : maximum and minimum values 
of  for ?irrelevant? documents in top-ranked 
 documents. 
max
0
ri
n
min
0
For the set of relevant documents, we can ob-
tain estimates of A  and B  by solving equations, 
r As
r As
max max
min min
1 1
1 1
= +
= +
???
B
B
)
 
It is easy to show that 
A r r s s= ? ?( ) / (max min max min1 1 1 1  and 
B s r r s s s= ? ?( ) (max min max min max min1 1 1 1 1 1 ) . 
1 http://research.nii.ac.jp/ntcir/ 
Similarly, for the set of irrelevant documents, 
we obtain that 
A r r s s= ? ?( ) / (max min max min0 0 0 0 )  and 
B s r r s s s= ? ?( ) (max min max min max min0 0 0 0 0 0 )
3.4 
3.5 
4 
4.1 
. 
Furthermore, we have to determine a priori val-
ues of r , r , r  and r , max1 min1 max0 min0
(a) For vector space model, it is reasonable 
that r  is assumed to be 1.0 and r  is 
0.0 according to cosine function (4). As 
for  and , it is necessary to set a 
margin between them, i.e., amount of 
difference from minimum value for rele-
vant documents to maximum value for 
irrelevant ones. If we take the margin as 
2.0, it is automatically determined that 
 and r . As a result, target 
values  for relevant and irrelevant 
documents are distributed from 0.6 to 
1.0, and from 0.0 and 0.4, respectively. 
max
1
rmin
1
.6 0=
min
0
rmax
0
ma
0rmin
1
r
x .4 0=
i
(b) For probabilistic model, we set arbitrar-
ily that r , and 
 as a trial in the experiment. 
This means that range of document 
scores is enlarged doubly, and each r  
for relevant documents is to be distrib-
uted in the range from s  to 2 . On 
the other hand, maximum value of r  for 
irrelevant documents is  complicated a 
little, i.e., 
1
max
1
max 2s= r smin max1 1=
max
1smax
rmin .
0 0 0=
i
1
i
),min( 0min
1
min
0
max ssr =  
2/)],min(),[max( 0min
1
min
0
max
1
max ssss ?+ , 
since there is no guarantee that s  is al-
ways greater than s , and s  is always 
smaller than s  
max
1
max
0
min
0
min
1
Segmentation of Japanese text 
The test collection NTCIR-1 basically consists of 
documents written in Japanese (as well as the 
NTCIR-3 patent test collection). We need to seg-
ment each text into a set of terms automatically for 
indexing the Japanese documents and queries, of 
which text has no explicit boundary between terms 
unlike English. 
In the experiment, each term was identified by 
matching with entries in a machine-readable dic-
tionary. We used a dictionary of the ChaSen[14], 
which is a well-known morphological analyzer for 
Japanese text, and selected each longest entry 
matched with a portion of text as a index term. 
Also, an ?unknown? string was decomposed ac-
cording to change of kinds of character, Hiragana, 
Katakana, Kanji and so on (Hiragana strings were 
not taken as index terms). 
Also, for identifying compound words as con-
tent-bearing terms, we employed a heuristic rule 
that an adjacent pair of index terms identified by 
dictionary matching is automatically combined 
into a compound word. 
Results 
The NTCIR-1 collection includes 332,918 records, 
and average document length, i.e., average of the 
total number of terms appearing in each document, 
was 118.0. Table 1 shows values of mean average 
precision of each run. 
As shown in Table 1, the Taylor formula based 
method outperforms the Rocchio method slightly, 
but clearly there is no statistically significant dif-
ference between ROCCHI and TYLVEC (.376 
and .378 at top 10, and .434 and .459 at top 20). 
The rate of improvement by feedback in vector 
space model is greater than that in probabilistic 
model. The run showing best performance in Table 
1 is the Taylor formula based method in the vector 
space model (TYLVEC) using top-ranked 20 
documents, which increases mean average preci-
sion at 101.6% from ORGVEC (from .228 to .459). 
Experiment on Pseudo Relevance Feed-
back using NTCIR-3 Patent Test Col-
lection 
Procedure  
In the previous section, the Taylor formula based 
method has proven to work well at the experiment 
using the NTCIR-1 test collection with relevance 
information. Next, we attempt to examine the ef-
fectiveness of pseudo relevance feedback method 
using the Taylor formula based feedback in the 
case of searching the NTCIR-3 patent test collec-
tion (with no relevance information). 
The method and procedure are almost same 
with those in the previous section. However, in the 
Rocchio method, D  is assumed to be empty (?  is 
0 in the Equation (1)). 
Table 1. Mean average precision (using the 
NTCIR-1 collection with relevance information) 
model Vector space  Probabilis-
tic 
initial search 
(baseline) 
ORGVEC 
.228 
ORGPRB
.268 
feedback ROCCHI TYLVEC TYLPRB 
top 10 docu-
ments 
.376 
(+65.2%) 
.378 
(+66.3%) 
.396 
(+48.0%) 
top 20 docu-
ments 
.434 
(+90.4%) 
.459 
(+101.6%) 
.450 
(+68.1%) 
 
In the experiment, only six runs were executed 
as shown in Table 2 (at the time of the NTCIR-3 
Workshop, only the six runs were submitted). We 
discern two kinds of run according to query (topic) 
fields used for run; (I) <ARTICLE> and <SUP-
PLEMENT> fields and (II) <DESCRIPTION> and 
<NARRATIVE> fields. The <ARTICLE> field 
includes a news article, i.e., in the NTCIR-3 Patent 
Task, the participants were asked to search the 
document collection for a news article related to 
the information needs of users. The number of top-
ics is 32. 
 
Table 2. Runs in the experiment using patent test 
collection 
Topic fields Initial run 
 
feedback 
<A><S>* <D><N>**
OKAPI TAYLOR Run1 Run2 
VECTOR ROCCHIO Run3 Run4 
OKAPI none Run5 Run6 
*<A>:<ARTICLE>, <S>:<SUPPLEMENT> 
**<D>:<DESCRIPTION>,<N>:<NARRATIVE 
4.2 Results 
In the indexing phase, 697,262 records were proc-
essed and average length of documents is 393.32. 
Table 3 shows search performance of each run. 
Unfortunately, pseudo relevance feedback using 
relevance feedback techniques has no effect on the 
performance. It seems that there are no statistically 
significant differences between any pairs of runs. 
However,  
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
OKAPI-none (baseline)
TA
Y
LO
R
 a
nd
 R
O
C
C
H
IO
ROCCHIO
TAYLOR
 
Figure 1. Topic-by-topic Analysis (in the case of 
using <DESCRIPTION> and <NARRATIVE>) 
 
Figure 1 is a plot of values of average precision 
by topic. We can compare the Taylor formula 
based method (OKAPI-TAYLOR) and the Roc-
chio method (VECTOR-ROCCHIO) with normal 
Okapi formula (OKAPI-none), in level of each 
topic. It should be noted that, in Figure 1, square 
indicates ROCCHIO and circle TAYLOR. 
Figure 1 shows that for most of topics, normal 
Okapi formula outperforms the Rocchio method 
and Taylor formula based method although the 
Rocchio method and Taylor formula based method 
are superior in some topics. 
 
Table 3. Average Precision and R-precision (Using NTCIR-3 Patent Test Collection) 
Topic Fields Initial run feedback Average precision R-precision 
OKAPI TAYLOR 0.1152 0.1421 
VECTOR ROCCHIO 0.1281 0.1565 
<ARTICLE> 
<SUPPLEMENT> 
OKAPI none 0.1282 0.1565 
OKAPI TAYLOR 0.1370 0.1820 
VECTOR ROCCHIO 0.1581 0.1896 
<DESCRIPTION> 
<NARRATIVE> 
OKAPI none 0.1583 0.1813 
 
5 Discussion 
Although the Rocchio method and Taylor formula 
based method have shown good performance in the 
preliminary experiment using the NTCIR-1 test 
collection with relevance judgment with relevance 
judgment information, unfortunately the pseudo 
relevance feedback was not able to show im-
provement of search effectiveness. A main reason 
for the failure may be that term selection process 
was omitted. In standard pseudo relevance feed-
back methods, better terms are usually selected 
from the set of top-ranked documents according to 
the term weights. We can expect that if the term 
selection process is applied, the performance is 
improved in the case of the Rocchio method. How-
ever, how can we select better terms in the case of 
the Taylor formula based method? 
The behavior of the Taylor formula based 
method in the process of term re-weighting is a 
little complicated. For example, we assume that 
there are only 6 distinct terms (from term1 to 
term6) in a database, and that 
b = ( . , . , . , . , . , . )05 05 05 05 05 05 T , 
which means that all term weights in the initial 
query vector are equal. The matrix of weights of 
terms in top-ranked 4 documents (from doc1 to 
doc4) is supposed to be that 
??
??
?
?
?
??
??
?
?
?
=
111200
112100
110021
110012
XA
.                             (16) 
A row of the matrix represents each document vec-
tor, e.g.,d  )1,1,0,0,1,2(1 =T
Furthermore, it is assumed that a set of numeri-
cal values indicating degree of relevance for each 
document was given by a user, and difference from 
initial document scores was calculates such that 
r sX X
T? = ? ?( . , . , . , . )01 0 2 01 0 2 .                           (17) 
Under these assumptions, relevance feedback by 
the Taylor formula based method is as follows. 
First, by the SVD algorithm, the transpose matrix 
of the A  can be decomposed as UX V? T , and after 
simple calculation, we can finally obtain that 
U V r s?? ? = ?1 0 0 01 01 0 0 0 0 0 0T X X T( ) ( . , . , . , . , . , . ) .      (18) 
This example represents well characteristics of 
the Taylor formula based method. From (17) we 
understand that scores of doc1 and doc2 have to be 
increased and those of doc3 and doc4 decreased. 
Intuitively, it seems that weights of both term1 and 
term2 should be augmented because they are only 
appearing in doc1 and doc2, neither doc3 nor doc4 
at all. However, a solution by (18) indicates that 
the weight of term1 is unchanged (only to that of 
term2, 0.1 is added). This is a result so as to keep 
the condition (17), which means that scores of 
doc1 and doc2 have to be increased by 0.1 and 0.2, 
respectively, for reaching at an ideal situation. Ac-
tually, we can calculate from (16) such that 
2 0 0 1 01 01? + ? =. .
1 0 0 2 01 0
.  for doc1 and that 
2? + ? =. . .  for doc2. The results indicate 
that the condition (17) is completely satisfied. As 
shown in the simple calculation, the Taylor for-
mula based method takes the difference r sX X?  
into consideration for re-weighting of search terms.  
On the other hand, in the case of the Rocchio 
method, re-weighting of search terms is done by 
looking into only A  regardless of . We sup-
pose that doc1 and doc2 were judged as relevant 
documents, and doc3 and doc4 irrelevant. In the 
condition, the Rocchio method adds simply 
(1+2)/2=1.5 to weights of both of term1 and term2, 
not considering document scores in an initial 
search.  
X s X
As shown in above example, in the case of the 
Taylor formula based method, term re-weighting is 
dependent on the values of r . Therefore, we 
can not use simply the vector (18) for selecting 
better terms. We have to consider carefully how to 
use the Equation (18) for term selection. Further 
investigation will be needed for executing term 
selection for pseudo relevance feedback in the case 
of the Taylor formula based method. 
sX ? X
6 Concluding Remarks 
In this paper, results of two experiments on rele-
vance feedback have been reported. The purpose of 
first experiment is to check performance of a new 
feedback method proposed by Kishida[1] (the Tay-
lor formula based method) in a normal situation 
with relevance information. The result has shown 
that the Taylor formula based method works well. 
The second experiment aims at examining effec-
tiveness of pseudo relevance feedback using the 
Taylor formula based method for searching a pat-
ent collection. Unfortunately, the pseudo relevance 
feedback did not show good performance. We need 
to devise a technique for selecting better terms 
[10] M. F. Moens and J. Dumortier. 2000. Text 
categorization: the assignment of subject de-
scriptors to magazine articles. Information 
Processing and Management, 36: 841-861. 
from top-ranked documents in the case of applying 
the new feedback method.  
References 
[1] K. Kishida. 2001. Feedback method for docu-
ment retrieval using numerical values on rele-
vance given by users. IPSJ SIG Notes 
Fundamental Infology, 61: 189-196. (in Japa-
nese) 
[11] C. Buckley, J. Allan, and G. Salton. 1994. 
Automatic routing and ad-hoc retrieval using 
SMART: TREC2. in D.K. Harman ed., The 
Second Text Retrieval Conference (TREC2). 
National Institute of Standards and Technology, 
Gaithersburg MD, 45-55. [2] K. Kishida. 2003. Experiment on Pseudo Rele-
vance Feedback Method Using Taylor Formula 
at NTCIR-3 Patent Retrieval Task. Proceed-
ings of the Third NTCIR Workshop on Re-
search in Information Retrieval, Automatic 
Text Summarization and Question Answering, 
NII, Tokyo.  http://research.nii.ac.jp/ntcir/ 
[12] S. E. Robertson, et al 1995. Okapi at TERC-3. 
in D.K. Harman ed. Overview of the Third 
Text Retrieval Conference (TREC-3). National 
Institute of Standards and Technology, 
Gaithersburg MD, 109-126. 
[13] D. A. Harville. 1997. Matrix Algebra from a 
Statistician?s Perspective. Springer, New York. [3] J. J. Rocchio, Jr. 1971. Relevance feedback in 
information retrieval. in G. Salton ed., The 
SMART Retrieval System: Experiments in 
Automatic Document Processing, Prentice-
Hall, Englewood Cliffs, NJ, 313-323. 
[14] Yuji Matsumoto, Akira Kitauchi, Tatsuo 
Yamashita, Yoshitaka Hirano, Hiroshi Matsuda, 
Kazuma Takaoka and Masayuki Asahara. 2000. 
Morphological Analysis System ChaSen version 
2.2.1 Manual. http://chasen.aist-nara.ac.jp/ 
[4] G. Salton and C. Buckley. 1990. Improving 
retrieval performance by relevance feedback. 
Journal of the American Society for Informa-
tion Science, 41: 288-297. Appendix. Detail of Calculation 
[5] P. Sarinivasan. 1996. Query expansion and 
MEDLINE. Information Processing and 
Management, 32: 431-4
If we assume a linear function (11), 
?
?
?
?
f X
T
X
T X
( ) ( )b
b
A b
b
A= = , 43. 
which is a well-known result in the field of linear 
algebra [13]. Therefore (13) becomes that 
[6] J. H. Lee. 1998. Combining the evidence of 
different relevance feedback methods for in-
formation retrieval. Information Processing 
and Management, 34: 681-691. 
f fX X X(
~) ( ) (~ )b b A b= + ? b  
(it should be noted that K = 0 ). 
By following our assumption that r  is equal to X
f X (
~)b  and noting that , we obtain that f X ( )b s=
[7] R. Mandala, T. Tokunaga and H. Tanaka. 2000. 
Query expansion using heterogeneous thesauri. 
Information Processing and Management, 36: 
361-378.  
X
XA b b r sX X(
~ )? = ? .                                     (A.1) 
The (14) is easily derived from (A.1). 
[8] M. Iwayama. 2000. Relevance feedback with a 
small number of relevance judgments: incre-
mental relevance feedback vs. Document clus-
tering. in Proceedings of the 23rd Annual 
International ACM SIGIR Conference on Re-
search and Development in Information Re-
trieval, ACM Press, 10-16. 
By using singular value decomposition we can 
obtain that . The transposition is that A U VXT = ? T( ) ( )A A U V V UX XT T T T T= = =? ? ,                (A.2) 
because  and U V  are orthogonal matrixes and ? is 
a diagonal matrix. Substituting (A.2) into (A.1), we 
finally obtain that 
V U b b r s? T X X(~ )? = ?  . 
~[9] G. Ciocca. and R. Schettini.1999. A relevance 
feedback mechanism for content-based image 
retrieval. Information Processing and Man-
agement, 35: 605-632. 
)(1 XX
T srVUbb ??+=? ? . 
 
 
 
