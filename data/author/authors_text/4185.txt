A Framework for MT and Multilingual NLG Systems Based on 
Uniform Lexico-Structural Processing 
Benoit Lavoie 
CoGenTex, Inc. 
840 Hanshaw Road 
Ithaca, NY 
USA, 14850 
benoit@cogentex.com 
Richard Kittredge 
CoGenTex, Inc. 
840 Hanshaw Road 
Ithaca, NY 
USA, 14850 
richard @ cogentex.com 
Tanya Korelsky 
CoGenTex, Inc. 
840 Hanshaw Road 
Ithaca, NY 
USA, 14850 
tanya @ cogentex.com 
Owen Rambow *
ATT Labs-Research, B233 
180 Park Ave, PO Box 971 
Florham Park, NJ 
USA, 07932 
rambow @research.att.com 
Abstract 
In this paper we describe an implemented 
framework for developing monolingual or 
multilingual natural language generation 
(NLG) applications and machine translation 
(MT) applications. The framework 
demonstrates a uniform approach to 
generation and transfer based on declarative 
lexico-structural transformations of 
dependency structures of syntactic or 
conceptual levels ("uniform lexico-structural 
processing"). We describe how this 
framework has been used in practical NLG 
and MT applications, and report he lessons 
learned. 
1 Introduction 
In this paper we present a linguistically 
motivated framework for uniform lexico- 
structural processing. It has been used for 
transformations of conceptual and syntactic 
structures during generation i monolingual nd 
multilingual natural language generation (NLG) 
and for transfer in machine translation (MT). 
Our work extends directions taken in systems 
such as Ariane (Vauquois and Boitet, 1985), 
FoG (Kittredge and Polgu6re, 1991), JOYCE 
(Rainbow and Korelsky, 1992), and LFS 
(Iordanskaja et al, 1992). Although it adopts 
the general principles found in the above- 
mentioned systems, the approach presented in 
this paper is more practical, and we believe, 
would eventually integrate better with emerging 
statistics-based approaches toMT. 
* The work performed on the framework by this co- 
author was done while at CoGenTex, Inc. 
The framework consists of a portable Java 
environment for building NLG or MT 
applications by defining modules using a core 
tree transduction engine and single declarative 
ASCII specification language for conceptual or 
syntactic dependency tree structures 1 and their 
transformations. Developers can define new 
modules, add or remove modules, or modify 
their connections. Because the processing of the 
transformation engine is restricted to 
transduction of trees, it is computationally 
efficient. 
Having declarative rules facilitates their reuse 
when migrating from one programming 
environment toanother; if the rules are based on 
functions pecific to a programming language, 
the implementation f these functions might no 
longer be available in a different environment. 
In addition, having all lexical information and 
all rules represented eclaratively makes it 
relatively easy to integrate into the framework 
techniques for generating some of the rules 
automatically, for example using corpus-based 
methods. The declarative form of 
transformations makes it easier to process them, 
compare them, and cluster them to achieve 
proper classification and ordering. 
1 In this paper, we use the term syntactic dependency 
(tree) structure as defined in the Meaning-Text 
Theory (MTT; Mel'cuk, 1988). However, we 
extrapolate from this theory when we use the term 
conceptual dependency (tree) structure, which has no 
equivalent in MTT (and is unrelated to Shank's CD 
structures proposed inthe 1970s). 
60 
Thus, the framework represents a generalized 
processing environment that can be reused in 
different ypes of natural language processing 
(NLP) applications. So far the framework has 
been used successfully to build a wide variety of 
NLG and MT applications in several limited 
domains (meteorology, battlefield messages, 
object modeling) and for different languages 
(English, French, Arabic, and Korean). 
In the next sections, we present the design of the 
core tree transduction module (Section 2), 
describe the representations that it uses (Section 
3) and the linguistic resources (Section 4). We 
then discuss the processing performed by the 
tree transduction module (Section 5) and its 
instantiation for different applications (Section 
6). Finally, we discuss lessons learned from 
developing and using the framework (Section 7) 
and describe the history of the framework 
comparing it to other systems (Section 8). 
2 The Framework's Tree Transduction Module 
The core processing engine of the framework is 
a generic tree transduction module for lexico- 
structural processing, shown in Figure 1. The 
module has dependency stuctures as input and 
output, expressed in the same tree formalism, 
although not necessarily at the same level (see 
Section 3). This design facilitates the pipelining 
of modules for stratificational transformation. I  
fact, in an application, there are usually several 
instantiations of this module. 
The transduction module consists of three 
processing steps: lexico-structural pre- 
processing, main lexico-structural processing, 
and lexico-structural post-processing. Each of 
these steps is driven by a separate grammar, and 
all three steps draw on a common feature data 
base and lexicon. The grammars, the lexicon 
and the feature data base are referred to as the 
linguistic resources (even if they sometimes 
apply to a conceptual representation). All 
linguistic resources are represented in a 
declarative manner. An instantiation of the tree 
transduction module consists of a specification 
of the linguistic resources. 
Input Dependency Structure 
~ L exlco-Structural Preproce~ing 
Intermediate Dependency StructttreL_~ 
Lexico-Structm'al Processing 
Intermediate + Dependency Structure 
~ Lexico-Structural 
Postprocessing 
Output / /~  
Dependency SUucturc 
i 
Figure 1: Design of the Tree Transduction Module 
3 The Framework's Representations 
The representations used by all instantiations of
the tree transduction module in the framework 
are dependency tree structures. The main 
characteristics of all the dependency tree 
structures are: 
? A dependency tree is unordered (in contrast 
with phrase structure trees, there is no 
ordering between the branches of the tree). 
? All the nodes in the tree correspond to 
lexemes (i.e., lexical heads) or concepts 
depending on the level of representation. I  
contrast with a phrase structure 
representation, there are no phrase-structure 
nodes labeled with nonterminal symbols. 
Labelled arcs indicate the dependency 
relationships between the lexemes. 
The first of these characteristics makes a 
dependency tree structure a very useful 
representation for MT and multilingual NLG, 
since it gives linguists a representation that 
allows them to abstract over numerous cross- 
linguistic divergences due to language specific 
ordering (Polgu~re, 1991). 
We have implemented 4 different types of 
dependency tree structures that can be used for 
NLG, MT or both: 
? Deep-syntactic structures (DSyntSs); 
? Surface syntactic structures (SSyntSs); 
61 
? Conceptual structures (ConcSs); 
? Parsed syntactic structures (PSyntSs). 
The DSyntSs and SSyntSs correspond closely to 
the equivalent structures of the Meaning-Text 
Theory (MTT; Mel'cuk, 1988): both structures 
are unordered syntactic representations, but a 
DSyntS only includes full meaning-bearing 
lexemes while a SSyntS also contains function 
words such as determiners, auxiliaries, and 
strongly governed prepositions. In the 
implemented applications, the DSyntSs are the 
pivotal representations involved in most 
transformations, as this is also often the case in 
practice in linguistic-based MT (Hutchins and 
Somers, 1997). Figure 2 illustrates a DSyntS 
from a meteorological application, MeteoCogent 
(Kittredge and Lavoie, 1998), represented using 
the standard graphical notation and also the 
RealPro ASCII notation used internally in the 
framework (Lavoie and Rambow, 1997). As 
Figure 2 illustrates, there is a straightforward 
mapping between the graphical notation and the 
ASCII notation supported in the framework. 
This also applies for all the transformation rules 
in the framework which illustrates the 
declarative nature of our approach, 
I 1 
LOW 
-5 TO 
't 
LOw 
( 
A'I~R -5 
ATTR TO 
( 
il HIGH 
( 
A'\]I~R 20 
) 
) 
) 
Low -S to high 20 
Figure 2: DSyntS (Graphical nd ASCII Notation) 
The ConcSs correspond to the standard frame- 
like structures used in knowledge representation, 
with labeled arcs corresponding to slots. We 
have used them only for a very limited 
meteorological domain (in MeteoCogent), and 
we imagine that they will typically be defined in 
a domain-specific manner. 
Figure 3 illustrates the mapping between an 
interlingua defined as a ConcS and a 
corresponding English DSyntS. This example, 
also taken from MeteoCogent, illustrates that the 
conceptual interlingua in NLG can be closer to a 
database representation f domain data than to 
its linguistic representations. 
As mentioned in (Polgu~re, 1991), the high level 
of abstraction of the ConcSs makes them a 
suitable interlingua for multilingual NLG since 
they bridge the semantic discrepancies between 
languages, and they can be produced easily from 
the domain data. However, most off-the-shelf 
parsers available for MT produce only syntactic 
structures, thus the DSyntS level is often more 
suitable for transfer. 
Cones  
#TEMPERATURE 
Low -5 to Mlgh 20 
DS~tS  
LOW 
-5 TO 
ItlGH 
Figure 3: ConcS Interlingua nd English DSyntS 
Finally, the PSyntSs correspond to the parser 
outputs represented using RealPro's dependency 
structure formalism. The PSyntSs may not be 
valid directly for realization or transfer since 
they may contain unsupported features or 
dependency relations. However, the PSyntSs 
are represented in a way to allow the framework 
to convert hem into valid DSyntS via lexico- 
structural processing. This conversion is done 
via conversion grammars customized for each 
parser. There is a practical need to convert one 
syntactic formalism to another and so far we 
have implemented converters for three off-the- 
shelf parsers (Palmer et al, 1998). 
4 The Framework's Linguistic Resources 
As mentioned previously, the framework is 
composed of instantiations of the tree 
62 
transduction module shown in Figure 1. Each 
module has the following resources: 
? Feature Data-Base: This consists of the 
feature system defining available features 
and their possible values in the module. 
? Lexicon: This consists of the available 
lexemes or concepts, depending on whether 
the module works at syntactic or conceptual 
level. Each lexeme and concept is defined 
with its features, and may contain specific 
lexico-structural ules: transfer rules for MT, 
mapping rules to the next level of 
representation for surface realization of 
DSyntS or lexicalization of ConcS. 
? Main Grammar: This consists of the lexico- 
structural mapping rules that apply at this 
level and which are not lexeme- or concept- 
specific (e.g. DSynt-rules for the DSynt- 
module, Transfer-rules for the Transfer 
module, etc.) 
? Preprocessing rammar: This consists of 
the lexico-structural mapping rules for 
transforming the input structures in order to 
make them compliant with the main 
grammar, if this is necessary. Such rules are 
used to integrate new modules together 
when discrepancies in the formalism need to 
be fixed. This grammar can also be used 
for adding default features (e.g. setting the 
default number of nouns to singular) or for 
applying default transformations (e.g. 
replacing non meaning-bearing lexemes 
with features). 
Postprocessing rammar: This consists of 
lexico-structural mapping rules for 
transforming the output structures before 
they can be processed by the next module. 
As for the preprocessing rules, these rules 
can be used to fix some discrepancies 
between modules. 
Our representation f the lexicon at the lexical 
level (as opposed to conceptual) is similar to the 
one found in RealPro. Figure 4 shows a 
specification for the lexeme SELL. This lexeme 
is defined as a verb of regular morphology with 
two lexical-structural mappings, the first one 
introducing the preposition TO for its 3 r? actant, 
and the preposition FOR for its 4 th actant: (a 
seller) X1 sells (merchandise) X2 to (a buyer) 
X3 for (a price) X4. What is important is that 
each mapping specifies a transformation 
between structures at different levels of 
representation but that are represented in one 
and the same representation formalism (DSyntS 
and SSyntS in this case). As we will see 
below, grammar ules are also expressed in a 
similar way. 
LEX~ME: SELL 
CATEGORY:  verb  
FEATURES:  \[ \] 
GOV-PATTERN: \ [  
DSYNT-RULE:  
SELL ( I I I  $X3 ) 
<- -> 
SELL 
( complet ive2  TO 
( p repos i t iona l  $X3 ) ) 
DSYNT-RULE : 
SELL ( IV $X4 ) 
<- -> 
SELL 
( complet ive3  FOR 
( p repos i t iona l  $X4 ) 
\] 
MORPHOLOGY:  \[ 
( \[ tense :past  \] so ld  \[ inv 
( \[ mood:past -par t  \] so ld  \[ inv 
( \[ \] sel l  \[ reg 
\] 
Figure 4: Specification ofLexeme SELL 
At the conceptual level, the conceptual lexicon 
associates lexical-structural mapping with 
concepts in a similar way. Figure 5 illustrates 
the mapping at the deep-syntactic level 
associated with the concept #TEMPERATURE. 
Except for the slight differences in the labelling, 
this type of specification is similar to the one 
used on the lexical level. The first mapping rule 
corresponds to one of the lexico-structural 
transformations u ed to convert he interlingual 
ConcS of Figure 3 to the corresponding DSyntS. 
ZONCEPT:  #TEMPERATURE 
5EXICAL:  \[ 
L~-RULE:  
#TEMPERATURE ( #min imum SX 
#maxim~ $Y 
<- -> 
LOW ( ATTR $X 
ATTR TO 
( II H IGH 
( ATTR SY ) ) ) 
LEX-RULE:  
#TEMPERATURE ( #min im~ SX 
<- -> 
LOW ( ATTR $X ) 
LEX-RULE:  
#TEMPE~TURE ( #max imum $X 
<- -> 
H IGH ( ATTR SX ) 
\] 
Figure 5: Specification ofConcept #TEMPERATURE 
63 
Note that since each lexicon entry can have 
more than one lexical-structural mapping rule, 
the list of these rules represents a small grammar 
specific to this lexeme or concept. 
Realization grammar ules of the main grammar 
include generic mapping rules (which are not 
lexeme-specific) such as the DSyntS-rule 
illustrated in Figure 6, for inserting a determiner. 
DSYNT-RULE:  
$X  \[ c lass :noun ar t i c le :de f  \] 
$X  ( determinat ive  THE ) 
Figure 6: Deep-Syntactic Rule for Determiner Insertion 
The lexicon formalism has also been extended to 
implement lexeme-specific lexico-structural 
transfer rules. Figure 7 shows the lexico- 
structural transfer of the English verb lexeme 
MOVE to French implemented for a military 
and weather domain (Nasr et al, 1998): 
Cloud will move into the western regions. 
Des nuages envahiront les rdgions ouest. 
They moved the assets forward. 
-.9 lls ont amen~ les ressources vers l 'avant. 
The 79 dcg moves forward. 
---~ La 79 dcg avance  vers l'avant. 
A disturbance will move north of Lake Superior. 
--~ Une perturbation se diplacera au nord du lac 
supdrieur. 
LEXEME : MO~'E 
CATEGORY : verb 
FEATORES : \[ \] 
TRANSFER: \[ 
TRANSFER-RULE: 
MOVE 
I ATTR INTO \ [ c lass :prepos i t ion \ ]  
( II SXl ) ) 
.-.> 
E2~VAH IR \[class:verb\] 
( II SX1 ) 
TRANSFER-RULE : 
MOVE 
( II $X2 ) 
AMENER \[class:verb\] 
\[ II $X2 ) 
TRANSFER-RULE: 
MOVE 
( ATTR SX \[Iexe~e:FORWARD class:adverb\] ) 
AVANCER 
( ATTR SX ) 
TRANSFER-RULE : 
MOVE 
<--> 
DEPLACER \[class:verb refl:?\] 
\] 
Figure 7: Lexico-Structural Transfer of English Lexerne 
MOVE to French 
More general exico-structural rules for transfer 
can also be implemented using our grammar rule 
formalism. Figure 8 gives an English-French 
transfer ule applied to a weather domain for the 
transfer of a verb modified by the adverb 
ALMOST: 
It almost rained. 
--o II a fail l i  pleuvoir. 
TRANSFER-RULE:  
SX  \[ c lass :verb  \] 
( ATTR ALMOST ) 
<- -> 
FA ILL IR  \[ c lass :verb  \] 
( I I  SX  \[ mood: in f  \] ) 
Figure 8: English to French Lexico-Structural 
Transfer Rule with Verb Modifier ALMOST 
More details on how the structural divergences 
described in (Dorr, 1994) can be accounted for 
using our formalism can be found in (Nasr et 
al., 1998). 
5 The Rule Processing 
Before being processed, the rules are first 
compiled and indexed for optimisation. Each 
module applies the following processing. 
The rules are assumed to be ordered from most 
specific to least specific. The application of the 
rules to the structures i  top-down in a recursive 
way from the f'n-st rule to the last. For the main 
grammar, before applying a grammar ule to a 
given node, dictionary lookup is carried out in 
order to first apply the lexeme- or concept- 
specific rules associated with this node. These 
are also assumed to be ordered from the most 
specific to the least specific. 
If a lexico-structural transformation involves 
switching a governor node with one of its 
dependents in the tree, the process is reapplied 
with the new node governor. When no more 
rules can be applied, the same process is applied 
to each dependent of the current governor. 
When all nodes have been processed, the 
processing is completed, 
6 Using the Framework to build Applications 
Figure 9 shows how different instantiations of 
the tree transduction module can be combined to 
64 
build NLP applications. The diagram does not 
represent a particular system, but rather shows 
the kind of transformations that have been 
implemented using the framework, and how they 
interact. Each arrow represents one type of 
processing implemented by an instantiation of 
the tree transduction module. Each triangle 
represents a different level of representation. 
Scope of the 
Framework 
~Conversion bl 
Parsed 
PSyntS LI 
Parsing 
Sentence 
PI "ng 
C'?nezoa~ 1 
~ e Transfer ~_~ , Co.verMon 
D$ ntS LI 
~SyntS ~ealizalion 
/ \  
SSyntS LI SSyntS 1.2 ~ yntS ealization 
A DSyntS L2 Parsed DSym51 PSyntS L2 
Realiza~o~ 
SSym~ Realizatio parsin 
Input Generated Generated Input 
Sentence LI Sentence LI Sentence 1.2 Sentence L2. 
I concS Concepmd suar.tm~ SSyntS Suffaee:Syntnetlc su'uet~'e 
os~ts t~sy~ac~ Psy~s ~d:~n~c 
Figure 9: Scope of the Framework's Transformations 
For example, in Figure 9, starting with the 
"Input Sentence LI" and passing through 
Parsing, Conversion, Transfer, DSyntS 
Realization and SSyntS Realization to 
"Generated Sentence L2" we obtain an Ll-to-L2 
MT system. Starting with "Sentence Planning" 
and passing through DSyntS Realization, and 
SSyntS Realization (including linearization and 
inflection) to "Generated Sentence LI", we 
obtain a monolingual NLG system for L1. 
So far the framework has been used successfully 
for building a wide variety of applications in 
different domains and for different languages: 
NLG: 
? Realization of English DSyntSs via SSyntS 
level for the domains of meteorology 
(MeteoCogent; Kittredge and Lavoie, 1998) 
and object modeling (ModelExplainer; 
Lavoie et al, 1997). 
? Generation of English text from conceptual 
interlingua for the meteorology domain 
(MeteoCogent). (The design of the 
interlingua can also support he generation 
of French but this functionality has not yet 
been implemented.) 
MT: 
? Transfer on the DSyntS level and realization 
via SSyntS level for English--French, 
English--Arabic, English---Korean and 
Korean--English. Translation in the 
meteorology and battlefield omains (Nasr 
et al, 1998). 
? Conversion of the output structures from 
off-the-shelf English, French and Korean 
parsers to DSyntS level before their 
processing by the other components in the 
framework (Palmer et al, 1998). 
7 Lessons Learned Using the Framework 
Empirical results obtained from the applications 
listed in Section 6 have shown that the approach 
used in the framework is flexible enough and 
easily portable to new domains, new languages, 
and new applications. Moreover, the time spent 
for development was relatively short compared 
to that formerly required in developing similar 
types of applications. Finally, as intended, the 
limited computational power of the transduction 
module, as well as careful implementation, 
including the compilation of declarative 
linguistic knowledge to Java, have ensured 
efficient run-time behavior. For example, in the 
MT domain we did not originally plan for a 
separate conversion step from the parser output 
to DSyntS. However, it quickly became apparent 
that there was a considerable gap between the 
output of the parsers we were using and the 
DSyntS representation that was required, and 
furthermore, that we could use the tree 
transduction module to quickly bridge this gap. 
Nevertheless, our tree transduction-based 
approach has some important limitations. In 
particular, the framework requires the developer 
of the transformation rules to maintain them and 
specify the order in which the rules must be 
applied. For a small or a stable grammar, this 
does not pose a problem. However, for large or 
rapidly changing grammar (such as a transfer 
grammar in MT that may need to be adjusted 
when switching from one parser to another), the 
65 
burden of the developer's task may be quite 
heavy. In practice, a considerable amount of 
time can be spent in testing a grammar after its 
revision. 
Another major problem is related to the 
maintenance of both the grammar and the 
lexicon. On several occasions during the 
development of these resources, the developer in 
charge of adding lexical and grammatical data 
must make some decisions that are domain 
specific. For example, in MT, writing transfer 
rules for terms that can have several meanings or 
uses, they may simplify the problem by 
choosing a solution based on the context found 
in the current corpus, which is a perfectly natural 
strategy. However, later, when porting the 
transfer esources to other domains, the chosen 
strategy may need to be revised because the 
context has changed, and other meanings or uses 
are found in the new corpora. Because the 
current approach is based on handcrafted rules, 
maintenance problems of this sort cannot be 
avoided when porting the resources to new 
domains. 
An approach such as the one described in (Nasr 
et al, 1998; and Palmer and al., 1998) seems to 
be solving a part of the problem when it uses 
corpus analysis techniques for automatically 
creating a first draft of the lexical transfer 
dictionary using statistical methods. However, 
the remaining work is still based on handcrafting 
because the developer must refine the rules 
manually. The current framework offers no 
support for merging handcrafted rules with new 
lexical rules obtained statistically while 
preserving the valid handcrafted changes and 
deleting the invalid ones. In general, a better 
integration of linguistically based and statistical 
methods during all the development phases is 
greatly needed. 
8 History of the Framework and Comparison 
with Other Systems 
The framework represents a generalization of 
several predecessor NLG systems based on 
Meaning-Text Theory: FoG (Kittredge and 
Polgu~re, 1991), LFS (Iordanskaja et al, 1992), 
and JOYCE (Rambow and Korelsky, 1992). 
The framework was originally developed for the 
realization of deep-syntactic structures in NLG 
(Lavoie and Rambow, 1997). It was later 
extended for generation of deep-syntactic 
structures from conceptual interlingua (Kittredge 
and Lavoie, 1998). Finally, it was applied to 
MT for transfer between deep-syntactic 
structures of different languages (Palmer et al, 
1998). The current framework encompasses the 
full spectrum of such transformations, i.e. from 
the processing of conceptual structures to the 
processing of deep-syntactic structures, either 
for NLG or MT. 
Compared to its predecessors (Fog, LFS, 
JOYCE), our approach as obvious advantages 
in uniformity, declarativity and portability. The 
framework has been used in a wider variety of 
domains, for more languages, and for more 
applications (NLG as well as MT). The 
framework uses the same engine for all the 
transformations at all levels because all the 
syntactic and conceptual structures are 
represented asdependency tree structures. 
In contrast, the predecessor systems were not 
designed to be rapidly portable. These systems 
used programming languages or scripts for the 
implementation f the transformation rules, and 
used different ypes of processing at different 
levels of representation. For instance, in LFS 
conceptual structures were represented as 
graphs, whereas syntactic structures were 
represented as trees which required different 
types of processing at these two levels. 
Our approach also has some disadvantages 
compared with the systems mentioned above. 
Our lexico-structural transformations are far 
less powerful than those expressible using an 
arbitrary programming language. In practice, 
the formalism that we are using for expressing 
the transformations is inadequate for long-range 
phenomena (inter-sentential or intra-sentential), 
including syntactic phenomena such as long- 
distance wh-movement and discourse 
phenomena such as anaphora nd ellipsis. The 
formalism could be extended to handle intra- 
sentential syntactic effects, but inter-sentential 
discourse phenomena probably require 
procedural rules in order to access lexemes in 
66
other sentences. In fact, LFS and JOYCE 
include a specific module for elliptical structure 
processing. 
Similarly, the limited power of the tree 
transformation rule formalism distinguishes the 
framework from other NLP frameworks based 
on more general processing paradigms uch as 
unification of FUF/SURGE in the generation 
domain (Elhadad and Robin, 1992). 
9 Status 
The framework is currently being improved in 
order to use XML-based specifications for 
representing the dependency structures and the 
transformation rules in order to offer a more 
standard development environment and to 
facilitate the framework extension and 
maintenance. 
Acknowledgements 
A first implementation of the framework (C++ 
processor and ASCII formalism for expressing 
the lexico-structural transformation rules) 
applied to NLG was developed under SBIR 
F30602-92-C-0015 awarded by USAF Rome 
Laboratory. The extensions to MT were 
developed under SBIR DAAL01-97-C-0016 
awarded by the Army Research Laboratory. The 
Java implementation and general improvements 
of the framework were developed under SBIR 
DAAD17-99-C-0008 awarded by the Army 
Research Laboratory. We are thankful to Ted 
Caldwell, Daryl McCullough, Alexis Nasr and 
Mike White for their comments and criticism on 
the work reported in this paper. 
References 
Dorr, B. J. (1994) Machine translation divergences: 
A formal description and proposed solution. In 
Computational Linguistics, vol. 20, no. 4, pp. 597- 
635. 
Elhadad, M. and Robin, J. (1992) Controlling 
Content Realization with Functional Unification 
Grammars. In Aspects of Automated Natural 
Language Generation, Dale, R., Hovy, E., Rosner, 
D. and Stock, O. Eds., Springer Verlag, pp. 89- 
104. 
Hutchins, W. J. and Somers, H. L. (1997) An 
Introduction to Machine Translation. Academic 
Press, second edition. 
Iordanskaja, L., Kim, M., Kittredge, R., Lavoie, B. 
and Polgu6re, A. (1992) Generation of Extended 
Bilingual Statistical Reports. In Proceedings of the 
15th International Conference on Computational 
Linguistics, Nantes, France, pp. 1019-1023. 
Kittredge, R. and Lavoie, B. (1998) MeteoCogent: A
Knowledge-Based Tool For Generating Weather 
Forecast Texts. In Proceedings of the American 
Meteorological Society AI Conference (AMS-98), 
Phoenix, Arizona, pp. 80--83. 
Kittredge, R. and Polgu~re, A. (1991) Dependency 
Grammars for Bilingual Text Generation: Inside 
FoG's Stratificational Models. In Proceedings of 
the International Conference on Current Issues in 
Computational Linguistics, Penang, Malaysia, pp. 
318-330. 
Lavoie, B. (1995) Interlingua for Bilingual Statistical 
Reports. In Notes of IJCAI-95 Workshop on 
Multilingual Text Generation, Montr6al, Canada, 
pp. 84---94. 
Lavoie, B. and Rambow, O. (1997) A Fast and 
Portable Realizer for Text Generation Systems. In 
Proceedings of the Fifth Conference on Applied 
Natural Language Processing, Washington, DC., 
pp. 265-268. 
Lavoie, B., Rambow, O. and Reiter, E. (1997) 
Customizable Descriptions of Object-Oriented 
Models. In Proceedings of the Fifth Conference on 
Applied Natural Language Processing, 
Washington, DC., pp. 253-256. 
Mel'cuk, I. (1988) Dependency Syntax. State 
University of New York Press, Albany, NY. 
Nasr, A., Rambow, O., Palmer, M. and Rosenzweig, 
J. (1998) Enriching lexical transfer with cross- 
linguistic semantic features. In Proceedings of the 
Interlingua Workshop at the MT Summit, San 
Diego, California. 
Palmer, M., Rambow, O. and Nasr, A. (1998) Rapid 
Prototyping of Domain-Specific Machine 
Translation Systems. In Proceedings of the Third 
Conference on Machine Translation in the 
Americas (AMTA-98), PA, USA, pp. 95-102. 
Polgu6re, A. (1991) Everything has not been said 
about interlinguae: the case of multi-lingual text 
generation system. In Proc. of Natural Language 
Processing Pacific Rim Symposium, Singapore. 
Rambow, O. and Korelsky, T. (1992) Applied Text 
Generation. In Proceedings of the 6th International 
Workshop on Natural Language Generation, 
Trento, Italy, pp. 40--47. 
Vauquois, B. and Boitet C. (1985) Automated 
translation at Grenoble University. In 
Computational Linguistics, Vol. 11, pp. 28-36. 
67 
