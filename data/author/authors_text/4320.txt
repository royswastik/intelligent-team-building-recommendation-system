Coling 2008: Companion volume ? Posters and Demonstrations, pages 71?74
Manchester, August 2008
Quantification and Implication in Semantic Calendar Expressions
Represented with Finite-State Transducers
Jyrki Niemi and Kimmo Koskenniemi
Department of General Linguistics, University of Helsinki
PO Box 9, FI?00014 University of Helsinki, Finland
{jyrki.niemi, kimmo.koskenniemi}@helsinki.fi
Abstract
This paper elaborates a model for rep-
resenting semantic calendar expressions
(SCEs), which correspond to the inten-
sional meanings of natural-language calen-
dar phrases. The model uses finite-state
transducers (FSTs) to mark denoted peri-
ods of time on a set of timelines repre-
sented as a finite-state automaton (FSA).
We present a treatment of SCEs corre-
sponding to quantified phrases (any Mon-
day; every May) and an implication oper-
ation for requiring the denotation of one
SCE to contain completely that of another.
1 Introduction
This paper elaborates the temporal representation
model proposed in Niemi and Koskenniemi (2007)
and developed in Niemi and Koskenniemi (2008).
This bracketing FST model covers temporal infor-
mation ranging from simple dates to such mean-
ings as 6?8 pm on every Monday in April, except
on Easter Monday. The model represents seman-
tic calendar expressions (SCEs) using finite-state
transducers (FSTs) that bracket periods of time on
timelines represented as a finite-state automaton
(FSA). Motivations for a finite-state representation
include an efficient treatment of periodicity and
certain kinds of sparse sets of sets common in cal-
endar information, as well as a well-known theory.
In this paper, we treat SCEs corresponding to
quantified calendar phrases, such as any Monday
and every May. We also present implication for
representing such cases as a course with compul-
sory attendance, whose all class times should co-
incide with the free slots of time of a student.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
2 Semantic Calendar Expressions (SCEs)
and the Bracketing FST Model
We use the semi-formal SCEs as the basic repre-
sentation. An SCE typically corresponds to the
disambiguated intensional meaning of a natural-
language calendar phrase. An SCE may denote ei-
ther a specific period of time or a set of periods.
An SCE is compiled to a sequence of bracketing
FSTs through regular (relation) expressions.
Table 1 lists some implemented SCE constructs
and the corresponding calendar phrases. Most con-
structs can be combined with each other.
Construct SCE example; calendar phrase
calendar period may; fri; calday
May; Friday; (calendar) day
list (union) union (mon, fri, sun)
Monday, Friday and Sunday
common part
(intersection)
intersect (aug, y2008)
August 2008
interval interval (may, jun)
May to June
anchored nth_following (3, mon, easter)
the third Monday after Easter
quantified any_n (1, mon)
any (single) Monday
Table 1: Examples of SCE constructs and the cor-
responding calendar phrases
The bracketing FST model represents the deno-
tations of an SCE as an acyclic timeline FSA defin-
ing a finite timeline string for each alternative de-
notation.1 A basic timeline FSA defines a single
timeline consisting of brackets and labels for cal-
endar periods, with no denotations marked. The
following is a (simplified) timeline for the year
2008 at month level (spaces separate symbols):
1Timeline FSAs are equivalent to the timeline FSTs of
Niemi and Koskenniemi (2008).
71
[y y2008 [m Jan ]m [m Feb ]m . . . [m Dec ]m ]y
To represent the denotation of an SCE s, a basic
timeline FSA is composed with a bracketing FST
(sequence) representing s. The result is a time-
line FSA with the denotation marked with marker
brackets {in . . . }in. The following timeline corre-
sponds to union (jan, mar) (January and March):
[y y2008 {i3 {i1 [m Jan ]m }i1 }i3 [m Feb ]m {i3 {i2
[m Mar ]m }i2 }i3 [m Apr ]m . . . [m Dec ]m ]y
First, January is marked with i1 and March with i2.
Then the bracketing FST for union marks each pe-
riod i1 and i2 with i3, the denotation of the whole.
3 Representing Quantified SCEs
A natural-language calendar phrase may be un-
quantified, such as Monday, or it may contain dif-
ferent kinds of quantification, such as all Mondays,
any Monday, three Mondays and all Mondays in
some May.2 We treat any Monday, some Monday
and one Monday as meaning any single Monday,
and we equate all Mondays, each Monday and ev-
ery Monday. Numerical quantification can be gen-
eralized to intervals, possibly open-ended, such as
two to five Mondays and at most four Mondays.
3.1 Collective Representation (All)
The meaning all Mondays is represented simply as
the SCE mon, which is represented as a timeline
FSA with each Monday marked on the same time-
line, as illustrated in Fig. 1 (simplified).3 We call
this the collective representation of an SCE.
Su {Mo} Tu Su {Mo} Tu Su {Mo} Tu
Figure 1: A timeline FSA for the SCE mon (all
Mondays); ?{Mo}? denotes a marked Monday
The denotation can be interpreted in several
ways: as the union of all Mondays, as all the parts
of a timeline that are part of a Monday, as the re-
curring ?event? of Monday, or as a disconnected
(non-convex) interval of all Mondays.
3.2 Distributive Representation (Any)
The meaning any Monday is represented as the
SCE any_n (1, mon) and as a timeline FSA with
2All could be regarded as universal quantification, and any
resembles existential quantification, but because of some dif-
ferences, we avoid using these logical terms.
3Each transition in the figures corresponds to a number
of states and transitions between them in the actual timeline
FSA, as the representation of each day consists of the calendar
day brackets, symbols for the day of the week and the day of
the month, and possibly finer granularities inside.
each Monday marked on a separate, alternative
timeline of its own, as illustrated in Fig. 2. We call
this the distributive representation of an SCE.4
Su
{Mo}
Mo Tu Su
{Mo}
Mo Tu Su
{Mo}
Tu Su Mo Tu Su Mo Tu
Figure 2: A timeline FSA representing the SCE
any_n (1, mon) denoting any single Monday
For numerical quantification, we combine col-
lective and distributive representation. For exam-
ple, any_n (3, mon) (three Mondays) is represented
as a timeline FSA defining a set of timelines, each
with three Mondays marked, so that the set covers
all possible combinations. A collective representa-
tion alone would not suffice: a single timeline with
all periods of three Mondays marked would in ef-
fect represent all Mondays.
3.3 Collective Representation as Primary
In natural language, an unquantified calendar
phrase, such as Monday, is typically underspeci-
fied and refers to the closest preceding or follow-
ing Monday relevant in the context. In the brack-
eting FST model, however, we represent unquanti-
fied expressions collectively: the SCE mon repre-
sents both Monday and all Mondays.
A major practical reason for preferring the col-
lective representation is that it is easy to construct
a bracketing FST that splits a single timeline with
each Monday marked to a set of alternative time-
lines, each with only one of the Mondays marked,
whereas the converse operation is not possible. In
effect, a bracketing FST can only refer to symbols
on a single timeline at a time.
A single timeline with all Mondays marked
can also be used as a basis for such an SCE as
nth_following (3, mon, easter) (the third Monday
after Easter). The bracketing FST correspond-
ing to nth_following gets as its input a timeline
with each Monday and each Easter marked. It
then counts the third Monday after each Easter and
marks it. This would be much more difficult with
each Monday marked on a timeline of its own.
3.4 Combining Quantified SCEs
Multiple SCEs with quantification can be com-
bined appropriately. For example, all Mondays in
4We deviate from the common terminology that uses col-
lective for all Mondays and distributive for each Monday.
72
any (single) May is represented by a timeline FSA
defining a set of timelines, each with all Mondays
of a single May marked, and any (single) Monday
in every May by one with a single Monday marked
in each May, covering all possible combinations.
Combinations of quantified SCEs can often be
represented compositionally with intersection. For
example, all Mondays in every May is represented
as intersect (mon, may), all Mondays in any May
as intersect (mon, any_n (1, may)) and any Mon-
day in any May as intersect (any_n (1, mon), any_n
(1, may)).
However, any Monday in every May cannot be
represented simply with intersection, since inter-
secting a set of timelines, each with only one
Monday marked, and a timeline with every May
marked would result in timelines with one Mon-
day marked in (at most) one May. For this case,
we have defined the SCE operation n_within_each
(n, period, frame), which marks n periods within
each frame. Any Monday in every May is then rep-
resented as n_within_each (1, mon, may).
3.5 Other Uses of Distributive Representation
A distributive representation is obligatory for an
SCE denoting possibly overlapping periods of
time, although each alternative timeline may con-
tain several non-overlapping periods.
For example, we represent n_consecutive (3,
calday) (three consecutive days) as a set of time-
lines, each with one possible combination of non-
overlapping periods of three consecutive days
marked.5 If each timeline had only a single period
marked, it would complicate representing such an
SCE as union (intersect (n_consecutive (3, cal-
day), may), intersect (n_consecutive (3, calday),
jun)) (three consecutive days in May and in June)
so that the periods of consecutive days in May and
June are marked on the same timeline. In contrast,
a single timeline with all possible non-overlapping
periods marked would not cover the periods over-
lapping with the marked ones.
A distributive representation is also used for
SCEs containing a distributive union operation to
represent a disjunctive meaning. For example,
distr_union (union (mon, fri), union (tue, sun))
5Since consecutive days are adjacent, they can be enclosed
in marker brackets and treated as a single connected period,
with several periods on a single timeline. In contrast, the mul-
tiple disconnected periods of three consecutive Mondays can-
not be represented in a general way on the same timeline in the
bracketing FST model, but only by having a different marker
bracket index for each period of three Mondays.
(Monday and Friday or Tuesday and Sunday) is
represented as a timeline FSA defining two time-
lines, one with every Monday and Friday marked
and the other with every Tuesday and Sunday.
3.6 Distributive Representation and First
Distributive representation requires special consid-
erations in conjunction with some SCE operations,
most notably nth_within (n, period, frame), which
marks the nth period within each longer period
frame. Although a period might be the nth marked
one within a frame on one timeline, alternative
timelines might contain earlier occurrences.
As an example, we consider nth_within (1,
n_consecutive (3, workday), jun) (the first period
of three consecutive working days in June).6 For
a June beginning on a Sunday, we have alterna-
tive timelines with the first period of three working
days beginning on Monday, Tuesday and Wednes-
day, but we would like to mark only the one be-
ginning on Monday. However, a bracketing FST
cannot refer to the alternative timelines to test if
any of them contains an earlier applicable period.
As a solution, we have such operations as
n_consecutive insert an alternative marker bracket
(denoted by [. . . ] below) on each timeline into each
position in which it adds a marker bracket ({. . . })
on another timeline. The following simplified al-
ternative timelines illustrate the example above:
Su { Mo [ Tu [ We } Th ] Fr ] Sa
Su [ Mo { Tu [ We ] Th } Fr ] Sa
Su [ Mo [ Tu { We ] Th ] Fr } Sa
The operation nth_within seeks the first marked pe-
riod in June with no opening alternative marker
bracket between it and the beginning of June.
4 Implication: All or Nothing
In some applications, an SCE may denote a set of
periods of times all of which should be contained
in those denoted by another SCE, or if impossi-
ble, none of them should be. For example, all the
class times of a course with compulsory attendance
should coincide with the free slots of time in the
calendar of a student wishing to attend the course.
An intersection of the class times and the student?s
free slots of time would also contain partial results
if he or she could attend only some of the classes.
6The SCE nth_within (1, n_consecutive (3, calday), jun)
(the first period of three consecutive (calendar) days in June)
denotes the same as first_n_within (3, calday, jun) (the first
three days in June). A similar rephrasing would not be correct
for working days, however.
73
To obtain the desired result, we use the opera-
tion impl (a, b) to mark all the periods a if and only
if they all are fully contained in the periods b. If a
point of time is in a, it must also be in b for a to
be marked, so the operation can be regarded of as
a kind of an implication a? b. Above, we would
compute impl (course, student_free).
A course with alternative instances would be
represented with a timeline FSA defining an alter-
native timeline for each instance. The bracketing
FST corresponding to the above implication would
then mark the class times of each instance that is
completely within the student?s free slots of time.
5 Related Work
We briefly mention the approaches to quantifica-
tion of some research related to the bracketing
FST model in purpose or coverage. TEL (En-
driss, 1998) represents universally quantified ex-
pressions like unquantified ones. TEL has numer-
ical quantification and quantifier negation. TCNL
(Han and Lavie, 2004) represents universal quan-
tification as an enumeration. An unquantified ex-
pression denotes an underspecified time. Like
TEL, TCNL has no explicit existential quantifica-
tion. Ohlbach and Gabbay (1998) represent uni-
versal quantification with a parametrized modal
operator ?always within a period? and existential
with ?sometime within a period?. Cukierman and
Delgrande (1998) represent quantified expressions
in a way resembling that of ours but unquantified
expressions effectively as existentially quantified.
TimeML (Saur? et al, 2006) represents quantifica-
tion by quantifier and frequency attributes. OWL-
Time (Pan and Hobbs, 2005) uses temporal aggre-
gates for universal and numerical quantification.
6 Discussion and Further Work
In our view, SCEs corresponding to typical quan-
tified calendar phrases can be represented in the
bracketing FST model fairly naturally, although
the naturalness of representing unquantified, un-
derspecified phrases collectively can be disputed,
and the representation of any Monday in every May
is not compositional. Implication, in turn, would
seem useful for representing a set of periods of
time fully contained in another set of periods.
Although some types of calendar information
are impossible to represent exactly or naturally
with finite-state methods, we find the bracketing
FST model a promising representation for many
common types. However, to be usable in practice,
the model needs further work in both coverage and
efficiency. Moreover, applications would benefit
from a component to parse a (restricted) natural-
language calendar phrase to an SCE and another
one to generate the former from the latter.
Acknowledgements
This paper represents independent work by the first
author based on the suggestions of the second au-
thor and funded by the Graduate School of Lan-
guage Technology in Finland. We thank the anony-
mous reviewers for their valuable comments.
References
Cukierman, Diana and James P. Delgrande. 1998.
Expressing time intervals and repetition within a
formalization of calendars. Computational Intelli-
gence, 14(4):563?597.
Endriss, Ulrich. 1998. Semantik zeitlicher Ausdr?cke
in Terminvereinbarungsdialogen. Verbmobil Report
227, Technische Universit?t Berlin, Fachbereich In-
formatik, Berlin, August.
Han, Benjamin and Alon Lavie. 2004. A framework
for resolution of time in natural language. ACM
Transactions on Asian Language Information Pro-
cessing (TALIP), 3(1):11?32, March.
Niemi, Jyrki and Kimmo Koskenniemi. 2007. Repre-
senting calendar expressions with finite-state trans-
ducers that bracket periods of time on a hierarchi-
cal timeline. In Nivre, Joakim, Heiki-Jaan Kaalep,
Kadri Muischnek, and Mare Koit, editors, Pro-
ceedings of the 16th Nordic Conference of Compu-
tational Linguistics NODALIDA-2007, pages 355?
362, Tartu, Estonia. University of Tartu.
Niemi, Jyrki and Kimmo Koskenniemi. 2008. Rep-
resenting and combining calendar information by
using finite-state transducers. In Proceedings
of the Seventh International Workshop on Finite-
State Methods and Natural Language Processing
(FSMNLP) 2008. To appear.
Ohlbach, Hans J?rgen and Dov Gabbay. 1998. Calen-
dar logic. Journal of Applied Non-classical Logics,
8(4):291?324.
Pan, Feng and Jerry R. Hobbs. 2005. Temporal aggre-
gates in OWL-Time. In Proceedings of the 18th In-
ternational Florida Artificial Intelligence Research
Society Conference (FLAIRS-2005), pages 560?565,
Clearwater Beach, Florida. AAAI Press.
Saur?, Roser, Jessica Littman, Bob Knippen, Robert
Gaizauskas, Andrea Setzer, and James Pustejovsky.
2006. TimeML annotation guidelines, version 1.2.1.
http://timeml.org/site/publications/timeMLdocs/
annguide_1.2.1.pdf, January.
74
Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 38?45,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
A Method for Compiling Two-level Rules with Multiple Contexts 
 
 
Kimmo Koskenniemi 
University of Helsinki 
Helsinki, Finland  
kimmo.koskenniemi@helsinki.fi 
Miikka Silfverberg 
University of Helsinki 
Helsinki, Finland 
miikka.silfverberg@helsinki.fi 
 
  
 
Abstract 
A novel method is presented for compiling 
two-level rules which have multiple context 
parts. The same method can also be applied 
to the resolution of so-called right-arrow rule 
conflicts. The method makes use of the fact 
that one can efficiently compose sets of two-
level rules with a lexicon transducer. By in-
troducing variant characters and using simple 
pre-processing of multi-context rules, all 
rules can be reduced into single-context rules. 
After the modified rules have been combined 
with the lexicon transducer, the variant char-
acters may be reverted back to the original 
surface characters. The proposed method ap-
pears to be efficient but only partial evidence 
is presented yet.   
1 Introduction 
Two-level rules can be compiled into length-
preserving transducers whose intersection effec-
tively reflects the constraints and the correspon-
dences imposed by the two-level grammar. Two-
level rules relate input strings (lexical representa-
tions) with output strings (surface representa-
tions). The pairs of strings are treated as charac-
ter pairs x:z consisting of lexical (input) char-
acters x and surface (output) characters z, and 
regular expressions based on such pairs.  Two-
level rule transducers are made length-preserving 
(epsilon-free) by using a place holder zero (0) 
within the rules and in the representations.  The 
zero is then removed after the rules have been 
combined by (virtual) intersection, before the 
result is composed with the lexicon.  There are 
four kinds of two-level rules:  
1. right-arrow rules or restriction rules, 
(x:z => LC _ RC) saying that the 
correspondence pair is allowed only if 
immediately preceded by left context LC 
and followed by right context RC, 
2. left-arrow rules or surface coercion 
rules,  (x:z <= LC _ RC) which say 
that in this context, the lexical character 
x may  only  correspond  to  the  surface  
character z, 
3. double-arrow rules (<=>), a shorthand 
combining these two requirements, and 
4. exclusion rules (x:z /<= LC _ RC) 
which forbid the pair x:z  to occur in 
this context. 
All types of rules may have more than one 
context part. In particular, the right-arrow rule  
x:z => LC1 _ RC1; LC2 _ RC2 would 
say that the pair x:z (which we call the centre 
of the rule) may occur in either one of these two 
contexts.  For various formulations of two-level 
rules, see e.g. (Koskenniemi, 1983), (Grimley-
Evans et al, 1996), (Black et.al., 1987), (Ruess-
ink, 1989), (Ritchie, 1992), (Kiraz, 2001) and a 
comprehensive survey on their formal interpreta-
tions, see (Vaillette, 2004). 
Compiling two-level rules into transducers is 
easy in all other cases except for right-arrow 
rules with multiple context-parts; see e.g. 
Koskenniemi (1983). Compiling right-arrow 
rules with multiple context parts is more difficult 
because the compilation of the whole rule is not 
in a simple relation to the component expressions 
in the rule; see e.g. Karttunen et al (1987).   
The method proposed here reduces multi-
context rules into a set of separate simple rules, 
one for each context, by introducing some auxil-
iary variant characters.  These auxiliary charac-
ters are then normalized back into the original 
surface characters after the intersecting composi-
tion of the lexicon and the modified rules. The 
method is presented in section 3. The compila-
tion of multiple contexts using the proposed 
scheme appears to be very simple and fast.  Pre-
liminary results and discussion about the compu-
tational complexity are presented in section 4. 
38
1.1 The compilation task with an example 
We make use of a simplified linguistic example 
where a stop k is realized as v between identical 
rounded close vowels (u, y). The example re-
sembles one detail of Finnish consonant grada-
tion but it is grossly simplified. According to the 
rule in the example, the lexical representation 
pukun would be realized as the surface repre-
sentation puvun.  This correspondence is tradi-
tionally represented as: 
p u k u n 
p u v u n 
where the upper tier represents the lexical or 
morphophonemic representation which we inter-
pret as the input, and the lower one corresponds 
to the surface representation which we consider 
as the output. 1   This two-tier representation is 
usually represented on a single line as a sequence 
of input and output character pairs where pairs of 
identical characters, such as p:p are abbreviated 
as a single p.  E.g.  the above pair  of  strings  be-
comes a string of pairs:  
p u k:v u n 
In our example we require that the correspon-
dence k:v may occur only between two identi-
cal rounded close vowels, i.e. either between two 
letters u or between two letters y. Multiple con-
texts are needed in the right-arrow rule which 
expresses this constraint. As a two-level gram-
mar, this would be: 
Alphabet a b ? k ? u v w ? 
 k:v; 
Rules 
k:v => u _ u; 
       y _ y;  
This grammar would permit sequences such as:  
p u k:v u n 
k y k:v y n 
p u k:v u k:v u n 
l u k:v u n k y k:v y n 
t u k k u 
but it would exclude sequences: 
p u k:v y n 
t u k:v a n 
                                               
1 In Xerox terminology, the input or lexical characters 
are called the upper characters, and the output or sur-
face characters are called the lower characters. Other 
orientations are used by some authors. 
Whereas one can always express  multi-
context left-arrow rules (<=) and exclusion rules 
(/<=)  equivalently  as  separate  rules,  this  does  
not  hold  for  right-arrow rules.  The  two  separate  
rules 
k:v => u _ u; 
k:v => y _ y;  
would be in conflict with each other permitting 
no occurrences of k:v at all, (unless we apply 
so-called conflict resolution which would effec-
tively combine the two rules back to a single rule 
with two context parts). 
2 Previous compilation methods 
The first compiler of two-level rules was imple-
mented by the first author in 1985 and it handled 
also multi-context rules (Koskenniemi, 1985). 
The compiler used a finite-state package written 
by Ronald Kaplan and Martin Kay at Xerox 
PARC, and a variant of a formula they used for 
compiling cascaded rewrite rules. Their own 
work was not published until 1994. Kosken-
niemi?s compiler was re-implemented in LISP by 
a student in her master?s thesis (Kinnunen, 
1987).  
Compilation of two-level rules in general re-
quires  some  care  because  the  centres  may  occur  
several times in pair strings, the contexts may 
overlap and the centres may act as part of a con-
text for another occurrence of the same centre.  
For other rules than right-arrow rules, each con-
text is yet another condition for excluding un-
grammatical  strings  of  pairs,  which  is  how  the  
rules are related to each other.  The context parts 
of a right-arrow rule are, however, permissions, 
one of which has to be satisfied.  Expressing un-
ions of context parts was initially a problem 
which required complicated algorithms. 
Some of the earlier compilation methods are 
mentioned below. They all produce a single 
transducer out of each multi-context right-arrow 
rule. 
2.1 Method based on Kaplan and Kay 
Kaplan and Kay (1994) developed a method 
around 1980 for compiling rewriting rules into 
finite-state transducers 2 . The method was 
adapted by Koskenniemi to the compilation of 
two-level rules by modifying the formula 
                                               
2 Douglas Johnson (1972) presented a similar tech-
nique earlier but his work was not well known in 
early 1980s. 
39
slightly. In this method, auxiliary left and right 
bracket characters (<1, >1, <2, >2, ...) 
were freely added in order to facilitate the check-
ing of the context conditions.  A unique left and 
right bracket was dedicated for each context part 
of  the  rule.   For  each  context  part  of  a  rule,  se-
quences with freely added brackets were then 
filtered with the context expressions so that only 
such sequences remained where occurrences of 
the brackets were delimited with the particular 
left or right context (allowing free occurrence of 
brackets for other context parts). Thereafter, it 
was easy to check that all occurrences of the cen-
tre  (i.e.  the  left  hand  part  of  the  rule  before  the  
rule operator) were delimited by some matching 
pair of brackets. As all component transducers in 
this expression were length-preserving (epsilon-
free), the constraints could be intersected with 
each other resulting in a single rule transducer 
for the multi-context rule (and finally the brack-
ets could be removed). 
2.2 Method of Grimley-Evans, Kiraz and 
Pulman 
Grimley-Evans, Kiraz and Pulman presented a 
simpler compilation formula for two-level rules 
(1996).  The method is prepared to handle more 
than two levels of representation, and it does not 
need the freely added brackets in the intermedi-
ate  stages.   Instead,  it  uses  a  marker  for  the rule  
centre and can with it express disjunctions of 
contexts.  Subtracting such a disjunction from all 
strings where the centre occurs expresses all pair 
strings which violate the multi-context rule.  
Thus, the negation of such a transducer is the 
desired result. 
2.3 Yli-Jyr??s method 
Yli-Jyr? (Yli-Jyr? et al, 2006) introduced a 
concept of Generalized Restriction (GR) where 
expressions with auxiliary boundary characters  
made it possible to express context parts of rules 
in a natural way, e.g. as: 
Pi* LC  Pi  RC Pi*   
Here Pi is the set of feasible pairs of characters 
and LC and RC are the left and right contexts. 
The two context parts of our example would cor-
respond to the following two expressions: 
Pi* u  Pi  u Pi* 
Pi* y  Pi  y Pi*     
Using such expressions, it is easy to express dis-
junctions of contexts as unions of the above ex-
pressions. This makes it logically simple to com-
pile multi-context right-arrow rules. The rule 
centre x:z can be expressed simply as: 
Pi*  x:z  Pi* 
The right-arrow rule  can be expressed as  an im-
plication where the expression for the centre im-
plies the union of the context parts.  Thereafter, 
one may just remove the auxiliary boundary 
characters, and the result is the rule-transducer. 
(It is easy to see that only one auxiliary character 
is needed when the length of the centres is one.) 
The compilation of rules with centres whose 
length is one using the GR seems very similar to 
that  of  Grimley-Evans  et  al.   The  nice  thing  
about GR is that one can easily express various 
rule types, including but not limited to the four 
types listed above. 
2.4 Intersecting compose 
It was observed somewhere around 1990 at 
Xerox that the rule sets may be composed with 
the lexicon transducers in an efficient way and 
that the resulting transducer was roughly similar 
in size as the lexicon transducer itself (Karttunen 
et al, 1992). This observation gives room to the 
new approach presented below. 
At that time, it was not practical to intersect 
complete two-level grammars if they contained 
many elaborate rules (and this is still a fairly 
heavy operation).  Another useful observation 
was that the intersection of the rules could be 
done in a joint single operation with the compo-
sition (Karttunen, 1994).  Avoiding the separate 
intersection made the combining of the lexicon 
and rules feasible and faster.  In addition to 
Xerox LEXC program, e.g. the HFST finite-state 
software contains this operation and it is rou-
tinely used when lexicons and two-level gram-
mars are combined into lexicon transducers 
(Lind?n et al, 2009). 
M?ns Huld?n has noted (2009) that the com-
posing of the lexicon and the rules is sometimes 
a heavy operation, but can be optimized if one 
first composes the output side of the lexicon 
transducer with the rules, and thereafter the 
original lexicon with this intermediate result. 
3 Proposed method for compilation 
The  idea  is  to  modify  the  two-level  grammar  so  
that the rules become simpler. The modified 
grammar will contain only simple rules with sin-
gle context parts. This is done at the cost that the 
grammar will transform lexical representations 
into slightly modified surface representations.  
40
The surface representations are, however, fixed 
after the rules have been combined with the lexi-
con so that the resulting lexicon transducer is 
equivalent to the result produced using earlier 
methods. 
3.1 The method through the example 
Let us return to the example in the introduction. 
The modified surface representation differs from 
the ultimate representation by having a slightly 
extended alphabet where some surface characters 
are expressed as their variants, i.e. there might be 
v1 or v2 in addition to v.  In particular, the first 
variant v1 will  be  used  exactly  where  the  first  
context of the original multi-context rule for k:v 
is satisfied, and v2 where the second context is 
satisfied. After extending the alphabet and split-
ting  the  rule,  our  example  grammar  will  be  as  
follows: 
Alphabet a b ? k ? u v w x y ? 
  k:v1 k:v2; 
Rules 
k:v1 => u _ u; 
k:v2 => y _ y;  
These rules would permit sequences such as:  
p u k:v1 u n 
k y k:v2 y n 
p u k:v1 u k:v1 u n  
but exclude a sequence  
p u k:v2 u n    
The output of the modified grammar is now as 
required, except that it includes these variants v1 
and v2 instead of v.   If  we first  perform the in-
tersecting composition of the rules and the lexi-
con, we then can compose the result with a trivial 
transducer which simply transforms both v1 and 
v2 into v. 
It  should  be  noted  that  here  the  context  ex-
pressions of these example rules do not contain v 
on the output side, and therefore the introduction 
of the variants v1 and v2 causes no further 
complications.   In  the  general  case,  the  variants  
should be added as alternatives of v in  the  con-
text expressions, see the explanation below. 
3.2 More general cases 
The strategy is to pre-process the two-level 
grammar in steps by splitting more complex con-
structions into simpler ones until we have units 
whose components are trivial to compile.  The 
intersection of the components will have the de-
sired effect when composed with a lexicon and a 
trivial correction module.   Assume, for the time 
being, that all centres (i.e. the left-hand parts) of 
the rules are of length one. 
(1) Split double-arrow (<=>) rules into one 
right-arrow (=>)  rule  and  one  left-arrow  (<=) 
rule with centres and context parts identical to 
those of the original double-arrow rule. 
(2) Unfold the iterative where clauses in left-
arrow rules by establishing a separate left-arrow 
rule for each value of the iterator variable, e.g. 
V:Vb <= [a | o | u] ?* _; 
 where V in (A O U) 
       Vb in (a o u) matched; 
becomes 
A:a <= [a | o | u] ?* _; 
O:o <= [a | o | u] ?* _; 
U:u <= [a | o | u] ?* _; 
Unfold the where clauses in right-arrow rules 
in  either  of  the  two  ways:  (a)  If  the  where 
clauses create disjoint centres (as above), then 
establish a separate right-arrow rule for each 
value  of  the  variable,  and  (b)  if  the  clause  does  
not affect the centre, then create a single multi-
context right-arrow rule whose contexts consist 
of the context parts of the original rule by replac-
ing the where clause variable by its values, one 
value at a time, e.g. 
k:v => Vu _ Vu; where Vu in (u y); 
becomes 
k:v => u _ u; 
       y _ y; 
If  there  are  set  symbols  or  disjunctions  in  the  
centres  of  a  right-arrow  rule,  then  split  the  rule  
into separate rules where each rule has just a sin-
gle pair as its centre, and the context part is iden-
tical to the context part (after the unfolding of the 
where clauses). 
Note that these two first steps would probably 
be common to any method of compiling multi-
context rules.  After these two steps, we have 
right-arrow, left-arrow and exclusion rules.  The 
right-arrow  rules  have  single  pairs  as  their  cen-
tres. 
(3) Identify the right-arrow rules which, after 
the unfolding, have multiple contexts, and record 
each  pair  which  is  the  centre  of  such  a  rule.   
Suppose that the output character (i.e. the surface 
character) of such a rule is z and there are n con-
text parts in the rule, then create n new auxiliary 
characters z1, z2, ..., zn  and denote the set consist-
ing of them by S(z).  
41
Split the rule into n distinct single-context 
right-arrow rules by replacing the z of the centre 
by each zi in turn. 
Our simple example rule becomes now. 
k:v1 => u _ u; 
k:v2 => y _ y; 
(4) When all rules have been split according to 
the above steps, we need a post-processing phase 
for the whole grammar. We have to extend the 
alphabet by adding the new auxiliary characters 
in it. If original surface characters (which now 
have variants) were referred to in the rules, each 
such reference must be replaced with the union 
of the original character and its variants. This 
replacement has to be done throughout the  
grammar. For any existing pairs x:z listed in the 
alphabet, we add there also the pairs x:z1, ..., x:zn.  
The same is done for all declarations of sets 
where z occurs  (as  an output  character).  Insert  a  
declaration for a new character set corresponding 
to S(z).  In  all  define  clauses  and  in  all  rule-
context expressions where z occurs as an output 
character,  it  is  replaced  by  the  set  S(z).   In  all  
centres of left-arrow rules where z occurs  as  the 
output character, it is replaced by S(z).  
The  purpose  of  this  step  is  just  to  make  the  
modified two-level grammar consistent in terms 
of its alphabet, and to make the modified rules 
treat  the occurrence of  any of  the output  charac-
ters z1,  z2,  ?, zn in the same way as the original 
rule treated z wherever it occurred in its contexts. 
 
After this pre-processing we only have right-
arrow, left-arrow and exclusion rules with a sin-
gle context part.  All rules are independent of 
each  other  in  such  a  way  that  their  intersection  
would have the effect we wish the grammar to 
have.  Thus, we may compile the rule set as such 
and  each  of  these  simple  rules  separately.   Any  
of the existing compilation formulas will do. 
After compiling the individual rules, they have 
to be intersected and composed with the lexicon 
transducer which transforms base forms and in-
flectional feature symbols into the morphopho-
nemic representation of the word-forms. The 
composing and intersecting is efficiently done as 
a single operation because it then avoids the pos-
sible explosion which can occur if intermediate 
result of the intersection is computed in full.   
The rules are mostly independent of each 
other, capable of recurring freely. Therefore 
something near the worst case complexity is 
likely  to  occur,  i.e.  the  size  of  the  intersection  
would have many states, roughly proportional to 
the  product  of  the  numbers  of  the  states  in  the  
individual rule transducers. 
The composition of the lexicon and the logical 
intersection of the modified rules is almost iden-
tical  to  the  composition  of  the  lexicon  and  the  
logical intersection of the original rules. The only 
difference is that the output (i.e. the surface) rep-
resentation contains some auxiliary characters zi 
instead of the original surface characters z. A 
simple transducer will correct this. (The trans-
ducer has just one (final) state and identity transi-
tions for all original surface characters and a re-
duction zi:z for  each of  the auxiliary characters.)   
This composition with the correcting transducer 
can be made only after the rules have been com-
bined with the lexicon.   
3.3 Right-arrow conflicts 
Right-arrow rules are often considered as per-
missions.   A  rule  could  be  interpreted  as  ?this  
correspondence pair may occur if the following 
context condition is met?.  Further permissions 
might  be  stated  in  other  rules.  As  a  whole,  any  
occurrence must get at least one permission in 
order to be allowed.  
The right-arrow conflict resolution scheme 
presented by Karttunen implemented this 
through an extensive pre-processing where the 
conflicts were first detected and then resolved 
(Karttunen et al, 1987). The resolution was done 
by copying context parts among the rules in con-
flict.  Thus, what was compiled was a grammar 
with rules extended with copies of context parts 
from other rules.  
The scenario outlined above could be slightly 
modified in order to implement the simple right-
arrow rule  conflict  resolution  in  a  way  which  is  
equivalent to the solution presented by Kart-
tunen.   All  that  is  needed is  that  one would first  
split the right-arrow rules with multiple context 
parts into separate rules.  Only after that, one 
would consider all right-arrow rules and record 
rules  with identical  centres.   For  groups of  rules  
with identical centres, one would introduce the 
further variants of the surface characters, a sepa-
rate  variant  for  each  rule.   In  this  scheme,  the  
conflict resolution of right-arrow rules is imple-
mented fairly naturally in a way analogous to the 
handling of multi-context rules. 
3.4 Note on longer centres in rules 
In the above discussion, the left-hand parts of 
rules,  i.e.  their  centres,  were  always  of  length  
one.  In fact, one may define rules with longer 
centres by a scheme which reduces them into 
42
rules with length one centres.  It appears that the 
basic rule types (the left and right-arrow rules) 
with longer centres can be expressed in terms of 
length one centres, if we apply conflict resolution 
for the right-arrow rules. 
We replace a right-arrow rule, e.g. 
x1:z1 x2:z2 ... xk:zk => LC _ RC; 
with k separate rules 
x1:z1 => LC _ x2:z2 ... xk:zk RC; 
x2:z2 => LC x1:z1 _ ... xk:zk RC; 
... 
xk:zk => LC x1:z1 x2:z2 ... _ RC; 
Effectively, each input character may be realized 
according  to  the  original  rule  only  if  the  rest  of  
the  centre  will  also  be  realized  according  to  the  
original rule.  
Respectively, we replace a left-arrow rule, e.g. 
x1:z1 x2:z2 ... xk:zk <= LC _ RC; 
with k separate rules 
x1:z1 <= LC _ x2: ... xk: RC; 
x2:z2 <= LC x1: _ ... xk: RC; 
... 
xk:zk <= LC x1: x2: ... _ RC; 
Here the realization of the surface string is forced 
for each of its character of the centre separately, 
without reference to what happens to other char-
acters  in  the  centre.   (Otherwise  the  contexts  of  
the separate rules would be too restrictive, and 
allow the default realization as well.) 
 
4 Complexity and implementation 
In order to implement the proposed method, one 
could write a pre-processor which just transforms 
the  grammar  into  the  simplified  form,  and  then  
use an existing two-level compiler. Alternatively, 
one could modify an existing compiler, or write a 
new compiler which would be somewhat simpler 
than the existing ones. We have not implemented 
the proposed method yet, but rather simulated the 
effects using existing two-level rule compilers. 
Because the pre-processing would be very fast 
anyway, we decided to estimate the efficiency of 
the proposed method through compiling hand-
modified rules with the existing HFST-TWOLC 
(Lind?n et al, 2009) and Xerox TWOLC3 two-
                                               
3 We used an old version 3.4.10 (2.17.7) which we 
thought would make use of the Kaplan and Kay for-
mula.  We suspected that the most recent versions 
might have gone over to the GR formula. 
level rule compilers. The HFST tools are built on 
top of existing open source finite-state packages 
OpenFST (Allauzen et al, 2007) and Helmut 
Schmid?s SFST (2005).  
It appears that all normal morphographemic 
two-level grammars can be compiled with the 
methods of Kaplan and Kay, Grimley-Evans and 
Yli-Jyr?. 
Initial tests of the proposed scheme are prom-
ising.  The  compilation  speed  was  tested  with  a  
grammar of consisting of 12 rules including one 
multi-context rule for Finnish consonant grada-
tion with some 8 contexts and a full Finnish lexi-
con. When the multi-context rule was split into 
separate rules, the compilation was somewhat 
faster (12.4 sec) to than when the rule was com-
piled a multi-context rule using the GR formula 
(13.9 sec).  The gain in the speed by splitting was 
lost at the additional work needed in the inter-
secting compose of the rules and the full lexicon 
and the final fixing of the variants. On the whole, 
the proposed method had no advantage over the 
GR method. 
In  order  to  see  how  the  number  of  context  
parts affects the compilation speed, we made 
tests with an extreme grammar simulating Dutch 
hyphenation rules. The hyphenation logic was 
taken out of TeX hyphenation patterns which had 
been converted into two-level rules. The first 
grammar consisted of a single two-level rule 
which had some 3700 context parts. This gram-
mar could not be compiled using Xerox TWOLC 
which applies the Kaplan and Kay method be-
cause more than 5 days on a dedicated Linux 
machine with 64 GB core memory was not 
enough for completing the computation.  When 
using  of  GR  method  of  HFST-TWOLC,  the  
compilation time was not a problem (34 min-
utes).  The method of Grimley-Evans et al 
would probably have been equally feasible.  
Compiling the grammar after splitting it into 
separate rules as proposed above was also feasi-
ble: about one hour with Xerox TWOLC and 
about 20 hours with HFST-TWOLC. The differ-
ence between these two implementations de-
pends most likely on the way they handle alpha-
bets.  The Xerox tool makes use of a so-called 
'other' symbol which stands for characters not 
mentioned in the rule. It also optimizes the com-
putation by using equivalence classes of charac-
ter pairs.  These make the compilation less sensi-
tive to the 3700 new symbols added to the alpha-
bet than what happens in the HFST routines.    
Another test was made using a 50 pattern sub-
set of the above hyphenation grammar.  Using 
43
the  Xerox  TWOLC,  the  subset  compiled  as  a  
multi-context rule in 28.4 seconds, and when 
split according to the method proposed here, it 
compiled in 0.04 seconds. Using the HFST-
TWOLC, the timings were 3.1 seconds and 5.4 
seconds, respectively.   These results corroborate 
the intuition that the Kaplan and Kay formula is 
sensitive  to  the  number  of  context  parts  in  rules  
whereas  the  GR  formula  is  less  sensitive  to  the  
number of context parts in rules. 
There are factors which affect the speed of 
HFST-TWOLC, including the implementation 
detail including the way of treating characters or 
character pairs which are not specifically men-
tioned in a particular transducer. We anticipate 
that there is much room for improvement in 
treating larger alphabets in HFST internal rou-
tines and there is no inherent reason why it 
should be slower than the Xerox tool. The next 
release of HFST will use Huld?n?s FOMA finite-
state package. FOMA implements the ?other? 
symbol and is expected to improve the process-
ing of larger alphabets. 
Our intuition and observation is that the pro-
posed compilation phase requires linear time 
with  respect  to  the  number  of  context  parts  in  a  
rule. Whether the proposed compilation method 
has an advantage over the compilation using the 
GR  or  Grimley-Evans  formula  remains  to  be  
seen. 
5 Acknowledgements 
Miikka Silfverberg, a PhD student at Finnish graduate 
school Langnet and the author of HFST-TWOLC 
compiler. His contribution consists of making all tests 
used here to estimate and compare the efficiency of 
the compilation methods.   
The current work is part of the FIN-CLARIN infra-
structure project at the University of Helsinki funded 
by the Finnish Ministry of Education. 
References 
Cyril Allauzen, Michael Riley, Johan Schalkwyk, 
Wojciech Skut and Mehryar Mohri. 2007. 
OpenFst: A General and Efficient Weighted Finite-
State Transducer Library. In Implementation and 
Application of Automata, Lecture Notes in Com-
puter Science. Springer, Vol. 4783/2007, 11-23. 
Alan Black, Graeme Ritchie, Steve Pulman, and Gra-
ham Russell. 1987. ?Formalisms for morphogra-
phemic description?. In Proceedings of the Third 
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, 11?18. 
Edmund Grimley-Evans, Georg A. Kiraz, Stephen G. 
Pulman. 1996. Compiling a Partition-Based Two-
Level Formalism. In COLING 1996, Volume 1: 
The 16th International Conference on Computa-
tional Linguistics, pp. 454-459. 
Huld?n, M?ns. 2009. Finite-State Machine Construc-
tion Methods and Algorithms for Phonology and 
Morphology. PhD Thesis, University of Arizona. 
Douglas C. Johnson. 1972. Formal Aspects of Phono-
logical Description. Mouton, The Hague. 
Ronald M. Kaplan and Martin Kay. 1994. Regular 
Models of Phonological Rule Systems. Computa-
tional Linguistics 20(3): 331?378. 
Lauri Karttunen. 1994. Constructing lexical transduc-
ers. In Proceedings of the 15th conference on 
Computational linguistics, Volume 1. pp. 406-411. 
Lauri Karttunen, Ronald M. Kaplan, and Annie Zae-
nen. 1992. Two-Level Morphology with Composi-
tion. Proceedings of the 14th conference on Com-
putational linguistics, August 23-28, 1992, Nantes, 
France. 141-148. 
Lauri Karttunen, Kimmo Koskenniemi, and Ronald. 
M. Kaplan. 1987. A Compiler for Two-level Pho-
nological Rules. In Dalrymple, M. et al Tools for 
Morphological Analysis. Center for the Study of 
Language and Information. Stanford University. 
Palo Alto.  
Maarit Kinnunen. 1987. Morfologisten s??nt?jen 
k??nt?minen ??rellisiksi automaateiksi. (Translat-
ing morphological rules into finite-state automata. 
Master?s thesis.). Department of Computer Sci-
ence, University of Helsinki 
George Anton Kiraz. 2001. Computational Nonlinear 
Morphology: With Emphasis on Semitic Lan-
guages. Studies in Natural Language Processing. 
Cambridge University Press, Cambridge. 
Kimmo Koskenniemi. 1983. Two-Level Morph-
ology: A General Computational Model for 
Word-form Recognition and Production. Uni-
versity of Helsinki, Department of General Lin-
guistics, Publications No. 11. 
Kimmo Koskenniemi. 1985. Compilation of automata 
from morphological two-level rules. In F. Karlsson 
(ed.), Papers from the fifth Scandinavian Confer-
ence of Computational Linguistics, Helsinki, De-
cember 11-12, 1985. pp. 143-149. 
Krister Lind?n, Miikka Silfverberg and Tommi Piri-
nen. 2009. HFST Tools for Morphology ? An Effi-
cient Open-Source Package for Construction of 
Morphological Analyzers. In State of the Art in 
Computational Morphology (Proceedings of Work-
shop on Systems and Frameworks for Computa-
tional Morphology, SFCM 2009). Springer. 
Graeme Ritchie. 1992. Languages generated by two-
level morphological rules?. Computational Lin-
guistics, 18(1):41?59. 
44
H. A. Ruessink. 1989. Two level formalisms. Utrecht 
Working Papers in NLP. Technical Report 5. 
Helmut Schmid. 2005. A Programming Language for 
Finite State Transducers. In Proceedings of the 5th 
International Workshop on Finite State Methods in 
Natural Language Processing (FSMNLP 2005). 
pp. 50-51. 
Nathan Vaillette. 2004. Logical Specification of Fi-
nite-State Transductions for Natural Language 
Processing. PhD Thesis, Ohio State University. 
Anssi Yli-Jyr? and Kimmo Koskenniemi. 2006. 
Compiling Generalized Two-Level Rules and 
Grammars.  International Conference on NLP: Ad-
vances in natural language processing. Springer. 
174 ? 185. 
 
45
