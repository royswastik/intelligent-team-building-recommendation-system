Generation of Relative Referring Expressions based on Perceptual Grouping
Kotaro FUNAKOSHI
Department of Computer Science
Tokyo Institute of Technology
Meguro O?okayama 2-12-1,
Tokyo 152-8552, Japan
koh@cl.cs.titech.ac.jp
Satoru WATANABE
Department of Computer Science
Tokyo Institute of Technology
Meguro O?okayama 2-12-1,
Tokyo 152-8552, Japan
satoru w@cl.cs.titech.ac.jp
Naoko KURIYAMA
Department of Human System Science
Tokyo Institute of Technology
Meguro O?okayama 2-12-1,
Tokyo 152-8552, Japan
kuriyama@hum.titech.ac.jp
Takenobu TOKUNAGA
Department of Computer Science
Tokyo Institute of Technology
Meguro O?okayama 2-12-1,
Tokyo 152-8552, Japan
take@cl.cs.titech.ac.jp
Abstract
Past work of generating referring expressions
mainly utilized attributes of objects and bi-
nary relations between objects. However, such
an approach does not work well when there
is no distinctive attribute among objects. To
overcome this limitation, this paper proposes a
method utilizing the perceptual groups of ob-
jects and n-ary relations among them. The key
is to identify groups of objects that are natu-
rally recognized by humans. We conducted psy-
chological experiments with 42 subjects to col-
lect referring expressions in such situations, and
built a generation algorithm based on the re-
sults. The evaluation using another 23 subjects
showed that the proposed method could effec-
tively generate proper referring expressions.
1 Introduction
In the last two decades, many researchers have stud-
ied the generation of referring expressions to enable
computers to communicate with humans about con-
crete objects in the world.
For that purpose, most past work (Appelt, 1985;
Dale and Haddock, 1991; Dale, 1992; Dale and
Reiter, 1995; Heeman and Hirst, 1995; Horacek,
1997; Krahmer and Theune, 2002; van Deemter,
2002; Krahmer et al, 2003) makes use of attributes
of an intended object (the target) and binary rela-
tions between the target and others (distractors) to
distinguish the target from distractors. Therefore,
these methods cannot generate proper referring ex-
pressions in situations where no significant surface
difference exists between the target and distractors,
and no binary relation is useful to distinguish the
target. Here, a proper referring expression means
a concise and natural linguistic expression enabling
hearers to distinguish the target from distractors.
For example, consider indicating object b to per-
son P in the situation shown in Figure 1. Note that
person P does not share the label information such
as a and b with the speaker. Because object b is
not distinguishable from objects a or c by means of
their appearance, one would try to use a binary re-
lation between object b and the table, i.e., ?A ball
to the right of the table?. 1 However, ?to the right
of ? is not a discriminatory relation, for objects a
and c are also located to the right of the table. Us-
ing a and c as a reference object instead of the ta-
ble does not make sense, since a and c cannot be
uniquely identified because of the same reason that
b cannot be identified. Such situations have never
drawn much attention, but can occur easily and fre-
quently in some domains such as object arrange-
ment (Tanaka et al, 2004).
van der Sluis and Krahmer (2000) proposed us-
ing gestures such as pointing in situations like those
shown in Figure 1. However, pointing and gazing
are not always available depending on the positional
relation between the speaker and the hearer.
In the situation shown in Figure 1, a speaker can
indicate object b to person P with a simple expres-
sion ?the front ball? without using any gesture. In
order to generate such an expression, one must be
able to recognize the salient perceptual group of the
objects and use the n-ary relative relations in the
group. 2
In this paper, we propose a method of generat-
1In this paper, we simply assume that all participants share
the appropriate reference frame (Levinson, 2003). We mention
this issue in the last section.
2Although Krahmer et al claim that their method can han-
dle n-ary relations (Krahmer et al, 2003), they provide no de-
tails. We think their method cannot directly handle situations
we discuss here.
ing referring expressions that utilizes n-ary relations
among members of a group. Our method recognizes
groups by using Tho?risson?s algorithm (Tho?risson,
1994). As the first step of our research project, we
deal with the limited situations where only homoge-
neous objects are randomly arranged (see Figure 2).
Therefore, we handle positional n-ary relation only,
and other types of n-ary relation such as size, e.g.,
?the biggest one?, are not mentioned.
Speakers often refer to multiple groups in the
course of referring to the target. In these cases, we
can observe two types of relations: the intra-group
relation such as ?the front two among the five near
the desk?, and the inter-group relation such as ?the
two to the right of the five?. We define that a sub-
sumption relation between two groups is an intra-
group relation.
In what follows, Section 2 explains the exper-
iments conducted to collect expressions in which
perceptual groups are used. The proposed method is
described and evaluated in Section 3. In Section 4,
we examine a possibility to predict the adequacy of
an expression in terms of perceptual grouping. Fi-
nally, we conclude the paper in Section 5.
P
a
b
c
Table
Figure 1: An example of problematic situations
2 Data Collection
We conducted a psychological experiment with 42
Japanese undergraduate students to collect referring
expressions in which perceptual groups are used. In
order to evaluate the collected expressions, we con-
ducted another experiment with a different group of
44 Japanese undergraduate students. There is no
overlap between the subjects of those two experi-
ments. Details of this experiment are described in
the following subsections.
2.1 Collecting Referring Expressions
Method Subjects were presented 2-dimensional
bird?s-eye images in which several objects of the
same color and the same size were arranged and the
subjects were requested to convey a target object to
the third person drawn in the same image. We used
12 images of arrangements. In each image, three
to nine objects were arranged manually so that the
objects distributes non-uniformly. An example of
images presented to subjects is shown in Figure 2.
Labels a, . . . , f, x in the image are assigned for pur-
poses of illustration and are not assigned in the ac-
tual images presented to the subjects. Each subject
was asked to describe a command so that the person
in the image picks a target object that is enclosed
with dotted lines. When a subject could not think
of a proper expression, she/he was allowed to aban-
don that arrangement and proceed to the next one.
Referring expressions designating the target object
were collected from these subjects? commands.
P
a
b
e
f
c d
x
Figure 2: A visual stimulus of the experiment
Analysis We presented 12 arrangements to 42
subjects and obtained 476 referring expressions.
Twenty eight judgments were abandoned in the ex-
periment. Observing the collected expressions, we
found that starting from a group with all of the ob-
jects, subjects generally narrow down the group to
a singleton group that has the target object. There-
fore, a referring expression can be formalized as a
sequence of groups (SOG) reflecting the subject?s
narrowing down process.
The following example shows an observed ex-
pression describing the target x in Figure 2 with the
corresponding SOG representation below it.
?hidari oku ni aru mittu no tama no uti no
itiban migi no tama.?
(the rightmost ball among the three balls
at the back left)
SOG:[{a, b, c, d, e, f, x}, {a, b, x}, {x}] 3
where
{a, b, c, d, e, f, x} denotes all objects in
the image (total set),
{a, b, x} denotes the three objects at the
back left, and
{x} denotes the target.
3We denote an SOG representation by enclosing groups
with square brackets.
Since narrowing down starts from the total set,
the SOG representation starts with a set of all ob-
jects and ends with a singleton group with the tar-
get. Translating the collected referring expressions
into the SOG representation enables us to abstract
and classify the expressions. On average, we ob-
tained about 40 expressions for each arrangement,
and classified them into 8.4 different SOG represen-
tations.
Although there are two types of relations be-
tween groups as we mentioned in Section 1, the ex-
pressions using only intra-group relations made up
about 70% of the total.
2.2 Evaluating the Collected Expressions
Method Subjects were presented expressions col-
lected in the experiment described in Section 2.1 to-
gether with the corresponding images, and were re-
quested to indicate objects referred to by the expres-
sions. The presented images are the same as those
used in the previous experiment except that there are
no marks on the targets. At the same time, subjects
were requested to express their confidence in select-
ing the target, and evaluate the conciseness, and the
naturalness of the given expressions on a scale of 1
to 8.
Because the number of expressions that we could
evaluate with subjects was limited, we chose a max-
imum of 10 frequent expressions for each arrange-
ment. The expressions were chosen so that as many
different SOG representations were included as pos-
sible. If an arrangement had SOGs less than 10,
several expressions that had the same SOG but dif-
ferent surface realizations were chosen. The resul-
tant 117 expressions were evaluated by 49 subjects.
Each subject evaluated about 29.5 expressions.
Analysis Discarding incomplete answers, we ob-
tained 1,429 evaluations in total. 12.2 evaluations
were obtained for each expression on average.
We measured the quality of each expression in
terms of an evaluation value that is defined in (1).
This measure is used to analyze what kind of ex-
pressions are preferred and to set up a scoring func-
tion (6) for machine-generated expressions as de-
scribed in Section 3.1.
(evaluation value)
= (accuracy)? (confidence)
?
(naturalness) + (conciseness)
2
(1)
According to our analysis, the expressions with
only intra-group relations (84 samples) obtained
high accuracies (Ave. 79.3%) and high evaluation
values (Ave. 33.1), while the expressions with inter-
group relations (33 samples) obtained lower accura-
cies (Ave. 69.1%) and lower evaluation values (Ave.
19.7).
The expressions with only intra-group relations
are observed more than double as many as the ex-
pressions with inter-group relations. We provide a
couple of example expressions indicating object x
in Figure 2 to contrast those two types of expres-
sions below.
? without inter-group relations
? ?the rightmost ball among the three balls
at the back left?
? with inter-group relations
? ?the ball behind the two front balls?
In addition, expressions explicitly mentioning all
the objects obtained lower evaluation values. Con-
sidering these observations, we built a generation
algorithm using only intra-group relations and did
not mention all the objects explicitly.
Among these expressions, we selected those with
which the subjects successfully identified the target
with more than 90% accuracy. These expressions
are used to extract parameters of our generation al-
gorithm in the following sections.
3 Generating Referring Expressions
3.1 Generation Algorithm
Given an arrangement of objects and a target, our al-
gorithm generates referring expressions by the fol-
lowing three steps:
Step 1: enumerate perceptual groups based on the
proximity between objects
Step 2: generate the SOG representations by com-
bining the groups
Step 3: translate the SOG representations into lin-
guistic expressions
In the rest of this section, we illustrate how these
three steps generate referring expressions in the sit-
uation shown in Figure 2.
Step 1: Enumerating Perceptual Groups.
To generate perceptual groups from an arrangement,
Tho?risson?s algorithm (Tho?risson, 1994) is adopted.
Given a list of objects in an arrangement, the al-
gorithm generates groups based on the proximity of
the objects and returns a list of groups. Only groups
containing the target, that is x, are chosen because
SOG: [{a, b, c, d, e, f, x}, {a, b, x}, {x}]
? E(R({a, b, c, d, e, f, x}, {a, b, x})) + E({a, b, x}) + E(R({a, b, x}, {x})) + E({x})
? ?hidari oku no?+?mittu no tama?+?no uti no migihasi no?+?tama?
(at the back left) (three balls) (rightmost . . . among) (ball)
Figure 3: An example of surface realization
we handle intra-group relations only as mentioned
before, and that implies that all groups mentioned
in an expression must include the target. Then, the
groups are sorted in descending order of the group
size. Finally a singleton group consisting of the tar-
get is added to the end of the list if such a group is
missing in the list. The resultant group list, GL, is
the output of Step 1.
For example, the algorithm recognizes the fol-
lowing groups given the arrangement shown in Fig-
ure 2:
{{a, b, c, d, e, f, x}, {a, b, c, d, x},
{a, b, x}, {c, d}, {e, f}}.
After filtering out the groups without the target and
adding a singleton group with the target, we obtain
the following list:
{{a, b, c, d, e, f, x}, {a, b, c, d, x}, {a, b, x}, {x}}.
(2)
Step 2: Generating the SOG Representations.
In this step, the SOG representations introduced in
Section 2 are generated from the GL of Step 1,
which generally has a form like (3), where G
i
de-
notes a group, and G
0
is a group of all the objects.
Here, we narrow down the objects starting from the
total set (G
0
) to the target ({x}).
[G
0
, G
1
, . . . , G
m?2
, {x}] (3)
Given a group list GL, all possible SOGs are gen-
erated. From a group list of size m, 2m?2 SOG
representations can be generated since G
0
and {x}
should be included in the SOG representation. For
example, from a group list of {G
0
, G
1
, G
2
, {x}},
we obtain four SOGs: [G
0
, {x}], [G
0
, G
1
, {x}],
[G
0
, G
2
, {x}], and [G
0
, G
1
, G
2
, {x}].
For example, one of the SOG representations
generated from list (2) is
[{a, b, c, d, e, f, x}, {a, b, x}, {x}]. (4)
Note that any two groups G
i
and G
j
in a list of
groups generated by Tho?risson?s algorithm with re-
gard to one feature, e.g., proximity in this paper, are
mutually disjoint (G
i
?G
j
= ?), otherwise one sub-
sumes the other (G
i
? G
j
or G
j
? G
i
). No inter-
secting groups without a subsumption relation are
generated.
Step 3: Generating Linguistic Expressions.
In the last step, the SOG representations are trans-
lated into linguistic expressions. Since Japanese is
a head-final language, the order of linguistic ex-
pressions for groups are retained in the final lin-
guistic expression for the SOG representation. That
is, an SOG representation [G
0
, G
1
, . . . , G
n?2
, {x}]
can be realized as shown in (5), where E(X) de-
notes a linguistic expression for X, R(X,Y ) de-
notes a relation between X and Y , and ?+? is a
string concatenation operator.
E(G
0
) + E(R(G
0
, G
1
)) + E(G
1
) + . . .
+E(R(G
n?2
, {x})) + E({x}) (5)
As described in Section 2.2, expressions that ex-
plicitly mention all the objects obtain lower evalu-
ation values, and expressions using intra-group re-
lations obtain high evaluation values. Considering
these observations, our algorithm does not use the
linguistic expression corresponding to all the ob-
jects, that is E(G
0
), and only uses intra-group re-
lations for R(X,Y ).
Possible expressions of X are collected from the
experimental data in Section 2.1, and the first ap-
plicable expression is selected when realizing a lin-
guistic expression for X, i.e., E(X). Therefore,
this algorithm produces one linguistic expression
for each SOG even though there are some other pos-
sible expressions.
For example, the SOG representation (4) is real-
ized as shown in Figure 3.
Note that there is no mention of all the objects,
{a, b, c, d, e, f, x}, in the linguistic expression.
3.2 Evaluation of Generated Expressions
We implemented the algorithm described in Sec-
tion 3.1, and evaluated the output with 23 under-
graduate students. The subjects were different from
those of the previous experiments but were of the
same age group, and the experimental environment
Accuracy (%) Naturalness Conciseness Confidence Eval. val.
Human-12-all 87.3 4.82 5.27 6.14 29.3
Human-12-90 97.9 5.20 5.62 6.50 35.0
Human-12-100 100 5.36 5.73 6.65 37.2
System-12 91.0 5.60 6.25 6.32 40.1
System-20 88.4 5.09 5.65 6.25 35.2
System-Average 89.2 5.24 5.82 6.27 36.6
Table 1: Summary of evaluation
was the same. The evaluation of the output was per-
formed in the same manner as that of Section 2.2.
The results are shown in Table 1. ?Human-
12-all? shows the average values of all expres-
sions collected from humans with 12 arrangements
as described in Section 2.2. ?Human-12-90? and
?Human-12-100? show the average values of ex-
pressions by humans that gained more than 90%
and 100% in accuracy in the same evaluation ex-
periment respectively.
?System-12? shows the average values of expres-
sions generated by the algorithm for the 12 arrange-
ments used in the data collection experiment de-
scribed in Section 2.1. The algorithm generated 18
expressions for the 12 arrangements, which were
presented to each subject in random order for eval-
uation.
?System-20? shows the average values of expres-
sions generated by the algorithm for 20 randomly
generated arrangements that generate at least two
linguistic expressions each. The algorithm gen-
erated 48 expressions for these 20 arrangements,
which were evaluated in the same manner as that
of ?System-12?.
?System-Average? shows the micro average of
expressions of both ?System-12? and ?System-20?.
?Accuracy? shows the rates at which the sub-
jects could identify the correct target objects from
the given expressions. Comparing the accuracies of
?Human-12-*? and ?System-12?, we find that the
algorithm generates good expressions. Moreover,
the algorithm is superior to human in terms of ?Nat-
uralness? and ?Conciseness?. However, this result
should be interpreted carefully. Further investiga-
tion of the expressions revealed that humans often
sacrificed naturalness and conciseness in order to
describe the target as precisely as possible for com-
plex arrangements.
4 Scoring SOG Representations
The algorithm presented in the previous section out-
puts several possible expressions. Therefore, we
have to choose one of the expressions by calculat-
ing their scores.
The scores can be computed using various mea-
sures, such as complexity of expressions, and
salience of referent objects. In this section, we in-
vestigate whether the adequacies of the courses of
narrowing down can be predicted: that is, whether
meaningful scores of SOG representations can be
calculated.
4.1 Method for SOG Scoring
An SOG representation has a form as stated in (3).
We presumed that, when a speaker tries to narrow
down an object group from G
i
to G
i+1
, there is
an optimal ratio between the dimensions of G
i
and
G
i+1
. In other words, narrowing down a group from
a very big one to a very small one might cause hear-
ers to become confused.
For example, consider the following two expres-
sions that both indicate object x in Figure 2. Hearers
would prefer (i) to (ii) though (ii) is simpler than (i).
(i) ?the rightmost ball among the three balls at the
back left?
(ii) ?the fourth ball from the right?
In fact, we found (i) among the expressions col-
lected in Section 2.1, but did not find (ii) among
them. Our algorithm generated both (i) and (ii)
in Section 3.2, and the two expressions gained the
evaluation values of 44.4 and 32.1 respectively.
If our presumption is correct, we can expect
to choose better expressions by choosing expres-
sions that have adequate dimension ratios between
groups.
Calculation Formula
The total score of an SOG representation is calcu-
lated by averaging the scores given by functions f
1
and f
2
whose parameters are dimension ratios be-
tween two consecutive groups as given in (6), where
n is the number of groups in the SOG.
score(SOG) = 1
n ? 1
{
n?3
?
i=0
f
1
(
dim(G
i+1
)
dim(G
i
)
)
+ f
2
(
dim({x})
dim(G
n?2
)
)
} (6)
The dimension of a group dim is defined as the
average distance between the centroid of the group
and that of each object. The dimension of the sin-
gleton group {x} is defined as a constant value. Be-
cause of this idiosyncrasy of the singleton group
{x} compared to other groups, f
2
was introduced
separately from f
1
even though both functions rep-
resent the same concept, as described below.
The optimal ratio between two groups, and that
from a group to the target were found through the
quadratic regression analysis of data collected in the
experiment described in Section 2.2. f
1
and f
2
are
the two regression curves found through analysis
representing correlations between dimension ratios
and values calculated based on human evaluation as
in formula (1).We could not find direct correlations
between dimension ratios and accuracies.
4.2 Results
We checked to what extent the scores of generated
expressions given by formula (6) conformed with
the human evaluation given by formula (1) as agree-
ment. Agreement was calculated as follows using
20 randomly generated arrangements described in
Section 3.2.
First, the generated expressions were ordered ac-
cording to the score given by formula (6) and the
human evaluation given by formula (1). All binary
order relations between two expressions were ex-
tracted from these two ordered lists of expressions.
The agreement was defined as the ratio of the same
binary order relations among all binary order rela-
tions.
The agreement between scores and the human
evaluation was 45.8%. The score did not predict
SOG representations that would generate better ex-
pressions very well. Further research is required to
conclusively rule out the use of dimension ratios for
prediction or whether other factors are involved.
5 Concluding Remarks and Future Work
This paper proposed an algorithm that generates re-
ferring expressions using perceptual groups and n-
ary relations among them. The algorithm was built
on the basis of the analysis of expressions that were
collected through psychological experiments. The
performance of the algorithm was evaluated by 23
subjects and it generated promising results.
In the following, we look at future work to be
done.
Recognizing salient geometric formations:
Tho?risson?s algorithm (Tho?risson, 1994) cannot
recognize a linear arrangement of objects as a
group, although such arrangements are quite salient
for humans. This is one of the reasons for the
disconformity between the evaluations given by the
algorithm and those of the humans subjects.
We can enumerate most of such geometric ar-
rangements salient for human subject by referring to
geometric terms found in lexicons and thesauri such
as ?line?, ?circle?, ?square? and so on. Tho?risson?s
algorithm should be extended to recognize these ar-
rangements.
Using relations other than positional relations:
In this paper, we focused on positional relations of
perceptual groups. Other relations such as degree of
color and size should be treated in the same manner.
Tho?risson?s original algorithm (Tho?risson, 1994)
takes into account these relations as well as posi-
tional relations of objects when calculating similar-
ity between objects to generate groups. However, if
we generate groups using multiple relations simul-
taneously, the assumption used in Step 1 of our al-
gorithm that any pair of groups in an output list do
not intersect without a subsumption relation cannot
be held. Therefore, the mechanism generating SOG
representations (Step 2 in Section 3.1) must be re-
considered.
Resolving reference frames and differences of
perspective: We assumed that all participants in
a conversation shared the same reference frame.
However, when we apply our method to conversa-
tional agent systems, e.g., (Cavazza et al, 2002;
Tanaka et al, 2004), reference frames must be prop-
erly determined each time to generate referring ex-
pressions. Although there are many studies con-
cerning reference frames, e.g., (Clark, 1973; Her-
skovits, 1986; Levinson, 2003), little attention has
been paid to how reference frames are determined in
terms of the perceptual groups and their elements.
In addition to reference frames, differences of
perspective also have to be taken into account to
produce proper referring expressions since humans
often view spatial relations between objects in a
3-dimensional space by projecting them on a 2-
dimensional plane. In the experiments, we pre-
sented the subjects with 2-dimensional bird?s-eye
images. The result might have been different if we
had used 3-dimensional images instead, because the
projection changes the sizes of objects and spatial
relations among them.
Integration with conventional methods: In this
paper, we focused on a limited situation where in-
herent attributes of objects do not serve any identi-
fying function, but this is not the case in general. An
algorithm integrating conventional attribute-based
methods and the proposed method should be formu-
lated to achieve the end goal.
A possible direction would be to enhance the al-
gorithm proposed by Krahmer et al (Krahmer et
al., 2003). They formalize an object arrangement
(scene) as a labeled directed graph in which ver-
tices model objects and edges model attributes and
binary relations, and regard content selection as a
subgraph construction problem. Their algorithm
performs searches directed by a cost function on a
graph to find a unique subgraph.
If we consider a perceptual group as an ordinary
object as shown in Figure 4, their algorithm is appli-
cable. It will be able to handle not only intra-group
relations (e.g., the edges with labels ?front?, ?mid-
dle?, and ?back? in Figure 4) but also inter-group re-
lations (e.g., the edge from ?Group 1? to ?Table? in
Figure 4). However, introducing perceptual groups
as vertices makes it difficult to design the cost func-
tion. A well-designed cost function is indispensable
for generating concise and comprehensible expres-
sions. Otherwise, an expression like ?a ball in front
of a ball in front of a ball? for the situation shown in
Figure 1 would be generated.
Group 1
b c a
Table
front_of front_of right_of
back_of back_of left_of
front
middle back
right_of
right_of
right_of
Figure 4: A simplified graph with a group vertex for
the situation shown in Figure 1
References
Douglas E. Appelt. 1985. Planning English refer-
ring expressions. Artificial Intelligence, 26:1?33.
Mark Cavazza, Fred Charles, and Steven J. Mead.
2002. Character-based interactive stroytelling.
IEEE Intelligent Systems, 17(4):17?24.
Herbert H. Clark. 1973. Space, time, semantics,
and the child. In T. E. Moore, editor, Cogni-
tive development and the acquisition of language,
pages 65?110. Academic Press.
Robert Dale and Nicholas Haddock. 1991. Gener-
ating referring expressions involving relations. In
Proceedings of the Fifth Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics(EACL?91), pages 161?166.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gen-
eration of referring expressions. Cognitive Sci-
ence, 19(2):233?263.
Robert Dale. 1992. Generating referring expres-
sions: Constructing descriptions in a domain of
objects and processes. MIT Press, Cambridge.
Peter Heeman and Graem Hirst. 1995. Collabo-
rating referring expressions. Computational Lin-
guistics, 21(3):351?382.
Annette Herskovits. 1986. Language and Spa-
tial cognition: an interdisciplinary study of the
prepositions in English. Cambridge University
Press.
Helmut Horacek. 1997. An algorithm for gener-
ating referential descriptions with flexible inter-
faces. In Proceedings of the 35th Annual Meeting
of the Association for Computational Linguistics,
pages 206?213.
Emiel Krahmer and Marie?t Theune. 2002. Efficient
context-sensitive generation of descriptions. In
Kees van Deemter and Rodger Kibble, editors,
Information Sharing: Givenness and Newness in
Language Processing. CSLI Publications, Stan-
ford, California.
Emiel Krahmer, Sebastiaan van Erk, and Andre?
Verleg. 2003. Graph-based generation of re-
ferring expressions. Computational Linguistics,
29(1):53?72.
Stephen C. Levinson, editor. 2003. Space in Lan-
guage and Cognition. Cambridge University
Press.
Hozumi Tanaka, Takenobu Tokunaga, and Yusuke
Shinyama. 2004. Animated agents capable of
understanding natural language and perform-
ing actions. In Helmut Prendinger and Mituru
Ishizuka, editors, Life-Like Characters, pages
429?444. Springer.
Kristinn R. Tho?risson. 1994. Simulated perceptual
grouping: An application to human-computer in-
teraction. In Proceedings of the Sixteenth An-
nual Conference of the Cognitive Science Society,
pages 876?881.
Kees van Deemter. 2002. Generating referring ex-
pressions: Boolean extensions of the incremental
algorithm. Computational Linguistics, 28(1):37?
52.
Ielka van der Sluis and Emiel Krahmer. 2000.
Generating referring expressions in a multimodal
context: An empirically oriented approach. Pre-
sented at the CLIN meeting 2000, Tilburg.
Towards an Extrinsic Evaluation of Referring Expressions
in Situated Dialogs
Philipp SPANGER IIDA Ryu TOKUNAGA Takenobu
{philipp,ryu-i,take}@cl.cs.titech.ac.jp
TERAI Asuka KURIYAMA Naoko
asuka@nm.hum.titech.ac.jp kuriyama@hum.titech.ac.jp
Tokyo Institute of Technology
Abstract
In the field of referring expression gener-
ation, while in the static domain both in-
trinsic and extrinsic evaluations have been
considered, extrinsic evaluation in the dy-
namic domain, such as in a situated col-
laborative dialog, has not been discussed
in depth. In a dynamic domain, a cru-
cial problem is that referring expressions
do not make sense without an appropriate
preceding dialog context. It is unrealistic
for an evaluation to simply show a human
evaluator the whole period from the be-
ginning of a dialog up to the time point
at which a referring expression is used.
Hence, to make evaluation feasible it is
indispensable to determine an appropriate
shorter context. In order to investigate the
context necessary to understand a referring
expression in a situated collaborative dia-
log, we carried out an experiment with 33
evaluators and a Japanese referring expres-
sion corpus. The results contribute to find-
ing the proper contexts for extrinsic evalu-
tion in dynamic domains.
1 Introduction
In recent years, the NLG community has paid sig-
nificant attention to the task of generating referring
expressions, reflected in the seting-up of several
competitive events such as the TUNA and GIVE-
Challenges at ENLG 2009 (Gatt et al, 2009; By-
ron et al, 2009).
With the development of increasingly complex
generation systems, there has been heightened in-
terest in and an ongoing significant discussion on
different evaluation measures for referring expres-
sions. This discussion is carried out broadly in the
field of generation, including in the multi-modal
domain, e.g. (Stent et al, 2005; Foster, 2008).
!"#$%&
!"#$%&'()$)&'
'($)*$+%"&,#-+."/
*+),&#(&'
&#),&#(&'
-$,$./#&0!"#$%1
234456
78$#0!"#$%10
234496
:$#0!*,0;<=&(0!"#$%1
2344>6
;)/&$0!"#$%1
234456
?/,!$#0@A$<B*,
2344C6
D*<E0@0F$))0
2344G6
H8&(0I$I*,
234J46
K/()*,#!"#$%&#
234496
Figure 1: Overview of recent work on evaluation
of referring expressions
Figure 1 shows a schematic overview of recent
work on evaluation of referring expressions along
the two axes of evaluation method and domain in
which referring expressions are used.
There are two different evaluation methods cor-
responding to the bottom and the top of the verti-
cal axis in Figure 1: intrinsic and extrinsic eval-
uations (Sparck Jones and Galliers, 1996). In-
trinsic methods often measure similarity between
the system output and the gold standard corpora
using metrics such as tree similarity, string-edit-
distance and BLEU (Papineni et al, 2002). Intrin-
sic methods have recently become popular in the
NLG community. In contrast, extrinsic methods
evaluate generated expressions based on an exter-
nal metric, such as its impact on human task per-
formance.
While intrinsic evaluations have been widely
used in NLG, e.g. (Reiter et al, 2005), (Cahill
and van Genabith, 2006) and the competitive 2009
TUNA-Challenge, there have been a number of
criticisms against this type of evaluation. (Reiter
and Sripada, 2002) argue, for example, that gener-
ated text might be very different from a corpus but
still achieve the specific communicative goal.
An additional problem is that corpus-similarity
metrics measure how well a system reproduces
what speakers (or writers) do, while for most NLG
systems ultimately the most important considera-
tion is its effect on the human user (i.e. listener
or reader). Thus (Khan et al, 2009) argues that
?measuring human-likeness disregards effective-
ness of these expressions?.
Furthermore, as (Belz and Gatt, 2008) state
?there are no significant correlations between in-
trinsic and extrinsic evaluation measures?, con-
cluding that ?similarity to human-produced refer-
ence texts is not necessarily indicative of quality
as measured by human task performance?.
From early on in the NLG community, task-
based extrinsic evaluations have been considered
as the most meaningful evaluation, especially
when having to convince people in other commu-
nities of the usefulness of a system (Reiter and
Belz, 2009). Task performance evaluation is rec-
ognized as the ?only known way to measure the ef-
fectiveness of NLG systems with real users? (Re-
iter et al, 2003). Following this direction, the
GIVE-Challenges (Koller et al, 2009) at INLG
2010 (instruction generation) also include a task-
performance evaluation.
In contrast to the vertical axis of Figure 1, there
is the horizontal axis of the domain in which refer-
ring expressions are used. Referring expressions
can thus be distinguished according to whether
they are used in a static or a dynamic domain, cor-
responding to the left and right of the horizontal
axis of Figure 1. A static domain is one such as the
TUNA corpus (van Deemter, 2007), which col-
lects referring expressions based on a motionless
image. In contrast, a dynamic domain comprises a
constantly changing situation where humans need
context information to identify the referent of a re-
ferring expression.
Referring expressions in the static domain have
been evaluated relatively extensively. A recent ex-
ample of an intrinsic evaluation is (van der Sluis
et al, 2007), who employed the Dice-coefficient
measuring corpus-similarity. There have been a
number of extrinsic evaluations as well, such as
(Paraboni et al, 2006) and (Khan et al, 2009), re-
spectively measuring the effect of overspecifica-
tion on task performance and the impact of gener-
ated text on accuracy as well as processing speed.
They belong thus in the top-left quadrant of Fig-
ure 1.
Over a recent period, research in the generation
of referring expressions has moved to dynamic do-
mains such as situated dialog, e.g. (Jordan and
Walker, 2005) and (Stoia et al, 2006). However,
both of them carried out an intrinsic evaluation
measuring corpus-similarity or asking evaluators
to compare system output to expressions used by
human (the right bottom quadrant in Figure 1).
The construction of effective generation sys-
tems in the dynamic domain requires the imple-
mentation of an extrinsic task performance evalu-
ation. There has been work on extrinsic evalua-
tion of instructions in the dynamic domain on the
GIVE-2 challenge (Byron et al, 2009), which is a
task to generate instructions in a virtual world. It is
based on the GIVE-corpus (Gargett et al, 2010),
which is collected through keyboard interaction.
The evaluation measures used are e.g. the number
of successfully completed trials, completion time
as well as the numbers of instructions the system
sent to the user. As part of the JAST project, a
Joint Construction Task (JCT) puzzle construction
corpus (Foster et al, 2008) was created which is
similar in some ways in its set-up to the REX-
J corpus which we use in the current research.
There has been some work on evaluating gener-
ation strategies of instructions for a collaborative
construction task on this corpus, both considering
intrinsic as well as extrinsic measures (Foster et
al., 2009). Their main concern is, however, the in-
teraction between the text structure and usage of
referring expressions. Therefore, their ?context?
was given a priori.
However, as can be seen from Figure 1, in the
field of referring expression generation, while in
the static domain both intrinsic and extrinsic eval-
uations have been considered, the question of re-
alizing an extrinsic evaluation in the dynamic do-
main has not been dealt with in depth by previous
work. This paper addresses this shortcoming of
previous work and contributes to ?filling in? the
missing quadrant of Figure 1 (the top-right).
The realization of such an extrinsic evaluation
faces one key difficulty. In a static domain, an ex-
trinsic evaluation can be realized relatively easily
by showing evaluators the static context (e.g. any
image) and a referring expression, even though
this is still costly in comparison to intrinsic meth-
ods (Belz and Gatt, 2008).
In contrast, an extrinsic evaluation in the dy-
namic domain needs to present an evaluator with
the dynamic context (e.g. a certain length of the
recorded dialog) preceding a referring expression.
It is clearly not feasible to simply show the whole
preceding dialog; this would make even a very
small-scale evaluation much too costly. Thus, in
order to realize a cost-effective extrinsic evalua-
tion in a dynamic domain we have to deal with the
additional parameter of time length and content of
the context shown to evaluators.
This paper investigates the context necessary for
humans to understand different types of referring
expressions in a situated domain. This work thus
charts new territory and contributes to developing
a extrinsic evaluation in a dynamic domain. Sig-
nificantly, we consider not only linguistic but also
extra-linguistic information as part of the context,
such as the actions that have been carried out in the
preceding interaction. Our results indicate that, at
least in this domain, extrinsic evaluation results
in dynamic domains can depend on the specific
amount of context shown to the evaluator. Based
on the results from our evaluation experiments, we
discuss the broader conclusions to be drawn and
directions for future work.
2 Referring Expressions in the REX-J
Corpus
We utilize the REX-J corpus, a Japanese corpus
of referring expressions in a situated collaborative
task (Spanger et al, 2009a). It was collected by
recording the interaction of a pair of dialog partic-
ipants solving the Tangram puzzle cooperatively.
The goal of the Tangram puzzle is to construct a
given shape by arranging seven pieces of simple
figures as shown in Figure 2
!"#$%&'#()
*"+,-.!%#+)#
Figure 2: Screenshot of the Tangram simulator
In order to record the precise position of every
piece and every action by the participants, we im-
plemented a simulator. The simulator displays two
areas: a goal shape area, and a working area where
pieces are shown and can be manipulated.
We assigned different roles to the two partici-
pants of a pair: solver and operator. The solver
can see the goal shape but cannot manipulate the
pieces and hence gives instructions to the opera-
tor; by contrast, the operator can manipulate the
pieces but can not see the goal shape. The two
participants collaboratively solve the puzzle shar-
ing the working area in Figure 2.
In contrast to other recent corpora of refer-
ring expressions in situated collaborative tasks
(e.g. COCONUT corpus (Di Eugenio et al, 2000)
and SCARE corpora (Byron et al, 2005)), in
the REX-J corpus we allowed comparatively large
real-world flexibility in the actions necessary to
achieve the task (such as flipping, turning and
moving of puzzle pieces at different degrees), rel-
ative to the task complexity. The REX-J corpus
thus allows us to investigate the interaction of lin-
guistic and extra-linguistic information. Interest-
ingly, the GIVE-2 challenge at INLG 2010 notes
its ?main novelty? is allowing ?continuous moves
rather than discrete steps as in GIVE-1?. Our work
is in line with the broader research trend in the
NLG community of trying to get away from sim-
ple ?discrete? worlds to more realistic settings.
The REX-J corpus contains a total of 1,444 to-
kens of referring expressions in 24 dialogs with a
total time of about 4 hours and 17 minutes. The
average length of each dialog is 10 minutes 43
seconds. The asymmetric data-collection setting
encouraged referring expressions from the solver
(solver: 1,244 tokens, operator: 200 tokens). We
exclude from consideration 203 expressions refer-
ring to either groups of pieces or whose referent
cannot be determined due to ambiguity, thus leav-
ing us 1,241 expressions.
We identified syntactic/semantic features in the
collected referring expressions as listed in Table 1:
(a) demonstratives (adjectives and pronouns), (b)
object attribute-values, (c) spatial relations and (d)
actions on an object. The underlined part of the
examples denotes the feature in question.
3 Design of Evaluation Experiment
The aim of our experiment is to investigate the
?context? (content of the time span of the recorded
Table 1: Syntactic and semantic features of refer-
ring expressions in the REX-J corpus
Feature Tokens Example
(a) demonstrative 742 ano migigawa no sankakkei
(that triangle at the right side)
(b) attribute 795 tittyai sankakkei
(the small triangle)
(c) spatial relations 147 hidari no okkii sankakkei
(the small triangle on the left)
(d) action-mentioning 85 migi ue ni doketa sankakkei
(the triangle you put away to
the top right)
interaction prior to the uttering of the referring ex-
pression) necessary to enable successful identifi-
cation of the referent of a referring expression.
Our method is to vary the context presented to
evaluators and then to study the impact on human
referent identification. In order to realize this, for
each instance of a referring expression, we vary
the length of the video shown to the evaluator.
!"#$ %&'()*+,!-./0#
!1#$ 234*&,&5,678+*4,9&+:3(;,8+*8,3(,)7*,63<'=8)&+
!%#$ >))*+8(?*,3(?='43(;,)7*,+*5*++3(;,*@A+*663&(,)&,
*B8='8)*,!67&9(,3(,+*4#
!C#$ D)8+)E+*A*8),F'))&(
!G#$ D*=*?)3&(,F'))&(6,!-.H#,8(4,IJ,4&(K),:(&9I.F'))&(,,,,,,,,,,
!"#
!1#
!%#
!C#
!G#
Figure 3: The interface presented to evaluators
The basic procedure of our evaluation experi-
ment is as follows:
(1) present human evaluators with speech and
video from a dialog that captures shared
working area of a certain length previous to
the uttering of a referring expression,
(2) stop the video and display as text the next
solver?s utterance including the referring ex-
pression (shown in red),
(3) ask the evaluator to identify the referent
of the presented referring expression (if the
evaluator wishes, he/she can replay the video
as many times as he likes),
(4) proceed to the next referring expression (go
to (1)).
Figure 3 shows a screenshot of the interface pre-
pared for this experiment.
The test data consists of three types of referring
expressions: DPs (demonstrative pronouns),
AMEs (action-mentioning expressions), and
OTHERs (any other expression that is neither a
DP nor AME, e.g intrinsic attributes and spatial
relations). DPs are the most frequent type of
referring expression in the corpus. AMEs are
expressions that utilize an action on the referent
such as ?the triangle you put away to the top
right? (see Table 1)1. As we pointed out in our
previous paper (Spanger et al, 2009a), they are
also a fundamental type of referring expression in
this domain.
The basic question in investigating a suitable
context is what information to consider about the
preceding interaction; i.e. over what parameters to
vary the context. In previous work on the gener-
ation of demonstrative pronouns in a situated do-
main (Spanger et al, 2009b), we investigated the
role of linguistic and extra-linguistic information,
and found that time distance from the last action
(LA) on the referent as well as the last mention
(LM) to the referent had a significant influence on
the usage of referring expressions. Based on those
results, we focus on the information on the refer-
ent, namely LA and LM.
For both AMEs and OTHERs, we only consider
two possibilities of the order in which LM and LA
appear before a referring expression (REX), de-
pending on which comes first. These are shown in
Figure 4, context patterns (a) LA-LM and (b) LM-
LA. Towards the very beginning of a dialog, some
referring expressions have no LM and LA; those
expressions are not considered in this research.
All instances of AMEs and OTHERs in our test
data belong to either the LA-LM or the LM-LA
1An action on the referent is usually described by a verb
as in this example. However, there are cases with a verb el-
lipsis. While this would be difficult in English, it is natural
and grammatical in Japanese.
!"#$%&'()&*+'()&
,($%
-./%+ !"#$
%&
'"()
!"#$%&'%($)"**+,-
!"#*+'()&$%&'()&
,($%
-./%+ !"#$
%*
'"()
!.#$%('%&$)"**+,-
!"#
$%&'()&
,($%
-./%+
%*
'"()
!/#$%('%&0$)"**+,-
*+'()&
-./%+
-./%+
Figure 4: Schematic overview of the three context
Patterns
pattern. For each of these two context patterns,
there are three possible contexts2: Both (including
both LA and LM), LA/LM (including either LA or
LM) and None (including neither). Depending on
the order of LA and LM prior to an expression,
only one of the variations of LA/LM is possible
(see Figure 4 (a) and (b)).
In contrast, DPs tend to be utilized in a deic-
tic way in such situated dialogs (Piwek, 2007).
We further noted in (Spanger et al, 2009b), that
DPs in a collaborative task are also frequently used
when the referent is under operation. While they
belong neither to the LA-LM nor the LM-LA pat-
tern, it would be inappropriate to exclude those
cases. Hence, for DPs we consider another situa-
tion where the last action on the referent overlaps
with the utterance of the DP (Figure 4 (c) LM-LA?
pattern). In this case, we consider an ongoing op-
eration on the referent as a ?last action?. Another
peculiarity of the LM-LA? pattern is that we have
no None context in this case, since there is no way
to show a video without showing LA (the current
operation).
Given the three basic variations of context, we
recruited 33 university students as evaluators and
2To be more precise, we set a margin at the beginning of
contexts as shown in Figure 4.
divided them equally into three groups, i.e. 11
evaluators per group. As for the referring ex-
pressions to evaluate, we selected 60 referring ex-
pressions used by the solver from the REX-J cor-
pus (20 from each category), ensuring all were
correctly understood by the operator during the
recorded dialog. We selected those 60 instances
from expressions where both LM and LA ap-
peared within the last 30 secs previous to the re-
ferring expression. This selection excludes initial
mentions, as well as expressions where only LA
or only LM exists or they do not appear within 30
secs. Hence the data utilized for this experiment
is limited in this sense. We need further experi-
ments to investigate the relation between the time
length of contexts and the accuarcy of evaluators.
We will return to this issue in the conclusion.
We combined 60 referring expressions and the
three contexts to make the test instances. Follow-
ing the Latin square design, we divided these test
instances into three groups, distributing each of
the three contexts for every referring expression
to each group. The number of contexts was uni-
formly distributed over the groups. Each instance
group was assigned to each evaluator group.
For each referring expression instance, we
record whether the evaluator was able to correctly
identify the referent, how long it took them to
identify it and whether they repeated the video
(and if so how many times).
Reflecting the distribution of the data available
in our corpus, the number of instances per context
pattern differs for each type of referring expres-
sion. For AMEs, overwhelmingly the last action
on the referent was more recent than the last men-
tion. Hence we have only two LA-LM patterns
among the 20 AMEs in our data. For OTHERs, the
balance is 8 to 12, with a slight majority of LM-
LA patterns. For DPs, there is a strong tendency to
use a DP when a piece is under operation (Spanger
et al, 2009b). Of the 20 DPs in the data, 2 were
LA-LM, 5 were LM-LA pattern while 13 were of
the LM-LA? pattern (i.e. their referents were under
operation at the time of the utterance). For these
13 instances of LM-LA? we do not have a None
context.
The average stimulus times, i.e. time period of
presented context, were 7.48 secs for None, 11.04
secs for LM/LA and 18.10 secs for Both.
Table 2: Accuracy of referring expression identification per type and context
Type context pattern\Context None LM/LA Both Increase [None ? Both]
(LA-LM) 0.909 0.955 0.955 0.046
DP (20/22) (21/22) (21/22)
(LM-LA) 0.455 0.783 0.843 0.388
(25/55) (155/198) (167/198)
Total 0.584 0.800 0.855 0.271
(LA-LM) 0.227 0.455 0.682 0.455
AME (5/22) (10/22) (15/22)
(LM-LA) 0.530 0.859 0.879 0.349
(105/198) (170/198) (174/198)
Total 0.500 0.818 0.859 0.359
(LA-LM) 0.784 0.852 0.943 0.159
OTHER (69/88) (75/88) (83/88)
(LM-LA) 0.765 0.788 0.879 0.114
(101/132) (104/132) (116/132)
Total 0.773 0.814 0.905 0.132
Overall 0.629 0.811 0.903 0.274
(325/517) (535/660) (576/638)
4 Results and Analysis
In this section we discuss the results of our evalua-
tion experiment. In total 33 evaluators participated
in our experiment, each solving 60 problems of
referent identification. Taking into account the ab-
sence of the None context for the DPs of the LM-
LA? pattern (see (c) in Figure 4), we have 1,815
responses to analyze. We focus on the impact of
the three contexts on the three types of referring
expressions, considering the two context patterns
LA-MA and LM-LA.
4.1 Overview of Results
Table 2 shows the micro averages of the accura-
cies of referent identification of all evaluators over
different types of referring expressions with differ-
ent contexts. Accuracies increase with an increase
in the amount of information in the context; from
None to Both by between 13.2% (OTHERs) and
35.9% (AMEs). The average increase of accuracy
is 27.4%.
Overall, for AMEs the impact of the context is
the greatest, while for OTHERs it is the smallest.
This is not surprising given that OTHERs tend to
include intrinsic attributes of the piece and its spa-
tial relations, which are independent of the pre-
ceding context. We conducted ANOVA with the
context as the independent variable, testing its ef-
fect on identification accuracy. The main effect
of the context was significant on accuracy with
F (2, 1320) = 9.17, p < 0.01. Given that for
DPs we did not have an even distribution between
contexts, we only utilized the results of AMEs and
OTHERs.
There are differences between expression types
in terms of the impact of addition of LM/LA into
the context, which underlines that when studying
context, the relative role and contribution of LA
and LM (and their interaction) must be looked at in
detail for different types of referring expressions.
Over all referring expressions, the addition into
a None context of LM yields an average increase
in accuracy of 9.1% for all referring expression
types, while for the same conditions the addition
of LA yields an average increase of 21.3%. Hence,
interestingly for our test data, the addition of LA
to the context has a positive impact on accuracy by
more than two times over the addition of LM.
It is also notable that even with neither LA nor
LM present (i.e. the None context), the evaluators
were still able to correctly identify referents in be-
tween 50?68.6% (average: 62.9%) of the cases.
While this accuracy would be insufficient for the
evaluation of machine generated referring expres-
sions, it is still higher than one might expect and
further investigation of this case is necessary.
4.2 Demonstrative Pronouns
For DPs, there is a very clear difference between
the two patterns (LM-LA and LA-LM) in terms of
the increase of accuracy with a change of context.
While accuracy for the LA-LM pattern remains at
a high level (over 90%) for all three contexts (and
there is only a very small increase from None to
Both), for the LM-LA pattern there is a strong in-
crease from None to Both of 38.8%.
The difference in accuracy between the two
context patterns of DPs in the None context might
come from the mouse cursor effect. The two ex-
pressions of LA-LM pattern happened to have a
mouse cursor on the referent, when they were
used, resulting in high accuracy. On the other
hand, 4 out of 5 expressions of LM-LA pattern did
not have a mouse cursor on the referent. We have
currently no explanation for the relation between
context patterns and the mouse position. While
we have only 7 expressions in the None context
for DPs and hence cannot draw any decisive con-
clusions, we note that the impact of the mouse po-
sition is a likely factor.
For the LM-LA pattern, there is an increase
in accuracy of 32.8% from None to the LA-
context. Overwhelmingly, this represents in-
stances in which the referents are being operated
at the point in time when the solver utters a DP
(this is in fact the LM-LA? pattern, which has no
None context). For those instances, the current
operation information is sufficient to identify the
referents. In contrast, addition of LM leads only
to a small increase in accuracy of 5.6%. This re-
sult is in accordance with our previous work on the
generation of DPs, which stressed the importance
of extra-linguistic information in the framework of
considering the interaction between linguistic and
extra-linguistic information.
4.3 Action-mentioning Expressions
While for AMEs the number of instances is very
uneven between patterns (similar to the distribu-
tion for DPs), there is a strong increase in accuracy
from the None context to the Both context for both
patterns (between 30% to almost 50%). However,
there is a difference between the two patterns in
terms of the relative contribution of LM and LA to
this increase.
While for the LA-LM pattern the impact of
adding LM and LA is very similar, for the LM-LA
pattern the major increase in accuracy is due to
adding LA into the None context. This indicates
that for AMEs, LA has a stronger impact on ac-
curacy than LM, as is to be expected. The strong
increase for AMEs of the LM-LA pattern when
adding LA into the context is not surprising, given
that the evaluators were able to see the action men-
tioned in the AME.
For the opposite reason, it is not surprising that
AMEs show the lowest accuracy in the None con-
text, given that the last action on the referent is
not seen by the evaluators. However, accuracy
was still slightly over 50% in the LM-LA pattern.
Overall, of the 18 instances of AMEs of the LM-
LA pattern, in the None context a majority of eval-
uators correctly identified 9 and erred on the other
9. Further analysis of the difference between cor-
rectly and incorrectly identified AMEs led us to
note again the important role of the mouse cursor
also for AMEs.
Comparing to the LM-LA pattern, we had very
low accuracy even with the Both context. As we
mentioned in the previous section, we had very
skewed test instances for AME, i.e. 18 LM-LA
patterns vs. 2 LA-LM patterns. We need further
investigation on the LA-LM pattern of AME with
more large number of instances.
Of the 18 LM-LA instances of AMEs, there are
14 instances that mention a verb describing an ac-
tion on the referent. The referents of 6 of those
14 AMEs were correctly determined by the evalu-
ators and in all cases the mouse cursor played an
important role in enabling the evaluator to deter-
mine the referent. The evaluators seem to utilize
the mouse position at the time of the uttering of the
referring expression as well as mouse movements
in the video shown. In contrast, for 8 out of the
9 incorrectly determined AMEs no such informa-
tion from the mouse was available. There was a
very similar pattern for AMEs that did not include
a verb. These points indicate that movements and
the position of the mouse both during the video as
well as the time point of the uttering of the refer-
ring expression give important clues to evaluators.
4.4 Other Expressions
There is a relatively even gain in identification ac-
curacy from None to Both of between about 10?
15% for both patterns. However, there is a simi-
lar tendency as for AMEs, since there is a differ-
ence between the two patterns in terms of the rel-
ative contribution of LM and LA to this increase.
While for the LA-LM pattern the impact of adding
LM and LA is roughly equivalent, for the LM-LA
pattern the major increase in accuracy is due to
adding LM into the LA-context.
For this pattern of OTHERs, LM has a stronger
impact on accuracy than LA, which is exactly the
opposite tendency to AMEs. For OTHERs (e.g.
use of attributes for object identification), seeing
the last action on the target has a less positive im-
pact than listening to the last linguistic mention.
Furthermore, we note the relatively high accuracy
in the None context for OTHERs, underlining the
context-independence of expressions utilizing at-
tributes and spatial relations of the pieces.
4.5 Error Analysis
We analyzed those instances whose referents were
not correctly identified by a majority of evalua-
tors in the Both context. Among the three expres-
sion types, there were about 13?16% of wrong an-
swers. In total for 7 of the 60 expressions a ma-
jority of evaluators gave wrong answers (4 DPs, 2
AMEs and 1 OTHER). Analysis of these instances
indicates that some improvements of our concep-
tion of ?context? is needed.
For 3 out of the 4 DPs, the mouse was not over
the referent or was closer to another piece. In addi-
tion, these DPs included expressions that pointed
to the role of a piece in the overall construction of
the goal shape, e.g. ?soitu ga atama (that is the
head)?, or where a DP is used as part of a more
complex referring expression, e.g. ?sore to onazi
katati . . . (the same shape as this)?, intended to
identify a different piece. For a non-participant
of the task, such expressions might be difficult to
understand in any context. This phenomenon is
related to the ?overhearer-effect? (Schober et al,
1989).
The two AMEs that the majority of evaluators
failed to identify in the Both context were also
misidentified in the LA context. Both AMEs were
missing a verb describing an action on the referent.
While for AMEs including a verb the accuracy in-
creased from None to Both by 50%, for AMEs
without a verb there was an increase by slightly
over 30%, indicating that in the case where an
AME lacks a verb, the context has a smaller pos-
itive impact on accuracy than for AMEs that in-
clude a verb. In order to account for those cases,
further work is necessary, such as investigating
how to account for the information on the distrac-
tors.
5 Conclusions and Future Work
In order to address the task of designing a flexi-
ble experiment set-up with relatively low cost for
extrinsic evaluations of referring expressions, we
investigated the context that needs to be shown to
evaluators in order to correctly determine the ref-
erent of an expression.
The analysis of our results showed that the con-
text had a significant impact on referent identifi-
cation. The impact was strongest for AMEs and
DPs and less so for OTHERs. Interestingly, we
found for both DPs and AMEs that including LA
in the context had a stronger positive impact than
including LM. This emphasizes the importance of
taking into account extra-linguistic information in
a situated domain, as considered in this study.
Our analysis of those expressions whose refer-
ent was incorrectly identified in the Both context
indicated some directions for improving the ?con-
text? used in our experiments, for example look-
ing further into AMEs without a verb describing
an action on the referent. Generally, there is a
necessity to account for mouse movements during
the video shown to evaluators as well as the prob-
lem for extrinsic evaluations of how to address the
?overhearer?s effect?.
While likely differing in the specifics of the set-
up, the methodology in the experiment design dis-
cussed in this paper is applicable to other domains,
in that it allows a low-cost flexible design of eval-
uating referring expressions in a dynamic domain.
In order to avoid the additional effort of analyzing
cases in relation to LM and LA, in the future it will
be desirable to simply set a certain time period and
base an evaluation on such a set-up.
However, we cannot simply assume that a
longer context would yield a higher identification
accuracy, given that evaluators in our set-up are
not actively participating in the interaction. Thus
there is a possibility that identification accuracy
actually decreases with longer video segments,
due to a loss of the evaluator?s concentration. Fur-
ther investigation of this question is indicated.
Based on the work reported in this paper, we
plan to implement an extrinsic task-performance
evaluation in the dynamic domain. Even with
the large potential cost-savings based on the re-
sults reported in this paper, extrinsic evaluations
will remain costly. Thus one important future task
for extrinsic evaluations will be to investigate the
correlation between extrinsic and intrinsic evalua-
tion metrics. This in turn will enable the use of
cost-effective intrinsic evaluations whose results
are strongly correlated to task-performance eval-
uations. This paper made an important contribu-
tion by pointing the direction for further research
in extrinsic evaluations in the dynamic domain.
References
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic
evaluation measures for referring expression genera-
tion. In Proceedings of ACL-08: HLT, Short Papers,
pages 197?200.
Donna Byron, Thomas Mampilly, Vinay Sharma, and
Tianfang Xu. 2005. Utilizing visual attention for
cross-modal coreference interpretation. In CON-
TEXT 2005, pages 83?96.
Donna Byron, Alexander Koller, Kristina Striegnitz,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2009. Report on the First NLG
Challenge on Generating Instructions in Virtual En-
vironments (GIVE). In Proceedings of the 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG 2009), pages 165?173.
Aoife Cahill and Josef van Genabith. 2006. Ro-
bust PCFG-based generation using automatically
acquired lfg approximations. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1033?
1040.
Barbara Di Eugenio, Pamela. W. Jordan, Richmond H.
Thomason, and Johanna. D Moore. 2000. The
agreement process: An empirical investigation of
human-human computer-mediated collaborative di-
alogues. International Journal of Human-Computer
Studies, 53(6):1017?1076.
Mary Ellen Foster, Ellen Gurman Bard, Markus Guhe,
Robin L. Hill, Jon Oberlander, and Alois Knoll.
2008. The roles of haptic-ostensive referring expres-
sions in cooperative, task-based human-robot dia-
logue. In Proceedings of 3rd Human-Robot Inter-
action, pages 295?302.
Mary Ellen Foster, Manuel Giuliani, Amy Isard, Colin
Matheson, Jon Oberlander, and Alois Knoll. 2009.
Evaluating description and reference strategies in a
cooperative human-robot dialogue system. In Pro-
ceedings of the 21st international jont conference
on Artifical intelligence (IJCAI 2009), pages 1818?
1823.
Mary Ellen Foster. 2008. Automated metrics that
agree with human judgements on generated output
for an embodied conversational agent. In Proceed-
ings of the 5th International Natural Language Gen-
eration Conference (INLG 2008), pages 95?103.
Andrew Gargett, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The give-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC 2010), pages 2401?2406.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The
TUNA-REG Challenge 2009: Overview and eval-
uation results. In Proceedings of the 12th European
Workshop on Natural Language Generation (ENLG
2009), pages 174?182.
Pamela W. Jordan and Marilyn A. Walker. 2005.
Learning content selection rules for generating ob-
ject descriptions in dialogue. Journal of Artificial
Intelligence Research, 24:157?194.
Imtiaz Hussain Khan, Kees van Deemter, Graeme
Ritchie, Albert Gatt, and Alexandra A. Cleland.
2009. A hearer-oriented evaluation of referring ex-
pression generation. In Proceedings of the 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG 2009), pages 98?101.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Sara Dalzel-Job, Jon
Oberlander, and Johanna Moore. 2009. Validating
the web-based evaluation of nlg systems. In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 301?304.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics (ACL 2002), pages 311?318.
Ivandre? Paraboni, Judith Masthoff, and Kees van
Deemter. 2006. Overspecified reference in hierar-
chical domains: Measuring the benefits for readers.
In Proceedings of the 4th International Natural Lan-
guage Generation Conference (INLG 2006), pages
55?62.
Paul L.A. Piwek. 2007. Modality choise for generation
of referring acts. In Proceedings of the Workshop on
Multimodal Output Generation (MOG 2007), pages
129?139.
Ehud Reiter and Anja Belz. 2009. An investiga-
tion into the validity of some metrics for automat-
ically evaluating natural language generation sys-
tems. Computational Linguistics, 35(4):529?558.
Ehud Reiter and Somayajulu Sripada. 2002. Should
corpora texts be gold standards for NLG? In Pro-
ceesings of 2nd International Natural Language
Generation Conference (INLG 2002), pages 97?104.
Ehud Reiter, Roma Robertson, and Liesl M. Osman.
2003. Lessons from a failure: generating tailored
smoking cessation letters. Artificial Intelligence,
144(1-2):41?58.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,
and Ian Davy. 2005. Choosing words in computer-
generated weather forecasts. Artificial Intelligence,
167(1-2):137?169.
Michael F. Schober, Herbert, and H. Clark. 1989. Un-
derstanding by addressees and overhearers. Cogni-
tive Psychology, 21:211?232.
Philipp Spanger, Masaaki Yasuhara, Ryu Iida, and
Takenobu Tokunaga. 2009a. A Japanese corpus
of referring expressions used in a situated collabo-
ration task. In Proceedings of the 12th European
Workshop on Natural Language Generation (ENLG
2009), pages 110 ? 113.
Philipp Spanger, Masaaki Yasuhara, Iida Ryu, and
Tokunaga Takenobu. 2009b. Using extra linguistic
information for generating demonstrative pronouns
in a situated collaboration task. In Proceedings of
PreCogSci 2009: Production of Referring Expres-
sions: Bridging the gap between computational and
empirical approaches to reference.
Karen Sparck Jones and Julia R. Galliers. 1996. Eval-
uating Natural Language Processing Systems: An
Analysis and Review. Springer-Verlag.
Amanda Stent, Matthew Marge, and Mohit Singhai.
2005. Evaluating evaluation methods for generation
in the presence of variation. In Linguistics and In-
telligent Text Processing, pages 341?351. Springer-
Verlag.
Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2006. Noun phrase
generation for situated dialogs. In Proceedings of
the 4th International Natural Language Generation
Conference (INLG 2006), pages 81?88.
Kees van Deemter. 2007. TUNA: Towards a unified
algorithm for the generation of referring expres-
sions. Technical report, Aberdeen University.
www.csd.abdn.ac.uk/research/tuna/pubs/TUNA-
final-report.pdf.
Ielka van der Sluis, Albert Gatt, and Kees van Deemter.
2007. Evaluating algorithms for the generation of
referring expressions: Going beyond toy domains.
In Proceedings of Recent Advances in Natural Lan-
guae Processing (RANLP 2007).
