TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 17?24,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Extractive Automatic Summarization: Does more linguistic knowledge 
make a difference? 
 
Daniel S. Leite1, Lucia H. M. Rino1, Thiago A. S. Pardo2, Maria das Gra?as V. Nunes2 
N?cleo Interinstitucional de Ling??stica Computacional (NILC) 
http://www.nilc.icmc.usp.br 
1Departamento de Computa??o, UFSCar  
CP 676, 13565-905 S?o Carlos - SP, Brazil 
2Instituto de Ci?ncias Matem?ticas e de Computa??o, Universidade de S?o Paulo 
CP 668, 13560-970 S?o Carlos - SP, Brazil 
{daniel_leite; lucia}@dc.ufscar.br , {taspardo,gracan}@icmc.usp.br 
 
 
Abstract 
In this article we address the usefulness of 
linguistic-independent methods in extrac-
tive Automatic Summarization, arguing 
that linguistic knowledge is not only useful, 
but may be necessary to improve the in-
formativeness of automatic extracts. An as-
sessment of four diverse AS methods on 
Brazilian Portuguese texts is presented to 
support our claim. One of them is Mihal-
cea?s TextRank; other two are modified 
versions of the former through the inclusion 
of varied linguistic features. Finally, the 
fourth method employs machine learning 
techniques, tackling more profound and 
language-dependent knowledge. 
1 Introduction 
Usually, automatic summarization involves produc-
ing a condensed version of a source text through 
selecting or generalizing its relevant content. As a 
result, either an extract or an abstract will be pro-
duced. An extract is produced by copying text seg-
ments and pasting them into the final text preserving 
the original order. An abstract instead is produced 
by selecting and restructuring information from the 
source text. The resulting structure is thus linguisti-
cally realized independently of the surface choices 
of the source text. This comprises, thus, a rewriting 
task. 
This article focuses solely on extracts of source 
texts written in Brazilian Portuguese. For extrac-
tive Automatic Summarization (AS), several meth-
ods have been suggested that are based upon 
statistics or data readily available in the source 
text. Word frequency (Luhn, 1958) and sentence 
position (Edmundson, 1969) methods are classic 
examples of that. Usually, extractive AS does not 
take into account linguistic and semantic knowl-
edge in order to be portable to distinct domains or 
languages (Mihalcea, 2005). Graph-based methods 
aim at the same and have been gaining a lot of in-
terest because they usually do not rely on any lin-
guistic resource and run pretty fast. Exemplars of 
those are LexRank (Erkan and Radev, 2004) and 
TextRank (Mihalcea and Tarau, 2004). In spite of 
their potentialities, we claim that there is a com-
promise in pursuing a language-free setting: how-
ever portable a system may be, it may also produce 
extracts that lack the degree of informativeness 
needed for use. Informativeness, in the current 
context, refers to the ability of an automatic sum-
marizer to produce summaries that convey most 
information of reference, or ideal, summaries. Our 
assessment thus aimed at verifying if parsimonious 
use of linguistic knowledge could improve extrac-
tive AS. 
We argue that the lack of linguistic knowledge 
in extractive AS can be the reason for weak per-
formance regarding informativeness. This argu-
ment follows from acknowledging that 
improvements on the scores usually obtained in 
that field have not been expressive lately. The most 
common metrics used to date, precision and recall, 
signal average results, suggesting that it is not 
enough to pursue completely language-free sys-
tems, no matter the current demands for portability 
in the global communication scenario. We focus 
here on TextRank, which can be used for summa-
17
rizing Brazilian Portuguese texts due to its lan-
guage independence. To show that linguistic 
knowledge does make a difference in extractive 
AS, we compared four automatic summarizers: 
TextRank itself, two other modified versions of 
that, and SuPor-2 (Leite and Rino, 2006). 
TextRank works in a completely unsupervised 
way. Our two variations, although still 
unsupervised, include diverse linguistic knowledge 
in the preprocessing phase. SuPor-2 is the only 
machine learning-based system amongst the four 
ones, and it was built to summarize texts in 
Brazilian Portuguese, although it may be 
customized to other languages. Unlike the others, it 
embeds more sophisticated decision features that 
rely on varied linguistic resources. Some of them 
correspond to full summarization methods by 
themselves: Lexical Chaining (Barzilay and 
Elhadad, 1997), Relationship Mapping (Salton et 
al., 1997), and Importance of Topics (Larocca Neto 
et al, 2000). This is its unique and distinguishing 
characteristic.   
In what follows we first review the different lev-
els of processing in extractive AS (Section 2), then 
we describe TextRank and its implementation to 
summarize Brazilian Portuguese texts (Section 3). 
Our suggested modifications of TextRank are pre-
sented in Section 4, whilst SuPor-2 is described in 
Section 5. Finally, we compare the results of the 
four automatic summarizers when running on Bra-
zilian Portuguese texts (Section 6), and make some 
remarks on linguistic independence for extractive 
AS in Section 7. 
2 A Review of Automatic Summarization 
Mani (2001) classifies AS methods based upon 
three levels of linguistic processing to summarize a 
text, namely: 
 
? Shallow level. At this level only features at the 
surface of the text are explored. For example, 
location (Edmunson, 1969), sentence length 
and presence of signaling phrases (e.g., Kupiec 
et al, 1995). Combined, such features may 
yield a salience function that drives selection 
of sentences of the source text to include in a 
summary. 
 ? Entity level. The aim here is to build an inter-
nal representation of the source text that con-
veys its entities and corresponding 
relationships. These amount to the information 
that allows identifying important text seg-
ments. Examples of such relations are word 
cooccurrence (e.g., Salton et al, 1997), syno-
nyms and antonyms (e.g., Barzilay and Elha-
dad, 1997), logical relations, such as 
concordance or contradiction, and syntactic 
relations. 
 ? Discourse level. At this level the whole struc-
ture of the source text is modeled, provided 
that its communicative goals can be grasped 
from the source text. The discourse structure is 
intended to help retrieving, e.g., the main top-
ics of the document (e.g, Barzilay and Elha-
dad, 1997; Larocca Neto et al, 2000) or its 
rhetorical structure (e.g., Marcu, 1999), in or-
der to provide the means for AS. 
 
In this work we mainly focus on the entity level. 
Special entities and their relations thus provide the 
means to identify important sentences for building 
an extract. In turn, there is a loss of independence 
from linguistic knowledge, when compared to shal-
lower approaches. Actually, apart from TextRank, 
the other systems described in this paper target en-
tity level methods, as we shall see shortly.  
3 The TextRank Method 
The unsupervised TextRank method (Mihalcea and 
Tarau, 2004) takes after Google?s PageRank (Brin 
and Page, 1998), a graph-based system that helps 
judge the relevance of a webpage through incoming 
and outgoing links. PageRank directed graphs repre-
sent webpages as nodes and their linking to other 
webpages as edges. A random walk model is thus 
applied to build a path between the nodes, in order 
to grade the importance of a webpage in the graph. 
Similarly to grading webpages through travers-
ing a graph, TextRank attempts to weight sentences 
of a text by building an undirected graph. Nodes are 
now sentences, and edges express their similarity 
degrees to other sentences in the text. Actually, the 
degree of similarity is based upon content overlap. 
As such, similarity degrees help assess the overall 
cohesive structure of a text. The more content over-
lap a sentence has with other sentences, the more 
important it is and more likely it is to be included in 
the extract.. Similarity is calculated through equa-
tion [1] (Mihalcea and Tarau, 2004), where Si and Sj 
are sentences and wk is a common token between 
18
them. The numerator is the sum of common words 
between Si and Sj. To reduce bias, normalization of 
the involved sentences length takes place, as shows 
the denominator. 
 
|)log(||)log(|
|}|{|),(
ji
jkikk
ji SS
SwSwwSSSim +
???=  [1] 
 
Once the graph and all similarity degrees are 
produced, sentence importance is calculated by the 
random walk algorithm shown in equation [2]. 
TR(Vi) signals sentence importance, d is an arbitrary 
parameter in the interval [0,1], and N is the number 
of sentences in the text. Parameter d integrates the 
probability of jumping from one vertex to another 
randomly chosen. Thus, it is responsible for random 
walking. This parameter is normally set to 0.85 (this 
value is also used in TextRank). 
 
? ?
-
= -
=
??
??
?
?
??
??
?
?
??+-=
1
0
1
0
),(
),()()1()(
N
j
N
k
kj
ji
ji
SSSim
SSSimVTRddVTR  [2] 
 
Initial TR similarity values are randomly set in 
the [0,1] interval. After successive calculations, 
those values converge to the targeted importance 
value. After calculating the importance of the verti-
ces, the sentences are sorted in reverse order and the 
top ones are selected to compose the extract. As 
usual, the number of sentences of the extract is de-
pendent upon a given compression rate. 
  Clearly, TextRank is not language dependent. 
For this reason Mihalcea (2005) could use it to 
evaluate AS on texts in Brazilian Portuguese, be-
sides reporting results on texts in English. She also 
explored distinct means of representing a text with-
out considering linguistic knowledge, emphasizing 
TextRank language and domain independence. She 
varies, e.g., the ways the graphs could be traversed 
using both directed and undirected graphs. Once a 
sentence is chosen to compose an extract, having 
undirected graphs makes possible, to look forward ? 
from the sentence to its outgoing edges (i.e., focus-
ing on the set of its following sentences in the text) 
? or to look backward, considering that sentence 
incoming edges and, thus, the set of its preceding 
sentences in the text. 
Another variation proposed by Mihalcea is to 
replace the PageRank algorithm (Equation [2]) by 
HITS (Kleinberg, 1999). This works quite simi-
larly to PageRank. However, instead of aggregat-
ing the scores for both incoming and outgoing 
links of a node in just one final score, it produces 
two independent scores. These are correspondingly 
named ?authority? and ?hub? scores. 
4 Improving TextRank through varia-
tions on linguistic information 
To improve the similarity scores between sen-
tences in TextRank we fed it with more linguistic 
knowledge, yielding its two modified versions. The 
first variation focused just upon basic preprocess-
ing; the second one, on the use of a thesaurus to 
calculate semantic similarity to promote AS deci-
sions. However, we did not modify the main ex-
tractive algorithm of TextRank: we kept the graph 
undirected and used PageRank as the score deter-
miner. Actually, we modified only the method of 
computing the edges weights. 
4.1 Using Basic Preprocessing Methods 
In applying Equation 1 for similarity scores, only 
exact matches between two words are allowed. 
Since in Brazilian Portuguese there are many mor-
phological and inflexional endings for most words, 
this process becomes troublesome: important 
matches may be ignored. To overcome that, we used 
a stemmer for Brazilian Portuguese (Caldas Jr. et 
al., 2001) based upon Porter?s algorithm (1980). We 
also removed stopwords from the source text, be-
cause they are not useful in determining similarity. 
The resulting version of TextRank is named hereaf-
ter ?TextRank+Stem+StopwordsRem?. 
4.2 Using a Thesaurus 
Our second TextRank variation involved plugging 
into the system a Brazilian Portuguese thesaurus 
(Dias-da-Silva et al, 2003). Our hypothesis here is 
that semantic similarity of the involved words is 
also important to improve the informativeness of 
the extracts under production. Thus, an extractive 
summarizer should consider not only word repeti-
tion in the source text, but also synonymy and an-
tonymy.  
Although plugging the thesaurus into the 
automatic summarizer did not imply changing its 
main method of calculating similarity, there were 
some obstacles to overcome concerning the follow-
ing:  
19
 
 
 
 
 
 Figure 1. SuPor-2 training phase 
 
  
Figure 2. SuPor-2 extraction phase 
 
a) Should we consider only synonyms or both 
synonyms and antonyms in addition to term 
repetition (reiteration)? 
 
b) How to acknowledge, and disentangle, se-
mantic similarity, when polissemy, for ex-
ample, is present? 
 
c) Once the proper relations have been 
determined, how should they be weighted? 
Just considering all thesaural relations to be 
equally important might not be the best ap-
proach. 
Concerning (a), synonyms, antonyms, and term 
repetition were all considered, as suggested by oth-
ers (e.g., Barzilay and Elhadad, 1997). We did not 
tackle (b) to choose the right sense of a word be-
cause of the lack of an effective disambiguation 
procedure for Brazilian Portuguese. Finally, in 
tackling (c) and, thus, grading the importance of 
the relations for sentence similarity, we adopted 
the same weights proposed by Barzilay and Elha-
dad (1997) in their lexical chaining method, which 
is discussed in more detail below. For both reitera-
tion and synonymy, they assume a score of 10 for 
the considered lexical chain; for antonymy, they 
suggest a score of 7. The resulting version of Tex-
tRank is named here ?TextRank+Thesaurus?. 
5 The SuPor-2 System 
SuPor-2 is an extractive summarizer built from 
scratch for Brazilian Portuguese. It embeds differ-
ent features in order to identify and extract relevant 
sentences of a source text. To configure SuPor-2 
for an adequate combination of such features we 
employ a machine learning approach. Figures 1 
and 2 depict the training and extraction phases, 
respectively. 
For training, machine learning is carried out by a 
Na?ve-Bayes classifier that employs Kernel meth-
ods for numeric feature handling, known as Flexi-
ble Bayes (John and Langley, 1995). This 
environment is provided by WEKA1 (Witten and 
Frank, 2005), which is used within SuPor-2 itself. 
The training corpus comprises both source texts 
and corresponding reference extracts. Every sen-
tence from a source text is represented in the train-
                                                        
1 Waikato Environment for Knowledge Analysis. Available at 
http://www.cs.waikato.ac.nz/ml/weka/ (December, 2006) 
20
ing dataset as a tuple of the considered features. 
Each tuple is labeled with its class, which signals if 
the sentence appears in a reference extract. The 
class label will be true if the sentence under focus 
matches a sentence of the reference extract and 
false otherwise. 
Once produced, the training dataset is used by 
the Bayesian classifier to depict the sentences that 
are candidates to compose the extract (Figure 2). In 
other words, the probability for the ?true? class is 
computed and the top-ranked sentences are se-
lected, until reaching the intended compression 
rate. 
When computing features, three full methods 
(M) and four corpus-based parameters (P) are con-
sidered. Both methods and parameters are mapped 
onto the feature space and are defined as follows: 
 
(M) Lexical Chaining (Barzilay and Elhadad, 
1997). This method computes the connectedness 
between words aiming at determining lexical 
chains in the source text. The stronger a lexical 
chain, the more important it is considered for ex-
traction. Both an ontological resource and Word-
Net (Miller et al, 1990) are used to identify 
different relations, such as synonymy or antonym, 
hypernymy or hyponymy, that intervene to com-
pute connectedness. The lexical chains are then 
used to produce three sets of sentences. To identify 
and extract sentences from those sets, three heuris-
tics are  made available, namely: (H1) selecting 
every sentence s of the source text based on each 
member m of every strong lexical chain of the text. 
In this case, s is the sentence that contains the first 
occurrence of m; (H2) this heuristics is similar to 
the former one, but instead of considering all the 
members of a strong lexical chain, it uses only the 
representative ones. A representative member is 
one whose frequency is greater than the average 
frequency of all words in the chain; (H3) a sen-
tence s is chosen by focusing only on representa-
tive lexical chains of every topic of the source text. 
In SuPor-2, the mapping of this method onto a 
nominal feature is accomplished by signaling 
which heuristics have recommended the sentence. 
Thus, features in the domain may range over the 
values {?None?, ?H1?, ?H2?, ?H3?, ?H1H2?, 
?H1H3?, ?H2H3?, ?H1H2H3?}. 
 
(M) Relationship Mapping (Salton et al, 
1997). This method performs similarly to the pre-
vious one and also to TextRank in that it builds up 
a graph interconnecting text segments. However, it 
considers paragraphs instead of sentences as verti-
ces. Hence, graph edges signal the connectiveness 
of the paragraphs of the source text. Similarity 
scores between two paragraphs are thus related to 
the degree of connectivity of the nodes. Similarly 
to Lexical Chaining, Salton et al also suggest three 
different ways of producing extracts. However, 
they now depend on the way the graph is traversed. 
The so-called dense or bushy path (P1), deep path 
(P2), and segmented path (P3) aim at tackling dis-
tinct textual problems that may damage the quality 
of the resulting extracts. The dense path considers 
that paragraphs are totally independent from each 
other, focusing on the top-ranked ones (i.e., the 
ones that are denser). As a result, it does not guar-
antee that an extract will be cohesive. The deep 
path is intended to overcome the former problem 
by choosing paragraphs that may be semantically 
inter-related. Its drawback is that only one topic, 
even one that is irrelevant, may be conveyed in the 
extract. Thus, it may lack proper coverage of the 
source text. Finally, the segmented path aims at 
overcoming the limitations of the former ones, ad-
dressing all the topics at once. Similarly to Lexical 
Chaining, features in the Relationship method 
range over the set {?None?,?P1?,?P2?,?P3?, ?P1P2?, 
?P1P3?, ?P2P3?, ?P1P2P3?}. 
 
(M) Importance of Topics (Larocca Neto et 
al., 2000). This method also aims at identifying the 
main topics of the source text, however through the 
TextTiling algorithm (Hearst, 1993). Once the top-
ics of the source text have been determined, the 
first step is to select sentences that better express 
the importance of each topic. The amount of sen-
tences, in this case, is proportional to the topic im-
portance. The second step is to determine the 
sentences that will actually be included in the ex-
tract. This is carried out by measuring their simi-
larity to their respective topic centroids (Larocca 
Neto et al, 2000). The method thus signals how 
relevant a sentence is to a given topic. In SuPor-2 
this method yields a numeric feature whose value 
conveys the harmonic mean between the sentence 
similarity to the centroid of the topic in which it 
appears and the importance of that topic.  
(P) Sentence Length (Kupiec et al, 1995). 
This parameter just signals the normalized count of 
words of a sentence. 
21
(P) Sentence Location (Edmundson, 1969). 
This parameter takes into account the position of a 
sentence in the text. It is valued, thus, in 
{?II?,?IM?,?IF?,?MI?,?MM?,?MF?,?FI?,?FM?,?FF?}. 
In this set the first letter of each label signals the 
position of the sentence within a paragraph (Initial, 
Medium, or Final). Similarly, the second letter sig-
nals the position of the paragraph within the text. 
(P) Occurrence of proper nouns (e.g., Kupiec 
et al, 1995). This parameter accounts for the num-
ber of proper nouns in a sentence.  
(P) Word Frequency (Luhn, 1958). This pa-
rameter mirrors the normalized sum of the word 
frequency in a sentence. 
SuPor-2 provides a flexible way of combining 
linguistic and non-linguistic features for extraction. 
There are profound differences from TextRank. 
First, it is clearly language-dependent. Also, its 
graph-based methods do not assign weights to their 
vertices in order to select sentences for extraction. 
Instead, they traverse a graph in very specific  and 
varied ways that mirror both linguistic interde-
pendencies and important connections between the 
nodes. 
6 Assessing the Four Systems 
To assess the degree of informativeness of the sys-
tems previously described, we adopt ROUGE2 (Lin 
and Hovy, 2003), whose recall rate mirrors the in-
formativeness degree of automatically generated 
extracts by correlating automatic summaries with 
ideal ones. 
The two modified versions of TextRank require 
linguistic knowledge but at a low cost. This is cer-
tainly due to varying only preprocessing, while the 
main decision procedure is kept unchanged and 
language-independent. Those three systems do not 
need training, one of the main arguments in favor 
of TextRank (Mihalcea and Tarau, 2004). In con-
trast, SuPor-2 relies on training and this is certainly 
one of its main bottlenecks. It also employs lin-
guistic knowledge for both preprocessing and ex-
traction, which TextRank purposefully avoids. 
However, using WEKA has made its adjustments 
less demanding and more consistent, indicating 
that scaling up the system is feasible.  
                                                        
2 Recall-Oriented Understudy for Gisting Evaluation. Avail-
able at http://haydn.isi.edu/ROUGE/ (January, 2007). 
In our assessment, the same single-document 
summarization scenario posed by Mihalcea (2005) 
was adopted, namely: (a) we considered the Brazil-
ian Portuguese TeM?rio corpus (Pardo and Rino, 
2003); (b) we used the same baseline, which se-
lects top-first sentences to include in the extract; 
(c) we adopted a 70-75% compression rate, making 
it compatible with the compression rate of the ref-
erence summaries; and (d) ROUGE was used for 
evaluation in its Ngram(1,1) 95% confidence rate 
setting, without stopwords removal. TeM?rio com-
prises 100 newspaper articles from online Brazilian 
newswire. A set of corresponding manual summa-
ries produced by an expert in Brazilian Portuguese 
is also included in TeM?rio. These are our refer-
ence summaries. 
For training and testing SuPor-2, we avoided 
building an additional training corpus by using a 
10-fold cross-validation procedure. Finally, we 
produced three sets of extracts using ?TextRank +  
Stem + StopwordsRem?, ?TextRank + Thesaurus?, 
and SuPor-2 on the TeM?rio source texts. Results 
for informativeness are shown in Table 1. Since 
Mihalcea?s setting was kept unchanged, we just 
included in that table the same results presented in 
(Mihalcea, 2005), i.e., we did not run her systems 
all over again. We also reproduced for comparison 
the TextRank variations reported by Mihalcea, es-
pecially regarding graph-based walks by PageRank 
and HITS. Shaded lines correspond to our sug-
gested methods presented in Sections 4 and 5, 
which involve differing degrees of dependence on 
linguistic knowledge. 
It can be seen that ?TextRank+Thesaurus? and 
?TextRank+Stem+StopwordsRem? considerably 
outperformed all other versions of TextRank. 
Compared with Mihalcea's best version, i.e., with 
'TextRank (PageRank - backward)', those two 
methods represented a 6% and 9% improvement, 
respectively. We can conclude that neither the way 
the graph is built nor the choice of the graph-based 
ranking algorithm affects the results as signifi-
cantly as do the linguistic-based methods. Clearly, 
both variations proposed in this paper signal that 
linguistic knowledge, even if only used at the pre-
processing stage, provides more informative ex-
tracts than those produced when no linguistic 
knowledge at all is considered. Moreover, at that 
stage little modeling and computational effort is 
demanded, since lexicons, stoplists, and thesauri 
22
are quite widely available nowadays for several 
Romance languages. 
Even the baseline outperformed most versions 
of TextRank, showing that linguistic independence 
in a random walk model for extractive AS should 
be reconsidered. Actually, this shows that linguis-
tic knowledge does make a difference, at least for 
summarizing newswire texts in Brazilian Portu-
guese. 
In addition, SuPor-2 performance exceeds the 
best version of TextRank that uses no linguistic 
knowledge ? ?TextRank (PageRank - backward)? ? 
by about 14%. 
 
 
System ROUGE NGram(1,1) 
SuPor-2 0,5839 
TextRank+Thesaurus 0,5603 
TextRank+Stem+StopwordsRem 0,5426 
TextRank (PageRank - backward) 0,5121 
TextRank (HIT hub - forward) 0,5002 
TextRank (HITS authority - backward) 0,5002 
Baseline 0,4963 
TextRank (PageRank - undirected) 0,4939 
TextRank (HITS authority - forward) 0,4834 
TextRank (HIT hub - backward) 0,4834 
TextRank (HITS authority - undirected) 0,4814 
TextRank (HIT hub - undirected) 0,4814 
TextRank (PageRank - forward) 0,4574 
 
Table 1. Informativeness comparison between ex-
tractive summarizers 
7 Final Remarks 
A critical issue in the comparison presented above 
is the contrast between having an unsupervised or 
supervised summarizer, which is related to the is-
sue on having linguistic-independent extractive 
summarizers. Perhaps the question that we should 
pose here is how interesting and useful an extrac-
tive automatic summarizer that is totally independ-
ent from linguistic knowledge can actually be. To 
our view, the more non-informative an extract, the 
less useful it may be. So, summarizers that do not 
reach a minimum threshold concerning informa-
tiveness are deemed to failure nowadays. Clearly, 
SuPor-2 requires language-dependent resources, 
but its main extraction procedure is still general 
enough to make it portable and adaptable to new 
domains and languages. Hence, SuPor-2 assess-
ment suggests that it may be interesting to scale up 
SuPor-2. 
Considering that SuPor-2 is one of the best ex-
tractive summarizers for Brazilian Portuguese texts 
(Leite and Rino, 2006) and ?TextRank+Thesaurus? 
performed only 4% below it, we can also argue  in 
favor of providing even simple linguistic proce-
dures for extractive AS. The latter system shows 
that TextRank can yield extracts nearly as informa-
tive as those produced by the former, when em-
bedding stemming and stopwords removal. It can 
also perform AS with little computational effort 
and no training, when compared to the supervised 
SuPor-2. As a conclusion, we see that some lin-
guistic knowledge may boost TextRank perform-
ance without too much effort, since language-
dependent resources for preprocessing texts in 
natural language are usually available and easy to 
handle, concerning our addressed approach. 
There are many experiments that may be derived 
from our discussion in this paper (1) Although the 
reported results suggest that linguistic knowledge 
does make a difference when embedded in lan-
guage-free extractive summarizers, the perform-
ance of the top systems assessed through ROUGE 
should be more comprehensively licensed through 
additional assessment tasks. (2) These could also 
incorporate other graph-based algorithms than 
TextRank, such as the LexRank one, aiming at re-
assuring our claim and scaling up graph-based ap-
proaches. (3) Since we addressed language-
independence (thus portability) versus language-
dependence for informativeness, it would also be 
interesting to explore other domains or languages 
to support our claim or, at least, to look for other 
findings to confirm if linguistic knowledge indeed 
makes a difference. (4) Other TextRank variations 
could also be explored, to see if adding more fea-
tures would make TextRank closer to SuPor-2. 
Acknowledgements 
This work has been supported by the Brazilian re-
search funding agencies CNPq, CAPES and 
FAPESP.
23
References 
B. C. Dias-da-Silva, M. F. Oliveira, H. R. Moraes, C. 
Paschoalino, R. Hasegawa, D. Amorin and A. C. 
Nascimento. 2000. Constru??o de um Thesaurus Ele-
tr?nico para o Portugu?s do Brasil. In Proceedings of 
the V Encontro para o Processamento Computacio-
nal da L?ngua Portuguesa Escrita e Falada 
(PROPOR 2000), S?o Carlos, Brasil , 1-11. 
C. Lin and E. H. Hovy. 2003. Automatic Evaluation of 
Summaries Using N-gram Co-occurrence Statistics. 
In Proceedings of Language Technology Conference 
(HLT-NAACL 2003), Edmonton, Canada.  
D. Marcu. 1999. Discourse Trees Are Good Indicators 
of Importance in Text. In Mani, I., Maybury, M. T. 
(Eds.). 1999. Advances in Automatic Text Summari-
zation. MIT Press. 
D. S. Leite and L. H. M. Rino. 2006. Selecting a Feature 
Set to Summarize Texts in Brazilian Portuguese. In J. 
S. Sichman et al (eds.): Proceedings of 18th. Brazil-
ian Symposium on Artificial Intelligence (SBIA'06) 
and 10th. Ibero-American Artificial Intelligence Con-
ference (IBERAMIA'06). Lecture Notes on Artificial 
Intelligence, No. 4140, Springer-Verlag, 462-471. 
G. Erkan and D R. Radev. 2004. LexRank: Graph-based 
Lexical Centrality as Salience in Text Summariza-
tion. Journal of Artificial Intelligence Research 
22:457-479 
G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross and 
K. Miller. 1990. Introduction to WordNet: An On-
line Lexical Database. International Journal of Lexi-
cography 3(4):235-244 
G. Salton, and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information 
Processing & Management 24 : 513-523.. Reprinted 
in: K. Sparck-Jones and P. Willet (eds.). 1997. Read-
ings in Information Retrieval, Morgan Kaufmann, 
323-328.  
H. Luhn. 1958. The automatic creation of literature ab-
stracts. IBM Journal of Research and Development 
2:159-165 
H. P. Edmundson. 1969. New methods in automatic 
extracting. Journal of the Association for Computing 
Machinery 16:264-285. 
I. Witten and E. Frank. 2005. Data Mining: Practical 
machine learning tools and techniques, 2nd ed. Mor-
gan Kaufmann, San Francisco. 
I. Mani. 2001. Automatic Summarization. John Benja-
min?s Publishing Company.  
I. Mani and M. T. Maybury. 1999. Advances in Auto-
matic Text Summarization. MIT Press. 
J. Caldas Junior, C. Y. M. Imamura and S. O. Rezende. 
Avalia??o de um Algoritmo de Stemming para a 
L?ngua Portuguesa. In Proceedings of the 2nd Con-
gress of Logic Applied to Technology 
(LABTEC?2001), vol. II. Faculdade SENAC de Ci?n-
cias Exatas e Tecnologia, S?o Paulo, Brasil (2001), 
267-274. 
J. M. Kleinberg. 1999. Authoritative sources in hyper-
linked environment. Journal of the ACM, 46(5):604-
632. 
J. Kupiec, J. Pedersen and F. Chen. 1995. A trainable 
document summarizer. In Proceedings of the 18th 
ACM-SIGIR Conference on Research & Develop-
ment in Information Retrieval, 68-73. 
J. Larocca Neto, A. D. Santos, C. A. A. Kaestner and A. 
A. Freitas. 2000. Generating Text Summaries 
through the Relative Importance of Topics. Lecture 
Notes in Artificial Intelligence, No. 1952. Springer-
Verlag, 200-309 
M. A. Hearst. 1993. TextTiling: A Quantitative Ap-
proach to Discourse Segmentation. Technical Report 
93/24. University of California, Berkeley. 
M. F. Porter. 1980. An Algorithm for Suffix Stripping. 
Program, 14 (3) : 130-137 
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing 
Order into Texts. In  Proceedings of the Conference 
on Empirical Methods in Natural Language Process-
ing (EMNLP 2004), Barcelona, Spain, July.  
R. Mihalcea. 2005. Language Independent Extractive 
Summarization. In Proceedings of the 43th Annual 
Meeting of the Association for Computational Lin-
guistics, Companion Volume (ACL2005), Ann Ar-
bor, MI, June. 
R. Barzilay and M. Elhadad. 1997. Using lexical chains 
for text summarization. In Proceedings of the Intelli-
gent Scalable Text Summarization Workshop 
(ISTS'97), ACL, Madrid, Spain.  
S. Brin and L. Page. 1998. The anatomy of a large-scale 
hypertextual Web search engine. Computer Networks 
and ISDN Systems 30:1-7. 
T. A. S. Pardo and L.H.M. Rino. 2003. TeM?rio: A cor-
pus for automatic text summarization (in Portu-
guese). NILC Tech. Report NILC-TR-03-09  
24
