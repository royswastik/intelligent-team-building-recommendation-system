Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 708?715, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Using Sketches to Estimate Associations
Ping Li
Department of Statistics
Stanford University
Stanford, California 94305
pingli@stat.stanford.edu
Kenneth W. Church
Microsoft Research
One Microsoft Way
Redmond, Washington 98052
church@microsoft.com
Abstract
We should not have to look at the en-
tire corpus (e.g., the Web) to know if two
words are associated or not.1 A powerful
sampling technique called Sketches was
originally introduced to remove duplicate
Web pages. We generalize sketches to
estimate contingency tables and associa-
tions, using a maximum likelihood esti-
mator to find the most likely contingency
table given the sample, the margins (doc-
ument frequencies) and the size of the
collection. Not unsurprisingly, computa-
tional work and statistical accuracy (vari-
ance or errors) depend on sampling rate,
as will be shown both theoretically and
empirically. Sampling methods become
more and more important with larger and
larger collections. At Web scale, sampling
rates as low as 10?4 may suffice.
1 Introduction
Word associations (co-occurrences) have a wide
range of applications including: Speech Recogni-
tion, Optical Character Recognition and Information
Retrieval (IR) (Church and Hanks, 1991; Dunning,
1993; Manning and Schutze, 1999). It is easy to
compute association scores for a small corpus, but
more challenging to compute lots of scores for lots
of data (e.g. the Web), with billions of web pages
(D) and millions of word types (V ). For a small
corpus, one could compute pair-wise associations by
multiplying the (0/1) term-by-document matrix with
its transpose (Deerwester et al, 1999). But this is
probably infeasible at Web scale.
1This work was conducted at Microsoft while the first author
was an intern. The authors thank Chris Meek, David Hecker-
man, Robert Moore, Jonathan Goldstein, Trevor Hastie, David
Siegmund, Art Own, Robert Tibshirani and Andrew Ng.
Approximations are often good enough. We
should not have to look at every document to de-
termine that two words are strongly associated. A
number of sampling-based randomized algorithms
have been implemented at Web scale (Broder, 1997;
Charikar, 2002; Ravichandran et al, 2005).2
A conventional random sample is constructed by
selecting Ds documents from a corpus of D doc-
uments. The (corpus) sampling rate is DsD . Of
course, word distributions have long tails. There
are a few high frequency words and many low fre-
quency words. It would be convenient if the sam-
pling rate could vary from word to word, unlike con-
ventional sampling where the sampling rate is fixed
across the vocabulary. In particular, in our experi-
ments, we will impose a floor to make sure that the
sample contains at least 20 documents for each term.
(When working at Web scale, one might raise the
floor somewhat to perhaps 104.)
Sampling is obviously helpful at the top of the
frequency range, but not necessarily at the bottom
(especially if frequencies fall below the floor). The
question is: how about ?ordinary? words? To answer
this question, we randomly picked 15 pages from
a Learners? dictionary (Hornby, 1989), and selected
the first entry on each page. According to Google,
there are 10 million pages/word (median value, ag-
gregated over the 15 words), no where near the floor.
Sampling can make it possible to work in mem-
ory, avoiding disk. At Web scale (D ? 10 billion
pages), inverted indexes are large (1500 GBs/billion
pages)3, probably too large for memory. But a sam-
ple is more manageable; the inverted index for a
10?4 sample of the entire web could fit in memory
on a single PC (1.5 GB).
2http://labs.google.com/sets produces fascinating sets, al-
though we don?t know how it works. Given the seeds, ?Amer-
ica? and ?China,? http://labs.google.com/sets returns: ?Amer-
ica, China, Japan, India, Italy, Spain, Brazil, Persia, Europe,
Australia, France, Asia, Canada.?
3This estimate is extrapolated from Brin and Page (1998),
who report an inverted index of 37.2 GBs for 24 million pages.
708
Table 1: The number of intermediate results after the
first join can be reduced from 504,000 to 120,000,
by starting with ?Schwarzenegger & Austria? rather
than the baseline (?Schwarzenegger & Terminator?).
The standard practice of starting with the two least
frequent terms is a good rule of thumb, but one can
do better, given (estimates of) joint frequencies.
Query Hits (Google)
Austria 88,200,000
Governor 37,300,000
Schwarzenegger 4,030,000
Terminator 3,480,000
Governor & Schwarzenegger 1,220,000
Governor & Austria 708,000
Schwarzenegger & Terminator 504,000
Terminator & Austria 171,000
Governor & Terminator 132,000
Schwarzenegger & Austria 120,000
1.1 An Application: The Governator
Google returns the top k hits, plus an estimate of
how many hits there are. Table 1 shows the number
of hits for four words and their pair-wise combina-
tions. Accurate estimates of associations would have
applications in Database query planning (Garcia-
Molina et al, 2002). Query optimizers construct a
plan to minimize a cost function (e.g., intermediate
writes). The optimizer could do better if it could
estimate a table like Table 1. But efficiency is im-
portant. We certainly don?t want to spend more time
optimizing the plan than executing it.
Suppose the optimizer wanted to construct a plan
for the query: ?Governor Schwarzenegger Termi-
nator Austria.? The standard solution starts with
the two least frequent terms: ?Schwarzenegger? and
?Terminator.? That plan generates 504,000 interme-
diate writes after the first join. An improvement
starts with ?Schwarzenegger? with ?Austria,? reduc-
ing the 504,000 down to 120,000.
In addition to counting hits, Table 1 could also
help find the top k pages. When joining the first pair
of terms, we?d like to know how far down the rank-
ing we should go. Accurate estimates of associations
would help the optimizer make such decisions.
It is desirable that estimates be consistent, as well
as accurate. Google, for example, reports 6 million
hits for ?America, China, Britain,? and 23 million for
?America, China, Britain, Japan.? Joint frequencies
decrease monotonically: s ? S =? hits(s) ? hits(S).
f = a + c
f = a + b
D = a+b+c+d
a
c
y ~y
~x
x
d y
xb
(a)
x
~x
y ~y
a b
c d y
x
s s s s
s s
ss
n  = a + b
sD = a +b + c +d
n  = a + c
s s
ss
(b)
Figure 1: (a): A contingency table for word x and
word y. Cell a is the number of documents that con-
tain both x and y, b is the number that contain x but
not y, c is the number that contain y but not x, and
d is the number that contain neither x nor y. The
margins, fx = a + b and fy = a + c are known as
document frequencies in IR. D is the total number
of documents in the collection. (b): A sample con-
tingency table, with ?s? indicating the sample space.
1.2 Sampling and Estimation
Two-way associations are often represented as two-
way contingency tables (Figure 1(a)). Our task is to
construct a sample contingency table (Figure 1(b)),
and estimate 1(a) from 1(b). We will use a max-
imum likelihood estimator (MLE) to find the most
likely contingency table, given the sample and vari-
ous other constraints. We will propose a sampling
procedure that bridges two popular choices: (A)
sampling over documents and (B) sampling over
postings. The estimation task is straightforward and
well-understood for (A). As we consider more flexi-
ble sampling procedures such as (B), the estimation
task becomes more challenging.
Flexible sampling procedures are desirable. Many
studies focus on rare words (Dunning, 1993; Moore,
2004); butterflies are more interesting than moths.
The sampling rate can be adjusted on a word-by-
word basis with (B), but not with (A). The sampling
rate determines the trade-off between computational
work and statistical accuracy.
We assume a standard inverted index. For each
word x, there are a set of postings, X. X contains a
set of document IDs, one for each document contain-
ing x. The size of postings, fx = |X|, corresponds
to the margins of the contingency tables in Figure
1(a), also known as document frequencies in IR.
The postings lists are approximated by sketches,
skX, first introduced by Broder (1997) for remov-
ing duplicate web pages. Assuming that document
IDs are random (e.g., achieved by a random permu-
tation), we can compute skX, a random sample of
709
X, by simply selecting the first few elements of X.
In Section 3, we will propose using sketches
to construct sample contingency tables. With this
novel construction, the contingency table (and sum-
mary statistics based on the table) can be estimated
using conventional statistical methods such as MLE.
2 Broder?s Sketch Algorithm
One could randomly sample two postings and inter-
sect the samples to estimate associations. The sketch
technique introduced by Broder (1997) is a signifi-
cant improvement, as demonstrated in Figure 2.
Assume that each document in the corpus of size
D is assigned a unique random ID between 1 and D.
The postings for word x is a sorted list of fx doc IDs.
The sketch, skX, is the first (smallest) sx doc IDs in
X. Broder used MINs(Z) to denote the s smallest
elements in the set, Z . Thus, skX = MINsx(X).
Similarly, Y denotes the postings for word y, and
skY denotes its sketch, MINsy(Y ). Broder assumed
sx = sy = s.
Broder defined resemblance (R) and sample re-
semblance (Rs) to be:
R = aa + b + c , Rs =
|MINs(skX ? skY ) ? skX ? skY |
|MINs(skX ? skY )|
.
Broder (1997) proved that Rs is an unbiased esti-
mator of R. One could use Rs to estimate a but he
didn?t do that, and it is not recommended.4
Sketches were designed to improve the coverage
of a, as illustrated by Monte Carlo simulation in Fig-
ure 2. The figure plots, E
(as
a
)
, percentage of inter-
sections, as a function of (postings) sampling rate,
s
f , where fx = fy = f , sx = sy = s. The solid lines
(sketches), E (asa
)
? sf , are above the dashed curve
(random sampling), E (asa
)
= s2f2 . The difference is
particularly important at low sampling rates.
3 Generalizing Sketches: R? Tables
Sketches were first proposed for estimating resem-
blance (R). This section generalizes the method to
construct sample contingency tables, from which we
can estimate associations: R, LLR, cosine, etc.
4There are at least three problems with estimating a from
Rs. First, the estimate is biased. Secondly, this estimate uses
just s of the 2 ? s samples; larger samples ? smaller errors.
Thirdly, we would rather not impose the restriction: sx = sy.
0  0.2 0.4 0.6 0.8 10
0.5
1
Sampling rates
Pe
rc
en
ta
ge
 o
f i
ne
rs
ec
tio
ns
Random sampling
Sketch
Figure 2: Sketches (solid curves) dominate random
sampling (dashed curve). a=0.22, 0.38, 0.65, 0.80,
0.85f , f=0.2D, D=105. There is only one dashed
curve across all values of a. There are different but
indistinguishable solid curves depending on a.
Recall that the doc IDs span the integers from 1
to D with no gaps. When we compare two sketches,
skX and skY , we have effectively looked at Ds =
min{skX(sx), skY(sy)} documents, where skX(j) is
the jth smallest element in skX. The following
construction generates the sample contingency ta-
ble, as, bs, cs, ds (as in Figure 1(b)). The example
shown in Figure 3 may help explain the procedure.
Ds = min{skX(sx), skY(sy)}, as = |skX ? skY |,
nx = sx ? |{j : skX(j) > Ds}|,
ny = sy ? |{j : skY(j) > Ds}|,
bs = nx ? as, cs = ny ? as, ds = Ds ? as ? bs ? cs.
Given the sample contingency table, we are now
ready to estimate the contingency table. It is suffi-
cient to estimate a, since the rest of the table can be
determined from fx, fy and D. For practical appli-
cations, we recommend the convenient closed-form
approximation (8) in Section 5.1.
4 Margin-Free (MF) Baseline
Before considering the proposed MLE method, we
introduce a baseline estimator that will not work as
well because it does not take advantage of the mar-
gins. The baseline is the multivariate hypergeomet-
ric model, usually simplified as a multinomial by as-
suming ?sample-with-replacement.?
The sample expectations are (Siegrist, 1997),
E(as) = DsD a, E(bs) =
Ds
D b,
E(cs) = DsD c, E(ds) =
Ds
D d. (1)
710
Y:  2   4   5   8   15    19   21     24   27   28   31 
f
X:  3   4   7   9   10   15   18      19   24   25   28
= 11 = 5 = 18f a Dx y = 11 s
= 5= 7= 7sy= 7sx
b c= 5= 2as s s= 3
n nx y
ds = 8
(a)
    
 
9     10     11    12    13    14    15   16  
1      2      3      4      5      6      7      8   
17   18     19    20    . . . . . .             D = 36 
(b)
Figure 3: (a): The two sketches, skX and skY
(larger shaded box), are used to construct a sam-
ple contingency table: as, bs, cs, ds. skX consists
of the first sx = 7 doc IDs in X, the postings for
word x. Similarly, skY consists of the first sy = 7
doc IDs in Y , the postings for word y. There are 11
doc IDs in both X and Y , and a = 5 doc IDs in
the intersection: {4, 15, 19, 24, 28}. (a) shows that
Ds = min(18, 21) = 18. Doc IDs 19 and 21 are
excluded because we cannot determine if they are in
the intersection or not, without looking outside the
box. As it turns out, 19 is in the intersection and
21 is not. (b) enumerates the Ds = 18 documents,
showing which documents contain x (small circles)
and which contain y (small squares). Both proce-
dures, (a) and (b), produce the same sample contin-
gency table: as = 2, bs = 5, cs = 3 and ds = 8.
The margin-free estimator and its variance are
a?MF =
D
Ds
as, Var(a?MF ) =
D
Ds
1
1
a + 1D?a
D ?Ds
D ? 1 . (2)
For the multinomial simplification, we have
a?MF,r = DDs
as, Var(a?MF,r) = DDs
1
1
a + 1D?a
. (3)
where ?r? indicates ?sample-with-replacement.?
The term D?DsD?1 ? D?DsD is often called the
?finite-sample correction factor? (Siegrist, 1997).
5 The Proposed MLE Method
The task is to estimate the contingency table from
the samples, the margins and D. We would like to
use a maximum likelihood estimator for the most
probable a, which maximizes the (full) likelihood
(probability mass function, PMF) P (as, bs, cs, ds; a).
Unfortunately, we do not know the exact expres-
sion for P (as, bs, cs, ds; a), but we do know the con-
ditional probability P (as, bs, cs, ds|Ds; a). Since the
doc IDs are uniformly random, sampling the first
Ds contiguous documents is statistically equivalent
to randomly sampling Ds documents from the cor-
pus. Based on this key observation and Figure 3,
conditional on Ds, P (as, bs, cs, ds|Ds; a) is the PMF
of a two-way sample contingency table.
We factor the full likelihood into:
P (as, bs, cs, ds; a) = P (as, bs, cs, ds|Ds; a)? P (Ds; a).
P (Ds; a) is difficult. However, since we do not ex-
pect a strong dependency of Ds on a, we maxi-
mize the partial likelihood instead, and assume that
is good enough. An example of partial likelihood is
the Cox proportional hazards model in survival anal-
ysis (Venables and Ripley, 2002, Section 13.3) .
Our partial likelihood is
P (as, bs, cs, ds|Ds; a) =
` a
as
?`fx?a
bs
?`fy?a
cs
?`D?fx?fy+a
ds
?
`D
Ds
?
?
as?1
Y
i=0
(a? i) ?
bs?1
Y
i=0
(fx ? a? i) ?
cs?1
Y
i=0
(fy ? a? i)
?
ds?1
Y
i=0
(D ? fx ? fy + a? i), (4)
where
(n
m
)
= n!m!(n?m)! . ??? is ?proportional to.?
We now derive an MLE for (4), a result that was
not previously known, to the best of our knowledge.
Let a?MLE maximizes logP (as, bs, cs, ds|Ds; a):
as?1
X
i=0
log(a? i) +
bs?1
X
i=0
log (fx ? a? i)
+
cs?1
X
i=0
log (fy ? a? i) +
ds?1
X
i=0
log (D ? fx ? fy + a? i) ,
whose first derivative, ? logP (as,bs,cs,ds|Ds;a)?a , is
as?1
X
i=0
1
a? i ?
bs?1
X
i=0
1
fx ? a? i
?
cs?1
X
i=0
1
fy ? a? i
+
ds?1
X
i=0
1
D ? fx ? fy + a? i
. (5)
Since the second derivative, ?
2 logP (as,bs,cs,ds|Ds;a)
?a2 ,
is negative, the log likelihood function is concave,
hence has a unique maximum. One could numeri-
cally solve (5) for ? logP (as,bs,cs,ds|Ds;a)?a = 0. How-
ever, we derive the exact solution using the follow-
ing updating formula from (4):
711
P (as, bs, cs, ds|Ds; a) = P (as, bs, cs, ds|Ds; a? 1)?
fx ? a + 1? bs
fx ? a + 1
fy ? a + 1? cs
fy ? a + 1
D ? fx ? fy + a
D ? fx ? fy + a? ds
a
a? as
= P (as, bs, cs, ds|Ds; a? 1)? g(a). (6)
Since our MLE is unique, it suffices to find a from
g(a) = 1, which is a cubic function in a.
5.1 A Convenient Practical Approximation
Rather than solving the cubic equation for the ex-
act MLE, the following approximation may be more
convenient. Assume we sample nx = as + bs from
X and obtain as co-occurrences without knowledge
of the samples from Y . Further assuming ?sample-
with-replacement,? as is then binomially distributed,
as ? Binom(nx, afx ). Similarly, assume as ?
Binom(ny, afy ). Under these assumptions, the PMF
of as is a product of two binomial PMFs:
 
fx
nx
!
? a
fx
?as ?fx ? a
fx
?bs
 
fy
ny
!
? a
fy
?as ?fy ? a
fy
?cs
? a2as (fx ? a)bs (fy ? a)cs . (7)
Setting the first derivative of the logarithm of (7) to
be zero, we obtain 2asa ? bsfx?a ?
cs
fy?a = 0, which is
quadratic in a and has a solution:
a?MLE,a = fx (2as + cs) + fy (2as + bs)2 (2as + bs + cs)
?
q
(fx (2as + cs)? fy (2as + bs))2 + 4fxfybscs
2 (2as + bs + cs)
. (8)
Section 6 shows that a?MLE,a is very close to a?MLE .
5.2 Theoretical Evaluation: Bias and Variance
How good are the estimates? A popular metric
is mean square error (MSE): MSE(a?) = E (a?? a)2 =
Var (a?) +Bias2 (a?). If a? is unbiased, MSE(a?) =Var (a?) =
SE2 (a?), where SE is the standard error. Here all ex-
pectations are conditional on Ds.
Large sample theory (Lehmann and Casella,
1998, Chapter 6) says that, under ?sample-with-
replacement,? a?MLE is asymptotically unbiased and
converges to Normal with mean a and variance 1I(a) ,
where I(a), the Fisher Information, is
I(a) = ?E
?
?2
?a2 logP (as, bs, cs, ds|Ds; a, r)
?
. (9)
Under ?sample-with-replacement,? we have
P (as, bs, cs, ds|Ds; a, r) ?
? a
D
?as
?
?fx ? a
D
?bs
?
?
fy ? a
D
?cs
?
?
D ? fx ? fy + a
D
?ds
, (10)
Therefore, the Fisher Information, I(a), is
E(as)
a2 +
E(bs)
(fx ? a)2
+ E(cs)
(fy ? a)2
+ E(ds)
(D ? fx ? fy + a)2
.
(11)
We plug (1) from the margin-free model into (11)
as an approximation, to obtain
Var (a?MLE) ?
D
Ds ? 1
1
a + 1fx?a +
1
fy?a +
1
D?fx?fy+a
, (12)
which is 1I(a) multiplied by
D?Ds
D , the ?finite-
sample correction factor,? to consider ?sample-
without-replacement.?
We can see that Var (a?MLE) is less than
Var (a?MF ) in (2). In addition, a?MLE is asymptoti-
cally unbiased while a?MF is no longer unbiased un-
der margin constraints. Therefore, we expect a?MLE
has smaller MSE than a?MF . In other words, the pro-
posed MLE method is more accurate than the MF
baseline, in terms of variance, bias and mean square
error. If we know the margins, we ought to use them.
5.3 Unconditional Bias and Variance
a?MLE is also unconditionally unbiased:
E (a?MLE ? a) = E (E (a?MLE ? a|Ds)) ? E(0) = 0. (13)
The unconditional variance is useful because often
we would like to estimate the errors before knowing
Ds (e.g., for choosing sample sizes).
To compute the unconditional variance of a?MLE ,
we should replace DDs with E
(
D
Ds
)
in (12). We
resort to an approximation for E
?
D
Ds
?
. Note that
skX(sx) is the order statistics of a discrete random
variable (Siegrist, 1997) with expectation
E
`
skX(sx)
?
= sx(D + 1)fx + 1
? sxfx
D. (14)
By Jensen?s inequality, we know that
E
?
Ds
D
?
? min
 
E
`
skX(sx)
?
D ,
E
`
skY(sy)
?
D
!
= min
?
sx
fx
, syfy
?
(15)
E
? D
Ds
?
? 1
E
`Ds
D
? ? max
?fx
sx
, fysy
?
. (16)
712
Table 2: Gold standard joint frequencies, a. Docu-
ment frequencies are shown in parentheses. These
words are frequent, suitable for evaluating our algo-
rithms at very low sampling rates.
THIS HAVE HELP PROGRAM
THIS (27633) ? 13517 7221 3682
HAVE (17396) 13517 ? 5781 3029
HELP (10791) 7221 5781 ? 1949
PROGRAM (5327) 3682 3029 1949 ?
Replacing the inequalities with equalities underes-
timates the variance, but only slightly.
5.4 Smoothing
Although not a major emphasis here, our evalua-
tions will show that a?MLE+S , a smoothed version
of the proposed MLE method, is effective, espe-
cially at low sampling rates. a?MLE+S uses ?add-
one? smoothing. Given that such a simple method
is as effective as it is, it would be worth considering
more sophisticated methods such as Good-Turing.
5.5 How Many Samples Are Sufficient?
The answer depends on the trade-off between com-
putation and estimation errors. One simple rule is
to sample ?2%.? (12) implies that the standard er-
ror is proportional to
p
D/Ds ? 1. Figure 4(a) plots
p
D/Ds ? 1 as a function of sampling rate, Ds/D, in-
dicating a ?elbow? about 2%. However, 2% is too
large for high frequency words.
A more reasonable metric is the ?coefficient of
variation,? cv = SE(a?)a . At Web scale (10 billion
pages), we expect that a very small sampling rate
such as 10?4 or 10?5 will suffice to achieve a rea-
sonable cv (e.g., 0.5). See Figure 4(b).
6 Evaluation
Two sets of experiments were run on a collection of
D = 216 web pages, provided by MSN. The first ex-
periment considered 4 English words shown in Ta-
ble 2, and the second experiment considers 968 En-
glish words with mean df = 2135 and median df =
1135. They form 468,028 word pairs, with mean co-
occurrences = 188 and median = 74.
6.1 Small Dataset Monte Carlo Experiment
Figure 5 evaluates the various estimate methods by
MSE over a wide range of sampling rates. Doc IDs
0   0.02 0.05 0.1 0.150
10
20
30
Samplig rates
R
el
at
iv
e 
SE
(a)
105 106 107 108 109 1010
10?5
10?3
10?1
100
 D
Sa
m
pl
in
g 
ra
te
s
 f
x
 = 0.0001?D
 f
x
 =0.01?D
0.001
 fy = 0.1? fx
 a = 0.05 ? fy
(b)
Figure 4: How large should the sampling rate be?
(a): We can sample up to the ?elbow point? (2%),
but after that there are diminishing returns. (b): An
analysis based on cv = SEa = 0.5 suggests that we can
get away with much lower sampling rates. The three
curves plot the critical value for the sampling rate,
Ds
D , as a function of corpus size, D. At Web scale,
D ? 1010, sampling rates above 10?3 to 10?5 sat-
isfy cv < 0.5, at least for these settings of fx, fy
and a. The settings were chosen to simulate ?ordi-
nary? words. The three curves correspond to three
choices of fx: D/100, D/1000, and D/10, 000.
fy = fx/10, a = fy/20. SE is based on (12).
were randomly permuted 105 times. For each per-
mutation we constructed sketches from the inverted
index at a series of sampling rates. The figure shows
that the proposed method, a?MLE , is considerably
better (by 20% ? 40%) than the margin-free base-
line, a?MF . Smoothing is effective at low sampling
rates. The recommended approximation, a?MLE,a, is
remarkably close to the exact solution.
Figure 6 shows agreement between the theoreti-
cal and empirical unconditional variances. Smooth-
ing reduces variances, at low sampling rates. We
used the empirical E
?
D
DS
?
to compute the theoreti-
cal variances. The approximation, max
(
fx
sx ,
fy
sy
)
, is
> 0.95E
?
D
DS
?
at sampling rates > 0.01.
Figure 7 verifies that the proposed MLE is unbi-
ased, unlike the margin-free baselines.
6.2 Large Dataset Experiment
The large experiment considers 968 English words
(468,028 pairs) over a range of sampling rates. A
floor of 20 was imposed on sample sizes.
As reported in Figure 8, the large experiment con-
firms once again that proposed method, a?MLE , is
considerably better than the margin-free baseline (by
713
0.001 0.01 0.1 10
0.2
0.4
N
or
m
al
iz
ed
 M
SE
0.
5
MF
MLE,a
MLE
MLE+S
IND
THIS ? HAVE
0.001 0.01 0.1 10
0.2
0.4
THIS ? HELP
0.001 0.01 0.1 10
0.2
0.4
N
or
m
al
iz
ed
 M
SE
0.
5
THIS ? PROGRAM
0.001 0.01 0.1 10
0.2
0.4 HAVE ? HELP
0.001 0.01 0.1 10
0.2
0.4
0.5
N
or
m
al
iz
ed
 M
SE
0.
5
Sampling rates
HAVE ? PROGRAM
IND
MF
MLE+S
MLE,a
MLE
0.001 0.01 0.1 10
0.2
0.4
0.6
Sampling rates
HELP ? PROGRAM
Figure 5: The proposed method, a?MLE outperforms
the margin-free baseline, a?MF , in terms of MSE
0.5
a .
The recommended approximation, a?MLE,a, is close
to a?MLE . Smoothing, a?MLE+S , is effective at low
sampling rates. All methods are better than assum-
ing independence (IND).
15% ? 30%). The recommended approximation,
a?MLE,a, is close to a?MLE . Smoothing, a?MLE+S
helps at low sampling rates.
6.3 Rank Retrieval: Top k Associated Pairs
We computed a gold standard similarity cosine rank-
ing of the 468,028 pairs using a 100% sample: cos =
a?
fxfy
. We then compared the gold standard to rank-
ings based on smaller samples. Figure 9(a) com-
pares the two lists in terms of agreement in the top k.
For 3 ? k ? 200, with a sampling rate of 0.005, the
agreement is consistently 70% or higher. Increasing
sampling rate, increases agreement.
The same comparisons are evaluated in terms of
precision and recall in Figure 9(b), by fixing the top
1% of the gold standard list but varying the top per-
centages of the sample list. Again, increasing sam-
pling rate, increases agreement.
0.001 0.01 0.1 10
0.1
0.2
Sampling rates
N
or
m
al
iz
ed
 s
ta
nd
ar
d 
er
ro
r
MLE
MLE+S
Theore.
HAVE ? PROGRAM
0.001 0.01 0.1 10
0.1
0.2
0.3
0.4
Sampling rates
MLE
MLE+S
Theore.
HELP ? PROGRAM
Figure 6: The theoretical and empirical variances
show remarkable agreement, in terms of SE(a?)a .
Smoothing reduces variances at low sampling rates.
0.001 0.01 0.1 10  
0.02
0.05
Sampling rates
N
or
m
al
iz
ed
 a
bs
ol
ut
e 
bi
as
HAVE ? PROGRAM
MF
MLE
MLE+S
0.001 0.01 0.1 10  
0.2
0.04
0.06
Sampling rates
HELP ? PROGRAM
MLE+S
MF
MLE
Figure 7: Biases in terms of |E(a?)?a|a . a?MLE is prac-
tically unbiased, unlike a?MF . Smoothing increases
bias slightly.
7 Conclusion
We proposed a novel sketch-based procedure for
constructing sample contingency tables. The
method bridges two popular choices: (A) sam-
pling over documents and (B) sampling over post-
ings. Well-understood maximum likelihood estima-
tion (MLE) techniques can be applied to sketches
(or to traditional samples) to estimate word associa-
tions. We derived an exact cubic solution, a?MLE , as
well as a quadratic approximation, a?MLE,a. The ap-
proximation is recommended because it is close to
the exact solution, and easy to compute.
The proposed MLE methods were compared em-
pirically and theoretically to a margin-free (MF)
baseline, finding large improvements. When we
know the margins, we ought to use them.
Sample-based methods (MLE & MF) are often
better than sample-free methods. Associations are
often estimated without samples. It is popular to
assume independence: (Garcia-Molina et al, 2002,
Chapter 16.4), i.e., a? ? fxfyD . Independence led to
large errors in our experiments.
Not unsurprisingly, there is a trade-off between
computational work (space and time) and statistical
714
0.001 0.01 0.1 10
0.2
0.4
0.6
Sampling rates
R
el
at
iv
e 
av
g.
 a
bs
. e
rro
r IND
MLE+S
MLE
MF
MLE,a
Figure 8: We report the (normalized) mean absolute
errors (divided by the mean co-occurrences, 188).
All curves are averaged over three permutations.
The proposed MLE and the recommended approxi-
mation are very close and both are significantly bet-
ter than the margin-free (MF) baseline. Smoothing,
a?MLE+S , helps at low sampling rates. All estima-
tors do better than assuming independence.
accuracy (variance or errors); reducing the sampling
rate saves work, but costs accuracy. We derived
formulas for variance, showing precisely how accu-
racy depends on sampling rate. Sampling methods
become more and more important with larger and
larger collections. At Web scale, sampling rates as
low as 10?4 may suffice for ?ordinary? words.
We have recently generalized the sampling algo-
rithm and estimation method to multi-way associa-
tions; see (Li and Church, 2005).
References
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. In Proceedings
of the Seventh International World Wide Web Confer-
ence, pages 107?117, Brisbane, Australia.
A. Broder. 1997. On the resemblance and containment
of documents. In Proceedings of the Compression and
Complexity of Sequences, pages 21?29, Positano, Italy.
M. S. Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proceedings of the thiry-
fourth annual ACM symposium on Theory of comput-
ing, pages 380?388, Montreal, Quebec, Canada.
K. Church and P. Hanks. 1991. Word association norms,
mutual information and lexicography. Computational
Linguistics, 16(1):22?29.
S. Deerwester, S. T. Dumais, G. W. Furnas, and T. K.
Landauer. 1999. Indexing by latent semantic analy-
3 10 100 2000  
20
40
60
80
100
Top
Pe
rc
en
ta
ge
 o
f a
gr
ee
m
en
t ( 
% 
)
0.5
0.005
(a)
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
Recall
Pr
ec
is
io
n
Top 1 %
0.005 0.01
0.03
0.02
0.5
(b)
Figure 9: (a): Percentage of agreements in the gold
standard and reconstructed (from samples) top 3 to
200 list. (b):Precision-recall curves in retrieving the
top 1% gold standard pairs, at different sampling
rates. For example, 60% recall and 70% precision
is achieved at sampling rate = 0.02.
sis. Journal of the American Society for Information
Science, 41(6):391?407.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
H. Garcia-Molina, J. D. Ullman, and J. D. Widom. 2002.
Database Systems: the Complete Book. Prentice Hall,
New York, NY.
A. S. Hornby, editor. 1989. Oxford Advanced Learner?s
Dictionary. Oxford University Press, Oxford, UK.
E. L. Lehmann and G. Casella. 1998. Theory of Point
Estimation. Springer, New York, NY, second edition.
P. Li and K. W. Church. 2005. Using sketches to esti-
mate two-way and multi-way associations. Technical
report, Microsoft Research, Redmond, WA.
C. D. Manning and H. Schutze. 1999. Foundations of
Statistical Natural Language Processing. The MIT
Press, Cambridge, MA.
R. C. Moore. 2004. On log-likelihood-ratios and the
significance of rare events. In Proceedings of EMNLP
2004, pages 333?340, Barcelona, Spain.
D. Ravichandran, P. Pantel, and E. Hovy. 2005. Ran-
domized algorithms and NLP: Using locality sensitive
hash function for high speed noun clustering. In Pro-
ceedings of ACL, pages 622?629, Ann Arbor.
K. Siegrist. 1997. Finite Sampling Models,
http://www.ds.unifi.it/VL/VL EN/urn/index.html. Vir-
tual Laboratories in Probability and Statistics.
W. N. Venables and B. D. Ripley. 2002. Modern Ap-
plied Statistics with S. Springer-Verlag, New York,
NY, fourth edition.
715
A Sketch Algorithm for Estimating Two-Way
and Multi-Way Associations
Ping Li?
Stanford University
Kenneth W. Church??
Microsoft Corporation
We should not have to look at the entire corpus (e.g., the Web) to know if two (or more) words are
strongly associated or not. One can often obtain estimates of associations from a small sample.
We develop a sketch-based algorithm that constructs a contingency table for a sample. One can
estimate the contingency table for the entire population using straightforward scaling. However,
one can do better by taking advantage of the margins (also known as document frequencies). The
proposed method cuts the errors roughly in half over Broder?s sketches.
1. Introduction
We develop an algorithm for efficiently computing associations, for example, word
associations.1 Word associations (co-occurrences, or joint frequencies) have a wide range
of applications including: speech recognition, optical character recognition, and infor-
mation retrieval (IR) (Salton 1989; Church and Hanks 1991; Dunning 1993; Baeza-Yates
and Ribeiro-Neto 1999; Manning and Schutze 1999). The Know-It-All project computes
such associations at Web scale (Etzioni et al 2004). It is easy to compute a few association
scores for a small corpus, but more challenging to compute lots of scores for lots of data
(e.g., the Web), with billions of Web pages (D) and millions of word types.
Web search engines produce estimates of page hits, as illustrated in Tables 1?
3.2 Table 1 shows hits for two high frequency words, a and the, suggesting that the
total number of English documents is roughly D ? 1010. In addition to the two high-
frequency words, there are three low-frequency words selected from The New Oxford
Dictionary of English (Pearsall 1998). The low-frequency words demonstrate that there
are many hits, even for relatively rare words.
How many page hits do ?ordinary? words have? To address this question, we ran-
domly picked 15 pages from a learners? dictionary (Hornby 1989), and selected the first en-
try on each page. According to Google, there are 10 million pages/word (median value,
aggregated over the 15 words). To compute all two-way associations for the 57,100 en-
tries in this dictionary would probably be infeasible, let alne all multi-way associations.
? Department of Statistical Science, Cornell University, Ithaca, NY 14853. E-mail: pl332@cornell.edu.
?? Microsoft Research, Microsoft Corp., Redmond, WA 98052. E-mail: church@microsoft.com.
1 This paper considers boolean (0/1) data. See Li, Church, and Hastie (2006, 2007) for generalizations to
real-valued data (and lp distances).
2 All experiments with MSN.com and Google were conducted in August 2005.
Submission received: 6 December 2005; revised submission received: 5 September 2006; accepted for
publication: 7 December 2006.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 3
Table 1
Page hits for a few high-frequency words and a few low-frequency words (as of August 2005).
Query Hits (MSN.com) Hits (Google)
A 2,452,759,266 3,160,000,000
The 2,304,929,841 3,360,000,000
Kalevala 159,937 214,000
Griseofulvin 105,326 149,000
Saccade 38,202 147,000
Table 2
Estimates of page hits are not always consistent. Joint frequencies ought to decrease
monotonically as we add terms to the query, but estimates produced by current state-of-the-art
search engines sometimes violate this invariant.
Query Hits (MSN.com) Hits (Google)
America 150,731,182 393,000,000
America, China 15,240,116 66,000,000
America, China, Britain 235,111 6,090,000
America, China, Britain, Japan 154,444 23,300,000
Table 3
This table illustrates the usefulness of joint counts in query planning for databases. To minimize
intermediate writes, the optimal order of joins is: ((?Schwarzenegger? ? ?Austria?) ?
?Terminator?) ? ?Governor,? with 136,000 intermediate results. The standard practice starts
with the least frequent terms, namely, ((?Schwarzenegger? ? ?Terminator?) ? ?Governor?) ?
?Austria,? with 579,100 intermediate results.
Query Hits (Google)
Austria 88,200,000
Governor 37,300,000
One-way Schwarzenegger 4,030,000
Terminator 3,480,000
Governor, Schwarzenegger 1,220,000
Governor, Austria 708,000
Schwarzenegger, Terminator 504,000
Two-way Terminator, Austria 171,000
Governor, Terminator 132,000
Schwarzenegger, Austria 120,000
Governor, Schwarzenegger, Terminator 75,100
Three-way Governor, Schwarzenegger, Austria 46,100
Schwarzenegger, Terminator, Austria 16,000
Governor, Terminator, Austria 11,500
Four-way Governor, Schwarzenegger, Terminator, Austria 6,930
Estimates are often good enough. We should not have to look at every document
to determine whether two words are strongly associated or not. One could use the
estimated co-occurrences from a small sample to compute the test statistics, most com-
monly Pearson?s chi-squared test, the likelihood ratio test, Fisher?s exact test, cosine
similarity, or resemblance (Jaccard coefficient) (Dunning 1993; Manning and Schutze
1999; Agresti 2002; Moore 2004).
306
Li and Church Sketch for Estimating Associations
Sampling can make it possible to work in physical memory, avoiding disk accesses.
Brin and Page (1998) reported an inverted index of 37.2 GBs for 24 million pages. By
extrapolation, we should expect the size of the inverted indexes for current Web scale
to be 1.5 TBs/billion pages, probably too large for physical memory. A sample is more
manageable.
When estimating associations, it is desirable that the estimates be consistent. Joint
frequencies ought to decrease monotonically as we add terms to the query. Table 2
shows that estimates produced by current search engines are not always consistent.
1.1 The Data Matrix, Postings, and Contingency Tables
We assume a term-by-document matrix, A, with n rows (words) and D columns (doc-
uments). Because we consider boolean (0/1) data, the (i, j)th entry of A is 1 if word i
occurs in document j and 0 otherwise. Computing all pair-wise associations of A is a
matrix multiplication, AAT.
Because word distributions have long tails, the term-by-document matrix is highly
sparse. It is common practice to avoid materializing the zeros in A, by storing the matrix
in adjacency format, also known as postings, and an inverted index (Witten, Moffat, and
Bell 1999, Section 3.2). For each word W, the postings list, P, contains a sorted list of
document IDs, one for each document containing W.
Figure 1(a) shows a contingency table. The contingency table for words W1 and W2
can be expressed as intersections (and complements) of their postings P1 and P2 in the
obvious way:
a = |P1 ? P2|, b = |P1 ? ?P2|, c = |?P1 ? P2|, d = |?P1 ? ?P2| (1)
where ?P1 is short-hand for ?? P1, and ? = {1, 2, 3, . . . , D} is the set of all document
IDs. As shown in Figure 1(a), we denote the margins by f1 = a + b = |P1| and f2 = a +
c = |P2|.
For larger corpora, it is natural to introduce sampling. For example, we can ran-
domly sample Ds (out of D) documents, as illustrated in Figure 1(b). This sampling
scheme, which we call sampling over documents, is simple and easy to describe?but we
can do better, as we will see in the next subsection.
Figure 1
(a) A contingency table for word W1 and word W2. Cell a is the number of documents that
contain both W1 and W2, b is the number that contain W1 but not W2, c is the number that
contain W2 but not W1, and d is the number that contain neither. The margins, f1 = a + b and
f2 = a + c are known as document frequencies in IR. D = a + b + c + d is the total number
of documents in the collection. For consistency with the notation we use for multi-way
associations, a, b, c, and d are also denoted, in parentheses, by x1, x2, x3, and x4, respectively.
(b) A sample contingency table (as, bs, cs, ds), where the subscript s indicates the sample space.
The cells are also numbered as (s1, s2, s3, s4).
307
Computational Linguistics Volume 33, Number 3
1.2 Sampling Over Documents and Sampling Over Postings
Sampling over documents selects Ds documents randomly from a collection of D docu-
ments, as illustrated in Figure 1.
The task of computing associations is broken down into three subtasks:
1. Compute sample contingency table.
2. Estimate contingency table for population from sample.
3. Summarize contingency table to produce desired measure of association:
cosine, resemblance, mutual information, correlation, and so on.
Sampling over documents is simple and well understood. The estimation task is
straightforward if we ignore the margins. That is, we simply scale up the sample in
the obvious way: a?MF = as DDs . We refer to these estimates as the ?margin-free? baseline.
However, we can do better when we know the margins, f1 = a + b and f2 = a + c (called
document frequencies in IR), using a maximum likelihood estimator (MLE) with fixed
margin constraints.
Rare words can be a challenge for sampling over documents. In terms of the term-
by-document matrix A, sampling over documents randomly picks a fraction ( DsD ) of
columns from A. This is a serious drawback because A is highly sparse (as word
distributions have long tails) with a few high-frequency words and many low-frequency
words. The jointly non-zero entries in A are unlikely to be sampled unless the sampling
rate DsD is high. Moreover, the word sparsity differs drastically from one word to another;
it is thus desirable to have a sampling mechanism that can adapt to the data sparsity
with flexible sample sizes. One size does not fit all.
?Sampling over postings? is an interesting alternative to sampling over docu-
ments. Unfortunately, it doesn?t work out all that well either (at least using a sim-
ple straightforward implementation), but we present it here nevertheless, because it
provides a convenient segue between sampling over documents and our sketch-based
recommendation.
?Naive sampling over postings? obtains a random sample of size k1 from P1, de-
noted as Z1, and a random sample Z2 of size k2 from P2. Also, we denote aNs = |Z1 ? Z2|.
We then use aNs to infer a. For simplicity, assume k1 = k2 = k and f1 = f2 = f . It follows
that3 E
(
aNs
a
)
= k
2
f 2 . In other words, under naive sampling over postings, one could
estimate the associations by f
2
k2 a
N
s .
3 Suppose there are m defectives among N objects. We randomly pick k objects (without replacement) and
obtain x defectives. Then x follows a hypergeometric distribution, x ? HG(N, m, k). It is known that E(x) =
m
N k. In our setting, suppose we know that among Z1 (of size k1), there are a
Z1
s samples that belong to the
original intersection P1 ? P2. Similarly, suppose we know that there are a
Z2
s samples among Z2 (of size k2)
that belong to P1 ? P2. Then aNs = |Z1 ? Z2| ? HG(a, a
Z1
s , a
Z2
s ). Therefore E
(
aNs
)
= 1a a
Z1
s a
Z2
s . Because a
Z1
s
and aZ2s are both random, we should use conditional expectations: E
(
aNs
)
= E
(
E
(
aNs |a
Z1
s , a
Z2
s
))
=
E
(
1
a a
Z1
s a
Z2
s
)
= 1a E
(
aZ1s
)
E
(
aZ2s
)
. (Recall that Z1 and Z2 are independent.) Note that a
Z1
s ? HG( f1, a, k1)
and aZ2s ? HG( f2, a, k2), that is, E
(
aZ1s
)
= af1
k1 and E
(
aZ2s
)
= af2
k2. Therefore, E
(
aNs
)
= 1a af1
k1 af2 k2,
namely, E
(
aNs
a
)
= k1k2f1f2
.
308
Li and Church Sketch for Estimating Associations
Figure 2
The proposed sketch method (solid curve) produces larger counts (as) with less work (k).
With ?naive sampling over postings,? there is an undesirable quadratic: E
(
aNs
a
)
= k2
f 2
(dashed
curve), whereas with sketches, E
( as
a
)
? kf . These results were generated by simulation,
with f1 = f2 = f = 0.2D, D = 105 and a = 0.22, 0.38, 0.65, 0.80, 0.85f . There is only one
dashed curve across all values of a. There are different (but indistinguishable) solid curves
depending on a.
Of course, the quadratic relation, E
(
aNs
a
)
= k
2
f 2 , is undesirable; 1% effort returns only
0.01% useful information. Ideally, to maximize the signal, we?d like to see large counts
in a small sample, not small counts in a large sample. The crux is as, which tends to have
the smallest counts. We?d like as to be as large as possible, but we?d also like to do as
little work (k) as possible. The next subsection on sketches proposes an improvement,
where 1% effort returns roughly 1% useful information, as illustrated in Figure 2.
1.3 An Improvement Based on Sketches
A sketch is simply the front of the postings (after a random permutation). We find it
helpful, as an informal practical metaphor, to imagine a virtual machine architecture
where sketches (Broder 1997), the front of the postings, reside in physical memory, and
the rest of the postings are stored on disk. More formally, the sketch, K = MINk(?(P)),
contains the k smallest postings, after applying a random permutation ? to document
IDs, ? = {1, 2, 3, . . . , D}, to eliminate whatever structure there might be.
Given two words, W1 and W2, we have two sets of postings, P1 and P2, and two
sketches, K1 = MINk1 (?(P1)) and K2 = MINk2 (?(P2)). We construct a sample contin-
gency table from the two sketches. Let ?s = {1, 2, 3, . . . , Ds} be the sample space, where
Ds is set to min(max(K1), max(K2)). With this choice of Ds, all the document IDs in the
sample space,?s, can be assigned to the appropriate cell in the sample contingency table
without looking outside the sketch. One could use a smaller Ds, but doing so would
throw out data points unnecessarily.
The sample contingency table is constructed from K1 and K2 in O(k1 + k2) time,
using a straightforward linear pass over the two sketches:
as = |K1 ? K2 ? ?s| = |K1 ? K2| bs = |K1 ? ?K2 ? ?s|
(2)
cs = |?K1 ? K2 ? ?s| ds = |?K1 ? ?K2 ? ?s|
309
Computational Linguistics Volume 33, Number 3
The final step is an estimation task. The margin-free (MF) estimator recovers the
original contingency table by a simple scaling. For better accuracy, one could take
advantage of the margins by using a maximum likelihood estimator (MLE).
With ?sampling over documents,? it is convenient to express the sampling rate in
terms of Ds and D, whereas with sketches, it is convenient to express the sampling rate
in terms of k and f . The following two approximations allow us to flip back and forth
between the two views:
E
(
Ds
D
)
? min
(
k1
f1
, k2
f2
)
(3)
E
(
D
Ds
)
? max
(
f1
k1
,
f2
k2
)
(4)
In other words, using sketches with size k, the corresponding sample size Ds in
?sampling over documents? would be Ds ? Df k, where Df represents the data sparsity.
Because the estimation errors (variances) are inversely proportional to sample size,
we know the proposed algorithm improves ?sampling over documents? by a factor
proportional to the data sparsity.
1.4 Improving Estimates Using Margins
When we know the margins, we ought to use them. The basic idea is to maximize the
likelihood of the sample contingency table under margin constraints. In the pair-wise
case, we will show that the resultant maximum likelihood estimator is the solution to a
cubic equation, which has a remarkably accurate quadratic approximation.
The use of margins for estimating contingency tables was suggested in the 1940s
(Deming and Stephan 1940; Stephan 1942) for a census application. They developed
a straightforward iterative estimation method called iterative proportional scaling,
which was an approximation to the maximum likelihood estimator.
Computing margins is usually much easier than computing interactions. For a data
matrix A of n rows and D columns, computing all marginal l2 norms costs only O(nD),
whereas computing all pair-wise associations (or l2 distances) costs O(n2D). One could
compute the margins in a separate prepass over the data, without increasing the time
and space complexity, though we suggest computing the margins while applying the
random permutation ? to all the document IDs on all the postings.
1.5 An Example
Let?s start with conventional random sampling over documents, using a running exam-
ple in Figure 3. We choose a sample of Ds = 18 documents randomly out of a collection
of D = 36. After applying the random permutation, document IDs will be uniformly
random. Thus, we can construct the random sample by picking any Ds documents. For
convenience, we pick the first Ds. The sample contingency table is then constructed, as
illustrated in Figure 3.
The recommended procedure is illustrated in Figure 4. The two sketches, K1 and
K2, are highlighted in the large box. We find it convenient, as an informal practi-
cal metaphor, to think of the large box as physical memory. Thus, the sketches re-
side in physical memory, and the rest are paged out to disk. We choose Ds to be
min(max(K1), max(K2)) = min(18, 21) = 18, so that we can compute the sample contin-
310
Li and Church Sketch for Estimating Associations
Figure 3
In this example, the corpus contains D = 36 documents. The population is: ? = {1, 2, . . . , D}.
The sample space is ?s = {1, 2, . . . , Ds}, where Ds = 18. Circles denote documents containing
W1, and squares denote documents containing W2. The sample contingency table is: as =
|{4, 15}| = 2, bs = |{3, 7, 9, 10, 18}| = 5, cs = |{2, 5, 8}| = 3, ds = |{1, 6, 11, 12, 13, 14, 16, 17}| = 8.
Figure 4
This procedure, which we recommend, produces the same sample contingency table as in
Figure 3: as = 2, bs = 5, cs = 3, and ds = 8. The two sketches, K1 and K2 (larger shaded box),
reside in physical memory, and the rest of the postings are paged out to disk. K1 contains
of the first k1 = 7 document IDs in P1 and K2 contains of the first k2 = 7 IDs in P2. We
assume P1 and P2 are already permuted, otherwise we should write ?(P1) and ?(P2) instead.
Ds = min(max(K1), max(K2))= min(18, 21) = 18. The sample contingency table is computed
from the sketches (large box) in time k1 + k2, but documents exceeding Ds are excluded from ?s
(small box), because we can?t tell if they are in the intersection or not, without looking outside
the sketch. As it turns out, 19 is in the intersection and 21 is not.
gency table for?s = {1, 2, 3, . . . , Ds} in physical memory in time O (k1 + k2) from K1 and
K2. In this example, documents 19 and 21 (highlighted in the smaller box) are excluded
from ?s. It turns out that 19 is part of the intersection, and 21 is not, but we would have
to look outside the sketches (and suffer a page fault) to determine that. The resulting
sample contingency table is the same as in Figure 3:
as = |{4, 15}| = 2 bs = |K1 ? ?s| ? as = 7 ? 2 = 5
cs = |K2 ? ?s| ? as = 5 ? 2 = 3 ds = Ds ? (as + bs + cs) = 8
1.6 A Five-Word Example
Figure 5 shows an example with more than two words. There are D = 15 documents in
the collection. We generate a random permutation ? as shown in Figure 5(b). For every
ID in postings Pi in Figure 5(a), we apply the random permutation ?, but we only store
the ki smallest IDs as a sketch Ki, that is, Ki = MINki (?(Pi)). In this example, we choose
k1 = 4, k2 = 4, k3 = 4, k4 = 3, k5 = 6. The sketches are stored in Figure 5(c). In addition,
because ?(Pi) operates on every ID in Pi, we know the total number of non-zeros in Pi,
denoted by fi = |Pi|.
The estimation procedure is straightforward if we ignore the margins. For example,
suppose we need to estimate the number of documents containing the first two words.
In other words, we need to estimate the inner product between P1 and P2, denoted
by a(1,2). (We have to use the additional subscript (1,2) because we have more than
311
Computational Linguistics Volume 33, Number 3
Figure 5
The original postings sets are given in (a). There are D = 15 documents in the collection. We
generate a random permutation ? as shown in (b). We apply ? to the postings Pi and store the
sketch Ki = MINki (?(Pi)). For example, ?(P1) = {11, 13, 1, 12, 15, 6, 8}. We choose k1 = 4; and
hence the four smallest IDs in ?(P1) are K1 = {1, 6, 8, 11}. We choose k2 = 4, k3 = 4, k4 = 3,
and k5 = 6.
just two words in the vocabulary.) We calculate, from sketches K1 and K2, the sample
inner product as,(1,2) = |{6}| = 1, and the corresponding corpus sample size, denoted
by Ds,(1,2) = min(max(K1), max(K2)) = min(11, 12) = 11. Therefore, the ?margin-free?
estimate of a(1,2) is simply as,(1,2) DDs,(1,2) = 1
15
11 = 1.4.
This estimate can be compared to the ?truth,? which is obtained from the complete
postings list, as opposed to the sketch. In this case, P1 and P2 have 4 documents in
common. And therefore, the estimation error is 4 ? 1.4 or 2.6 documents.
Similarly, for P1 and P5, Ds,(1,5) = min(11, 6) = 6, as,(1,5) = 2. Hence, the ?margin-
free? estimate of a(1,5) is simply 2 156 = 5.0. In this case, the estimate matches the ?truth?
perfectly.
The procedure can be easily extended to more than two rows. Suppose we
would like to estimate the three-way inner product (three-way joins) among P1,
P4, and P5, denoted by a(1,4,5). We calculate the three-way sample inner product
from K1, K4, and K5, as,(1,4,5) = |{6}| = 1, and the corpus sample size Ds,(1,4,5) =
min(max(K1), max(K4), max(K5)) = min(11, 12, 6) = 6. Then the ?margin-free? estimate
of a(1,4,5) is 1 156 = 2.5.
Of course, we can improve these estimates by taking advantage of the margins.
2. Applications
There is a large literature on sketching techniques (e.g., Alon, Matias, and Szegedy 1996;
Broder 1997; Vempala 2004). Such techniques have applications in information retrieval,
databases, and data mining (Broder et al 1997; Haveliwala, Gionis, and Indyk 2000;
Haveliwala et al 2002).
Broder?s sketches (Broder 1997) were originally introduced to detect duplicate
documents in Web crawls. Many URLs point to the same (or nearly the same) HTML
blobs. Approximate answers are often good enough. We don?t need to find all such
pairs, but it is handy to find many of them, without spending more than it is worth on
computational resources.
In IR applications, physical memory is often a bottleneck, because the Web collec-
tion is too large for memory, but we want to minimize seeking data in the disk as the
query response time is critical (Brin and Page 1998). As a space saving device, dimension
reduction techniques use a compact representation to produce approximate answers in
physical memory.
312
Li and Church Sketch for Estimating Associations
Section 1 mentioned page hit estimation. If we have a two-word query, we?d like
to know how many pages mention both words. We assume that pre-computing and
storing page hits is infeasible, at least not for infrequent pairs of words (and multi-word
sequences).
It is customary in information retrieval to start with a large boolean term-by-
document matrix. The boolean values indicate the presence or absence of a term in a
document. We assume that these matrices are too large to store in physical memory.
Depending on the specific applications, we can construct an inverted index and store
sketches either for terms (to estimate word association) or for documents (to estimate
document similarity).
2.1 Association Rule Mining
?Market-basket? analysis and association rules (Agrawal, Imielinski, and Swami 1993;
Agrawal and Srikant 1994; Agrawal et al 1996; Hastie, Tibshirani, and Friedman
2001, Chapter 14.2) are useful tools for mining commercial databases. Commercial
databases tend to be large and sparse (Aggarwal and Wolf 1999; Strehl and Ghosh
2000). Various sampling algorithms have been proposed (Toivonen 1996; Chen, Haas,
and Scheuermann 2002). The proposed algorithm scales better than traditional ran-
dom sampling (i.e., a fixed sample of columns of the data matrix) for reasons men-
tioned earlier. In addition, the proposed algorithm makes it possible to estimate
association rules on-line, which may have some advantage in certain applications
(Hidber 1999).
2.2 All Pair-Wise Associations (Distances)
In many applications, including distance-based classification or clustering and bi-gram
language modeling (Church and Hanks 1991), we need to compute all pair-wise asso-
ciations (or distances). Given a data matrix A of n rows and D columns, brute force
computation of AAT would cost O(n2D), or more efficiently, O(n2 f? ), where f? is the
average number of non-zeros among all rows of A. Brute force could be very time-
consuming. In addition, when the data matrix is too large to fit in the physical memory,
the computation may become especially inefficient.
Using our proposed algorithm, the cost of computing AAT can be reduced to
O(nf? ) + O(n2k?), where k? is the average sketch size. It costs O(nf? ) for constructing
sketches and O(n2k?) for computing all pair-wise associations. The savings would be
significant when k? 
 f? . Note that AAT is called ?Gram Matrix? in machine learning; and
various algorithms have been proposed for speeding up the computation (e.g., Drineas
and Mahoney 2005).
Ravichandran, Pantel, and Hovy (2005) computed pair-wise word associations
(boolean data) among n ? 0.6 million nouns in D ? 70 million Web pages, using random
projections. We have discovered that in boolean data, our method exhibits (much)
smaller errors (variances); but we will present the detail in other papers (Li, Church,
and Hastie 2006, 2007).
For applications which are mostly interested in finding the strongly associated
pairs, the n2 might appear to be a show stopper. But actually, in a practical application,
we implemented an inverted index on top of the sketches, which made it possible to
find many of the most interesting associations quickly.
313
Computational Linguistics Volume 33, Number 3
2.3 Database Query Optimization
In databases, an important task is to determine the order of joins, which has a
large impact on the system performance (Garcia-Molina, Ullman, and Widom 2002,
Chapter 16). Based on the estimates of two-way, three-way, and even higher-order join
sizes, query optimizers construct a plan to minimize a cost function (e.g., intermediate
writes). Efficiency is critical as we certainly do not want to spend more time optimizing
the plan than executing it.
We use an example (called Governator) to illustrate that estimates of two-way and
multi-way association can help the query optimizer.
Table 3 shows estimates of hits for four words and their two-way, three-way, and
four-way combinations. Suppose the optimizer wants to construct a plan for the query:
?Governor, Schwarzenegger, Terminator, Austria.? The standard solution starts with the
least frequent terms: ((?Schwarzenegger? ? ?Terminator?) ? ?Governor?) ? ?Austria.?
That plan generates 579,100 intermediate writes after the first and second joins. An im-
provement would be ((?Schwarzenegger? ? ?Austria?) ? ?Terminator?) ? ?Governor,?
reducing the 579,100 down to 136,000.
3. Outline of Two-Way Association Results
To approximate the associations between words W1 and W2, we work with sketches K1
and K2. We first determine Ds = min(max(K1), max(K2)) and then construct the sample
contingency table on ?s = {1, 2, . . . , Ds}. The contingency table for the entire document
collection,? = {1, 2, . . . , D}, is estimated using a maximum likelihood estimator (MLE):
a?MLE = argmax
a
Pr (as, bs, cs, ds|Ds; a) (5)
Section 5 will show that a?MLE is the solution to a cubic equation:
f1 ? a + 1 ? bs
f1 ? a + 1
f2 ? a + 1 ? cs
f2 ? a + 1
D ? f1 ? f2 + a
D ? f1 ? f2 + a ? ds
a
a ? as = 1 (6)
Instead of solving a cubic equation, we recommend a convenient and accurate quadratic
approximation:
a?MLE,a =
f1 (2as + cs) + f2 (2as + bs) ?
?
(
f1 (2as + cs) ? f2 (2as + bs)
)2
+ 4f1 f2bscs
2 (2as + bs + cs)
(7)
We will compare the proposed MLE to two baselines: the independence baseline,
a?IND, and the margin-free baseline, a?MF:
a?IND =
f1f2
D a?MF = as
D
Ds
(8)
The margin-free baseline has smaller errors than the independence baseline, but we
can do even better if we know the margins, as is common in practice.
As expected, computational work and statistical accuracy (variance or errors) de-
pend on sampling rate. The larger the sample, the better the estimate, but the more
work we have to do.
314
Li and Church Sketch for Estimating Associations
These results are demonstrated both empirically and theoretically. In our field, it is
customary to end with a large empirical evaluation. But there are always lingering ques-
tions. Do the results generalize to other collections with more documents or different
documents? This paper attempts to put such questions to rest by deriving closed-form
expressions for the variances.
Var (a?MLE) ?
E
(
D
Ds
)
? 1
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
, (9)
?
max
(
f1
k1
, f2k2
)
? 1
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
. (10)
Var (a?MF) =
E
(
D
Ds
)
? 1
1
a + 1D?a
?
max
(
f1
k1
, f2k2
)
? 1
1
a + 1D?a
. (11)
These formulas establish the superiority of the proposed method over the alterna-
tives, not just for a particular data set, but more generally. These formulas will also be
used to determine stopping rules. How many samples do we need? We will use such
an argument to suggest that a sampling rate of 10?3 may be sufficient for certain Web
applications.
The proposed method generalizes naturally to multi-way associations, as presented
in Section 6. Section 7 describes Broder?s sketches, which were designed for estimating
resemblance, a particular association statistic. It will be shown, both theoretically and
empirically, that our proposed method reduces the mean square error (MSE) by about
50%. In other words, the proposed method achieves the same accuracy with about half
the sample size (work).
4. Evaluation of Two-Way Associations
We evaluated our two-way association sampling/estimation algorithm with a chunk
of Web crawls (D = 216) produced by the crawler for MSN.com. We collected two sets
of English words which we will refer to as the small data set and the large data set.
The small data set contains just four high frequency words: THIS, HAVE, HELP and
PROGRAM (see Table 4), whereas the large data set contains 968 words (i.e., 468,028
pairs). The large data set was constructed by taking a random sample of English words
that appeared in at least 20 documents in the collection. The histograms of the margins
and co-occurrences have long tails, as expected (see Figure 6).
For the small data set, we applied 105 independent random permutations to the
D = 216 document IDs, ? = {1, 2, . . . , D}. High-frequency words were selected so we
could study a large range of sampling rates ( kf ), from 0.002 to 0.95. A pair of sketches
was constructed for each of the 6 pairs of words in Table 4, each of the 105 permutations
and each sampling rate. The sketches were then used to compute a sample contingency
table, leading to an estimate of co-occurrence, a?. An error was computed by comparing
this estimate, a?, to the appropriate gold standard value for a in Table 4. Mean square
errors (MSE = E(a? ? a)2) and other statistics were computed by aggregating over the 105
315
Computational Linguistics Volume 33, Number 3
Table 4
Small dataset: co-occurrences and margins for the population. The task is to estimate these
values, which will be referred to as the gold standard, from a sample.
Case # Words Co-occurrence (a) Margin ( f1) Margin ( f2)
Case 2-1 THIS, HAVE 13,517 27,633 17,369
Case 2-2 THIS, HELP 7,221 27,633 10,791
Case 2-3 THIS, PROGRAM 3,682 27,633 5,327
Case 2-4 HAVE, HELP 5,781 17,369 10,791
Case 2-5 HAVE, PROGRAM 3,029 17,369 5,327
Case 2-6 HELP, PROGRAM 1,949 17,369 5,327
Monte Carlo trials. In this way, the small data set experiment made it possible to verify
our theoretical results, including the approximations in the variance formulas.
The larger experiment contains many words with a large range of frequencies;
and hence the experiment was repeated just six times (i.e., six different permutations).
With such a large range of frequencies and sampling rates, there is a danger that some
samples would be too small, especially for very rare words and very low sampling rates.
A floor was imposed to make sure that every sample contains at least 20 documents.
4.1 Results from Large Monte Carlo Experiment
Figure 7 shows that the proposed methods (solid lines) are better than the baselines
(dashed lines), in terms of MSE, estimated by the large Monte Carlo experiment over the
small data set, as described herein. Note that errors generally decrease with sampling
rate, as one would expect, at least for the methods that take advantage of the sample.
The independence baseline (a?IND), which does not take advantage of the sample, has
very large errors. The sample is a very useful source of information; even a small sample
is much better than no sample.
The recommended quadratic approximation, a?MLE,a, is remarkably close to the ex-
act MLE solution. Both of the proposed methods, a?MLE,a and a?MLE (solid lines), have
Figure 6
Large data set: histograms of document frequencies, df (left), and co-occurrences, a (right). Left:
max document frequency df = 42,564, median = 1135, mean = 2135, standard deviation = 3628.
Right: max co-occurrence a = 33,045, mean = 188, median = 74, standard deviation = 459.
316
Li and Church Sketch for Estimating Associations
much smaller MSE than the margin-free baseline a?MF (dashed lines), especially at low
sampling rates. When we know the margins, we ought to use them.
Note that MSE can be decomposed into variance and bias: MSE(a?) = E (a? ? a)2 = Var (a?)
+Bias2 (a?). If a? is unbiased, MSE(a?) = Var (a?) = SE2 (a?), where SE is called ?standard error.?
4.1.1 Margin Constraints Improve Smoothing. Though not a major emphasis of this paper,
Figure 8 shows that smoothing is effective at low sampling rates, but only for those
methods that take advantage of the margin constraints (solid lines as opposed to dashed
lines). Figure 8 compares smoothed estimates (a?MLE, a?MLE,a, and a?MF) with their un-
smoothed counterparts. The y-axis reports percentage improvement of the MSE due
to smoothing. Smoothing helps the proposed methods (solid lines) for all six word
pairs, and hurts the baseline methods (dashed lines), for most of the six word pairs. We
believe margin constraints keep the smoother from wandering too far astray; without
margin constraints, smoothing can easily do more harm than good, especially when the
smoother isn?t very good. In this experiment, we used the simple ?add-one? smoother
that replaces as, bs, cs, and ds with as + 1, bs + 1, cs + 1, and ds + 1, respectively. We could
have used a more sophisticated smoother (e.g., Good?Turing), but if we had done so,
it would have been harder to see how the margin constraints keep the smoother from
wandering too far astray.
4.1.2 Monte Carlo Verification of Variance Formula. How accurate is the ap-
proximation of the variance in Equations (9) and (11)? Figure 9 shows that the
Monte Carlo simulation is remarkably close to the theoretical formula (9). Formula
(11) is the same as (9), except that E
(
D
Ds
)
is replaced with the approximation
Figure 7
The proposed estimator, a?MLE, outperforms the margin-free baseline, a?MF, in terms of
?
MSE
a .
The quadratic approximation, a?MLE,a, is close to a?MLE. All methods are better than assuming
independence (IND).
317
Computational Linguistics Volume 33, Number 3
Figure 8
Smoothing improves the proposed MLE estimators but hurts the margin-free estimator in most
cases. The vertical axis is the percentage of relative improvement in
?
MSE of each smoothed
estimator with respect to its un-smoothed version.
Figure 9
Normalized standard error, SE(a?)a , for the MLE. The theoretical variance formula (9) fits the
simulation results so well that the curves are indistinguishable. Also, smoothing is effective in
reducing variance, especially at low sampling rates.
max
(
f1
k1
, f2k2
)
. Theoretically, we expect max
(
f1
k1
, f2k2
)
? E
(
D
Ds
)
. Figure 10 verifies the
inequality, and shows that the inequality is not too far from an equality. We will
use (11) instead of (9), because the differences are not too large, and (11) is more
convenient.
4.1.3 Monte Carlo Estimate of Bias. Finally, we also compare the biases in Figure 11 for
Case 2-5 and Case 2-6. The figure shows that the MLE estimator is essentially unbiased.
318
Li and Church Sketch for Estimating Associations
Figure 10
For all 6 cases, the ratios max
(
f1
k1
, f2k2
)
/
E
(
D
Ds
)
are close to 1, and the differences roughly
monotonically decrease with increasing sampling rates. When the sampling rates ? 0.005
(roughly the sketch sizes ? 20), max
(
f1
k1
, f2k2
)
is an accurate approximation of E
(
D
Ds
)
.
Figure 11
Biases in terms of |E(a?)?a|a . a?MLE is practically unbiased. Smoothing increases bias slightly.
4.2 Results from Large Data Set Experiment
In Figure 12, the large data set experiment confirms the findings of the large Monte
Carlo experiment: The proposed MLE method is better than the margin-free and inde-
pendence baselines. The recommended quadratic approximation, a?MLE,a, is close to the
exact solution, a?MLE.
4.3 Rank Retrieval by Cosine
We are often interested in finding top ranking pairs according to some measure of sim-
ilarity such as cosine. Performance improves with sampling rate for this task (as well
as almost any other task; there is no data like more data), but nevertheless, Figure 13
shows that we can find many of the top ranking pairs, even at low sampling rates.
Note that the estimate of cosine, a?
f1f2
, depends solely on the estimate of a, because
we know the margins, f1 and f2. If we sort word pairs by their cosines, using estimates
of a based on a small sample, the rankings will hopefully be close to what we would
319
Computational Linguistics Volume 33, Number 3
Figure 12
(a) The proposed MLE methods (solid lines) have smaller errors than the baselines (dashed
lines). We report the mean absolute errors (normalized by the mean co-occurrences, 188). All
curves are averaged over six permutations. The two solid lines, the proposed MLE and the
recommended quadratic approximation, are close to one another. Both are well below the
margin-free (MF) baseline and the independence (IND) baseline. (b) Percentage of improvement
due to smoothing. Smoothing helps MLE, but hurts MF.
Figure 13
We can find many of the most obvious associations with very little work. Two sets of cosine
scores were computed for the 468,028 pairs in the large dataset experiment. The gold standard
scores were computed over the entire dataset, whereas sample scores were computed over a
sample of the data set. The plots show the percentage of agreement between these two lists, as a
function of S. As expected, agreement rates are high (? 100%) at high sampling rates (0.5). But it
is reassuring that agreement rates remain pretty high (? 70%) even when we crank the sampling
rate way down (0.003).
obtain if we used the entire data set. This section will compare the rankings based on a
small sample to a gold standard, the rankings based on the entire data set.
How should we evaluate rankings? We follow the suggestion in Ravichandran,
Pantel, and Hovy (2005) of reporting the percentage of agreements in the top-S.
That is, we compare the top-S pairs based on a sample with the top-S pairs based
on the entire data set. We report the intersection of the two lists, normalized by S.
Figure 13(a) emphasizes high precision region (3 ? S ? 200), whereas Figure 13(b)
emphasizes higher recall, extending S to cover all 468,028 pairs in the large dataset
experiment. Of course, agreement rates are high at high sampling rates. For example, we
have nearly ? 100% agreement at a sampling rate of 0.5. It is reassuring that agreement
rates remain fairly high (? 70%), even when we push the sampling rate way down
320
Li and Church Sketch for Estimating Associations
(0.003). In other words, we can find many of the most obvious associations with very
little work.
The same comparisons can be evaluated in terms of precision and recall, by fix-
ing the top-LG gold standard list but varying the length of the sample list LS. More
precisely, recall = relevant/LG, and precision = relevant/LS, where ?relevant? means
the retrieved pairs in the gold standard list. Figure 14 gives a graphical representation
of this evaluation scheme, using notation in Manning and Schutze (1999), Chapter 8.1.
Figure 15 presents the precision?recall curves for LG = 1%L and 10%L, where L =
468, 028. For each LG, there is one precision?recall curve corresponding to each sampling
rate. All curves indicate the precision?recall trade-off and that the only way to improve
both precision and recall simultaneously is to increase the sampling rate.
4.4 Summary
To summarize the main results of the large and small data set experiments, we found
that the proposed MLE (and the recommended quadratic approximation) have smaller
Figure 14
Definitions of recall and precision. L = total number of pairs. LG = number of pairs from the top
of the gold standard similarity list. LS = number of pairs from the top of the reconstructed
similarity list.
Figure 15
Precision?recall curves in retrieving the top 1% and top 10% gold standard pairs, at different
sampling rates from 0.003 to 0.5. Note that the precision is always larger than LGL .
321
Computational Linguistics Volume 33, Number 3
errors than the two baselines (the MF baseline and the independence (IND) base-
line). Margin constraints improve smoothing, because the margin constraints keep the
smoother from wandering too far astray. Monte Carlo simulations verified the variance
formulas (9) and (11), and showed that the proposed MLE method is essentially un-
biased. The ranking experiment showed that we can find many of the most obvious
associations with very little work.
5. The Maximum Likelihood Estimator (MLE)
Section 4 evaluated the proposed method empirically; this section will explore the sta-
tistical theory behind the method. The task is to estimate the contingency table (a, b, c, d)
from the sample contingency table (as, bs, cs, ds), the margins, and D.
We can factor the (full) likelihood (probability mass function, PMF) Pr(as, bs, cs, ds; a)
into
Pr(as, bs, cs, ds; a) = Pr(as, bs, cs, ds|Ds; a) ? Pr(Ds; a) (12)
We seek the a that maximizes the partial likelihood Pr(as, bs, cs, ds|Ds; a), that is,
a?MLE = argmax
a
Pr (as, bs, cs, ds|Ds; a) = argmax
a
log Pr (as, bs, cs, ds|Ds; a) (13)
Pr(as, bs, cs, ds|Ds; a) is just the PMF of a two-way sample contingency table. That is
relatively straightforward, but Pr(Ds; a) is difficult. As illustrated in Figure 16, there is no
strong dependency of Ds on a, and therefore, we can focus on the easy part.
Before we delve into maximizing Pr(as, bs, cs, ds|Ds; a) under margin constraints, we
will first consider two simplifications, which lead to two baseline estimators. The inde-
pendence baseline does not use any samples, whereas the margin-free baseline does not
take advantage of the margins.
Figure 16
This experiment shows that E(Ds) is not sensitive to a. D = 2 ? 107, f1 = D/20, f2 = f1/2.
The different curves correspond to a = 0, 0.05, 0.2, 0.5, and 0.9 f2. These curves are almost
indistinguishable except at very low sampling rates. Note that, at sampling rate = 10?5,
the sample size k2 = 5 only.
322
Li and Church Sketch for Estimating Associations
5.1 The Independence Baseline
Independence assumptions are often made in databases (Garcia-Molina, Ullman, and
Widom 2002, Chapter 16.4) and NLP (Manning and Schutze 1999, Chapter 13.3). When
two words W1 and W2 are independent, the size of intersections, a, follows a hypergeo-
metric distribution,
Pr(a) =
(
f1
a
)(
D ? f1
f2 ? a
)/(
D
f2
)
, (14)
where
(
n
m
)
= n!m!(n?m)! . This distribution suggests an estimator
a?IND = E(a) =
f1 f2
D . (15)
Note that (14) is also a common null-hypothesis distribution in testing the indepen-
dence of a two-way contingency table, that is, the so-called Fisher?s exact test (Agresti
2002, Section 3.5.1).
5.2 The Margin-Free Baseline
Conditional on Ds, the sample contingency table (as, bs, cs, ds) follows the multivariate
hypergeometric distribution with moments4
E(as|Ds) =
Ds
D a, E(bs|Ds) =
Ds
D b, E(cs|Ds) =
Ds
D c, E(ds|Ds) =
Ds
D d,
Var(as|Ds) = Ds aD
(
1 ? aD
) D ? Ds
D ? 1 (16)
where the term D?DsD?1 ? 1 ?
Ds
D , is known as the ?finite population correction factor.?
An unbiased estimator and its variance would be
a?MF = DDs
as, Var(a?MF|Ds) = D
2
D2s
Var(as|Ds) = DDs
1
1
a + 1D?a
D ? Ds
D ? 1 . (17)
We refer to this estimator as ?margin-free? because it does not take advantage of the
margins.
The multivariate hypergeometric distribution can be simplified to a multinomial
assuming ?sample-with-replacement,? which is often a good approximation when DsD
is small. According to the multinomial model, an estimator and its variance would be:
a?MF,r = DDs
as, Var(a?MF,r|Ds) = DDs
1
1
a + 1D?a
(18)
That is, for the margin-free model, the ?sample-with-replacement? simplification still
results in the same estimator but slightly overestimates the variance.
4 http://www.ds.unifi.it/VL/VL EN/urn/urn4.html.
323
Computational Linguistics Volume 33, Number 3
Note that these expectations in (16) hold both when the margins are known, as well
as when they are not known, because the samples (as, bs, cs, ds) are obtained randomly
without consulting the margins. Of course, when we know the margins, we can do
better than when we don?t.
5.3 The Exact MLE with Margin Constraints
Considering the margin constraints, the partial likelihood Pr (as, bs, cs, ds|Ds; a) can be
expressed as a function of a single unknown parameter, a:
Pr (as, bs, cs, ds|Ds; a) =
(
a
as
)( b
bs
)(
c
cs
)( d
ds
)
( a+b+c+d
as+bs+cs+ds
)
=
(
a
as
)(f1?a
bs
)(f2?a
cs
)(D?f1?f2+a
ds
)
(
D
Ds
)
? a!
(a ? as)!
? ( f1 ? a)!
( f1 ? a ? bs)!
? ( f2 ? a)!
( f2 ? a ? cs)!
? (D ? f1 ? f2 + a)!
(D ? f1 ? f2 + a ? ds)!
(19)
=
as?1
?
i=0
(a ? i) ?
bs?1
?
i=0
( f1 ? a ? i) ?
cs?1
?
i=0
( f2 ? a ? i) ?
ds?1
?
i=0
(D ? f1 ? f2 + a ? i)
where the multiplicative terms not mentioning a are discarded, because they do not
contribute to the MLE.
Let a?MLE be the value of a that maximizes the partial likelihood (19), or equivalently,
maximizes the log likelihood, log Pr (as, bs, cs, ds|Ds; a):
as?1
?
i=0
log(a ? i) +
bs?1
?
i=0
log
(
f1 ? a ? i
)
+
cs?1
?
i=0
log
(
f2 ? a ? i
)
+
ds?1
?
i=0
log
(
D ? f1 ? f2 + a ? i
)
whose first derivative, ? log Pr(as,bs,cs,ds|Ds;a)?a , is
as?1
?
i=0
1
a ? i ?
bs?1
?
i=0
1
f1 ? a ? i
?
cs?1
?
i=0
1
f2 ? a ? i
+
ds?1
?
i=0
1
D ? f1 ? f2 + a ? i
(20)
Because the second derivative, ?
2 log Pr(as,bs,cs,ds|Ds;a)
?a2 ,
?
as?1
?
i=0
1
(a ? i)2 ?
bs?1
?
i=0
1
( f1 ? a ? i)2
?
cs?1
?
i=0
1
( f2 ? a ? i)2
?
ds?1
?
i=0
1
(D ? f1 ? f2 + a ? i)2
is negative, the log likelihood function is concave, and therefore, there is a unique
maximum. One could solve (20) for ? log Pr(as,bs,cs,ds|Ds;a)?a = 0 numerically, but it turns out
there is a more direct solution using the updating formula from (19):
Pr (as, bs, cs, ds|Ds; a) = Pr (as, bs, cs, ds|Ds; a ? 1) ? g(a)
324
Li and Church Sketch for Estimating Associations
Because we know that the MLE exists and is unique, it suffices to find the a such that
g(a) = 1,
g(a) = aa ? as
f1 ? a + 1 ? bs
f1 ? a + 1
f2 ? a + 1 ? cs
f2 ? a + 1
D ? f1 ? f2 + a
D ? f1 ? f2 + a ? ds
= 1 (21)
which is cubic in a (because the fourth term vanishes).
We recommend a straightforward numerical procedure for solving g(a) = 1. Note
that g(a) = 1 is equivalent to q(a) = log g(a) = 0. The first derivative of q(a) is
q?(a) =
(
1
f1 ? a + 1
? 1
f1 ? a + 1 ? bs
)
+
(
1
f2 ? a + 1
? 1
f2 ? a + 1 ? cs
)
(22)
+
(
1
D ? f1 ? f2 + a
? 1
D ? f1 ? f2 + a ? ds
)
+
(
1
a ?
1
a ? as
)
We can solve for q(a) = 0 iteratively using Newton?s method: a(new) = a(old) ? q(a
(old) )
q?(a(old) ) . See
Appendix 1 for a C code implementation.
5.4 The ?Sample-with-Replacement? Simplification
Under the ?sample-with-replacement? assumption, the likelihood function is slightly
simpler:
Pr(as, bs, cs, ds|Ds; a, r) =
(
Ds
as, bs, cs, ds
)
(
a
D
)as ( b
D
)bs ( c
D
)cs ( d
D
)ds
? aas ( f1 ? a)bs ( f2 ? a)cs (D ? f1 ? f2 + a)ds (23)
Setting the first derivative of the log likelihood to be zero yields a cubic equation:
as
a ?
bs
f1 ? a
? cs
f2 ? a
+
ds
D ? f1 ? f2 + a
= 0 (24)
As shown in Section 5.2, using the margin-free model, the ?sample-with-
replacement? assumption amplifies the variance but does not change the estimation.
With our proposed MLE, the ?sample-with-replacement? assumption will change the
estimation, although in general we do not expect the differences to be large. Figure 17
gives an (exaggerated) example, to show the concavity of the log likelihood and the
difference caused by assuming ?sample-with-replacement.?
5.5 A Convenient Practical Quadratic Approximation
Solving a cubic equation for the exact MLE may be so inconvenient that one may prefer
the less accurate margin-free baseline because of its simplicity. This section derives a
convenient closed-form quadratic approximation to the exact MLE.
The idea is to assume ?sample-with-replacement? and that one can identify as from
K1 without knowledge of K2. In other words, we assume a
(1)
s ? Binomial
(
as + bs, af1
)
,
325
Computational Linguistics Volume 33, Number 3
Figure 17
An example: as = 20, bs = 40, cs = 40, ds = 800, f1 = f2 = 100, D = 1000. The estimated a? = 43 for
?sample-with-replacement,? and a? = 51 for ?sample-without-replacement.? (a) The likelihood
profile, normalized to have a maximum = 1. (b) The log likelihood profile, normalized to have a
maximum = 0.
a(2)s ? Binomial
(
as + cs, af2
)
, and a(1)s and a
(2)
s are independent with a
(1)
s = a
(2)
s = as.
The PMF of
(
a(1)s , a
(2)
s
)
is a product of two binomials:
[
(
f1
as + bs
)(
a
f1
)as ( f1 ? a
f1
)bs
]
?
[(
f2
as + cs
)(
a
f2
)as ( f2 ? a
f2
)cs]
? a2as
(
f1 ? a
)bs ( f2 ? a
)cs (25)
Setting the first derivative of the logarithm of (25) to be zero, we obtain
2as
a ?
bs
f1 ? a
? cs
f2 ? a
= 0 (26)
which is quadratic in a and has a convenient closed-form solution:
a?MLE,a =
f1 (2as + cs) + f2 (2as + bs) ?
?
( f1 (2as + cs) ? f2 (2as + bs))2 + 4f1 f2bscs
2 (2as + bs + cs)
(27)
The second root can be ignored because it is always out of range:
f1 (2as + cs) + f2 (2as + bs) +
?
( f1 (2as + cs) ? f2 (2as + bs))2 + 4f1 f2bscs
2 (2as + bs + cs)
? f1 (2as + cs) + f2 (2as + bs) + | f1 (2as + cs) ? f2 (2as + bs) |
2 (2as + bs + cs)
?
{
f1 if f1 (2as + cs) ? f2 (2as + bs)
f2 if f1 (2as + cs) < f2 (2as + bs)
? min( f1, f2)
The evaluation in Section 4 showed that a?MLE,a is close to a?MLE.
326
Li and Church Sketch for Estimating Associations
5.6 The Conditional Variance and Bias
Usually, a maximum likelihood estimator is nearly unbiased. Furthermore, assuming
?sample-with-replacement,? we can apply the large sample theory5 (Lehmann and
Casella 1998, Theorem 6.3.10), which says that a?MLE is asymptotically unbiased and
converges in distribution to a Normal with mean a and variance 1I(a) , where I(a), the
expected Fisher Information, is
I(a) = ?E
(
?2
?a2
log Pr (as, bs, cs, ds|Ds; a, r)
)
= E
(
as
a2
+
bs
( f1 ? a)2
+
cs
( f2 ? a)2
+
ds
(D ? f1 ? f2 + a)2
?
?
?
?
Ds
)
=
E(as|Ds)
a2
+
E(bs|Ds)
(
f1 ? a
)2 +
E(cs|Ds)
(
f2 ? a
)2 +
E(ds|Ds)
(
D ? f1 ? f2 + a
)2
=
Ds
D
(
1
a +
1
f1 ? a
+ 1
f2 ? a
+ 1
D ? f1 ? f2 + a
)
(28)
where we evaluate E(as|Ds), E(bs|Ds), E(cs|Ds), E(ds|Ds) by (16).
For ?sampling-without-replacement,? we correct the asymptotic variance 1I(a) by
multiplying by the finite population correction factor 1 ? DsD :
Var (a?MLE|Ds) ? 1I(a)
(
1 ? DsD
)
=
D
Ds
? 1
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
(29)
Comparing (17) with (29), we know that Var (a?MLE|Ds) < Var (a?MF|Ds), and the dif-
ference could be substantial. In other words, when we know the margins, we ought to
use them.
5.7 The Unconditional Variance and Bias
Errors are a combination of variance and bias. Fortunately, we don?t need to be con-
cerned about bias, at least asymptotically:
E (a?MLE ? a) = E (E (a?MLE ? a|Ds)) ? E(0) = 0 (30)
The unconditional variance can be computed using the conditional variance
formula:
Var (a?MLE) = E (Var (a?MLE |Ds )) + Var (E (a?MLE |Ds ))
?
E
(
D
Ds
)
? 1
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
(31)
5 See Rosen (1972a, 1972b) for the rigorous regularity conditions that ensure convergence in the case of
?sample-without-replacement.?
327
Computational Linguistics Volume 33, Number 3
because E (a?MLE|Ds) ? a, which is a constant. Hence Var (E (a?MLE|Ds)) ? 0.
To evaluate E
(
D
Ds
)
exactly, we need PMF Pr(Ds; a), which is unavailable. Even if it
were available, E
(
D
Ds
)
probably wouldn?t have a convenient closed-form.
Here we recommend the approximations, (3) and (4), mentioned previously. To de-
rive these approximations, recall that Ds = min (max(K1), max(K2)). Using the discrete
order statistics distribution (David 1981, Exercise 2.1.4),6 we obtain:
E (max(K1)) =
k1(D + 1)
f1 + 1
? k1
f1
D, E (max(K2)) ?
k2
f2
D (32)
The min function can be considered to be concave. By Jensen?s inequality (see Cover
and Thomas 1991, Theorem 2.6.2), we know that
E
(
Ds
D
)
= E
(
min
(
max(K1k1)
D ,
max(K2)
D
))
? min
(
E(max(K1)
D ,
E(max(K2)
D
)
= min
(
k1
f1
, k2
f2
)
(33)
The reciprocal function is convex. Again by Jensen?s inequality, we have
E
(
D
Ds
)
= E
(
1
Ds/D
)
? 1
E
(
Ds
D
) ? max
(
f1
k1
,
f2
k2
)
(34)
By replacing the inequalities with equalities, we obtain (35) and (36):
E
(
Ds
D
)
? min
(
k1
f1
, k2
f2
)
(35)
E
(
D
Ds
)
? max
(
f1
k1
,
f2
k2
)
(36)
In our experiments, when the sample size is reasonably large (Ds ? 20), the errors
in (35) and (36) are usually within 5%.
Approximations (35) and (36) provide an intuitive relationship between two views
of the sampling rate: (a) DsD , which depends on corpus size and (b)
k
f , which depends on
the size of the postings. The difference between these two views is important when the
term-by-document matrix is sparse, which is often the case in practice.
Using (36), we obtain the following approximation for the unconditional variance:
Var (a?MLE) ?
max
(
f1
k1
, f2k2
)
? 1
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
(37)
6 Also, see http://www.ds.unifi.it/VL/VL EN/urn/urn5.html.
328
Li and Church Sketch for Estimating Associations
5.8 The Variance of h(a?MLE )
We can estimate any function h(a) by h(a?MLE). In practical applications, h could be any
measure of association including cosine, resemblance, mutual information, etc. When
h(a) is a nonlinear function of a, h(a?MLE) will be biased. One can remove the bias to
some extent using Taylor expansions. See some examples in Li and Church (2005).
Bias correction is important for small samples and highly nonlinear h?s (e.g., the log
likelihood ratio, LLR).
The bias of h(a?MLE) decreases with sample size. Precisely, the delta method (Agresti
2002, Chapter 3.1.5) says that h(a?MLE) is asymptotically unbiased and the variance of
h(a?MLE) is
Var(h(a?MLE)) ? Var(a?MLE)(h?(a))2 (38)
provided h?(a) exists and is non-zero. Non-asymptotically, it is easy to show that
Var(h(a?MLE)) ? Var(a?MLE)(h?(a))2 if h(a) is convex (39)
Var(h(a?MLE)) ? Var(a?MLE)(h?(a))2 if h(a) is concave (40)
5.9 How Many Samples Are Sufficient?
The answer depends on the trade-off between computational costs (time and space)
and estimation errors. For very infrequent words, we might afford to sample 100%. In
general, a reasonable criterion is the coefficient of variation, cv = SE(a?)a , SE =
?
Var(a?).
We consider the estimate is accurate if the cv is below some threshold ?0 (e.g., ?0 = 0.1).
The cv can be expressed as
cv =
SE(a?)
a ?
1
a
?
?
?
?
max
(
f1
k1
, f2k2
)
? 1
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
(41)
Figure 18(a) plots the required sampling rate min
(
k1
f1
, k2f2
)
computed from (41). The
figure shows that at Web scale (i.e., D ? 10 billion), a sampling rate as low as 10?3 may
suffice for ?ordinary? words (i.e., f1 ? 107 = 0.001D). Figure 18(b) plots the required
sample size k1, for the same experiment in Figure 18(a), where for simplicity, we assume
k1
f1
= k2f2 . The figure shows that, after D is large enough, the required sample size does
not increase as much.
To apply (41) to the real data, Table 5 presents the critical sampling rates and sample
sizes for all pair-wise combinations of the four-word query Governor, Schwarzenegger,
Terminator, Austria. Here we assume the estimates in Table 3 are exact. The table verifies
that only a very small sample may suffice to achieve a reasonable cv.
5.10 Tail Bound and Multiple Comparisons Effect
To choose the sample size, it is often necessary to consider the effect of multiple compar-
isons. For example, when we estimate all pair-wise associations among n data points,
329
Computational Linguistics Volume 33, Number 3
Figure 18
(a) An analysis based on cv = SEa = 0.1 suggests that we can get away with very low sampling
rates. The three curves plot the critical value for the sampling rate, min
(
k1
f1
, k2f2
)
, as a function of
corpus size, D. At Web scale, D ? 1010, sampling rates above 10?2 to 10?4 satisfy cv ? 0.1, at
least for these settings of f1, f2, and a. The settings were chosen to simulate ?ordinary? words.
The three curves correspond to three choices of f1: D/100, D/1000, and D/10, 000. f2 = f1/10,
a = f2/20. (b) The critical sample size k1 (assuming
k1
f1
= k2f2 ), corresponding to the sampling rates
in (a).
Table 5
The critical sampling rates and sample sizes (for cv = 0.1) are computed for all two-way
combinations among the four words Governor, Schwarzenegger, Terminator, Austria, assuming the
estimated document frequencies and two-way associations in Table 3 are exact. The required
sampling rates are all very small, verifying our claim that for ?ordinary? words, a sampling rate
as low as 10?3 may suffice. In these computations, we used D = 5 ? 109 for the number of
English documents in the collection.
Query Critical Sampling Rate
Governor, Schwarzenegger 5.6 ? 10?5
Governor, Terminator 7.2 ? 10?4
Governor, Austria 1.4 ? 10?4
Schwarzenegger, Terminator 1.5 ? 10?4
Schwarzenegger, Austria 8.1 ? 10?4
Terminator, Austria 5.5 ? 10?4
we are estimating n(n?1)2 pairs simultaneously. A convenient approach is to bound the
tail probability
Pr (|a?MLE ? a| > a) ? ?/p (42)
where ? (e.g., 0.05) is the level of significance,  is the specified accuracy (e.g.,  < 0.5),
and p is the correction factor for multiple comparisons. The most conservative choice is
p = n22 , known as the Bonferroni Correction. But often it is reasonable to let p be much
smaller (e.g., p = 100).
We can gain some insight from (42). In particular, our previous argument based on
coefficient of variations (cv) is closely related to (42).
330
Li and Church Sketch for Estimating Associations
Assuming a?MLE ? N (a, Var (a?MLE)), then, based on the known normal tail bound,
Pr (|a?MLE ? a| > a) ? 2 exp
(
? 
2a2
2Var (a?MLE)
)
= 2 exp
(
? 
2
2cv2
)
(43)
combined with (42), leads to the following criterion on cv
cv ? 
?
? 1
2 log
(
?/2p
) (44)
For example, if we let ? = 0.05, p = 100, and  = 0.4, then (44) will output cv ? 0.1.
5.11 Sample Size Selection Based on Storage Constraints
Suppose we can compute the maximum allowed total samples, T, for example, based
on the available memory. That is,
?n
i=1 ki = T, where n is the total number of words. We
could allocate T according to document frequencies fj, that is,
kj =
fj
?n
i=1 fi
T (45)
Usually, we will need to define a lower bound kl and an upper bound ku, which have
to be selected from engineering experience, depending on the specific applications. We
will truncate the computed kj if it is outside [kl, ku]. Equation (45) implies a uniform
corpus sampling rate, which may not be always desirable, but the confinement by
[kl, ku] can effectively vary the sampling rates.
More carefully, we can minimize the total number of ?unused? samples. For a pair,
Wi and Wj, if
ki
fi
? kjfj , then on average, there are
(
ki
fi
? kjfj
)
fi samples unused in Ki. This
is the basic idea behind the following linear program for choosing the ?optimal? sample
sizes:
Minimize
n
?
i=1
n
?
j=i+1
[
fi
(
ki
fi
?
kj
fj
)
+
+ fj
(
kj
fj
? ki
fi
)
+
]
subject to
n
?
i=1
ki = T, ki ? fi, kl ? ki ? ku (46)
where (z)+ = max(0, z), is the positive part of z. This program can be modified (possibly
no longer a linear program) to consider other factors in different applications. For
example, some applications may care more about the very rare words, so we would
weight the rare words more.
5.12 When Will Sketches Not Perform Well?
We consider three scenarios. (A) f1 and f2 are both large; (B) f1 and f2 are both small; (C)
f1 is very large but f2 is very small. Conventional sampling over documents can handle
situation (A), but will perform poorly on (B) because there is a good chance that the
sample will miss the rare words. The sketch algorithm can handle both (A) and (B) well.
331
Computational Linguistics Volume 33, Number 3
In fact, it will do very well when both words are rare because the equivalent sampling
rate DsD ? min
(
k1
f1
, k2f2
)
can be high, even 100%.
When f2 
 f1, no sampling method can work well unless we are willing to sample
P1 with a sufficiently large sample. Otherwise even if we let
k2
f2
= 100%, the corpus
sampling rate, DsD ?
k1
f1
, will be low. For example, Google estimates 14,000,000 hits
for Holmes, 37,500 hits for Diaconis, and 892 joint hits. Assuming D = 5 ? 109 and
cv = 0.1, the critical sample size for Holmes would have to be 1.4 ? 106, probably too
large as a sample.7
6. Extension to Multi-Way Associations
Many applications involve multi-way associations, for example, association rules, data-
bases, and Web search. The ?Governator? example in Table 3, for example, made use
of both two-way and three-way associations. Fortunately, our sketch construction and
estimation algorithm can be naturally extended to multi-way associations. We have
already presented an example of estimating multi-way associations in Section 1.6. When
we do not consider the margins, the estimation task is as simple as in the pair-wise case.
When we do take advantage of margins, estimating multi-way associations amounts to
a convex program. We will also analyze the theoretical variances.
6.1 Multi-Way Sketches
Suppose we are interested in the associations among m words, denoted by W1,
W2, . . . , Wm. The document frequencies are f1, f2, . . . , and fm, which are also the lengths
of the postings P1, P2, . . . , Pm. There are N = 2m combinations of associations, denoted
by x1, x2, . . . , xN. For example,
a = x1 = |P1 ? P2 ? . . . ? Pm?1 ? Pm|
x2 = |P1 ? P2 ? . . . ? Pm?1 ? ?Pm|
x3 = |P1 ? P2 ? . . . ? ?Pm?1 ? Pm|
. . .
xN?1 = |?P1 ? ?P2 ? . . . ? ?Pm?1 ? Pm|
xN = |?P1 ? ?P2 ? . . . ? ?Pm?1 ? ?Pm| (47)
which can be directly corresponded to the binary representation of integers.
Using the vector and matrix notation, X = [x1, x2, . . . , xN]T, F = [ f1, f2, . . . , fm, D]T,
where the superscript ?T? stands for ?transpose?, that is, we always work with col-
umn vectors. We can write down the margin constraints in terms of a linear matrix
equation as
AX = F (48)
7 Readers familiar with random projections can verify that in this case we need k = 6.6 ? 107 projections in
order to achieve cv = 0.1. See Li, Hastie, and Church (2006a, 2006b) for the variance formula of random
projections.
332
Li and Church Sketch for Estimating Associations
where A is the constraint matrix. If necessary, we can use A(m) to identify A for different
m values. For example, when m = 2 or m = 3,
A(2) =
?
?
1 1 0 0
1 0 1 0
1 1 1 1
?
? A(3) =
?
?
?
?
1 1 1 1 0 0 0 0
1 1 0 0 1 1 0 0
1 0 1 0 1 0 1 0
1 1 1 1 1 1 1 1
?
?
?
?
(49)
For each word Wi, we sample the ki smallest elements from its permuted postings,
?(Pi), to form a sketch, Ki. Recall ? is a random permutation on ? = {1, 2, . . . , D}. We
compute
Ds = min{max(K1), max(K2), . . . , max(Km)}. (50)
After removing the elements in all m Ki?s that are larger than Ds, we intersect these
m trimmed sketches to generate the sample table counts. The samples are denoted as
S = [s1, s2, . . . , sN]T.
Conditional on Ds, the samples S are statistically equivalent to Ds random samples
over documents from the corpus. The corresponding conditional PMF and log PMF
would be
Pr(S|Ds; X) =
(
x1
s1
)(
x2
s2
)
. . .
(
xN
sN
)
(
D
Ds
) ?
N
?
i=1
si?1
?
j=0
(xi ? j) (51)
log Pr(S|Ds; X) ? Q =
N
?
i=1
si?1
?
j=0
log(xi ? j) (52)
The log PMF is concave, as in two-way associations. A partial likelihood MLE solu-
tion, namely, the X? that maximizes log Pr(S|Ds; X?), will again be adopted, which leads
to a convex optimization problem. But first, we shall discuss two baseline estimators.
6.2 Baseline Independence Estimator
Assuming independence, an estimator of x1 would be
x?1,IND = D
m
?
i=1
fi
D (53)
which can be easily proved using a conditional expectation argument.
By the property of the hypergeometric distribution, E(|Pi ? Pj|) =
fi fj
D . Therefore,
E(x1) = E(|P1 ? P2 ? . . . ? Pm|) = E(| ?mi=1 Pi|)
= E(E(|P1 ? (?mi=2Pi)||(?mi=2Pi))) =
f1
DE(| ?
m
i=2 Pi|)
=
f1 f2 . . . fm?2
Dm?2
E(|Pm?1 ? Pm|) = D
m
?
i=1
fi
D (54)
333
Computational Linguistics Volume 33, Number 3
6.3 Baseline Margin-Free Estimator
The conditional PMF Pr(S|Ds; X) is a multivariate hypergeometric distribution, based
on which we can derive the margin-free estimator:
E(si|Ds) =
Ds
D xi, x?i,MF =
D
Ds
si, Var(x?i,MF|Ds) = DDs
1
1
xi +
1
D?xi
D ? Ds
D ? 1 (55)
We can see that the margin-free estimator remains its simplicity in the multi-way case.
6.4 The MLE
The exact MLE can be formulated as a standard convex optimization problem,
minimize ? Q = ?
N
?
i=1
si?1
?
j=0
log(xi ? j)
subject to AX = F, and X  S (56)
where X  S is a compact representation for xi ? si, 1 ? i ? N.
This optimization problem can be solved by a variety of standard methods such
as Newton?s method (Boyd and Vandenberghe 2004, Chapter 10.2). Note that we can
ignore the implicit inequality constraints, X  S, if we start with a feasible initial guess.
It turns out that the formulation in (56) will encounter numerical difficulty due
to the inner summation in the objective function Q. Smoothing will bring in more
numerical issues. Recall that in estimating two-way associations we do not have this
problem, because we have eliminated the summation in the objective function, using an
(integer) updating formula. In multi-way associations, it seems not easy to reformulate
the objective function Q in a similar form.
To avoid the numerical problems, a simple solution is to assume ?sample-with-
replacement,? under which the conditional likelihood and log likelihood become
Pr(S|Ds; X, r) ?
N
?
i=1
(xi
D
)si ?
N
?
i=1
xsii (57)
log Pr(S|Ds; X, r) ? Qr =
N
?
i=1
si log xi (58)
Our MLE problem can then be reformulated as
minimize ? Q = ?
N
?
i=1
si log xi
subject to AX = F, and X  S (59)
which is again a convex program. To simplify the notation, we neglect the subscript ?r.?
334
Li and Church Sketch for Estimating Associations
We can compute the gradient (Q) and Hessian (2Q). The gradient is a vector of
the first derivatives of Q with respect to xi, for 1 ? i ? N,
Q =
[
?Q
?xi
, 1 ? i ? N
]
=
[ s1
x1 ,
s2
x2 , . . . ,
sN
xN
]T
(60)
The Hessian is a matrix whose (i, j)th entry is the partial derivative ?
2Q
?xixj
, that is,
2Q = ?diag
[
s1
x21
, s2
x22
, . . . , sN
x2N
]
(61)
The Hessian has a very simple diagonal form, implying that Newton?s method will
be a good algorithm for solving this optimization problem. We implement, in Appen-
dix 2, the equality constrained Newton?s method with feasible start and backtracking
line search (Boyd and Vandenberghe 2004, Algorithm 10.1). A key step is to solve for
Newton?s step, Xnt:
[
?2 Q AT
A 0
] [
Xnt
dummy
]
=
[
Q
0
]
. (62)
Because the Hessian 2Q is a diagonal matrix, solving for Newton?s step in (62) can
be sped up substantially (e.g., using the block matrix inverse formula).
6.5 The Covariance Matrix
We apply the large sample theory to estimate the covariance matrix of the MLE. Recall
that we have N = 2m variables and m + 1 constraints. The effective number of variables
would be 2m ? (m + 1), which is also the dimension of the covariance matrix.
We seek a partition of A = [A1, A2], such that A2 is invertible. We may have to
switch some columns of A in order to find an invertible A2. In our construction, the
jth column of A2 is the column of A such that last entry of the jth row of A is 1. An
example for m = 3 would be
A(3)1 =
?
?
?
?
1 1 1 0
1 1 0 1
1 0 1 1
1 1 1 1
?
?
?
?
A(3)2 =
?
?
?
?
1 0 0 0
0 1 0 0
0 0 1 0
1 1 1 1
?
?
?
?
(63)
where A(3)1 is the [1 2 3 5] columns of A
(3) and A(3)2 is the [4 6 7 8] columns of A
(3). We can
see that A2 constructed this way is always invertible because its determinant is always
one.
Corresponding to the partition of A, we partition X = [X1, X2]T. For example, when
m = 3, X1 = [x1, x2, x3, x5]T, X2 = [x4, x6, x7, x8]T. We can then express X2 to be
X2 = A
?1
2 (F ? A1X1) = A
?1
2 F ? A
?1
2 A1X1 (64)
335
Computational Linguistics Volume 33, Number 3
The log likelihood function Q, which is separable, can then be expressed as
Q(X) = Q1(X1) + Q2(X2) (65)
By the matrix derivative chain rule, the Hessian of Q with respect to X1 would be
21Q = 21Q1 +21Q2 = 21Q1 +
(
A?12 A1
)T
22 Q2
(
A?12 A1
)
(66)
where we use 21 and 22 to indicate the Hessians are with respect to X1 and X2,
respectively.
Conditional on Ds, the Expected Fisher Information of X1 is
I(X1) = E
(
?21 Q|Ds
)
= ?E(21Q1|Ds) ?
(
A?12 A1
)T
E(22Q2|Ds)
(
A?12 A1
)
(67)
where
E(?21 Q1|Ds) = diag
[
E
(
si
x2i
)
, xi ? X1
]
=
Ds
D diag
[
1
xi , xi ? X1
]
(68)
E(?22 Q2|Ds) =
Ds
D diag
[
1
xi , xi ? X2
]
(69)
By the large sample theory, and also considering the finite population correction
factor, we can approximate the (conditional) covariance matrix of X1 to be
Cov(X1|Ds) ? I(X1)?1
(
1 ? DsD
)
=
(
D
Ds
? 1
)(
diag
[
1
xi , xi ? X1
]
+
(
A?12 A1
)T
diag
[
1
xi , xi ? X2
] (
A?12 A1
)
)?1
(70)
For a sanity check, we verify that this approach recovers the same variance formula
in the two-way association case. Recall that, when m = 2, we have
2Q = ?
?
?
?
?
?
?
?
s1
x21
0 0 0
0
s2
x22
0 0
0 0
s3
x23
0
0 0 0
s4
x24
?
?
?
?
?
?
?
, 21Q1 = ?
s1
x21
, 22Q2 = ?
?
?
?
?
s2
x22
0 0
0
s3
x23
0
0 0
s4
x24
?
?
?
?
(71)
A(2) =
?
?
1 1 0 0
1 0 1 0
1 1 1 1
?
? , A(2)1 =
?
?
1
1
1
?
? , A(2)2 =
?
?
1 0 0
0 1 0
1 1 1
?
? (72)
336
Li and Church Sketch for Estimating Associations
(
A?12 A1
)T
22 Q2A?12 A1 = ?
[
1 1 ?1
]
?
?
?
?
s2
x22
0 0
0
s3
x23
0
0 0
s4
x24
?
?
?
?
?
?
1
1
?1
?
? = ? s2
x22
? s3
x23
? s4
x24
(73)
Hence,
?21 Q =
s1
x21
+
s2
x22
+
s3
x23
+
s4
x24
=
as
a2
+
bs
( f1 ? a)2
+
cs
( f2 ? a)2
+
ds
(D ? f1 ? f2 + a)2
(74)
which leads to the same Fisher Information for the two-way association as we have
derived.
6.6 The Unconditional Covariance Matrix
Similar to two-way associations, the unconditional variance of the proposed MLE can
be estimated by replacing DDs in (70) with E
(
D
Ds
)
, namely,
Cov(X1) ?
(
E
(
D
Ds
)
? 1
)
?
(
diag
[
1
xi , xi ? X1
]
+
(
A?12 A1
)T
diag
[
1
xi , xi ? X2
] (
A?12 A1
)
)?1
(75)
Similar to two-way associations, we recommend the following approximations:
E
(
Ds
D
)
? min
(
k1
f1
, k2
f2
, . . . , km
fm
)
(76)
E
(
D
Ds
)
? max
(
f1
k1
,
f2
k2
, . . . ,
fm
km
)
(77)
Again, the approximation (76) will overestimate E
(
Ds
D
)
and (77) will underestimate
E
(
D
Ds
)
hence also underestimating the unconditional variance.
6.7 Empirical Evaluation
We use the same four words as in Table 4 to evaluate the multi-way association al-
gorithm, as merely a sanity check. There are four different combinations of three-way
associations and one four-way association, as listed in Table 6.
We present results for x1 (i.e., a in two-way associations) for all cases. The evalua-
tions for four three-way cases are presented in Figures 19, 20 and 21. From these figures,
we see that the proposed MLE has lower MSE than the MF. As in the two-way case,
smoothing helps MLE but still hurts MF in most cases. Also, the experiments verify that
our approximate variance formulas are fairly accurate.
Figure 22 presents the evaluation results for the four-way association case, includ-
ing MSE, smoothing, and variance. The results are similar to the three-way case.
337
Computational Linguistics Volume 33, Number 3
Table 6
The same four words as in Table 4 are used for evaluating multi-way associations. There are in
total four three-way combinations and one four-way combination.
Case No. Words Co-occurrences
Case 3-1 THIS, HAVE, HELP 4940
Three-way Case 3-2 THIS, HAVE, PROGRAM 2575
Case 3-3 THIS, HELP, PROGRAM 1626
Case 3-4 HAVE, HELP, PROGRAM 1460
Four-way Case 4 THIS, HAVE, HELP, PROGRAM 1316
We have used the empirical E
(
D
Ds
)
to compute the unconditional variance. Fig-
ure 23 plots max
(
f1
k1
, f2k2 , . . . ,
fm
km
)
/ DDs for all cases. The figure indicates that using
max
(
f1
k1
, f2k2 , . . . ,
fm
km
)
to estimate E
(
D
Ds
)
is still fairly accurate when the sample size is
reasonable.
Combining the results of two-way associations for the same four words, we can
study the trend how the proposed MLE improve the MF baseline. Figure 24(a) sug-
Figure 19
In terms of
?
MSE(x1 )
x1 , the proposed MLE is consistently better than the MF, which is better than
the IND, for four three-way association cases.
338
Li and Church Sketch for Estimating Associations
Figure 20
The simple ?add-one? smoothing improves the estimation accuracies for the proposed MLE.
Smoothing, however, in all cases except Case 3-1 hurts the margin-free estimator.
gests that the proposed MLE is a big improvement over the MF baseline for two-
way associations, but the improvement becomes less and less noticeable with higher
order associations. This observation is not surprising, because the number of degrees
of freedom, 2m ? (m + 1), increases exponentially with m. In order words, the margin
constraints are most effective for small m, but the effectiveness decreases rapidly with m.
On the other hand, smoothing becomes more and more important as m increases,
as shown in Figure 24(b), partly because of the data sparsity in high order associations.
7. Related Work: Comparison with Broder?s Sketches
Broder?s sketches (Broder 1997), originally introduced for removing duplicates in the
AltaVista index, have been applied to a variety of applications (Broder et al 1997;
Haveliwala, Gionis, and Indyk 2000; Haveliwala et al 2002). Broder et al (1998, 2000)
presented some theoretical aspects of the sketch algorithm. There has been considerable
exciting work following up on this line of research including Indyk (2001), Charikar
(2002), and Itoh, Takei, and Tarui (2003).
Broder and his colleagues introduced two algorithms, which we will refer to as
the ?original sketch? and the ?minwise sketch? for estimating resemblance, R = |P1?P2||P1?P2| .
The original sketch uses a single random permutation on ? = {1, 2, 3, . . . , D}, and the
minwise sketch uses k random permutations. Both algorithms have similar estimation
accuracies, as will see.
339
Computational Linguistics Volume 33, Number 3
Figure 21
In terms of SE(x1 )x1 , the theoretical variance of MLE fits the empirical values very well. At low
sampling rates, smoothing effectively reduces the variance. Note that we plug in the empirical
E
(
D
Ds
)
into (75) to estimate the unconditional variance. The errors due to this approximation are
presented in Figure 23.
Figure 22
Four-way associations (Case 4). (a) The proposed MLE has smaller MSE than the margin-free
(MF) baseline, which has smaller MSE than the independence baseline. (b) Smoothing
considerably improves the accuracy for MLE and also slightly improves MF. (c) For the
proposed MLE, the theoretical prediction fits the empirical variance very well. Smoothing
considerably reduces variance.
Our proposed sketch algorithm is closer to Broder?s original sketch, with a few
important differences. A key difference is that Broder?s original sketch throws out half
of the sample, whereas we throw out less. In addition, the sketch sizes are fixed over all
words for Broder, whereas we allow different sizes for different words. Broder?s method
was designed for a single statistic (resemblance), whereas we generalize the method to
340
Li and Church Sketch for Estimating Associations
Figure 23
The ratios max
(
f1
k1
, f2k2 , . . . ,
fm
km
)
/ DDs are plotted for all cases. At sampling rates > 0.01, the ratios
are > 0.9 ? 0.95, indicating good accuracy.
Figure 24
(a) Combining the three-way, four-way, and two-way association results for the four words in
the evaluations, the average relative improvements of
?
MSE suggests that the proposed MLE is
consistently better than the MF baseline but the improvement decreases monotonically as the
order of associations increases. (b) Average
?
MSE improvements due to smoothing imply that
smoothing becomes more and more important as the order of association increases.
compute contingency tables (and summaries thereof). Broder?s method was designed
for pairwise associations, whereas our method generalizes to multi-way associations.
Finally, Broder?s method was designed for boolean data, whereas our method general-
izes to reals.
7.1 Broder?s Minwise Sketch
Suppose a random permutation ?1 is performed on the document IDs. We denote the
smallest IDs in the postings P1 and P2, by min(?1(P1)) and min(?1(P2)), respectively.
Obviously,
Pr (min(?1(P1)) = min(?1(P2))) =
|P1 ? P2|
|P1 ? P2|
= R (78)
341
Computational Linguistics Volume 33, Number 3
After k minwise independent permutations, denoted as ?1, ?2, . . . , ?k, we can
estimate R without bias, as a binomial probability, namely,
R?B,r = 1k
k
?
i=1
{min(?i(P1)) = min(?i(P2))} and Var
(
R?B,r
)
= 1
k
R(1 ? R) (79)
7.2 Broder?s Original Sketch
A single random permutation ? is applied to the document IDs. Two sketches are con-
structed: K1 = MINk1 (?(P1)), K2 = MINk2 (?(P2)).
8 Broder (1997) proposed an unbiased
estimator for the resemblance:
R?B =
|MINk(K1 ? K2) ? K1 ? K2|
|MINk(K1 ? K2)|
(80)
Note that intersecting by MINk(K1 ? K2) throws out half the samples, which can be
undesirable (and unnecessary).
The following explanation for (80) is slightly different from Broder (1997). We
can divide the set P1 ? P2 (of size a + b + c = f1 + f2 ? a) into two disjoint sets: P1 ? P2
and P1 ? P2 ? P1 ? P2. Within the set MINk(K1 ? K2) (of size k), the document IDs that
belong to P1 ? P2 would be MINk(K1 ? K2) ? K1 ? K2, whose size is denoted by aBs . This
way, we have a hypergeometric sample, that is, we sample k document IDs from P1 ? P2
randomly without replacement and obtain aBs IDs that belong to P1 ? P2. By the property
of the hypergeometric distribution, the expectation of aBs would be
E
(
aBs
)
= ak
f1 + f2 ? a
=? E
(
aBs
k
)
= a
f1 + f2 ? a
=
|P1 ? P2|
|P1 ? P2|
=? E(R?B) = R (81)
The variance of R?B, according to the hypergeometric distribution, is:
Var
(
R?B
)
= 1
k
R(1 ? R) f1 + f2 ? a ? k
f1 + f2 ? a ? 1
(82)
where the term f1+ f2?a?kf1+ f2?a?1 is the ?finite population correction factor.?
The minwise sketch can be considered as a ?sample-with-replacement? variate of
the original sketch. The analysis of minwise sketch is slightly simpler mathematically
whereas the original sketch is more efficient. The original sketch requires only one
random permutation and has slightly smaller variance than the minwise sketch, that
is, Var
(
R?B,r
)
? Var
(
R?B
)
. When k is reasonably small, as is common in practice, two
sketch algorithms have similar errors.
7.3 Why Our Algorithm Improves Broders?s Sketch
Our proposed sketch algorithm starts with Broder?s original (one permutation) sketch;
but our estimation method differs in two important aspects.
8 Actually, the method required fixing sketch sizes: k1 = k2 = k, a restriction that we find convenient to relax.
342
Li and Church Sketch for Estimating Associations
Firstly, Broder?s estimator (80) uses k out of 2 ? k samples. In particular, it uses only
aBs = |MINk(K1 ? K2) ? K1 ? K2| intersections, which is always smaller than as = |K1 ?
K2| available in the samples. In contrast, our algorithm takes advantage of all useful
samples up to Ds = min(max(K1), max(K2)), particularly all as intersections. If
k1
f1
= k2f2 ,
that is, if we sample proportionally to the margins:
k1 = 2k
f1
f1 + f2
k2 = 2k
f2
f1 + f2
(83)
it is expected that almost all samples will be utilized.
Secondly, Broder?s estimator (80) considers a two-cell hypergeometric model (a, b +
c) whereas the two-way association is a four-cell model (a, b, c, d), which is used in our
proposed estimator. Simpler data models often result in simpler estimation methods but
with larger errors.
Therefore, it is obvious that our proposed method has smaller estimator errors.
Next, we compare our estimator with Broder?s sketches in terms of the theoretical
variances.
7.4 Comparison of Variances
Broder?s method was designed to estimate resemblance. Thus, this section will compare
the proposed method with Broder?s sketches in terms of resemblance, R.
We can compute R from our estimated association a?MLE:
R?MLE =
a?MLE
f1 + f2 ? a?MLE
(84)
R?MLE is slightly biased. However, because the second derivative R??(a)
R??(a) =
2( f1 + f2)
( f1 + f2 ? a)3
? 2( f1 + f2)
max( f1, f2)3
? 4
max( f1, f2)2
(85)
is small (i.e., the nonlinearity is weak), it is unlikely that the bias will be noticeable in
practice.
By the delta method as described in Section 5.8, the variance of R?MLE is
approximately
Var
(
R?MLE
)
? Var(a?MLE)(R?(a))2 =
max
(
f1
k1
, f2k2
)
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
( f1 + f2)2
( f1 + f2 ? a)4
(86)
conservatively ignoring the ?finite population correction factor,? for convenience.
Define the ratio of the variances to be VB =
Var(R?MLE)
Var(R?B)
, then
VB =
Var
(
R?MLE
)
Var
(
R?B
) =
max
(
f1
k1
, f2k2
)
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
( f1 + f2)2
( f1 + f2 ? a)2
k
a( f1 + f2 ? 2a)
(87)
343
Computational Linguistics Volume 33, Number 3
To help our intuitions, let us consider some reasonable simplifications to VB. As-
suming a << min( f1, f2) < max( f1, f2) << D, then approximately
VB ?
k max( f1k1 ,
f2
k2
)
f1 + f2
=
?
?
?
?
?
max( f1, f2 )
f1+ f2
if k1 = k2 = k
1
2 if k1 = 2k
f1
f1+ f2
, k2 = 2k
f2
f1+ f2
(88)
which indicates that the proposed method is a considerable improvement over Broder?s
sketches. In order to achieve the same accuracy, our method requires only half as many
samples.
Figure 25 plots the VB in (87) for the whole range of f1, f2, and a, assuming equal
samples: k1 = k2 = k. We can see that VB ? 1 always holds and VB = 1 only when f1 =
f2 = a. There is also the possibility that VB is close to zero.
Proportional samples further reduce VB, as shown in Figure 26.
Figure 25
We plot VB in (87) for the whole range of f1, f2, and a, assuming equal samples: k1 = k2 = k. (a),
(b), (c), and (d) correspond to f2 = 0.2f1, f2 = 0.5f1, f2 = 0.8f1, and f2 = f1, respectively. Different
curves are for different f1?s, ranging from 0.05D to 0.95D spaced at 0.05D. The horizontal lines
are max( f1,f2 )f1+f2 . We can see that for all cases, VB ? 1 holds. VB = 1 when f1 = f2 = a, a trivial case.
When a/f2 is small, VB ? max( f1,f2 )f1+f2 holds well. It is also possible that VB is very close to zero.
344
Li and Church Sketch for Estimating Associations
Figure 26
Compared with equal samples in Figure 25, proportional samples further reduce VB.
We can show algebraically that VB in (87) is always less than unity unless f1 = f2 = a.
For convenience, we use the notion a, b, c, d in (87). Assuming k1 = k2 = k and f1 > f2,
we obtain
VB =
a + b
1
a + 1b +
1
c + 1d
(2a + b + c)2
(a + b + c)2
1
a(b + c)
(89)
To show VB ? 1, it suffices to show
(a + b)(2a + b + c)2bcd ? (bcd + acd + abd + abc)(a + b + c)2(b + c) (90)
which is equivalent to following true statement:
(a3(b ? c)2 + bc2(b + c)2 + a2(2b + c)(b2 ? bc + 2c2) + a(b + c)(b3 + 4bc2 + c2))d
+ abc(b + c)(a + b + c)2 ? 0 (91)
7.5 Empirical Evaluations
We have theoretically shown that our proposed method is a considerable improvement
over Broder?s sketch. Next, we would like to evaluate these theoretical results using the
same experiment data as in evaluating two-way associations (i.e., Table 4).
Figure 27 compares the MSE. Here we assume equal samples and later we will
show that proportional samples could further improve the results. The figure shows
that our MLE estimator is consistently better than Broder?s sketch. In addition, the
approximate MLE a?MLE,a still gives very close answers to the exact MLE, and the
simple ?add-one? smoothing improves the estimations at low sampling rates, quite
substantially.
Figure 28 illustrates the bias. As expected, estimating resemblance from a?MLE intro-
duces a small bias. This bias will be ignored since it is small compared to the MSE.
Figure 29 verifies that the variance of our estimator is always smaller than Broder?s
sketch. Our theoretical variance in (86) underestimates the true variances because the
approximation E
(
D
Ds
)
= max
(
f1
k1
, f2k2
)
underestimates the variance. In addition, because
345
Computational Linguistics Volume 33, Number 3
Figure 27
When estimating the resemblance, our algorithm gives consistently more accurate answers
than Broder?s sketch. In our experiments, Broder?s ?minwise? construction gives almost the
same answers as the ?original? sketch, thus only the ?minwise? results are presented here.
The approximate MLE again gives very close answers to the exact MLE. Also, smoothing
improves at low sampling rates.
the resemblance R(a) is a convex function of a, the delta method also underestimates
the variance. However, Figure 29 shows that the errors are not very large, and become
negligible with reasonably large sample sizes (e.g., 50). This evidence suggests that the
variance formula (86) is reliable.
Figure 28
Our proposed MLE has higher bias than the ?minwise? estimator because of the non-linearity of
resemblance. However, the bias is very small compared with the MSE.
346
Li and Church Sketch for Estimating Associations
Figure 29
Our proposed estimator has consistently smaller variances than Broder?s sketch. The theoretical
variance, computed by (86), slightly underestimates the true variance with small samples. Here
we did not plot the theoretical variance for Broder?s sketch because it is very close to the
empirical curve.
Finally, in Figure 30, we show that with proportional samples, our algorithm further
improves the estimates in terms of MSE. With equal samples, our estimators improve
Broder?s sketch by 30?50%. With proportional samples, improvements become 40?80%.
Note that the maximum possible improvement is 100%.
8. Conclusion
In databases, data mining, and information retrieval, there has been considerable in-
terest in sampling and sketching techniques (Chaudhuri, Motwani, and Narasayya
1998; Indyk and Motwani 1998; Manku, Rajagopalan, and Lindsay 1999; Charikar
2002; Achlioptas 2003; Gilbert et al 2003; Li, Hastie, and Church 2007; Li 2006), which
are useful for numerous applications such as association rules (Brin et al 1997; Brin,
Motwani, and Silverstein 1997), clustering (Guha, Rastogi, and Shim 1998; Broder 1998;
Aggarwal et al 1999; Haveliwala, Gionis, and Indyk 2000; Haveliwala et al 2002), query
optimization (Matias, Vitter, and Wang 1998; Chaudhuri, Motwani, and Narasayya
1999), duplicate detection (Broder 1997; Brin, Davis, and Garcia-Molina 1995), and
more. Sampling methods become more and more important with larger and larger
collections.
The proposed method generates random sample contingency tables directly from
the sketch, the front of the inverted index. Because the term-by-document matrix is
extremely sparse, it is possible for a relatively small sketch, k, to characterize a large
sample of Ds documents. The front of the inverted index not only tells us about the
presence of the word in the first k documents, but it also tells us about the absence
of the word in the remaining Ds ? k documents. This observation becomes increas-
ingly important with larger Web collections (with ever increasing sparsity). Typically,
Ds  k.
347
Computational Linguistics Volume 33, Number 3
Figure 30
Compared with Broder?s sketch, the relative MSE improvement should be, approximately,
min( f1, f2 )
f1+ f2
with equal samples, and 12 with proportional samples. The two horizontal lines in each
figure correspond to these two approximates. The actual improvements could be lower or
higher. The figure verifies that proportional samples can considerably improve the accuracies.
To estimate the contingency table for the entire population, one can use the ?margin-
free? baseline, which simply multiplies the sample contingency table by the appropriate
scaling factor. However, we recommend taking advantage of the margins (also known
as document frequencies). The maximum likelihood solution under margin constraints
is a cubic equation, which has a remarkably accurate quadratic approximation. The pro-
posed MLE methods were compared empirically and theoretically to the MF baseline,
finding large improvements. When we know the margins, we ought to use them.
Our proposed method differs from Broder?s sketches in important aspects. (1) Our
sketch construction allows more flexibility in that the sketch size can be different from
one word to the next. (2) Our estimation is more accurate. The estimator in Broder?s
sketches uses one half of the samples whereas our method always uses more. More
samples lead to smaller errors. (3) Broder?s method considers a two-cell model whereas
our method works with a more refined (hence more accurate) four-cell contingency
table model. (4) Our method extends naturally to estimating multi-way associations. (5)
Although this paper only considers boolean (0/1) data, our method extends naturally
to general real-valued data; see Li, Church, and Hastie (2006, 2007).
Although we have used ?word associations? for explaining the algorithm, the
method is a general sampling technique, with potential applications in Web search,
databases, association rules, recommendation systems, nearest neighbors, and machine
learning such as clustering.
Acknowledgments
The authors thank Trevor Hastie, Chris
Meek, David Heckerman, Mark Manasse,
David Siegmund, Art Owen, Robert
Tibshirani, Bradley Efron, Andrew Ng, and Tze
Leung Lai. Much of the work was conducted
at Microsoft while the first author was an
intern during the summers of 2004 and 2005.
348
Li and Church Sketch for Estimating Associations
Appendix 1: Sample C Code for Estimating Two-Way Associations
#include <stdio.h>
#include <math.h>
#define MAX(x,y) ( (x) > (y) ? (x) : (y) )
#define MIN(x,y) ( (x) < (y) ? (x) : (y) )
#define EPS 1e-10
#define MAX_ITER 50
int est_a_appr(int as,int bs,int cs, int f1, int f2);
int est_a_mle(int as,int bs, int cs, int ds, int f1, int f2,int D);
int main(void)
{
int f1 = 10000, f2 = 5000, D = 65536; // test data
int as = 25, bs = 45, cs = 150, ds = 540;
int a_appr = est_a_appr(as,bs,cs,f1,f2);
int a_mle = est_a_mle(as,bs,cs,ds,f1,f2,D);
printf("Estimate a_appr = %d\n",a_appr); // output 1138
printf("Estimate a_mle = %d\n",a_mle); // output 821
return 0;
}
// The approximate MLE is the solution to a quadratic equation
int est_a_appr(int as,int bs,int cs, int f1, int f2)
{
int sx = 2*as + bs, sy = 2*as + cs, sz = 2*as+bs+cs;
double tmp = (double)f1*sy + (double)f2*sx;
return (int)((tmp-sqrt(tmp*tmp-8.0*f1*f2*as*sz))/sz/2.0);
}
// Newton?s method to solve for the exact MLE
int est_a_mle(int as,int bs, int cs, int ds, int f1, int f2,int D)
{
int a_min = MAX(as,ds+f1+f2-D), a_max = MIN(f1-bs,f2-cs);
int a1 = est_a_appr(as,bs,cs,f1,f2); // A good start
a1 = MAX( a_min, MIN(a1, a_max) ); // Sanity check
int k = 0, a = a1;
do {
a = a1;
double q = log(a+EPS) - log(a-as+EPS)
+log(f1-a-bs+1+EPS) - log(f1-a+1+EPS)
+log(f2-a-cs+1+EPS) - log(f2-a+1+EPS)
+log(D-f1-f2+a+EPS) - log(D-f1-f2-ds+a+EPS);
double dq = 1.0/(a+EPS)-1.0/(a-as+EPS)
-1.0/(f1-a-bs+1+EPS) + 1.0/(f1-a+1+EPS)
-1.0/(f2-a-cs+1+EPS) + 1.0/(f2-a+1+EPS)
-1.0/(D-f1-f2-ds+a+EPS) + 1.0/(D-f1-f2+a+EPS);
a1 = (int)(a - q/dq); a1 = MAX(a_min, MIN(a1,a_max));
if( ++k > MAX_ITER ) break;
}while( a1 != a );
return a;
}
Appendix 2: Sample Matlab Code for Estimating Multi-Way Associations
function test_program
% A short program for testing the multi-way association algorithm.
% First generate a random gold standard dataset. Then construct
% sketches by sampling a certain portion of the postings. Associations
% are estimated by the exact MLE as well as the margin-free (MF) method.
%
clear all;
m = max(2,ceil(rand*6)); % Number of words (random)
D = 1000*m; % Total number of documents
f = ceil(rand(m,1)*D/2); % document frequencies (random)
349
Computational Linguistics Volume 33, Number 3
P{1} = sort(randsample(D,f(1))); % Posting of the first word (random)
Pc = setdiff(1:D, P{1})?; % Compliment of the posting
% The postings of words 2 to m are randomly generated. 30% are
% sampled from the postings of word 1.
for i = 2:m
k = ceil(0.3*min(f(i),f(1)));
P{i} = sort([randsample(P{1},k);randsample(Pc,f(i)-k)]); % Postings
end
X = compute_intersection(P,D); % Gold standard associations
pc = 1; % Pseudo-count(pc), pc=0 for no smoothing, pc=1 for "add-one".
sampling_rate = 0.1;
for i = 1:m
k = ceil(sampling_rate*f(i));
K{i} = P{i}(1:k); % Sketches
end
% Estimate the associations and covariance matrices
[X_MLE, X_MF, Var_c, Var_o] = multi_way_est(K,f,D,pc);
% Display the estimations of associations
[X X_MLE X_MF] % [Gold standard, MLE, MF]
__________________________________________________
function [X_MLE, X_MF, Var_c, Var_o] = multi_way_est(K,f,D,pc);
% Matlab code for estimating multi-way associations
% K: Sketches (Cell array data type)
% f: Document frequencies, a column vector
% D: Total number of documents
% pc: Pseudo-count for smoothing.
% X_MLE: Maximum likelihood estimator (MLE), a column vector
% X_MF : Margin-free (MF) estimator, a column vector
% Var_c: Conditional (on Ds) covariance matrix, using the estimated X,
% Var_o: Covariance computed using the observed Fisher information
%
pc = max(pc,1e-4); % Always use a small pc for numerical stability.
m = length(K); % The order of associations, i.e., number of words.
[A,A1,A2,A3,ind1,ind2] = gen_A(m); % Margin constraint matrix
for i = 1:m;
last_elem(i) = K{i}(end);
end
Ds = min(last_elem);
for i = 1:m
K{i} = K{i}(find(K{i}<=Ds)); % Trim sketches according to D_s
end
S = compute_intersection(K,Ds); % Intersect the sketches to get samples
[X_MLE, X_MF] = newton_est(pc,S,Ds,D,A,f); % Estimate X
% Conditional variance
Z_c = 1./(X_MLE+eps); Z1_c = diag(Z_c(ind1)); Z2_c = diag(Z_c(ind2));
Var_c = inv(Z1_c + A3?*Z2_c*A3)*(D/Ds-1);
% Observed variance
Z_o = S./(X_MLE+eps).^2; Z1_o = diag(Z_o(ind1)); Z2_o = diag(Z_o(ind2));
Var_o = inv(Z1_o + A3?*Z2_o*A3)*(D-Ds)/D;
_________________________________________________________
function [X_MLE,X_MF] = newton_est(pc,S,Ds,D,A,f)
% Estimate multi-way associations by solving a convex
% optimization problem using the Newton?s method.
%
NEWTON_ERR = 0.001; % Threshold for termination.
MAX_ITER = 50; % Maximum allowed iteration.
N = length(S); m = length(f); F = [f;D];
pc = min(pc,(D-Ds)/N); % Adjust pc, if Ds is close to D.
% Solve a quadratic programming problem to find an initial
350
Li and Church Sketch for Estimating Associations
% guess of the MLE that minimizes the 2-norm with respect to
% the MF estimation and satisfies the constraints.
while(1)
X_MF = (S+pc)./(Ds+N*pc)*D; % Margin-free estimations.
[X0,dummy,flag] = quadprog(2*eye(2^m),-2*X_MF,[],[],A,F,S+pc);
if(flag == 1) break; end
pc = pc/2; % Occasionally need reduce pc for a feasible solution.
end
S = S + pc; X_MLE = X0; iter = 0;
while(1);
D1 = -S./(X_MLE+eps); % Gradient (first derivatives)
D2 = diag(S./(X_MLE.^2+eps)); % Hessian (second derivatives)
% Solve a linear system of equations for the Newton?s step.
M = [D2 A?; A zeros(size(A,1),size(A,1))];
dx = M\[-D1; zeros(size(A,1),1)]; dx = dx(1:size(D2,1));
lambda = (dx?*D2*dx)^0.5; % Measure of errors
iter = iter + 1;
if(iter>MAX_ITER | lambda^2/2<NEWTON_ERR) break; end
% Backtracking line search for a good Newton step size.
z = 1; Alpha = 0.1; Beta = 0.5; iter2 = 0;
while(min(X_MLE+z*dx-S)<0|S?*log(X_MLE./(X_MLE+z*dx))>=Alpha*z*D1?*dx);
if(iter2 >= MAX_ITER) break; end
z = Beta*z; iter2 = iter2 + 1;
end
X_MLE = X_MLE + z*dx;
end
_________________________________________________________
function S = compute_intersection(K,Ds);
% Compute the intersections to generate a table with N = 2^m
% cells. The cells are ordered in terms of the binary representation
% of integers from 0 to 2^m-1, where m is the number of words.
%
m = length(K); bin_rep = char(dec2bin(0:2^m-1)); S = zeros(2^m,1);
for i = 0:2^m-1;
if(bin_rep(i+1,1) == ?0?)
c{i+1} = K{1};
else
c{i+1} = setdiff([1:Ds]?,K{1});
end
for j = 2:m
if(bin_rep(i+1,j) == ?0?)
c{i+1} = intersect(c{i+1},K{j});
else
c{i+1} = setdiff(c{i+1},K{j});
end
end
S(i+1) = length(c{i+1});
end
_________________________________________________________
function [A,A1,A2,A3,ind1,ind2] = gen_A(m)
% Generate the margin constraint matrix and compute its decompositions
% for analyzing the covariance matrix
%
t1 = num2str(dec2bin(0:2^m-1)); t2 = zeros(2^m,m*2-1);
t2(:,1:2:end) = t1; t2(:,2:2:end) = ?,?;
A = xor(str2num(char(t2))?,1); A = [A;ones(1,2^m)];
for i = 1:size(A,1);
[last_one(i)] = max(find(A(i,:)==1));
end
ind1 = setdiff((1:size(A,2)),last_one); ind2 = last_one;
A1 = A(:,ind1); A2 = A(:,ind2); A3 = inv(A2)*A1;
351
Computational Linguistics Volume 33, Number 3
References
Achlioptas, Dimitris. 2003. Database-friendly
random projections: Johnson-Lindenstrauss
with binary coins. Journal of Computer and
System Sciences, 66(4):671?687.
Aggarwal, Charu C., Cecilia Magdalena
Procopiuc, Joel L. Wolf, Philip S. Yu, and
Jong Soo Park. 1999. Fast algorithms for
projected clustering. In SIGMOD,
pages 61?72, Philadelphia, PA.
Aggarwal, Charu C. and Joel L. Wolf. 1999.
A new method for similarity indexing
of market basket data. In SIGMOD,
pages 407?418, Philadelphia, PA.
Agrawal, Rakesh, Tomasz Imielinski, and
Arun Swami. 1993. Mining association
rules between sets of items in large
databases. In SIGMOD, pages 207?216,
Washington, DC.
Agrawal, Rakesh, Heikki Mannila,
Ramakrishnan Srikant, Hannu Toivonen,
and A. Inkeri Verkamo. 1996. Fast
discovery of association rules. In U. M.
Fayyad, G. Pratetsky-Shapiro, P. Smyth,
and R. Uthurusamy, editors. Advances in
Knowledge Discovery and Data Mining.
AAAI/MIT Press, pages 307?328,
Cambridge, MA.
Agrawal, Rakesh and Ramakrishnan Srikant.
1994. Fast algorithms for mining
association rules in large databases.
In VLDB, pages 487?499, Santiago
de Chile, Chile.
Agresti, Alan. 2002. Categorical Data Analysis.
John Wiley & Sons, Inc., Hoboken, NJ,
second edition.
Alon, Noga, Yossi Matias, and Mario
Szegedy. 1996. The space complexity of
approximating the frequency moments.
In STOC, pages 20?29, Philadelphia, PA.
Baeza-Yates, Ricardo and Berthier
Ribeiro-Neto. 1999. Modern Information
Retrieval. ACM Press, New York, NY.
Boyd, Stephen and Lieven Vandenberghe.
2004. Convex Optimization. Cambridge
University Press, Cambridge, UK.
Brin, Sergey, James Davis, and Hector
Garcia-Molina. 1995. Copy detection
mechanisms for digital documents. In
SIGMOD, pages 398?409, San Jose, CA.
Brin, Sergey and Lawrence Page. 1998. The
anatomy of a large-scale hypertextual web
search engine. In WWW, pages 107?117,
Brisbane, Australia.
Brin, Sergy, Rajeev Motwani, and Craig
Silverstein. 1997. Beyond market baskets:
Generalizing association rules to
correlations. In SIGMOD, pages 265?276,
Tucson, AZ.
Brin, Sergy, Rajeev Motwani, Jeffrey D.
Ullman, and Shalom Tsur. 1997. Dynamic
itemset counting and implication rules
for market basket data. In SIGMOD,
pages 265?276, Tucson, AZ.
Broder, Andrei Z. 1997. On the resemblance
and containment of documents. In The
Compression and Complexity of Sequences,
pages 21?29, Positano, Italy.
Broder, Andrei Z. 1998. Filtering
near-duplicate documents. In FUN, Isola
d?Elba, Italy.
Broder, Andrei Z., Moses Charikar, Alan M.
Frieze, and Michael Mitzenmacher. 1998.
Min-wise independent permutations
(extended abstract). In STOC,
pages 327?336, Dallas, TX.
Broder, Andrei Z., Moses Charikar, Alan M.
Frieze, and Michael Mitzenmacher. 2000.
Min-wise independent permutations.
Journal of Computer Systems and Sciences,
60(3):630?659.
Broder, Andrei Z., Steven C. Glassman,
Mark S. Manasse, and Geoffrey Zweig.
1997. Syntactic clustering of the
web. In WWW, pages 1157?1166,
Santa Clara, CA.
Charikar, Moses S. 2002. Similarity
estimation techniques from rounding
algorithms. In STOC, pages 380?388,
Montreal, Canada.
Chaudhuri Surajit, Rajeev Motwani, and
Vivek R. Narasayya. 1998. Random
sampling for histogram construction:
How much is enough? In SIGMOD,
pages 436?447, Seattle, WA.
Chaudhuri, Surajit, Rajeev Motwani, and
Vivek R. Narasayya. 1999. On random
sampling over joins. In SIGMOD,
pages 263?274, Philadelphia, PA.
Chen, Bin, Peter Haas, and Peter
Scheuermann. 2002. New two-phase
sampling based algorithm for discovering
association rules. In KDD, pages 462?468,
Edmonton, Canada.
Church, Kenneth and Patrick Hanks. 1991.
Word association norms, mutual
information and lexicography.
Computational Linguistics, 16(1):22?29.
Cover, Thomas M. and Joy A. Thomas. 1991.
Elements of Information Theory. John Wiley
& Sons, Inc., New York, NY.
David, Herbert A. 1981. Order Statistics.
John Wiley & Sons, Inc., New York, NY,
second edition.
Deming, W. Edwards and Frederick F.
Stephan. 1940. On a least squares
adjustment of a sampled frequency table
when the expected marginal totals are
352
Li and Church Sketch for Estimating Associations
known. The Annals of Mathematical
Statistics, 11(4):427?444.
Drineas, Petros and Michael W. Mahoney.
2005. Approximating a gram matrix for
improved kernel-based learning. In COLT,
pages 323?337, Bertinoro, Italy.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Etzioni, Oren, Michael Cafarella, Doug
Downey, Stanley Kok, Ana-Maria Popescu,
Tal Shaked, Stephen Soderland, Daniel S.
Weld, and Alexander Yates. 2004.
Web-scale information extraction in
knowitall (preliminary results).
In WWW, pages 100?110, New York, NY.
Garcia-Molina, Hector, Jeffrey D. Ullman,
and Jennifer Widom. 2002. Database
Systems: The Complete Book. Prentice Hall,
New York, NY.
Gilbert, Anna C., Yannis Kotidis,
S. Muthukrishnan, and Martin J. Strauss.
2003. One-pass wavelet decompositions
of data streams. IEEE Transactions on
Knowledge and Data Engineering,
15(3):541?554.
Guha Sudipto, Rajeev Rastogi, and Kyuseok
Shim. 1998. Cure: An efficient clustering
algorithm for large databases. In SIGMOD,
pages 73?84, Seattle, WA.
Hastie, T., R. Tibshirani, and J. Friedman.
2001. The Elements of Statistical Learning:
Data Mining, Inference, and Prediction.
Springer, New York, NY.
Haveliwala, Taher H., Aristides Gionis, and
Piotr Indyk. 2000. Scalable techniques
for clustering the Web. In WebDB,
pages 129?134, Dallas, TX.
Haveliwala, Taher H., Aristides Gionis,
Dan Klein, and Piotr Indyk. 2002.
Evaluating strategies for similarity search
on the web. In WWW, pages 432?442,
Honolulu, HI.
Hidber, Christian. 1999. Online association
rule mining. In SIGMOD, pages 145?156,
Philadelphia, PA.
Hornby, Albert Sydney, editor. 1989. Oxford
Advanced Learner?s Dictionary of Current
English. Oxford University Press, Oxford,
UK, fourth edition.
Indyk, Piotr. 2001. A small approximately
min-wise independent family of hash
functions. Journal of Algorithm, 38(1):84?90.
Indyk, Piotr and Rajeev Motwani. 1998.
Approximate nearest neighbors: Towards
removing the curse of dimensionality.
In STOC, pages 604?613, Dallas, TX.
Itoh, Toshiya, Yoshinori Takei, and Jun Tarui.
2003. On the sample size of k-restricted
min-wise independent permutations
and other k-wise distributions. In STOC,
pages 710?718, San Diego, CA.
Lehmann, Erich L. and George Casella. 1998.
Theory of Point Estimation. Springer,
New York, NY, second edition.
Li, Ping. 2006. Very sparse stable random
projections, estimators and tail bounds
for stable random projections.
Technical report, available from
http://arxiv.org/PS cache/cs/pdf/
0611/0611114v2.pdf.
Li, Ping and Kenneth W. Church. 2005.
Using sketches to estimate two-way and
multi-way associations. Technical Report
TR-2005-115, Microsoft Research,
Redmond, WA, September.
Li, Ping, Kenneth W. Church, and Trevor J.
Hastie. 2006. Conditional random
sampling: A sketched-based sampling
technique for sparse data. Technical Report
2006-08, Department of Statistics, Stanford
University.
Li, Ping, Kenneth W. Church, and Trevor J.
Hastie. 2007. Conditional random
sampling: A sketch-based sampling
technique for sparse data. In NIPS,
pages 873?880. Vancouver, BC, Canada.
Li, Ping, Trevor J. Hastie, and Kenneth W.
Church. 2006a. Improving random
projections using marginal information.
In COLT, pages 635?649, Pittsburgh, PA.
Li, Ping, Trevor J. Hastie, and Kenneth W.
Church. 2006b. Very sparse random
projections. In KDD, pages 287?296,
Philadelphia, PA.
Li, Ping, Trevor J. Hastie, and Kenneth W.
Church. 2007. Nonlinear estimators
and tail bounds for dimensional
reduction in l1 using Cauchy random
projections. In COLT, pages 514?529,
San Diego, CA.
Manku, Gurmeet Singh, Sridhar
Rajagopalan, and Bruce G. Lindsay.
1999. Random sampling techniques
for space efficient online computation
of order statistics of large datasets.
In SIGCOMM, pages 251?262,
Philadelphia, PA.
Manning, Chris D. and Hinrich Schutze.
1999. Foundations of Statistical Natural
Language Processing. The MIT Press,
Cambridge, MA.
Matias, Yossi, Jeffrey Scott Vitter, and Min
Wang. 1998. Wavelet-based histograms
for selectivity estimation. In SIGMOD,
pages 448?459, Seattle, WA.
Moore, Robert C. 2004. On log-likelihood-
ratios and the significance of rare events.
353
Computational Linguistics Volume 33, Number 3
In EMNLP, pages 333?340, Barcelona,
Spain.
Pearsall, Judy, editor. 1998. The New Oxford
Dictionary of English. Oxford University
Press, Oxford, UK.
Ravichandran, Deepak, Patrick Pantel,
and Eduard Hovy. 2005. Randomized
algorithms and NLP: Using locality
sensitive hash function for high speed
noun clustering. In ACL, pages 622?629,
Ann Arbor, MI.
Rosen, Bengt. 1972a. Asymptotic theory
for successive sampling with varying
probabilities without replacement, I.
The Annals of Mathematical Statistics,
43(2):373?397.
Rosen, Bengt. 1972b. Asymptotic theory
for successive sampling with varying
probabilities without replacement, II.
The Annals of Mathematical Statistics,
43(3):748?776.
Salton, Gerard. 1989. Automatic Text
Processing: The Transformation, Analysis, and
Retrieval of Information by Computer.
Addison-Wesley, New York, NY.
Stephan, Frederick F. 1942. An iterative
method of adjusting sample frequency
tables when expected marginal totals are
known. The Annals of Mathematical
Statistics, 13(2):166?178.
Strehl, Alexander and Joydeep Ghosh.
2000. A scalable approach to balanced,
high-dimensional clustering of
market-baskets. In HiPC, pages 525?536,
Bangalore, India.
Toivonen, Hannu. 1996. Sampling large
databases for association rules. In VLDB,
pages 134?145, Bombay, India.
Vempala, Santosh. 2004. The Random
Projection Method. American Mathematical
Society, Providence, RI.
Witten, Ian H., Alstair Moffat, and
Timothy C. Bell. 1999. Managing Gigabytes:
Compressing and Indexing Documents and
Images. Morgan Kaufmann Publishing,
San Francisco, CA, second edition.
354
