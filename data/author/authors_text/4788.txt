Perceptron Learning for Chinese Word Segmentation
Yaoyong Li?, Chuanjiang Miao?, Kalina Bontcheva?, Hamish Cunningham?
?Department of Computer Science, The University of Sheffield, Sheffield, S1 4DP, UK
{yaoyong,kalina,hamish}@dcs.shef.ac.uk
?Institute of Chinese Information Processing, Beijing Normal University, Beijing, 100875, China
miaochj@bnu.edu.cn
Abstract
We explored a simple, fast and effective
learning algorithm, the uneven margins
Perceptron, for Chinese word segmen-
tation. We adopted the character-based
classification framework and trans-
formed the task into several binary clas-
sification problems. We participated
the close and open tests for all the four
corpora. For the open test we only used
the utf-8 code knowledge for discrimi-
nation among Latin characters, Arabic
numbers and all other characters. Our
system performed well on the as, cityu
and msr corpora but was clearly worse
than the best result on the pku corpus.
1 Introduction
We participated in the closed and open tests for
all the four corpora, referred to as, cityu, msr and
pku, respectively. We adopted the character-based
methodology for Chinese word segmentation, that
processed text character by character. We ex-
plored a simple and effective learning algorithm,
the Perceptron with Uneven Margins (PAUM) for
Chinese word segmentation task.
For the open task, we only used the minimal ex-
ternal information ? the utf-8 code knowledge to
distinguish Latin characters and Arabic numbers
from other characters, justified by the fact that
the English text requires no segmentation since
they has been segmented already, and another fact
that any Arabic number in one particular context
should have the same segmentation.
2 Character Based Chinese Word
Segmentation
We adopted the character based methodology for
Chinese word segmentation, in which every char-
acter in a sentence was checked one by one to
see if it was a word on its own or it was begin-
ning, middle, or end character of a multi-character
word. In contrast, another commonly used strat-
egy, the word based methodology segments a Chi-
nese sentence into the words in a pre-defined
word list possibly with probability information
about each word, according to some maximum
probability criteria ( see e.g. Chen (2003)). The
performance of word based segmentation is de-
pendent upon the quality of word list used, while
the character based method does not need any
word list ? it segments a sentence only based on
the characters in the sentence.
Using character based methodology, we trans-
form the word segmentation problem into four
binary classification problems, corresponding to
single-character word, the beginning, middle and
end character of multi-character word, respec-
tively. For each of the four classes a classifier was
learnt from training set using the one vs. all others
paradigm, in which every character in the train-
ing data belonging to the class considered was re-
garded as positive example and all other charac-
ters were negative examples.
After learning, we applied the four classifiers to
each character in test text and assigned the char-
acter the class which classifier had the maximal
output among the four. This kind of strategy has
been widely used in the applications of machine
learning to named entity recognition and has also
154
been used in Chinese word segmentation (Xue
and Shen, 2003). Finally a word delimiter (often a
blank space, depending on particular corpus) was
added to the right of one character if it was not the
last character of a sentence and it was predicted
as end character of word or as a single character
word.
3 Learning Algorithm
Perceptron is a simple and effective learning al-
gorithm. For a binary classification problem, it
checks the training examples one by one by pre-
dicting their labels. If the prediction is correct,
the example is passed; otherwise, the example is
used to correct the model. The algorithm stops
when the model classifies all training examples
correctly. The margin Perceptron not only classi-
fies every training example correctly but also out-
puts for every training example a value (before
thresholding) larger than a predefined parameter
(margin). The margin Perceptron has better gen-
eralisation capability than the standard Percep-
tron. Li et al (2002) proposed the Perceptron al-
gorithm with uneven margins (PAUM) by intro-
ducing two margin parameters ?+ and ?? into the
update rules for the positive and negative exam-
ples, respectively. Two margin parameters allow
the PAUM to handle imbalanced datasets better
than both the standard Perceptron and the margin
Perceptron. PAUMhas been successfully used for
document classification and information extrac-
tion (Li et al, 2005).
We used the PAUM algorithm to train a clas-
sifier for each of four classes for Chinese word
segmentation. For one test example, the output of
the Perceptron classifier before thresholding was
used for comparison among the four classifiers.
The important parameters of the learning algo-
rithm are the uneven margins parameters ?+ and
??. In all our experiments ?+ = 20 and ?? = 1
were used.
Table 1 presents the results for each of the
four classification problems, obtained from 4-fold
cross-validation on training set. Not surprisingly,
the classification for middle character of multi-
character word was much harder than other three
classification problems, since middle character of
Chinese word is less characteristic than beginning
or end character or single-character word. On the
other hand, improvement on the classification for
middle character, while keeping the performances
of other classification, would improve the overall
performance of segmentation.
Table 1: Results for each of the four classifiers:
F1 (%) averaged over 4-fold cross-validation on
training sets of the four corpora. C1, C2 and
C3 refer to the classifier for beginning, middle
and end character of multi-character word, re-
spectively, and C4 refers to the classifier for single
character word.
C1 C2 C3 C4
as 95.64 90.07 95.47 95.27
cityu 96.64 90.06 96.43 95.14
msr 96.36 89.79 96.00 94.99
pku 96.09 89.99 96.18 94.12
Support vector machines (SVM) is a popular
learning algorithm, which has been successfully
applied to many classification problems in natural
language processing. Similar to the PAUM, SVM
is a maximal margin algorithm. Table 2 presents
a comparison of performances and computation
times between the PAUM and the SVM with lin-
ear kernel1 on three subsets of cityu corpora with
different sizes. The performance of SVM was
better than the PAUM. However, the larger the
training data was, the closer the performance of
PAUM to that of SVM. On the other hand, SVM
took much longer computation time than PAUM.
As a matter of fact, we have run the SVM with
linear kernel on the whole cityu training corpus
using 4-fold cross-validation for one month and it
has not finished yet. In contrast, PAUM just took
about one hour to run the same experiment.
4 Features for Each Character
In our system every character was regarded as
one instance for classification. The features for
one character were the character form itself and
the character forms of the two preceding and
the two following characters of the current one.
In other word, the features for one character c0
were the character forms from a context win-
1The SVMlight package version 5.0, available from
http://svmlight.joachims.org/, was used to learn the SVM
classifiers in our experiments.
155
Table 2: Comparison of the Perceptron with SVM
for Chinese word segmentation: averaged F1 (%)
over the 4-fold cross-validation on three subsets
of cityu corpus and the computation time (in sec-
ond) for each experiment. The three subsets have
100, 1000 and 5000 sentences, respectively.
100 1000 5000
PAUM 73.55 78.00 88.08
4s 14s 92s
SVM 75.50 79.15 88.78
227s 3977s 49353s
dow centering at c0 and containing five char-
acters {c?2, c?1, c0, c1, c2} in a sentence. Our
experiments on training data showed that co-
occurrences of characters in the context win-
dow were helpful. Taking account of all co-
occurrences of characters in context window is
equivalent to using a quadratic kernel in Percep-
tron, while not using any co-occurrence amounts
to a linear kernel. Actually we can only use part
of co-occurrences as features, which can be re-
garded as some kind of semi-quadratic kernel.
Table 3 compares the three types of ker-
nel for Perceptron, where for the semi-
quadratic kernel we used the co-occurrences
of characters in context window as those
used in (Xue and Shen, 2003), namely
{c?2c?1, c?1c0, c0c1, c1c2, c?1c1}. It was
shown that the quadratic kernel gave much better
results than linear kernel and the semi-quadratic
kernel was slightly better than fully quadratic ker-
nel. Semi-quadratic kernel also led to less feature
and less computation time than fully quadratic
kernel. Therefore, this kind of semi-quadratic
kernel was used in our submissions.
Table 3: Comparisons between different kernels
for Perceptron: F1 (%) averaged over 4-fold
cross-validation on three training sets.
linear quadratic semi-quadratic
cityu 81.30 94.78 95.13
msr 79.80 94.78 94.93
pku 82.33 94.80 95.05
Actually it has been noted that quadratic ker-
nel for Perceptron, as well as for SVM, per-
formed better than linear kernel for informa-
tion extraction and other NLP tasks (see e.g.
Carreras et al (2003)). However, quadratic ker-
nel was usually implemented in dual form for
Perceptron and it took very long time for train-
ing. We implemented the quadratic kernel for
Perceptron in primal form by encoding the linear
and quadratic features into feature vector explic-
itly. Actually our implementation performed even
slightly better than the Perceptron with quadratic
kernel as we used only part of quadratic features,
and it was still as efficient as the Perceptron with
linear kernel.
5 Open Test
While closed test required the participants only to
use the information presented in training material,
open test allowed to use any external information
or resources besides the training data. In our sub-
missions for the open test we just used the min-
imal external information, namely the utf-8 code
knowledge for identifying a piece of English text
or an Arabic number. and What we did by us-
ing this kind of knowledge was to pre-process the
text by replacing each piece of English text with
a symbol ?E? and replacing every Arabic num-
ber with another symbol ?N?. This kind of pre-
processing resulted in a smaller training data and
less computation time and yet slightly better per-
formance on training data, as shown in Table 4
which compares the results of collapsing the En-
glish text only and collapsing both the English
text and Arabic number with those for closed test.
Table 4 also presents the 95% confidence intervals
for the F-measures.
6 Results on Test Data
Table 5 presents our official results on test corpora
for both close and open tests. First, comparing
with the results in Table 4, the results on test set
are significantly different from the result using 4-
fold cross validation on training set for all the four
corpora. The test result was better than the results
on training set for the msr corpus but was worse
for other three corpora, especially for the pku cor-
pora. We suspected that this may be caused by
difference between training and test data, which
needs further investigation.
156
Table 4: Comparisons between the results for
close and open tests: averaged F1 (%) and the
95% confidence interval on the 4-fold cross-
validation on the training sets of four corpora and
the computation time (in hour) for each experi-
ment. ?English? means only collapsing English
texts and ?E & N? means collapsing both English
texts and Arabic numbers.
close test English E & N
as 95.53?0.46 95.65?0.47 95.78?0.46
8.88h 7.66h 7.07h
cityu 95.13 ?1.49 95.25 ?1.48 95.25 ?1.48
1.03h 0.86h 0.82h
msr 94.92 ?0.36 94.98 ?0.40 95.00 ?0.39
2.62h 1.69h 1.62h
pku 95.05 ?0.43 95.08 ?0.36 95.15 ?0.46
0.70h 0.63h 0.60h
Secondly, the test results for close and open
tests are close to each other on other three corpora
except the pku corpora, for which the result for
open test is clearly better than that for close test.
This was mainly because of different encoding
of Arabic number in training and test sets of the
pku corpus. Since Arabic number was encoded in
three bytes in training set but was encoded in one
byte in test set for the pku corpora, for close test
the trained model for Arabic number was not ap-
plicable to the Arabic numbers in test set. How-
ever, for open test, as we replaced Arabic num-
ber with one symbol in both training and test sets,
the different encoding of Arabic number in train-
ing and test sets could not cause any problem at
all, which led to better result. On the other hand,
our pre-processing with respect to the English text
and Arabic numbers seemed have slightly effect
on the F-measure for other three corpora.
Finally, comparing with the results of closed
test from other participants, our F1 figures were
no more than 0.008 lower than the best ones on
the as, cityu and msr corpora, but was 0.023 lower
than the best one on the pku corpus.
7 Conclusion
We applied the uneven margins Perceptron to Chi-
nese word segmentation. The learning algorithm
is simple, fast and effective. The results obtained
Table 5: The official results on test set: F-measure
(%) for close and open tests, respectively.
as cityu msr pku
close 94.4 93.6 95.6 92.7
open 94.8 93.6 95.4 93.8
are encouraging.
The performance of Perceptron was close to
that of the SVM on Chinese word segmentation
for large training data. On the other hand, the
Perceptron took much less computation time than
SVM.We implemented the Perceptron with semi-
quadratic kernel in primal form. Our implemen-
tation was both effective and efficient.
Our system performed well for the three of four
corpora, as, cityu and msr corpora. But it was
significantly worse than the best result on the pku
corpora, which needs further investigation.
Acknowledgements
This work is supported by the EU-funded SEKT
project (http://www.sekt-project.org).
References
X. Carreras, L. Ma`rquez, and L. Padro?. 2003. Learn-
ing a perceptron-based named entity chunker via
online recognition feedback. In Proceedings of
CoNLL-2003, pages 156?159. Edmonton, Canada.
A. Chen. 2003. Chinese Word Segmentation Using
Minimal Linguistic Knowledge. In Proceedings of
the 2nd SIGHAN Workshop on Chinese Language
Processing.
Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and
J. Kandola. 2002. The Perceptron Algorithm with
Uneven Margins. In Proceedings of the 9th Inter-
national Conference on Machine Learning (ICML-
2002), pages 379?386.
Y. Li, K. Bontcheva, and H. Cunningham. 2005. Us-
ing UnevenMargins SVM and Perceptron for Infor-
mation Extraction. In Proceedings of Ninth Confer-
ence on ComputationalNatural Language Learning
(CoNLL-2005).
N. Xue and L. Shen. 2003. Chinese Word Segmen-
tation as LMR Tagging. In Proceedings of the 2nd
SIGHAN Workshop on Chinese Language Process-
ing.
157
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 72?79, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Using Uneven Margins SVM and Perceptron for Information Extraction
Yaoyong Li, Kalina Bontcheva and Hamish Cunningham
Department of Computer Science, The University of Shefeld, Shefeld, S1 4DP, UK
{yaoyong,kalina,hamish}@dcs.shef.ac.uk
Abstract
The classification problem derived from
information extraction (IE) has an imbal-
anced training set. This is particularly
true when learning from smaller datasets
which often have a few positive training
examples and many negative ones. This
paper takes two popular IE algorithms ?
SVM and Perceptron ? and demonstrates
how the introduction of an uneven margins
parameter can improve the results on im-
balanced training data in IE. Our experi-
ments demonstrate that the uneven margin
was indeed helpful, especially when learn-
ing from few examples. Essentially, the
smaller the training set is, the more bene-
ficial the uneven margin can be. We also
compare our systems to other state-of-the-
art algorithms on several benchmarking
corpora for IE.
1 Introduction
Information Extraction (IE) is the process of auto-
matic extraction of information about pre-specified
types of events, entities or relations from text such
as newswire articles or Web pages. IE is useful in
many applications, such as information gathering in
a variety of domains, automatic annotations of web
pages for Semantic Web, and knowledge manage-
ment.
A wide range of machine learning techniques
have been used for IE and achieved state-of-the-art
results, comparable to manually engineered IE sys-
tems. A learning algorithm usually learns a model
from a set of documents which have been manually
annotated by the user. Then the model can be used
to extract information from new documents. Manual
annotation is a time-consuming process. Hence, in
many cases learning from small data sets is highly
desirable. Therefore in this paper we also evaluate
the performance of our algorithms on small amounts
of training data and show their learning curve.
The learning algorithms for IE can be classified
broadly into two main categories: rule learning and
statistical learning. The former induces a set of
rules from training examples. There are many rule
based learning systems, e.g. SRV (Freitag, 1998),
RAPIER (Califf, 1998), WHISK (Soderland, 1999),
BWI (Freitag and Kushmerick, 2000), and (LP )2
(Ciravegna, 2001). Statistical systems learn a statis-
tical model or classifiers, such as HMMs (Freigtag
and McCallum, 1999), Maximal Entropy (Chieu and
Ng., 2002), the SVM (Isozaki and Kazawa, 2002;
Mayfield et al, 2003), and Perceptron (Carreras et
al., 2003). IE systems also differ from each other
in the NLP features that they use. These include
simple features such as token form and capitalisa-
tion information, linguistic features such as part-of-
speech, semantic information from gazetteer lists,
and genre-specific information such as document
structure. In general, the more features the system
uses, the better performance it can achieve.
This paper concentrates on classifier-based learn-
ing for IE, which typically converts the recognition
of each information entity into a set of classification
problems. In the framework discussed here, two bi-
nary classifiers are trained for each type of informa-
tion entity. One classifier is used for recognising the
entity?s start token and the other ? the entity?s end
token.
72
The classification problem derived from IE usu-
ally has imbalanced training data, in which positive
training examples are vastly outnumbered by neg-
ative ones. This is particularly true for smaller data
sets where often there are hundreds of negative train-
ing examples and only few positive ones. Two ap-
proaches have been studied so far to deal with imbal-
anced data in IE. One approach is to under-sample
majority class or over-sample minority class in order
to obtain a relatively balanced training data (Zhang
and Mani, 2003). However, under-sampling can
potentially remove certain important examples, and
over-sampling can lead to over-fitting and a larger
training set. Another approach is to divide the prob-
lem into several sub-problems in two layers, each of
which has less imbalanced training set than the orig-
inal one (Carreras et al, 2003; Sitter and Daelemans,
2003). The output of the classifier in the first layer is
used as the input to the classifiers in the second layer.
As a result, this approach needs more classifiers than
the original problem. Moreover, the classification
errors in the first layer will affect the performance of
the second one.
In this paper we explore another approach to han-
dle the imbalanced data in IE, namely, adapting
the learning algorithms for balanced classification to
imbalanced data. We particularly study two popular
classification algorithms in IE, Support Vector Ma-
chines (SVM) and Perceptron.
SVM is a general supervised machine learning
algorithm, that has achieved state of the art per-
formance on many classification tasks, including
NE recognition. Isozaki and Kazawa (2002) com-
pared three commonly used methods for named en-
tity recognition ? the SVM with quadratic kernel,
maximal entropy method, and a rule based learning
system, and showed that the SVM-based system per-
formed better than the other two. Mayfield et al
(2003) used a lattice-based approach to named en-
tity recognition and employed the SVM with cubic
kernel to compute transition probabilities in a lattice.
Their results on CoNLL2003 shared task were com-
parable to other systems but were not the best ones.
Previous research on using SVMs for IE adopts
the standard form of the SVM, which treats posi-
tive and negative examples equally. As a result, they
did not consider the difference between the balanced
classification problems, where the SVM performs
quite well, and the imbalanced ones. Li and Shawe-
Taylor (2003) proposes an uneven margins version
of the SVM and shows that the SVM with uneven
margins performs significantly better than the stan-
dard SVM on document classification problems with
imbalanced training data. Since the classification
problem for IE is also imbalanced, this paper inves-
tigates the SVM with uneven margins for IE tasks
and demonstrates empirically that the uneven mar-
gins SVM does have better performance than the
standard SVM.
Perceptron is a simple, fast and effective learn-
ing algorithm, which has successfully been applied
to named entity recognition (Carreras et al, 2003).
The system uses a two-layer structure of classifiers
to handle the imbalanced data. The first layer clas-
sifies each word as entity or non-entity. The second
layer classifies the named entities identified by the
first layer in the respective entity classes. Li et al
(2002) proposed another variant of Perceptron, the
Perceptron algorithm with uneven margins (PAUM),
designed especially for imbalanced data. In this pa-
per we explore the application of PAUM to IE.
The rest of the paper is structured as follows. Sec-
tion 2 describes the uneven margins SVM and Per-
ceptron algorithms. Sections 3.1 and 3.2 discuss
the classifier-based framework for IE and the exper-
imental datasets we used, respectively. We compare
our systems to other state-of-the-art systems on three
benchmark datasets in Section 3.3. Section 3.4 dis-
cusses the effects of the uneven margins parameter
on the SVM and Perceptron?s performances. Finally,
Section 4 provides some conclusions.
2 Uneven Margins SVM and Perceptron
Li and Shawe-Taylor (2003) introduced an uneven
margins parameter into the SVM to deal with imbal-
anced classification problems. They showed that the
SVM with uneven margins outperformed the stan-
dard SVM on document classification problem with
imbalanced training data. Formally, given a training
set Z = ((x1, y1), . . . , (xm, ym)),where xi is the n-
dimensional input vector and yi (= +1 or ?1) its
label, the SVM with uneven margins is obtained by
solving the quadratic optimisation problem:
minw, b, ? ?w,w? + C
m
?
i=1
?i
73
s.t. ?w,xi? + ?i + b ? 1 if yi = +1
?w,xi? ? ?i + b ? ?? if yi = ?1
?i ? 0 for i = 1, ...,m
We can see that the uneven margins parameter
? was added to the constraints of the optimisation
problem. ? is the ratio of negative margin to the
positive margin of the classifier and is equal to 1 in
the standard SVM. For an imbalanced dataset with
a few positive examples and many negative ones, it
would be beneficial to use larger margin for positive
examples than for the negative ones. Li and Shawe-
Taylor (2003) also showed that the solution of the
above problem could be obtained by solving a re-
lated standard SVM problem by, for example, using
a publicly available SVM package1 .
Perceptron is an on-line learning algorithm for
linear classification. It checks the training exam-
ples one by one by predicting their labels. If the
prediction is correct, the example is passed; other-
wise, the example is used to correct the model. The
algorithm stops when the model classifies all train-
ing examples correctly. The margin Perceptron not
only classifies every training example correctly but
also outputs for every training example a value (be-
fore thresholding) larger than a predefined parameter
(margin). The margin Perceptron has better general-
isation capability than the standard Perceptron. Li
et al (2002) proposed the Perceptron algorithm with
uneven margins (PAUM) by introducing two margin
parameters ?+ and ?? into the updating rules for the
positive and negative examples, respectively. Sim-
ilar to the uneven margins parameter in SVM, two
margin parameters allow the PAUM to handle im-
balanced datasets better than both the standard Per-
ceptron and the margin Perceptron. Additionally, it
is known that the Perceptron learning will stop after
limited loops only on a linearly separable training
set. Hence, a regularisation parameter ? is used in
PAUM to guarantee that the algorithm would stop
for any training dataset after some updates. PAUM
is simple and fast and performed very well on doc-
ument classification, in particularly on imbalanced
training data.
1The SVMlight package version 3.5, available from
http://svmlight.joachims.org/, was used to learn the SVM clas-
sifiers in our experiments.
3 Experiments
3.1 Classifier-Based Framework for IE
In the experiments we adopted a classifier-based
framework for applying the SVM and PAUM algo-
rithms to IE. The framework consists of three stages:
pre-processing of the documents to obtain feature
vectors, learning classifiers or applying classifiers to
test documents, and finally post-processing the re-
sults to tag the documents.
The aim of the preprocessing is to form input vec-
tors from documents. Each document is first pro-
cessed using the open-source ANNIE system, which
is part of GATE2 (Cunningham et al, 2002). This
produces a number of linguistic (NLP) features, in-
cluding token form, capitalisation information, to-
ken kind, lemma, part-of-speech (POS) tag, seman-
tic classes from gazetteers, and named entity types
according to ANNIE?s rule-based recogniser.
Based on the linguistic information, an input
vector is constructed for each token, as we iter-
ate through the tokens in each document (includ-
ing word, number, punctuation and other symbols)
to see if the current token belongs to an information
entity or not. Since in IE the context of the token is
usually as important as the token itself, the features
in the input vector come not only from the current
token, but also from preceding and following ones.
As the input vector incorporates information from
the context surrounding the current token, features
from different tokens can be weighted differently,
based on their position in the context. The weight-
ing scheme we use is the reciprocal scheme, which
weights the surrounding tokens reciprocally to the
distance to the token in the centre of the context
window. This reflects the intuition that the nearer
a neighbouring token is, the more important it is
for classifying the given token. Our experiments
showed that such a weighting scheme obtained bet-
ter results than the commonly used equal weighting
of features (Li et al, 2005).
The key part of the framework is to convert the
recognition of information entities into binary clas-
sification tasks ? one to decide whether a token is the
start of an entity and another one for the end token.
After classification, the start and end tags of the
2Available from http://www.gate.ac.uk/
74
entities are obtained and need to be combined into
one entity tag. Therefore some post-processing
is needed to guarantee tag consistency and to try
to improve the results by exploring other informa-
tion. The currently implemented procedure has three
stages. First, in order to guarantee the consistency
of the recognition results, the document is scanned
from left to right to remove start tags without match-
ing end tags and end tags without preceding start
tags. The second stage filters out candidate enti-
ties from the output of the first stage, based on their
length. Namely, a candidate entity tag is removed
if the entity?s length (i.e., the number of tokens) is
not equal to the length of any entity of the same type
in the training set. The third stage puts together all
possible tags for a sequence of tokens and chooses
the best one according to the probability which was
computed from the output of the classifiers (before
thresholding) via a Sigmoid function.
3.2 The Experimental Datasets
The paper reports evaluation results on three corpora
covering different IE tasks ? named entity recogni-
tion (CoNLL-2003) and template filling or scenario
templates in different domains (Jobs and CFP). The
CoNLL-20033 provides the most recent evaluation
results of many learning algorithms on named entity
recognition. The Jobs corpus4 has also been used re-
cently by several learning systems. The CFP corpus
was created as part of the recent Pascal Challenge
for evaluation of machine learning methods for IE5.
In detail, we used the English part of the CoNLL-
2003 shared task dataset, which consists of 946 doc-
uments for training, 216 document for development
(e.g., tuning the parameters in learning algorithm),
and 231 documents for evaluation (i.e., testing), all
of which are news articles taken from the Reuters
English corpus (RCV1). The corpus contains four
types of named entities ? person, location, organ-
isation and miscellaneous names. In the other two
corpora domain-specific information was extracted
into a number of slots. The Job corpus includes 300
computer related job advertisements and 17 slots en-
coding job details, such as title, salary, recruiter,
computer language, application, and platform. The
3See http://cnts.uia.ac.be/conll2003/ner/
4See http://www.isi.edu/info-agents/RISE/repository.html.
5See http://nlp.shef.ac.uk/pascal/.
CFP corpus consists of 1100 conference or work-
shop call for papers (CFP), of which 600 were anno-
tated. The corpus includes 11 slots such as work-
shop and conference names and acronyms, work-
shop date, location and homepage.
3.3 Comparison to Other Systems
Named Entity Recognition The algorithms are
evaluated on the CoNLL-2003 dataset. Since this set
comes with development data for tuning the learning
algorithm, different settings were tried in order to
obtain the best performance on the development set.
Different SVM kernel types, window sizes (namely
the number of tokens in left or right side of the token
at the centre of window), and the uneven margins
parameter ? were tested. We found that quadratic
kernel, window size 4 and ? = 0.5 produced best
results on the development set. These settings were
used in all experiments on the CoNLL-2003 dataset
in this paper, unless otherwise stated. The parameter
settings for PAUM described in Li et al (2002), e.g.
?+ = 50, ?? = 1, were adopted in all experiments
with PAUM, unless otherwise stated.
Table 1 presents the results of our system using
three learning algorithms, the uneven margins SVM,
the standard SVM and the PAUM on the CONLL-
2003 test set, together with the results of three
participating systems in the CoNLL-2003 shared
task: the best system (Florian et al, 2003), the
SVM-based system (Mayfield et al, 2003) and the
Perceptron-based system (Carreras et al, 2003).
Firstly, our uneven margins SVM system per-
formed significantly better than the other SVM-
based system. As the two systems are different from
each other in not only the SVM models used but
also other aspects such as the NLP features and the
framework, in order to make a fair comparison be-
tween the uneven margins SVM and the standard
SVM, we also present the results of the two learning
algorithms implemented in our framework. We can
see from Table 1 that, under the same experimental
settings, the uneven margins SVM again performed
better than the standard SVM.
Secondly, our PAUM-based system performed
slightly better than the system based on voted Per-
ceptron, but there is no significant difference be-
tween them. Note that they adopted different mech-
anisms to deal with the imbalanced data in IE (refer
75
Table 1: Comparison to other systems on CoNLL-2003 corpus: F -measure(%) on each entity type and the
overall micro-averaged F-measure. The 90% confidence intervals for results of other three systems are also
presented. The best performance figures for each entity type and overall appear in bold.
System LOC MISC ORG PER Overall
Our SVM with uneven margins 89.25 77.79 82.29 90.92 86.30
Systems Standard SVM 88.86 77.32 80.16 88.93 85.05
PAUM 88.18 76.64 78.26 89.73 84.36
Participating Best one 91.15 80.44 84.67 93.85 88.76(?0.7)
Systems Another SVM 88.77 74.19 79.00 90.67 84.67(?1.0)
Voted Perceptron 87.88 77.97 80.09 87.31 84.30(?0.9)
to Section 1). The structure of PAUM system is sim-
pler than that of the voted Perceptron system.
Finally, the PAUM system performed worse than
the SVM system. On the other hand, training time
of PAUM is only 1% of that for the SVM and the
PAUM implementation is much simpler than that of
SVM. Therefore, when simplicity and speed are re-
quired, PAUM presents a good alternative.
Template Filling On Jobs corpus our systems
are compared to several state-of-the-art learning sys-
tems, which include the rule based systems Rapier
(Califf, 1998), (LP )2 (Ciravegna, 2001) and BWI
(Freitag and Kushmerick, 2000), the statistical sys-
tem HMM (Freitag and Kushmerick, 2000), and the
double classification system (Sitter and Daelemans,
2003). In order to make the comparison as informa-
tive as possible, the same settings are adopted in our
experiments as those used by (LP )2, which previ-
ously reported the highest results on this dataset. In
particular, the results are obtained by averaging the
performance in ten runs, using a random half of the
corpus for training and the rest for testing. Only ba-
sic NLP features are used: token form, capitalisation
information, token types, and lemmas.
Preliminary experiments established that the
SVM with linear kernel obtained better results than
SVM with quadratic kernel on the Jobs corpus (Li
et al, 2005). Hence we used the SVM with linear
kernel in the experiments on the Jobs data. Note that
PAUM always uses linear kernel in our experiments.
Table 2 presents the results of our systems as well
as the other six systems which have been evaluated
on the Jobs corpus. Note that the results for all the
17 slots are available for only three systems, Rapier,
(LP )2 and double classification, while the results
for some slots were available for the other three sys-
tems. We computed the macro-averaged F1 (the
mean of the F1 of all slots) for our systems as well
as for the three fully evaluated systems in order to
make a comparison of the overall performance.
Firstly, the overall performance of our two sys-
tems is significantly better than the other three fully
evaluated systems. The PAUM system achieves the
best performance on 5 out of the 17 slots. The SVM
system performs best on the other 3 slots. Secondly,
the double classification system had much worse
overall performance than our systems and other two
fully evaluated systems. HMM was evaluated only
on two slots. It achieved best result on one slot but
was much worse on the other slot than our two sys-
tems and some of the others. Finally, somewhat sur-
prisingly, our PAUM system achieves better perfor-
mance than the SVM system on this dataset. More-
over, the computation time of PAUM is about 1/3 of
that of the SVM. Hence, the PAUM system performs
quite satisfactory on the Jobs corpus.
Our systems were also evaluated by participating
in a Pascal challenge ? Evaluating Machine Learn-
ing for Information Extraction. The evaluation pro-
vided not only the CFP corpus but also the linguistic
features for all tokens by pre-processing the docu-
ments. The main purpose of the challenge was to
evaluate machine learning algorithms based on the
same linguistic features. The only compulsory task
is task1, which used 400 annotated documents for
training and other 200 annotated documents for test-
ing. See Ireson and Ciravegna (2005) for a short
overview of the challenge. The learning methods ex-
plored by the participating systems included LP 2,
HMM, CRF, SVM, and a variety of combinations
76
Table 2: Comparison to other systems on the jobs corpus: F1 (%) on each entity type and overall perfor-
mance as macro-averaged F1. Standard deviations for the MA F1 of our systems are presented in parenthe-
sis. The highest score on each slot and overall performance appears in bold.
Slot SVM PAUM (LP )2 Rapier DCs BWI HMM semi-CRF
Id 97.7 97.4 100 97.5 97 100 ? ?
Title 49.6 53.1 43.9 40.5 35 50.1 57.7 40.2
Company 77.2 78.4 71.9 70.0 38 78.2 50.4 60.9
Salary 86.5 86.4 62.8 67.4 67 ? ? ?
Recruiter 78.4 81.4 80.6 68.4 55 ? ? ?
State 92.8 93.6 84.7 90.2 94 ? ? ?
City 95.5 95.2 93.0 90.4 91 ? ? ?
Country 96.2 96.5 81.0 93.2 92 ? ? ?
Language 86.9 87.3 91.0 81.8 33 ? ? ?
Platform 80.1 78.4 80.5 72.5 36 ? ? ?
Application 70.2 69.7 78.4 69.3 30 ? ? ?
Area 46.8 54.0 53.7 42.4 17 ? ? ?
Req-years-e 80.8 80.0 68.8 67.2 76 ? ? ?
Des-years-e 81.9 85.6 60.4 87.5 47 ? ? ?
Req-degree 87.5 87.9 84.7 81.5 45 ? ? ?
Des-degree 59.2 62.9 65.1 72.2 33 ? ? ?
Post date 99.2 99.4 99.5 99.5 98 ? ? ?
MA F1 80.8(?1.0) 81.6(?1.1) 77.2 76.0 57.9 ? ? ?
of different learning algorithms. Firstly, the sys-
tem of the challenge organisers, which is based on
LP 2 obtained the best result for Task1, followed by
one of our participating systems which combined the
uneven margins SVM and PAUM (see Ireson and
Ciravegna (2005)). Our SVM and PAUM systems
on their own were respectively in the fourth and fifth
position among the 20 participating systems. Sec-
ondly, at least six other participating system were
also based on SVM but used different IE framework
and possibly different SVM models from our SVM
system. Our SVM system achieved better results
than all those SVM-based systems, showing that the
SVM models and the IE framework of our system
were quite suitable to IE task. Thirdly, our PAUM
based system was not as good as our SVM system
but was still better than the other SVM based sys-
tems. The computation time of the PAUM system
was about 1/5 of that of our SVM system.
Table 3 presents the per slot results and over-
all performance of our SVM and PAUM systems
as well as the system with the best overall result.
Compared to the best system, our SVM system per-
formed better on two slots and had similar results
on many of other slots. The best system had ex-
tremely good results on the two slots, C-acronym
and C-homepage. Actually, the F1 values of the best
system on the two slots were more than double of
those of every other participating system.
3.4 Effects of Uneven Margins Parameter
A number of experiments were conducted to inves-
tigate the influence of the uneven margins parameter
on the SVM and Perceptron?s performances. Table 4
show the results with several different values of un-
even margins parameter respectively for the SVM
and the Perceptron on two datasets ? CoNLL-2003
and Jobs. The SVM with uneven margins (? < 1.0)
had better results than the standard SVM (? = 1).
We can also see that the results were similar for the ?
between 0.6 and 0.4, showing that the results are not
particularly sensitive to the value of the uneven mar-
gins parameter. The uneven margins parameter has
similar effect on Perceptron as on the SVM. Table 4
shows that the PAUM had better results than both the
standard Perceptron and the margin Perceptron
77
Table 3: Results of our SVM and PAUM systems
on CFP corpus: F-measures(%) on individual entity
type and the overall figures, together with the system
with the highest overall score. The highest score on
each slot appears in bold.
SLOT PAUM SVM Best one
W-name 51.9 54.2 35.2
W-acronym 50.4 60.0 86.5
W-date 67.0 69.0 69.4
W-homepage 69.6 70.5 72.1
W-location 60.0 66.0 48.8
W-submission 70.2 69.6 86.4
W-notification 76.1 85.6 88.9
W-camera-ready 71.5 74.7 87.0
C-name 43.2 47.7 55.1
C-acronym 38.8 38.7 90.5
C-homepage 7.1 11.6 39.3
Micro-average 61.1 64.3 73.4
Our conjecture was that the uneven margins pa-
rameter was more helpful on small training sets, be-
cause the smaller a training set is, the more imbal-
anced it could be. Therefore we carried out exper-
iments on a small numbers of training documents.
Table 5 shows the results of the SVM and the uneven
margins SVM on different numbers of training doc-
uments from CoNLL-2003 and Jobs datasets. The
performance of both the standard SVM and the un-
even margins SVM improves consistently as more
training documents are used. Moreover, compared
to the results one large training sets shown in Table
4, the uneven margins SVM obtains more improve-
ments on small training sets than the standard SVM
model. We can see that the smaller the training set
is, the better the results of the uneven margins SVM
are in comparison to the standard SVM.
4 Conclusions
This paper studied the uneven margins versions of
two learning algorithms ? SVM and Perceptron ? to
deal with the imbalanced training data in IE. Our ex-
periments showed that the uneven margin is helpful,
in particular on small training sets. The smaller the
training set is, the more beneficial the uneven margin
could be. We also showed that the systems based on
the uneven margins SVM and Perceptron were com-
Table 4: The effects of uneven margins parameter
of the SVM and Perceptron, respectively: macro av-
eraged F1(%) on the two datasets CoNLL-2003 (de-
velopment set) and Jobs. The standard deviations for
the Jobs dataset show the statistical significances of
the results. In bold are the best performance figures
for each dataset and each system.
? 1.0 0.8 0.6 0.4 0.2
Conll 89.0 89.6 89.7 89.2 85.3
Jobs 79.0 79.9 81.0 80.8 79.0
?1.4 ?1.2 ?0.9 ?1.0 ?1.3
(?+, ??) (0,0) (1,1) (50,1)
Conll 83.5 83.9 84.4
Jobs 74.1 78.8 81.6
?1.5 ?1.0 ?1.1
parable to other state-of-the-art systems.
Our SVM system obtained better results than
other SVM-based systems on the CoNLL-2003 cor-
pus and CFP corpus respectively, while being sim-
pler than most of them. This demonstrates that our
SVM system is both effective and efficient.
We also explored PAUM, a simple and fast
learning algorithm for IE. The results of PAUM
were somehow worse (about 0.02 overall F-measure
lower) than those of the SVM on two out of three
datasets. On the other hand, PAUM is much faster
to train and easier to implement than SVM. It is also
worth noting that PAUM outperformed some other
learning algorithms. Therefore, even PAUM on its
own would be a good learning algorithm for IE.
Moreover, PAUM could be used in combination with
other classifiers or in the more complicated frame-
work such as the one in Carreras et al (2003).
Since many other tasks in Natural Language Pro-
cessing, like IE, often lead to imbalanced classifica-
tion problems and the SVM has been used widely
in Natural Language Learning (NLL), we can ex-
pect that the uneven margins SVM and PAUM are
likely to obtain good results on other NLL problems
as well.
Acknowledgements
This work is supported by the EU-funded SEKT
project (http://www.sekt-project.org).
78
Table 5: The performances of the SVM system with
small training sets: macro-averaged F1(%) on the
two datasets CoNLL-2003 (development set) and
Jobs. The uneven margins SVM (? = 0.4) is com-
pared to the standard SVM model with even margins
(? = 1). The standard deviations are presented for
results on the Jobs dataset.
size 10 20 30 40 50
? = 0.4
Conll 60.6 66.4 70.4 72.2 72.8
Jobs 51.6 60.9 65.7 68.6 71.1
?2.7 ?2.5 ?2.1 ?1.9 ?2.5
? = 1
Conll 46.2 58.6 65.2 68.3 68.6
Jobs 47.1 56.5 61.4 65.4 68.1
?3.4 ?3.1 ?2.7 ?1.9 ?2.1
References
M. E. Califf. 1998. Relational Learning Techniques for
Natural Language Information Extraction. Ph.D. the-
sis, University of Texas at Austin.
X. Carreras, L. Ma`rquez, and L. Padro?. 2003. Learn-
ing a perceptron-based named entity chunker via on-
line recognition feedback. In Proceedings of CoNLL-
2003, pages 156?159. Edmonton, Canada.
H. L. Chieu and H. T. Ng. 2002. A Maximum En-
tropy Approach to Information Extraction from Semi-
Structured and Free Text. In Proceedings of the Eigh-
teenth National Conference on Artificial Intelligence,
pages 786?791.
F. Ciravegna. 2001. (LP)2, an Adaptive Algorithm for
Information Extraction from Web-related Texts. In
Proceedings of the IJCAI-2001 Workshop on Adaptive
Text Extraction and Mining, Seattle.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A Framework and Graphical
Development Environment for Robust NLP Tools and
Applications. In Proceedings of the 40th Anniversary
Meeting of the Association for Computational Linguis-
tics (ACL?02).
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named Entity Recognition through Classifier Combi-
nation. In Proceedings of CoNLL-2003, pages 168?
171. Edmonton, Canada.
D. Freigtag and A. K. McCallum. 1999. Information Ex-
traction with HMMs and Shrinkage. In Proceesings
of Workshop on Machine Learnig for Information Ex-
traction, pages 31?36.
D. Freitag and N. Kushmerick. 2000. Boosted Wrapper
Induction. In Proceedings of AAAI 2000.
D. Freitag. 1998. Machine Learning for Information Ex-
traction in Informal Domains. Ph.D. thesis, Carnegie
Mellon University.
N. Ireson and F. Ciravegna. 2005. Pascal Chal-
lenge The Evaluation of Machine Learning
for Information Extraction. In Proceedings of
Dagstuhl Seminar Machine Learning for the
Semantic Web (http://www.smi.ucd.ie/Dagstuhl-
MLSW/proceedings/).
H. Isozaki and H. Kazawa. 2002. Efficient Support
Vector Classifiers for Named Entity Recognition. In
Proceedings of the 19th International Conference on
Computational Linguistics (COLING?02), pages 390?
396, Taipei, Taiwan.
Y. Li and J. Shawe-Taylor. 2003. The SVM with
Uneven Margins and Chinese Document Categoriza-
tion. In Proceedings of The 17th Pacific Asia Con-
ference on Language, Information and Computation
(PACLIC17), Singapore, Oct.
Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and
J. Kandola. 2002. The Perceptron Algorithm with Un-
even Margins. In Proceedings of the 9th International
Conference on Machine Learning (ICML-2002), pages
379?386.
Y. Li, K. Bontcheva, and H. Cunningham. 2005. SVM
Based Learning System For Information Extraction.
In Proceedings of Sheffield Machine Learning Work-
shop, Lecture Notes in Computer Science. Springer
Verlag.
J. Mayfield, P. McNamee, and C. Piatko. 2003. Named
Entity Recognition Using Hundreds of Thousands of
Features. In Proceedings of CoNLL-2003, pages 184?
187. Edmonton, Canada.
A. De Sitter and W. Daelemans. 2003. Information ex-
traction via double classification. In Proceedings of
ECML/PRDD 2003 Workshop on Adaptive Text Ex-
traction and Mining (ATEM 2003), Cavtat-Dubrovnik,
Croatia.
S. Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34(1):233?272.
J. Zhang and I. Mani. 2003. KNN Approach to Un-
balanced Data Distributions: A Case Study Involv-
ing Information Extraction. In Proceedings of the
ICML?2003 Workshop on Learning from Imbalanced
Datasets.
79
