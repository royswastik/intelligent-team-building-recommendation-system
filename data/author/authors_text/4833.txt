Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 217?220,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Computing Confidence Scores for All Sub Parse Trees 
 
Feng Lin Fuliang Weng 
Department of Computer Science and Engineering Research and Technology Center 
Fudan University Robert Bosch LLC 
Shanghai 200433, P.R. China Palo Alto, CA, 94303, USA 
fenglin@fudan.edu.cn fuliang.weng@us.bosch.com 
 
 
 
 
Abstract 
Computing confidence scores for applica-
tions, such as dialogue system, informa-
tion retrieving and extraction, is an active 
research area. However, its focus has been 
primarily on computing word-, concept-, 
or utterance-level confidences. Motivated 
by the need from sophisticated dialogue 
systems for more effective dialogs, we 
generalize the confidence annotation to all 
the subtrees, the first effort in this line of 
research. The other contribution of this 
work is that we incorporated novel long 
distance features to address challenges in 
computing multi-level confidence scores. 
Using Conditional Maximum Entropy 
(CME) classifier with all the selected fea-
tures, we reached an annotation error rate 
of 26.0% in the SWBD corpus, compared 
with a subtree error rate of 41.91%, a 
closely related benchmark with the 
Charniak parser from (Kahn et al, 2005). 
1 Introduction 
There has been a good amount of interest in ob-
taining confidence scores for improving word or 
utterance accuracy, dialogue systems, information 
retrieving & extraction, and machine translation 
(Zhang and Rudnicky, 2001; Guillevic et al, 2002; 
Gabsdil et al, 2003; Ueffing et al, 2007). 
However, these confidence scores are limited to 
relatively simple systems, such as command-n-
control dialogue systems. For more sophisticated 
dialogue systems (e.g., Weng et al, 2007), identi-
fication of reliable phrases must be performed at 
different granularity to ensure effective and 
friendly dialogues. For example, in a request of 
MP3 music domain ?Play a rock song by Cher?, if 
we want to communicate to the user that the sys-
tem is not confident of the phrase ?a rock song,? 
the confidence scores for each word, the artist 
name ?Cher,? and the whole sentence would not be 
enough. For tasks of information extraction, when 
extracted content has internal structures, confi-
dence scores for such phrases are very useful for 
reliable returns. 
As a first attempt in this research, we generalize 
confidence annotation algorithms to all sub parse 
trees and tested on a human-human conversational 
corpus, the SWBD. Technically, we also introduce 
a set of long distance features to address the chal-
lenges in computing multi-level confidence scores.  
This paper is organized as follows:  Section 2 in-
troduces the tasks and the representation for parse 
trees; Section 3 presents the features used in the 
algorithm; Section 4 describes the experiments in 
the SWBD corpus; Section 5 concludes the paper. 
2 Computing Confidence Scores for 
Parse Trees 
The confidence of a sub-tree is defined as the pos-
terior probability of its correctness, given all the 
available information. It is )|( xcorrectisspP  ? the 
posterior probability that the parse sub-tree sp is 
correct, given related information x. In real appli-
cations, typically a threshold or cutoff t is needed:  
 
??
?
<
?
txcorrectisspPifincorrec
txcorrectisspPifcorrect
issp
)|(,
)|(,
   (1) 
217
In this work, the probability )|( xcorrectisspP is 
calculated using CME modeling framework: 
( ) ( ) ( )???
?
???
?
= ?
j
jj yxfxZ
xyP ,exp
1
| ?   (2) 
where y?{sp is correct, sp is incorrect}, x is the 
syntactic context of the parse sub-tree sp, fj are the 
features, ?j are the corresponding weights, and Z(x) 
is the normalization factor.  
The parse trees used in our system are lexical-
ized binary trees. However, the confidence compu-
tation is independent of any parsing method used 
in generating the parse tree as long as it generates 
the binary dependency relations. An example of 
the lexicalized binary trees is given in Figure 1, 
where three important components are illustrated: 
the left sub-tree, the right sub-trees, and the 
marked head and dependency relation.  
Because the parse tree is already given, a bot-
tom-up left-right algorithm is used to traverse 
through the parse tree: for each subtree, compute 
its confidence, and annotate it as correct or wrong. 
3 Features 
Four major categories of features are used, includ-
ing, words, POS tags, scores and syntactic infor-
mation. Due to the space limitation, we only give a 
detailed description of the most important one1, 
lexical-syntactic features.  
The lexical-syntactic features include lexical, 
POS tag, and syntactic features. Word and POS tag 
features include the head and modifier words of the 
parse sub-tree and the two children of the root, as 
well as their combinations. The POS tags and hier-
archical POS tags of the corresponding words are 
                                                          
1 The other important one is the dependency score, which is 
the conditional probability of the last dependency relation in 
the subtree, given its left and right child trees 
also considered to avoid data sparseness. The 
adopted hierarchical tags are: Verb-related (V), 
Noun-related (N), Adjectives (ADJ), and Adverbs 
(ADV), similar to (Zhang et al 2006).  
Long distance structural features in statistical 
parsing lead to significant improvements (Collins 
et al, 2000; Charniak et al, 2005). We incorporate 
some of the reported features in the feature space 
to be explored, and they are enriched with different 
POS categories and grammatical types. Two eam-
ples are given below.  
 One example is the Single-Level Joint Head 
and Dependency Relation (SL-JHD). This feature 
is pairing the head word of a given sub-tree with its 
last dependency relation. To address the data 
sparseness problem, two additional SL-JHD fea-
tures are considered: a pair of the POS tag of the 
head of a given sub-tree and its dependency rela-
tion, a pair of the hierarchical POS tag of the head 
of a given sub-tree and its dependency relation. For 
example, for the top node in Figure 2, (restaurant 
NCOMP), (NN, NCOMP), and (N, NCOMP) are 
the examples for the three SL-JHD features. To 
compute the confidence score of the sub-tree, we 
include the three JHD features for the top node, 
and the JHD features for its two children. Thus, for 
the sub-tree in Figure 2, the following nine JHD 
features are included in the feature space, i.e., (res-
taurant NCOMP), (NN, NCOMP), (N, NCOMP), 
(restaurant NMOD), (NN NMOD), (N NMOD), 
(with POBJ), (IN POBJ), and (ADV POBJ).  
The other example feature is Multi-Level Joint 
Head and Dependency Relation (ML-JHD), which 
takes into consideration the dependency relations 
at multiple levels. This feature is an extension of 
SL-JHD. Instead of including only single level 
head and dependency relations, the ML-JHD fea-
ture includes the hierarchical POS tag of the head 
and dependency relations for all the levels of a 
given sub-tree. For example, given the sub-tree in 
Figure 3, (NCOMP, N, NMOD, N, NMOD, N, 
POBJ, ADV, NMOD, N) is the ML-JHD feature 
for the top node (marked by the dashed circle).  
In addition, three types of features are included:  
dependency relations, neighbors of the head of the 
current subtree, and the sizes of the sub-tree and its 
left and right children. The dependency relations 
include the top one in the subtree. The neighbors 
are typically within a preset distance from the head 
word. The sizes refer to the numbers of words or 
non-terminals in the subtree and its children. 
Figure 1. Example of parse sub-tree?s structure for 
phrase ?three star Chinese restaurant? 
star 
NN 
NP (star) 
NMOD 
NP (restaurant) 
NMOD 
Left Sub-tree 
three 
CD 
restaurant
NN 
NP (restaurant) 
NMOD 
Chinese 
NNP 
Right Sub-tree
218
Figure 3. ML-JHD Features 
a NNP 
CUISINENAME restaurant 
NN 
NP (restaurant) DT 
NP (restaurant) 
NMOD
with JJ 
good service
NN 
NP (service)IN 
PP (with) 
NMOD
NMOD POBJ
NP (restaurant) 
NCOMP 
 
4 Experiments 
Experiments were conducted to see the perform-
ance of our algorithm in human to human dialogs ? 
the ultimate goal of a dialogue system. In our work, 
we use a version of the Charniak?s parser from 
(Aug. 16, 2005) to parse the re-segmented SWBD 
corpus (Kahn et al, 2005), and extract the parse 
sub-trees from the parse trees as experimental data.   
The parser?s training procedure is the same as 
(Kahn et al, 2005). The only difference is that they 
use golden edits in the parsing experiments while 
we delete all the edits in the UW Switchboard cor-
pus. The F-score of the parsing result of the 
Charniak parser without edits is 88.24%.  
The Charniak parser without edits is used to 
parse the training data, testing data and tuning data. 
We remove the sentences with only one word and 
delete the interjections in the hypothesis parse trees. 
Finally, we extract parse sub-trees from these hy-
pothesis parse trees. Based on the gold parse trees, 
a parse sub-tree is labeled with 1 (correct), if it has 
all the words, their POS tags and syntactic struc-
tures correct. Otherwise, it is 0 (incorrect). Among 
the 424,614 parse sub-trees from the training data, 
316,182 sub-trees are labeled with 1; among the 
38,774 parse sub-trees from testing data, 22,521 
ones are labeled with 1; and among the 67,464 
parse sub-trees from the tuning data, 38,619 ones 
are labeled with 1. In the testing data, there are 
5,590 sentences, and the percentage of complete 
bracket match2 is 57.11%, and the percentage of 
parse sub-trees with correct labels at the sentence 
level is 48.57%. The percentage of correct parse 
sub-trees is lower than that of the complete bracket 
match due to its stricter requirements.  
Table 1 shows our analysis of the testing data. 
There, the first column indicates the phrase length 
categories from the parse sub-trees. Among all the 
parse trees in the test data, 82.84% (first two rows) 
have a length equal to or shorter than 10 words. 
We converted the original parse sub-trees from the 
Charniak parser into binary trees.  
 
Length Sub-tree Types Number Ratio 
Correct 21,593 55.70%<=10 
Incorrect 10,525 27.14%
Correct 928 2.39%>10 
Incorrect 5,728 14.77%
Table 1. The analysis of testing data. 
 
We apply the model (2) from section 2 on the 
above data for all the following experiments. The 
performance is measured based on the confidence 
annotation error rate (Zhang and Rudnicky, 2001).  
SubtreesOfNumberTotal
ncorrectnotatedAsISubtreesAnOfNumber
ErrorAnnot =.
 
Two sets of experiments are designed to demon-
strate the improvements of our confidence comput-
ing algorithm, as well as the newly introduced 
features (see Table 2 and Table 3). 
Experiments were conducted to evaluate the ef-
fectiveness of each feature category for the sub-
tree level confidence annotation on SWBD corpus 
(Table 2). The baseline system uses the conven-
tional features: words and POS tags. Additional 
feature categories are included separately. The syn-
tactic feature category shows the biggest improve-
ment among all the categories.  
To see the additive effect of the feature spaces 
for the multi-level confidence annotation, another 
set of experiments were performed (Table 3). 
Three feature spaces are included incrementally: 
dependency score, hierarchical tags and syntactic 
features. Each category provides sizable reduction 
in error rate. Totally, it reduces the error rate by  
                                                          
2 Complete bracket match is the percentage of sentences where 
bracketing recall and precision are both 100%. 
Figure 2. SL-JHD Features 
a NNP 
CUISINENAME restaurant
NN 
NP (restaurant) DT 
NP (restaurant) 
NMOD
with JJ 
good service
NN
NP (service)IN 
PP (with) 
NMOD
NMOD POBJ
NP (restaurant) 
NCOMP Relation_Head
219
 Feature Space Description Annot. Error Relative Error Decrease 
Baseline Base features: Words, POS tag 36.2% \ 
Set 1 Base features + Dependency score 32.8% 9.4% 
Set 2 Base features + Hierarchical tags 35.3% 2.5% 
Set 3 Base features + Syntactic features 29.3% 19.1% 
Table 2. Comparison of different feature space (on SWBD corpus). 
 
 Feature Space Description Annot. Error Relative Error Decrease 
Baseline Base features: Words, POS tag 36.2% \ 
Set 4 + Dependency score 32.8% 9.4% 
Set 5 + Dependency score + hierarchical tags   32.7% 9.7% 
Set 6 + Dependency score + hierarchical tags   + syntactic features 26.0% 28.2% 
Table 3. Summary of experiment results with different feature space (on SWBD corpus).
 
10.2%, corresponding to 28.2% of a relative error 
reduction over the baseline. The best result of an-
notation error rate is 26% for Switchboard data, 
which is significantly lower than the 41.91% sub-
tree parsing error rate (see Table 1: 41.91% = 
27.14%+14.77%). So, our algorithm would also 
help the best parsing algorithms during rescoring 
(Charniak et al, 2005; McClosky et al, 2006).  
We list the performance of the parse sub-trees 
with different lengths for Set 6 in Table 4, using 
the F-score as the evaluation measure.  
Length Sub-tree Category F-score
Correct 82.3% <=10 
Incorrect 45.9% 
Correct 33.1% >10 
Incorrect 86.1% 
Table 4. F-scores for various lengths in Set 15. 
 
The F-score difference between the ones with 
correct labels and the ones with incorrect labels are 
significant. We suspect that it is caused by the dif-
ferent amount of training data. Therefore, we sim-
ply duplicated the training data for the sub-trees 
with incorrect labels. For the sub-trees of length 
equal to or less than 10 words, this training method 
leads to a 79.8% F-score for correct labels, and a 
61.4% F-score for incorrect labels, which is much 
more balanced than those in the first set of results. 
5 Conclusion 
In this paper, we generalized confidence annota-
tion algorithms to multiple-level parse trees and 
demonstrated the significant benefits of using long  
 
distance features in SWBD corpora. It is foresee-
able that multi-level confidence annotation can be 
used for many other language applications such as 
parsing, or information retrieval.  
References  
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. Proc. 
ACL, pages 173?180. 
Michael Collins. 2000. Discriminative reranking for natural 
language parsing. Proc. ICML, pages 175?182. 
Malte Gabsdil and Johan Bos. 2003. Combining Acoustic Con-
fidence Scores with Deep Semantic Analysis for Clarifica-
tion Dialogues. Proc. IWCS, pages 137-150. 
Didier Guillevic, et al 2002.  Robust semantic confidence 
scoring. Proc. ICSLP,  pages 853-856. 
Jeremy G. Kahn, et al 2005. Effective Use of Prosody in Pars-
ing Conversational Speech. Proc. EMNLP, pages 233-240. 
David McClosky, Eugene Charniak and Mark Johnson. 2006. 
Reranking and Self-Training for Parser Adaptation. Proc. 
COLING-ACL, pages 337-344. 
Nicola Ueffing and Hermann Ney. 2007. Word-Level Confi-
dence Estimation for Machine Translation. Computational 
Linguistics, 33(1):9-40. 
Fuliang Weng, et al, 2007. CHAT to Your Destination. Proc. 
of the 8th SIGDial workshop on Discourse and Dialogue, 
pages 79-86. 
Qi Zhang, Fuliang Weng and Zhe Feng. 2006. A Pro-gressive 
Feature Selection Algorithm for Ultra Large Feature 
Spaces. Proc. COLING-ACL, pages 561-568. 
Rong Zhang and Alexander I. Rudnicky. 2001. Word level 
confidence annotation using combinations of features. Proc. 
Eurospeech, pages 2105-2108. 
220
NAACL HLT Demonstration Program, pages 23?24,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
A Conversational In-car Dialog System
Baoshi Yan1 Fuliang Weng1 Zhe Feng1 Florin Ratiu2 Madhuri Raya1 Yao Meng1
Sebastian Varges2 Matthew Purver2 Annie Lien1 Tobias Scheideck1 Badri Raghunathan1
Feng Lin1 Rohit Mishra4 Brian Lathrop4 Zhaoxia Zhang4 Harry Bratt3 Stanley Peters2
Research and Technology Center, Robert Bosch LLC, Palo Alto, California1
Center for the Study of Language and Information, Stanford University, Stanford, California2
Speech Technology and Research Lab, SRI International, Menlo Park, California3
Electronics Research Lab, Volkswagen of America, Palo Alto, California4
Abstract
In this demonstration we present a con-
versational dialog system for automobile
drivers. The system provides a voice-
based interface to playing music, finding
restaurants, and navigating while driving.
The design of the system as well as the
new technologies developed will be pre-
sented. Our evaluation showed that the
system is promising, achieving high task
completion rate and good user satisfation.
1 Introduction
As a constant stream of electronic gadgets such as
navigation systems and digital music players en-
ters cars, it threatens driving safety by increasing
driver distraction. According to a 2005 report by
the National Highway Traffic Safety Administration
(NHTSA) (NHTSA, 2005), driver distraction and
inattention from all sources contributed to 20-25%
of police reported crashes. It is therefore impor-
tant to design user interfaces to devices that mini-
mize driver distraction, to which voice-based inter-
faces have been a promising approach as they keep
a driver?s hands on the wheel and eyes on the road.
In this demonstration we present a conversational
dialog system, CHAT, that supports music selection,
restaurant selection, and driving navigation (Weng
et al, 2006). The system is a joint research effort
from Bosch RTC, VWERL, Stanford CSLI, and SRI
STAR Lab funded by NIST ATP. It has reached a
promising level, achieving a task completion rate of
98%, 94%, 97% on playing music, finding restau-
rants, and driving navigation respectively.
Specifically, we plan to present a number of fea-
tures in the CHAT system, including end-pointing
with prosodic cues, robust natural language under-
standing, error identification and recovery strate-
gies, content optimization, full-fledged reponse gen-
eration, flexible multi-threaded, multi-device dialog
management, and support for random events, dy-
namic information, and domain switching.
2 System Descriptions
The spoken dialog system consists of a number of
components (see the figure on the next page). In-
stead of the hub architecture employed by Commu-
nicator projects (Seneff et al, 1998), it is devel-
oped in Java and uses flexible event-based, message-
oriented middleware. This allows for dynamic regis-
tration of new components. Among the component
modules in the figure, we use the Nuance speech
recognition engine with class-based n-grams and
dynamic grammars, and the Nuance Vocalizer as the
TTS engine. The Speech Enhancer removes noises
and echo. The Prosody module will provide addi-
tional features to the Natural Language Understand-
ing (NLU) and Dialog Manager (DM) modules to
improve their performance.
The NLU module takes a sequence of recognized
words and tags, performs a deep linguistic analysis
with probabilistic models, and produces an XML-
based semantic feature structure representation. Par-
allel to the deep analysis, a topic classifier assigns
n-best topics to the utterance, which are used in the
cases where the dialog manager cannot make any
sense of the parsed structure. The NLU module also
supports dynamic updates of the knowledge base.
The DM module mediates and manages interac-
23
tion. It uses an information-state-update approach to
maintain dialog context, which is then used to inter-
pret incoming utterances (including fragments and
revisions), resolve NPs, construct salient responses,
track issues, etc. Dialog states can also be used to
bias SR expectation and improve SR performance,
as has been performed in previous applications of
the DM. Detailed descriptions of the DM can be
found in (Lemon et al, 2002) (Mirkovic and Cave-
don, 2005).
The Knowledge Manager (KM) controls access
to knowledge base sources (such as domain knowl-
edge and device information) and their updates. Do-
main knowledge is structured according to domain-
dependent ontologies. The current KMmakes use of
OWL, a W3C standard, to represent the ontological
relationships between domain entities.
The Content Optimization module acts as an in-
termediary between the dialog management module
and the knowledge management module and con-
trols the amount of content and provides recommen-
dations to user. It receives queries in the form of se-
mantic frames from the DM, resolves possible ambi-
guities, and queries the KM. Depending on the items
in the query result as well as configurable properties,
the module selects and performs an appropriate op-
timization strategy (Pon-Barry et al, 2006).
The Response Generation module takes query re-
sults from the KM or Content Optimizer and gener-
ates natural language sentences as system responses
to user utterances. The query results are converted
into natural language sentences via a bottom-up ap-
proach using a production system. An alignment-
based ranking algorithm is used to select the best
generated sentence.
The system supports random events and dy-
namic external information, for example, the system
prompts users for the next turn when they drive close
to an intersection and dialogs can be carried out in
terms of the current dynamic situation. The user can
also switch among the three different applications
easily by explicitly instructing the system which do-
main to operate in.
3 Acknowledgement
This work is partially supported by the NIST Ad-
vanced Technology Program.
References
Oliver Lemon, Alex Gruenstein, and Stanley Peters.
2002. Collaborative activities and multi-tasking in
dialogue systems. In Traitement Automatique des
Langues (TAL), page 43(2).
Danilo Mirkovic and Lawrence Cavedon. 2005. Prac-
tical Plug-and-Play Dialogue Management. In Pro-
ceedings of the 6th Meeting of the Pacific Associa-
tion for Computational Linguistics (PACLING), page
43(2), Tokyo, Japan.
National Highway Traffic Safety Administration
NHTSA. 2005. NHTSA Vehicle Safety Rulemaking
and Supporting Research Priorities: Calendar Years
2005-2009. January.
Heather Pon-Barry, Fuliang Weng, and Sebastian Varges.
2006. Evaluation of content presentation strategies
for an in-car spoken dialogue system. In Proceedings
of the 9th International Conference on Spoken Lan-
guage Processing (Interspeech/ICSLP), pages 1930?
1933, Pittsburgh, PA, September.
Stephanie Seneff, Ed Hurley, Raymond Lau, Chris-
tine Pao, Philipp Schmid, and Victor Zue. 1998.
GALAXY-II: A Reference Architecture for Conversa-
tional System Development. In International Confer-
ence on Spoken Language Processing (ICSLP), page
43(2), Sydney, Australia, December.
Fuliang Weng, Sebastian Varges, Badri Raghunathan,
Florin Ratiu, Heather Pon-Barry, Brian Lathrop,
Qi Zhang, Tobias Scheideck, Harry Bratt, Kui Xu,
Matthew Purver, Rohit Mishra, Annie Lien, Mad-
huri Raya, Stanley Peters, Yao Meng, Jeff Russel,
Lawrence Cavedon, Liz Shriberg, and Hauke Schmidt.
2006. CHAT: A conversational helper for automo-
tive tasks. In Proceedings of the 9th International
Conference on Spoken Language Processing (Inter-
speech/ICSLP), pages 1061?1064, Pittsburgh, PA,
September.
24
