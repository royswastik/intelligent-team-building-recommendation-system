Analysis of Intention in Dialogues Using Category Trees and 
Its Application to Advertisement Recommendation 
Hung-Chi Huang Ming-Shun Lin Hsin-Hsi Chen 
Department of Computer Science and Information Engineering  
National Taiwan University  
Taipei, Taiwan  
{hchuang, mslin}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw  
 
 
Abstract 
We propose an intention analysis system 
for instant messaging applications.  The 
system adopts Yahoo! directory as category 
trees, and classifies each dialogue into one 
of the categories of the directory. Two 
weighting schemes in information retrieval, 
i.e., tf and tf-idf, are considered in our ex-
periments.  In addition, we also expand 
Yahoo! directory with the accompanying 
HTML files and explore different features 
such as nouns, verbs, hypernym, hyponym, 
etc.  Experiments show that category trees 
expanded with snippets together with noun 
features under tf scheme achieves a best F-
score, 0.86, when only 37.46% of utter-
ances are processed on the average.  This 
methodology is employed to recommend 
advertisements relevant to the dialogue. 
1 Introduction 
Instant messaging applications such as Google 
Talk, Microsoft MSN Messenger, Yahoo Messen-
ger, QQ, and Skype are very popular.  In the 
blooming instant messaging markets, sponsor links 
and advertisements support the free service.  Fig-
ure 1 shows an example of sponsor links in instant 
message applications.  They are usually randomly 
proposed and may be irrelevant to the utterance.  
Thus, they may not attract users? attentions and 
have no effects on advertisements.  This paper 
deals with the analysis of intention in the dialogues 
and the recommendation of relevant sponsor links 
in an ongoing conversation. 
In the related works, Fain and Pedersen (2006) 
survey sponsored search, suggesting the impor-
tance of matching advertising content to user inten-
tions.  How to match advertiser content to user 
queries is an important issue.  Yih et al (2006) 
aimed at extracting advertisement keywords from 
the intention on the web pages.  However, these 
works did not address the issues in dialogues. 
 
Figure 1.  A Sponsor Link in an IM Application 
In conventional dialogue management, how to 
extract semantic concepts, identify the speech act, 
and formulate the dialogue state transitions are im-
portant tasks.  The domain shift is a challenging 
problem (Lin and Chen, 2004).  In instant message 
applications, more challenging issues have to be 
tackled.  Firstly, the discussing topics of dialogues 
are diverse.  Secondly, the conversation may be 
quite short, so that the system should be responsive 
instantly when detecting the intention.  Thirdly, the 
utterance itself can be purely free-style and far be-
yond the formal grammar.  That is, self-defined or 
symbolic languages may be used in the dialogues.  
The following shows some example utterances. 
 James: dud, i c ur foto on Kelly?s door~  ^^|| 
 Antony: Orz?.kill me pls. >< 
An intention detecting system has to extract words 
from incomplete sentences in dialogues. Fourthly, 
the system should consider up-to-date terms, in-
stead of just looking up conventional dictionaries. 
625
Capturing the intention in a dialogue and rec-
ommending the advertisements before its ending 
are the goal of this approach.  This paper is organ-
ized as follows.  Section 2 shows an overview of 
the system architecture.  Section 3 discusses the 
category trees and the weighting functions for 
identifying the intention.  Section 4 presents the 
experimental results comparing with different uses 
of the category trees and word features.  Section 5 
concludes and remarks. 
2 System Overview 
Fain and Pedersen (2006) outlined six basic 
elements for sponsored search.  They are shown as 
follows: 
(1) advertiser-provided content, 
(2) advertiser-provided bids, 
(3) ensuring that advertiser content is relevant 
to the target keyword, 
(4) matching advertiser content to user queries, 
(5) displaying advertiser content in some rank 
order,  
(6) gathering data, metering clicks and charg-
ing advertisers. 
In instant messaging applications, a dialogue is 
composed of several utterances issuing by at least 
two users.  They are different from sponsored 
search in that advertiser content is matched to user 
utterances instead of user queries.  While reading 
users? conversation, an intention detecting system 
recommends suitable advertiser information at a 
suitable time.  The time of the recommendation 
and the effect of advertisement have a strong rela-
tionship.  The earlier the correct recommendation 
is, the larger the effect is. 
However, time and accuracy are trade-off.  At 
the earlier stages of a dialogue, the system may 
have deficient information to predict suitable ad-
vertisement.  Thus, a false advertisement may be 
proposed.  On the other hand, the system may have 
enough information at the later stages.  However, 
users may complete their talk at any time in this 
case, so the advertisement effect may be lowered.   
Figure 2 shows architecture of our system.  In 
each round of the conversation, we retrieve an ut-
terance from a given instant message application.  
Then, we parse the utterance and try to predict in-
tention of the dialogue based on current and previ-
ous utterances, and consult the advertisement data-
bases that provide sponsor links accordingly.  If 
the information in the utterances is enough for pre-
diction, then several candidates are proposed.  Fi-
nally, based on predefined criteria, the best candi-
date is selected and proposed to the IM application 
as the sponsor link in Figure 1. 
In the following sections, we will explore when 
to make sure the intention of a dialogue with con-
fidence and to propose suitable recommendations.  
In addition, we will also discuss what word fea-
tures (called cue words hereafter) in the utterances 
are useful for the intention determination.  We as-
sume sponsor links or advertisements are adjunct 
on the given category trees.  
 
Figure 2.  System Architecture 
3 Categorization of Dialogues 
3.1 Web Directory Used for Categorization 
We employ Yahoo! directory1 to assign a dialogue 
or part of a dialogue in category representing its 
intention.  Every word in dialogues is classified by 
the directory.  For example, by searching the term 
BMW, we could retrieve the category path: 
  >Business and Economy>? Makers>Vehicles 
Each category contains subcategories, which in-
clude some subsidiary categories. Therefore, we 
could take the directory as a hierarchical tree for 
searching the intention.  Moreover, each node of 
the tree has attributes from the node itself and its 
ancestors.  Our idea is to summarize all intentions 
from words in a dialog, and then conclude the in-
tention accordingly. 
The nodes sometimes are overlapped, that is, 
one node could be found in more than one path. 
For example, the car maker BMW has at least two 
other nodes: 
                                                 
1 http://dir.yahoo.com 
626
  >Regional>Countries>Germany>Business and 
     Economy>?>Dealers 
  >Recreation>Automotive>?Clubs and Organi- 
    zations>BMW Car Club of America 
The categories of BMW include Business and 
Economy, Regional, and Recreation.  This demon-
strates the nature of the word ambiguity, and is 
challenging when the system identifies the inten-
tion embedded in the dialogs. 
The downloaded Yahoo! directory brings up 
HTML documents with three basic elements, in-
cluding titles, links and snippet as shown in Figure 
3.  The following takes the three elements from a 
popular site as an example. 
Title: The White House  
Link: www.WhiteHouse.gov  
Snippet: Features statements and press releases 
by President George W. Bush as well? 
 
Figure 3. Sample HTML in Yahoo! Directory Tree 
We will explore different ways to use the three 
elements during intention identification. Table 1 
shows different models and total nodes.  YahooO 
and YahooX are two extreme cases.  The former 
employs the original category tree, while the latter 
expands the category tree with titles, links and 
snippets.  Thus, the former contains 7,839 nodes 
and the latter 78,519 nodes. 
 
Table 1.  Tree Expansion Scenarios 
 
Table 2.  Examples of Expanded Nodes 
Table 2 lists some examples to demonstrate the 
category tree expansion.  Some words inside the 
three elements rarely appear in dictionaries or en-
cyclopedias. Thus, we can summarize these trees 
and build a new dictionary with definitions.  For 
example, we could find the hottest web sites You-
Tube and MySpace, and even the most popular 
Chinese gamble game, Mahjong. 
3.2 Scoring Functions for Categorization  
Given a fragment F of a dialogue, which is com-
posed of utterances reading up to now, Formula 1 
determines the intention IINT of F by counting total 
scores of cue words w in F contributing to I.   
?
?
?=
Fw
INT IwbwtfI ),()(maxarg  (1)
where tf(w) is term frequency of w in F, and b(w,I) 
is 1 when w is in the paths corresponding to the 
intention IINT; b(w,I) is 0 otherwise. 
Formula 2 considers the discriminating capabil-
ity of each cue word.  It is similar to tf-idf scheme 
in information retrieval. 
?
?
??=
FwI
INT Iwbwdf
N
wtfI ),(
)(
log)(maxarg
 
where N is total number of intention
(2)
s, and df(w) is 
 
marized in Table 3 with 
explanation and examples. 
total intentions in which w appears. 
3.3 Features of Cue Words 
The features of possible cue words including nouns,
verbs, stop-words, word length, hypernym, hypo-
nym, and synonym are sum
627
 
Table 3.  Cue Words Explored 
Nouns and verbs form skeletons of concepts are 
important cues for similarity measures (Chen et al, 
2003), so that they are considered as features in our 
model.  Word length is used to filter out some un-
necessary words because the shorter the word is, 
the less meaningful the word might be.  Here we 
postulate that instant messaging users are not will-
ing to type long terms if unnecessary. 
In this paper, we regard words in an utterance of 
dialogues as query terms.  Rosie et al (2006) 
showed that query substitution may be helpful to 
retrieve more meaningful results.  Here, we use 
hypernym, hyponym and synonym specified in 
WordNet (Fellbaum, 1998) to expand the original 
utterance.  
3.4 Candidate Recommendation 
The proposed model also provides the ability to 
show the related advertisements after intention is 
confirmed.  As discussed, for each of node in the 
category tree, there is an accompanying HTML file 
to show some related web sites and even sponsors. 
Therefore, we can also use the category tree to put 
sponsor links into the HTML files, and just fetch 
the sponsor links from the HTML file on the node 
to the customers. 
The algorithm to select the suitable candidates 
could be shortly described as the Longest Path 
First.  Once we select the category of the intention, 
the nodes appearing in the chosen category will 
then be collected into a set. We will check the 
longest path and provide the sponsor links from the 
node. 
4 Experimental Results 
4.1 Performance of Different Models 
To prepare the experimental materials, we col-
lected 50 real dialogs from end-users, and asked 
annotators to tag the 50 dialogs with 14 given Ya-
hoo! directory categories shown in Table 4.  Aver-
age number of sentences is 12.38 and average 
number of words is 56.04 in each dialog.  We 
compare the system output with the answer keys, 
and compute precision, recall, and F-score for each 
method. 
 
Table 4.  Category Abbreviation 
Table 5 shows the performance of using For-
mula 1 (i.e., tf scheme).  This model is a combina-
tion of a scenario shown in Table 1 and features 
shown in Table 3.  For example, the YahooS-noun 
matches cue words of POS noun from utterances to 
the category tree expanded with snippets.  WL de-
notes word length.  Only cue words of length ? 
WL is considered.  C denotes the number of dia-
logues correctly analyzed.  NA denotes the number 
of undecidable dialogues.  P, R and F denote preci-
sion, recall and F-score. 
Table 5 shows that YahooS with noun features 
achieves a best performance.  Noun feature works 
impressively well with the orders, YahooS, Ya-
hooT, YahooX, and YahooL.  That meets our ex-
pectation because the information from snippets is 
well enough and does not bring in noise as the Ya-
hooX.  YahooT, however, has good but insufficient 
information, while YahooL is only suitable for dia-
logs directly related to links.  
Moreover, the experimental results show that 
verb is not a good feature no matter whether the 
category tree is expanded or not.  Although some 
verbs can explicitly point out the intention of dia-
logues, such as buy, sell, purchase, etc, the lack of 
verbs in Yahoo! directory makes the verb features 
less useful in the experiments.  Table 6 shows the 
performance of using Formula 2 (i.e., tf-idf 
scheme).  The original category tree with hyponym 
achieves the best performance, i.e., 56.56%.  How-
ever, it cannot compete with most of models with tf 
scheme. 
628
 
Table 5.  Performance of Models with tf Scheme 
 
Table 6. Performance of Models with tf-idf Scheme 
4.2 Hit Speed 
Besides precision, recall and F-score, we are al-
so interested if the system captures the intention of 
the dialogue at better timing.  We define one more 
metric called hit speed in Formula (3).  It repre-
sents how fast the sponsor links could be correctly 
suggested during the progress of conversations.  
For each utterance in a dialogue, we mark either X 
or a predicted category.  Here X denotes undecid-
able. 
Assume we have a dialogue of 7 utterances and 
consider the following scenario.  At first, our sys-
tem could not propose any candidates in the first 
two utterances.  Then, it decides the third and the 
fourth utterances are talking about Business and 
Economy.  Finally, it determines the intention of 
the dialogue is Computer and Internet after reading 
the next three utterances.  In this example, we get 
an answer string, XXBBCCC, based on the nota-
tions shown in Table 4.  If the intention annotated 
by human is Computer and Internet, then the sys-
tem starts proposing a correct intention from the 5th 
utterance.  In other words, the information in the 
first 4 utterances is not sufficient to make any deci-
sion or make wrong decision.   
Let CPL be the length of correct postfix of an 
answer string, e.g., 3, and N be total utterances in a 
dialogue, e.g., 7.  HitSpeed is defined as follows. 
N
CPL
HitSpeed =  (3)
In this case, the hit speed of intention identification 
is 3/7.  Intuitively, our goal is to get the hit speed 
as high as possible.  The sooner we get the correct 
intention, the better the recommendation effect is. 
The average hit speed is defined by Formulas (4) 
and (5).  The former considers only the correct dia-
logues, and the latter considers all the dialogues.  
Let M and N denote total dialogues and total cor-
rect dialogues, respectively. 
N
HitSpeed
vgHitSpeed
M
i i? == 1A  (4)
M
HitSpeed
vgHitSpeed
M
i i? == 1A  (5)
 
Figure 4.  Average Hit Speed by Formula (4) 
 
Figure 5.  Average Hit Speed by Formula (5) 
629
Figures 4 and 5 demonstrate average hit speeds 
computed by Formulas (4) and (5), respectively.   
Here four leading models shown in Table 5 are 
adopted and nouns are regarded as cue words. Fig-
ure 4 shows that the average hit speed in correctly 
answered dialogues is around 70%.  It means these 
models can correctly answer the intention when a 
dialogue still has 70% to go in the set of correctly 
answered dialogs. 
Figure 5 considers all the dialogues no matter 
whether their intentions are identified correctly or 
not.  We can still capture the intention with the hit 
speed 62.54% for the best model, i.e., YahooS-
noun.   
5 Concluding Remarks 
This paper captures intention in dialogues of in-
stant messaging applications.  A web directory 
such as Yahoo! directory is considered as a cate-
gory tree.  Two schemes, revised tf and tf-idf, are 
employed to classify the utterances in dialogues.  
The experiments show that the tf scheme using the 
category tree expanded with snippets together with 
noun features achieves the best F-score, 0.86.  The 
hit speed evaluation tells us the system can start 
making good decision when near only 37.46% of 
total utterances are processed.  In other words, the 
recommended advertisements can be placed to at-
tract users? attentions in the rest 62.54% of total 
utterances. 
Though the best model in the experiments is to 
use nouns as features, we note that another impor-
tant language feature, verbs, is not helpful due to 
the characteristic of the category tree we adopted, 
that is, the absence of verbs in Yahoo! directory. If 
some other data sources can provide the cue infor-
mation, verbs may be taken as useful features to 
boost the performance.  
In this paper, only one intention is assigned to 
the utterances.  However, there may be many par-
ticipants involving in a conversation, and the topics 
they are talking about in a dialogue may be more 
than one. For example, two couples are discussing 
a trip schedule together.  After the topic is finished, 
they may continue the conversation for selection of 
hotels and buying funds separately in the same in-
stant messaging dialogue.  In this case, our system 
only decides the intention is Recreation, but not 
including Business & Economy.  
Long time delay of response is another interest-
ing topic for instant messaging dialogues. Some-
times one participant could send a message, but 
have to wait for minutes or even hours to get re-
sponse.  Because the receiver might be absent, 
busy or just off-line, the system should be capable 
of waiting such a long time delay before a com-
plete dialogue is finished in practical applications. 
Opinion mining is also important to the pro-
posed model.  For example, dialogue participants 
may talk about buying digital cameras, and one of 
them has negative opinions on some products.  In 
such a case, an intelligent recommendation system 
should not promote such products.  Once opinion 
extraction is introduced to intention analysis sys-
tems, customers can get not only the conversation-
related, but also personally preferred sponsor links.  
Acknowledgments 
Research of this paper was partially supported by 
Excellent Research Projects of National Taiwan 
University, under the contract 95R0062-AE00-02. 
References 
H.H. Chen, J.J. Kuo, S.J. Huang, C.J. Lin and H.C. 
Wung. 2003. A Summarization System for Chinese 
News from Multiple Sources. Journal of American 
Society for Information Science and Technology, 
54(13), pp. 1224-1236. 
D. C. Fain and J. O. Pedersen. 2006. Sponsored Search: 
A Brief History. Bulletin of the American Society 
for Information Science and Technology, January. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Database. The MIT Press. 
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. 
Generating Query Substitutions. In Proceedings of 
the 15th International Conference on World Wide 
Web, 2006, pp. 387-396. 
K.K. Lin and H.H. Chen. 2004. Extracting Domain 
Knowledge for Dialogue Model Adaptation. In 
Proceedings of 5th International Conference on In-
telligent Text Processing and Computational Lin-
guistics, Lecture Notes in Computer Science, 
LNCS 2945, Springer-Verlag, pp. 70-78. 
W. Yih, J. Goodman, and V. R. Carvalho. 2006. Finding 
Advertising Keywords on Web Pages. In Proceed-
ings of the 15th International Conference on World 
Wide Web, pp. 213-222.  
630
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1009?1016,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Novel Association Measures Using Web Search with Double Checking 
 
 
Hsin-Hsi Chen Ming-Shun Lin Yu-Chuan Wei 
Department of Computer Science and Information Engineering  
National Taiwan University  
Taipei, Taiwan  
hhchen@csie.ntu.edu.tw;{mslin,ycwei}@nlg.csie.ntu.edu.tw
 
  
 
Abstract 
A web search with double checking 
model is proposed to explore the web as 
a live corpus.  Five association measures 
including variants of Dice, Overlap Ratio, 
Jaccard, and Cosine, as well as Co-
Occurrence Double Check (CODC), are 
presented. In the experiments on Ruben-
stein-Goodenough?s benchmark data set, 
the CODC measure achieves correlation 
coefficient 0.8492, which competes with 
the performance (0.8914) of the model 
using WordNet.  The experiments on link 
detection of named entities using the 
strategies of direct association, associa-
tion matrix and scalar association matrix 
verify that the double-check frequencies 
are reliable.  Further study on named en-
tity clustering shows that the five meas-
ures are quite useful.  In particular, 
CODC measure is very stable on word-
word and name-name experiments.  The 
application of CODC measure to expand 
community chains for personal name dis-
ambiguation achieves 9.65% and 14.22% 
increase compared to the system without 
community expansion.  All the experi-
ments illustrate that the novel model of 
web search with double checking is fea-
sible for mining associations from the 
web. 
1 Introduction 
In statistical natural language processing, re-
sources used to compute the statistics are indis-
pensable.  Different kinds of corpora have made 
available and many language models have been 
experimented.  One major issue behind the cor-
pus-based approaches is: if corpora adopted can 
reflect the up-to-date usage.  As we know, lan-
guages are live.  New terms and phrases are used 
in daily life.  How to capture the new usages is 
an important research topic. 
The Web is a heterogeneous document collec-
tion.  Huge-scale and dynamic nature are charac-
teristics of the Web.  Regarding the Web as a 
live corpus becomes an active research topic re-
cently.  How to utilize the huge volume of web 
data to measure association of information is an 
important issue.  Resnik and Smith (2003) em-
ploy the Web as parallel corpora to provide bi-
lingual sentences for translation models.  Keller 
and Lapata (2003) show that bigram statistics for 
English language is correlated between corpus 
and web counts.  Besides, how to get the word 
counts and the word association counts from the 
web pages without scanning over the whole col-
lections is indispensable. Directly managing the 
web pages is not an easy task when the Web 
grows very fast. 
Search engine provides some way to return 
useful information.  Page counts for a query de-
note how many web pages containing a specific 
word or a word pair roughly.  Page count is dif-
ferent from word frequency, which denotes how 
many occurrences a word appear.  Lin and Chen 
(2004) explore the use of the page counts pro-
vided by different search engines to compute the 
statistics for Chinese segmentation.  In addition 
to the page counts, snippets returned by web 
search, are another web data for training.  A 
snippet consists of a title, a short summary of a 
web page and a hyperlink to the web page.  Be-
cause of the cost to retrieve the full web pages, 
short summaries are always adopted (Lin, Chen, 
and Chen, 2005). 
Various measures have been proposed to 
compute the association of objects of different 
granularity like terms and documents.  Rodr?guez 
and Egenhofer (2003) compute the semantic 
1009
similarity from WordNet and SDTS ontology by 
word matching, feature matching and semantic 
neighborhood matching.  Li et al (2003) investi-
gate how information sources could be used ef-
fectively, and propose a new similarity measure 
combining the shortest path length, depth and 
local density using WordNet.  Matsuo et al 
(2004) exploit the Jaccard coefficient to build 
?Web of Trust? on an academic community. 
This paper measures the association of terms 
using snippets returned by web search.  A web 
search with double checking model is proposed 
to get the statistics for various association meas-
ures in Section 2.  Common words and personal 
names are used for the experiments in Sections 3 
and 4, respectively.  Section 5 demonstrates how 
to derive communities from the Web using asso-
ciation measures, and employ them to disam-
biguate personal names.  Finally, Section 6 con-
cludes the remarks. 
2 A Web Search with Double Checking 
Model 
Instead of simple web page counts and complex 
web page collection, we propose a novel model, 
a Web Search with Double Checking (WSDC), to 
analyze snippets.  In WSDC model, two objects X 
and Y are postulated to have an association if we 
can find Y from X (a forward process) and find X 
from Y (a backward process) by web search.  The 
forward process counts the total occurrences of Y 
in the top N snippets of query X, denoted as 
f(Y@X).  Similarly, the backward process counts 
the total occurrences of X in the top N snippets of 
query Y, denoted as f(X@Y).  The forward and 
the backward processes form a double check op-
eration. 
Under WSDC model, the association scores 
between X and Y are defined by various formulas 
as follows. 
???
???
?
+
+
=
=
=
Otherwise
YfXf
YXfXYf
YXf
orXYfif
YXeVariantDic
)()(
)@()@(
0)@(    
   0)@( 
0
    
),(
   (1) 
)()(
))@(),@((
,(
YfXf
YXfXYfmin
Y)XineVariantCos ?=
     (2) 
))@(),@(()()(
))@(),@((
    
YXfXYfmaxYfXf
YXfXYfmin
(X,Y)cardVariantJac
?+=
       (3) 
{ }
)}(),({
)@(),@(
 ),(
YfXfmin
YXfXYfmin
YXrlapVariantOve = (4) 
???
???
?
=
=
=
???
?
???
? ? Otherwise
YXf
orXYfif
YXCODC
Yf
YXf
Xf
XYflog
e
?
)(
)@(
)(
)@(
0)@(    
   0)@( 
0
),(
   
(5) 
Where f(X) is the total occurrences of X in the 
top N snippets of query X, and, similarly, f(Y) is 
the total occurrences of Y in the top N snippets of 
query Y.  Formulas (1)-(4) are variants of the 
Dice, Cosine, Jaccard, and Overlap Ratio asso-
ciation measure.  Formula (5) is a function 
CODC (Co-Occurrence Double-Check), which 
measures the association in an interval [0,1].  In 
the extreme cases, when f(Y@X)=0 or f(X@Y)=0, 
CODC(X,Y)=0; and when f(Y@X)=f(X) and 
f(X@Y)=f(Y), CODC(X,Y)=1.  In the first case, X 
and Y are of no association.  In the second case, 
X and Y are of the strongest association. 
3 Association of Common Words 
We employ Rubenstein-Goodenough?s (1965) 
benchmark data set to compare the performance 
of various association measures.  The data set 
consists of 65 word pairs.  The similarities be-
tween words, called Rubenstein and Goodenough 
rating (RG rating), were rated on a scale of 0.0 to 
4.0 for ?semantically unrelated? to ?highly syn-
onymous? by 51 human subjects.  The Pearson 
product-moment correlation coefficient, rxy, be-
tween the RG ratings X and the association 
scores Y computed by a model shown as follows 
measures the performance of the model. 
yx
i
n
i
i
xy ssn
yyxx
r
)1(
))((
1
?
??
=
?
=                             (6) 
Where x  and y  are the sample means of xi and 
yi, and sx and sy are sample standard deviations of 
xi and yi and n is total samples. 
Most approaches (Resink, 1995; Lin, 1998; Li 
et al, 2003) used 28 word pairs only.  Resnik 
(1995) obtained information content from 
WordNet and achieved correlation coefficient 
0.745.  Lin (1998) proposed an information-
theoretic similarity measure and achieved a cor-
relation coefficient of 0.8224.  Li et al (2003) 
combined semantic density, path length and 
depth effect from WordNet and achieved the cor-
relation coefficient 0.8914. 
1010
 100 200 300 400 500 600 700 800 900 
VariantDice 0.5332 0.5169 0.5352 0.5406 0.5306 0.5347 0.5286 0.5421 0.5250
VariantOverlap 0.5517 0.6516 0.6973 0.7173 0.6923 0.7259 0.7473 0.7556 0.7459
VariantJaccard 0.5533 0.6409 0.6993 0.7229 0.6989 0.738 0.7613 0.7599 0.7486
VariantCosine 0.5552 0.6459 0.7063 0.7279 0.6987 0.7398 0.7624 0.7594 0.7501
CODC (?=0.15) 0.5629 0.6951 0.8051 0.8473 0.8438 0.8492 0.8222 0.8291 0.8182
Jaccard Coeff* 0.5847 0.5933 0.6099 0.5807 0.5463 0.5202 0.4855 0.4549 0.4622
Table 1. Correlation Coefficients of WSDC Model on Word-Word Experiments 
Model RG Rating 
Resnik 
(1995) 
Lin 
(1998) 
Li et al
(2003) 
VariantCosine 
(#snippets=700) 
WSDC 
CODC(?=0.15, 
#snippets=600)
WSDC 
Correlation Coefficient - 0.7450 0.8224 0.8914 0.7624 0.8492 
chord-smile 0.02 1.1762 0.20 0 0 0 
rooster-voyage 0.04 0 0 0 0 0 
noon-string 0.04 0 0 0 0 0 
glass-magician 0.44 1.0105 0.06 0 0 0 
monk-slave 0.57 2.9683 0.18 0.350 0 0 
coast-forest 0.85 0 0.16 0.170 0.0019 0.1686 
monk-oracle 0.91 2.9683 0.14 0.168 0 0 
lad-wizard 0.99 2.9683 0.20 0.355 0 0 
forest-graveyard 1 0 0 0.132 0 0 
food-rooster 1.09 1.0105 0.04 0 0 0 
coast-hill 1.26 6.2344 0.58 0.366 0 0 
car-journey 1.55 0 0 0 0.0014 0.2049 
crane-implement 2.37 2.9683 0.39 0.366 0 0 
brother-lad 2.41 2.9355 0.20 0.355 0.0027 0.1811 
bird-crane 2.63 9.3139 0.67 0.472 0 0 
bird-cock 2.63 9.3139 0.83 0.779 0.0058 0.2295 
food-fruit 2.69 5.0076 0.24 0.170 0.0025 0.2355 
brother-monk 2.74 2.9683 0.16 0.779 0.0027 0.1956 
asylum-madhouse 3.04 15.666 0.97 0.779 0.0015 0.1845 
furnace-stove 3.11 1.7135 0.18 0.585 0.0035 0.1982 
magician-wizard 3.21 13.666 1 0.999 0.0031 0.2076 
journey-voyage 3.58 6.0787 0.89 0.779 0.0086 0.2666 
coast-shore 3.6 10.808 0.93 0.779 0.0139 0.2923 
implement-tool 3.66 6.0787 0.80 0.778 0.0033 0.2506 
boy-lad 3.82 8.424 0.85 0.778 0.0101 0.2828 
Automobile-car 3.92 8.0411 1 1 0.0144 0.4229 
Midday-noon 3.94 12.393 1 1 0.0097 0.2994 
gem-jewel 3.94 14.929 1 1 0.0107 0.3530 
Table 2. Comparisons of WSDC with Models in Previous Researches 
In our experiments on the benchmark data set, 
we used information from the Web rather than 
WordNet.  Table 1 summarizes the correlation 
coefficients between the RG rating and the asso-
ciation scores computed by our WSDC model. 
We consider the number of snippets from 100 to 
900.  The results show that CODC > VariantCo-
sine > VariantJaccard > VariantOverlap > Vari-
antDice.  CODC measure achieves the best per-
formance 0.8492 when ?=0.15 and total snippets 
to be analyzed are 600.  Matsuo et al (2004) 
used Jaccard coefficient to calculate similarity 
between personal names using the Web. The co-
efficient is defined as follows.   
1011
)(
)(
),( 
YXf
YXf
YXCoffJaccard ?
?=                                  (7) 
Where f(X?Y) is the number of pages including 
X?s and Y?s homepages when query ?X and Y? is 
submitted to a search engine; f(X?Y) is the num-
ber of pages including X?s or Y?s homepages 
when query ?X or Y? is submitted to a search en-
gine.  We revised this formula as follows and 
evaluated it with Rubenstein-Goodenough?s 
benchmark. 
)(
)(
),( *
YXf
YXf
YXCoffJaccard
s
s
?
?=                             (8) 
Where fs(X?Y) is the number of snippets in 
which X and Y co-occur in the top N snippets of 
query ?X and Y?; fs(X?Y) is the number of snip-
pets containing X or Y in the top N snippets of 
query ?X or Y?.  We test the formula on the same 
benchmark.  The last row of Table 1 shows that 
Jaccard Coeff* is worse than other models when 
the number of snippets is larger than 100.  
Table 2 lists the results of previous researches 
(Resink, 1995; Lin, 1998; Li et al, 2003) and our 
WSDC models using VariantCosine and CODC 
measures.  The 28 word pairs used in the ex-
periments are shown.  CODC measure can com-
pete with Li et al (2003).  The word pair ?car-
journey? whose similarity value is 0 in the papers 
(Resink, 1995; Lin, 1998; Li et al, 2003) is cap-
tured by our model.  In contrast, our model can-
not deal with the two word pairs ?crane-
implement? and ?bird-crane?.  
4 Association of Named Entities 
Although the correlation coefficient of WSDC 
model built on the web is a little worse than that 
of the model built on WordNet, the Web pro-
vides live vocabulary, in particular, named enti-
ties.  We will demonstrate how to extend our 
WSDC method to mine the association of per-
sonal names.  That will be difficult to resolve 
with previous approaches.  We design two ex-
periments ? say, link detection test and named 
entity clustering, to evaluate the association of 
named entities. 
Given a named-entity set L, we define a link 
detection test to check if any two named entities 
NEi and NEj (i?j) in L have a relationship R using 
the following three strategies. 
? Direct Association: If the double check 
frequency of NEi and NEj is larger than 0,  
 
Figure 1. Three Strategies for Link Detection 
i.e., f(NEj@NEi)>0 and f(NEi@NEj)>0, 
then the link detection test says ?yes?, i.e., 
NEi and NEj have direct association.  Oth-
erwise, the test says ?no?.  Figure 1(a) 
shows the direct association. 
? Association Matrix: Compose an n?n bi-
nary matrix M=(mij), where mij=1 if 
f(NEj@NEi)>0 and f(NEi@NEj)>0; mij=0 
if f(NEj@NEi)=0 or f(NEi@NEj)=0; and n 
is total number of named entities in L.  Let 
Mt be a transpose matrix of M.  The matrix 
A=M?Mt is an association matrix.  Here 
the element aij in A means that total aij 
common named entities are associated 
with both NEi and NEj directly.  Figure 1(b) 
shows a one-layer indirect association.  
Here, aij=3.  We can define NEi and NEj 
have an indirect association if aij is larger 
than a threshold ?.  That is, NEi and NEj 
should associate with at least ? common 
named entities directly.  The strategy of 
association matrix specifies: if aij??, then 
the link detection test says ?yes?, other-
wise it says ?no?.  In the example shown 
in Figure 1(b), NEi and NEj are indirectly 
associated when 0<??3. 
? Scalar Association Matrix: Compose a 
binary association matrix B from the asso-
ciation matrix A as: bij=1 if aij>0 and bij=0 
if aij=0.  The matrix S= B?Bt is a scalar as-
1012
sociation matrix. NEi and NEj may indi-
rectly associate with a common named en-
tity NEk.  Figure 1(c) shows a two-layer 
indirect association.  The ? = ?= nk kjikij bbs 1  
denotes how many such an NEk there are. 
In the example of Figure 1(c), two named 
entities indirectly associate NEi and NEj at 
the same time.  We can define NEi and NEj 
have an indirect association if sij is larger 
than a threshold ?.  In other words, if sij >?, 
then the link detection test says ?yes?, 
otherwise it says ?no?. 
To evaluate the performance of the above 
three strategies, we prepare a test set extracted 
from domz web site (http://dmoz.org), the most 
comprehensive human-edited directory of the 
Web.  The test data consists of three communi-
ties: actor, tennis player, and golfer, shown in 
Table 3.  Total 220 named entities are considered.  
The golden standard of link detection test is: we 
compose 24,090 (=220?219/2) named entity 
pairs, and assign ?yes? to those pairs belonging 
to the same community.  
Category Path in domz.org # of Person Names 
Top: Sports: Golf: Golfers  10 
Top: Sports: Tennis: Players:  
Female (+Male)  90 
Top: Arts: People: Image Galleries: 
Female (+Male): Individual 120 
Table 3. Test Set for Association Evaluation of 
Named Entities 
When collecting the related values for com-
puting the double check frequencies for any 
named entity pair (NEi and NEj), i.e., f(NEj@NEi), 
f(NEi@NEj), f(NEi), and f(NEj), we consider 
naming styles of persons.  For example, ?Alba, 
Jessica? have four possible writing: ?Alba, Jes-
sica?, ?Jessica Alba?, ?J. Alba? and ?Alba, J.?  
We will get top N snippets for each naming style, 
and filter out duplicate snippets as well as snip-
pets of ULRs including dmoz.org and 
google.com.  Table 4 lists the experimental re-
sults of link detection on the test set.  The preci-
sions of two baselines are: guessing all ?yes? 
(46.45%) and guessing all ?no? (53.55%).  All 
the three strategies are better than the two base-
lines and the performance becomes better when 
the numbers of snippets increase.  The strategy 
of direct association shows that using double 
checks to measure the association of named enti-
ties also gets good effects as the association of 
common words.  For the strategy of association 
matrix, the best performance 90.14% occurs in 
the case of 900 snippets and ?=6.  When larger 
number of snippets is used, a larger threshold is 
necessary to achieve a better performance.  Fig-
ure 2(a) illustrates the relationship between pre-
cision and threshold (?).  The performance de-
creases when ?>6.  The performance of the strat-
egy of scalar association matrix is better than that 
of the strategy of association matrix in some ? 
and ?.  Figure 2(b) shows the relationship be-
tween precision and threshold ? for some number 
of snippets and ?. 
In link detection test, we only consider the bi-
nary operation of double checks, i.e., f(NEj@NEi) 
> 0 and f(NEi@NEj) > 0, rather than utilizing the 
magnitudes of f(NEj@NEi) and f(NEi@NEj).  
Next we employ the five formulas proposed in 
Section 2 to cluster named entities.  The same 
data set as link detection test is adopted. An ag-
glomerative average-link clustering algorithm is 
used to partition the given 220 named entities 
based on Formulas (1)-(5).  Four-fold cross-
validation is employed and B-CUBED metric 
(Bagga and Baldwin, 1998) is adopted to evalu-
ate the clustering results.  Table 5 summarizes 
the experimental results.  CODC (Formula 5), 
which behaves the best in computing association 
of common words, still achieves the better per-
formance on different numbers of snippets in 
named entity clustering.  The F-scores of the 
other formulas are larger than 95% when more 
snippets are considered to compute the double 
check frequencies. 
 
Strategies 100 200 300 400 500 600 700 800 900 
Direct  
Association 59.20% 62.86% 65.72% 67.88% 69.83% 71.35% 72.05% 72.46% 72.55%
Association 
Matrix 
71.53% 
(?=1) 
79.95% 
(?=1) 
84.00%
(?=2) 
86.08%
(?=3) 
88.13%
(?=4) 
89.67%
(?=5) 
89.98% 
(?=5) 
90.09% 
(?=6) 
90.14%
(?=6) 
Scalar Asso-
ciation 
Matrix 
73.93% 
(?=1, 
?=6) 
82.69% 
(?=2, 
?=9) 
86.70%
(?=4, 
?=9) 
88.61%
(?=5, 
?=10) 
90.90%
(?=6, 
?=12) 
91.93%
(?=7, 
?=12) 
91.90% 
(?=7, 
?=18) 
92.20% 
(?=10, 
?=16) 
92.35%
(?=10,
?=18) 
Table 4. Performance of Link Detection of Named Entities 
1013
 
                                   (a)                                                                            (b)                              
Figure 2. (a) Performance of association matrix strategy.  (b) Performance of scalar association matrix 
strategy (where ? is fixed and its values reference to scalar association matrix in Table 4) 
  100 200 300 400 500 600 700 800 900 
P 91.70% 88.71% 87.02% 87.49% 96.90% 100.00% 100.00% 100.00% 100.00%
R 55.80% 81.10% 87.70% 93.00% 89.67% 93.61% 94.42% 94.88% 94.88%VariantDice 
F 69.38% 84.73% 87.35% 90.16% 93.14% 96.69% 97.12% 97.37% 97.37%
P 99.13% 87.04% 85.35% 85.17% 88.16% 88.16% 88.16% 97.59% 98.33%
R 52.16% 81.10% 86.24% 93.45% 92.03% 93.64% 92.82% 90.82% 93.27%VariantOverlap  
F 68.35% 83.96% 85.79% 89.11% 90.05% 90.81% 90.43% 94.08% 95.73%
P 99.13% 97.59% 98.33% 95.42% 97.59% 88.16% 95.42% 100.00% 100.00%
R 55.80% 77.53% 84.91% 88.67% 87.18% 90.58% 88.67% 93.27% 91.64%VariantJaccard 
F 71.40% 86.41% 91.12% 91.92% 92.09% 89.35% 91.92% 96.51% 95.63%
P 84.62% 97.59% 85.35% 85.17% 88.16% 88.16% 88.16% 98.33% 98.33%
R 56.22% 78.92% 86.48% 93.45% 92.03% 93.64% 93.64% 93.27% 93.27%VariantCosine 
F 67.55% 87.26% 85.91% 89.11% 90.05% 90.81% 90.81% 95.73% 95.73%
P 91.70% 87.04% 87.02% 95.93% 98.33% 95.93% 95.93% 94.25% 94.25%
R 55.80% 81.10% 90.73% 94.91% 94.91% 96.52% 98.24% 98.24% 98.24%CODC 
(?=0.15) 
F 69.38% 83.96% 88.83% 95.41% 96.58% 96.22% 97.07% 96.20% 96.20%
Table 5. Performance of Various Scoring Formulas on Named Entity Clustering 
5 Disambiguation Using Association of 
Named Entities 
This section demonstrates how to employ asso-
ciation mined from the Web to resolve the ambi-
guities of named entities.  Assume there are n 
named entities, NE1, NE2, ?, and NEn, to be dis-
ambiguated.  A named entity NEj has m accom-
panying names, called cue names later, CNj1, 
CNj2, ?, CNjm.  We have two alternatives to use 
the cue names.  One is using them directly, i.e., 
NEj is represented as a community of cue names 
Community(NEj)={CNj1, CNj2, ?, CNjm}.  The 
other is to expand the cue names CNj1, CNj2, ?, 
CNjm for NEj using the web data as follows.  Let 
CNj1 be an initial seed.  Figure 3 sketches the 
concept of community expansion. 
(1) Collection: We submit a seed to 
Google, and select the top N returned 
snippets.  Then, we use suffix trees to 
extract possible patterns (Lin and Chen, 
2006). 
(2) Validation: We calculate CODC score 
of each extracted pattern (denoted Bi) 
with the seed A.  If CODC(A,Bi) is 
strong enough, i.e., larger than a 
1014
threshold ?, we employ Bi as a new 
seed and repeat steps (1) and (2).  This 
procedure stops either expected number 
of nodes is collected or maximum 
number of layers is reached. 
(3) Union: The community initiated by the 
seed CNji is denoted by Commu-
nity(CNji)={Bji1, Bji2, ?, BBjir}, where Bjik 
is a new seed.  The Cscore score, com-
munity score, of BjikB  is the CODC score 
of Bjik with its parent divided by the 
layer it is located.  We repeat Collec-
tion and Validation steps until all the 
cue names CNji (1?i?m) of NEj are 
processed.  Finally, we have 
)()( 1 ji
m
ij CNCommunityNECommunity =?=  
 
Figure 3. A Community for a Seed ????? 
(?Chien-Ming Wang?) 
In a cascaded personal name disambiguation 
system (Wei, 2006), association of named enti-
ties is used with other cues such as titles, com-
mon terms, and so on.  Assume k clusters, c1 c2 ... 
ck, have been formed using title cue, and we try 
to place NE1, NE2, ?, and NEl into a suitable 
cluster.  The cluster c  is selected by the similar-
ity measure defined below. 
)()(
1
    
),(
1 ii
s
i
qj
pnscoreCpncount
r
cNEscore
?= ?=             (9) 
),(maxarg
)kq1(c q
qj cNEscorec ??
=                     (10) 
Where pn1, pn2, ?, pns are names which appear 
in both Community(NEj) and Community(cq); 
count(pni) is total occurrences of pni in Commu-
nity(cq); r is total occurrences of names in Com-
munity(NEj); Cscore(pni) is community score of 
pni.  
If score(NEj, c ) is larger than a threshold, 
then NEj is placed into cluster c .  In other words, 
NEj denotes the same person as those in c .  We 
let the new Community( c ) be the old Commu-
nity( c )?{CNj1, CNj2, ?, CNjm}.  Otherwise, NEj 
is left undecided. 
To evaluate the personal name disambiguation, 
we prepare three corpora for an ambiguous name 
???? ? (Chien-Ming Wang) from United 
Daily News Knowledge Base (UDN), Google 
Taiwan (TW), and Google China (CN).  Table 6 
summarizes the statistics of the test data sets.  In 
UDN news data set, 37 different persons are 
mentioned.  Of these, 13 different persons occur 
more than once.  The most famous person is a 
pitcher of New York Yankees, which occupies 
94.29% of 2,205 documents.  In TW and CN 
web data sets, there are 24 and 107 different per-
sons.  The majority in TW data set is still the 
New York Yankees?s ?Chien-Ming Wang?.  He 
appears in 331 web pages, and occupies 88.03%.  
Comparatively, the majority in CN data set is a 
research fellow of Chinese Academy of Social 
Sciences, and he only occupies 18.29% of 421 
web pages.  Total 36 different ?Chien-Ming 
Wang?s occur more than once.  Thus, CN is an 
unbiased corpus. 
 UDN TW CN 
# of documents 2,205 376 421 
# of persons 37 24 107 
# of persons of 
occurrences>1 13 9 36 
Majority  94.29% 88.03% 18.29%
Table 6. Statistics of Test Corpora 
 M1 M2 
P 0.9742 0.9674 (?0.70%) 
R 0.9800 0.9677 (?1.26%) UDN
F 0.9771 0.9675 (?0.98%) 
P 0.8760 0.8786 (?0.07%) 
R 0.6207 0.7287 (?17.40%) TW
F 0.7266 0.7967 (?9.65%) 
P 0.4910 0.5982 (?21.83%) 
R 0.8049 0.8378 (?4.09%) CN
F 0.6111 0.6980 (?14.22%) 
Table 7. Disambiguation without/with Commu-
nity Expansion 
1015
Table 7 shows the performance of a personal 
name disambiguation system without (M1)/with 
(M2) community expansion.  In the news data set 
(i.e., UDN), M1 is a little better than M2.  Com-
pared to M1, M2 decreases 0.98% of F-score.  In 
contrast, in the two web data sets (i.e., TW and 
CN), M2 is much better than M1.  M2 has 9.65% 
and 14.22% increases compared to M1.  It shows 
that mining association of named entities from 
the Web is very useful to disambiguate ambigu-
ous names.  The application also confirms the 
effectiveness of the proposed association meas-
ures indirectly. 
6 Concluding Remarks 
This paper introduces five novel association 
measures based on web search with double 
checking (WSDC) model.  In the experiments on 
association of common words, Co-Occurrence 
Double Check (CODC) measure competes with 
the model trained from WordNet.  In the experi-
ments on the association of named entities, 
which is hard to deal with using WordNet, 
WSDC model demonstrates its usefulness.  The 
strategies of direct association, association ma-
trix, and scalar association matrix detect the link 
between two named entities.  The experiments 
verify that the double-check frequencies are reli-
able.   
Further study on named entity clustering 
shows that the five measures ? say, VariantDice, 
VariantOverlap, ariantJaccard, VariantCosine 
and CODC, are quite useful.  In particular, 
CODC is very stable on word-word and name-
name experiments.  Finally, WSDC model is 
used to expand community chains for a specific 
personal name, and CODC measures the associa-
tion of community member and the personal 
name.  The application on personal name disam-
biguation shows that 9.65% and 14.22% increase 
compared to the system without community ex-
pansion. 
Acknowledgements 
Research of this paper was partially supported by 
National Science Council, Taiwan, under the 
contracts 94-2752-E-001-001-PAE and 95-2752-
E-001-001-PAE. 
References 
A. Bagga and B. Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Space 
Model.  Proceedings of 36th COLING-ACL Con-
ference, 79-85. 
F. Keller and M. Lapata. 2003. Using the Web to Ob-
tain Frequencies for Unseen Bigrams. Computa-
tional Linguistics, 29(3): 459-484. 
Y. Li, Z.A. Bandar and D. McLean. 2003. An Ap-
proach for Measuring Semantic Similarity between 
Words Using Multiple Information Sources.  IEEE 
Transactions on Knowledge and Data Engineering, 
15(4): 871-882. 
D. Lin. 1998. An Information-Theoretic Definition of 
Similarity. Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, 296-304. 
H.C. Lin and H.H. Chen. 2004. Comparing Corpus-
based Statistics and Web-based Statistics: Chinese 
Segmentation as an Example. Proceedings of 16th 
ROCLING Conference, 89-100. 
M.S. Lin, C.P. Chen and H.H. Chen. 2005. An Ap-
proach of Using the Web as a Live Corpus for 
Spoken Transliteration Name Access. Proceedings 
of 17th ROCLING Conference, 361-370. 
M.S. Lin and H.H. Chen. 2006. Constructing a 
Named Entity Ontology from Web Corpora. Pro-
ceedings of 5th International Conference on Lan-
guage Resources and Evaluation. 
Y. Matsuo, H. Tomobe, K. Hasida, and M. Ishizuka. 
2004. Finding Social Network for Trust Calcula-
tion. Proceedings of 16th European Conference on 
Artificial Intelligence, 510-514. 
P. Resnik. 1995. Using Information Content to Evalu-
ate Semantic Similarity in a Taxonomy. Proceed-
ings of the 14th International Joint Conference on 
Artificial Intelligence, 448-453. 
P. Resnik and N.A. Smith. 2003. The Web as a Paral-
lel Corpus. Computational Linguistics, 29(3): 349-
380. 
M.A. Rodr?guez and M.J. Egenhofer. 2003. Determin-
ing Semantic Similarity among Entity Classes from 
Different Ontologies.  IEEE Transactions on 
Knowledge and Data Engineering, 15(2): 442-456. 
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual Correlates of Synonymy. Communications of 
the ACM, 8(10): 627-633. 
Y.C. Wei. 2006. A Study of Personal Name Disam-
biguation. Master Thesis, Department of Computer 
Science and Information Engineering, National 
Taiwan University, Taiwan. 
 
 
 
 
1016
