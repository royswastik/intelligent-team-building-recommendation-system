BioNLP 2007: Biological, translational, and clinical language processing, pages 89?96,
Prague, June 2007. c?2007 Association for Computational Linguistics
BioNoculars: Extracting Protein-Protein Interactions from Biomedical Text
Amgad Madkour, *Kareem Darwish, Hany Hassan, Ahmed Hassan, Ossama Emam
Human Language Technologies Group
IBM Cairo Technology Development Center
P.O.Box 166 El-Ahram, Giza, Egypt
{amadkour,hanyh,hasanah,emam}@eg.ibm.com,*kareem@darwish.org
Abstract
The vast number of published medical doc-
uments is considered a vital source for rela-
tionship discovery. This paper presents a sta-
tistical unsupervised system, called BioNoc-
ulars, for extracting protein-protein interac-
tions from biomedical text. BioNoculars
uses graph-based mutual reinforcement to
make use of redundancy in data to construct
extraction patterns in a domain independent
fashion. The system was tested using MED-
LINE abstract for which the protein-protein
interactions that they contain are listed in the
database of interacting proteins and protein-
protein interactions (DIPPPI). The system
reports an F-Measure of 0.55 on test MED-
LINE abstracts.
1 Introduction
With the ever-increasing number of published
biomedical research articles and the dependency
of new research and previously published research,
medical researchers and practitioners are faced with
the daunting prospect of reading through hundreds
or possibly thousands of research articles to sur-
vey advances in areas of interest. Much work has
been done to ease access and discovery of articles
that match the interest of researchers via the use
of search engines such as PubMed, which provides
search capabilities over MEDLINE, a collection of
more than 15 million journal paper abstracts main-
tained by the National Library of Medicine (NLM).
However, with the addition of abstracts from more
than 5,000 medical journals to MEDLINE every
year, the number of articles containing information
that is pertinent to users needs has grown consider-
ably. These 5,000 journals constitute only a subset
of the published biomedical research. Further, med-
ical articles often contain redundant information and
only subsections of articles are typically of direct in-
terest to researchers. More advanced information
extraction tools have been developed to effectively
distill medical articles to produce key pieces of in-
formation from articles while attempting to elimi-
nate redundancy. These tools have focused on areas
such as protein-protein interaction, gene-disease re-
lationship, and chemical-protein interaction (Chun
et al, 2006). Many of these tools have been used
to extract key pieces of information from MED-
LINE. Most of the reported information extraction
approaches use sets of handcrafted rules in conjunc-
tion with manually curated dictionaries and ontolo-
gies.
This paper presents a fully unsupervised statisti-
cal technique to discover protein-protein interaction
based on automatically discoverable repeating pat-
terns in text that describe relationships. The paper
is organized as follows: section 2 surveys related
work; section 3 describes BioNoculars; Section 4
describes the employed experimental setup; section
5 reports and comments on experimental results; and
section 6 concludes the paper.
2 Background
The background will focus primarily on the tagging
of Biomedical Named Entities (BNE), such genes,
gene-products, proteins, and chemicals and the Ex-
89
traction of protein-protein interactions from text.
2.1 BNE Tagging
Concerning BNE tagging, the most common ap-
proaches are based on hand-crafted rules, statisti-
cal classifiers, or a hybrid of both (usually in con-
junction with dictionaries of BNE). Rule-based sys-
tems (Fukuda et al, 1998; Hanisch et al, 2003; Ya-
mamoto et al, 2003) that use dictionaries tend to
exhibit high precision in tagging named entities but
generally with lower tagging recall. They tend to
lag the latest published research and are sensitive
to the expression of the named entities. Dictionar-
ies of BNE are typically laborious and expensive to
build, and they are dependant on nomenclatures and
specific species. Statistical approaches (Collier et
al., 2000; Kazama et al, 2002; Settles, 2004) typ-
ically improve recall at the expense of precision,
but are more readily retargetable for new nomen-
clatures and organisms. Hybrid systems (Tanabe
and Wilbur, 2002; Mika and Rost, 2004) attempt to
take advantage of both approaches. Although these
approaches tend to generate acceptable recognition,
they are heavily dependent on the type of data on
which they are trained.
(Fukuda et al, 1998) proposed a rule-based pro-
tein name extraction system called PROPER (PRO-
tein Proper-noun phrase Extracting Rules) system,
which utilizes a set of rules based on the surface
form of text in conjunction with a Part-Of-Speech
(POS) tagging to identify what looks like a protein
without referring to any specific BNE dictionary.
They reported a 94.7% precision and a 98.84% re-
call for the identification of BNEs. The results that
they achieved seem to be too specific to their train-
ing and test sets.
(Hanisch et al, 2003) proposed a rule-based
protein and gene name extraction system called
ProMiner, which is based on the construction of a
general-purpose dictionary along with different dic-
tionaries of synonyms and an automatic curation
procedure based on a simple token model of protein
names. Results showed that their system achieved a
0.80 F-measure score in the name extraction task on
the BioCreative test set (BioCreative).
(Yamamoto et al, 2003) proposed the use of mor-
phological analysis to improve protein name tag-
ging. Their approach tags proteins based on mor-
pheme chunking to properly determine protein name
boundary. They used the GENIA corpus for training
and testing and obtained an F-measure score of 0.70
for protein name tagging.
(Collier et al, 2000) used a machine learning ap-
proach to protein name extraction based on a linear
interpolation Hidden Markov Model (HMM) trained
using bi-grams. They focused on finding the most
likely protein sequence classes (C) for a given se-
quence of words (W), by maximizing the probabil-
ity of C given W, P(C?W). Unlike traditional dic-
tionary based methods, the approach uses no manu-
ally crafted patterns. However, their approach may
misidentify term boundaries for phrases containing
potentially ambiguous local structures such as co-
ordination and parenthesis. They reported an F-
measure score of 0.73 for different mixtures of mod-
els tested on 20 abstracts.
(Kazama et al, 2002) proposed a machine learn-
ing approach to BNE tagging based on support vec-
tor machines (SVM), which was trained on the GE-
NIA corpus. Their preliminary results of the system
showed that the SVM with the polynomial kernel
function outperforms techniques of Maximum En-
tropy based systems.
Yet another BNE tagging system is ABNER (Set-
tles, 2005), which utilizes machine learning, namely
conditional random fields, with a variation of or-
thographic and contextual features and no seman-
tic or syntactic features. ABNER achieves an F-
measure score of 0.71 on the NLPA 2004 shared
task dataset corpus and 0.70 on the BioCreative cor-
pus.and scored an F1-measure of 51.8set.
(Tanabe and Wilbur, 2002) used a combination
of statistical and knowledge-based strategies, which
utilized automatically generated rules from transfor-
mation based POS tagging and other generated rules
from morphological clues, low frequency trigrams,
and indicator terms. A key step in their method is
the extraction of multi-word gene and protein names
that are dominant in the corpus but inaccessible to
the POS tagger. The advantage of such an approach
is that it is independent of any biomedical domain.
However, it can miss single word gene names that
do not occur in contextual gene theme terms. It
can also incorrectly tag compound gene names, plas-
mids, and phages.
(Mika and Rost, 2004) developed NLProt, which
90
combines the use of dictionaries, rules-based filter-
ing, and machine learning based on an SVM classi-
fier to tag protein names in MEDLINE. The NLProt
system used rules for pre-filtering and the SVM for
classification, and it achieved a precision of 75% and
recall 76%.
2.2 Relationship Extraction
As for the extraction of interactions, most efforts in
extraction of biomedical interactions between enti-
ties from text have focused on using rule-based ap-
proaches due to the familiarity of medical terms that
tend to describe interactions. These approaches have
proven to be successful with notably good results. In
these approaches, most researchers attempted to de-
fine an accurate set of rules to describe relationship
types and patterns and to build ontologies and dic-
tionaries to be consulted in the extraction process.
These rules, ontologies, and dictionaries are typi-
cally domain specific and are often not generalizable
to other problems.
(Blaschke et al, 1999) reported a domain spe-
cific approach for extracting protein-protein interac-
tions from biomedical text based on a set of pre-
defined patterns and words describing interactions.
Later work attempted to automatically extract inter-
actions, which are referenced in the database of in-
teracting proteins (Xenarios et al, 2000), from the
text mentioning the interactions (Blaschke and Va-
lencia, 2001). They achieved surprisingly low recall
(25%), which they attributed to problems in properly
identifying protein names in the text.
(Koike et al, 2005) developed a system called
PRIME, which was used to extract biological func-
tions of genes, proteins, and their families. Their
system used a shallow parser and sentence struc-
ture analyzer. They extracted so-called ACTOR-
OBJECT relationships from the shallow parsed sen-
tences using rule based sentence structure analysis.
The identification of BNEs was done by consulting
the GENA gene name dictionary and family name
dictionary. In extracting the biological functions of
genes and proteins, their system reported a recall of
64% and a precision of 94%.
Saric et al developed a system to extract gene
expression regulatory information in yeast as well
as other regulatory mechanisms such phosphoryla-
tion (Saric et al, 2004; Saric et al, 2006). They
used a rule based named entity recognition module,
which recognizes named entities via cascading finite
state automata. They reported a precision of 83-90%
and 86-95% for the extraction of gene expression
and phosphorylation regulatory information respec-
tively.
(Leroy and Chen, 2005) used linguistic parsers
and Concept Spaces, which use a generic co-
occurrence based technique that extracts relevant
medical phrases using a noun chunker. Their system
employed UMLS (Humphreys and Lindberg, 1993),
GO (Ashburner et al, 2000), and GENA (Koike and
Takagi, 2004) to further improve extraction. Their
main purpose was entity identification and cross ref-
erence to other databases to obtain more knowledge
about entities involved in the system.
Other extraction approaches such as the one re-
ported on by (Cooper and Kershenbaum, 2005) uti-
lized a large manually curated dictionary of many
possible combinations of gene/protein names and
aliases from different databases and ontologies.
They annotated their corpus using a dictionary-
based longest matching technique. In addition, they
used filtering with a maximum entropy based named
entity recognizer in order to remove the false posi-
tives that were generated from merging databases.
The problem with this approach is the resulting in-
consistencies from merging databases, which could
hurt the effectiveness of the system. They reported
a recall of 87.1 % and a precision of 78.5% in the
relationship extraction task.
Work by (Mack et al, 2004) used the Munich In-
formation Center for Protein Sequences (MIPS) for
entity identification. Their system was integrated in
the IBM Unstructured Information Management Ar-
chitecture (UIMA) framework (Ferrucci and Lally,
2004) for tokenization, identification of entities, and
extraction of relations. Their approach was based on
a combination of computational linguistics, statis-
tics, and domain specific rules to detect protein in-
teractions. They reported a recall of 61% and a pre-
cision of 97%.
(Hao et al, 2005) developed an unsupervised ap-
proach, which also uses patterns that were deduced
using minimum description lengths. They used pat-
tern optimization techniques to enhance the patterns
by introducing most common keywords that tend to
describe interactions.
91
(Jo?rg et. al., 2005) developed Ali Baba which
uses sequence alignments applied to sentences an-
notated with interactions and part of speech tags.It
also uses finite state automata optimized with a ge-
netic algorithm in its approach. It then matches the
generated patterns against arbitrary text to extract in-
teractions and their respective partners. The system
scored an F1-measure of 51.8% on the LLL?05 eval-
uation set.
The aforementioned systems used either rule-
based approaches, which require manual interven-
tion from domain experts, or statistical approaches,
either supervised or semi-supervised, which also re-
quire manually curated training data.
3 BioNoculars
BioNoculars is a relationship extraction system that
based on a fully unsupervised technique suggested
by (Hassan et al, 2006) to automatically extract
protein-protein interaction from medical articles. It
can be retargeted to different domains such as pro-
tein interactions in diseases. The only requirement
is to compile domain specific taggers and dictionar-
ies, which would aid the system in performing the
required task.
The approach uses an unsupervised graph-based
mutual reinforcement, which depends on the con-
struction of generalized extraction patterns that
could match instances of relationships (Hassan et
al., 2006). Graph-based mutual reinforcement is
similar to the idea of hubs and authorities in web
pages depicted by the HITS algorithm (Kleinberg,
1998). The basic idea behind the algorithm is that
the importance of a page increases when more and
more good pages link to it. The duality between pat-
terns and extracted information (tuples) leads to the
fact that patterns could express different tuples, and
tuples in turn could be expressed by different pat-
terns. Tuple in this context contains three elements,
namely two proteins and the type of interaction be-
tween them. The proposed approach is composed of
two main steps, namely initial pattern construction
and then pattern induction.
For pattern construction, the text is POS tagged
and BNE tagged. The tags of Noun Phrases or se-
quences of nouns that constitute a BNE are removed
and replaced with a BNE tag. Then, an n-gram lan-
guage model is built on the tagged text (using tags
only) and is used to construct weighted finite state
machines. Paths with low cost (high language model
probabilities) are chosen to construct the initial set
of patterns; the intuition is that paths with low cost
(high probability) are frequent and could represent
potential candidate patterns. The number of candi-
date initial patterns could be reduced significantly
by specifying the candidate types of entities of in-
terest. In the case of BioNoculars, the focus was
on relationships between BNEs of type PROTEIN.
The candidate patterns are then applied to the tagged
stream to produce in-sentence relationship tuples.
As for pattern induction, due to the duality in the
patterns and tuples relation, patterns and tuples are
represented by a bipartite graph as illustrated in Fig-
ure 1.
Figure 1: A bipartite graph representing patterns and
tuples
Each pattern or tuple is represented by a node in
the graph. Edges represent matching between pat-
terns and tuples. The pattern induction problem can
be formulated as follows: Given a very large set of
data D containing a large set of patterns P, which
match a large set of tuples T, the problem is to iden-
tify , which is the set of patterns that match the set
of the most correct tuples T. The intuition is that
the tuples matched by many different patterns tend
to be correct and the patterns matching many differ-
ent tuples tend to be good patterns. In other words,
BioNoculars attempts to choose from the large space
of patterns in the data the most informative, high-
est confidence patterns that could identify correct tu-
ples; i.e. choosing the most authoritative patterns in
analogy with the hub-authority problem. The most
authoritative patterns can then be used for extracting
relations from free text. The following pattern-tuple
pairs show how patterns can match tuples in the cor-
pus:
(protein) (verb) (noun) (prep.) (protein)
92
Cla4 induces phosphorylation of Cdc24
(protein) (I-protein) (Verb) (prep.) (protein)
NS5A interacts with Cdk1
The proposed approach represents an unsuper-
vised technique for information extraction in general
and particularly for relations extraction that requires
no seed patterns or examples and achieves signifi-
cant performance. Given enough domain text, the
extracted patterns can support many types of sen-
tences with different styles (such passive and active
voice) and orderings (the interaction of X and Y vs.
X interacts with Y).
One of the critical prerequisites of the above-
mentioned approach is the use of a POS tagger,
which is tuned for biomedical text, and a BNE tag-
ger to properly identify BNEs. Both are critical for
determining the types of relationships that are of in-
terest. For POS tagging, a decision tree based tagger
developed by (Schmid, 1994) was used in combi-
nation with a model, which was trained on a cor-
rected/revised GENIA corpus provided by (Saric et
al., 2004) and was reported to achieve 96.4% tagging
accuracy (Saric et al, 2006). This POS tagger will
be referred to as the Schmid tagger. For BNE tag-
ging, ABNER was used. The accuracy of ABNER
is approximately state of the art with precision and
recall of 74.5% and 65.9% respectively with training
done using the BioCreative corpora (BioCreative).
Nonetheless we still face entity identification prob-
lems such as missed identifications in the text which
in turn affects our results considerably. We do be-
lieve if we use a better identification method , we
would yield better results.
4 Experimental Setup
Experiments aimed at extracting protein-protein
interactions for Bakers yeast (Sacharomyces
Cerevesiae) to assess BioNoculars (Cherry et al,
1998). The experiments were performed using
109,440 MEDLINE abstracts that contained the
varying names of the yeast, namely Sacharomyces
cerevisiae, S. Cerevisiae, Bakers yeast, Brewers
yeast and Budding yeast. MEDLINE abstracts
typically summarize the important aspects of papers
possibly including protein-protein interactions if
they are of relevance to the article. The goal was
to deduce the most appropriate extraction patterns
that can be later used to extract relations from any
document. All the MEDLINE abstracts were used
for pattern extraction except for 70 that were set
aside for testing. There were no test documents in
the training set. To build ground-truth, the test set
was semi-manually POS and BNE tagged. They
were also annotated with the interactions that are
contained in the text. There was a condition that
all the abstracts that are used for testing must have
entries in the Database of Interacting Proteins and
Protein-Protein Interactions (DIPPPI), which is
a subset of the Database of Interacting Proteins
(DIP) (Xenarios et al, 2000) restricted to proteins
from yeast. DIPPPI lists the known protein-protein
interactions in the MEDLINE abstracts. There were
297 protein-protein interactions in the test set of 70
abstracts. One of the disadvantages of DIPPPI is
that the presence of interactions is indicated without
mentioning their types or from which sentences
they were extracted. Although BioNoculars is able
to guess the sentence from which an interaction was
extracted and the type of interaction, this informa-
tion was ignored when evaluating against DIPPPI.
Unfortunately, there is no standard test set for the
proposed task, and most of the evaluation sets are
proprietary. The authors hope that others can benefit
from their test set, which is freely available.
The abstracts used for pattern extraction were
POS tagged using the Schmid tagger and BNE tag-
ging was done using ABNER. The patterns were re-
stricted to only those with protein names. For extrac-
tion of interaction tuples, the test set was POS and
BNE tagged using the Schmid tagger and ABNER
respectively. A varying number of final patterns
were then used to extract tuples from the test set and
the average recall and precision were computed. An-
other setup was used in which the relationships were
filtered using preset keywords for relationships such
as inhibits, interacts, and activates to properly com-
pare BioNoculars to systems in the literature that use
such keywords. The keywords were obtained from
the (Hakenberg et al, 2005) and (Temkin and Gilder,
2003). One of the generated pattern-tuple pairs was
as follows:
(PROTEIN) (Verb) (Conjunction) (PROTEIN)
NS5A interacts with Cdk1
One consequence of tuple extraction is generation
of redundant tuples, which contain the same enti-
93
Pattern Count 30 59 78 103 147 192 205 217
Recall 0.51 0.70 0.76 0.81 0.84 0.89 0.89 0.93
Precision 0.47 0.42 0.43 0.35 0.30 0.26 0.26 0.16
FMeasure 0.49 0.53 0.55 0.49 0.44 0.40 0.40 0.27
Table 1: Recall, Precision, and F-measure for extrac-
tion of tuples using a varying number of top rated
patterns
ties and relations. Consequently, all protein aliases
and full text names were resolved to a unified nam-
ing scheme and the unified scheme was used to re-
place all variations of protein names in patterns. All
potential protein-protein interactions that BioNocu-
lars extracted were compared to those in the DIPPPI
databases.
5 Results and Discussion
For the first set of experiments, the experimental
setup described above was used without modifica-
tion. Table 1 and Figure 2 report on the resulting
recall and precision when taking different number
of highest rated patterns. The highest rated 217 pat-
terns were divided on a linear scale into 8 clusters
based on their relative weights.
Figure 2: Recall, Precision, and F-measure for tuple
extraction using a varying number of top patterns
As expected, Figure 2 clearly shows an inverse
relationship between precision and recall. This is
because using more extraction patterns yields more
tuples thus increasing recall at the expense of pre-
cision. The F-measure (with ? = 1) peeks at 78
patterns, which seems to provide the best score
given that precision and recall are equally important.
However, the technique seems to favor recall, reach-
ing a recall of 93% when using all 217 patterns. The
Pattern Count 30 59 78 103 147 192 205 217
Recall 0.31 0.44 0.46 0.48 0.64 0.73 0.74 0.78
Precision 0.31 0.36 0.35 0.34 0.39 0.35 0.35 0.37
FMeasure 0.31 0.40 0.40 0.40 0.48 0.47 0.48 0.50
Table 2: Recall, Precision, and Recall for extraction
of tuples using a varying number of top rated patters
keyword filtering
low precision levels warrant thorough investigation.
In the second set of experiments, extracted tuples
were filtered using preset keywords indicating inter-
actions. Table 2 and Figure 3 show the results of the
experiments.
Figure 3: Recall, Precision, and F-measure for tu-
ple extraction using a varying number of top patterns
with keyword filtering
The results show that filtering with keywords led
to lower recall, but precision remained fairly steady
as the number of patterns changed. Nonetheless, the
best precision in Figure 3 is lower than the best pre-
cision in Figure 2 and the maximum F-measure for
this set of experiments is lower than the maximum
F-measure when no filtering was used. The BioNoc-
ulars system with no filtering can be advantageous
for recall oriented applications. The use of no filter-
ing suggests that some interaction may be expressed
in more generic forms or patterns. An intermediate
solution would be to increase the size of the list of
most commonly occurring keywords to filter the ex-
tracted tuples further.
Currently, ABNER, which is used by the system,
has a precision of 75.4% and a recall of 65.9%. Per-
haps improved tagging may improve the extraction
effectiveness.
The effectiveness of BioNoculars needs to be
94
thoroughly compared to existing systems via the use
of standard test sets, which are not readily available.
Most of previously reported work has been tested
on proprietary test sets or sets that are not publicly
available. The creation of standard publicly avail-
able test set can prompt research in this area.
6 Conclusion and Future Work
This paper presented a system for extracting
protein-protein interaction from biomedical text call
BioNoculars. BioNoculars uses a statistical un-
supervised learning algorithm, which is based on
graph mutual reinforcement and data redundancy
to extract extraction patterns. The system is re-
call oriented and is able to properly extract 93% of
the interaction mentions from test MEDLINE ab-
stracts. Nonetheless, the systems precision remains
low. Precision can be enhanced by using keywords
that describe interactions to filter to the resulting in-
teraction, but this would be at the expense of recall.
As for future work, more attention should be fo-
cused on improving extraction patterns. Currently,
the system focuses on extracting interactions be-
tween exactly two proteins. Some of the issues that
need to be handled include complex relationship (X
and Y interact with A and B), linguistic variabil-
ity (passive vs. active voice; presence of superflu-
ous words such as modifiers, adjectives, and prepo-
sitional phrases), protein lists (W interacts with X,
Y, and Z), nested interactions (W, which interacts
with X, also interacts with Y). Resolving these is-
sues would require an investigation of how patterns
can be generalized in automatic or semi-automatic
ways. Further, the identification of proteins in the
text requires greater attention. Also, the BioNocu-
lars approach can be combined with other rule-based
approaches to produce better results.
References
Ashburner, M., C. A. Ball, J. A. Blake, D. Botstein, H.
Butler, J. M. Cherry, A. P. Davis, K. Dolinski, S. S.
Dwight, J. T. Eppig, M. A. Harris, D. P. Hill, L. Issel-
Tarver, A. Kasarskis, S. Lewis, J. C. Matese, J. E.
Richardson, M. Ringwald, G. M. Rubin, and G. Sher-
lock. 2000. Gene ontology: tool for the unification of
biology. Nature Genetics,volume 25 pp.25-29.
BioCreative. 2004. [Online].
Blaschke C., M. A. Andrade, C. Ouzounis, and A. Valen-
cia. 1999. Automatic Extraction of Biological Infor-
mation from Scientific Text: Protein-Protein Interac-
tions. ISMB99, pp. 60-67.
Blaschke, C. and A. Valencia. 2001. Can Bibliographic
Pointers for Known Biological Protein Interactions
as a Case Study. Comparative and Functional Ge-
nomics,vol. 2: 196-206.
Cherry, J. M., C. Adler, C. Ball, S. A. Chervitz, S. S.
Dwight, E. T. Hester, Y. Jia, G. Juvik, T. Roe, M.
Schroeder, S. Weng, and D. Botstein. 1998. SGD:
Saccharomyces Genome Database. Nucleic Acids Re-
search, 26, 73-9.
Chun, H. W., Y. Tsuruka, J. D. Kim, R. Shiba, N. Nagata,
T. Hishiki, and J. Tsujii. 2006. Extraction of Gene-
Disease Relations from MEDLINE Using Domain Dic-
tionaries and Machine Learning. Pacific Symposium
on Biocomputing 11:4-15.
Collier, N., C. Nobata, and J. Tsujii. 2000. Extracting
the Names of Genes and Gene Products with a Hidden
Markov Model. COLING, 2000, pp. 201207.
Cooper, J. and A. Kershenbaum. 2005. Discovery of
protein-protein interactions using a combination of
linguistic, statistical and graphical information. BMC
Bioinformatics.
DIPPPI http://www2.informatik.hu-berlin.de/ haken-
ber/corpora. 2006.
Ferrucci, D. and A. Lally. 2004. UIMA: an architec-
tural approach to unstructured information processing
in the corporate research environment. Natural Lan-
guage Engineering 10, No. 3-4, 327-348.
Fukuda, K., T. Tsunoda, A. Tamura, and T. Takagi. 1998.
Toward information extraction: identifying protein
names from biological papers. PSB, pages 705716.
Hakenberg, J., C. Plake, U. Leser, H. Kirsch, and D.
Rebholz-Schuhmann. 2005. LLL?05 Challenge:
Genic Interaction Extraction with Alignments and Fi-
nite State Automata. Proc Learning Language in Logic
Workshop (LLL?05) at ICML 2005, pp. 38-45. Bonn,
Germany.
Hanisch, D., J. Fluck, HT. Mevissen, and R. Zimmer.
2003. Playing biologys name game: identifying pro-
tein names in scientific text. PSB, pages 403414.
Hao, Y., X. Zhu, M. Huang, and M. Li. 2005. Discov-
ering patterns to extract protein-protein interactions
from the literature: Part II. Bioinformatics, Vol. 00
no. 0 2005 pages 1-7.
95
Hassan, H., A. Hassan, and O. Emam. 2006. Un-
supervised Information Extraction Approach Using
Graph Mutual Reinforcement. Proceedings of Em-
pirical Methods for Natural Language Processing (
EMNLP ).
Humphreys B. L. and D. A. B. Lindberg. 1993. The
UMLS project: making the conceptual connection be-
tween users and the information they need. Bulletin of
the Medical Library Association, 1993; 81(2): 170.
Jo?rg Hakenberg, Conrad Plake, Ulf Leser. 2005. Genic
Interaction Extraction with Alignments and Finite
State Automata. Proc Learning Language in Logic
Workshop (LLL?05) at ICML 2005, pp. 38-45. Bonn,
Germany (August 2005)
Kazama, J., T. Makino, Y. Ohta, and J. Tsujii. 2002. Tun-
ing Support Vector Machines for Biomedical Named
Entity Recognition. ACL Workshop on NLP in
Biomedical Domain, pages 18.
Kleinberg, J. 1998. Authoritative sources in a hy-
perlinked environment. In Proc. Ninth Ann. ACM-
SIAM Symp. Discrete Algorithms, pages 668-677,
ACM Press, New York.
Koike A. and T. Takagi. 2004. Gene/protein/family
name recognition in biomedical literature. BioLINK
2004: Linking Biological Literature, Ontologies, and
Database, pp. 9-16.
Koike, A., Y. Niwa, and T. Takagi 2005. Automatic
extraction of gene/protein biological functions from
biomedical text. Bioinformatics, Vol. 21, No. 7.
Leroy, G. and H. Chen. 2005. Genescene: An Ontology-
enhanced Integration of Linguistic and Co-Occurance
based Relations in Biomedical Text. JASIST Special
Issue on Bioinformatics.
Mack, R. L., S. Mukherjea, A. Soffer, N. Uramoto, E. W.
Brown, A. Coden, J. W. Cooper, A. Inokuchi, B. Iyer,
Y. Mass, H. Matsuzawa, L. V. Subramaniam. 2004.
Text analytics for life science using the Unstructured
Information Management Architecture. IBM Systems
Journal 43(3): 490-515.
Mika, S. and B. Rost. 2004. NLProt: extracting pro-
tein names and sequences from papers. Nucleic Acids
Research, 32 (Web Server issue): W634W637.
Saric, J., L. J. Jensen, R. Ouzounova, I. Rojas, and P.
Bork. 2004. Extracting regulatory gene expression
networks from PUBMED. Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, Barcelona, Spain, pp.191-198.
Saric, J., L. J. Jensen, R. Ouzounova, I. Rojas, and P.
Bork. 2006. Extraction of regulatory gene/protein
networks from Medline. Bioinformatics Vol.22 no
6,pp. 645-650.
Schmid, H. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In the International Conference
on New Methods in Language Processing, Manch-
ester, UK.
Settles, B. 2004. Biomedical Named Entity Recognition
Using Conditional Random Fields and Rich Feature
Sets. In Proceedings of the International Joint Work-
shop on Natural Language Processing in Biomedicine
and its Applications (NLPBA), Geneva, Switzerland,
pages 104-107.
Settles, B. 2005. ABNER: an open source tool for au-
tomatically tagging genes, proteins, and other entity
names in text. Bioinformatics, 21(14): 3191-3192.
Tanabe L., and W. J. Wilbur. 2002. Tagging gene
and protein names in biomedical text. Bioinformatics,
18(8):11241132.
Temkin, J. M. and M. R. Gilder. 2003. Extraction
of protein interaction information from unstructured
text using a context-free grammar. Bioinformatics
19(16):2046-2053.
Xenarios I, Rice DW, Salwinski L, Baron MK, Marcotte
EM, Eisenberg D. 2000. DIP: the Database of Inter-
acting Proteins. Nucleic Acids Res 28: 289291.
Yamamoto, K., T. Kudo, A. Konagaya, Y. Matsumoto.
2003. Protein Name Tagging for Biomedical Annota-
tion in Text. Proceedings of the ACL 2003 Workshop
on Natural Language Processing in Biomedicine, pp.
65-72.
96
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 57?61,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Language Independent Transliteration Mining System Using Finite State
Automata Framework
Sara Noeman and Amgad Madkour
Human Language Technologies Group
IBM Cairo Technology Development Center
P.O.Box 166 El-Haram, Giza, Egypt
{noemans,amadkour}@eg.ibm.com
Abstract
We propose a Named Entities translitera-
tion mining system using Finite State Au-
tomata (FSA). We compare the proposed
approach with a baseline system that uti-
lizes the Editex technique to measure the
length-normalized phonetic based edit dis-
tance between the two words. We sub-
mitted three standard runs in NEWS2010
shared task and ranked first for English
to Arabic (WM-EnAr) and obtained an F-
measure of 0.915, 0.903, and 0.874 re-
spectively.
1 Introduction
Named entities transliteration is a crucial task in
many domains such as cross lingual information
retrieval, machine translation, and other natural
language processing applications. In the previous
NEWS 2009 transliteration task, we introduced a
statistical approach for transliteration generation
only using the bilingual resources (about 15k par-
allel names) provided for the shared task. For
NEWS2010, the shared task focuses on acquisi-
tion of a reasonably sized, good quality names
corpus to complement the machine transliteration
task. Specifically, the task focuses on mining the
Wikipedia paired entities data (inter-wiki-links) to
produce high-quality transliteration data that may
be used for transliteration generation tasks.
2 Related Work
Finite state Automata is used to tackle many Nat-
ural Language Processing challenges. Hassan
(2008) et al proposed the use of finite state au-
tomata for language-independent text correction.
It consists of three phases : detecting misspelled
words, generating candidate corrections for them
and ranking corrections. In detecting the mis-
pelled words, they compose the finite state au-
tomaton representation of the dictionary with the
input string. Onaizan (2002) et al proposed
the use of probabilistic finite state machines for
machine transliteration of names in Arabic text.
They used a hybrid approach between phonetic-
based and spelling-based models. Malik (2008)
et al proposed a Hindi Urdu machine translit-
eration system using finite state machines. They
introduced UIT (universal intermediate transcrip-
tion) on the same pair according to thier phonetic
properties as a means of representing the language
and created finite state transducers to represent
them. Sherif (2007) proposed the use of memo-
ryless stochastic transducer for extracting translit-
eration through word similarity metrics.
Other approaches for transliteration include
translation of names through mining or through
using machine translation systems resources. Has-
san (2007) et al proposed a framework for extrac-
tion of named entity translation pairs. This is done
through searching for candidate documents pairs
through an information retrieval system and then
using a named entity matching system which re-
lies on the length-normalized phonetic based edit
distance between the two words. They also use
a phrase-based translation tables to measure simi-
larity of extracted named entities. Noeman (2009)
also used a phrase based statistical machine trans-
lation (PBSMT) approach to create a substring
based transliteration system through the generated
phrase table, thus creating a language indepen-
dent approach to transliteration. Other resources
have been used to perform transliteration. Chang
(2009) et. al proposed the use of a romanization
table in conjunction with an unsupervised con-
straint driven learning algorithm in order to iden-
tify transliteration pairs without any labelled data.
3 System architecture
The approach consists of three main phases which
are (1) Transliteration model learning, (2) Fi-
57
Figure 1: Transliteration table learning in PBSMT
nite State machine formalization of the generated
transliteration model and (3) Generating Candi-
date transliterations. Figure (1) illustrates Translit-
eration table learning in PBSMT framework. A
detailed description of each phase is given in the
following sections.
3.1 Transliteration model learning
The objective of NEWS2010 shared task is to de-
velop a system for mining single word translitera-
tion pairs from the standard Wikipedia paired top-
ics (Wikipedia Inter-Language Links, or WIL1),
using a seed data of only 1000 parallel names. The
aim is to learn one-to-many character sequence
mappings on both directions.
We propose the use of MOSES framework1 for
PBSMT training which was applied on the 1k par-
allel seed data. The proposed approach depends on
the formulation of the transliteration problem us-
ing the PBSMT approach used in Machine trans-
lation. Giza++ Hidden Markov Model (HMM)
aligner2 proposed by Och (1999) was also used
over the parallel character sequences. Heuristics
were used to extend substring to substring map-
pings based on character-to-character alignment.
This generated a substring to substring translation
model such as in Koehn (2003). The phrase ?sub-
string? table was filtered out to obtain all possi-
ble substrings alignment of each single character
in the language alphabet in both directions. This
means that for each character in the source lan-
guage (English) alphabet, substrings mapped to it
are filtered with a threshold. Also for each char-
acter in the target language (Arabic) alphabet, all
English substrings mapped to it are filtered with
a threshold. These two one-to-many alignments
were intersected in one ?Transliteration Arabic-to-
English mapping?. We obtained a character align-
ment table which we refer to as ?Ar2En list?. Fig-
ure(2) illustrates a sample one-to-many alignment
mapping.
1MOSES Framework: http://www.statmt.org/moses/
2GIZA++ Aligner: http://fjoch.com/GIZA++.html
Figure 2: One to Many Alignment Mapping
1
a:a/0
b:b/0
2/0.25
<epsilon>:a/0
<epsilon>:b/0
a:<epsilon>/0
a:b/0
b:<epsilon>/0
b:a/0
3
a:b/0
4
b:a/0
a:a/0
b:b/0
b:a/0
a:b/0
Figure 3: Edit distance 1 FSM
3.2 FSM formalization of Transliteration
Model
The proposed method makes use of the finite state
automaton representation for the Ar2En character
alignment list, where the input is the source char-
acter and the output is the target character. We re-
fer to this finite state transducer (FST) as ?Ar2En
FST?. For each source word, we build a Finite
State Acceptor (FSA), such that each candidate
source word FSA is composed with the ?Ar2En
FST?. For the target words list, we build a finite
state acceptor (FSA) that contains a path for each
word in the target Wiki-Link.
3.3 Generating Candidate transliterations
The task of generating candidate transliterations
at edit distance k from initial source candidate
transliterations using Levenshtein transducer can
be divided into two sub tasks: Generating a list of
words that have edit distance less than or equal k
to the input word, and selecting the words inter-
58
1a:a/0
b:b/0
2/0.25
<epsilon>:a/0
<epsilon>:b/0
a:<epsilon>/0
a:b/0
b:<epsilon>/0
b:a/0
4
a:b/0
5
b:a/0
a:a/0
b:b/0
3/0.5
<epsilon>:a/0
<epsilon>:b/0
a:<epsilon>/0
a:b/0
b:<epsilon>/0
b:a/0
6
a:b/0
7
b:a/0
b:a/0
a:b/0
a:a/0
b:b/0
b:a/0
a:b/0
Figure 4: Edit distance 2 FSM
secting with the target inter-wiki-link words. This
is similar to the spelling correction technique that
used FSM which was introduced by Hassan (2008)
et. al. In the spelling correction task , after gener-
ating the list of words within edit distance k to the
input word, the system selects a subset of those
words that exist in a large dictionary. In order to
accomplish this same scenario, we created a sin-
gle transducer (Levenshtein transducer) that when
composed with an FSM representing a word gen-
erates all words within an edit distance k from the
input word. We then compose the resulting FSM
with an FSA (finite state acceptor) of all words in
the target inter-wiki-link. The Levenshtein trans-
ducer is language independent and is built only
using the alphabet of the target language. Figure
(3) and Figure (4) illustrate the Levenshtein trans-
ducer for edit distance 1 and 2 over a limited set of
vocabulary (a, b).
4 Data and Resources Processing
After revising the training data (inter-wiki-links)
released, we discovered that English and Arabic
words contained many stress marks and non nor-
malized characters. We therefore applied normal-
ization on Arabic and English characters to in-
crease source target matching probability, thus in-
creasing the recall of data mining. We also nor-
malized Arabic names, removing all diacritics and
kashida. Kashida is a type of justification used in
some cursive scripts such as Arabic. Also we nor-
malized Alef () with hamza and madda to go to
?bare Alef?.
Figure 5: Using Levenshtein edit-1 FST
5 Standard runs
We submitted 6 runs derived from 3 experiments.
For each experiment, we submitted 2 runs, one
with normalized Arabic and English characters,
and the other with the stress marks and special
characters. It is important to note that we run the
mining in the Arabic to English direction, thus the
Arabic side is the source and the English side is
the target.
5.1 Using Levenshtein edit distance 1 FST
Figure (5) illustrates the algorithm used to con-
duct the first experiment. We subjected all source
words to be composed with Levenshtein edit dis-
tance 1. For each Wiki-Link, we build a finite
state acceptor (FSA) that contains a path for each
word in the Arabic Wiki-Link. We refer to it as
FSA[@ArWords]. Similarly, for the English name
candidates we build a finite state acceptor (FSA)
that contains a path for each word in the English
Wiki-Link. We refer to it as FSA[@EnWords].
The generated @ArWords and @EnWords are the
lists of words in the Arabic and English wiki-links
respectively. The result of this experiment was re-
ported as Standard-3 ?normalized characters? and
Standard-4 ?without normalized characters?.
5.2 Using Levenshtein up to edit distance 2
FST
Figure (6) illustrates the algorithm used to conduct
the second experiment. We use a threshold on the
number of characters in a word to decide whether
it will be subjected for ?composed with? edit dis-
59
Figure 6: Using Levenshtein edit-2 FST
tance 0 or 1 or 2. We use a threshold of 3 for
edit distance 1 and a threshold of 7 for edit dis-
tance 2. The threshold values are set based on our
previous experience from dealing with Arabic text
and could be derived from the data we obtained.
If word length is less than or equal 3 letters, then
it is not composed with Levenshtein FSTs, and if
word length is between 4 to 7 letters, we compose
it with edit distance 1 FST. Longer words are com-
posed with edit distance 2 FST. The result of the
experiment was reported in two submitted runs:
Standard-5 ?normalized characters? and Standard-
6 ?without normalized characters?.
5.3 Baseline
We use a length-normalized phonetic edit distance
to measure the phonetic similarity between the
source and target Named Entities in the inter-wiki-
links. We use the Editex technique Zobel (1996)
that makes use of the phonetic characteristics of
individual characters to estimate their similarity.
Editex measures the phonetic distance between
pairs of words by combining the properties of
edit distances with a letter grouping strategy that
groups letters with similar pronunciations. The re-
sult of this experiment was reported in two submit-
ted runs: Standard-1 ?normalized characters? and
Submission F-Score Precision Recall
Standard-6 0.915 0.887 0.945
Standard-4 0.903 0.859 0.952
Standard-2 0.874 0.923 0.830
Standard-5 0.723 0.701 0.747
Standard-3 0.716 0.681 0.755
Standard-1 0.702 0.741 0.666
Table 1: Shared Task Results
Standard-2 ?without normalized characters?.
6 Results
Table (1) illustrates the results of the shared task
given on the runs we submitted.
Our baseline run (Standard-2) reports highest
precision of 0.923 and lowest recall of 0.830 (low-
est F-score = 0.874). The reason is that Editex
technique measures the edit distance based on let-
ter grouping strategy which groups letters with
similar pronunciations. It operates on character to
character level. Letters that are mapped to multi-
characters will suffer a large edit distance and may
exceed the matching threshold used.
The two runs Standard-4 and Standard-6 are
implemented using edit-distance FSM matching
between source and target. They cover one-to-
many character mapping. We notice that Standard-
6 run reports higher precision of 0.887 compared
to 0.859 for Standard-4 run. This reflects the ef-
fect of using variable edit-distance according to
the source word length. The Standard-6 reports
a Recall of 0.945 producing our best F-Score of
0.915. Standard-6 recall degrades only 0.7% from
Standard-4 Recall (0.952).
7 Conclusion
We proposed a language independent transliter-
ation mining system that utilizes finite state au-
tomaton. We demonstrated how statistical tech-
niques could be used to build a language indepen-
dent machine transliteration system through uti-
lizing PBMT techniques. We performed 3 stan-
dard experiments each containing two submis-
sions. FSM edit distance matching outperformed
Editex in F-Score and Recall. The proposed ap-
proach obtained the highest F-Score of 0.915 and
a recall of 0.945 in the shared task.
60
References
Ahmed Hassan, Haytham Fahmy, Hany Hassan 2007.
Improving Named Entity Translation by Exploiting
Comparable and Parallel Corpora. AMML07
Ahmed Hassan, Sara Noeman, Hany Hassan 2008.
Language Independent Text Correction using Finite
State Automata. IJCNLP08.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney 1999. Improved Alignment Models for Statisti-
cal Machine Translation. EMNLP.
Justin Zobel and Philip Dart 1996. Phonetic string
matching: Lessons from information retrieval. In
Proceedings of the Annual ACM References Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR).
M. G. Abbas Malik, Christian Boitet, Pushpak Bhat-
tacharyya 2008. Hindi Urdu Machine Transliter-
ation using Finite-state Transducers. Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (Coling 2008), pages 537544
Ming-Wei Chang, Dan Goldwasser, Dan Roth,
Yuancheng Tu 2009. Unsupervised Constraint
Driven Learning For Transliteration Discovery.
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Philipp Koehn, Franz Josef Och, Daniel Marc 2003.
Statistical Phrase-Based Translation. Proc. Of the
Human Language Technology Conference, HLT-
NAACL2003, May.
Sara Noeman 2009. Language Independent Transliter-
ation system using PBSMT approach on substrings.
Proceedings of the 2009 Named Entities Workshop:
Shared Task on Transliteration.
Tarek Sherif, Grzegorz Kondrak 2007. Bootstrapping
a Stochastic Transducer for Arabic-English Translit-
eration Extraction. Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 864871
Yasser Al-Onaizan, Kevin Knight 2002. Machine
Transliteration of Names in Arabic Text. ACL
Workshop on Comp. Approaches to Semitic Lan-
guages.
61
