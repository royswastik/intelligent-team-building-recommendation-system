Chinese Word Segmentation in FTRD Beijing 
Heng LI 
France Telecom R&D Bei-
jing 
heng.li@francetelec
om.com
Yuan DONG 
France Telecom R&D Beijing
yuan.dong@francetele
com.com
Xinnian MAO 
France Telecom R&D Bei-
jing
xin-
nian.mao@francetelec
om.com
Haila WANG 
France Telecom R&D Bei-
jing  
haila.wang@francete
lecom.com
Wu LIU 
Beijing University of Posts 
and Telecommunications  
wu.liu@francetelecom
.com.cn
Abstract
This paper presents a word segmenta-
tion system in France Telecom R&D 
Beijing, which uses a unified approach 
to word breaking and OOV identifica-
tion. The output can be customized to 
meet different segmentation standards 
through the application of an ordered 
list of transformation. The system par-
ticipated in all the tracks of the seg-
mentation bakeoff -- PK-open, PK-
closed, AS-open, AS-closed, HK-open, 
HK-closed, MSR-open and MSR-
closed -- and achieved the state-of-the-
art performance in MSR-open, MSR-
close and PK-open tracks. Analysis of 
the results shows that each component 
of the system contributed to the scores. 
1 Introduction
The development of the Chinese word segmen-
tation system presented in this bakeoff began in 
Feb. this year, and will last for one year with the 
support of the ILAB Beijing initial project 
within France Telecom R&D.  
Although the project last only half year by 
now, the main components of the system has 
been implemented, including code identification 
and conversion, basic segmentation, factoid de-
tection, morphological analysis, name entity 
identification, segmentation standards adaptor, 
except the components of code identification 
and conversion and segmentation standards 
adaptors, other components are integrated in a 
statistical framework of n-gram language model. 
2 System Description 
2.1 Code identification and conversion 
For processing both Simplified and Traditional 
Chinese text from a variety of locales, including 
Mainland China, Hong Kong and Taiwan, we 
choose UTF-8 as internal character representa-
tion within the system. The ability to transpar-
ently handle Chinese text from any Chinese 
locale greatly simplifies the logic of the segmen-
tation system. 
2.2 N-gram language model  
In our system, Chinese words can be categorized 
into one of the following types: lexicon words, 
morphological words, factoids, name entities. 
These types of words are processed in different 
ways in our system, and are incorporated into a 
unified statistical framework of the trigram lan-
guage model. 
2.2.1 Basic segmentation 
Each input sentence is first segmented into indi-
vidual characters. These characters and the char-
acter strings are then looked up in a lexicon. For 
the efficient search, the lexicon is represented by 
a TRIE compressed in a double-array data struc-
150
ture. Given a character string, all its prefix 
strings that form lexicon words can be retrieved 
efficiently by browsing the TRIE whose root 
represents its first character.  
2.2.2 Factoid detection 
There are twenty four kinds of factoid words, 
such as time, date, money, etc. All the factoid 
words are represented as regular expressions, 
and compiled into a compressed DFA with the 
row-index algorithm. 
2.2.3 Morphological analysis 
As (Wu 2003) discussed in the paper, it is those 
morphologically derived words (MDWs hereaf-
ter) that are most controversial and most likely 
to be treated differently in different standards 
and different systems. In our system, there are 
six main categories of morphological processes, 
affixation, directional verb, resultative verb, 
splitting verb, reduplication and merging, and 
we employ a chart parsing algorithm augmented 
with word lattices structure which incorporates 
the morphological rules especially designed for 
Chinese languages with restrictive CFG.  
2.2.4 Name entity identification  
Our NE identification concentrates on three 
types of NEs, namely, personal names (PERs), 
location names (LOCs) and organization names 
(ORGs). For Chinese person names, we only 
consider PN candidates that begin with a family 
name stored in the family name list and follow a 
given name which is of one or two characters 
long. For transliterations of foreign person 
names, a PN candidate would be generated if it 
contains only characters stored in a transliterated 
character list. For location names and organiza-
tions names, we only use the LN list and ON list 
to generate the candidates. 
2.3 Segmentation standards adaptor 
In this bakeoff, there are four segmentation 
standards and slightly different from ours. Stan-
dard adaptation is conducted with the applica-
tion of an ordered list of transformations on the 
output of our segmentation system. The method 
we use is Transformation-Based Learning, and 
the transformation templates are lexicalized 
templates. In our system, we designed 14 lexi-
calized templates. 
2.4 Speed 
As we optimized our lexicon and decoding 
process, the speed of segmentation is very fast. 
On a single 2.80 GHz, 1G bytes memory, Xeon 
machine, the system is able to process about 
0.73 Mega bytes per second. 
The speed may vary according to the sen-
tence lengths: given texts of the same size, those 
containing longer sentences will take more time. 
The number reported here is an average of the 
time taken to process the test sets of the eight 
tracks we participated in. 
3 Evaluation 
3.1 Open tracks 
In the open tracks, we used four lexicons of 
210,319 entries, 165,103 entries, 174,268 entries, 
165,655 entries respectively on AS-open, HK-
open, MSR-open, PK-open tracks, which in-
clude the entries of 2,430 MDWs, 12,487 PNs, 
22,907 LNs and 29,032 ONs, 10,414 four-
character idioms, plus the word lists generated 
from the training data provided by the bakeoff. 
We use the training data provided by the bakeoff 
for training our trigram word-based language 
model. We also used a family name list (which 
contains 399 entries in our system), and a 1,021-
entry transliterated name character list. 
3.2 Closed tracks 
In the close tracks, the lexicon we use could 
only be generated from the training data pro-
vided by the bakeoff. We could only use the 
training data provided by the bakeoff for train-
ing our word-based language model. Also, since 
the training data we used is only from the bake-
off, there does not exist any different standards, 
standards adaptor component is not necessarily 
needed.  
3.3 Result analysis 
Our system is designed so that components such 
as the factoid detection and NE identification 
can be switched on or off, so that we can inves-
tigate the relative contribution of each compo-
nent to the overall word segmentation 
performance. The results are summarized in the 
table 1. For comparison, we also include in the 
table (Row 1) the results of using FMM. Row 2 
shows the baseline results of our system, where 
only the lexicon is used. Each cell in the table 
has six fields. From the top, there are respec-
tively Precision, Recall, F-measure, OOV Recall, 
IV Recall and Speed (Mega bytes/second). We 
don't list the speed in Row 6 since it decreases a 
factor of 10 to 60 because of application of 
thousands of TBL rules.  
151
PKo PKc MSRo MSRc ASo ASc HKo HKc
1. FMM 
0.857 
0.925 
0.891 
0.143 
0.947 
2.435 
0.841 
0.906 
0.872 
0.069 
0.957 
2.570 
0.921 
0.968 
0.945 
0.107 
0.971 
2.951 
0.917 
0.957 
0.936 
0.025 
0.982 
3.090 
0.871
0.925
0.898
0.097
0.947
2.813
0.864 
0.911 
0.887 
0.014 
0.952 
2.937 
0.842 
0.928 
0.885 
0.175 
0.961 
2.748 
0.838 
0.908 
0.872 
0.162 
0.968 
2.850 
2. Baseline 
0.869 
0.941 
0.905 
0.235 
0.960 
0.967 
0.855 
0.928 
0.890 
0.069 
0.987 
1.017 
0.931 
0.973 
0.952 
0.275 
0.987 
0.879 
0.926 
0.969 
0.947 
0.025 
0.995 
0.923 
0.891
0.943
0.917
0.132
0.982
0.703
0.877 
0.942 
0.908 
0.014 
0.984 
0.728 
0.863 
0.930 
0.897 
0.194 
0.985 
0.921 
0.851 
0.929 
0.888 
0.162 
0.990 
0.956 
3. 2+FT 
0.946 
0.951 
0.948 
0.748 
0.963 
0.819 
0.919 
0.950 
0.934 
0.448 
0.980 
0.879 
0.950 
0.973 
0.961 
0.396 
0.990 
0.779 
0.940 
0.973 
0.956 
0.205 
0.944 
0.787 
0.903
0.945
0.924
0.180
0.979
0.631
0.900 
0.947 
0.923 
0.156 
0.983 
0.635 
0.873 
0.932 
0.902 
0.292 
0.983 
0.821 
0.862 
0.932 
0.895 
0.215 
0.989 
0.830 
4. 3+MA 
0.946 
0.951 
0.948 
0.748 
0.963 
0.807 
0.919 
0.950 
0.934 
0.448 
0.980 
0.879 
0.950 
0.973 
0.961 
0.371 
0.989 
0.753 
0.940 
0.973 
0.956 
0.205 
0.944 
0.787 
0.903
0.945
0.924
0.181
0.979
0.626
0.900 
0.947 
0.923 
0.156 
0.983 
0.635 
0.873 
0.932 
0.902 
0.295 
0.983 
0.815 
0.862 
0.932 
0.895 
0.215 
0.989 
0.830 
5. 4+NE 
0.951 
0.957 
0.954 
0.788 
0.967 
0.679 
0.919 
0.950 
0.934 
0.448 
0.980 
0.879 
0.956 
0.973 
0.965 
0.454 
0.956 
0.716 
0.940 
0.973 
0.956 
0.205 
0.944 
0.787 
0.920
0.949
0.934
0.330
0.977
0.604
0.900 
0.947 
0.923 
0.156 
0.983 
0.635 
0.900 
0.938 
0.918 
0.411 
0.980 
0.748 
0.862 
0.932 
0.895 
0.215 
0.989 
0.830 
6. 5+adaptation 
0.960 
0.964 
0.962 
0.788 
0.974 
0.919 
0.950 
0.934 
0.449 
0.980 
0.957 
0.975 
0.966 
0.453 
0.989 
0.940 
0.974 
0.957 
0.210 
0.995 
0.919
0.952
0.935
0.311
0.981
0.900 
0.948 
0.923 
0.158 
0.983 
0.901 
0.940 
0.920 
0.410 
0.982 
0.862 
0.932 
0.895 
0.215 
0.989 
Table 1. Our system results on all the tracks. 
From Table 1 we can find that, in rows 1 and 2, 
the dictionary-based methods already achieve 
quite good recall, but the precisions are not very 
good because they cannot correctly identify un-
known words that are not in the lexicon such as 
factoids and name entities. We also find that 
even using the same lexicon, our approach that 
is based on the N-gram language models outper-
forms the greedy approach because the use of 
context model resolves more ambiguities in 
segmentation. As shown in Rows 3 to 5, when 
components are switched on in turn, the overall 
word segmentation performance increases con-
sistently. The morphological analysis has no 
contribution to the overall performance in Row 
4. The main reason is that the number of MDWs 
used in our system is very small (only 2,430) 
and there may exist very small MDWs in the test 
sets. The similar cases occur on NE identifica-
tion in the close tracks in Row 5 since we would 
not do NE identification at all in the close tracks. 
We also notice that the contribution of NE iden-
tification is very little in the open tracks, which 
shows that the performance of NE identification 
is not very good in our system, and explains 
why our OOV recall is not very high compared 
152
with other participants in the bakeoff. This is 
one area of our future work to improve. The re-
sults of standards adaptation on four bakeoff test 
sets are shown in Row 6. It turns out that per-
formance except IV recall improves slightly 
across the board in all four test sets. The main 
reason is that the training data and lexicon we 
used are mainly from the four providers in the 
bakeoff, there does not exist any different seg-
mentation standards.  
4 Conclusions 
The evaluation results show that the closed tests 
is not very good compared with other partici-
pants, the one main reason is that the word-
based language model we used is not competi-
tive compared with other algorithms in the 
closed tracks. One area of our future work is to 
apply other machine learning algorithm, like 
Maximum Entropy (ME), Support Vector Ma-
chine (SVM), Conditional Random Field (CRF), 
etc. 
Acknowledgements 
The work reported here was a team effort. We 
thank Wu Liu, Haitao Zeng, Nan He for their 
help in the experimentation and evaluation of 
the system. 
References 
Andi Wu. 2003. Customizable segmentation of mor-
phologically derived words in Chinese. Interna-
tional Journal of Computational Linguistics and 
Chinese Language Processing, 8(1): 1-27. 
Aoe, J. 1989. An Efficient Digital Search Algorithm 
by Using a Double-Array Structure. IEEE Trans-
actions on Software Engineering, Vol. 15, 9: 
1066-1077. 
George Anton Kiraz. 1999. Compressed Storage of 
Sparse Finite-State Transducers. 4th International 
Workshop on Automata Implementation, Pages: 
109-121. 
Jian Sun, Ming Zhou and Jianfeng Gao. 2003. Chi-
nese named entity identification using class-based 
language model. International Journal of Compu-
tational Linguistics and Chinese Language Proc-
essing, 8(1). 
Jianfeng Gao, Mu Li, Andi Wu and Chang-Ning 
Huang. 2004a. Chinese word segmentation: a 
pragmatic approach. Microsoft Research Techni-
cal Report, MSR-TR-2004-123. 
Julia Hockenmaier, Chris Brew. 1998. Error driven 
segmentation of Chinese. Communications of 
COLIPS, 8(1): 69-84. 
Xinnian Mao, Heng Li, Yuan Dong, Haila Wang. 
2005. Chinese Morphological Analyzer. IEEE
NLP-KE 2005, submitted. 
153
Chinese Word Segmentation and Named Entity Recognition Based on 
Conditional Random Fields 
Xinnian Mao 
France Telecom R&D Center (Beijing), Bei-
jing, 100080, P.R.China 
xinnian.mao@orange-
ftgroup.com 
Saike He 
University of Posts and Telecommunications, 
Beijing, 100876, P.R.China 
  
Sencheng Bao 
University of Posts and Telecommunications, 
Beijing, 100876, P.R.China  
Yuan Dong1,2 
1France Telecom R&D Center (Beijing), 
Beijing, 100080, P.R.China 
2University of Posts and Telecommunica-
tions, Beijing, 100876, P.R.China 
yuan.dong@orange-
ftgroup.com 
Haila Wang 
France Telecom R&D Center (Beijing), Bei-
jing, 100080, P.R.China 
haila.wang@orange-
ftgroup.com  
 
Abstract 
Chinese word segmentation (CWS), named 
entity recognition (NER) and part-of-
speech tagging is the lexical processing in 
Chinese language. This paper describes the 
work on these tasks done by France Tele-
com Team (Beijing) at the fourth Interna-
tional Chinese Language Processing Bake-
off. In particular, we employ Conditional 
Random Fields with different features for 
these tasks. In order to improve NER rela-
tively low recall; we exploit non-local fea-
tures and alleviate class imbalanced distri-
bution on NER dataset to enhance the re-
call and keep its relatively high precision. 
Some other post-processing measures such 
as consistency checking and transforma-
tion-based error-driven learning are used to 
improve word segmentation performance. 
Our systems participated in most CWS and 
POS tagging evaluations and all the NER 
tracks. As a result, our NER system 
achieves the first ranks on MSRA open 
track and MSRA/CityU closed track. Our 
CWS system achieves the first rank on 
CityU open track, which means that our 
systems achieve state-of-the-art perform-
ance on Chinese lexical processing. 
1 Introduction 
Different from most European languages, there is 
no space to mark word boundary between Chinese 
characters, so Chinese word segmentation (CWS) 
is the first step for Chinese language processing. 
From another point that there is no capitalization 
information to indicate entity boundary, which 
makes Chinese named entity recognition (NER) 
more difficult than European languages. And part-
of-speech tagging (POS tagging) provides valuable 
information for deep language processing such as 
parsing, semantic role labeling and etc. This paper 
presents recent research progress on CWS, NER 
and POS tagging done by France Telecom Team 
(Beijing). Recently, Conditional Random Fields1 
(CRFs) (Lafferty et al, 2001) have been success-
fully employed in various natural language proc-
essing tasks and achieve the state-of-the-art per-
formance, in our system, we use it as the basic 
framework and incorporate some other post-
processing measures for CWS, NER and POS tag-
ging tasks.  
2 Chinese Named Entity Recognition 
NER is always limited by its lower recall due to 
the imbalanced distribution where the NONE class 
dominates the entity classes. Classifiers built on 
such dataset typically have a higher precision and a 
lower recall and tend to overproduce the NONE 
                                                 
1 We use the CRF++ V4.5 software from 
http://chasen.org/~taku/software/CRF++/ 
90
Sixth SIGHAN Workshop on Chinese Language Processing
class (Kambhatla, 2006). Taking SIGHAN Bakeoff 
2006 (Levow, 2006) as an example, the recall is 
lower about 5% than the precision for each submit-
ted system on MSRA and CityU closed track. If we 
could improve NER recall but keep its relatively 
high precision, the overall F-measure will be im-
proved as a result. We design two kinds of effec-
tive features: 0/1 features and non-local features to 
achieve this objective. Our final systems utilize 
these features together with the local features to 
perform NER task. 
2.1 Local Features 
The local features are character-based and are in-
stantiated from the following temples: 
Unigram: Cn (n=-2,-1, 0, 1, 2). 
Bigram: CnCn+1 (n=-2,-1, 0, 1) and C-1C1. 
Where C0 is the current character, C1 the next 
character, C2 the second character after C0, C-1 the 
character preceding C0, and C-2 the second charac-
ter before C0.  
2.2 0/1 Features 
In order to alleviate the imbalanced class distribu-
tion, we assign 1 to all the characters which are 
labeled as entity and 0 to all the characters which 
are labeled as NONE in training data. In such way, 
the class distribution can be alleviated greatly, tak-
ing Bakeoff 2006 MSRA NER training data for 
example, if we label the corpus with 10 classes, the 
class distribution is 0.81(B-PER):1.70(B-LOC):0.95(B-
ORG):0.81(I-PER):0.88(I-LOC):2.87(I-ORG):0.76(E-
PER):1.42(E-LOC):0.94(E-ORG):88.86(NONE), if we 
change the label scheme to 2 labels (0/1), the class 
distribution is 11.14 (entity):88.86(NONE). We 
train the 0/1 CRFs tagger using the local features 
alone. For the 0/1 features, during the training 
stage, they are assigned with 2-fold cross valida-
tion, and during the testing stage, they are assigned 
with the 0/1 tagger.  
2.3 Non-local Features 
Most empirical approaches including CRFs cur-
rently employed in NER task make decision only 
on local context for extract inference, which is 
based on the data independent assumption. But 
often this assumption does not hold because non-
local dependencies are prevalent in natural lan-
guage (including the NER task). How to utilize the 
non-local dependencies is a key issue in NER task. 
Up to now, few researches have been devoted to 
this issue; existing works mainly focus on using 
the non-local information for improving NER label 
consistency (Krishnan and Manning, 2006). There 
are two methods to use non-local information. One 
is to add additional edges to graphical model struc-
ture to represent the distant dependencies and the 
other is to encode the non-locality with non-local 
features. In the first approach, heuristic rules are 
used to find the dependencies (Bunescu and 
Mooney, 2004) or penalties for label inconsistency 
are required to handset ad-hoc (Finkel et al, 2005). 
Furthermore, high computational cost is spent for 
approximate inference. In order to establish the 
long dependencies easily and overcome the disad-
vantage of the approximate inference, Krishnan 
and Manning (2006) propose a two-stage approach 
using CRFs framework with extract inference. 
They represent the non-locality with non-local fea-
tures, and extract them from the output of the first 
stage CRF with local context alone; then they in-
corporate the non-local features into the second 
CRF. But the features in this approach are only 
used to improve label consistency in European 
languages. Similar with their work encoding the 
non-local information with non-local feature, and 
we also exploit the non-local features under two-
stage architecture. Different from their features are 
activated on the recognized entities coming from 
the first CRF, the non-local features we design are 
used to recall more missed entities which are seen 
in the training data or unseen entities but some of 
their occurrences being recognized correctly in the 
first stage, so our non-local features are activated 
on the raw character sequence.  
Different NER in European languages, where 
entity semantic classification is more difficult 
compared with boundary detection, in Chinese, the 
situation is opposite.  So we encode different use-
ful information for Chinese NER two subtasks: 
entity boundary detection and entity semantic clas-
sification. Three kinds of non-local features are 
designed; they are fired on the token sequences if 
they are matched with certain entity in the entity 
list in forward maximum matching (FMM) way. 
Token-position features (NF1): These refer to 
the position information (start, middle and last) 
assigned to the token sequence which is matched 
with the entity list exactly. These features enable 
us to capture the dependencies between the identi-
cal candidate entities and their boundaries. 
91
Sixth SIGHAN Workshop on Chinese Language Processing
Entity-majority features (NF2): These refer to 
the majority label assigned to the token sequence 
which is matched with the entity list exactly. These 
features enable us to capture the dependencies be-
tween the identical entities and their classes, so 
that the same candidate entities of different occur-
rences can be recalled favorably, and their label 
consistencies can be considered too. 
Token-position & entity-majority features 
(NF3): These features capture non-local informa-
tion from NF1 and NF2 simultaneously. They take 
into account the entity boundary and semantic 
class information at the same time. 
Figure 1 shows the flow of using non-local fea-
tures under CRFs framework in two-stage architec-
ture. The first CRF is trained with local features 
alone, and then we test the testing data with the 
first CRF and get the entities plus their type from 
the output. The second CRF utilizes the 0/1 fea-
tures and the non-local features derived from the 
entity list which is merged by the output of the first 
CRF from the testing data and the entities extracted 
directly from the training data. We compare the 
three kinds of non-local features on MSRA and 
CityU closed track in SIGHAN 2006 and we find 
that the NF3 is the best (Mao etc, 2007). So we 
only incorporate the NF3 into our final NER sys-
tem.  
 
Figure 1. The flow using non-local features 
in two-stage architecture 
2.4 Results 
We employ BIOE1 label scheme for the NER task 
because we found it performs better than IOB2 on 
Bakeoff 2006 (Levow, 2006) NER MSRA and 
CityU corpora. Table 1 presents the official results 
on the MSRA and CityU corpus. The F-measure 
on MSRA open track is so high just because the 
testing data in Bakeoff 2007 is part of its Bakeoff 
2006 training dataset and we utilize this corpus for 
training the final CRFs classifier. The F-measure 
on CityU open track is not much superior to its 
closed track because we only use its Bakeoff 2006 
corpus to train the 0/1 CRFs, but not use the Bake-
off 2006 corpus to train final classifier. 
 
Run ID  F-Score  Run ID  F-Score 
cityu_c  84.99 cityu_o  87.92 
msra_c  92.81 msra_o 99.88 
 Table 1: The official results on NER  
closed(c) tracks and open(o) tracks 
3 Chinese Word Segmentation 
Type Feature 
Unigram Cn (n=-2,-1, 0, 1, 2). 
Bigram CnCn+1 (n=-2,-1,0, 1) 
Jump C-1C1 
Punc Pu (C0) 
Date, Digit, Letter T-1T0T1 
Table 2: The features used in our CWS systems 
 
Table 2 lists the features we used in our CWS sys-
tems. After the raw corpus is processed by CRFs, 
two other post-processing measures are performed. 
We utilize transformation-based error-driven learn-
ing (TBL)2 to further improve CWS and perform 
consistency checking among different occurrences 
of a particular character sequence. For TBL, we 
use the template defined in (He et al). Our CWS 
system participate almost all the tracks and table 3 
lists the official results.  
 
Run ID F-Score Run ID F-Score 
cityu_c_a 94.43 cityu_o_a 96.97 
cityu_c_b error (94.31) cityu_o_b 96.86 
ckip_c_a 93.17 ckip_o_a 93.25 
ckip_c_b 93.06 ckip_o_b 93.64 
ctb_c_a 94.86 ctb_o_a 97.93 
ctb_c_b 94.74 ctb_o_b 97.28 
ncc_c_a 92.99 sxu_c_a 95.46 
ncc_c_b 92.89 sxu_c_b 95.17 
Table 3: The official results on CWS closed(c) 
tracks and open(o) tracks 
 
In the table 3, run (a) means that we only per-
form consistency checking; run (b) means that 
                                                 
2 We use the TBL software from 
http://nlp.cs.jhu.edu/~rflorian/fntbl/index.html 
92
Sixth SIGHAN Workshop on Chinese Language Processing
TBL is performed after consistency checking is 
done. We make a mistake on cityu_c_b because we 
rename cityu_c_a as cityu_c_b, so the two results 
are the same, after we correct the mistake and 
score again; we achieve an F-measure of 94.31%. 
In the closed tracks, we first train initial CRFs 
with 3-fold cross-validation; then we test the train-
ing data (three parts) with the three trained CRFs, 
we train the TBL learner on the training data com-
pared it with the testing result from the initial 
CRFs. The consistency checking is inspired by (Ng 
and Low, 2004). Table 4 lists the corpus used to 
train the CRFs and TBL learner in the open tracks. 
 
 CRFs  TBL  
CityU 2005,2006,2007 2003 
CKIP 2007 2006 
CTB 2006,2007 2007 
Table 4. Corpora used to train the CRFs classi-
fier and the TBL learner 
 
In the open track, we collect the consistency list 
from all its correspondent Bakeoff corpora, the 
gazetteer extract from People Daily 2000 and idi-
oms, slang from GKB. From the table 3 in the 
closed test, we can confirm that TBL may not im-
prove CWS performance, while in most cases, per-
formance will surely draw back. The reason lies in 
the fact that the learning capability of CRFs is su-
perior to that of TBL, if they are trained with the 
same corpus, TBL may modify some correctly tags 
by CRFs. This can be seen from Table 3 that re-
sults without TBL (in run (a)) are almost superior 
to that with TBL (in run (b)).  
4 Part-of-speech Tagging 
For POS tagging task, apart from the local features 
same as used in NER, two other features are de-
signed to improve the performance. 
? Ambiguous part-of-speech: this feature is 
true when the word has more than 2 kinds 
of part-of-speech. 
? Major part-of-speech: The feature is as-
signed as the major part-of-speech for any 
word. We do not assign the value to the 
new words. 
Table 5 shows the performance in the closed 
tracks. Because we only used the simple features 
and do not process the unknown word specially, 
our performance is not satisfactory. 
 
Run ID F-Score Run ID F-Score 
cityu_c 87.93 ctb_c 92.03 
ckip_c 87.93 ncc_c 91.72 
ctb_c 92.03   
Table 5: The official results on POS tagging in 
closed tracks 
References 
R. Bunescu and R. J. Mooney. 2004. Collective Infor-
mation Extraction with Relational Markov Networks. 
In Proceedings of the 42nd ACL, 439?446. 
J. Finkel, T. Grenager, and C. D. Manning. 2005. Incor-
porating Non-local Information into Information Ex-
traction Systems by Gibbs Sampling. In Proceedings 
of the 42nd ACL, 363?370. 
Nan He, Xinnian Mao, Yuan Dong, Haila Wang, 2007. 
Transformation-based Error-driven Learning as Post-
processing for Chinese Word Segmentation, In Pro-
ceedings of the 7th International Conference on Chi-
nese Computing, 46-51, Wuhan, China.   
N. Kambhatla. 2006. Minority Vote: At-Least-N Voting 
Improves Recall for Extracting Relations. In Pro-
ceeding of the 44th ACL, 460?466. 
V. Krishnan and C. D Manning. 2006. An Effective 
Two-Stage Model for Exploiting Non-Local Depend-
encies in Named Entity Recognition. In Proceedings 
of the 44th ACL, 1121?1128. 
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of the 18th ICML, 282?289, San Francisco, CA. 
G. Levow. 2006. The Third International Chinese Lan-
guage Processing Bakeoff: Word Segmentation and 
Named Entity Recognition. In Proceedings of 
SIGHAN-2006, 108-117. Sydney, Australia.  
Xinnian Mao, Xu Wei, Yuan Dong, Saike He and Haila 
Wang, 2007. Using Non-local Features to Improve 
Named Entity Recognition Recall, In Proceedings of 
the 21th Pacific Asia Conference on Language, In-
formation and Computation, 303-310, Seoul, Korea. 
Hwee Tou Ng, Jin Kiat Low, 2004. Chinese Part-of-
Speech Tagging: One-at-a-Time or All at Once? 
Word-based or Character based? In Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, Spain. 
93
Sixth SIGHAN Workshop on Chinese Language Processing
