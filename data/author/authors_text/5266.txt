A Multiple-Document Summarization System with User
Interaction
Hiroyuki SAKAI
Toyohashi University of Technology
1-1 Hibarigaoka, Tempaku,
Toyohashi 441-8580,
Japan,
sakai@smlab.tutkie.tut.ac.jp
Shigeru MASUYAMA
Toyohashi University of Technology
1-1 Hibarigaoka, Tempaku,
Toyohashi 441-8580,
Japan,
masuyama@tutkie.tut.ac.jp
Abstract
We propose a multiple-document summa-
rization system with user interaction. Our
system extracts keywords from sets of docu-
ments to be summarized and shows the key-
words to a user on the screen. Among them,
the user selects some keywords reflecting
his/her needs. Our system controls the pro-
duced summary by using these selected key-
words. For evaluation of our method, we
participated in TSC3 of NTCIR4 workshop
by letting our system select 12 best key-
words regarding scoring by the system. Our
participated system attained the best per-
formance in content evaluation among sys-
tems not using sets of questions. Moreover,
we evaluated effectiveness of user interac-
tion in our system. With user interaction,
our system attained both higher coverage
and precision than that without user inter-
action.
1 Introduction
Recent rapid progress of computer and com-
munication technologies enabled us to access
enormous amount of machine-readable informa-
tion easily. However, this has caused the in-
formation overload problem. In order to solve
this problem, automatic summarization meth-
ods have been studied (Mani and T.Maybury,
1999). In particular, the necessity for a
multiple-document summarization has been in-
creasing and the multiple-document summa-
rization technology has been intensively studied
recently (Mani, 2001).
In this paper, we define multiple-document
summarization as a process for producing a
summary from a relevant document set. Such a
document set may be very large and may con-
tain a number of topics. It is preferable that
a summary produced by a multiple-document
summarization system from the document set
covers all topics contained in the document set.
However, it is difficult to produce a summary
that covers all the topics in the document set
with a small number of characters. For example,
a document set relevant to ?releasing AIBO?
contains some topics, e.g., what is AIBO?, how
to sell AIBO?, etc. Moreover, sentences recog-
nized as important sentences considerably dif-
fer person to person (Nomoto and Matsumoto,
2001). This is because ?summarization need?,
i.e., topics a different person wants to read, may
differ. Hence, we propose a multiple-document
summarization system with user interaction for
coping appropriately with user?s summarization
need. Our system extracts keywords from a doc-
ument set to be summarized and shows the key-
words to a user. Among them, the user selects
keywords reflecting user?s summarization need.
Our system controls a produced summary by
using the keywords selected by the user. For re-
alizing our purpose, we have devised a scoring
method for keywords extraction specialized to
our purpose. We would like to emphasize here
the fact that scoring of words for extracting key-
words shown to a user is crucial for the system
performance as well as different from those used
in usual automatic indexing.
We participated in TSC3 (Text Summariza-
tion Challenge - 3) of NTCIR4 workshop 1 and
attained the best performance in content evalu-
ation among systems not using sets of questions.
Note that our system participated in TSC3 is an
automatic summarization system without user
interaction by letting our system with user in-
teraction select 12 best keywords regarding scor-
ing by the system. Moreover, we evaluated ef-
fectiveness of user interaction and that with user
interaction attained both higher coverage and
precision than that without user interaction.
2 Feature of our multiple-document
summarization system
Our multiple-document summarization system
proposed in this paper is different from previ-
1http://www.lr.pi.titech.ac.jp/tsc/index-en.html
ously proposed multiple-document summariza-
tion methods (see, e.g.,(Barzilay et al, 1999),
(Mani and Bloedorn, 1999), (Goldstein et al,
2000), (Ando et al, 2000), (Lin and Hovy,
2002), (Nobata and Sekine, 2002), (Hirao et al,
2003)) in that: (1) Our system can produce a
summary coping appropriately with each user?s
summarization need by letting a user select key-
words reflecting user?s summarization need. (2)
The keywords are extracted automatically from
a document set to be summarized by calculating
a score to each noun contained in the document
set. The formula to calculate scores consists of
not only frequency of nouns and document fre-
quency used in tf ? idf but also distribution of
nouns in the document set and location of nouns
in documents or the document set. The rea-
son why such factors are used will be explained
in the next section. (3) Our system deletes re-
dundant adnominal verb phrases in sentences to
reduce the number of characters in a sentence.
The deletable adnominal verb phrases are de-
cided statistically by using entropy based on a
probability that verbs modify noun, etc. Our
previous method (Sakai and Masuyama, 2002)
adjusted to multiple-document summarization
so that more deletable adnominal verb phrases
are recognized, is used in this system.
The interactive summarization system has
been introduced for the first time by (Saggion
and Lapalme, 2002). The system proposed in
(Saggion and Lapalme, 2002) is based on shal-
low syntactic and semantic analysis, conceptual
identification and text re-generation, while, our
system is based on a statistical method.
3 The method to extract relevant
keywords
A relevant document set S to be summarized
may be regarded as a document set obtained
by a hypothetical query from the entire doc-
ument set ? to be considered. In TSC3, the
entire document set consists of newspaper arti-
cles, Mainichi newspaper and Yomiuri newspa-
per, Japanese daily newspapers, from January
1 to December 31, 1998, 1999. We explain a
method to extract keywords relevant to such a
hypothetical query from document set S. Here,
we define such keywords as relevant keywords ti,
i = 1, 2, . . . , k. We assign scores to nouns con-
tained in document set S and nouns assigned a
large score are extracted as relevant keywords.
A large score is assigned if a noun fulfills the
following four conditions.
1. The noun that appears frequently in the
document set S to be summarized.
2. The noun that appears uniformly in each
document d ? S.
3. The noun that appears in the beginning
of a document (i.e., the 1st sentence) and
in the beginning of the document set in
chronological order (i.e., the 1st docu-
ment).
4. The noun that does not appear frequently
in entire document set ?.
Our method for extracting relevant keywords
consists of the following two steps.
Step 1: Calculate score W (ti, S) of noun ti
(i = 1, . . . , n) contained in document set
S.
Step 2: Extract the nouns with k largest score
W (ti, S) as relevant keywords.
The score W (ti, S) is calculated by formula 1.
W (ti, S) = (0.5 + Tf(ti, S)maxi=1,...,nTf(ti, S))
?(0.5 + En(ti, S)maxi=1,...,nEn(ti, S))
?maxd?S 1 + nl(d)? nlf(ti, d)nl(d)
?maxd?S 1 + |S| ? rt(ti, d)|S|
?idf(ti,?) (1)
where,
Tf(ti, S): frequency of noun ti contained in
document set S. This is calculated by for-
mula 2.
Tf(ti, S) =
?
d?S
tf(ti, d) (2)
where, tf(ti, d) is a frequency of noun ti in
document d.
En(ti, S): entropy based on the probability
that noun ti appears in document d ? S.
This is calculated by formula 3 to be intro-
duced later.
nl(d): the number of sentences in document
d ? S.
nlf(ti, d): the line number of a sentence con-
taining noun ti for the first time in docu-
ment d ? S.
rt(ti, d): the document number of document
d containing noun ti for the first time in
document set S in chronological order.
idf(ti,?): idf(Baeza-Yates and Ribeiro-Neto,
1999) value assigned to noun ti in entire
document set ?.
En(ti, S) is an entropy based on a probability
that noun ti appears in document d ? S. For ex-
ample, En(ti, S) assigned to noun ti contained
only in one document d ? S is 0. Though such
noun ti may be an important noun for document
d, it may be an irrelevant noun for document set
S. Hence, noun ti that is assigned small entropy
value should not be extracted as a relevant key-
word. However, a noun that appears uniformly
in each document contained in document set S
has a large entropy value. En(ti, S) is calcu-
lated by formula 3.
En(ti, S) = ?
?
d?S
P (ti, d) log2(P (ti, d)) (3)
where, P (ti, d) = tf(ti, d)Tf(ti, S) (4)
The 3rd term in formula 1 is to assign a large
value to a noun appearing in the beginning of a
document. The 4th term in formula 1 is to as-
sign a large value to a noun appearing in the
beginning of a document set in chronological
order. The reason why these members are in-
cluded is that the 1st sentence in the 1st docu-
ment frequently contains important information
(see, e.g.,(Nobata and Sekine, 2002)).
4 The method to extract important
sentences
The method to extract important sentences
measures similarity between a sentence and the
set of relevant keywords selected by a user, and
extracts sentences assigned large similarity as
important sentences. The similarity is calcu-
lated as cosine metric between a vector of a
sentence and a vector of the set of relevant key-
words. If the same noun as relevant keywords
is contained frequently in a sentence, the co-
sine metric assigned to the sentence has a large
value. The method to extract important sen-
tences is summarized as follows: Here, we define
relevant keywords shown to a user as keyword
set K and define relevant keywords selected by
a user as keyword set U .
Step 1: Re-calculate score of relevant key-
words ti?s by the following formula. Here,
we define the number of keywords shown to
a user to be k.
W ?(ti, S) =
{ (1 + 0.5k)W (ti, S), ti ? U
W (ti, S), otherwise (5)
Step 2: Generate relevant keyword vector VK
consisting of W ?(ti, S) (i = 1, . . . , k) as-
signed to each relevant keyword (ti ? K).
VK = (W ?(t1, S), W ?(t2, S), . . . , W ?(tk, S))
Step 3: Generate sentence vector Vs consist-
ing of W ?(tj , S) (j = 1, . . . ,m) assigned to
each noun contained in sentence s (tj ? s).
Vs = (W ?(t1, S), W ?(t2, S), . . . , W ?(tm, S))
Step 4: Calculate a cosine metric between
vector VK and vector Vs as similarity
sim(s,K) by formula 6.
sim(s,K) = VK ?Vs|VK||Vs| (6)
Step 5: Extract the sentences with m largest
similarity sim(s,K) as important sen-
tences and output these m sentences in
chronological order.
In document set S, the 1st sentence contained
in the 1st document containing important sen-
tences in chronological order is always adopted
as an important sentence in order to improve
the readability.
5 The method to delete redundant
information
In the multiple-document summarization, it is
necessary to measure the degree of closeness of
contents in extracted sentences (or documents)
and to delete redundant information. This is
because, the documents including the same con-
tents may exist in a document set to be summa-
rized. Our multiple-document summarization
system identifies close sentences in extracted
important sentences set and close documents in
the document set, and deletes redundant infor-
mation contained therein.
First, redundant information contained in the
sentences set is deleted as follows.
Step 1: Measure the difference d(s1, s2) be-
tween cosine metric sim(s1,K) assigned to
sentence s1 and sim(s2,K) assigned to sen-
tence s2.
d(s1, s2) = |sim(s1,K)? sim(s2,K)| (7)
Step 2: If d(s1, s2) has a value smaller than a
threshold value, delete sentence si having a
smaller cosine metric sim(si,K).
We determined the threshold value to be 0.0001
in Step 2. This is a sufficiently small value to
regard contents of s1 identical to contents of s2.
Next, redundant information contained in the
document set is deleted as follows. Here, we
define a set of important sentences contained in
document di as sdi. The method is as follows.
Step 1: Generate vector Vsd1 , consisting of
W ?(ti, S) (i = 1, . . . , n) assigned to nouns
contained in sd1.
Vsd1 = (W ?(t1, S),W ?(t2, S), . . . ,W ?(tn, S))
Step 2: Generate vector Vsd2 , consisting of
W ?(tj , S) (j = 1, . . . ,m) assigned to nouns
contained in sd2.
Vsd2 = (W ?(t1, S),W ?(t2, S), . . . ,W ?(tm, S))
Step 3: Calculate a cosine metric between vec-
tor Vsd1 and vector Vsd2 as similarity
sim(sd1, sd2).
Step 4: If sim(sd1, sd2) has a value larger than
a threshold value, delete document di (i =
1 or 2) having a smaller score W (sdi) (sdi
is in di). Score W (sdi) is calculated by the
following formula 8.
W (sdi) =
?
s?sdi
sim(s,K) (8)
Here, documents d1 and d2 are newspaper arti-
cles issued on the same day. We determined the
threshold value to be 0.85 in Step 4 by trial and
error using sample data provided by the orga-
nizer of TSC3. Note that this sample data has
not used in the formal run as a document set
to be summarized. Note that if di is deleted,
sentences contained in document di are not ex-
tracted and the important sentences extracted
by our system are changed. Hence, our system
executes this algorithm to delete documents and
the algorithm to extract important sentences it-
eratively until no document is deleted by this
algorithm.
6 The method to reduce the number
of characters in a sentence
Our system deletes redundant adnominal verb
phrases in sentences to reduce the number of
characters in a sentence. We define adnomi-
nal verb phrases as phrases that modify a noun
and include a verb modifying the noun. For ex-
ample, in the case of ?SONY ga kaihatsu shita
aibo( : the AIBO devel-
oped by SONY?, ?SONY ga kaihatsu shita(
: developed by SONY)? is an
adnominal verb phrase, which modifies noun
?aibo( : AIBO)?. Here, the adnominal
verb phrase ?SONY ga kaihatsu shita(
: developed by SONY)? may be deleted
if a user has known that AIBO was developed
by SONY. We define an adnominal verb phrase
modifying a noun n as V P (n). Redundant
adnominal verb phrases are deleted by an im-
proved method of (Sakai and Masuyama, 2002)
proposed by us in order to apply to multiple
documents summarization. For more details,
please refer to reference (Sakai and Masuyama,
2002) 2. The method is as follows.
Step 1: Calculate score endf(n) to assign to
noun n modified by adnominal verb phrase
V P (n) by formula 9.
Step 2: Calculate score W (V P (n), s) for ad-
nominal verb phrase V P (n) by formula 12.
Step 3: Delete adnominal verb phrase V P (n)
if the score endf(n) has a value smaller
than threshold value ?(endf(n)) and the
score W (V P (n), s) has a value smaller than
threshold value ?(W (V P (n), s)).
We decided threshold value ?(endf(n)) as 0.7
and threshold value ?(W (V P (n), s)) as 8.7 in
Step 3. These threshold values are decided by
experiments with training corpus not to be sum-
marized in the experiments. Score endf(n) ex-
presses the degree of modifier necessity of noun
n and is calculated by formula 9.
endf(n) = 1 +H(n)idf(n,?) (9)
Here, H(n) is an entropy based on a probabil-
ity that verbs modify noun n. It reflects ?fre-
quency of modification of noun n by adnomi-
nal verb phrases?, ?variety of adnominal verb
phrases modifying noun n?. H(n) is calculated
by formula 10:
H(n) = ?
?
v?V (n)
P (v, n) log2(P (v, n)) (10)
2The method of deleting adnominal verb phrases pro-
posed in (Sakai and Masuyama, 2002) attained precision
79.3%.
P (v, n) = f(v, n)?
v?V (n) f(v, n)
(11)
where,
V (n): set of verbs contained in adnominal verb
phrases modifying noun n in entire docu-
ment set ?,
f(v, n): frequency of verb v modifying noun n
in entire document set ?.
Next, W (V P (n), s) is calculated by formula 12.
W (V P (n), s) = NM(n)IM(V P (n), s)0.5 + 0.5CV (n, s) (12)
NM(n) = 0.5 + endf(n)J(n) (13)
where,
IM(V P (n), s): a factor to reflect rating of con-
text in adnominal verb phrase V P (n) con-
tained in sentence s.
CV (n, s): the number of occurrences of noun n
modified by adnominal verb phrases from
the 1st sentence in the 1st document to sen-
tence s in document d ? S in document set
S in chronological order.
J(n): the number of common nouns contained
in noun n if noun n is a compound noun.
The IM(V P (n), s) is calculated by formula 14.
IM(V P (n), s) = 0.5 +R
?
c?V P (n)
I(c, s) (14)
I(c, s) = W
?(c, S)
0.5 + 0.5CT (c, s) (15)
where,
R: the number of segments composing adnom-
inal verb phrase V P (n),
W ?(c, S): the score calculated by formula 5 to
noun c contained in adnominal verb phrase
V P (n).
CT (c, s): the number of occurrences of noun c
contained in adnominal verb phrases from
the 1st sentence in the 1st document to sen-
tence s in document d ? S in document set
S in chronological order.
We introduced CV (n, s) in formula 12 and
CT (c, s) in formula 15 in order to recognize
more deletable adnominal verb phrases than our
previous method applied directly to multiple-
document summarization.
Figure 1: A summary produced by our system
Figure 2: A summary produced by changing rel-
evant keywords
7 Implementation
We implemented our method and developed a
multiple-document summarization system. We
employed JUMAN 3 as a morphological ana-
lyzer, and KNP4 as a parser. We show a sum-
mary produced by our system in Figure 1. The
document set to be summarized contains 9 doc-
uments relevant to ?releasing AIBO? and the
summary consists of less than 236 characters.
Moreover, we show a summary in Figure 2 when
a user selects keywords relevant to the move-
ment and performance of AIBO (e.g., ?
(artificial intelligence)?) and deletes keywords
relevant to the way to sell (e.g., ? (Reser-
vation)?). Comparing Figure 1 with Figure 2,
we can make sure that summaries have been
changed by keywords selected by a user.
8 Evaluations of our system in TSC3
We participate in TSC3 (Text Summarization
Challenge - 3) of NTCIR4 workshop for evalu-
ation of information access technologies. The
purpose of TSC3 is to evaluate performance
of automatic multiple-document summarization
that summarizes newspaper articles from two
sources (Mainichi newspaper and Yomiuri news-
paper from January 1 to December 31, 1998,
3http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/juman.html
4http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/knp.html
Figure 3: Content evaluation
1999.) Our system participated in TSC3 is
not a system with user interaction for realizing
automatic multiple documents summarization.
Hence, we define the following execution of our
system to be ?Auto? for realizing an automatic
multiple-document summarization system with-
out user interaction.
Auto: The execution of our system where 12
best keywords regarding scoring by the sys-
tem are selected.
The number of keywords selected by the system
is determined by trial and error using sample
data provided by the organizer of TSC3. The
main evaluation method of TSC3 is :
Content evaluation: Human judges match
summaries they produced with system re-
sults at sentence level, and evaluate the re-
sults based on the degree of the matching
(how well they match). The sentences in
the human-produced summaries have val-
ues that show the degree of importance,
and these values are taken into account at
the final evaluation 5.
8.1 Evaluation results of TSC3
The result of content evaluation is shown in
Figure 3 6. Here, ?AUTO? shows our system
that participated in TSC3. ?Lead? is the lead
method, a baseline method. In TSC3, we are
given the sets of questions about important in-
formation of the document sets by the organizer
of TSC3. Note that these sets of questions are
produced from summaries made by human as
correct data. (For example: when will AIBO
5http://www.lr.pi.titech.ac.jp/tsc/cfp3/
task description e.html
6About 383 characters are involved in a summary of
?short? and about 742 characters are involved in a sum-
mary of ?long?.
be released ? etc.) Here, we exclude evalua-
tion results of a system that uses the sets of
questions for producing summaries of multiple
documents 7. The reason is as follows. As
mentioned above, the sets of questions are pro-
duced from summaries made by human as cor-
rect data. Hence, we consider that using the
sets of questions as machine-readable informa-
tion for producing summaries is not realistic.
Moreover, we consider that comparing systems
using the sets of questions with systems not us-
ing them by ranking is unfair.
By the result shown in figure 3, our system
that implemented ?AUTO? has attained the
best performance among the systems not using
the sets of questions.
8.2 Evaluation of user interaction
Our system is essentially a multiple-document
summarization system with user interaction.
Hence, we evaluate effectiveness of user interac-
tion of our system in this subsection. For eval-
uating it, we consider the following execution of
our system:
Interaction: Execution of our system where
relevant keywords contained in the set of
questions are selected, and relevant key-
words not contained in the set of questions
are deleted.
The ?Interaction? simulates user interaction on
our system. (i.e., we regard the set of questions
mentioned at the beginning of Sec.8.1 as user?s
summarization need. Since the set of questions
produced from summaries by human (i.e., user),
we will be able to regard the questions as user?s
summarization need.) The coverage and pre-
cision of ?Interaction? is shown in Figure 4.
Moreover, the coverage and precision of ?Auto?
and ?Lead? are shown for comparison. Here,
the coverage and precisions which take redun-
dancy into account are obtained by using the
scoring tool provided for the subtask in TSC3.
9 Discussion
From the result shown in figure 3, our system
participating in TSC3 as ?Auto? attained the
best performance among the systems not using
the sets of questions. We think the reason why
the good performance was attained is that the
first 12 keywords extracted from a document
set to be summarized by scoring by our method
7In TSC3, systems not using the sets of questions and
systems using them were evaluated together.
Figure 4: Evaluation of user interaction
were appropriate. Sentences extracted by using
keywords irrelevant to the document set may
not probably be important.
From the result shown in figure 4, we conclude
that the ?Interaction? is more effective than the
?AUTO?. Moreover, the effectiveness of user in-
teraction in the case of ?long? is more remark-
able than that of ?short?. The reason why the
effectiveness of user interaction in the case of
?long? is more remarkable is as follows. In the
case of ?short?, our system has to extract sen-
tences fewer than that of ?long?. Even if a user
had changed relevant keywords to use for sen-
tence extraction, the sentences extracted by our
system are not necessarily changed in the case
of ?short?. However, the extracted sentences
are greatly changed in the case of ?long? when
a user had changed relevant keywords. Hence,
we consider that sentences are extracted well
by changing relevant keywords in the case of
?long?.
Acknowledgment
This work was supported in part by The
21st Century COE Program ?Intelligent Human
Sensing?, from the ministry of Education, Cul-
ture, Sports, Science and Technology of Japan
and The Grant-in-Aid from the Japan Society
for the Promotion of Science.
References
R. Ando, B. Boguraev, R. Byrd, and M. Neff.
2000. Multi-document summarization by vi-
sualizing topical content. In Proceedings of
the ANLP/NAACL 2000 Workshop on Auto-
matic Summarization, pages 79?88.
R. Baeza-Yates and B. Ribeiro-Neto. 1999.
Modern Information Retrieval. Addison Wes-
ley.
R. Barzilay, K. McKeown, and M. Elhadad.
1999. Information fusion in the context of
multi-document summarization. In Proceed-
ings of the 37th Annual Meeting of the Asso-
ciation for Computatonal Linguistics, pages
550?557.
J. Goldstein, V. Mittal, J. Carbonel, and
M. Kantrowitz. 2000. Multi-document sum-
marization by sentence extraction. In Pro-
ceedings of the ANLP/NAACL 2000 Work-
shop on Automatic Summarization, pages 40?
48.
T. Hirao, K. Takeushi, H. Isozaki, Y. Sasaki,
and E. Maeda. 2003. Svm-based multi-
document summarization integrating sen-
tence extraction with bunsetsu eliminate. IE-
ICE Trans. on Information and Systems,
E86-D(9):1702?1709.
C-Y. Lin and E. Hovy. 2002. From single to
multi-document summarizaton: A prototype
system and its evaluation. In Proceedings of
the 40th Anniversary Meeting of the Asso-
ciation for Computational Linguistics (ACL-
02), pages 457?464.
I. Mani and E. Bloedorn. 1999. Summariz-
ing similarities and differences among related
documents. Information Retrieval, 1(1):35?
67.
I. Mani and M. T.Maybury. 1999. Advances
in Automatic Text Summarization. the MIT
Press.
I. Mani. 2001. Automatic Summarization.
John Benjamins Publishing Company.
C. Nobata and S. Sekine. 2002. A summariza-
tion system with categorization of document
sets. In Working Notes of the Third NTCIR
Workshop Meeting, pages 33?38.
T. Nomoto and Y. Matsumoto. 2001. An ex-
perimental comparison of supervised and un-
supervised approaches to text summariza-
tion. In Proceedings of the 2001 IEEE Inter-
national Conference on Data Mining, pages
630?632.
H. Saggion and G. Lapalme. 2002. Generat-
ing indicative-informative summaries with su-
mum. Computational Linguistics, 28(4):497 ?
526.
H. Sakai and S. Masuyama. 2002. Unsupervised
knowledge acquisition about the deletion pos-
sibility of adnominal verb phrases. In Pro-
ceedings of Workshop on Multilingual Sum-
marization and Question Answering 2002
(post-conference workshop to be held in con-
junction with COLING-2002), pages 49?56.
 
		