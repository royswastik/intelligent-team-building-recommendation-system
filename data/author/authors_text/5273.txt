Geographic reference analysis for geographic document querying
Fr?d?rik Bilhaut, Thierry Charnois, Patrice Enjalbert and Yann Mathet
GREYC, CNRS UMR 6072, Universit? de Caen
Campus 2, F-14032 Caen Cedex ? FRANCE
{fbilhaut,charnois,patrice,mathet}@info.unicaen.fr
Abstract
The work presented in this paper concerns In-
formation Retrieval from geographical docu-
ments, i.e. documents with a major geographic
component. The final aim, in response to an
informational query of the user, is to return a
ranked list of relevant passages in selected doc-
uments, allowing text browsing within them.
We consider in this paper the spatial component
of the texts and the queries. The idea is to per-
form an off-line linguistic analysis of the doc-
ument, extracting spatial expressions (i.e. ex-
pressions denoting geographical localisations).
The point is that such expressions are (in gen-
eral) much more complex than simple place
names. We present a linguistic analyser which
recognises them, performing a semantic analy-
sis and computing symbolic representations of
their "content". These representations, stored
in the text thanks to XML annotation, will act
as indexes of passages with which queries are
compared. The matching of queries with text
expressions is a complex process, needing sev-
eral kinds of numeric and symbolic computa-
tions. A prospective outline of it is described.
1 Presentation of the GeoSem project.
Passage extraction from geographical
document
The work presented in this paper concerns Information
Retrieval (IR) from geographical documents, i.e. docu-
ments with a major geographic component. Let?s precise
at once that we are mainly interested in human geogra-
phy, where the phenomena under consideration are of so-
cial or economic nature. Such documents are massively
produced and consumed by academics as well as state or-
ganisations, marketing services of private companies and
so on. The final aim is, in response to an informational
query of the user, to return not only a set of documents
(taken as wholes) from the available collection of docu-
ments, but also a list of relevant passages allowing text
browsing within them.
Geographical information is spatialised information,
information so to speak anchored in a geographical space.
This characteristic is immediately visible on geographical
documents, which describe how some phenomena (often
quantified, either in a numeric or qualitative manner) are
related with a spatial and also, often, temporal localisa-
tion. Figure 1 gives an example of this informational
structure, extracted from our favourite corpus (H?rin,
1994), relative to the educational system in France. As
a consequence a natural way to query documents will be
through a 3-dimensional topic, Phenomenon-Space-Time
as shown in Figure 2. The goal is to select passages that
fulfil the whole bunch of criteria and to return them to the
user in relevance order.
The system we designed and currently develop for that
purpose is divided in two tasks: an off-line one, devoted
to linguistic analysis of the text, and an online one con-
cerning querying itself. Let?s give an overall view of the
process, focusing on the spatial dimension of texts and
analysis. Other aspects of the project, including espe-
cially the analysis of expressions denoting phenomena,
techniques used to link the three components of infor-
mation (Space, Time, Phenomena) and implementation
issues can be found in (Bilhaut, 2003).
Concerning text analysis, the goal is to locate, extract
and analyse the expressions which refer to some geo-
graphical localisation 1 so that they act as indexes of
text passages. The first remark to do is that we have to
cope (in general) with complex nominal expressions, not
only named geographical entities, as exemplified in fig-
ure 3. Indeed the collection of (proper) place names can
1Temporal expressions (expressing temporal localisation)
are treated in a similar manner.
De 1965 ? 1985, le nombre de lyc?ens
a augment? de 70%, mais selon des rythmes
et avec des intensit?s diff?rents selon les
acad?mies et les d?partements. Faible dans
le Sud-Ouest et le Massif Central, mod?r?e
en Bretagne et ? Paris, l?augmentation a ?t?
consid?rable dans le Centre-Ouest, et en Al-
sace. [...] Intervient aussi l?allongement des
scolarit?s, qui a ?t? plus marqu? dans les d?-
partements o?, au milieu des ann?es 1960, la
poursuite des ?tudes apr?s l??cole primaire
?tait loin d??tre la r?gle.
From 1965 to 1985, the number of high-
school students has increased by 70%, but at
different rythms and intensities depending on
academies and departments. Lower in South-
West and Massif Central, moderate in Brittany
and Paris, the rise has been considerable in
Mid-West and Alsace. [...] Also occurs the
schooling duration increase which was more
important in departments where, in the middle
of the 60?s, study continuation after primary
school was far from being systematic.
Figure 1: Excerpt from (H?rin, 1994)
not constitute an adequate index: a mention of "north of
Paris" or "north of France" has obviously not the same
meaning as "Paris"or "France", not to speak of "south of
a Bordeaux-Gen?ve line". Moreover, some expressions
("industrial towns" or "rural departments?...)2" involve
a "qualitative" (demographic, sociological, economic...)
characterisation of the selected areas, involving some
knowledge of this kind.
The conclusion is that a literal matching of "queries"
against "text expressions" simply can?t do. Expres-
sions (and queries) must receive a linguistic analysis,
discovering their structure and producing some kind of
semantic representation. This is the goal of the off-line
text processing step. A linguistic analyser of spatial
expressions (nominal and prepositional phrases) have
been designed, which recognise them and produces
a symbolic representation of their "content". These
representations are associated with the text, thanks to
XML annotation, and constitute the index with which
queries will be compared. The linguistic analysis is
described in section 2.
Assuming that such an analysis is performed, we are
2
"departments" denotes in France administrative districts,
roughly equivalent to "counties"
Find the passages which concern:
- Le retard scolaire dans l?Ouest de la
France depuis les ann?es 1950.
- Educational difficulties in West of France
since the 50?s.
- L??volution des effectifs dans
l?enseignement secondaire ? Paris / dans
la r?gion parisienne.
- Variations of the number of pupils in sec-
ondary school in Paris / in Paris area
- L??volution des effectifs scolaires dans les
r?gions rurales.
- Variations of the number of pupils in rural
areas.
- Les mutations du personnel enseignant
dans les acad?mies du Sud.
- Transfers of the teaching staff to southern
districts.
Figure 2: Typical queries on geographical documents.
ready for querying. Clearly the easier way for a user to
formulate his/her query is to use also natural language.
The first step will be to apply the same linguistic analy-
sis, producing a symbolic representation of the same na-
ture as what was extracted from text. We have then to
perform some matching between (the representations of)
the query and the text. This is not a trivial task, as the
reader can guess, considering expressions and queries in
figures 2 and 3. To achieve this task, we will use ref-
erential information associated with named geographical
entities (long-lat coordinates) together with some compu-
tation exploiting the symbolic representations produced
by the linguistic analysis. A (prospective) sketch of this
process is described in section 3.
Summing up to situate the project among current re-
search, we see that the goals are those of Document Re-
trieval, but at an intra-document level, selecting passages
(Callan, 1994). But the methods are rather (though not
exclusively) those of Information Extraction in the sense
of MUC?s (Pazienza, 1997) and we are quite close to An-
swer Extraction in the sense of (Molla, 2000). In partic-
ular, the spatial component of geographical texts needs
much more than an access to geographical resources as
gazetteers: it needs both a specific semantic analysis of
complex linguistic expressions, and some symbolic and
numeric spatial computation for matching the query with
text. Let?s now consider these two aspects in turn.
QUANT : TYPE : ZONE
: administrative : qualification : position : named geo. entity
(1) : : : : ? Paris
(2) : : : au nord de : la France
(3) Quelques : villes : maritimes : :
(4a) Le quart des : : : :
(4b) Tous les : d?partements : : du nord de : la France
(4c) Quelques : : : :
(4d) Quinze : : : :
(5) Quelques : villes : maritimes : : de la Normandie
(6) Les : d?partements : les plus ruraux : situ?s au sud de : la Loire
(1): in Paris
(2): in north of France
(3): some seaboard towns
(4a/b/c/d): The quarter of / All / Some / Fifteen / districts of north of France
(5) Some Seaboard towns of Normandy
(6) The most rural districts situated from south of Loire
Table 1: Structure of spatial expressions
- Paris
- Les villes industrielles d??le de France.
- Industrial towns in ?le de France.
- La moiti? nord de la France.
- The northern half of France.
- Les d?partements ruraux du nord de la
France.
- Rural departments in the north of France.
- Au sud d?une ligne Bordeaux-Gen?ve.
- In the south of a Bordeaux-Gen?ve line.
Figure 3: Typical spatial expressions
2 Spatial analysis
2.1 Description of spatial expressions
Table 1 shows a significant sample of spatial expres-
sions found in our corpus. It enlightens the two com-
ponents which characterises their informational structure,
[Type] and [Zone], Which can be altogether present or
not. Hence three kinds of expressions can be considered:
1. Expressions in the first class contains only the
[Zone] part and denotes a georeferenced area (ex-
amples (1) and (2)). They are anchored in a
named place (Caen, France, Normandy...), later
called ?named geographical entity? (egn), on top of
which some spatial, geometrical, operations can act
(north/south of, the western/eastern part of, the sur-
roundings of...).
2. The second type of expressions denote a set of
places and can be summarised by the canonical
form [QUANTIFICATION]+[TYPE]+[ZONE]. The
set is quantified by a determiner (all, some, the, most
of...). The places are generally given an administra-
tive type (town, region...), and are located in a zone.
Sometimes, a further qualification, either sociolog-
ical (rural, urbanised, more or less densely popu-
lated, ...) or physical (near seaboard, mountainous,
...) specifies the type. We call this most general
type of expressions ?LocGeo? (geographical local-
isations): examples (4a?d),(5),(6).
3. Finally, the form [QUANTIFICATION] + [TYPE]
is a variant of the second form with a zone not ex-
pressed but implicit (and dependent on the context).
Note that this kind of expression is recognised by
our analyser if the qualification field is present. That
means that expressions as ?the districts? are not con-
sidered as a geographical entity in opposite of ?the
most rural districts? which can be geographically
determined: example (3).
Note that all components of the expression must be
taken into account when the semantic representation
is computed: not only the elements of geographical
type but also the quantification, the qualification, the
type which are crucial for querying and matching the
representation (see section 3).
2.2 Semantic representations
The semantics of expressions is represented by feature
structures as shown in figure 4.
(1)
?
?
?
?
?
?
?
?
?
zone :
?
?
?
?
?
?
?
egn :
[
ty_zone : ville
nom : Paris
]
loc : interne
coord :
[
lat : 45 .6333333
long : 5.7333333
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Paris
(4b)
?
?
?
?
?
?
?
?
quant :
[
type : exhaustif
]
type :
[
ty_zone : departement
]
zone :
?
?
?
?
egn :
[
ty_zone : pays
nom : France
]
loc : interne
position : nord
?
?
?
?
?
?
?
?
?
?
?
?
Tous les d?partements du nord de la France
(5)
?
?
?
?
?
?
?
?
?
quant :
[
type : relatif
]
type :
[
ty_zone : ville
geo : maritime
]
zone :
?
?
?
egn :
[
ty_zone : region
nom : Normandie
]
loc : interne
?
?
?
?
?
?
?
?
?
?
?
?
Quelques villes maritimes de la Normandie
Figure 4: Spatial expressions accompanied by their se-
mantic representation
The ?quant? feature corresponds to the quantification
[QUANT] part of expressions, expressed by generalised
determiners. It will be used to associate some approxi-
mate cardinality to the set of elements selected in the Loc-
Geo, allowing to compute some ?relevance value? with
respect to a given query (see section 3). Four types of
quantification are distinguished:
? absolute: ?fifteen districts?
? relative: ?32 per cent / half of the towns?
? exhaustive: ?all?, ?the?
? exhaustive-negative: ?no?, ?not any?
The ?type? field gives the qualitative characterisation
of the selected places. It can be administrative (ty_zone)
and/or socio-economic (geo).
Finally the ?zone? feature gives an abstract description
of the geographical localisation. It is defined by four pos-
sible sub-features:
? ?egn? (named geographical entity) with the name
and type of this entity;
? the ?coord? field gives the coordinates of the named
place, when available.
? when expressed, the ?position? describes the spatial
operator acting on the egn (for example a N-S-E-W
orientation);
? this information is completed by the ?loc? feature
(localisation), which can take only two values, in-
ternal or external, according to whether the geomet-
rically selected area ?position? criterium applies in-
side or outside the zone: ?in the north of France? is
internal to France, while ? north of Paris? is (proba-
bly) external.
It must be mentioned here that his last feature
raises interesting and difficult semantical consider-
ations and the implemented procedures yet subject
to strong limitations. Notably, it is strictly local (it
does not consider the context of the analysed ex-
pression), while the localisation can be ambiguous:
?north of Paris? can also denote an internal (north-
ern) part of Paris. More generally, a precise study of
spatial prepositions remains to be done.
The semantics of extracted phrases (represented as
feature structures) are exemplified in Fig. 4. Example
(4.b) stipulates an exhaustive determination selecting
all entities of the given TYPE (departments) located
in ZONE. This zone matches with the northern half
inside the named geographic entity (France). In (5) the
determination (induced by "quelques / some") is relative,
i.e. only a part of the elements given by the type is to
be considered. Here, TYPE stipulates that we only keep
from ZONE (Northern Normandy) the "towns" which
are "seaboard".
In fact, the structure (and semantics) of spatial expres-
sions is significantly more complex allowing notably:
? some kind of recursivity as in: "les villes maritimes
des d?partements ruraux du nord de la France" (the
seaboard towns of rural districts in north of France)
where the LocGeo ?villes maritimes? is embedded
inside the LocGeo ?d?partements ruraux du nord de
la France?.
? Geometrically defined zones: "le triangle Avignon-
Aix-Marseille" (the Avignon-Aix-Marseille trian-
gle); and areas defined by some kind of bounds:
"du Sud-Ouest ? la Bourgogne" (from South-West to
Burgundy).
? Enumerations of different kinds: "? Paris, Lyon et
Marseille" (in Paris, Lyon and Marseille), "dans les
d?partements de Bretagne et de Normandie" (in the
departments of Brittany and Normandy), "dans la
France du Centre et de l?Ouest" (in Center and West
of France), etc.
Such enumerations are quite frequent in the corpus,
and treated by the linguistic analyser. In particular
the different entity types appearing in an enumer-
ation are considered, and simultaneously the lexi-
cal head of the phrase is correctly distributed over
the constituents of its expansion, as in: ?dans les
d?partements bretons et normands, ? Paris, et dans
les r?gions du sud et du sud-ouest? where ?d?-
partements? is distributed over ?bretons? and ?nor-
mands?, and similarly ?r?gions? over ?sud? and
?sud-ouest?.
? some anaphoric expressions not still treated (?ces r?-
gions?, these regions).
2.3 Implementation
The whole process is implemented using the Lingua
Stream platform, designed for the project 3. We assume
a tokenisation and a morphological analysis of the text:
presently we use Tree-Tagger (Schmid, 1994) which de-
livers the lemma and part-of-speech (POS) categorisa-
tion. This is turned into a form acceptable by Prolog
(list of terms) and a definite clause grammar (DCG) per-
forms altogether syntactic and semantic analyses. The se-
mantic representations are synthesised in a compositional
process. Prolog proves to be an interesting choice here
since it allows complex semantic computations to be in-
tegrated in the grammar, and unification on feature struc-
tures thanks to GULP (Covington, 1994). Presently, the
grammar contains 160 rules and an internal lexical base
of about 200 entries, including grammatical words and
administrative or socio-economic terms. A gazetteer of
10000 named places located in France is used as external
lexicon, providing administrative types and geographical
coordinates4.
2.4 Evaluation and results
The analyser was designed by observation of (H?rin,
1994), and a qualitative evaluation on several other texts
seems to indicate that we captured correctly the general
structure of spatial expressions. However a more pre-
cise, quantitative, evaluation on a wide and diversified
corpus is still an open question, left for further work.
Another important aspect concerns evaluation of the se-
mantic analysers, esp. the spatial one. We have to com-
pare the semantic structures computed by the system with
expected ones and hence to define a relevant and robust
measure of adequation between complex feature struc-
tures.
3publicly available from the web site:
http//users.info.unicaen.fr/ fbilhaut/linguastream.html
4publicly available from the web site:
http://www.nima.mil/gns/html/
The whole process takes about 30? on our favourite
corpus (H?rin, 1994) (200 text pages). 900 expressions
are recognised. Though processing time is not so crucial
for off-line analysis, we also want to improve the sys-
tem?s efficiency: working on the grammars and their im-
plementation techniques (such as bottom-up parsing and
compilation of feature structures as described in (Cov-
ington, 1994)),we hope to gain a factor 2 or 3. Other,
possibly more efficient, parsing methods could also be
considered if necessary, provided a good integration in
the LinguaStream platform is preserved.
3 Semantic matching
The work presented now is mainly prospective. As an-
nounced in part I, once a user has entered a query, the sys-
tem has to determine whether a given passage is relevant
or not. More precisely, we expect the system to deliver,
rather than a yes/no value, a relevance degree, so that a
ranked list of passages (from best to least) would be re-
turned to the user as an answer to his/her query. This task
requires to perform some matching between the semantic
representation of the request (calculated on-line) and the
one of a given passage (precalculated, i.e. off-line). Fig-
ure 5 shows a query (Q1) and six passage excerpts (from
P1 to P6) so that we have an overview of the problem :
for each passage, is this passage relevant, how relevant is
it, and according to what criteria ?
Q1) Which passages address Paris ?
P1) La capitale [...] - the capital (city).
P2) Les villes de la Seine ? l?exception de
Paris - Towns in the department of Seine, Paris
excepted
P3) Les grandes villes fran?aises- Big cities in
France.
P4) La moiti? nord de la France - The northern
half of France.
P5) Au sud d?une ligne Bordeaux-Gen?ve -
South of a Bordeaux-Gen?ve line.
P6) La plupart des villes d?Ile de France - Most
of towns in Ile de France.
Figure 5: One query and six passage excerpts
We can split the process in two major steps. First, a
compatibility diagnosis is delivered. In other words, it
consists in stating whether a passage could be relevant
or not, relying on the fact that there is no geographic in-
compatibility. This step should obviously ban P2 and P5.
Second, if a given passage is considered as "compatible",
the system computes a relevance degree, i.e. delivers a
value from zero (worst, shouldn?t happen if the first step
does its job) to one (best, you can?t find a more relevant
passage with respect to the query). This step would de-
liver a ranked list such as [P1, P3, P6, P4]. (The precise
order is still a open question, since it is not obvious that
P3 should come before P6, for instance).
Let?s go deeper in these two steps.
3.1 Compatibility computation
From examples in Figure 5. we can immedialtely see
that this task involves in the general case much more
than a simple word matching between the query and
text expressions. Otherwise, the system would only be
able to return passages which contain "Paris", namely
P4, which precisely should be banned as result of the
negation form (?Paris excepted?). Let?s present what
kind of knowledge and algorithms the system can use to
process query Q1 relatively to passages P1 to P6.
P1: In the context this excerpt is taken from, we are
concerned with France. A gazetteer or a GIS would
clearly say that France?s capital is Paris. Hence, the
answer is quite easy to get but, once more, doesn?t rely
on direct word matching.
P2: This excerpt has a two components structure,
namely "les villes d?Ile de France" and "? l?exception de
Paris". The first one needs an answer to :
- a "is type of X T?" question, X being here Paris, and T
"city", and
- a "does X belongs to Y ?" question, X still being Paris,
and Y being a french department, la Seine.
For both these questions, a gazetteer will do (and answer
positively). The second component "? l?exception de
Paris" states a restriction over the first requirement
and, since "Paris" matches "Paris", P2 would finally be
banned.
P3: Paris is a french city, as a gazetteer would say. But
we?re concerned here with more sofisticated semantics
as we have to interpret "big cities" (a piece of knowledge
not in the scope of a gazetteer). Since qualifiers such as
"big", "middle", "small" and so on, are relative to a set of
entities, we propose to generate off-line a resource such
as the rank of a X entity relatively to a criterium C among
all entities of a given type T. We can then interpret ?big?
as ?to be in the upper 20%?, and so on with ?medium?
or ?small?. Note that french qualifier "grand" is quite
ambiguous, (denoting population as well as surface), but
"largest" clearly involves surface criterion.
In the next two examples, we?ll have to go deeper in
geographic computation. Indeed, we understand that P4
is compatible with Q1, and that P5 isn?t, but how could
the system know that ? It has to compute it. This requires
a kind of geometric compatibility between shapes
associated with the entities denoted by the request and
text expressions, in topological terms: does X cover (or
intersect) Y ? Contrary to examples P1 to P3, where this
compatibility was de facto, embedded in gazetteer-like
knowledge, we have now to cope with complex and
dynamic geographic compatibility that no gazetteer nor
GIS could directly deliver : "the northern half of France"
nor "the south of a Bordeaux-Gen?ve line" couldn?t be
indexed in a database.
P4: "northern half of X" requires to cut the area
associated with X in two, so as to keep only the part
situated above the middle line. Here, X is France. A
request to a GIS gives the minimum and the maximum
longitude, so that we obtain the middle line, and finally
the shape as given in Figure 6. Then, we look at the
long-latt coordinates of Paris, and conclude that Paris
matches this passage.
P5: For similar reasons, but with a more sophisticated
shape, we obtain a polygon as given in Figure 7. And we
conclude here with a no answer, since the coordinates of
Paris locate it out of the polygon.
Figure 6: The northern half of France
We now evoke briefly other problems which must be
faced. First, observe that there are some subtilties in the
compatibility relation, as illustrated in the following ex-
ample. How to interpret fluzzy relations like "north of X"
or "south of X" ? What part of the area associated with X
do we have to keep ? Further work must be done to make
the best choice between something like Figure 6 where
we keep half of it, and another one where we would keep
a smaller part, or for exemple a cone above X. How to
interpret the french "nord de X" in terms of topological
?in? and ?out? relations ? "Le nord de Paris" is indeed
ambiguous and can mean "a certain part located in Paris,
in the north of it" as well as "a certain region out of Paris,
Figure 7: The south of a Bordeaux-Gen?ve line
more in the north?. Another problem concerns the socio-
economic qualification of geographical areas. As already
mentionned and shown in Table 1, such characterisations
appear in a significant way in the corpus, and should be
integrated in the matching process. Clearly again, simple
word matching is not adequate and we must invoke some
semantic knowledge, formalised as a semantic net or the
like.
3.2 Computing a relevance degree
At this point, the system should be able to state if a given
passage matches a query. However, a major task still re-
maining is to sort the passages from best to worst. This
task is quite difficult since it involves heterogenous data.
3.2.1 Quantification
If we consider examples envolving quantitification, such
as (4a)-(4d), we must admit that all entities which match
the ZONE and TYPE need not to be relevant. For exam-
ple in (4c) ?some? means that only a part of the set of
specified departments is concerned, on contrary to (4b)
where ?all? is the determiner. Which ones ? the linguis-
tic expression does not say. Howeve we can compute a
probability for any of these entities to be concerned so
that we can propose the following ranked list: (4b), (4d),
(4a), (4c) for a request concerning Calvados (department
situated in the north of France):
(4a) The semantic of "the quarter" gives (no GIS needed)
a wheight equals to 25%.
(4b) The semantic of "all" gives a wheight equal to
100%.
(4c) The semantic of "some" indicates that few entities
are concerned. In this case, we stipulate a number of
5 entities (that?s a heuristic). This leads to a wheight
equals to 5n , n being the number of districts included
in the zone. A request to the GIS gives n = 52.
Hence, the weight is 552 = 9.6%
(4d) In the same way, we obtain here 1552 = 29%.
Both linguistic knowledge (for semantic interpretation
of the determiners) and geographical knowledge (for an
evaluation of probability) will be needed in this process.
3.2.2 Granularity
We adopted a liberal strategy, which selects all pas-
sages compatible with the query. The counterpart is
clearly the risk of noisy answers. The granularity cri-
terion can provide a useful numeric evaluation of the
relevance of an expression. For example, consider the
query ?find passages concening the city of Caen?, with
respect to passages mentionning ?Caen? itself, ?the Cal-
vados? (department to which Caen belongs), ?Basse-
Normandie? (Caen?s administrative Region), and finally
?the northern half of France?. For granularity reasons
(left however for further consideration), it seems desir-
able to presents the four passages in this order, from the
closest to the farthest level.
3.2.3 Negation
The paradox is as follows : if we say A is true for all
X but Y, we positively say that not A is true for Y. Com-
ing back to P2, the fact that Paris is explicitely excluded
from the set of towns is a very strong information, almost
as strong as a positive mention. We think this problem
can be managed thanks to some symetric, i.e. negative
relevance value. Hence P2 receives degree -1 with re-
spect to query Q1. The user can choose to visualize or
not passages with negative degrees among other answers.
4 Conclusion
The work presented in this paper concerns passage ex-
traction from geographical documents. We focused on
the most characteristic aspect of the project, namely
the interpretation of the spatial component of texts and
queries. We first described a linguistic analyser which
performs a semantic analysis of expressions denoting ge-
ographical localisations. This analyser is operationnal
and will in the future be developped and experimented,
in order to cope with a greater variety of expression and
cover large corpora. Then we addressed the question of
the actual querying of text. We outlined a method for
matching user requests with the computed representa-
tions of spatial expressions. We believe that some aspects
of this process can be readily implemented. However it
also raises some difficult questions, we plan to investigate
in the next future.
Acknowledgements
Special thanks to S?verine Beaudet for her decisive con-
tribution in the development of the spatial analyser.
References
Andr?e Borillo, 1998, L?espace et son expression en
fran?ais, Ophrys Press, Paris.
Fr?d?rik Bilhaut, Thierry Charnois, Patrice Enjalbert,
Yann Mathet, 2003, Passag extraction in geographi-
cal documents, Proc. Intelligent Information Systems
2003, New Trends in Intelligent Information Process-
ing ans Web Mining, Zakopane, Poland (to appear).
James P. Callan, 1994, Passage-Level Evidence in Docu-
ment Retrieval, Proc. 7th Ann. Int. ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, Dublin, Ireland.
Michael A. Covington, 1994, GULP 3.1: An Extention
of Prolog for Unification-Based Grammar, Research
Report, AI.
Robert H?rin, R?mi Rouault, 1994, Atlas de la France
Scolaire de la Maternelle au Lyc?e, RECLUS - La
Documentation Fran?aise, Dynamiques du Territoire,
14.
Diego Molla, Rolf Schwitter, Michael Hess, Rachel
Fournier, 2000, Extrans, an Answer Extraction System,
Traitement Automatique des Langues, Hermes Science
Publication, 41-2, 495-522.
Maria Teresa Pazienza (Ed.), 1997, Information Extrac-
tion, Springer Verlag.
Helmut Schmid, 1994, Probabilistic Part-of-Speech Tag-
ging Using Decision Trees, Intl. Conference on New
Methods in Language Processing. Manchester, UK.
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 6?11,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Propagation Strategies for Building Temporal Ontologies
Md. Hasanuzzaman
Normandie University
GREYC UMR 6072
Caen, France
Ga
?
el Dias
Normandie University
GREYC UMR 6072
Caen, France
St
?
ephane Ferrari
Normandie University
GREYC UMR 6072
Caen, France
Yann Mathet
Normandie University
GREYC UMR 6072
Caen, France
Abstract
In this paper, we propose to build temporal
ontologies from WordNet. The underlying
idea is that each synset is augmented with
its temporal connotation. For that purpose,
temporal classifiers are iteratively learned
from an initial set of time-sensitive synsets
and different propagation strategies to give
rise to different TempoWordNets.
1 Introduction
Temporality has recently received increased at-
tention in Natural Language Processing (NLP)
and Information Retrieval (IR). Initial works have
been proposed in NLP and are exhaustively sum-
marized in (Mani et al., 2005). More recently,
the introduction of the TempEval task (Verhagen
et al., 2009) in the Semantic Evaluation workshop
series has clearly established the importance of
time to deal with different NLP tasks. The ulti-
mate aim of research in this area is the automatic
identification of temporal expressions (timexes),
events and temporal relations within a text in the
TimeML format (Pustejovsky et al., 2005).
In IR, the time dimension has also received par-
ticular attention for the past few years. Accord-
ing to (Metzger, 2007), time is one of the key five
aspects that determine a document credibility be-
sides relevance, accuracy, objectivity and cover-
age. So, the value of information or its quality is
intrinsically time-dependent. As a consequence, a
new reasearch field called Temporal Information
Retrieval (T-IR) has emerged (Alonso et al., 2011)
and deals with all classical IR tasks such as crawl-
ing (Kulkarni et al., 2011), indexing (Anand et al.,
2012) or ranking (Kanhabua et al., 2011) from the
time viewpoint.
However, both NLP and IR evidence the lack
of temporal lexical resources. For example, auto-
matic temporal ordering of events in text is usu-
ally performed via various linguistic mechanisms
including the use of time expressions such as ?be-
fore?,?after? or ?during? that explicitly assert a
temporal relation. In particular, (Derczynski and
Gaizauskas, 2012) investigate the role of tempo-
ral signals in temporal relation extraction over the
TimeBank annotated corpus. However, the list of
such expressions is limited. From the IR view-
point, most methodologies rely on the presence of
explicit timexes and hardly bridge the gap when
no explicit mention of time is available. One re-
cent exception is proposed in (Jatowt et al., 2013)
where text time-tagging is seen as a classification
task, but no use of specific temporal clues is intro-
duced or proposed.
Inspired by SentiWordNet (Esuli and Sebas-
tiani, 2006), we propose to introduce the tempo-
ral connotation of each synset in WordNet (Miller,
1995) by iteratively learning temporal classifiers
from an initial set of time-sensitive synsets and a
given propagation strategy. As such, each synset
is automatically time-tagged with four dimensions
i.e. atemporal, past, present and future, thus giv-
ing rise to different TempoWordNets depending
on the propagation strategy.
TempoWordNets are evaluated both manually
and automatically. First, results show that man-
ual annotation of time-tagged synsets is a hard
task for humans. Second, automatic evaluation
based on sentence temporal classification shows
that the introduction of time-augmented lexical
knowledge bases (TempoWordNets) allows 3.9%
improvements of F
1
-measure against the vector
space model representation and 4.2% against the
semantic vector space model obtained with the ex-
isting WordNet time subtree.
2 Related Work
A great deal of works have been proposed in tem-
poral NLP. Most recent studies have been devel-
oped in the context of the TempEval evaluation
contests, which were initiated by (Verhagen et al.,
6
2007). TempEval was initially divided into three
challenges: (A) identifying temporal relations be-
tween events and time expressions, (B) identifying
temporal relations between events and the docu-
ment creation time and (C) identifying the tem-
poral relations between contiguous pairs of matrix
verbs. In TempEval-2 (Pustejovsky and Verhagen,
2009), the best performing systems were based
on conditional random fields mixed with parsing
methodologies (UzZaman and Allen, 2010). More
recently, in TempEval-3 (UzZaman et al., 2013),
new systems have been performing at high level
of performance for all three tasks such as the
rule-based multilingual temporal tagger Heidel-
time (Str?otgen and Gertz, 2013). In IR, the work
of (Baeza-Yates, 2005) defines the foundations of
T-IR. Since, research have been tackling several
topics such as query understanding (Metzler et al.,
2009), temporal snippets generation (Alonso et al.,
2007), temporal ranking (Kanhabua et al., 2011),
temporal clustering (Alonso et al., 2009), future
retrieval (Radinsky and Horvitz, 2013) or tempo-
ral image retrieval (Dias et al., 2012).
As expressed in (Str?otgen and Gertz, 2013),
time taggers usually contain pattern files with
words and phrases, which are typically used to
express temporal expressions in a given language
(e.g. names of months). In fact, most temporal
NLP tasks rely on a time-sensitive vocabulary. On
the contrary, T-IR systems usually do not use in-
formation about time in language although they
could benefit from it when facing the recurrent
problem of missing explicit timexes.
WordNet is a good place to start to find time-
sensitive concepts. Indeed, one can list a set
of 21 temporal synsets by iteratively following
the hyponym relation from the concept of time
(synset # 00028270) represented by the follow-
ing gloss: the continuum of experience in which
events pass from the future through the present to
the past. However, likewise the tennis problem ev-
idenced in (Fellbaum, 1998), most temporal words
are not under the concept of time. For example,
concepts such as ?prediction?, ?remember?, ?an-
cient? or ?fresh? clearly have a time dimension al-
though they are not listed under the time subtree
of WordNet. Based on the intial ideas of (Moens
and Steedman, 1987) on temporal ontologies and
inspired by SentiWordNet (Esuli and Sebastiani,
2006), we propose to enrich all WordNet synsets
with their temporal connotation.
3 TempoWordNet as SentiWordNet
In (Dias et al., 2014), we first proposed to build
TempoWordNet based on the idea of (Esuli and
Sebastiani, 2006) for SentiWordNet. Each synset
is automatically time-tagged with four dimensions
i.e. atemporal, past, present and future by per-
forming a two-step process.
A first temporal classifier is built based on a set
of manually selected seed synsets and their corre-
sponding glosses tagged as past, present and fu-
ture. This process is then iterated based on the
repetitive lexico-semantic expansion
1
of the initial
seeds lists until cross-validation accuracy drops.
This first step results in a three-class temporal
classifier and an expanded list of temporal synset
candidates.
A second temporal classifier is then learned to
time-tag synsets as atemporal or temporal. This
process is obtained by taking the final list of ex-
panded seed synsets from the previous learning
problem and randomly choosing a balanced num-
ber atemporal synsets. A 10-fold cross-validation
is then used to learn the model.
TempoWordNet is finally obtained by (1) classi-
fying all WordNet synsets as atemporal or tempo-
ral with the second classifier and (2) the resulting
temporal synsets are tagged as past, present and
future by the first classifier. This step is detailed in
(Dias et al., 2014) and all materials can be found
at http://tempowordnet.greyc.fr.
4 Diversified Expansion Strategies
The initial strategy proposed in the previous sec-
tion evidences a clear lack. As the expansion pro-
cess is semantically driven, the temporal conno-
tation is highly depend on the initial seeds lists
and as a consequence may not spread over a wide
range of concepts in WordNet. As such, we pro-
pose two different strategies of expansion: (1) the
probabilistic expansion and (2) the hybrid (proba-
bilistic combined with semantic) expansion.
Probabilistic Expansion: We first learn a tem-
poral vs. atemporal classifier based on the ini-
tial hand-crafted set of seeds proposed in (Dias
et al., 2014). In particular, the seeds defined as
past, present and future are markers of temporal-
ity, while the list of atemporal synsets is the ob-
vious counterpart. Based on this list of tempo-
1
Only exisiting lexico-semantic links inWordNet are used
to propagate the temporal connotation.
7
ral and atemporal synsets, a 10-fold cross vali-
dation process is performed to learn the temporal
vs. atemporal model, which is used to time-tag
the whole WordNet. The synsets (or glosses) with
highest temporal and atemporal values in Word-
Net are then used for the expansion process of the
seeds lists. The process is iteratively performed
and stops when accuracy drops.
After building the temporal vs. atemporal clas-
sifier, WordNet is divided into two subsets: tem-
poral synsets and atemporal ones. In order to
fine tune the temporal part of WordNet, we learn
a three-class classifier (i.e. past, present and fu-
ture) based on the initial past, present and future
seeds lists and the probabilistic expansion exclu-
sively
2
within the temporal part of WordNet. So, a
10-fold cross validation process is iteratively per-
formed until accuracy drops.
The results of the probabilistic expansion are
presented in Table 1 and Table 2, when the expan-
sion is based on the maximum probability value
3
.
Steps 1 2 3
Precision 87.3 100 100
Recall 86.7 100 100
F
1
-measure 86.9 100 100
Table 1: Cross validation for temporal vs. atem-
poral at each iteration. Probabilistic Expansion.
Steps 1 2 3
Precision 80.0 99.7 99.6
Recall 80.1 99.7 99.6
F
1
-measure 80.0 99.7 99.6
Table 2: Cross validation for past, present and fu-
ture at each iteration. Probabilistic Expansion.
Note that in our experiment, Support Vector
Machines (SVM) with a linear kernel
4
over the
vector space model representation of the synsets
(i.e. each synset is represented by its gloss en-
coded as a vector of unigrams weighted by their
frequency) have been used to classify all the
synsets of WordNet. The results show that in both
cases the expansion process stops at iteration 2.
2
Only temporal synsets are classified as past, present or
future and used for the expansion process. Note that unbal-
anced sets can be formed.
3
That means that all the synsets getting the highest value
produced by the classifier are used to expand the initial seeds
lists.
4
We used theWeka implementation SMOwith default pa-
rameters.
Hybrid Expansion: Choosing synsets from
WordNet with highest probability assigned by a
classifier learned on the glosses of initial seeds
lists can lead to the well-known semantic shift
problem. So, the idea of the hybrid expansion
is to control the expansion process so that the
most probable time-sensitive synsets are also cho-
sen based on their semantic distance with the ex-
panded seed synsets at the previous iteration. The
process is straightforward when compared to the
probabilistic expansion.
First, a two-class (temporal vs. atemporal) text
classifier is trained based on the glosses of each
synsets contained in the initial seed lists to clas-
sify all the synsets of WordNet. Thereafter, Word-
Net synsets with highest probability are selected as
candidates for expansion. From these candidates,
only the ones that present the maximum seman-
tic similarity to the previous seeds lists are cho-
sen for expansion. Note that the semantic simi-
larity is calculated between the candidate synset
and all synsets in the previous expanded seeds
lists. Once candidates for expansion have been
chosen, a 10-fold cross validation process is itera-
tively performed until accuracy becomes steady.
Second, a three-class (past, present and fu-
ture) classifier is learned over the temporal part of
WordNet with the hybrid expansion process in the
same exact manner as explained for the previous
probabilistic expansion. Results for the expansion
process are presented in the Table 3 and Table 4
for the same experimental setups as for the prob-
abilistic expansion and using the (Leacock et al.,
1998) semantic similarity measure
5
.
Steps 1 2 ... 25 26 27
Precision 87.3 94.1 ... 96.0 97.2 96.6
Recall 86.7 93.2 ... 95.5 97.0 96.3
F
1
-measure 86.9 93.6 ... 95.7 97.1 96.4
Table 3: Cross validation for temporal vs. atem-
poral at each iteration. Hybrid Expansion.
Steps 1 2 ... 15 16 17
Precision 80.0 75.7 ... 95.7 96.4 95.6
Recall 80.1 74.3 ... 95.1 96.0 95.0
F
1
-measure 80.0 74.9 ... 95.4 96.2 95.3
Table 4: Cross validation for past, present and fu-
ture at each iteration. Hybrid Expansion.
5
Different configurations as well as different similarity
metrics have been tested but these experiments are out-of-
the-scope of this paper.
8
Representation Uni.+SW Uni.+SW+Wn Uni.+SW+TWnL Uni.+SW+TWnP Uni.+SW+TWnH
Precision 85.8 85.6 87.8 89.8 89.5
Recall 85.7 85.3 87.8 89.5 89.4
F
1
-measure 85.8 85.4 87.8 89.6 89.4
Table 5: Evaluation results for sentence classification with different TempoWordNets. Balanced corpus:
346 sentences for past, 346 sentences for present and 346 sentences for future.
Evaluation: In order to intrinsically evaluate
the time-tagged WordNets (TempoWordNets), we
first performed an inter-annotation process over
samples of 50 automatically time-tagged Word-
Net synsets. In particular, three different anno-
tators were presented with temporal synsets and
their respective glosses, and had to decide upon
their correct classification (temporal vs. atempo-
ral). The results of the multirater agreement eval-
uation are presented in Table 6. In particular, we
processed the free-marginal multirater kappa val-
ues (Randolph, 2005) and the fixed-marginal mul-
tirater kappa (Siegel and Castellan, 1988) as no
bias is present in the data. Overall figures assess
moderate agreement for the three TempoWord-
Nets: TWnL for the lexico-semantic expansion,
TWnP for the probabilistic expansion and TWnH
for the hybrid expansion.
Metric TWnL TWnP TWnH
Fixed-marginal ? 0.5073 0.5199 0.4197
Free-marginal ? 0.5199 0.5199 0.4399
Table 6: Inter-annotator agreement.
These results evidence the difficulty of the task
for humans as they do not agree on a great deal of
decisions. This is particularly due to the fact that
the temporal dimension of synsets is judged upon
their glosses and not directly on their inherent con-
cept. For example, ?dinosaur? can be classified as
temporal or atemporal as its gloss any of numer-
ous extinct terrestrial reptiles of the Mesozoic era
allows both interpretations.
So, we performed a new experiment based on
those examples where human annotator agreement
was 100%. From this dataset, we performed an
inter-annotator agreement process with four an-
notators (three human annotators plus the classi-
fier). The underlying idea is to understand to what
extent the built TempoWordNets comply with the
?easy? cases. Results are illustrated in Table 7 and
clearly show the enhanced intrinsic quality of the
hybrid expansion strategy with an almost adequate
agreement for the free-marginal ?.
Metric TWnL TWnP TWnH
Fixed-marginal ? 0.4133 0.4767 0.5655
Free-marginal ? 0.4242 0.5161 0.6896
Table 7: Inter-annotation for ?easy? cases.
5 Sentence Temporal Classification
In order to evaluate TempoWordNets, we pro-
posed to test their capability to enhance the exter-
nal task of sentence temporal classification. For
that purpose, we used the corpus developed by
(Dias et al., 2014), which contains 1455 sen-
tences distributed as follows: 724 for past, 385
for present and 346 for future. Different sentence
representations have been used. First, we pro-
posed to represent each sentence with the classi-
cal vector space model using the tf.idf weighting
scheme for unigrams without stop-words removal
(Uni.+SW). Then, we proposed a semantic vector
space representation where each sentence is aug-
mented with the synonyms of any temporal word
contained in it. In particular, we proposed that
the words were matched directly from the Word-
Net time subtree (Uni.+SW+Wn) or from Tem-
poWordNet (Uni.+SW+TWnL, Uni.+SW+TWnP
and Uni.+SW+TWnH) and weighted with tf.idf.
The results of our experiments are reported in Ta-
ble 5. The results evidence that the WordNet time
subtree does not embody enough time-related in-
formation and the process of automatically time-
tagging WordNet can improve the task of sentence
temporal classification, especially with the proba-
bilistic or the hybrid expansion.
6 Conclusion
In this paper, we proposed the first steps towards
the automatic construction of temporal ontologies.
In particular, we presented and evaluated different
propagation strategies to time tag WordNet giving
rise to different TempoWordNets. First results are
promising and we deeply believe that such a re-
source can be important for time related applica-
tions both in NLP and IR. All resources can be
found at http://tempowordnet.greyc.fr.
9
References
O. Alonso, R. Baeza-Yates, and M. Gertz. 2007. Ex-
ploratory search using timelines. In Proceedings of
the ACM SIGCHI Workshop on Exploratory Search
and HCI.
O. Alonso, M. Gertz, and R. Baeza-Yates. 2009. Clus-
tering and exploring search results using timeline
constructions. In Proceedings of the 18th ACM
Conference on Information and Knowledge Man-
agement (CIKM), pages 97?106. ACM.
O. Alonso, J. Str?otgen, R. Baeza-Yates, and M. Gertz.
2011. Temporal information retrieval: Challenges
and opportunities. In Proceedings of the 1st Interna-
tional Temporal Web Analytics Workshop (TWAW),
pages 1?8.
A. Anand, S. Bedathur, K. Berberich, and R. Schenkel.
2012. Index maintenance for time-travel text search.
In Proceedings of the 35th International ACM Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 235?244.
Ricardo Baeza-Yates. 2005. Searching the future. In
Proceedings of the ACM SIGIR Workshop on Math-
ematical/Formal Methods in Information Retrieval,
pages 1?6.
L. Derczynski and R. Gaizauskas. 2012. A corpus-
based study of temporal signals. arXiv:1203.5066.
G. Dias, J.G. Moreno, A. Jatowt, and R. Campos.
2012. Temporal web image retrieval. In Proceed-
ings of the 19th Edition of the International Sympo-
sium on String Processing and Information Retrieval
(SPIRE), pages 199?204. Springer.
G. Dias, Md. Hasanuzzaman, S. Ferrari, and Y. Mathet.
2014. Tempowordnet for sentence time tagging. In
Proceedings of the 4th ACM Temporal Web Analyt-
ics Workshop (TEMPWEB).
A. Esuli and F. Sebastiani. 2006. Sentiwordnet:
A publicly available lexical resource for opinion
mining. In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC), pages
417?422.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Bradford Books.
A. Jatowt, C.-M. Au Yeung, and K. Tanaka. 2013. Es-
timating document focus time. In Proceedings of the
22nd ACM International Conference on Information
and Knowledge Management (CIKM), pages 2273?
2278.
N. Kanhabua, R. Blanco, and M. Matthews. 2011.
Ranking related news predictions. In Proceedings of
the 34th International ACMConference on Research
and Development in Information Retrieval (SIGIR),
pages 755?764.
A. Kulkarni, J. Teevan, K.M. Svore, and S. Dumais.
2011. Understanding temporal query dynamics. In
Proceedings of the 4th ACM International Confer-
ence on Web Search and Data Mining (WSDM),
pages 167?176.
C. Leacock, G.A. Miller, and M. Chodorow. 1998.
Using corpus statistics and wordnet relations for
sense identification. Computational Linguisics,
24(1):147?165.
I. Mani, J. Pustejovsky, and R. Gaizauskas. 2005. The
language of time: a reader, volume 126. Oxford
University Press.
M.J. Metzger. 2007. Making sense of credibility on
the web: Models for evaluating online information
and recommendations for future research. Journal
of the American Society for Information Science and
Technology, 58(13):2078?2091.
D. Metzler, R. Jones, F. Peng, and R. Zhang. 2009.
Improving search relevance for implicitly temporal
queries. In Proceedings of the 32nd International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval (SIGIR), pages 700?
701.
G.A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39?
41.
M. Moens and M. Steedman. 1987. Temporal ontol-
ogy in natural language. In Proceedings of the 25th
Annual Meeting on Association for Computational
Linguistics (ACL), pages 1?7.
J. Pustejovsky and M. Verhagen. 2009. Semeval-
2010 task 13: evaluating events, time expressions,
and temporal relations (tempeval-2). In Proceedings
of the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions, pages 112?
116.
J. Pustejovsky, B. Ingria, R. Sauri, J. Castano,
J. Littman, R. Gaizauskas, A. Setzer, G. Katz, and
I. Mani. 2005. The specification language timeml.
The language of time: A reader, pages 545?557.
K. Radinsky and E. Horvitz. 2013. Mining the web
to predict future events. In Proceedings of the 6th
ACM International Conference on Web Search and
Data Mining (WSDM), pages 255?264.
J.J. Randolph. 2005. Free-marginal multirater kappa
(multirater ?free): an alternative to fleiss? fixed-
marginal multirater kappa. Joensuu Learning and
Instruction Symposium.
N. Siegel and J.N. Castellan. 1988. Nonparametric
Statistics for the Social Sciences. Mcgraw-hill edi-
tion.
J. Str?otgen and M. Gertz. 2013. Multilingual and
cross-domain temporal tagging. Language Re-
sources and Evaluation (LRE), 47(2):269?298.
10
N. UzZaman and J.F. Allen. 2010. Trips and trios
system for tempeval-2: Extracting temporal infor-
mation from text. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
276?283.
N. UzZaman, H. Llorens, L. Derczynski, M. Verhagen,
J. Allen, and J. Pustejovsky. 2013. Semeval-2013
task 1: Tempeval-3: Evaluating time expressions,
events, and temporal relations. In Proceedings of the
7th International Workshop on Semantic Evaluation
(SemEval).
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
G. Katz, and J. Pustejovsky. 2007. Semeval-2007
task 15: Tempeval temporal relation identification.
In Proceedings of the 4th International Workshop on
Semantic Evaluations, pages 75?80.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
J. Moszkowicz, and J. Pustejovsky. 2009. The tem-
peval challenge: Identifying temporal relations in
text. Language Resources and Evaluation (LRE),
43(2):161?179.
11
