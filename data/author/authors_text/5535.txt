Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 648?657,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Improving Web Search Relevance with Semantic Features
Yumao Lu Fuchun Peng Gilad Mishne Xing Wei Benoit Dumoulin
Yahoo! Inc.
701 First Avenue
Sunnyvale, CA, 94089
yumaol,fuchun,gilad,xwei,benoitd@yahoo-inc.com
Abstract
Most existing information retrieval (IR)
systems do not take much advantage of
natural language processing (NLP) tech-
niques due to the complexity and limited
observed effectiveness of applying NLP
to IR. In this paper, we demonstrate that
substantial gains can be obtained over a
strong baseline using NLP techniques, if
properly handled. We propose a frame-
work for deriving semantic text matching
features from named entities identified in
Web queries; we then utilize these features
in a supervised machine-learned ranking
approach, applying a set of emerging ma-
chine learning techniques. Our approach
is especially useful for queries that contain
multiple types of concepts. Comparing to
a major commercial Web search engine,
we observe a substantial 4% DCG5 gain
over the affected queries.
1 Introduction
Most existing IR models score documents pri-
marily based on various term statistics. In tra-
ditional models?from classic probabilistic mod-
els (Croft and Harper, 1979; Fuhr, 1992), through
vector space models (Salton et al, 1975; Narita
and Ogawa, 2000), to well studied statistical lan-
guage models (Ponte and Croft, 2000; Lafferty
and Zhai, 2001)?these term statistics have been
captured directly in the ranking formula. More re-
cently, learning to rank approaches to IR (Fried-
man, 2002) have become prominent; in these
frameworks, that aim at learning a ranking func-
tion from data, term statistics are often modeled
as term matching features in a machine learning
process.
Traditional text matching features are mainly
based on frequencies of n-grams of the user?s
query in a variety of document sections, such as
the document title, body text, anchor text, and so
on. Global information such as frequency of term
or term group in the corpus may also be used, as
well as its combination with local statistics ? pro-
ducing relative scores such as tf ? idf or BM25
scores (Robertson et al, 1995). Matching may
be restricted to certain window sizes to enforce
proximity, or may be more lenient, allowing un-
ordered sequences and nonconsecutive sequences
for a higher recall.
Even before machine learning was applied to
IR, NLP techniques such as Named Entity Recog-
nition (NER), Part-of-Speech (POS) tagging, and
parsing have been applied to both query model-
ing and document indexing (Smeaton and van Ri-
jsbergen, 1988; Narita and Ogawa, 2000; Sparck-
Jones, 1999). For example, statistical concept
language models generalize classic n-gram mod-
els to concept n-gram model by enforcing query
term proximity within each concept (Srikanth and
Srihari, 2003). However, researchers have of-
ten reported limited gains or even decreased per-
formance when applying NLP to IR (Voorhees,
1999).
Typically, concepts detected through NLP tech-
niques either in the query or in documents are
used as proximity constraints for text match-
ing (Sparck-Jones, 1999), ignoring the actual con-
cept type. The machine learned approach to docu-
ment ranking provides us with an opportunity to
revisit the manner in which NLP information is
used for ranking. Using knowledge gained from
NLP application as features rather than heuris-
tically allows us much greater flexibility in the
amount and variability of information used ? e.g.,
incorporating knowledge about the actual entity
types. This has several benefits: first, entity types
appearing in queries are an indicator of the user?s
intent. A query consisting of a business category
and a location (e.g., hotels Palo Alto) appears to be
648
informational, and perhaps is best answered with
a page containing a list of hotels in Palo Alto.
Queries containing a business name and a location
(e.g., Fuki Sushi Palo Alto) are more navigational
in nature ? for many users, the intent is finding the
home page of a specific business. Similarly, entity
types appearing in documents are an indicator of
the document type. For example, if ?Palo Alto?
appears ten times in document?s body text, it is
more likely to be a local listing page than a home
page. For the query hotels Palo Alto, a local listing
page may be a good page, while for the query Fuki
Sushi Palo Alto a listing page is not a good page.
In addition, knowledge of the particular entities
in queries allows us to incorporate external knowl-
edge about these entities, such as entity-specific
stopwords (?inc.? as in Yahoo Inc. or ?services?
as in kaiser medical service), and so on.
Finally, even when using named entities only
for deriving proximity-related features, we can
benefit from applying different levels of proxim-
ity for different entities. For example, for enti-
ties like cities (e.g., ?River Side?), the proximity
requirement is fairly strict: we should not allow
extra words between the original terms, and pre-
serve their order. For other entities the proximity
constraint can be relaxed?for example, for per-
son names, due to the middle name convention:
Hillary Clinton vs. Hillary R. Clinton.
In this paper, we propose a systematic approach
to modeling semantic features, incorporating con-
cept types extracted from query analysis. Ver-
tical attributes, such as city-state relationships,
metropolitan definition, or idf scores from a do-
main specific corpus, are extracted for each con-
cept type from vertical database. The vertical at-
tributes, together with the concept attributes, are
used to compose a set of semantic features for ma-
chine learning based IR models. A few machine
learning techniques are discussed to further im-
prove relevance for subclass of difficult queries
such as queries containing multiple types of con-
cepts. Figure 1 shows an overview of our ap-
proach; after discussing related work in Section 2,
we spend Sections 3 to 5 of the paper describing
the components of our system. We then evaluate
the effectiveness of our approach both using gen-
eral queries and with a set of ?difficult? queries;
our results show that the techniques are robust, and
particularly effective for this type of queries. We
conclude in Section 7.
Tagger1 Tagger2 Tagger n
Resolution Module
Query
with
Annotations
Query
Query
Linguistic
Analysis
Vertical AttributeLocation 
DB
Vertical AttributeBusiness
DB
Vertical Attribute
Semantic Text Matching
Document 
Index
Semantic Features
Specialized
Ranking
Module
Specialized
RankingModule
Specialized
RankingModule
...
DB
Name
...
......
Figure 1: Ranking with Semantic Features
2 Related Work
There is substantial body of work involving us-
age of NLP techniques to improve information re-
trieval (Brants, 2003; Strzalkowski et al, 1996).
Allan and Ragahavan (Allan and Raghavan, 2002)
use Part-of-Speech tagging to reduce ambiguity
of difficult queries by converting short queries
to questions. In other POS-tags work, Aram-
patzis et al (Arampatzis et al, 1990) observed
an improvement when using nouns only for re-
trieval. Croft et al (Croft et al, 1991) and Tong
et al (Buckley et al, 1993; Tong et al, 1996) ex-
plored phrases and structured queries and found
phrases are effective in improving retrieval per-
formance. Voorhees (Voohees, 1993) uses word
sense disambiguation to improve retrieval perfor-
mance. One IR domain that consistently benefits
from usage of various NLP techniques is question
answering, where queries are formed in natural
language format; e.g., (Peng et al, 2005).
In general, however, researchers often observe
limited gains or even degraded performance when
applying NLP to IR (Voorhees, 1999). Having
said this, most past studies use small datasets and
a modest baseline; it is unclear whether a similar
conclusion would be reached when using a state-
of-art system such as a commercial web search
engine as a baseline, and a full-web corpus ? as
we do in this paper. This leads to another differ-
ence between this work and existing work involv-
ing named entity recognition for retrieval. Most
649
previous research on usage of named entities in
IR combines entity detection in documents and
queries (Prager et al, 2000). Entity detection in
document has a high indexing cost that is often
overlooked, but cannot be ignored in the case of
commercial search engines. For this reason, we
restrict NLP processing to queries only ? although
we believe that document-side NLP processing
will provide additional useful information.
3 Query Analysis
We begin by briefly describing our approach to
named entity recognition in web queries, which
serves as the basis for deriving the semantic text
matching features.
Named entity recognition (NER) is the task of
identifying and classifying entities, such as per-
son names or locations, in text. The majority of
state-of-the-art NER methods utilize a statistical
approach, attempting to learn a mapping between
a sequence of observations (words) and a sequence
of tags (entity types). In these methods, the se-
quential nature of the data is often central to the
model, as named entities tend to appear in particu-
lar context in text. For example, for most types of
text, in the two sequences met with X and buy the
Y, the likelihood of X being a person name is sub-
stantially higher than the corresponding likelihood
of Y . Indeed, many named entity taggers perform
well when applied to grammatical text with suf-
ficient contexts, such as newswire text (Sang and
Meulder, 2003).
Web queries, however, tend to be short, with
most queries consisting of 1?3 words, and lack
context ? posing a particular challenge for iden-
tifying named entities in them. Existing work on
NER in web queries focuses on tailoring a solu-
tion for a particular entity type and its usage in
web search (Wang et al, 2005; Shen et al, 2008);
in contrast, we aim at identifying a large range
of possible entities in web queries, and using a
generic solution for all of them.
In web queries, different entity types may bene-
fit from different detection techniques. For exam-
ple, an entity type with a large variability among
instances as well as existence of external resources
like product name calls for an approach that can
make use of many features, such as a conditional
random field; for entity types that are more struc-
tured like person names, a grammar-based ap-
proach can be more effective (Shen et al, 2008).
To this end, we utilize multiple approaches for en-
tity detection and combine them into a single, co-
herent ?interpretation? of the query.
Given a query, we use several entity recogniz-
ers in parallel, one for each of the common en-
tity types found in web queries. The modeling
types may differ between the recognizers: some
are Markovian models, while others are just dic-
tionary lookups; the accuracy of each recognizer
is also different. We then have a machine-learned
disambiguation module that combines output from
different taggers, ranking the tagging sequences.
The details of scoring is out of the scope of this
paper, and we omit it for simplicity.
4 Semantic Text Matching Features
Our proposed semantic features operate at the
semantic type level rather than at the term level:
instead of matching a term (or set of terms) in doc-
uments, we match their semantic type. Given the
query San Francisco colleges and the annotation
[San Francisco]
CityName
[colleges]
BusinessCategory
,
the semantic text matching features would de-
scribe how relevant a document section is for a en-
tity of type CityName, for BusinessCategory,
and for their combination.
Concretely, we exploit a set of features that
attempts to capture proximity, general relevance,
and vertical relevance for each type of semantic
tag and for each section of the document. We now
review these feature by their broad types.
4.1 Semantic Proximity Features
Proximity features?features that capture the de-
gree to which search terms appear close to each
other in a document?are among the most impor-
tant feature sets in ranking functions. Traditional
proximity features are typically designed for all
query terms (Metzler and Croft, 2005) and may
suffer from wrong segmentations of the query. For
example, for the query New York city bus char-
ter, a traditional proximity feature may treat ?city
bus? similarly to ?York city.? But given detailed
information about the entities in the query in their
types, we can enforce proximity for ?New York
city? and ?bus charter? more accurately. Different
types of entities usually have different proximity
characteristics in relevant documents. Strongly-
bound entities such as city names typically have
very high proximity in relevant documents, while
entities such as business names may have much
650
lower proximity: a search for Kaiser medical of-
fice, for example, may be well-served with docu-
ments referring to Kaiser Permanente medical of-
fice, and as we mentioned before, person names
matches may also benefit from lenient proximity
enforcement. This is naturally addressed by treat-
ing each entity type differently.
We propose a set of semantic proximity fea-
tures that associate each semantic tag type with
generic proximity measures. We also consider tag-
ging confidence together with term group proxim-
ity; we discuss these two approaches next.
4.1.1 Semantic Minimum Coverage (SMC)
Minimum Coverage (MC) is a popular span based
proximity distance measure, which is defined as
the length of the shortest document segment that
cover the query term at least once in a docu-
ment (Tao and Zhai, 2007). We extend this mea-
sure to Semantic Minimum Coverage (SMC) for
each semantic type t in document section s and
define it as
SMC
t,s
=
1
|{k|T
k
= t}|
?
i?{k|T
k
=t}
w
i
MC
i,s
,
where w
i
is a weight for tagged term group i,
MC
i,s
is the the minimum coverage of term group
i in document section s, {k|T
k
= t} denotes the
set of all concepts having type t, and |{k|T
k
= t}|
is the size of the set. The definition of the weight
w is flexible. We list a few candidate weight-
ing schemes in this paper: uniform weights (wu),
weights based on idf scores (widf) and ?strength?-
based weight (ws), which we define as follows:
w
u
= 1;
w
idf
=
c
f
q
where c is a constant and f
q
is the frequency of the
term group in a large query log;
w
s
= min
l
MI
l
where MI
l
is the point-wise mutual information of
the l-th consecutive pair within the semantic tag.
We can also combine strength and idf scores such
that the weight reflects both relative importance
and constraints in proximity. In this paper, we use
w
si
= w
s
w
idf
.
In Section 6, we use all four weighting schemes
mentioned above in the semantic feature set.
4.1.2 Semantic Moving Average BM25
(SMABM25)
BM25, a commonly-used bag-of-words relevance
estimation method (Robertson et al, 1995), is de-
fined (when applied to document sections) as
BM25 =
?
j
idf
j
f
j,s
(c
1
+ 1)
f
i,s
+ c
1
(1? c
2
+ c
2
l
s
?
l
s
)
where f
j,s
is the frequency of term j in section s,
l
s
is the length of section s, ?l
s
is the average length
of document section s, c
1
, c
2
, c
3
are constants and
the idf score of term j is defined as
idf
j
= log
c
4
? d
j
+ c
5
d
j
+ c
5
,
where d
j
is the number of sections in all collec-
tions that contains term j and c
4
, c
5
are constants.
To characterize proximity, we could use a fixed
length sliding window and calculate the average
BM25. We further associate each sliding average
BM25 with each type of semantic term groups.
This results in a Semantic Moving Average BM25
(SMABM25) of type t, which we define as fol-
lows:
1
|{k|T
k
= t}|
?
i?{k|T
k
=t}
(1/M)
?
m
BM25
m
where m is a fixed length sliding window m and
M is the total number of sliding windows (that de-
pends on the length of the section window size).
4.2 Semantic Vertical Relevance Features
Vertical databases contain a large amount of struc-
tured domain knowledge typically discarded by
traditional web relevance features. Having access
to the semantic types in queries, we can tap into
that knowledge to improve accuracy. For exam-
ple, term frequencies in different corpora can as-
sist in determining relevance given an entity type.
As we mentioned in Section 1, we observe that
term frequency in a database of business names
provides an indication of the business brand, the
key part of the business name phrase. While both
?yahoo? and ?inc? are very common terms on the
web, in a database of businesses only ?inc? is com-
mon enough to be considered a stopword in the
context of business names.
We propose a Vertical Moving Average BM25
(VMABM25) as a feature aiming at quantifying
the vertical knowledge for web search. The ba-
sic idea here is to replace the idf score idf
j
of
651
SMABM25 with an idf score calculated from a
vertical database for type t, namely idft
j
:
1
|{k|T
k
= t}|
?
i?{k|T
k
=t}
(1/M)
?
m
BM25
m,t
where
BM25
m,t
=
?
j
idft
j
f
j,s
(c1 + 1)
f
i,s
+ c
1
(1? c
2
+ c
2
l
s
?
l
s
)
where the idft
j
is associated with the semantic type
t and calculated from the corpus associated with
that type.
VMABM25 links vertical knowledge, proxim-
ity, and page relevance together; we show later that
it is one of most salient features among all seman-
tic features.
4.3 Generalized Semantic Features
Finally, we develop a generalized feature based on
the previous features by removing tags. Semantic
features are often sparse, as many queries contain
one entity or no entities at all; generalized features
increase their coverage by combining the basic se-
mantic features. An entity without tag is essen-
tially a segment.
A segment feature x
i
for query i does not have
entity type and can be expressed as
x
i
=
1
K
i
K
i
?
k=1
x
T (k)
where K
i
is the number of segments in the query
and T (k) is the semantic type associated with kth
concept.
Although these features are less informative
than type-specific features, one advantage of using
them is that they have substantially higher cover-
age. In our experiments, more than 40% of the
queries have some identified entity. Another rel-
atively subtle advantage is that segment features
have no type related errors: the only possible error
is a mistake in entity boundaries.
5 Ranking Function Optimization
The ultimate goal of the machine learning ap-
proach to web search is to learn a ranking func-
tion h(x
i
), where x
i
is a feature vector of a query-
document pair i, such that the error
L(h) ?
N
?
i=1
(y
i
? h(x
i
))
2 (1)
is minimized. Here, y
i
is the actual relevance score
for the query-document pair i (typically assigned
by a human) and N is the number of training sam-
ples.
As mentioned in the previous Section, an inher-
ent issue with semantic features is their sparse-
ness. User queries are usually short, with an av-
erage length of less than 3 words. Text matching
features that are associated with the semantic type
of query term or term groups are clearly sparse
comparing with traditional, non-entity text match-
ing features ? that can be derived for any query.
When a feature is very sparse, it is unlikely that
it would play a very meaningful role in a machine
learned ranking function, since the error L would
largely depend on other samples that do not con-
tain the specific semantic features at all. To over-
come the spareness issue and take advantage of
semantic features, we suggested generalizing our
features; but we also exploit a few ranking func-
tion modeling techniques.
First, we use a ?divide-and-conquer? approach.
Long queries usually contain multiple concepts
and could be difficult to retrieve relevant docu-
ments. Semantic features, however, are rich in
this set of queries. We may train special models
to further optimize our ranking function for those
queries. The loss function over ranking function h
becomes
L
C
(h) ?
?
i?C
(y
i
? h(x
i
))
2 (2)
where C is the training set that falls into a pre-
defined subclass. For example, queries containing
both location and business name, queries contains
both location and business category, etc, are good
candidates to apply semantic features.
To this end, we first classify queries into several
classes, each of which has multiple types of enti-
ties. The semantic features of those types would
be dense for this subclass of queries. We then
train models that may rank the specific class of
queries well. This approach, however, may suf-
fer from significantly less training samples due to
training data partition resulted from the query clas-
sification. Increasing the modeling accuracy, then,
comes at a cost of reduced data available for train-
ing. We apply two techniques to address this is-
sue. The first approach is to over-weight subclass
training samples such that the subclass of queries
plays a more important role in modeling while still
652
keeping a large pool of the overall training sam-
ples. The second approach is model adaptation:
a generalized incremental learning method. Here,
instead of being over-weighted in a joint optimiza-
tion, the subclass of training data is used to mod-
ify an existing model such that the new model is
?adapted? to the subclass problem. We elaborate
on our approaches as follows.
5.1 Weighted Training Samples
To take advantage of both large a pool of training
samples and sparse related semantic features for
a subclass of queries, we could modify the loss
function as follows
L
w
C
(h) ? w
?
i?C
(y
i
? h(x
i
))
2
+
?
i?
?
C
(y
i
? h(x
i
))
2
,
(3)
where ?C is the complement of set C. Here, the
weight w is a compromise between loss function
(1) and (2). When w = 1, we have
L
1
C
(h) ? L(h);
when w? > ?
L
?
C
(h) ? L
C
(h).
A large weight may help optimize the training for
a special subclass of queries, and a small weight
may help to preserve good generality of the ranker.
We could use cross-validation to select the weight
w to optimize a the ranking function for a sub-
class of queries. In practice, a small w is desired
to avoid overfitting.
5.2 Model Adaptation
Model adaptation is an emerging machine learn-
ing technique that is used for information retrieval
applications with limited amount of training data.
In this paper, we apply Trada, proposed by Chen
et al (Chen et al, 2008), as our adaptation algo-
rithm.
The Trada algorithm aims at adapting tree-
based models. A popular tree based regression ap-
proach is Gradient Boosting Trees (GBT) , which
is an additive model h(x) =
?
K
k=1
?
k
h
k
(x),
where each regression tree h
k
is sequentially op-
timized with a hill-climbing procedure. As with
other decision trees, a binary regression tree h
k
(x)
consists of a set of decision nodes; each node is
associated with a feature variable and a splitting
value that partition the data into two parts, with the
corresponding predicted value defined in the leave
node. The basic idea of Trada is to apply piece-
wise linear transformation to the base model based
on the new training data. A set of linear transfor-
mations are applied to each decision node, either
predict or split point or both, such that the new pre-
dict or the split point of a node in a decision tree
satisfies
v = (1? p
C
)v? + p
C
v
C
where v? denotes predict or split point of that node
in the base mode and v
C
denotes predict or split
point of that node using new data set C, and
the weight p
C
depends on the number of origi-
nal training data and new training data that fall
through the node. For each node, the split or pre-
dict can be estimated by
p
C
=
?n
C
n+ ?n
C
,
where n is the number of training sample of the
base model that fall through the node, n
C
is the
number of new training sample that fall through
the node, and ? is a parameter that can be deter-
mined using cross validation. The parameter ?
is used to over-weight new training data, an ap-
proach that is very effective in practice. For new
features that are not included in the base model,
more trees are allowed to be added to incorporate
them.
6 Experiments
We now measure the effectiveness of our proposal,
and answer related questions, through extensive
experimental evaluation. We begin by examining
the effectiveness of features as well as the model-
ing approaches introduced in Section 5 on a par-
ticular class of queries?those with a local intent.
We proceed by evaluating whether if the type asso-
ciated with each entity really matters by compar-
ing results with type dependent semantic features
and segment features. Finally, we examine the ro-
bustness of our features by measuring the change
in the accuracy of our resulting ranking function
when the query analysis is wrong; we do this by
introducing simulated noise into the query analy-
sis results.
6.1 Dataset
Our training, validation and test sets are human-
labeled query-document pairs. Each item in the
653
sets consists of a feature vector x
i
represent-
ing the query and the document, and a judg-
ment score y
i
assigned by a human. There are
around 600 features in each vector, including both
the newly introduced semantic features and exist-
ing features; features are either query-dependent
ones, document-dependent ones, or query-and-
document-dependent features.
The training set is based on uniformly sampled
Web queries from our query log, and top ranked
documents returned by commercial search engines
for these queries; this set consists of 1.24M query-
document pairs.
We use two additional sets for validation and
testing. One set is based on uniformly sampled
Web queries, and contains 42790 validation sam-
ples and 70320 test samples. The second set is
based on uniformly sampled local queries. By lo-
cal queries, we mean queries that contain at least
two types of semantic tags: a location tag (such
as street, city or state name) and a business tag (a
business name or business category). We refer to
this class of queries ?local queries,? as users often
type this kind of queries in local vertical search.
The local query set consists of 11040 validation
samples and 39169 test samples. In the training
set we described above, there are 56299 training
samples out of the 1.24M total number of training
samples that satisfy the definition of local queries.
We call this set local training subset.
6.2 Evaluation Metrics
To evaluate the effectiveness of our semantic
features we use Discounted Cumulative Gain
(DCG) (Jarvelin and Kekalainen, 2000), a widely-
used metric for measuring Web search relevance.
(Jarvelin and Kekalainen, 2000). Given a query
and a ranked list of K documents (K = 5 in our
experiments), the DCG for this query is defined as
DCG(K) =
K
?
i=1
y
i
log
2
(1 + i)
. (4)
where y
i
? [0, 10] is a relevance score for the
document at position i, typically assigned by a hu-
man, where 10 is assigned to the most relevant
documents and 0 to the least relevant ones.
To measure statistical significance, we use the
Wilcoxon test (Wilcoxon, 1945); when the p-value
is below 0.01 we consider a difference to be statis-
tically significant and mark it with a bold font in
the result table.
6.3 Experimental Results
We use Stochastic Gradient Boosting Trees
(SGBT) (Friedman, 2002), a robust none linear
regression algorithm, for training ranking func-
tions and, as mentioned earlier, Trada (Chen et al,
2008) for model adaptation.
Training parameters are selected to optimize the
relevance on a separated validation set. The best
resulting is evaluated against the test set; all results
presented here use the test set for evaluation.
6.3.1 Feature Effectiveness with Ranking
Function Modeling
We apply the modeling approaches introduced in
Section 5 to improve feature effectiveness on ?dif-
ficult? queries?those more than one entity type;
we evaluate these approaches with the semantic-
feature-rich set, the local query test set. We split
training sets into two parts: one set belongs to the
local queries, the other is the rest. We first weight
the local queries and use the combined dataset as
training data to learn the ranking functions; we
train functions with and without the semantic fea-
tures. We evaluate these functions against the lo-
cal query test set. The results are summarized in
Table 1, where w denotes the weight assigned to
the local training set, bolded numbers are statis-
tically significant result compared to the baseline,
uniformly weighted training data without seman-
tic features (with superscript b). It is interesting
to observe that without semantic features, over-
weighted local training data does not have statis-
tically significant impact on the test performance;
with semantic features, a proper weight over train-
ing samples does improve test performance sub-
stantially.
Table 1: Evaluation of Ranking Models Trained
Against Over-weighted Local Queries with Se-
mantic Features on the Local Query Test Set
w/o semantic features w/ semantic features
Weight DCG(5) Impr. DCG(5) Impr.
w = 0 8.09
b
- 8.25 2.0%
w = 2 8.09 0.02% 8.26 2.1%
w = 4 8.13 0.49% 8.34 3.1%
w = 8 8.13 0.49% 8.42 4.1%
w = 16 8.13 0.49% 8.30 2.6%
w = 32 8.04 ?0.60% 8.27 2.2%
Next, we use the local query training set as ?new
data? in the tree adaptation approach. In tree adap-
tations, all parameters are set to optimize the per-
formance over the local validation set. We com-
654
pare two major adaptation approaches proposed
in (Chen et al, 2008): adapting predict only and
adapting both predict and split. We use the model
trained with the combined training and uniform
weights as the baseline; results are summarized in
Table 2.
Table 2: Trada Algorithms with Semantic Features
on Local Query Test Set
w/o semantic feat. w/ semantic feat.
Ada. Appr. DCG(5) Impr. DCG(5) Impr.
Combined data 8.09b - 8.25 2.0%
Ada. predict 8.02 ?0.1% 8.14 0.6%
Ada. predict 8.00 ?0.1% 8.17 1.0%
& split
Comparing Tables 1 and 2, note that using the
combined training data with local query training
samples over-weighted achieves better results than
tree adaption. The latter approach, however, has
the advantage of far less training time, since the
adaptation is over a much smaller local query
training set. With the same hardware, it takes
just a few minutes to train an adaptation model,
while it takes days to train a model over the entire,
combined training data. Considering that massive
model validation tasks are required to select good
training parameters, training many different mod-
els with over a million training samples becomes
prohibitly costly. Applying tree adaptation tech-
niques makes research and prototyping of these
models feasible.
6.3.2 Type Dependent Semantic Features vs.
Segment Features
Our next experiment compares type-dependent
features and segment features, evaluating models
trained with these features against the local query
test set. No special modeling approach is applied
here; results are summarized in Table 3. We ob-
serve that by using type-dependent semantic fea-
tures only, we can achieve as much as by using
all semantic features. Since segment features only
convey proximity information while the base fea-
ture set aleady contain a systematic set of prox-
imity measures, the improvement through segment
features is not as significant as the the type depen-
dent ones.
6.3.3 Robustness of Semantic Features
Our final set of experiments aims at evaluating the
robustness of our semantic features by introducing
Table 3: Type-dependent Semantic Features vs.
Segment Features
Feature set DCG(5)
base + type dependent semantic features 8.23
base + segment features 8.19
base + all semantic features 8.25
simulated errors to the output of our query analy-
sis. Concretely, we manipulate the precision and
the recall of a specific type of entity tagger, t, on
the training and test set. To decrease the recall of
type t, we uniformly remove a set of a% tags of
type t ? preserving precision. To decrease preci-
sion, we uniformly select a set of query segments
(viewing the entity detection as simple segmenta-
tion, as detailed earlier) and assign the semantic
type t to those segments. Since the newly added
term group are selected from query segmentation
results, the introduced errors are rather semantic
type error than boundary error or proximity error.
The total number of newly assigned type t tags are
b% of the original number of type t tags in the
training set. By doing this, we decrease the preci-
sion of type t while keeping the recall of it at the
same level.
Suppose the original tagger achieves precision
p and recall r. By removing a% of tags, we have
estimated precision p? and recall r? defined as fol-
lows:
r? =
100r ? ar
100
,
p? = p.
By adding b% more term group to this type, we
have estimated precision and recall as
p? =
100p
100 + bp
,
r? = r.
In the experiment reported here we use BUSI-
NESS NAME as the target semantic type for this ro-
bustness experiment. An editorial test shows that
our tagger achieves 74% precision and 66% recall
based on a random set of human labeled queries
for this entity type. We train ranking models with
various values of a and b. When we reduce the
estimated recall, we evaluate these models against
the local test set since other data are not affected.
The results are summarized in Table 4.
When we reduce the precision, we evaluate the
resulting models against the general test set as
655
Table 4: Relevance with simulated error on local
query test set
a b p? r? DCG(5) Impr.
0 0 0.74 0.66 8.25 ?
10 0 0.74 0.594 8.21 0.48%
20 0 0.74 0.528 8.19 0.72%
40 0 0.74 0.396 8.18 0.85%
Table 5: Search relevance with simulated error for
semantic features on general test set
a b p? r? DCG(5) Impr.
0 0 0.74 0.66 10.11 -
0 10 0.689 0.66 10.11 0.00%
0 20 0.645 0.66 10.12 0.10%
0 40 0.571 0.66 10.12 0.10%
0 60 0.513 0.66 10.12 0.10%
0 80 0.465 0.66 10.11 0.00%
0 100 0.425 0.66 10.10 ?0.10%
simulated errors would virtually affect any sam-
ples with certain probability. Results appear in
Table 5. The results are quite interesting: when
the recall of business name entity decreases, we
observe statistically significant relevance degrada-
tion: if less entities are discovered, search rele-
vance is hit. The experiments with simulated pre-
cision error, however, are less conclusive. One
may note the experiments are conducted over the
general test set. Therefore, it is not clear if the pre-
cision of the NER system really has insignificant
impact on the IR relevance or just the impact is
diluted in a larger test set.
6.4 Case Analysis
In this section, we take a close look at a few
cases where our new semantic features help
most and where they fail. For the query sil-
verado ranch in irving texas, with no semantic
features, the ranking function ranks a local
listing page for this business, http://local.
yahoo.com/info-28646193, as the top
document. With semantic features, the ranking
function ranks the business home page: http:
//www.silveradoranchparties.com/
as top URL. Examining the two documents, the
local listing page actually contains much more rel-
evant anchor text, which are the among the most
salient features in traditional ranking models. The
home page, however, contains almost no relevant
anchor text: for a small business home page, this
is not a rare situation. Looking at the semantic
features of these two pages, the highest resolution
of location, the city name ?Irving,? appears in the
document body text 19 times in the local listing
page body text, and only 2 times in the home page
body text. The training process learns, then, that
for a query for a local business name (rather than
a business category), home pages?even with
fewer location terms in them?are likely to be
more relevant than a local listing page that usually
contain high frequency location terms.
In some cases, however, our new features do
hurt performance. For the query pa treasur-
ers office, the ranking function with no seman-
tic features ranks the document http://www.
patreasury.org highest, while the one with
semantic features ranks the page http://www.
pikepa.org/treasurer.htm higher. The
latter page is somewhat relevant: it is a treasurer?s
office in Pennsylvania. However, it belongs to a
specific county, which makes it less relevant than
the former page. This is a classic error that we ob-
serve: a mismatch of the intended location area.
While users are looking for state level business,
we provide results of county level. To resolve
this type of error, query analysis and semantic text
matching are no longer enough: here, the rank-
ing function needs to know that Pike County is a
county in Pennsylvania, Milford is a city in Pike
County, and neither are referred to by the user.
Document-side entity recognition, however, may
provide this type of information, helping to ad-
dress this type of errors.
7 Conclusion and Future Research
In this paper, we investigate how semantic features
can improve search relevance in a large-scale in-
formation retrieval setting; to our knowledge, it is
the first study of this approach on a web scale. We
present a set of features that incorporate semantic
and vertical knowledge into the retrieval process,
propose techniques to handle the sparseness prob-
lem for these features, and describe how they fit
in the learning process. We demonstrate that these
carefully designed features significantly improve
relevance, particularly for difficult queries ? long
queries with multiple entities.
The work reported here focuses on query-side
processing, avoiding the indexing cost of docu-
ment processing. We are currently investigating
document-side analysis to complement the query-
side work, and believe that this will further boost
the retrieval accuracy; we hope to report on this in
a follow-up study.
656
References
J. Allan and H. Raghavan. 2002. Using Part-of-Speech
Patterns to Reduce Query Ambiguity. In Proceed-
ings of SIGIR.
A. T. Arampatzis, Th. P. Weide, C. H. A. Koster,
and P. Bommel. 1990. Text Filtering using
Linguistically-motivated Indexing Terms. Techni-
cal Report CSI-R9901, Computing Science Institute,
University of Nijmegen, Nijmegen,The Netherlands.
Thorsten Brants. 2003. Natural Language Processing
in Information Retrieval. In Proceedings of CLIN.
C. Buckley, J. Allan, and G. Salton. 1993. Auto-
matic Routing and Ad-hoc Retrieval Using SMART:
TREC 2. In Proceedings of TREC-2.
Keke Chen, Rongqing Lu, C.K. Wong, Gordon Sun,
Larry Heck, and Belle Tseng. 2008. Trada: tree
based ranking function adaptation. In Proceedings
of CIKM.
W.B. Croft and D.J. Harper. 1979. Using Probabilistic
Models of Document Retrieval without Relevance
Information. Journal of Documentation, 37:285?
295.
W. Bruce Croft, Howard R. Turtle, and David D. Lewis.
1991. The Use of Phrases and Structured queries in
Information Retrieval. In Proceedings of SIGIR.
J. H. Friedman. 2002. Stochastic Gradient Boost-
ing. Computational Statistics and Data Analysis,
38(4):367?378.
N. Fuhr. 1992. Probabilistic Models in Information
Retrieval. The Computer Journal, 35:243?255.
K. Jarvelin and J. Kekalainen. 2000. IR Evalua-
tion Methods for Retrieving Highly Relevant Doc-
uments. In Proceedings of SIGIR.
J. Lafferty and C. Zhai. 2001. Document Language
Models, Query Models and Risk Minimization for
Information Retrieval. In Proceedings of SIGIR.
Donald Metzler and W. Bruce Croft. 2005. A markov
random field model for term dependencies. In Pro-
ceedings of SIGIR.
M. Narita and Y. Ogawa. 2000. The Use of Phrases
from Query Texts in Information Retrieval. In Pro-
ceedings of SIGIR.
Fuchun Peng, Ralph Weischedel, Ana Licuanan, and
Jinxi Xu. 2005. Combining Deep Linguistics
Analysis and Surface Pattern Learning: A Hybrid
Approach to Chinese Definitional Question Answer-
ing. In In Proceedings of the HLT-EMNLP.
J. Ponte and W.B. Croft. 2000. A Language Modeling
Approach to Informaiton Retrieval. In Proceedings
of SIGIR.
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by Predictive
Annotation. In Proceedings of SIGIR.
Stephen Robertson, Steve Walker, Susan Jones, Miche-
line Hancock-Beaulieu, and Mike Gatford. 1995.
Okapi at TREC-3. In Proceedings of TREC-3.
G. Salton, C. S. Yang, and C. T. Yu. 1975. A Theory of
Term Importance in Automatic Text Analysis. Jour-
nal of the Ameican Society of Information Science,
26:33?44.
Erik Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the CoNLL-2003 Shared Task:
Language-Independent Named Entity Recognition.
In Proceedings of CoNLL.
Dou Shen, Toby Walkery, Zijian Zheng, Qiang Yang,
and Ying Li. 2008. Personal Name Classification in
Web Queries. In Proceedings of WSDM.
A.F. Smeaton and C.J. van Rijsbergen. 1988. Experi-
ments on Incorporating Syntactic Processing of User
Queries into a Document Retrieval Stragegy. In Pro-
ceedings of SIGIR.
K. Sparck-Jones, 1999. What is the Role of NLP in Text
Retrieval, pages 1?25. Kluwer.
M. Srikanth and R.K. Srihari. 2003. Incorporating
Query Term Dependencies in Language Models for
Document Retrieval. In Proceedings of SIGIR.
Tomek Strzalkowski, Jose Perez-Carballo, Jussi Karl-
gren, Anette Hulth, Pasi Tapanainen, and Timo
Lahtinen. 1996. Natural Language Information Re-
trieval: TREC-8 Report. In Proceedings of TREC-8.
Tao Tao and ChengXiang Zhai. 2007. An exploration
of proximity measures in information retrieval. In
Proceedings of SIGIR.
X. Tong, C. Zhai, N. Millic-Frayling, and D Evans.
1996. Evaluation of Syntactic Phrase Indexing ?
CLARIT NLP Track Report. In Proceedings of
TREC-5.
E. Voohees. 1993. Using WordNet to Disambiguate
Word Senses for Text Retrieval. In Proceedings of
SIGIR.
Ellen Voorhees. 1999. Natural Language Processing
and Information Retrieval. Lecture Notes in Com-
puter Science, 1714:32?48.
Lee Wang, Chuang Wang, Xing Xie, Josh Forman,
Yansheng Lu, Wei-Ying Ma, and Ying Li. 2005.
Detecting dominant locations from search queries.
In Proceedings of SIGIR.
F. Wilcoxon. 1945. Individual Comparisons by Rank-
ing Methods. Biometrics, 1:80?83.
657
Why Are They Excited?
Identifying and Explaining Spikes in Blog Mood Levels
Krisztian Balog Gilad Mishne Maarten de Rijke
ISLA, University of Amsterdam
Kruislaan 403, 1098 SJ Amsterdam
kbalog,gilad,mdr@science.uva.nl
Abstract
We describe a method for discovering ir-
regularities in temporal mood patterns ap-
pearing in a large corpus of blog posts,
and labeling them with a natural language
explanation. Simple techniques based
on comparing corpus frequencies, coupled
with large quantities of data, are shown to
be effective for identifying the events un-
derlying changes in global moods.
1 Introduction
Blogs, diary-like web pages containing highly
opinionated personal commentary, are becoming
increasingly popular. This new type of media of-
fers a unique look into people?s reactions and feel-
ings towards current events, for a number of rea-
sons. First, blogs are frequently updated, and like
other forms of diaries are typically closely linked
to ongoing events in the blogger?s life. Second, the
blog contents tend to be unmoderated and subjec-
tive, more so than mainstream media?expressing
opinions, thoughts, and feeling. Finally, the large
amount of blogs enables aggregation of thousands
of opinions expressed every minute; this aggrega-
tion allows abstractions of the data, cleaning out
noise and focusing on the main issues.
Many blog authoring environments allow blog-
gers to tag their entries with highly individual (and
personal) features. Users of LiveJournal, one of
the largest weblog communities, have the option
of reporting their mood at the time of the post;
users can either select a mood from a predefined
list of common moods such as ?amused? or ?an-
gry,? or enter free-text. A large percentage of Live-
Journal users tag their postings with a mood. This
results in a stream of hundreds of weblog posts
tagged with mood information per minute, from
hundreds of thousands of users across the globe.
The collection of such mood reports from many
bloggers gives an aggregate mood of the blogo-
sphere for each point in time: the popularity of
different moods among bloggers at that time.
In previous work, we introduced a tool for
tracking the aggregate mood of the blogosphere,
and showed how it reflects global events (Mishne
and de Rijke, 2006a). The tool?s output includes
graphs showing the popularity of moods in blog
posts during a given interval; e.g., Figure 1 plots
the mood level for ?scared? during a 10 day pe-
riod. While such graphs reflect some expected
patterns (e.g., an increase in ?scared? around Hal-
loween in Figure 1), we have also witnessed spikes
and drops for which no associated event was
Figure 1: Blog posts labeled ?scared? during the October 26?
November 5, 2005 interval. The dotted (black) curve indi-
cates the absolute number of posts labeled ?scared,? while
the solid (red) curve shows the rate of change.
known to us. In this paper, we address this is-
sue: we seek algorithms for identifying unusual
changes in mood levels and explaining the under-
lying reasons for these changes. By ?explanation?
we mean a short snippet of text that describes the
event that caused the unusual mood change.
To produce such explanations, we proceed as
follows. If unusual spikes occur in the level of
mood m, we examine the language used in blog
posts labeled with m around and during the pe-
riod in which the spike occurs. We interpret words
207
that are not expected given a long-term language
model for m as signals for the spike in m?s level.
To operationalize the idea of ?unexpected words?
for a given mood, we use standard methods for
corpus comparison; once identified, we use the
?unexpected words? to consult a news corpus from
which we retrieve a small text snippet that we then
return as the desired explanation.
In Section 2 we briefly discuss related work.
Then, we detail how we detect spikes in mood lev-
els (in Section 3) and how we generate natural lan-
guage explanations for such spikes (in Section 4).
Experimental results are presented in Section 5,
and in Section 6 we present our conclusions.
2 Related work
As to burstiness phenomena in web data, Klein-
berg (2002) targets email and research papers, try-
ing to identify sharp rises in word frequencies in
document streams. Bursts can be found by search-
ing periods when a given word tends to appear at
unusually short intervals. Kumar et al (2003) ex-
tend Kleinberg?s algorithm to discover dense pe-
riods of ?bursty? intra-community link creation in
the blogspace, while Nanno et al (2004) extend it
to work on blogs. We use a simple comparison be-
tween long-term and short-term language models
associated with a given mood to identify unusual
word usage patterns.
Recent years have witnessed an increase in re-
search on extracting subjective and other non-
factual aspects of textual content; see (Shanahan et
al., 2005) for an overview. Much work in this area
focuses on recognizing and/or annotating evalu-
ative textual expressions. In contrast, work that
explores mood annotations is relatively scarce.
Mishne (2005) reports on text mining experiments
aimed at automatically tagging blog posts with
moods. Mishne and de Rijke (2006a) lift this work
to the aggregate level, and use natural language
processing and machine learning to estimate ag-
gregate mood levels from the text of blog entries.
3 Detecting spikes
Our first task is to identify spikes in moods re-
ported in blog posts. Many of the moods reported
by LiveJournal users display a cyclic behavior.
There are some obvious moods with a daily cycle.
For instance, people feel awake in the mornings
and tired in the evening (Figure 2). Other moods
show a weekly cycle. For instance, people drink
more at the weekends (Figure 3).
Figure 2: Daily cycles for ?awake? and ?tired.?
Figure 3: Weekend cycles for ?drunk.?
Our idea of detecting spikes tries to deal with
these cyclic events and aims at finding global
changes. Let POSTS (mood, date, hour) be the
number of posts labelled with a given mood and
created within a one-hour interval at the speci-
fied date. Similarly, ALLPOSTS (date, hour) is
the number of all posts created within the interval
specified by the date and hour. The ratio of posts
labeled with a given mood to all posts could be
expressed for all days of a week (Sunday, . . . , Sat-
urday) and for all one-hour intervals (0, . . . , 23)
using the formula:
R(mood, day, hour) =
?
DW (date)=day POSTS (mood, date, hour)
?
DW (date)=day ALLPOSTS (date, hour)
,
where day = 0, . . . , 6 and DW (date) is a day-of-
the-week function that returns 0, . . . , 6 depending
on the date argument.
The level of a given mood is changed within
a one-hour interval of a day, if the ratio of posts
labelled with that mood to all posts, created within
the interval, is significantly different from the ratio
that has been observed on the same hour of the
similar day of the week. Formally:
D(mood, date, hour) =
POSTS(mood,date,hour)
ALLPOSTS(date,hour)
R(mood,DW (date), hour) .
If |D| (the absolute value of D) exceeds a thresh-
old we conclude that a spike has occurred, while
208
the sign of D makes it possible to distinguish be-
tween positive and negative spikes. The absolute
value of D expresses the degree of the peak.
This method of identifying spikes allows us to
look at a period of a few hours instead of only
one, which is an effective smoothing method, es-
pecially if a sufficient number of posts cannot be
observed for a given mood.
4 Explaining peaks
Our next task is to explain the peaks identified by
the methods listed previously. We proceed in two
steps. First, we discover features in the peaking
interval which display a significantly different lan-
guage usage from that found in the general lan-
guage associated with the mood. Then we form
queries using these ?overused? words as well as
the date(s) of the peaking interval and run these as
queries against a news corpus.
4.1 Overused words To discover the reasons
underlying mood changes we use corpus-based
techniques to identify changes in language usage.
We compare two corpora: (1) the full set of blog
posts, referred to as the standard corpus, and (2) a
corpus associated with the peaking interval, re-
ferred to as the sample corpus.
To compare word frequencies across the two
corpora we apply the log-likelihood statistical
test (Dunning, 1993). Let Oi be the observed
frequency of a term, Ni its total frequency, and
Ei = (Ni ?
?
i Oi)/
?
i Ni its expected frequency
in corpus i (where i takes values 1 and 2 for the
standard and sample corpus, respectively). Then,
the log-likelihood value is calculated according to
this formula: ?2 ln? = 2?i Oi ln
(
Oi
Ei
)
.
4.2 Finding explanations Given the start and
end dates of a peaking interval and a list of
overused words from this period, a query is
formed. This query is then submitted to (head-
lines of) a news corpus. A headline is retrieved if
it contains at least one of the overused words and
is dated within the peaking interval or the day be-
fore the beginning of the peak. The hits are ranked
based on the number of overused terms contained
in the headline.
5 Experiments
In this section we illustrate our methods with some
examples and provide a preliminary analysis of
their effectiveness.
5.1 The blog corpus Our corpus consists of
all public blogs published in LiveJournal during
a 90 day period from July 5 to October 2, 2005,
adding up to a total of 19 million blog posts. For
each entry, the text of the post along with the date
and time are indexed. Posts without an explicit
mood indication (10M) are discarded. We applied
standard preprocessing steps (stopword removal,
stemming) to the text of blog posts.
5.2 The news corpus The collection con-
tains around 1000 news headlines that have
been published in Wikinews (http://www.
wikinews.org) during the period of July-
September, 2005.
5.3 Case studies We present three particular
cases where an irregular behavior in a certain
mood could be observed. We examine how accu-
rately the overused terms describe the events that
caused the spikes.
5.3.1 Harry Potter In July, 2005, a peak in
?excited? was discovered; see Figure 4, where the
shaded (green) area indicates the ?peak area.?
Figure 4: Peak in ?excited? around July 16, 2005.
Step 1 of our peak explanation method (Sec-
tion 4) reveals the following overused terms dur-
ing the peak period: ?potter,? ?book,? ?excit,?
?hbp,? ?read,? ?princ,? ?midnight.? Step 2 of
our peak explanation method (Section 4) exploits
these words to retrieve the following headline
from the news collection: ?July 16. Harry Potter
and the Half-Blood Prince released.?
5.3.2 Hurricane Katrina Our next exam-
ple illustrates the need for careful thresholding
when defining peaks (see Section 3). We show
peaks in ?worried? discovered around late Au-
gust, with a 40% and 80% threshold. Clearly, far
more peaks are identified with the lower threshold,
while the peaks identified in the bottom plot (with
the higher threshold), all appear to be clear peaks.
The overused terms during the peak period include
?orlean,? ?worri,? ?hurrican,? ?gas,? ?katrina? In
209
Figure 5: Peaks in ?worried? around August 29, 2005. (Top:
threshold 40% change; bottom: threshold 80% change)
Step 2 of our explanation method we retrieve the
following news headlines (top 5 shown only):
(Sept 1) Hurricane Katrina: Resources regarding
missing/located people
(Sept 2) Crime in New Orleans sharply increases
after Hurricane Katrina
(Sept 1) Fats Dominomissing in the wake of Hur-
ricane Katrina
(Aug 30) At least 55 killed by Hurricane Katrina;
serious flooding across affected region
(Aug 26) Hurricane Katrina strikes Florida, kills
seven
5.3.3 London terror attacks On July 7 a
sharp spike could be observed in the ?sad? mood;
see Figure 6; the tone of the shaded area shows the
degree of the peak. Overused terms identified for
this period include ?london,? ?attack,? ?terrorist,?
?bomb,? ?peopl?, ?explos.? Consulting our news
Figure 6: Peak in ?sad? around July 7, 2005.
corpus produced the following top ranked results:
(July 7) Coordinated terrorist attack hits London
(July 7) British PrimeMinister Tony Blair speaks
about London bombings
(July 7) Bomb scare closes main Edinburgh thor-
oughfare
(July 7) France raises security level to red in re-
sponse to London bombings
(July 6) Tanzania accused of supporting terror-
ism to destabilise Burundi
5.4 Failure analysis Evaluation of the meth-
ods described here is non-trivial. We found that
our peak detection method is effective despite its
simplicity. Anecdotal evidence suggests that our
approach to finding explanations underlying un-
usual spikes and drops in mood levels is effective.
We expect that it will break down, however, in case
the underlying cause is not news related but, for in-
stance, related to celebrations or public holidays;
news sources are unlikely to cover these.
6 Conclusions
We described a method for discovering irregulari-
ties in temporal mood patterns appearing in a large
corpus of blog posts, and labeling them with a
natural language explanation. Our method shows
that simple techniques based on comparing corpus
frequencies, coupled with large quantities of data,
are effective for identifying the events underlying
changes in global moods.
Acknowledgments This research was supported
by the Netherlands Organization for Scientific
Research (NWO) under project numbers 016.-
054.616, 017.001.190, 220-80-001, 264-70-050,
365-20-005, 612.000.106, 612.000.207, 612.013.-
001, 612.066.302, 612.069.006, 640.001.501, and
640.002.501.
References
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Comput. Ling., 19(1):61?74.
J. Kleinberg. 2002. Bursty and hierarchical structure in
streams. In Proc. 8th ACM SIGKDD Intern. Conf. on
Knowledge Discovery and Data Mining, pages 1?25.
R. Kumar, J. Novak, P. Raghavan, and A. Tomkins. 2003. On
the bursty evolution of blogspace. In Proc. 12th Intern.
World Wide Web Conf., pages 568?576.
G. Mishne and M. de Rijke. 2006a. Capturing global mood
levels using blog posts. In AAAI 2006 Spring Symp. on
Computational Approaches to Analysing Weblogs (AAAI-
CAAW 2006). To appear.
G. Mishne and M. de Rijke. 2006b. MoodViews: Tools
for blog mood analysis. In AAAI 2006 Spring Symp. on
Computational Approaches to Analysing Weblogs (AAAI-
CAAW 2006).
G. Mishne. 2005. Experiments with mood classification in
blog posts. In Style2005 ? 1st Workshop on Stylistic Anal-
ysis of Text for Information Access, at SIGIR 2005.
T. Nanno, T. Fujiki, Y. Suzuki, and M. Okumura. 2004. Au-
tomatically collecting, monitoring, and mining Japanese
weblogs. In Proc. 13th International World Wide Web
Conf., pages 320?321.
J.G. Shanahan, Y. Qu, and J. Wiebe, editors. 2005. Comput-
ing Attitude and Affect in Text: Theory and Applications.
Springer.
210
