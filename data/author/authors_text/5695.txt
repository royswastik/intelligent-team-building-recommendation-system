ASSIST: Automated semantic assistance for translators
Serge Sharoff, Bogdan Babych
Centre for Translation Studies
University of Leeds, LS2 9JT UK
{s.sharoff,b.babych}@leeds.ac.uk
Paul Rayson, Olga Mudraya, Scott Piao
UCREL, Computing Department
Lancaster University, LA1 4WA, UK
{p.rayson,o.moudraia,s.piao}@lancs.ac.uk
Abstract
The problem we address in this paper is
that of providing contextual examples of
translation equivalents for words from the
general lexicon using comparable corpora
and semantic annotation that is uniform
for the source and target languages. For
a sentence, phrase or a query expression in
the source language the tool detects the se-
mantic type of the situation in question and
gives examples of similar contexts from
the target language corpus.
1 Introduction
It is widely acknowledged that human transla-
tors can benefit from a wide range of applications
in computational linguistics, including Machine
Translation (Carl and Way, 2003), Translation
Memory (Planas and Furuse, 2000), etc. There
have been recent research on tools detecting trans-
lation equivalents for technical vocabulary in a re-
stricted domain, e.g. (Dagan and Church, 1997;
Bennison and Bowker, 2000). The methodology
in this case is based on extraction of terminology
(both single and multiword units) and alignment
of extracted terms using linguistic and/or statisti-
cal techniques (D?jean et al, 2002).
In this project we concentrate on words from the
general lexicon instead of terminology. The ratio-
nale for this focus is related to the fact that trans-
lation of terms is (should be) stable, while gen-
eral words can vary significantly in their transla-
tion. It is important to populate the terminologi-
cal database with terms that are missed in dictio-
naries or specific to a problem domain. However,
once the translation of a term in a domain has been
identified, stored in a dictionary and learned by
the translator, the process of translation can go on
without consulting a dictionary or a corpus.
In contrast, words from the general lexicon ex-
hibit polysemy, which is reflected differently in
the target language, thus causing the dependency
of their translation on corresponding context. It
also happens quite frequently that such variation
is not captured by dictionaries. Novice translators
tend to rely on dictionaries and use direct trans-
lation equivalents whenever they are available. In
the end they produce translations that look awk-
ward and do not deliver the meaning intended by
the original text.
Parallel corpora consisting of original texts
aligned with their translations offer the possibility
to search for examples of translations in their con-
text. In this respect they provide a useful supple-
ment to decontextualised translation equivalents
listed in dictionaries. However, parallel corpora
are not representative: millions of pages of orig-
inal texts are produced daily by native speakers
in major languages, such as English, while trans-
lations are produced by a small community of
trained translators from a small subset of source
texts. The imbalance between original texts and
translations is also reflected in the size of parallel
corpora, which are simply too small for variations
in translation of moderately frequent words. For
instance, frustrate occurs 631 times in 100 million
words of the BNC, i.e. this gives in average about
6 uses in a typical parallel corpus of one million
words.
2 System design
2.1 The research hypothesis
Our research hypothesis is that translators can be
assisted by software which suggests contextual ex-
139
amples in the target language that are semantically
and syntactically related to a selected example in
the source language. To enable greater coverage
we will exploit comparable rather than parallel
corpora.
Our research hypothesis leads us to a number of
research questions:
? Which semantic and syntactic contextual fea-
tures of the selected example in the source
language are important?
? How do we find similar contextual examples
in the target language?
? How do we sort the suggested target lan-
guage contextual examples in order to max-
imise their usefulness?
In order to restrict the research to what is
achievable within the scope of this project, we are
focussing on translation from English to Russian
using a comparable corpus of British and Rus-
sian newspaper texts. Newspapers cover a large
set of clearly identifiable topics that are compara-
ble across languages and cultures. In this project,
we have collected a 200-million-word corpus of
four major British newspapers and a 70-million-
word corpus of three major Russian newspapers
for roughly the same time span (2003-2004).1
In our proposed method, contexts of uses of En-
glish expressions defined by keywords are com-
pared to similar Russian expressions, using se-
mantic classes such as persons, places and insti-
tutions. For instance, the word agreement in the
example the parties were frustratingly close to
an agreement = ??????? ???? ?? ???????? ??????
? ?????????? ?????????? belongs to a seman-
tic class that also includes arrangement, contract,
deal, treaty. In the result, the search for collo-
cates of ??????? (close) in the context of agree-
ment words in Russian gives a short list of mod-
ifiers, which also includes the target: ?? ????????
??????.
2.2 Semantic taggers
In this project, we are porting the Lancaster En-
glish Semantic Tagger (EST) to the Russian lan-
guage. We have reused the existing semantic field
taxonomy of the Lancaster UCREL semantic anal-
ysis system (USAS), and applied it to Russian. We
1Russian newspapers are significantly shorter than their
British counterparts.
have also reused the existing software framework
developed during the construction of a Finnish Se-
mantic Tagger (L?fberg et al, 2005); the main ad-
justments and modifications required for Finnish
were to cope with the Unicode character set (UTF-
8) and word compounding.
USAS-EST is a software system for automatic
semantic analysis of text that was designed at
Lancaster University (Rayson et al, 2004). The
semantic tagset used by USAS was originally
loosely based on Tom McArthur?s Longman Lexi-
con of Contemporary English (McArthur, 1981).
It has a multi-tier structure with 21 major dis-
course fields, subdivided into 232 sub-categories.2
In the ASSIST project, we have been working on
both improving the existing EST and developing a
parallel tool for Russian - Russian Semantic Tag-
ger (RST). We have found that the USAS semantic
categories were compatible with the semantic cat-
egorizations of objects and phenomena in Russian,
as in the following example:3
poor JJ I1.1- A5.1- N5- E4.1- X9.1-
?????? A I1.1- A6.3- N5- O4.2- E4.1-
However, we needed a tool for analysing the
complex morpho-syntactic structure of Russian
words. Unlike English, Russian is a highly in-
flected language: generally, what is expressed in
English through phrases or syntactic structures
is expressed in Russian via morphological in-
flections, especially case endings and affixation.
For this purpose, we adopted a Russian morpho-
syntactic analyser Mystem that identifies word
forms, lemmas and morphological characteristics
for each word. Mystem is used as the equivalent
of the CLAWS part-of-speech (POS) tagger in the
USAS framework. Furthermore, we adopted the
Unicode UTF-8 encoding scheme to cope with the
Cyrillic alphabet. Despite these modifications, the
architecture of the RST software mirrors that of
the EST components in general.
The main lexical resources of the RST include
a single-word lexicon and a lexicon of multi-word
expressions (MWEs). We are building the Russian
lexical resources by exploiting both dictionaries
and corpora. We use readily available resources,
e.g. lists of proper names, which are then se-
2For the full tagset, see http://www.comp.lancs.
ac.uk/ucrel/usas/
3I1.1- = Money: lack; A5.1- = Evaluation: bad; N5- =
Quantities: little; E4.1- = Unhappy; X9.1- = Ability, intel-
ligence: poor; A6.3- = Comparing: little variety; O4.2- =
Judgement of appearance: bad
140
mantically classified. To bootstrap the system, we
have hand-tagged the 3,000 most frequent Russian
words based on a large newspaper corpus. Subse-
quently, the lexicons will be further expanded by
feeding texts from various sources into the RST
and classifying words that remain unmatched. In
addition, we will experiment with semi-automatic
lexicon construction using an existing machine-
readable English-Russian bilingual dictionary to
populate the Russian lexicon by mapping words
from each of the semantic fields in the English lex-
icon in turn. We aim at coverage of around 30,000
single lexical items and up to 9,000 MWEs, com-
pared to the EST which currently contains 54,727
single lexical items and 18,814 MWEs.
2.3 The user interface
The interface is powered by IMS Corpus Work-
bench (Christ, 1994) and is designed to be used in
the day-to-day workflow of novice and practising
translators, so the syntax of the CWB query lan-
guage has been simplified to adapt it to the needs
of the target user community.
The interface implements a search model for
finding translation equivalents in monolingual
comparable corpora, which integrates a number of
statistical and rule-based techniques for extending
search space, translating words and multiword ex-
pressions into the target language and restricting
the number of returned candidates in order to max-
imise precision and recall of relevant translation
equivalents. In the proposed search model queries
can be expanded by generating lists of collocations
for a given word or phrase, by generating sim-
ilarity classes4 or by manual selection of words
in concordances. Transfer between the source
language and target language is done via lookup
in a bilingual dictionary or via UCREL seman-
tic codes, which are common for concepts in both
languages. The search space is further restricted
by applying knowledge-based and statistical fil-
ters (such as part-of-speech and semantic class fil-
ters, IDF filter, etc), by testing the co-occurrence
of members of different similarity classes or by
manually selecting the presented variants. These
procedures are elementary building blocks that are
used in designing different search strategies effi-
cient for different types of translation equivalents
4Simclasses consist of words sharing collocates and are
computed using Singular Value Decomposition, as used by
(Rapp, 2004), e.g. Paris and Strasbourg are produced for
Brussels, or bus, tram and driver for passenger.
and contexts.
The core functionality of the system is intended
to be self-explanatory and to have a shallow learn-
ing curve: in many cases default search parame-
ters work well, so it is sufficient to input a word
or an expression in the source language in or-
der to get back a useful list of translation equiv-
alents, which can be manually checked by a trans-
lator to identify the most suitable solution for a
given context. For example, the word combina-
tion frustrated passenger is not found in the ma-
jor English-Russian dictionaries, while none of the
candidate translations of frustrated are suitable in
this context. The default search strategy for this
phrase is to generate the similarity class for En-
glish words frustrate, passenger, produce all pos-
sible translations using a dictionary and to test co-
occurrence of the resulting Russian words in target
language corpora. This returns a list of 32 Rus-
sian phrases, which follow the pattern of ?annoyed
/ impatient / unhappy + commuter / passenger /
driver?. Among other examples the list includes
an appropriate translation ??????????? ????????
(?unsatisfied passenger?).
The following example demonstrates the sys-
tem?s ability to find equivalents when there is
a reliable context to identify terms in the two
languages. Recent political developments in
Russia produced a new expression ?????????????
?????????? (?representative of president?), which
is as yet too novel to be listed in dictionaries.
However, the system can help to identify the peo-
ple that perform this duty, translate their names
to English and extract the set of collocates that
frequently appear around their names in British
newspapers, including Putin?s personal envoy and
Putin?s regional representative, even if no specific
term has been established for this purpose in the
British media.
As words cannot be translated in isolation and
their potential translation equivalents also often
consist of several words, the system detects not
only single-word collocates, but also multiword
expressions. For instance, the set of Russian
collocates of ?????????? (bureaucracy) includes
???????? (Brussels), which offers a straightfor-
ward translation into English and has such mul-
tiword collocates as red tape, which is a suitable
contextual translation for ??????????.
More experienced users can modify default pa-
rameters and try alternative strategies, construct
141
their own search paths from available basic build-
ing blocks and store them for future use. Stored
strategies comprise several elementary stages but
are executed in one go, although intermediate re-
sults can also be accessed via the ?history? frame.
Several search paths can be tried in parallel and
displayed together, so an optimal strategy for a
given class of phrases can be more easily identi-
fied.
Unlike Machine Translation, the system does
not translate texts. The main thrust of the sys-
tem lies in its ability to find several target language
examples that are relevant to the source language
expression. In some cases this results in sugges-
tions that can be directly used for translating the
source example, while in other cases the system
provides hints for the translator about the range of
target language expressions beyond what is avail-
able in bilingual dictionaries. Even if the preci-
sion of the current version is not satisfactory for an
MT system (2-3 suitable translations out of 30-50
suggested examples), human translators are able
to skim through the suggested set to find what is
relevant for the given translation task.
3 Conclusions
The set of tools is now under further development.
This involves an extension of the English seman-
tic tagger, development of the Russian tagger with
the target lexical coverage of 90% of source texts,
designing the procedure for retrieval of semanti-
cally similar situations and completing the user in-
terface. Identification of semantically similar sit-
uations can be improved by the use of segment-
matching algorithms as employed in Example-
Based MT and translation memories (Planas and
Furuse, 2000; Carl and Way, 2003).
There are two main applications of the pro-
posed methodology. One concerns training trans-
lators and advanced foreign language (FL) learn-
ers to make them aware of the variety of transla-
tion equivalents beyond the set offered by the dic-
tionary. The other application pertains to the de-
velopment of tools for practising translators. Al-
though the Russian language is not typologically
close to English and uses another writing system
which does not allow easy identification of cog-
nates, Russian and English belong to the same
Indo-European family and the contents of Rus-
sian and English newspapers reflect the same set
of topics. Nevertheless, the application of this
research need not be restricted to the English-
Russian pair only. The methodology for multilin-
gual processing of monolingual comparable cor-
pora, first tested in this project, will provide a
blueprint for the development of similar tools for
other language combinations.
Acknowledgments
The project is supported by two EPSRC grants:
EP/C004574 for Lancaster, EP/C005902 for Leeds.
References
Peter Bennison and Lynne Bowker. 2000. Designing a
tool for exploiting bilingual comparable corpora. In
Proceedings of LREC 2000, Athens, Greece.
Michael Carl and Andy Way, editors. 2003. Re-
cent advances in example-based machine transla-
tion. Kluwer, Dordrecht.
Oliver Christ. 1994. A modular and flexible archi-
tecture for an integrated corpus query system. In
COMPLEX?94, Budapest.
Ido Dagan and Kenneth Church. 1997. Ter-
might: Coordinating humans and machines in bilin-
gual terminology acquisition. Machine Translation,
12(1/2):89?107.
Herv? D?jean, ?ric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In COLING 2002.
Laura L?fberg, Scott Piao, Paul Rayson, Jukka-Pekka
Juntunen, Asko Nyk?nen, and Krista Varantola.
2005. A semantic tagger for the Finnish language.
In Proceedings of the Corpus Linguistics 2005 con-
ference.
Tom McArthur. 1981. Longman Lexicon of Contem-
porary English. Longman.
Emmanuel Planas and Osamu Furuse. 2000. Multi-
level similar segment matching algorithm for trans-
lation memories and example-based machine trans-
lation. In COLING, 18th International Conference
on Computational Linguistics, pages 621?627.
Reinhard Rapp. 2004. A freely available automatically
generated thesaurus of related words. In Proceed-
ings of LREC 2004, pages 395?398.
Paul Rayson, Dawn Archer, Scott Piao, and Tony
McEnery. 2004. The UCREL semantic analysis
system. In Proceedings of the workshop on Be-
yond Named Entity Recognition Semantic labelling
for NLP tasks in association with LREC 2004, pages
7?12.
142
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 136?143,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Assisting Translators in Indirect Lexical Transfer 
Bogdan Babych, Anthony Hartley, Serge Sharoff
  Centre for Translation Studies 
  University of Leeds, UK 
{b.babych,a.hartley,s.sharoff}@leeds.ac.uk
Olga Mudraya 
  Department of Linguistics 
  Lancaster University, UK 
 o.mudraya@lancs.ac.uk 
Abstract 
We present the design and evaluation of a 
translator?s amenuensis that uses compa-
rable corpora to propose and rank non-
literal solutions to the translation of expres-
sions from the general lexicon. Using dis-
tributional similarity and bilingual diction-
aries, the method outperforms established 
techniques for extracting translation 
equivalents from parallel corpora. The in-
terface to the system is available at: 
http://corpus.leeds.ac.uk/assist/v05/  
1 Introduction 
This paper describes a system designed to assist 
humans in translating expressions that do not nec-
essarily have a literal or compositional equivalent 
in the target language (TL). In the spirit of (Kay, 
1997), it is intended as a translator's amenuensis 
"under the tight control of a human translator ? to 
help increase his productivity and not to supplant him". 
One area where human translators particularly 
appreciate assistance is in the translation of expres-
sions from the general lexicon. Unlike equivalent 
technical terms, which generally share the same 
part-of-speech (POS) across languages and are in 
the ideal case univocal, the contextually appropri-
ate equivalents of general language expressions are 
often indirect and open to variation. While the 
transfer module in RBMT may acceptably under-
generate through a many-to-one mapping between 
source and target expressions, human translators, 
even in non-literary fields, value legitimate varia-
tion. Thus the French expression il faillit ?chouer 
(lit.: he faltered to fail) may be variously rendered 
as he almost/nearly/all but failed; he was on the 
verge/brink of failing/failure; failure loomed. All 
of these translations are indirect in that they in-
volve lexical shifts or POS transformations. 
Finding such translations is a hard task that can 
benefit from automated assistance. 'Mining' such 
indirect equivalents is difficult, precisely because 
of the structural mismatch, but also because of the 
paucity of suitable aligned corpora. The approach 
adopted here includes the use of comparable cor-
pora in source and target languages, which are 
relatively easy to create. The challenge is to gener-
ate a list of usable solutions and to rank them such 
that the best are at the top. 
Thus the present system is unlike SMT (Och and 
Ney, 2003), where lexical selection is effected by a 
translation model based on aligned, parallel cor-
pora, but the novel techniques it has developed are 
exploitable in the SMT paradigm. It also differs 
from now traditional uses of comparable corpora 
for detecting translation equivalents (Rapp, 1999) 
or extracting terminology (Grefenstette, 2002), 
which allows a one-to-one correspondence irre-
spective of the context. Our system addresses diffi-
culties in expressions in the general lexicon, whose 
translation is context-dependent. 
The structure of the paper is as follows. In Sec-
tion 2 we present the method we use for mining 
translation equivalents. In Section 3 we present the 
results of an objective evaluation of the quality of 
suggestions produced by the system by comparing 
our output against a parallel corpus. Finally, in 
Section 4 we present a subjective evaluation focus-
ing on the integration of the system into the work-
flow of human translators. 
2 Methodology 
The software acts as a decision support system for 
translators. It integrates different technologies for 
136
extracting indirect translation equivalents from 
large comparable corpora. In the following subsec-
tions we give the user perspective on the system 
and describe the methodology underlying each of 
its sub-tasks. 
2.1 User perspective 
Unlike traditional dictionaries, the system is a 
dynamic translation resource in that it can success-
fully find translation equivalents for units which 
have not been stored in advance, even for idiosyn-
cratic multiword expressions which almost cer-
tainly will not figure in a dictionary. While our 
system can rectify gaps and omissions in static 
lexicographical resources, its major advantage is 
that it is able to cope with an open set of transla-
tion problems, searching for translation equivalents 
in comparable corpora in runtime. This makes it 
more than just an extended dictionary. 
Contextual descriptors 
From the user perspective the system extracts indi-
rect translation equivalents as sets of contextual 
descriptors ? content words that are lexically cen-
tral in a given sentence, phrase or construction. 
The choice of these descriptors may determine the 
general syntactic perspective of the sentence and 
the use of supporting lexical items. Many transla-
tion problems arise from the fact that the mapping 
between such descriptors is not straightforward. 
The system is designed to find possible indirect 
mappings between sets of descriptors and to verify 
the acceptability of the mapping into the TL. For 
example, in the following Russian sentence, the 
bolded contextual descriptors require indirect 
translation into English. 
???? ???????? ????? ???????????-
?????? ?????, ? ??????? ????????? 
?????? ???????????? 
(Children attend badly repaired schools, in 
which [it] is missing the most necessary) 
Combining direct translation equivalents of 
these words (e.g., translations found in the Oxford 
Russian Dictionary ? ORD) may produce a non-
natural English sentence, like the literal translation 
given above. In such cases human translators usu-
ally apply structural and lexical transformations, 
for instance changing the descriptors? POS and/or 
replacing them with near-synonyms which fit to-
gether in the context of a TL sentence (Munday, 
2001: 57-58). Thus, a structural transformation of 
????? ????????????????? (badly repaired) may 
give in poor repair while a lexical transformation 
of ????????? ?????? ???????????? ([it] is missing 
the most necessary) gives lacking basic essentials. 
Our system models such transformations of the 
descriptors and checks the consistency of the re-
sulting sets in the TL. 
Using the system 
Human translators submit queries in the form of 
one or more SL descriptors which in their opinion 
may require indirect translation. When the transla-
tors use the system for translating into their native 
language, the returned descriptors are usually suf-
ficient for them to produce a correct TL construc-
tion or phrase around them (even though the de-
scriptors do not always form a naturally sounding 
expression). When the translators work into a non-
native language, they often find it useful to gener-
ate concordances for the returned descriptors to 
verify their usage within TL constructions. 
For example, for the sentence above translators 
may submit two queries: ????? ????????-
????????? (badly repaired) and ????????? 
???????????? (missing necessary). For the first 
query the system returns a list of descriptor pairs 
(with information on their frequency in the English 
corpus) ranked by distributional proximity to the 
original query, which we explain in Section 2.2. At 
the top of the list come: 
bad repair = 30  (11.005) 
bad maintenance = 16  (5.301) 
bad restoration = 2  (5.079) 
poor repair = 60  (5.026)? 
Underlined hyperlinks lead translators to actual 
contexts in the English corpus, e.g., poor repair 
generates a concordance containing a desirable TL 
construction which is a structural transformation of 
the SL query: 
in such a poor state of repair 
bridge in as poor a state of repair as the highways 
building in poor repair. 
dwellings are in poor repair; 
Similarly, the result of the second query may 
give the translators an idea about possible lexical 
transformation: 
missing need = 14  (5.035) 
important missing = 8 (2.930) 
missing vital = 8  (2.322) 
lack necessary = 204  (1.982)? 
essential lack = 86  (0.908)? 
137
The concordance for the last pair of descriptors 
contains the phrase they lack the three essentials, 
which illustrates the transformation. The resulting 
translation may be the following: 
Children attend schools that are in poor re-
pair and lacking basic essentials 
Thus our system supports translators in making 
decisions about indirect translation equivalents in a 
number of ways: it suggests possible structural and 
lexical transformations for contextual descriptors; 
it verifies which translation variants co-occur in 
the TL corpus; and it illustrates the use of the 
transformed TL lexical descriptors in actual con-
texts. 
2.2 Generating translation equivalents 
We have generalised the method used in our previ-
ous study (Sharoff et al, 2006) for extracting 
equivalents for continuous multiword expressions 
(MWEs). Essentially, the method expands the 
search space for each word and its dictionary trans-
lations with entries from automatically computed 
thesauri, and then checks which combinations are 
possible in target corpora. These potential transla-
tion equivalents are then ranked by their similarity 
to the original query and presented to the user. The 
range of retrievable equivalents is now extended 
from a relatively limited range of two-word con-
structions which mirror POS categories in SL and 
TL to a much wider set of co-occurring lexical 
content items, which may appear in a different or-
der, at some distance from each other, and belong 
to different POS categories.  
The method works best for expressions from the 
general lexicon, which do not have established 
equivalents, but not yet for terminology. It relies 
on a high-quality bilingual dictionary (en-ru ~30k, 
ru-en ~50K words, combining ORD and the core 
part of Multitran) and large comparable corpora 
(~200M En, ~70M Ru) of news texts. 
For each of the SL query terms q the system 
generates its dictionary translation Tr(q) and its 
similarity class S(q) ? a set of words with a similar 
distribution in a monolingual corpus. Similarity is 
measured as the cosine between collocation vec-
tors, whose dimensionality is reduced by SVD us-
ing the implementation by Rapp (2004). The de-
scriptor and each word in the similarity class are 
then translated into the TL using ORD or the Mul-
titran dictionary, resulting in {Tr(q)? Tr(S(q))}. 
On the TL side we also generate similarity classes, 
but only for dictionary translations of query terms 
Tr(q) (not for Tr(S(q)), which can make output too 
noisy). We refer to the resulting set of TL words as 
a translation class T.  
T = {Tr(q) ? Tr(S(q)) ? S(Tr(q))} 
Translation classes approximate lexical and 
structural transformations which can potentially be 
applied to each of the query terms. Automatically 
computed similarity classes do not require re-
sources like WordNet, and they are much more 
suitable for modelling translation transformations, 
since they often contain a wider range of words of 
different POS which share the same context, e.g., 
the similarity class of the word lack contains words 
such as absence, insufficient, inadequate, lost, 
shortage, failure, paucity, poor, weakness, inabil-
ity, need. This clearly goes beyond the range of 
traditional thesauri. 
For multiword queries, the system performs a 
consistency check on possible combinations of 
words from different translation classes. In particu-
lar, it computes the Cartesian product for pairs of 
translation classes T1 and T2 to generate the set P 
of word pairs, where each word (w1 and w2) comes 
from a different translation class: 
P = T1 ? T2 = {(w1, w2) | w1 ? T1 and w2 ? T2}  
Then the system checks whether each word pair 
from the set P exists in the database D of discon-
tinuous content word bi-grams which actually co-
occur in the TL corpus: 
P? = P ? D 
The database contains the set of all bi-grams that 
occur in the corpus with a frequency ? 4 within a 
window of 5 words (over 9M bigrams for each 
language). The bi-grams in D and in P are sorted 
alphabetically, so their order in the query is not 
important. 
Larger N-grams (N > 2) in queries are split into 
combinations of bi-grams, which we found to be 
an optimal solution to the problem of the scarcity 
of higher order N-grams in the corpus. Thus, for 
the query gain significant importance the system 
generates P?1(significant importance), P?2(gain impor-
tance), P?3(gain significant) and computes P? as:  
P? = {(w1,w2,w3)| (w1,w2) ? P?1 & (w1, w3) ? P?2 
& (w2,w3) ? P?3 }, 
which allows the system to find an indirect equiva-
lent ???????? ??????? ???????? (lit.: receive 
weighty meaning). 
138
Even though P? on average contains about 2% - 
4% of the theoretically possible number of bi-
grams present in P, the returned number of poten-
tial translation equivalents may still be large and 
contain much noise. Typically there are several 
hundred elements in P?, of which only a few are 
really useful for translation. To make the system 
usable in practice, i.e., to get useful solutions to 
appear close to the top (preferably on the first 
screen of the output), we developed methods of 
ranking and filtering the returned TL contextual 
descriptor pairs, which we present in the following 
sections. 
2.3 Hypothesis ranking 
The system ranks the returned list of contextual 
descriptors by their distributional proximity to the 
original query, i.e. it uses scores cos(vq, vw) gener-
ated for words in similarity classes ? the cosine of 
the angle between the collocation vector for a word 
and the collocation vector for the query or diction-
ary translation of the query. Thus, words whose 
equivalents show similar usage in a comparable 
corpus receive the highest scores. These scores are 
computed for each individual word in the output, 
so there are several ways to combine them to 
weight words in translation classes and word com-
binations in the returned list of descriptors.  
We established experimentally that the best way 
to combine similarity scores is to multiply weights 
W(T) computed for each word within its translation 
class T. The weight W(P?(w1,w2)) for each pair of 
contextual descriptors (w1, w2)?P? is computed as: 
W(P?(w1,w2)) = W(T(w1)) ? W(T(w2)); 
Computing W(T(w)), however, is not straightfor-
ward either, since some words in similarity classes 
of different translation equivalents for the query 
term may be the same, or different words from the 
similarity class of the original query may have the 
same translation. Therefore, a word w within a 
translation class may have come by several routes 
simultaneously, and may have done that several 
times. For each word w in T there is a possibility 
that it arrived in T either because it is in Tr(q) or 
occurs   n times in Tr(S(q)) or k times in S(Tr(q)). 
We found that the number of occurrences n and 
k of each word w in each subset gives valuable in-
formation for ranking translation candidates. In our 
experiments we computed the weight W(T) as the 
sum of similarity scores which w receives in each 
of the subsets. We also discovered that ranking 
improves if for each query term we compute in 
addition a larger (and potentially noisy) space of 
candidates that includes TL similarity classes of 
translations of the SL similarity class S(Tr(S(q))). 
These candidates do not appear in the system out-
put, but they play an important role in ranking the 
displayed candidates. The improvement may be 
due to the fact that this space is much larger, and 
may better support relevant candidates since there 
is a greater chance that appropriate indirect equiva-
lents are found several times within SL and TL 
similarity classes. The best ranking results were 
achieved when the original W(T) scores were mul-
tiplied by 2 and added to the scores for the newly 
introduced similarity space S(Tr(S(q))): 
W(T(w))= 2?(1 if w?Tr(q) )+  
2??( cos(vq, vTr(w)) | {w | w? Tr(S(q)) } ) +  
2??( cos(vTr(q), vw) | {w | w? S(Tr(q)) } ) + 
?(cos(vq, vTr(w))?cos (vTr(q), vw) |  
{w | w? S(Tr(S(q))) } ) 
For example, the system gives the following 
ranking for the indirect translation equivalents of 
the Russian phrase ??????? ???????? (lit.: weighty 
meaning) ? figures in brackets represent W(P?) 
scores for each pair of TL descriptors: 
1. significant importance = 7 (3.610)  
2. significant value = 128    (3.211)  
3. measurable value = 6       (2.657)?  
8. dramatic importance = 2    (2.028)  
9. important significant = 70 (2.014)  
10. convincing importance = 6 (1.843) 
The Russian similarity class for ??????? 
(weighty, ponderous) contains: ???????????? 
(convincing) (0.469), ???????? (significant) 
(0.461), ???????? (notable) (0.452) ?????-
?????? (dramatic) (0.371). The equivalent of 
significant is not at the top of the similarity class of 
the Russian query, but it appears at the top of the 
final ranking of pairs in P?, because this hypothesis 
is supported by elements of the set formed by 
S(Tr(S(q))); it appears in similarity classes for no-
table (0.353) and dramatic (0.315), which contrib-
uted these values to the W(T) score of significant: 
W(T(significant)) = 
    2 ? (Tr(????????)=significant (0.461))  
+ (Tr(????????)=notable (0.452)  
  ? S(notable)=significant (0.353)) 
+ (Tr(???????????)=dramatic (0.371)  
  ? S(dramatic)= significant (0.315)) 
The word dramatic itself is not usable as a 
translation equivalent in this case, but its similarity 
139
class contains the support for relevant candidates, 
so it can be viewed as useful noise. On the other 
hand, the word convincing does not receive such 
support from the hypothesis space, even though its 
Russian equivalent is ranked higher in the SL simi-
larity class. 
2.4 Semantic filtering 
Ranking of translation candidates can be further 
improved when translators use an option to filter 
the returned list by certain lexical criteria, e.g., to 
display only those examples that contain a certain 
lexical item, or to require one of the items to be a 
dictionary translation of the query term. However, 
lexical filtering is often too restrictive: in many 
cases translators need to see a number of related 
words from the same semantic field or subject do-
main, without knowing the lexical items in ad-
vance. In this section we present the semantic fil-
ter, which is based on Russian and English seman-
tic taggers which use the same semantic field tax-
onomy for both languages. 
The semantic filter displays only those items 
which have specified semantic field tags or tag 
combinations; it can be applied to one or both 
words in each translation hypothesis in P?. The 
default setting for the semantic filter is the re-
quirement for both words in the resulting TL can-
didates to contain any of the semantic field tags 
from a SL query term. 
In the next section we present evaluation results 
for this default setting (which is applied when the 
user clicks the Semantic Filter button), but human 
translators have further options ? to filter by tags 
of individual words, to use semantic classes from 
SL or TL terms, etc. 
For example, applying the default semantic filter 
for the output of the query ????? ???????-
?????????? (badly repaired) removes the high-
lighted items from the list: 
 1. bad repair = 30       (11.005)  
[2. good repair = 154     (8.884) ] 
 3. bad rebuild = 6       (5.920)  
[4. bad maintenance = 16  (5.301) ] 
 5. bad restoration = 2   (5.079)  
 6. poor repair = 60      (5.026)  
[7. good rebuild = 38     (4.779) ] 
 8. bad construction = 14 (4.779)  
Items 2 and 7 are generated by the system be-
cause good, well and bad are in the same similar-
ity cluster for many words (they often share the 
same collocations). The semantic filter removes 
examples with good and well on the grounds that 
they do not have any of the tags which come from 
the word ????? (badly): in particular, instead of 
tag A5? (Evaluation: Negative) they have tag A5+ 
(Evaluation: Positive). Item 4 is removed on the 
grounds that the words ????????????????? 
(repaired) and maintenance do not have any tags 
in common ? they appear ontologically too far 
apart from the point of view of the semantic tagger. 
The core of the system?s multilingual semantic 
tagging is a knowledge base in which single words 
and MWEs are mapped to their potential semantic 
field categories. Often a lexical item is mapped to 
multiple semantic categories, reflecting its poten-
tial multiple senses. In such cases, the tags are ar-
ranged by the order of likelihood of meanings, 
with the most prominent first. 
3 Objective evaluation 
In the objective evaluation we tested the perform-
ance of our system on a selection of indirect trans-
lation problems, extracted from a parallel corpus 
consisting mostly of articles from English and 
Russian newspapers (118,497 words in the R-E 
direction, 589,055 words in the E-R direction). It 
has been aligned on the sentence level by JAPA 
(Langlais et al, 1998), and further on the word 
level by GIZA++ (Och and Ney, 2003). 
3.1 Comparative performance 
The intuition behind the objective evaluation 
experiment is that the capacity of our tool to find 
indirect translation equivalents in comparable cor-
pora can be compared with the results of automatic 
alignment of parallel texts used in translation mod-
els in SMT: one of the major advantages of the 
SMT paradigm is its ability to reuse indirect 
equivalents found in parallel corpora (equivalents 
that may never come up in hand-crafted dictionar-
ies). Thus, automatically generated GIZA++ dic-
tionaries with word alignment contain many exam-
ples of indirect translation equivalents. 
We use these dictionaries to simulate the genera-
tor of translation classes T, which we recombine to 
construct their Cartesian product P, similarly to the 
procedure we use to generate the output of our sys-
tem. However, the two approaches generate indi-
rect translation equivalence hypotheses on the ba-
sis of radically different material: the GIZA dic-
tionary uses evidence from parallel corpora of ex-
140
isting human translations, while our system re-
combines translation candidates on the basis of 
their distributional similarity in monolingual com-
parable corpora. Therefore we took GIZA as a 
baseline. 
Translation problems for the objective evalua-
tion experiment were manually extracted from two 
parallel corpora: a section of about 10,000 words 
of a corpus of English and Russian newspapers, 
which we also used to train GIZA, and a section of 
the same length from a corpus of interviews pub-
lished on the Euronews.net website. 
We selected expressions which represented 
cases of lexical transformations (as illustrated in 
Section 0), containing at least two content words 
both in the SL and TL. These expressions were 
converted into pairs of contextual descriptors ? 
e.g., recent success, reflect success ? and submit-
ted to the system and to the GIZA dictionary. We 
compared the ability of our system and of GIZA to 
find indirect translation equivalents which matched 
the equivalents used by human translators. The 
output from both systems was checked to see 
whether it contained the contextual descriptors 
used by human translators. We submitted 388 pairs 
of descriptors extracted from the newspaper trans-
lation corpus and 174 pairs extracted from the Eu-
ronews interview corpus. Half of these pairs were 
Russian, and the other half English. 
We computed recall figures for 2-word combi-
nations of contextual descriptors and single de-
scriptors within those combinations. We also show 
the recall of translation variants provided by the 
ORD on this data set. For example, for the query 
????????? ???????????? ([it] is missing neces-
sary [things]) human translators give the solution 
lacking essentials; the lemmatised descriptors are 
lack and essential. ORD returns direct translation 
equivalents missing and necessary. The GIZA dic-
tionary in addition contains several translation 
equivalents for the second term (with alignment 
probabilities) including: necessary ~0.332, need 
~0.226, essential ~0.023. Our system returns both 
descriptors used in human translation as a pair ? 
lack essential (ranked 41 without filtering and 22 
with the default semantic filter). Thus, for a 2-word 
combination of the descriptors only the output of 
our system matched the human solution, which we 
counted as one hit for the system and no hits for 
ORD or GIZA. For 1-word descriptors we counted 
2 hits for our system (both words in the human 
solution are matched), and 1 hit for GIZA ? it 
matches the word essential ~0.023 (which also il-
lustrates its ability to find indirect translation 
equivalents). 
 2w descriptors 1w descriptors 
 news interv news interv 
ORD 6.7% 4.6% 32.9% 29.3% 
GIZA++ 13.9% 3.4% 35.6% 29.0%
Our system 21.9% 19.5% 55.8% 49.4%
Table 1 Conservative estimate of recall 
It can be seen from Table 1 that for the newspa-
per corpus on which it was trained, GIZA covers a 
wider set of indirect translation variants than ORD. 
But our recall is even better both for 2-word and 1-
word descriptors. 
However, note that GIZA?s ability to retrieve 
from the newspaper corpus certain indirect transla-
tion equivalents may be due to the fact that it has 
previously seen them frequently enough to gener-
ate a correct alignment and the corresponding dic-
tionary entry. 
The Euronews interview corpus was not used for 
training GIZA. It represents spoken language and 
is expected to contain more ?radical? transforma-
tions. The small decline in ORD figures here can 
be attributed to the fact that there is a difference in 
genre between written and spoken texts and conse-
quently between transformation types in them. 
However, the performance of GIZA drops radi-
cally on unseen text and becomes approximately 
the same as the ORD. 
This shows that indirect translation equivalents 
in the parallel corpus used for training GIZA are 
too sparse to be learnt one by one and successfully 
applied to unseen data, since solutions which fit 
one context do not necessarily suit others. 
The performance of our system stays at about 
the same level for this new type of text; the decline 
in its performance is comparable to the decline in 
ORD figures, and can again be explained by the 
differences in genre. 
3.2 Evaluation of hypothesis ranking 
As we mentioned, correct ranking of translation 
candidates improves the usability of the system. 
Again, the objective evaluation experiment gives 
only a conservative estimate of ranking, because 
there may be many more useful indirect solutions 
further up the list in the output of the system which 
are legitimate variants of the solutions found in the 
141
parallel corpus. Therefore, evaluation figures 
should be interpreted in a comparative rather then 
an absolute sense. 
We use ranking by frequency as a baseline for 
comparing the ranking described in Section 2.3 ? 
by distributional similarity between a candidate 
and the original query. 
Table 2 shows the average rank of human solu-
tions found in parallel corpora and the recall of 
these solutions for the top 300 examples. Since 
there are no substantial differences between the 
figures for the newspaper texts and for the inter-
views, we report the results jointly for 556 transla-
tion problems in both selections (lower rank fig-
ures are better). 
 Recall Average rank 
2-word descriptors 
frequency (baseline) 16.7% rank=93.7
distributional similarity 19.5% rank=44.4
sim. + semantic filter 14.4% rank=26.7
1-word descriptors 
frequency (baseline) 48.2% rank=42.7
distributional similarity 52.8% rank=21.6
sim. + semantic filter 44.1% rank=11.3
Table 2 Ranking: frequency, similarity and filter 
It can be seen from the table that ranking by 
similarity yields almost a twofold improvement for 
the average rank figures compared to the baseline. 
There is also a small improvement in recall, since 
there are more relevant examples that appear 
within the top 300 entries. 
The semantic filter once again gives an almost 
twofold improvement in ranking, since it removes 
many noisy items. The average is now within the 
top 30 items, which means that there is a high 
chance that a translation solution will be displayed 
on the first screen. The price for improved ranking 
is decline in recall, since it may remove some rele-
vant lexical transformations if they appear to be 
ontologically too far apart. But the decline is 
smaller: about 26.2% for 2-word descriptors and 
16.5% for 1-word descriptors. The semantic filter 
is an optional tool, which can be used to great ef-
fect on noisy output: its improvement of ranking 
outweighs the decline in recall. 
Note that the distribution of ranks is not normal, 
so in Figure 1 we present frequency polygons for 
rank groups of 30 (which is the number of items 
that fit on a single screen, i.e., the number of items 
in the first group (r030) shows solutions that will 
be displayed on the first screen). The majority of 
solutions ranked by similarity appear high in the 
list (in fact, on the first two or three screens). 
0
10
20
30
40
50
60
70
r0
30
r0
60
r0
90
r1
20
r1
50
r1
80
r2
10
r2
40
r2
70
r3
00
similarity
frequency
 
Figure 1 Frequency polygons for ranks 
4 Subjective evaluation 
The objective evaluation reported above uses a 
single reference translation and is correspondingly 
conservative in estimating the coverage of the sys-
tem. However, many expressions studied have 
more than one fluent translation. For instance, in 
poor repair is not the only equivalent for the Rus-
sian expression ????? ?????????????????. It is 
also possible to translate it as unsatisfactory condi-
tion, bad state of repair, badly in need of repair, 
and so on. The objective evaluation shows that the 
system has been able to find the suggestion used 
by a particular translator for the problem studied. It 
does not tell us whether the system has found some 
other translations suitable for the context. Such 
legitimate translation variation implies that the per-
formance of a system should be studied on the ba-
sis of multiple reference translations, though typi-
cally just two reference translations are used (Pap-
ineni, et al 2001). This might be enough for the 
purposes of a fully automatic MT tool, but in the 
context of a translator's amanuensis which deals 
with expressions difficult for human translators, it 
is reasonable to work with a larger range of ac-
ceptable target expressions. 
With this in mind we evaluated the performance 
of the tool with a panel of 12 professional transla-
tors. Problematic expressions were highlighted and 
the translators were asked to find suitable sugges-
tions produced by the tool for these expressions 
and rank their usability on a scale from 1 to 5 (not 
acceptable to fully idiomatic, so 1 means that no 
usable translation was found at all). 
Sentences themselves were selected from prob-
lems discussed on professional translation forums 
proz.com and forum.lingvo.ru. Given the range of 
corpora used in the system (reference and newspa-
142
per corpora), the examples were filtered to address 
expressions used in newspapers. 
The goal of the subjective evaluation experiment 
was to establish the usefulness of the system for 
translators beyond the conservative estimate given 
by the objective evaluation. The intuition behind 
the experiment is that if there are several admissi-
ble translations for the SL contextual descriptors, 
and system output matches any of these solutions, 
then the system has generated something useful. 
Therefore, we computed recall on sets of human 
solutions rather than on individual solutions. We 
matched 210 different human solutions to 36 trans-
lation problems. To compute more realistic recall 
figures, we counted cases when the system output 
matches any of the human solutions in the set. 
Table 3 compares the conservative estimate of the 
objective evaluation and the more realistic estimate 
on a single data set. 
 2w default 2w with sem filt 
Conservative  32.4%; r=53.68 21.9%; r=34.67 
Realistic 75.0%;   r=7.48 61.1%;   r=3.95 
Table 3 Recall and rank for 2-word descriptors 
Since the data set is different, the figures for the 
conservative estimate are higher than those for the 
objective evaluation data set. However, the table 
shows the there is a gap between the conservative 
estimate and the realistic coverage of the transla-
tion problems by the system, and that real coverage 
of indirect translation equivalents is potentially 
much higher. 
Table 4 shows averages (and standard deviation 
?) of the usability scores divided in four groups: (1) 
solutions that are found both by our system and the 
ORD; (2) solutions found only by our system; (3) 
solutions found only by ORD (4) solutions found 
by neither: 
 system (+) system (?) 
ORD (+) 4.03 (0.42) 3.62 (0.89)
ORD (?) 4.25 (0.79) 3.15 (1.15)
Table 4 Human scores and ? for system output 
It can be seen from the table that human users find 
the system most useful for those problems where 
the solution does not match any of the direct dic-
tionary equivalents, but is generated by the system. 
5 Conclusions 
We have presented a method of finding indirect 
translation equivalents in comparable corpora, and 
integrated it into a system which assists translators 
in indirect lexical transfer. The method outper-
forms established methods of extracting indirect 
translation equivalents from parallel corpora. 
We can interpret these results as an indication 
that our method, rather than learning individual 
indirect transformations, models the entire family 
of transformations entailed by indirect lexical 
transfer. In other words it learns a translation strat-
egy which is based on the distributional similarity 
of words in a monolingual corpus, and applies this 
strategy to novel, previously unseen examples. 
The coverage of the tool and additional filtering 
techniques make it useful for professional transla-
tors in automating the search for non-trivial, indi-
rect translation equivalents, especially equivalents 
for multiword expressions. 
References 
Gregory Grefenstette. 2002. Multilingual corpus-based 
extraction and the very large lexicon. In: Lars Borin, 
editor, Language and Computers, Parallel corpora, 
parallel worlds, pages 137-149. Rodopi. 
Martin Kay. 1997. The proper place of men and ma-
chines in language translation. Machine Translation, 
12(1-2):3-23. 
Philippe Langlais, Michel Simard, and Jean V?ronis. 
1998. Methods and practical issues in evaluating 
alignment techniques. In Proc. Joint COLING-ACL-
98, pages 711-717. 
Jeremy Munday. 2001. Introducing translation studies. 
Theories and Applications. Routledge, New York. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19-51. 
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. 
(2001). Bleu: a method for automatic evaluation of 
machine translation, RC22176 W0109-022: IBM. 
Reinhard Rapp. 1999. Automatic identification of word 
translations from unrelated English and German cor-
pora. In Procs. the 37th ACL, pages 395-398. 
Reinhard Rapp. 2004. A freely available automatically 
generated thesaurus of related words. In Procs. LREC 
2004, pages 395-398, Lisbon. 
Serge Sharoff, Bogdan Babych and Anthony Hartley 
2006. Using Comparable Corpora to Solve Problems 
Difficult for Human Translators. In: Proceedings of 
the COLING/ACL 2006 Main Conference Poster 
Sessions, pp. 739-746. 
143
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 2?11,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Measuring MWE Compositionality Using Semantic Annotation 
Scott S. L. Piao1, Paul Rayson1, Olga Mudraya2, Andrew Wilson2 and Roger Garside1 
 
1Computing Department 
Lancaster University 
Lancaster, UK 
{s.piao, p.rayson, r.garside}@lancaster.ac.uk 
2Dept. of Linguistics and EL 
Lancaster University 
Lancaster, UK 
{o.mudraya, a.wilson}@lancaster.ac.uk 
 
 
Abstract 
This paper reports on an experiment in 
which we explore a new approach to the 
automatic measurement of multi-word 
expression (MWE) compositionality. We 
propose an algorithm which ranks MWEs 
by their compositionality relative to a 
semantic field taxonomy based on the 
Lancaster English semantic lexicon (Piao 
et al, 2005a). The semantic information 
provided by the lexicon is used for meas-
uring the semantic distance between a 
MWE and its constituent words. The al-
gorithm is evaluated both on 89 manually 
ranked MWEs and on McCarthy et als 
(2003) manually ranked phrasal verbs. 
We compared the output of our tool with 
human judgments using Spearman?s 
rank-order correlation coefficient. Our 
evaluation shows that the automatic rank-
ing of the majority of our test data 
(86.52%) has strong to moderate correla-
tion with the manual ranking while wide 
discrepancy is found for a small number 
of MWEs. Our algorithm also obtained a 
correlation of 0.3544 with manual rank-
ing on McCarthy et als test data, which 
is comparable or better than most of the 
measures they tested. This experiment 
demonstrates that a semantic lexicon can 
assist in MWE compositionality meas-
urement in addition to statistical algo-
rithms. 
1 Introduction 
Over the past few years, compositionality and 
decomposability of MWEs have become impor-
tant issues in NLP research. Lin (1999) argues 
that ?non-compositional expressions need to be 
treated differently than other phrases in many 
statistical or corpus?based NLP methods?. Com-
positionality means that ?the meaning of the 
whole can be strictly predicted from the meaning 
of the parts? (Manning & Sch?tze, 2000). On the 
other hand, decomposability is a metric of the 
degree to which the meaning of a MWE can be 
assigned to its parts (Nunberg, 1994; Riehemann, 
2001; Sag et al, 2002). These two concepts are 
closely related. Venkatapathy and Joshi (2005) 
suggest that ?an expression is likely to be rela-
tively more compositional if it is decomposable?. 
While there exist various definitions for 
MWEs, they are generally defined as cohesive 
lexemes that cross word boundaries (Sag et al, 
2002; Copestake et al, 2002; Calzolari et al, 
2002; Baldwin et al, 2003), which include 
nominal compounds, phrasal verbs, idioms, col-
locations etc. Compositionality is a critical crite-
rion cutting across different definitions for ex-
tracting and classifying MWEs. While semantics 
of certain types of MWEs are non-compositional, 
like idioms ?kick the bucket? and ?hot dog?, 
some others can have highly compositional se-
mantics like the expressions ?traffic light? and 
?audio tape?. 
Automatic measurement of MWE composi-
tionality can have a number of applications. One 
of the often quoted applications is for machine 
translation (Melamed, 1997; Hwang & Sasaki, 
2005), in which non-compositional MWEs need 
special treatment. For instance, the translation of 
a highly compositional MWE can possibly be 
inferred from the translations of its constituent 
words, whereas it is impossible for non-
compositional MWEs, for which we need to 
identify the translation equivalent for the MWEs 
as a whole. 
In this paper, we explore a new method of 
automatically estimating the compositionality of 
MWEs using lexical semantic information, 
sourced from the Lancaster semantic lexicon 
(Piao et al, 2005a) that is employed in the 
USAS1 tagger (Rayson et al, 2004). This is a 
                                                 
1 UCREL Semantic Analysis System 
2
large lexical resource which contains nearly 
55,000 single-word entries and over 18,800 
MWE entries. In this lexicon, each MWE2 and 
the words it contains are mapped to their poten-
tial semantic categories using a semantic field 
taxonomy of 232 categories. An evaluation of 
lexical coverage on the BNC corpus showed that 
the lexical coverage of this lexicon reaches 
98.49% for modern English (Piao et al, 2004).  
Such a large-scale semantic lexical resource al-
lows us to examine the semantics of many 
MWEs and their constituent words conveniently 
without resorting to large corpus data. Our ex-
periment demonstrates that such a lexical re-
source provides an additional approach for auto-
matically estimating the compositionality of 
MWEs. 
One may question the necessity of measuring 
compositionality of manually selected MWEs. 
The truth is, even if the semantic lexicon under 
consideration was compiled manually, it does not 
exclusively consist of non-compositional MWEs 
like idioms. Built for practical discourse analysis, 
it contains many MWEs which are highly com-
positional but depict certain entities or semantic 
concepts. This research forms part of a larger 
effort to extend lexical resources for semantic 
tagging. Techniques are described elsewhere 
(e.g. Piao et al, 2005b) for finding new candi-
date MWE from corpora. The next stage of the 
work is to semi-automatically classify these can-
didates using an existing semantic field taxon-
omy and, to assist this task, we need to investi-
gate patterns of compositionality. 
2 Related Work  
In recent years, various approaches have been 
proposed to the analysis of MWE compositional-
ity. Many of the suggested approaches employ 
statistical algorithms. 
One of the earliest studies in this area was re-
ported by Lin (1999) who assumes that ?non-
compositional phrases have a significantly dif-
ferent mutual information value than the phrases 
that are similar to their literal meanings? and 
proposed to identify non-compositional MWEs 
in a corpus based on distributional characteristics 
of MWEs. Bannard et al (2003) tested tech-
niques using statistical models to infer the mean-
ing of verb-particle constructions (VPCs), focus-
                                                 
2 In this lexicon, many MWEs are encoded as templates, 
such as driv*_* {Np/P*/J*/R*} mad_JJ,  which represent 
variational forms of a single MWE, For further details, see 
Rayson et al, 2004.  
ing on prepositional particles. They tested four 
methods over four compositional classification 
tasks, reporting that, on all tasks, at least one of 
the four methods offers an improvement in preci-
sion over the baseline they used. 
McCarthy et al (2003) suggested that compo-
sitional phrasal verbs should have similar 
neighbours as for their simplex verbs. They 
tested various measures using the nearest 
neighbours of phrasal verbs and their simplex 
counterparts, and reported that some of the 
measures produced results which show signifi-
cant correlation with human judgments. Baldwin 
et al (2003) proposed a LSA-based model for 
measuring the decomposability of MWEs by ex-
amining the similarity between them and their 
constituent words, with higher similarity indicat-
ing the greater decomposability.  They evaluated 
their model on English noun-noun compounds 
and verb-particles by examining the correlation 
of the results with similarities and hyponymy 
values in WordNet. They reported that the LSA 
technique performs better on the low-frequency 
items than on more frequent items. Venkatapathy 
and Joshi (2005) measured relative composition-
ality of collocations having verb-noun pattern 
using a SVM (Support Vector Machine) based 
ranking function. They integrated seven various 
collocational and contextual features using their 
ranking function, and evaluated it against manu-
ally ranked test data. They reported that the SVM 
based method produces significantly better re-
sults compared to methods based on individual 
features. 
The approaches mentioned above invariably 
depend on a variety of statistical contextual in-
formation extracted from large corpus data. In-
evitably, such statistical information can be af-
fected by various uncontrollable ?noise?, and 
hence there is a limitation to purely statistical 
approaches. 
In this paper, we contend that a manually 
compiled semantic lexical resource can have an 
important part to play in measuring the composi-
tionality of MWEs. While any approach based on 
a specific lexical resource may lack generality, it 
can complement purely statistical approaches by 
importing human expert knowledge into the 
process. Particularly, if such a resource has a 
high lexical coverage, which is true in our case, 
it becomes much more useful for dealing with 
general English. It should be emphasized that we 
propose our semantic lexical-based approach not 
as a substitute for the statistical approaches. 
3
Rather we propose it as a potential complement 
to them.   
In the following sections, we describe our ex-
periment and explore this approach to the issue 
of automatic estimation of MWE compositional-
ity. 
3 Measuring MWE compositionality 
with semantic field information 
In this section, we propose an algorithm for 
automatically measuring MWE compositionality 
based on the Lancaster semantic lexicon. In this 
lexicon, the semantic field of each word and 
MWE is encoded in the form of semantic tags. 
We contend that the compositionality of a MWE 
can be estimated by measuring the distance be-
tween semantic fields of an MWE and its con-
stituent words based on the semantic field infor-
mation available from the lexicon. 
The lexicon employs a taxonomy containing 
21 major semantic fields which are further di-
vided into 232 sub-categories. 3  Tags are de-
signed to denote the semantic fields using letters 
and digits. For instance, tag N3.2 denotes the 
category of {SIZE} and Q4.1 denotes {media: 
Newspapers}. Each entry in the lexicon maps a 
word or MWE to its potential semantic field 
category/ies. More often than not, a lexical item 
is mapped to multiple semantic categories, re-
flecting its potential multiple senses. In such 
cases, the tags are arranged by the order of like-
lihood of meanings, with the most prominent one 
at the head of the list. For example, the word 
?mass? is mapped to tags N5, N3.5, S9, S5 and 
B2, which denote its potential semantic fields of 
{QUANTITIES},  {MEASUREMENT: 
WEIGHT}, {RELIGION AND SUPERNATU-
RAL}, {GROUPS AND AFFILIATION} and 
{HEALTH AND DISEASE}. 
 The lexicon provides direct access to the se-
mantic field information for large number of 
MWEs and their constituent words. Furthermore, 
the lexicon was analysed and classified manually 
by a team of linguists based on the analysis of 
corpus data and consultation of printed and elec-
tronic corpus-based dictionaries, ensuring a high 
level of consistency and accuracy of the semantic 
analysis.  
In our context, we interpret the task of measur-
ing the compositionality of MWEs as examining 
the distance between the semantic tag of a MWE 
and the semantic tags of its constituent words. 
                                                 
3 For the complete semantic tagset, see website: 
http://www.comp.lancs.ac.uk/ucrel/usas/ 
Given a MWE M and its constituent words wi (i 
= 1, .., n), the compositionality D can be meas-
ured by multiplying the semantic distance SD 
between M and each of its constituent words wi. 
In practice, the square root of the product is used 
as the score in order to reduce the range of actual 
D-scores, as shown below: 
 
(1)   ?
=
=
n
i
iwMSDMD
1
),()(  
 
where D-score ranges between [0, 1], with 1 in-
dicating the strongest compositionality and 0 the 
weakest compositionality. 
In the semantic lexicon, as the semantic in-
formation of function words is limited, they are 
classified into a single grammatical bin (denoted 
by tag Z5). In our algorithm, they are excluded 
from the measuring process by using a stop word 
list. Therefore, only the content constituent 
words are involved in measuring the composi-
tionality. Although function words may form an 
important part of many MWEs, such as phrasal 
verbs, because our algorithm solely relies on se-
mantic field information, we assume they can be 
ignored.  
 The semantic distance between a MWE and 
any of its constituent words is calculated by 
quantifying the similarity between their semantic 
field categories. In detail, if the MWE and a con-
stituent word do not share any of the major 21 
semantic domains, the SD is assigned a small 
value ?.4 If they do, three possible cases are con-
sidered: 
 
Case a. If they share the same tag, and the con-
stituent word has only one tag, then SD 
is one. 
Case b. If they share a tag or tags, but the con-
stituent words have multiple candidate 
tags, then SD is weighted using a vari-
able ? based on the position of the 
matched tag in the candidate list as well 
as the number of candidate tags. 
Case c. If they share a major category, but their 
tags fall into different sub-categories 
(denoted by the trailing digits following 
a letter), SD is further weighted using a 
                                                 
4 We avoid using zero here in order to avoid producing se-
mantic distance of zero indiscriminately when any one of 
the constituent words produces zero distance regardless of 
other constituent words. 
4
variable ? which reflects the difference 
of the sub-categories. 
With respect to weight ?, suppose L is the 
number of candidate tags of the constituent word 
under consideration, N is the position of the spe-
cific tag in the candidate list (the position starts 
from the top with N=1), then the weight ? is cal-
culated as 
 
(2)  
2
1
L
NL +?=? , 
 
where N=1, 2, ?, n and N<=L. Ranging between 
[1, 0), ? takes into account both the location of 
the matched tag in the candidate tag list and the 
number of candidate tags. This weight penalises 
the words having more candidate semantic tags 
by giving a lower value for their higher degree of 
ambiguity. As either L or N increases, the ?-
value decreases.       
Regarding the case c), where the tags share the 
same head letter but different digit codes, i.e. 
they are from the same major category but in 
different sub-categories, the weight ? is calcu-
lated based on the number of sub-categories they 
share. As we mentioned earlier, a semantic tag 
consists of an initial letter and some trailing dig-
its divided by points, e.g. S1.1.2 {RECIPROC-
ITY}, S1.1.3 {PARTICIPATION}, S1.1.4 {DE-
SERVE} etc. If we let T1, T2 be a pair of semantic 
tags with the same initial letters, which have ki 
and kj trailing digit codes (denoting the number 
of sub-division layers) respectively and share n 
digit codes from the left, or from the top layer, 
then ? is calculated as follows: 
 
(3)   
k
n=? ; 
(4)   . ),max( ji kkk =
 
where ? ranges between (0, 1). In fact, the cur-
rent USAS taxonomy allows only the maximum 
three layers of sub-division, therefore ? has one 
of three possible scores: 0.500 (1/2), 0.333 (1/3) 
and 0.666 (2/3). In order to avoid producing zero 
scores, if the pair of tags do not share any digit 
codes except the head letter, then n is given a 
small value of 0.5. 
Combining all of the weighting scores, the 
semantic distance SD in formula (1) is calculated 
as follows: 
 
(5)  ( )
??
?
?
??
?
?
?
=
?
?
=
=
.  then   c), if
;  then   b), if
1;  then   a), if
;   then   matches,   tagno if
,
1
1
n
i
ii
n
i
iiwMSD
??
?
?
 
where ? is given a small value of 0.001 for our 
experiment5. 
Some MWEs and single words in the lexicon 
are assigned with combined semantic categories 
which are considered to be inseparable, as shown 
below: 
petrol_NN1 station_NN1 M3/H1 
where the slash means that this MWE falls under 
the categories of M3 {VEHICLES AND TRANS-
PORTS ON LAND} and H1 {ARCHITECTURE 
AND KINDS OF HOUSES AND BUILDINGS} 
at the same time. For such cases, criss-cross 
comparisons between all possible tag pairs are 
carried out in order to find the optimal match 
between the tags of the MWE and its constituent 
words. 
By way of further explanation, the word 
?brush? as a verb has candidate semantic tags of 
B4 {CLEANING AND PERSONAL CARE} and 
A1.1.1 {GENERAL ACTION, MAKING} etc. On 
the other hand, the phrasal verb ?brush down? 
may fall under either B4 category with the sense 
of cleaning or G2.2 category {ETHICS} with the 
sense of reprimand. When we apply our algo-
rithm to it, we get the D-score of 1.0000 for the 
sense of cleaning, indicating a high degree of 
compositionality, whereas we get a low D-score 
of 0.0032 for the sense of reprimand, indicating 
a low degree of compositionality. Note that the 
word ?down? in this MWE is filtered out as it is 
a functional word. 
The above example has only a single constitu-
ent content word. In practice, many MWEs have 
more complex structures than this example. In 
order to test the performance of our algorithm, 
we compared its output against human judgments 
of compositionality, as reported in the following 
section. 
4 Manually Ranking MWEs for 
Evaluation 
In order to evaluate the performance of our 
tool against human judgment, we prepared a list 
                                                 
5 As long as ? is small enough, it does not affect the ranking 
of D-scores. 
5
of 89 MWEs6 and asked human raters to rank 
them via a website. The list includes six MWEs 
with multiple senses, and these were treated as 
separate MWE. The Lancaster MWE lexicon has 
been compiled manually by expert linguists, 
therefore we assume that every item in this lexi-
con is a true MWE, although we acknowledge 
that some errors may exist. 
Following McCarthy et al?s approach, we 
asked the human raters to assign each MWE a 
number ranging between 0 (opaque) and 10 
(fully compositional). Both native and non-native 
speakers are involved, but only the data from 
native speakers are used in this evaluation. As a 
result, three groups of raters were involved in the 
experiment.  Group 1 (6 people) rated MWEs 
with indexes of 1-30, Group 2 (4 people) rated 
MWEs with indexes of 31-59 and Group 3 (five 
people) rated MWEs with indexes of 6-89. 
In order to test the level of agreement between 
the raters, we used the procedures provided in 
the 'irr' package for R (Gamer, 2005). With this 
tool, the average intraclass correlation coefficient 
(ICC) was calculated for each group of raters 
using a two-way agreement model (Shrout & 
Fleiss, 1979). As a result, all ICCs exceeded 0.7 
and were significant at the 95% confidence level, 
indicating an acceptable level of agreement be-
tween raters. For Group 1, the ICC was 0.894 
(95% ci = 0.807 < ICC < 0.948), for Group 2 it 
was 0.9 (95% ci=0.783<ICC<0.956) and for 
Group 3 it was 0.886 (95% ci =  0.762 < ICC < 
0.948). 
Based on this test, we conclude that the man-
ual ranking of the MWEs is reliable and is suit-
able to be used in our evaluation. Source data for 
the human judgements is available from our 
website in spreadsheet form7. 
5 Evaluation 
In our evaluation, we focused on testing the 
performance of the D-score against human rat-
ers? judgment on ranking different MWEs by 
their degree of compositionality, as well as dis-
tinguishing the different degrees of composition-
ality for each sense in the case of multiple tags.  
The first step of the evaluation was to imple-
ment the algorithm in a program and run the tool 
on the 89 test MWEs we prepared. Fig. 1 illus-
trates the D-score distribution in a bar chart. As 
shown by the chart, the algorithm produces a 
widely dispersed distribution of D-scores across 
                                                 
6 Selected at random from the Lancaster semantic lexicon. 
7 http://ucrel.lancs.ac.uk/projects/assist/ 
the sample MWEs, ranging from 0.000032 to 
1.000000. For example, the tool assigned the 
score of 1.0 to the FOOD sense and 0.001 to the 
THIEF senses of ?tea leaf? successfully distin-
guishing the different degrees of compositional-
ity of these two senses. 
 
MWE Compositionality Distribution
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86
89 MWEs
D
-s
co
re
 
 
Fig 1: D-score distribution across 89 sample 
MWEs 
 
As shown in Fig. 1, some MWEs share the 
same scores, reflecting the limitation of the num-
ber of ranks that our algorithm can produce as 
well as the limited amount of semantic informa-
tion available from a lexicon. Nonetheless, the 
algorithm produced 45 different scores which 
ranked the MWEs into 45 groups (see the steps 
in the figure). Compared to the eleven scores 
used by the human raters, this provides a fine-
grained ranking of the compositionality.   
The primary issue in our evaluation is the ex-
tent to which the automatic ranking of the MWEs 
correlates with the manual ranking of them. As 
described in the previous section, we created a 
list of 89 manually ranked MWEs for this pur-
pose. Since we are mainly interested in the ranks 
rather than the actual scores, we examined the 
correlation between the automatic and manual 
rankings using Spearman?s correlation coeffi-
cient. (For the full ranking list, see Appendix). 
In the manually created list, each MWE was 
ranked by 3-6 human raters. In order to create a 
unified single test data of human ranking, we 
calculated the average of the human ranks for 
each MWE. For example, if two human raters 
give ranks 3 and 4 to a MWE, then its rank is 
(3+4)/2=3.5. Next, the MWEs are sorted by the 
averaged ranks in descending order to obtain the 
combined ranks of the MWEs. Finally, we sorted 
the MWEs by the D-score in the same way to 
obtain a parallel list of automatic ranks. For the 
calculation of Spearman?s correlation coefficient, 
if n MWEs are tied to a score (either D-score or 
the average manual ranks), their ranks were ad-
6
justed by dividing the sum of their ranks by the 
number of MWEs involved. Fig. 2 illustrates the 
correspondence between the adjusted automatic 
and manual rankings. 
 
Auto vs. Manual Ranks Comparison
(n=89, rho=0.2572)
0
20
40
60
80
100
0 20 40 60 80 100
auto ranks
m
an
ua
l r
an
ks
 
 
Fig. 2: Scatterplot of automatic vs. manual 
ranking. 
 
As shown in Fig. 2, the overall correlation seems 
quite weak. In the automatic ranking, quite a few 
MWEs are tied up to three ranks, illustrated by 
the vertically aligned points. The precise correla-
tion between the automatic and manual rankings 
was calculated using the function provided in R 
for Windows 2.2.1.  Spearman's rank correlation 
(rho) for these data was 0.2572 (p=0.01495), 
indicating a significant though rather weak posi-
tive relationship. 
In order to find the factors causing this weak 
correlation, we tested the correlation for those 
MWEs whose rank differences were less than 20, 
30, 40 and 50 respectively. We are interested to 
find out how many of them fall under each of the 
categories and which of their features affected 
the performance of the algorithm. As a result, we 
found 43, 54, 66 and 77 MWEs fall under these 
categories respectively, which yield different 
correlation scores, as shown in Table 1.  
 
numb of 
MWEs 
Percent 
(%) 
Rank 
diff 
rho-
score 
Sig. 
43 48.31 <20 0.9149 P<0.001 
54 60.67 <30 0.8321 P<0.001 
66 74.16 <40 0.7016 P<0.001 
77 86.52 <50 0.5084 P<0.001 
89 (total) 100.00 <=73 0.2572 P<0.02 
 
Table 1: Correlation coefficients corresponding 
different rank differences. 
 
As we expected, the rho decreases as the rank 
difference increases, but all of the four categories 
containing a total of 77 MWEs (86.52%) show 
reasonably high correlations, with the minimum 
score of 0.5084. 8 In particular, 66 of them 
(74.16%), whose ranking differences are less 
than 40, demonstrate a strong correlation with 
rho-score 0.7016, as illustrated by Fig. 3 
 
ScatterPlot of Auto vs. Man Ranks for 66 MWEs
(rank_diff < 40)
0
20
40
60
80
100
0 20 40 60 80 10auto ranks
m
an
 r
an
ks
0
 
 
Fig 3: ScatterPlot for 66 MWEs (rank_diff < 
40) which shows a strong correlation 
 
Our manual examination shows that the algo-
rithm generally pushes the highly compositional 
and non-compositional MWEs towards opposite 
ends of the spectrum of the D-score. For example, 
those assigned with score 1 include ?aid worker?, 
?audio tape? and ?unemployment figure?. On the 
other hand, MWEs such as ?tea leaf? (meaning 
thief), ?kick the bucket? and ?hot dog? are given 
a low score of 0.001. We assume these two 
groups of MWEs are generally treated as highly 
compositional and opaque MWEs respectively. 
However, the algorithm could be improved. A 
major problem found is that the algorithm pun-
ishes longer MWEs which contain function 
words. For example, ?make an appearance? is 
scored 0.000114 by the algorithm, but when the 
article ?an? is removed, it gets a higher score 
0.003608. Similarly, when the preposition ?up? 
is removed from ?keep up appearances?, it gets 
0.014907 compared to the original 0.000471, 
which would push up their rank much higher. To 
address this problem, the algorithm needs to be 
refined to minimise the impact of the function 
words to the scoring process. 
Our analysis also reveals that 12 MWEs with 
rank differences (between automatic and manual 
ranking) greater than 50 results in a degraded 
overall correlation. Table 2 lists these words, in 
which the higher ranks indicate higher composi-
tionality.  
 
                                                 
8 Salkind (2004: 88) suggests that r-score ranges 0.4~0.6, 
0.6~0.8 and 0.8~1.0 indicate moderate, strong and very 
strong correlations respectively. 
7
MWE Sem. Tag9 Auto 
rank 
Manual 
rank 
plough into A9- 53.5 3 
Bloody Mary F2 53.5 2 
pillow fight K6 26 80.5 
lollipop lady M3/S2 70 15 
cradle snatcher S3.2/T3/S2 73.5 17.5 
go bananas X5.2+++ 65 8.5 
make an appearance S1.1.3+ 2 58.5 
keep up appearances A8/S1.1.1 4 61 
sandwich course P1 69 11.5 
go bananas B2-/X1 68 10 
Eskimo roll M4 71.5 5 
in other words Z4 12.5 83 
 
Table 2: Twelve MWEs having rank differences 
greater than 50. 
 
Let us take ?pillow fight? as an example. The 
whole expression is given the semantic tag K6, 
whereas neither ?pillow? nor ?fight? as individ-
ual word is given this tag. In the lexicon, ?pil-
low? is classified as H5 {FURNITURE AND 
HOUSEHOLD FITTINGS} and ?fight? is as-
signed to four semantic categories including S8- 
{HINDERING}, X8+ {HELPING}, E3- {VIO-
LENT/ANGRY}, and K5.1 {SPORTS}. For this 
reason, the automatic score of this MWE is as 
low as 0.003953 on the scale of [0, 1]. On the 
contrary, human raters judged the meaning of 
this expression to be fairly transparent, giving it 
a high score of 8.5 on the scale of [0, 10]. Similar 
contrasts occurred with the majority of the 
MWEs with rank differences greater than 50, 
which are responsible for weakening the overall 
correlation. 
Another interesting case we noticed is the 
MWE ?pass away?. This MWE has two major 
senses in the semantic lexicon L1- {DIE} and 
T2- {END} which were ranked separately. Re-
markably, they were ranked in the opposite order 
by human raters and the algorithm. Human raters 
felt that the sense DIE is less idiomatic, or more 
compositional, than END, while the algorithm 
indicated otherwise. The explanation of this 
again lies in the semantic classification of the 
lexicon, where ?pass? as a single word contains 
the sense T2- but not L1-. Consequently, the 
automatic score for ?pass away? with the sense 
                                                 
                                                
9 Semantic tags occurring in Table 2: A8 (seem), A9 (giving 
possession), B2 (health and disease), F2 (drink), K6 (chil-
dren?s games and toys), M3 (land transport), M4 (swim-
ming), P1 (education), S1.1.1 (social actions), S1.1.3 (par-
ticipation), S2 (people), S3.2 (relationship), T3 (time: age), 
X1 (psychological actions), X5.2 (excited), Z4 (discourse 
bin) 
L1- is much lower (0.001) than that with the 
sense of T2- (0.007071). 
In order to evaluate our algorithm in compari-
son with previous work, we also tested it on the 
manual ranking list created by McCarthy et al
(2003).10 We found that 79 of the 116 phrasal 
verbs in that list are included in the Lancaster 
semantic lexicon. We applied our algorithm on 
those 79 items to compare the automatic ranks 
against the average manual ranks using the 
Spearman?s rank correlation coefficient (rho). As 
a result, we obtained rho=0.3544 with signifi-
cance level of p=0.001357. This result is compa-
rable with or better than most measures reported 
by McCarthy et al(2003). 
6 Discussion 
The algorithm we propose in this paper is dif-
ferent from previous proposed statistical methods 
in that it employs a semantic lexical resource in 
which the semantic field information is directly 
accessible for both MWEs and their constituent 
words. Often, typical statistical algorithms meas-
ure the semantic distance between MWEs and 
their constituent words by comparing their con-
texts comprising co-occurrence words in near 
context extracted from large corpora, such as 
Baldwin et als algorithm (2003). 
When we consider the definition of the com-
positionality as the extent to which the meaning 
of the MWE can be guessed based on that of its 
constituent words, a semantic lexical resource 
which maps MWEs and words to their semantic 
features provides a practical way of measuring 
the MWE compositionality. The Lancaster se-
mantic lexicon is one such lexical resource 
which allows us to have direct access to semantic 
field information of large number of MWE and 
single words. Our experiment demonstrates the 
potential value of such semantic lexical resources 
for the automatic measurement of MWE compo-
sitionality. Compared to statistical algorithms 
which can be affected by a variety of un-
controllable factors, such as size and domain of 
corpora, etc., an expert-compiled semantic lexi-
cal resource can provide much more reliable and 
?clean? lexical semantic information. 
However, we do not suggest that algorithms 
based on semantic lexical resources can substi-
tute corpus-based statistical algorithms. Rather, 
we suggest it as a complement to existing statis-
tical algorithms. As the errors of our algorithm 
 
10This list is available at website: 
http://mwe.stanford.edu/resources/  
8
reveal, the semantic information provided by the 
lexicon alone may not be rich enough for a very 
fine-grained distinction of MWE compositional-
ity. In order to obtain better results, this algo-
rithm needs to be combined with statistical tech-
niques. 
A limitation of our approach is language-
dependency. In order to port our algorithm to 
languages other than English, one needs to build 
similar semantic lexicon in those languages. 
However, similar semantic lexical resources are 
already under construction for some other lan-
guages, including Finnish and Russian (L?fberg 
et al, 2005; Sharoff et al, 2006), which will al-
low us to port our algorithm to those languages. 
7 Conclusion 
In this paper, we explored an algorithm based 
on a semantic lexicon for automatically measur-
ing the compositionality of MWEs. In our 
evaluation, the output of this algorithm showed 
moderate correlation with a manual ranking. We 
claim that semantic lexical resources provide 
another approach for automatically measuring 
MWE compositionality in addition to the exist-
ing statistical algorithms. Although our results 
are not yet conclusive due to the moderate scale 
of the test data, our evaluation demonstrates the 
potential of lexicon-based approaches for the 
task of compositional analysis. We foresee, by 
combining our approach with statistical algo-
rithms, that further improvement can be ex-
pected. 
8 Acknowledgement 
The work reported in this paper was carried 
out within the UK-EPSRC-funded ASSIST Pro-
ject (Ref. EP/C004574). 
References  
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, 
and Dominic Widdows. 2003. An Empirical Model 
of Multiword Expression Compositionality. In 
Proc. of the ACL-2003 Workshop on Multiword 
Expressions: Analysis, Acquisition and Treatment,  
pages 89-96, Sapporo, Japan. 
Colin Bannard, Timothy Baldwin, and Alex Las-
carides. 2003. A statistical approach to the seman-
tics of verb-particles. In Proc. of the ACL2003 
Workshop on Multiword Expressions: Analysis, 
Acquisition and Treatment, pages 65?72, Sapporo. 
Nicoletta Calzolari, Charles Fillmore, Ralph Grish-
man, Nancy Ide, Alessandro Lenci, Catherine 
MacLeod, and Antonio Zampolli. 2002. Towards 
best practice for multiword expressions in compu-
tational lexicons. In Proc. of the Third Interna-
tional Conference on Language Resources and 
Evaluation (LREC 2002), pages 1934?1940, Las 
Palmas, Canary Islands. 
Ann Copestake, Fabre Lambeau, Aline Villavicencio, 
Francis Bond, Timothy Baldwin, Ivan A. Sag, and 
Dan Flickinger. 2002. Multiword expressions: Lin-
guistic precision and reusability. In Proc. of the 
Third International Conference on Language Re-
sources and Evaluation (LREC 2002), pages 1941?
1947, Las Palmas, Canary Islands. 
Matthias  Gamer. 2005. The irr Package: Various Co-
efficients of Interrater Reliability and Agreement. 
Version 0.61 of 11 October 2005.  Available from:   
cran.r-project.org/src/contrib/Descriptions/irr.html 
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proc. of the 37th Annual 
Meeting of the ACL, pages 317?324, College Park, 
USA. 
Laura L?fberg, Scott Piao, Paul Rayson, Jukka-Pekka 
Juntunen, Asko Nyk?nen, and Krista Varantola. 
2005. A semantic tagger for the Finnish language. 
In Proc. of the Corpus Linguistics 2005 conference, 
Birmingham, UK. 
Christopher D. Manning and Hinrich Sch?tze. 2000. 
Foundations of Statistical Natural Language Proc-
essing. The MIT Press, Cambridge, Massachusetts. 
Diana McCarthy, Bill Keller, and John Carroll. 2003. 
Detecting a continuum of compositionality in 
phrasal verbs. In Proc. of the ACL-2003 Workshop 
on Multiword Expressions: Analysis, Acquisition 
and Treatment, pages 73?80, Sapporo, Japan. 
Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Proc. 
of the 2nd Conference on Empirical Methods in 
Natural Language Processing , Providence, USA. 
Geoffrey Nunberg, Ivan A. Sag, and Tom Wasow. 
1994. Idioms. Language, 70: 491?538. 
Scott S.L. Piao, Paul Rayson, Dawn Archer and Tony 
McEnery. 2004. Evaluating Lexical Resources for 
a Semantic Tagger. In Proc. of LREC-04, pages 
499?502, Lisbon, Portugal. 
Scott S.L. Piao, Dawn Archer, Olga Mudraya, Paul 
Rayson, Roger Garside, Tony McEnery and An-
drew Wilson. 2005a. A Large Semantic Lexicon 
for Corpus Annotation. In Proc. of the Corpus Lin-
guistics Conference 2005, Birmingham, UK. 
Scott S.L. Piao., Paul Rayson, Dawn Archer, Tony 
McEnery. 2005b. Comparing and combining a se-
mantic tagger and a statistical tool for MWE ex-
traction. Computer Speech and Language, 19, 4: 
378?397. 
9
Paul Rayson, Dawn Archer, Scott Piao, and Tony 
McEnery. 2004. The UCREL Semantic Analysis 
System. In Proc. of LREC-04 Workshop: Beyond 
Named Entity Recognition Semantic Labeling for 
NLP Tasks, pages 7?12, Lisbon, Portugal. 
Susanne Riehemann. 2001. A Constructional Ap-
proach to Idioms and Word Formation. Ph.D. the-
sis, Stanford University, Stanford. 
 Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann 
Copestake, and Dan Flickinger. 2002. Multiword 
Expressions: A Pain in the Neck for NLP. In Proc. 
of the 3rd International Conference on Intelligent 
Text Processing and Computational Linguistics 
(CICLing-2002), pages 1?15, Mexico City, Mexico. 
Neil J. Salkind. 2004. Statistics for People Who Hate 
Statistics. Sage: Thousand Oakes, US. 
Serge Sharoff, Bogdan Babych, Paul Rayson, Olga 
Mudraya and Scott Piao. 2006. ASSIST: Auto-
mated semantic assistance for translators. Proceed-
ings of EACL 2006, pages 139?142, Trento, Italy. 
Patrick E. Shrout and Joseph L. Fleiss. 1979. Intra-
class Correlations: Uses in Assessing Rater Reli-
ability. Psychological Bulletin (2), 420?428. 
Sriram Venkatapathy and Aravind K. Joshi. 2005. 
Measuring the relative compositionality of verb-
noun (V-N) collocations by integrating features. In 
Proc. of Human Language Technology Conference 
and Conference on Empirical Methods in Natural 
Language Processing (HLT/EMNLP 2005), pages 
899?906, Vancouver, Canada. 
 
Appendix: Manual vs. Automatic Ranks 
of Sample MWEs 
The table below shows the human and auto-
matic rankings of 89 sample MWEs. The MWEs 
are sorted in ascending order by manual average 
ranks. The top items are supposed to be the most 
compositional ones. For example, according to 
the manual ranking, facial expression is the most 
compositional MWE while tea leaf is the most 
opaque one. This table also shows that some 
MWEs are tied up with the same ranks. For the 
definitions of the full semantic tagset, see web-
site http://www.comp.lancs.ac.uk/ucrel/usas/. 
 
MWE Tag Sem tag Man 
rank 
Auto. 
rank 
facial expression B1 1 9 
aid worker S8/S2 2 4 
audio tape K3 3.5 4 
leisure activities K1 3.5 36.5 
advance warning T4/Q2.2 5 36.5 
living space H2 6 51 
in other words Z4 7 77.5 
unemployment fig-
ures 
I3.1/N5 8 4 
camera angle Q4.3 9.5 45 
pillow fight K6 9.5 64 
youth club S5/T3 11.5 4 
petrol station M3/H1 11.5 36.5 
palm tree L3 13 9 
rule book G2.1/Q4.1 14 4 
ball boy K5.1/S2.2 15 13 
goal keeper K5.1/S2 16.5 4 
kick in E3- 16.5 36.5 
ventilation shaft H2 18 47 
directory enquiries Q1.3 19 14 
phone box Q1.3/H1 21 18.5 
lose balance M1 21 53 
bend the rules A1.7 21 54.5 
big nose X7/X2.4 23 67 
quantity control N5/A1.7 24 11.5 
act of God S9 25 36.5 
air bag A15/M3 26 62.5 
mind stretching A12 27 59 
plain clothes B5 28 36.5 
keep up appearances A8/S1.1.1 29 86 
examining board P1 30 23 
open mind X6 31.5 49 
make an appearance S1.1.3+ 31.5 88 
cable television Q4.3 33 15 
king size N3.2 34 36.5 
action point X7 35 61 
keep tight rein on A1.7 36 28 
noughts and crosses K5.2 37 77.5 
tea leaf L3/F2 38 4 
single minded X5.1 39.5 77.5 
window dressing I2.2 39.5 77.5 
street girl G1.2/S5 42 36.5 
just over the horizon S3.2/S2.1 42 60 
pressure group T1.1.3 42 16.5 
air proof O4.1 44.5 57.5 
heart of gold S1.2.2 44.5 77.5 
lose heart X5.2 46 26 
food for thought X2.1/X5.1 47 89 
play part S8 48 68 
look down on S1.2.3 49 77.5 
arm twisting Q2.2 50 36.5 
take into account A1.8 51 69 
kidney bean F1 52 9 
come alive A3+ 53 52 
break new ground T3/T2 54 54 
make up to S1.1.2 55 65 
by virtue of C1 56.5 36.5 
snap shot A2.2 56.5 27 
pass away L1- 58 77.5 
long face E4.1 59 77.5 
bossy boots S1.2.3/S2 60 77.5 
plough into M1/A1.1.2 61 11.5 
kick in T2+ 62 50 
animal magnetism S1.2 63 55.5 
sixth former P1/S2 64 77.5 
pull the strings S7.1 65 62.5 
couch potato A1.1.1/S2 66 77.5 
think tank S5/X2.1 67 36.5 
come alive X5.2+ 68 24 
hot dog F1 69 77.5 
cheap shot G2.2-/Q2.2 70 66 
10
rock and roll K2 71 48 
bright as a button S3.2/T3/S2 72.5 87 
cradle snatcher X9.1+ 72.5 16.5 
alpha wave B1 74 77.5 
lollipop lady M3/S2 75 20 
pass away X5.2+ 76.5 57.5 
plough into T2- 76.5 36.5 
piece of cake P1 78.5 77.5 
sandwich course A12 78.5 21 
go bananas B2-/X1 80 22 
go bananas X5.2+++ 81.5 36.5 
go bananas E3- 81.5 25 
kick the bucket L1 83 77.5 
on the wagon F2 84 36.5 
Eskimo roll M4 85 18.5 
acid house K2 86 46 
plough into A9- 87 36.5 
Bloody Mary F2 88 36.5 
tea leaf G2.1-/S2mf 89 77.5 
 
11
