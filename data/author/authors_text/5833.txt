Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 617?624
Manchester, August 2008
Investigating Statistical Techniques for Sentence-Level Event
Classification
Martina Naughton
School of Computer Science,
University College Dublin,
Belfield, Dublin 4, Ireland
Nicola Stokes
NICTA Victoria Laboratory,
University of Melbourne,
Victoria, Australia
Joe Carthy
School of Computer Science
University College Dublin
Belfield, Dublin 4, Ireland
Abstract
The ability to correctly classify sentences
that describe events is an important task for
many natural language applications such
as Question Answering (QA) and Sum-
marisation. In this paper, we treat event
detection as a sentence level text classifi-
cation problem. We compare the perfor-
mance of two approaches to this task: a
Support Vector Machine (SVM) classifier
and a Language Modeling (LM) approach.
We also investigate a rule based method
that uses hand crafted lists of terms derived
from WordNet. These terms are strongly
associated with a given event type, and can
be used to identify sentences describing in-
stances of that type. We use two datasets in
our experiments, and evaluate each tech-
nique on six distinct event types. Our re-
sults indicate that the SVM consistently
outperform the LM technique for this task.
More interestingly, we discover that the
manual rule based classification system is
a very powerful baseline that outperforms
the SVM on three of the six event types.
1 Introduction
Event detection is a core Natural Language Pro-
cessing (NLP) task that focuses on the automatic
identification and classification of various event
types in text. This task has applications in au-
tomatic Text Summararisation and Question An-
swering (QA). For example, event recognition is
a core task in QA since the majority of web user
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
questions have been found to relate to events and
situations in the world (Saur?? et al, 2005). For
complex questions such as How many people were
killed in Baghdad in March?, QA systems often
rely on event detection systems to identify all rel-
evant events in a set of documents before formu-
lating an answer. More recently, much research in
summarisation has focused on the use of phrasal
concepts such as events to represent sentences in
extractive summarisation systems. Specifically,
(Filatova and Hatzivassiloglou, 2004) use event-
based features to represent sentences and shows
that this approach improves the quality of the final
summaries when compared with a baseline bag-of-
words approach.
In this paper, we investigate the use of statistical
methods for identifying the sentences in a docu-
ment that describe one or more instances of a spec-
ified event type. We treat this task as a text classi-
fication problem where each sentence in a given
document is either classified as containing an in-
stance of the target event or not. We view this
task as a filtering step in a larger pipeline NLP ar-
chitecture (e.g. a QA system) which helps speed
up subsequent processing by removing irrelevant,
non-event sentences.
Two event detection approaches are explored in
this paper. More specifically, we train a Support
Vector Machine (SVM) using a variety of term,
lexical and additional event based features to en-
code each train/test instance. We also adopt a prob-
abilistic language modeling approach that captures
how text within sentences that describe event in-
stances is likely to be generated. We estimate a
series of models using three well-known smooth-
ing approaches, including LaPlace, Jelinek-Mercer
and Absolute Discounting Smoothing. Their over-
all behavior on classification performance is ex-
617
amined. One advantage of language modeling for
text classification is that instead of explicitly pre-
computing features and selecting a subset based
on arbitrary decisions (as is often the case with
standard classification learning approaches such as
an SVM), the language modeling approach simply
considers all terms occurring in the text as candi-
date features, and implicitly considers the contri-
bution of every feature in the final model. Thus,
language modeling approaches avoids a potentially
error-prone feature selection process.
Event classification at a sentence level is a chal-
lenging task. For example, if the target event is
?Die?, we want our system to extract sentences
like ?5 people were killed in the explosion.? and
?A young boy and his mother were found dead on
Wednesday evening.?. However, it also needs to
detect complex cases like: ?An ambulance rushed
the soldier to hospital, but efforts to save him
failed.? and reject instances like ?Fragmentation
mines have a killing range of 100 feet.?. It seems
intuitive that a na??ve system that selects only sen-
tences that contain terms with senses connected
with death like ?kill?, ?die? or ?execute? as pos-
itive instances would catch many positive cases.
However, there are instances where this approach
would fail. In this work we evaluate the effective-
ness of such a shallow NLP approach, by devel-
oping a manual rule based system that finds sen-
tences connected to a target event type using a
hand crafted list of terms created with senses found
in WordNet.
We use two datasets in our experiments. The
first is the ACE 2005 Multilingual Training Corpus
(Walker et al, 2006) that was annotated for 33 dif-
ferent event types. However, within the ACE data
the number of instances referring to each event
type is somewhat limited. For this reason, we se-
lect the six types with the highest frequency in the
data. These include ?Die?, ?Attack?, ?Transport?,
?Meet?, ?Injure? and ?Charge-indict? types. The
second corpus is a collection of articles from the
Iraq Body Count (IBC) database
1
annotated for
the ?Die? event. This dataset arose from a larger
humanitarian project that focuses on the collec-
tion of fatalities statistics from unstructured news
data. We use this additional corpus to augment the
amount of data used for training and testing the
?Die? event type, and to investigate the use of extra
training data on overall classification performance.
1
http://www.iraqbodycount.org/
Overall, our results demonstrate that the trained
SVM proves to be more effective than the LM
based approach for this task across all event types.
We also show that our baseline system, a hand
crafted rule-based system, performs surprisingly
well. The remainder of this paper is organised as
follows. Section 2 covers related work. We con-
tinue with details of the datasets used in the exper-
iments in Section 3. Section 4 describes our event
classification approaches while Section 5 presents
their results. We conclude with a discussion of ex-
perimental observations and opportunities for fu-
ture work in Section 6.
2 Background and Related Work
Event detection, in the context of news stories, has
been an active area of research for the best part of
ten years. For example, the NIST sponsored Topic
Detection and Tracking (TDT) project, which be-
gan in 1998 investigated the development of tech-
nologies that could detect novel events in seg-
mented or unsegmented news streams, and track
the progression of these event over time (Allan et
al., 1998). Although this project ended in 2004,
event detection is still investigated by more re-
cently established projects such as the Automatic
Content Extraction (ACE) program, and in do-
mains outside of news text such as Biomedical
Text Processing (Murff et al, 2003).
The aim of the TDT First Story Detection (FSD)
or New Event Detection (NED) task was to flag
documents that discuss breaking news stories as
they arrive on a news stream. Dragon Systems
adopted a LM approach to this task (Allan et al,
1998; Yamron et al, 2002) building discriminator
topic models from the collection and representing
documents using unigram term frequencies. Then
they used a single-pass clustering algorithm to de-
termine the documents that describe new events.
The overall goal of the TDT Event Tracking task
was to track the development of specific events
over time. However, these TDT tasks were some-
what restrictive in the sense that detection is car-
ried out at document level. Our work differs from
TDT research since event detection is performed
at a sentence level where the amount of data to
build discriminate models for recognising event in-
stances is far more limited.
The goal of the ACE Event Detection and
Recognition task is to identify all event instances
(as well as the attributes and participants of each
618
Table 1: ACE Corpus Statistics
Die Injure Attack Meet Transport Charge-Indict
Number of Documents 154 50 235 84 181 43
Avg. document length 29.19 29.74 29.62 31.41 32.78 14.81
Avg. event instances per document 2.31 1.64 3.55 1.55 2.55 1.72
Avg. event instances per sentence 1.13 1.11 1.12 1.02 1.08 1.03
Table 2: IBC Corpus Statistics
IBC Corpus
Number of Documents 332
Number of Sources 77
Avg. document length 25.98
Avg. events per document 4.6
Avg. events per sentence 1.14
instance) of a pre-specified set of event types. An
ACE event is defined as a specific occurence in-
volving zero or more ACE entities
2
, values and
time expressions. Two spans of text are used to
identify each event: the event trigger and the event
mention. An event trigger or anchor is the word
that most clearly expresses its occurrence. In many
cases, this will be the main verb in the event men-
tion. It can also appear as a noun (?The meeting
lasted 5 hours.?) or an adjective (?the dead men
. . . ?). The event mention is the sentence that de-
scribes the event. Even though the task of iden-
tifying event mentions is not directly evaluated in
ACE, systems still need to identify them so that
the various attributes and participants within the
mention can be extracted. The algorithms evalu-
ated in this paper can also be applied to the detec-
tion of event mentions that contain the ACE events.
Overall five sites participated in this task in 2005.
The most similar work to that describe in this pa-
per is detailed in (Ahn, 2006), who treats the task
of finding all event triggers (used to identify each
event) as a word classification task where the task
is to classify every term in a document with a la-
bel defined by 34 classes. Features used included
various lexical, WordNet, dependency and related
entity features.
3 Corpora
The ACE 2005 Multilingual Corpus was annotated
for Entities, Relations and Events. It consists of
articles originating from six difference sources in-
cluding Newswire (20%), Broadcast News (20%),
Broadcast Conversation (15%), Weblog (15%),
2
An ACE Entity is an entity identified using guidelines
outlined by ACE Entity Detection and Recognition task.
Usenet Newsgroups (15%) and Conversational
Telephone Speech (15%). Statistics on the docu-
ments in this collection are presented in Table 1.
We evaluate our methods on the following event
types which have a high number of instances in the
collection: ?Die?, ?Attack?, ?Transport?, ?Meet?,
?Injure? and ?Charge-indict?.
The data we use from the IBC database con-
sists of Newswire articles gathered from 77 differ-
ent news sources. Statistics describing this dataset
are contained in Table 2. To obtain a gold standard
set of annotations for articles in the IBC corpus,
we asked ten volunteers to mark up all the ?Die?
event instances. To maintain consistency across
both datasets, events in the IBC corpus were iden-
tified in a manner that conforms to the ACE an-
notation guidelines. In order to approximate the
level of inter-annotation agreement achieved for
the IBC corpus, two annotators were asked to an-
notate a disjoint set of 250 documents. Inter-rater
agreements were calculated using the kappa statis-
tic that was first proposed by (Cohen, 1960). Using
the annotated data, a kappa score of 0.67 was ob-
tained, indicating that while the task is difficult for
humans the data is still useful for our training and
test purposes. Discrepancies were adjudicated and
resolved by an independent volunteer.
4 Event Detection as Classification
We treat the task of determining whether a given
sentence describes an instance of the target event
as a binary text classification task where it is as-
signed one of the following classes:
? On-Event Sentence: a sentence that contains
one or more instances of the target event type.
? Off-Event Sentence: a sentence that does not
contain any instances of the target event type.
4.1 A Machine Learning Approach
In an attempt to develop a gold standard ap-
proach for this task we use Support Vector Ma-
chines (SVM) to automatically classify each in-
stance as either an ?on-event? or ?off-event? sen-
tence. SVMs have been shown to be robust in
619
classification tasks involving text where the dimen-
sionality is high (Joachims, 1998). Each sentence
forms a train/test instance for our classifier and is
encoded using the following set of features.
Terms: Stemmed terms with a frequency in the
training data greater than 2 were used as a term
feature. Stopwords were not used as term features.
Noun Chunks: All noun chunks (e.g. ?ameri-
can soldier?) with a frequency greater than 2 in the
training data were also used as a feature.
Lexical Information: The presence or absence of
each part of speech (POS) tag and chunk tag was
used as a feature. We use the Maximum Entropy
POS tagger and Chunker that are available with the
C&C Toolkit (Curran et al, 2007). The POS Tag-
ger uses the standard set of grammatical categories
from the Penn Treebank and the chunker recog-
nises the standard set of grammatical chunk tags:
NP, VP, PP, ADJP, ADVP and so on.
Additional Features: We added the following
additional features to the feature vector: sen-
tence length, sentence position, presence/absence
of negative terms (e.g. no, not, didn?t, don?t, isn?t,
hasn?t), presence/absence of a modal terms (e.g.
may, might, shall, should, must, will), a looka-
head feature that indicates whether the next sen-
tence is an event sentence, a look-back feature in-
dicating whether or not the previous sentence is
an event sentence and the presence/absence of a
time-stamp. Time-stamps were identified using in-
house software developed by the Language Tech-
nology Group at the University of Melbourne
3
.
In the past, feature selection methods have been
found to have a positive effect on classification ac-
curacy of text classification tasks. To examine the
effects of such techniques on this task, we use In-
formation Gain (IG) to reduce the number of fea-
tures used by the classifier by a factor of 2.
4.2 Language Modeling Approaches
The Language modeling approach presented here
is based on Bayesian decision theory. Consider the
situation where we wish to classify a sentence s
k
into a category c ? C = {C
1
. . . . . . C
|C|
}. One
approach is to choose the category that has the
largest posterior probability given the training text:
c
?
= argmax
c?C
{Pr(c|s
k
)} (1)
Specifically, we construct a language model
LM(c
i
) for each class c
i
. All models built
3
http://www.cs.mu.oz.au/research/lt/
are unigram models that use a maximum like-
lihood estimator to approximate term probabili-
ties. According to this model (built from sentences
{s
1
. . . s
m
} belonging to class c
i
in the training
data) we can calculate the probability that term w
was generated from class c
i
as:
P (w|LM(c
i
)) =
tf(w, c
i
)
|c
i
|
(2)
where tf(w, c
i
) is the term frequency of term w in
c
i
(that is, {s
1
. . . s
m
}) and |c
i
| is the total number
of terms in class c
i
. We make the usual assump-
tions that word co-occurences are independent. As
a result, the probability of a sentence is the product
of the probabilities of its terms. We calculate the
probability that a given test sentence s
k
belongs to
class c
i
as follows:
P (s
k
|LM(c
i
)) =
?
w?s
k
P (w|LM(c
i
)) (3)
However, this model will generally under-
estimate the probability of any unseen word in the
sentence, that is terms that do not appear in the
training data used to build the language model. To
combat this, smoothing techniques are used to as-
sign a non-zero probability to the unseen words,
which improves the accuracy of the overall term
probability estimation. Many smoothing meth-
ods have been proposed over the years, and in
general, they work by discounting the probabili-
ties of seen terms and assign this extra probability
mass to unseen words. In IR, it has been found
that the choice of smoothing method significantly
affects retrieval performance (Zhai and Lafferty,
2001; Kraaij and Spitters, 2003). For this reason,
we experiment with the Laplace, Jelinek-Mercer
and Absolute Discounting Smoothing methods,
and compare their effects on classification perfor-
mance in Section 5.
For this classification task, we normalise all
numeric references, locations, person names and
organisations to ?DIGIT?, ?LOC?, ?PER?, and
?ORG? respectively. This helps to reduce the di-
mensionality of our models, and improve their
classification accuracy, particular in cases where
unseen instances of these entities occur in the test
data.
4.3 Baseline Measures
We compare the performance our ML and LM ap-
proaches to the following plausible baseline sys-
tems: Random assigns each instance (sentence)
620
randomly to one of the possible classes. While
Majority Class Baseline assigns each instance to
the class that is most frequent in the training data.
In our case, this is the ?off-event? class.
According to the ACE annotation guidelines
4
event instances are identified in the text by find-
ing event triggers that explicitly mark the occur-
rence of the event. As a result, each event instance
tagged in our datasets have a corresponding trigger
that the annotators used to identify it. For exam-
ple, terms like ?killing?, ?death? and ?murder? are
common triggers used to identify the ?Die? event
type. Therefore, we expect that a system that se-
lects sentences containing one or more candidate
trigger terms as positive ?on-event? sentences for
a given event type, would be a suitable baseline
for this task. To investigate this further we add the
following baseline system:
Manual Trigger-Based Classification: For each
event type, we use WordNet to manually create
a list of terms that are synonyms or hyponyms
(is a type of) of the event type. For example, in
the case of the ?Meet? and ?Die? events common
trigger terms include {?encounter?, ?visit?, ?re-
unite?} and {?die?, ?suicide?, ?assassination?} re-
spectively. We classify each sentence for a given
event type as follows: if a sentence contains one
or more terms in the trigger list for that event type
then it is assigned to the ?on-event? class for that
type. Otherwise it is assigned to the ?off-event?
class. Table 3 contains the number of trigger terms
used for each event
5
.
Table 3: Trigger term lists for the six event types used in the
experiments.
Event Type Number of triggers terms
Die 29
Transport 14
Meet 12
Injure 10
Charge-Indict 8
Attack 8
5 Evaluation Methodology & Results
A standard measure for classification performance
is classification accuracy. However for corpora
where the class distribution is skewed (as is the
case in our datasets where approx. 90% of the in-
4
Available at http://projects.ldc.upenn.
edu/ace/annotation/
5
The lists of the trigger terms used for each event type are
available at http://inismor.ucd.ie/
?
martina/
stances belong to the ?off-event? class) this mea-
sure can be misleading. So instead we have used
precision, recall and F1 to evaluate each technique.
If a is the number of sentences correctly classi-
fied by a system to class i, b is the total num-
ber of sentences classified to class i by a system,
and c is the total number of human-annotated sen-
tences in class i. Then the precision and recall
for class i can be defined as follows: Prec
i
=
a
b
,
Recall
i
=
a
c
. Finally, F1 (the harmonic mean be-
tween precision and recall) for class i is defined as
F1
i
=
2?Prec
i
?Recall
i
Prec
i
+Recall
i
. In the results presented in
this section we present the precision recall and F1
for each class as well as the overall accuracy score.
Results: In our experiments we use a relatively
efficient implementation of an SVM called the Se-
quential Minimal Optimisation (SMO) algorithm
(Platt, 1999) which is provided by the Weka frame-
work (Witten and Frank, 2000). Results presented
in this section are divided into two parts. In the
first part, all results were obtained using the IBC
dataset where the target event type is ?Die?. We
provide a more detailed comparison of the perfor-
mance of each algorithm using this type as more
data was available for it. In the second section,
we examine the effectiveness of each approach for
all six event types (listed in Section 3) using the
ACE data. All reported scores were generated us-
ing 50:50 randomly selected train/test splits aver-
aged over 5 runs.
As part of the first set of results Table 4
shows the precision, recall and F1 achieved for
the ?on-event? and ?off-event? classes as well
as the overall classification accuracy obtained by
each approach. Two variations of the SVM were
built. The first version (denoted in the table
by SVM(All Features IG)) was built using all
terms, nouns chunks, lexical and additional fea-
tures to encode each train/test instance where the
features were reduced using IG. In the second
version, the same features were used but no fea-
ture reduction was carried out (denoted in the
table by SVM(All Features)). LangModel(JM),
LangModel(DS) and LangModel(LP) represent
language models smoothed using Jelinek-Mercer,
Discount Smoothing and LaPlace techniques re-
spectively. Overall these results suggest that the
SVM using IG for feature selection is the most ef-
fective method for correctly classifying both ?on-
event? and ?off-event? sentences. Specifically, it
achieves 90.23% and 96.70% F1 score for these
621
Table 4: % Precision, Recall and F1 for both classes as well as the classification accuracy achieved by all algorithms using a
50:50 train/test split where the target event type is ?Die?.
Algorithm
On-Event Class Off-Event Class
Accuracy
Precision Recall F1 Precision Recall F1
SVM(All Features IG) 90.61 89.87 90.23 96.15 97.26 96.70 94.60
SVM(All Features) 89.63 88.52 89.06 96.08 96.49 96.28 94.45
Trigger-Based Classification 83.10 93.34 87.92 97.25 93.24 95.20 93.09
LangModel(DS) 63.11 82.4 71.46 93.13 83.16 87.86 82.98
LangModel(JM) 59.46 86.01 70.31 94.22 79.53 86.25 81.22
LangModel(LP) 59.22 79.56 67.89 91.89 80.88 86.03 80.54
Majority Class (?off-event?) 0.0 0.0 0.0 74.50 100.00 85.38 74.17
Random 26.57 51.73 35.10 75.58 50.0 60.18 50.34
Table 5: % F1 for both classes achieved by the SVM using
different combinations of features.
Features F1(On-Event) F1(Off-Event)
terms 89.52 96.43
terms + nc 89.58 96.31
terms + nc + lex 89.62 96.44
All Features 90.23 96.70
classes respectively. When IG is not used we see a
marginal decrease of approx. 1% in these scores.
The fact that both versions of the SVM obtain
approx. 90% F1 scores for the ?on-event? class
is extremely encouraging when you consider the
large skew in class distribution that is present here
(i.e., the majority of training instances belong to
the?off-event? class).
To examine the effects of the various features on
overall performance, we evaluated the SVM using
different feature combinations. These results are
shown in Table 5 where ?terms?, ?nc?, and ?lex?
denote the terms, noun chunks and lexical feature
sets respectively. ?All Features? includes these fea-
tures and the ?Additional Features? described in
Section 4.1. One obvious conclusion from this ta-
ble is that terms alone prove to be the most valu-
able features for this task. Only a little increase in
performance is achieved by adding the other fea-
ture sets.
The graphs in Figure 1 shows the % F1 of both
classes achieved by all methods using varying lev-
els of training data. From these graphs we see
that the SVM obtains over 80% F1 for the ?on-
event? class and over 90% F1 for the ?off-event?
class when only 10% of the training data is used.
These results increase gradually when the amount
of training data increases. For levels of train-
ing data greater than 30% the SVM consistency
achieves higher F1 scores for both classes than all
other methods for this task.
In general, the language modeling based tech-
niques are not as effective as the SVM approach
for this classification task. However, from Ta-
ble 4 we see that all language models achieve ap-
prox. 70% F1 for the ?on-event? class and ap-
prox. 86% F1 for the ?off-event? class when only
50% of the IBC data is used to build the mod-
els. This is encouraging since they require little
or no feature engineering and less time to train.
Models smoothed with the Laplace method tend to
have the least impact out of the three model vari-
ations. This is due to the fact that this method
assigns the same probability to all unseen terms.
Thus, a term like ?professor? that may only occur
once in the dataset has the same likelihood of oc-
curring in an ?on-event? sentence as a term like
?kill? that has a very high frequency in the dataset.
In contrast, the Jelinek-Mercer and Absolute Dis-
counting smoothing methods estimate the proba-
bility of unseen terms according to a background
model built using the entire collection. Therefore,
the probabilities assigned to unseen words is pro-
portional to their global distribution in the entire
corpus. Consequently, the probabilities assigned
to unseen terms tend to be more reliable approxi-
mations of true term probabilities.
Overall, the trigger-based classification base-
line approach performs very well achieving simi-
lar scores to the SVM. This suggests that select-
ing sentences with terms associated with the target
event is an effective way of solving this problem.
That said, it still makes mistakes that the SVM and
language models have the ability to correct. For
example, many sentences that contain terms like
?suicide? and ?killing? as part of a noun phase (e.g.
?suicide driver? or ?killing range?) do not report a
death. The trigger classification baseline will clas-
sify these as an ?on-event? instances whereas the
SVM correctly places them in the ?off-event? cat-
egory. More interesting are the cases missed by
the trigger classification baseline and SVM that are
622
20
30
40
50
60
70
80
90
100
10 20 30 40 50 60 70 80 90
F
1
(
O
n
-
E
v
e
n
t
)
Percentage Training
SVM
Trigger-Based Classificaton
LangModel(LP)
LangModel(JM)
LangModel(DS)
Random
20
30
40
50
60
70
80
90
100
10 20 30 40 50 60 70 80 90
F
1
(
O
f
f
-
E
v
e
n
t
)
Percentage Training
SVM
Trigger-Based Classificaton
LangModel(LP)
LangModel(JM)
LangModel(DS)
Random
Figure 1: % F1 for the on-event (top) and off-event (bottom)
classes for all methods using varying levels of training data
where the target event is ?Die?.
corrected by the language models. These include
sentences like ?Three bodies were found yesterday
in central Baghdad.? and ?If the Americans have
killed them, then why dont they show the tape.?. In
fact, it turns out that over 50% of the errors pro-
duced by the SVM and manual trigger-based ap-
proach when the target event is ?Die? are classified
correctly by the language models. Although this is
encouraging, the overall error rate of the language
modeling approach is too high to rely on it alone.
However, this evidence suggests that it may prove
useful in the future to somehow combine the pre-
dictions of all three approaches in a way that im-
proves overall classification performance.
We now move on to the second part of the exper-
iments. Here, we present the results for six event
types (as listed in Section 3) using only data from
ACE corpus. Figure 2 shows the % F1 of the ?on-
event? class achieved by all approaches for each
event type. We have omitted the % F1 scores for
the ?off-event? class as they do not vary signifi-
cantly across event types and are similar to those
reported in Table 4. The SVM, language mod-
els, trigger-based and random baselines achieve
0
20
40
60
80
100
Die Charge-Indict Meet Attack Injure Transport
F
1
(
O
n
-
E
v
e
n
t
)
ACE Event Types
SVM
Trigger-Based Classificaton
LangModel(LP)
LangModel(JM)
LangModel(DS)
Random
Figure 2: % F1 of the ?on-event? class achieved by
all methods for the six ACE event types.
approx. 96%, 95%, 86% and 60% ?off-event? F1
scores respectively across all event types.
On the other hand, Figure 2 demonstrates that
the performance of each approach for the ?on-
event? class varies considerably across the event
types. For instance, the trigger-based classification
baseline out-performs all other approaches achiev-
ing over 60% F1 score for the ?Meet?, ?Die? and
?Charge-Indict? types. However for events like
?Attack? and ?Transport? this baselines F1 score
drops to approx. 20% thus achieving scores that
are only marginally above the random baseline. In-
terestingly, we notice that although it performs bet-
ter for events like ?Meet? and ?Charge-Indict?, the
number of trigger terms used to detect these types
is much smaller than the number used for the ?At-
tack? and ?Transport? types (see Table 3). This in-
dicates that event types where this simple baseline
performs well are those where the vocabulary used
to describe them is small. Event types where it
achieves poor results are broader types like ?Trans-
port? and ?Attack? that cover a larger spectrum of
event instances from heterogeneous contexts and
situations. However, we see from Figure 2 that the
SVM performs well on such event types and as a
result out-performs the trigger-based selection pro-
cess by approximately a factor of 4 for the ?Attack?
event and a factor of 2 for the ?Transport? event.
When we compare both datasets we find that the
ACE data is made up of newswire articles, Broad-
cast news, broadcast conversational texts, weblogs,
usenet newsgroup texts and conversational tele-
phone speech that has been transcribed, whereas
the IBC corpus consists mainly of newswire arti-
cles reporting fatalities during the Iraqi War. As
623
a result, event instances in the ACE data describ-
ing the ?Die? event type are likely to report fatali-
ties not only from Iraq but also from more diverse
contexts and situations. To investigate how perfor-
mance differs for the ?Die? event type across these
datasets, we compare the IBC results in Table 4
with the ACE results in Figure 2. We find that the
F1 scores of the ?off-event? class are not affected
much. However, the F1 scores for the ?on-event?
class for the SVM and trigger-based baseline are
reduced by margins of approx. 12% and 5% re-
spectively. We also notice that the performance of
the unigram language models are reduced signifi-
cantly by a factor of 2 indicating that they struggle
to approximate accurate term probabilities when
the vocabulary is more diverse and the amount of
training data is limited.
6 Discussion
Sentence level event classification is an important
first step for many NLP applications such as QA
and summarisation systems. For each event type
used in our experiments we treated this as a binary
classification task and compared a variety of ap-
proaches for identifying sentences that described
instances of that type. The results showed that the
trained SVM was more effective than the language
modeling approaches across all event types. An-
other interesting contribution of this paper is that
the trigger-based classification baseline performed
better than expected. Specifically, for three of the
six event types it out-performed the trained SVM.
This suggests that although there are cases where
such terms appear in sentences that do not describe
instances of a given type (for instance, ?The boy
was nearly killed.?), these cases are in the minor-
ity. However, the success of this baseline is some-
what dependent on the nature of the event in ques-
tion. For broader events like ?Transport? and ?At-
tack? where the trigger terms can be harder to pre-
dict, it performs quiet poorly. Therefore, as part
of future work, we hope to investigate ways of au-
tomating the creation of these term lists for a spec-
ified event type as this proved to be an effective
approach to this task.
Acknowledgements. This research was sup-
ported by the Irish Research Council for Science,
Engineering & Technology (IRCSET) and IBM
under grant RS/2004/IBM/1. The authors also
wishes to thank the members of the Language
Technology Research Group at the University of
Melbourne and NICTA for their helpful discus-
sions regarding this research.
References
Ahn, David. 2006. The stages of event extraction. In Pro-
ceedings of the ACL Workshop on Annotating and Reason-
ing about Time and Events, pages 1?8, Sydney, Australia,
July.
Allan, James, Jaime Carbonell, George Doddington, Jonathon
Yamron, and Yiming Yang. 1998. Topic detection and
tracking pilot study. final report.
Cohen, Jacob. 1960. A coeficient of agreement for nomi-
nal scales. Educational and Psychological Measurement,
20(1):37?46.
Curran, James, Stephen Clark, and Johan Bos. 2007. Lin-
guistically motivated large-scale nlp with c & c and boxer.
In Proceedings of the ACL 2007 Demonstrations Session
(ACL-07 demo), pages 29?32.
Filatova, Elena and Vasileios Hatzivassiloglou. 2004. Event-
based extractive summarization. In In Proceedings of ACL
Workshop on Summarization, pages 104 ? 111.
Joachims, Thorsten. 1998. Text categorization with support
vector machines: learning with many relevant features. In
N?edellec, Claire and C?eline Rouveirol, editors, Proceed-
ings of the 10th ECML, pages 137?142, Chemnitz, DE.
Springer Verlag, Heidelberg, DE.
Kraaij, Wessel and Martijn Spitters. 2003. Language models
for topic tracking. In Croft, Bruce and John Lafferty, edi-
tors, Language Models for Information Retrieval. Kluwer
Academic Publishers.
Murff, Harvey, Vimla Patel, George Hripcsak, and David
Bates. 2003. Detecting adverse events for patient safety
research: a review of current methodologies. Journal of
Biomedical Informatics, 36(1/2):131?143.
Platt, John. 1999. Fast training of support vector machines
using sequential minimal optimization. Advances in kernel
methods: support vector learning, pages 185?208.
Saur??, Roser, Robert Knippen, Marc Verhagen, and James
Pustejovsky. 2005. Evita: a robust event recognizer for
qa systems. In HLT, pages 700?707.
Walker, Christopher., Stephanie. Strassel, Julie Medero, and
Linguistic Data Consortium. 2006. ACE 2005 Multilin-
gual Training Corpus. Linguistic Data Consortium, Uni-
versity of Pennsylvania.
Witten, Ian and Eibe Frank. 2000. Data mining: practical
machine learning tools and techniques with Java imple-
mentations. Morgan Kaufmann Publishers Inc.
Yamron, JP, L. Gillick, P. van Mulbregt, and S. Knecht. 2002.
Statistical models of topical content. The Kluwer Interna-
tional Series on Information Retrieval, pages 115?134.
Zhai, Chengxiang and John Lafferty. 2001. A study of
smoothing methods for language models applied to ad hoc
information retrieval. In Research and Development in In-
formation Retrieval, pages 334?342.
624
Proceedings of the ACL 2007 Student Research Workshop, pages 31?36,
Prague, June 2007. c?2007 Association for Computational Linguistics
Exploiting Structure for Event Discovery Using the MDI Algorithm
Martina Naughton
School of Computer Science & Informatics
University College Dublin
Ireland
martina.naughton@ucd.ie
Abstract
Effectively identifying events in unstruc-
tured text is a very difficult task. This is
largely due to the fact that an individual
event can be expressed by several sentences.
In this paper, we investigate the use of clus-
tering methods for the task of grouping the
text spans in a news article that refer to the
same event. The key idea is to cluster the
sentences, using a novel distance metric that
exploits regularities in the sequential struc-
ture of events within a document. When
this approach is compared to a simple bag
of words baseline, a statistically significant
increase in performance is observed.
1 Introduction
Accurately identifying events in unstructured text is
an important goal for many applications that require
natural language understanding. There has been an
increased focus on this problem in recent years. The
Automatic Content Extraction (ACE) program1 is
dedicated to developing methods that automatically
infer meaning from language data. Tasks include
the detection and characterisation of Entities, Rela-
tions, and Events. Extensive research has been ded-
icated to entity recognition and binary relation de-
tection with significant results (Bikel et al, 1999).
However, event extraction is still considered as one
of the most challenging tasks because an individual
event can be expressed by several sentences (Xu et
al., 2006).
In this paper, we primarily focus on techniques
for identifying events within a given news article.
Specifically, we describe and evaluate clustering
1http://www.nist.gov/speech/tests/ace/
methods for the task of grouping sentences in a news
article that refer to the same event. We generate
sentence clusters using three variations of the well-
documented Hierarchical Agglomerative Clustering
(HAC) (Manning and Schu?tze, 1999) as a baseline
for this task. We provide convincing evidence sug-
gesting that inherent structures exist in the manner in
which events appear in documents. In Section 3.1,
we present an algorithm which uses such structures
during the clustering process and as a result a mod-
est increase in accuracy is observed.
Developing methods capable of identifying all
types of events from free text is challenging for sev-
eral reasons. Firstly, different applications consider
different types of events and with different levels of
granularity. A change in state, a horse winning a
race and the race meeting itself can be considered
as events. Secondly, interpretation of events can be
subjective. How people understand an event can de-
pend on their knowledge and perspectives. There-
fore in this current work, the type of event to extract
is known in advance. As a detailed case study, we
investigate event discovery using a corpus of news
articles relating to the recent Iraqi War where the tar-
get event is the ?Death? event type. Figure 1 shows
a sample article depicting such events.
The remainder of this paper is organised as fol-
lows: We begin with a brief discussion of related
work in Section 2. We describe our approach to
Event Discovery in Section 3. Our techniques are
experimentally evaluated in Section 4. Finally, we
conclude with a discussion of experimental observa-
tions and opportunities for future work in Section 5.
2 Related Research
The aim of Event Extraction is to identify any in-
stance of a particular class of events in a natural
31
World News
Insurgents Kill 17 in Iraq
In Tikrit, gunmen killed 17 Iraqis as they were heading to work Sunday at a U.S. military facility.
Capt. Bill Coppernoll, said insurgents fired at several buses of Iraqis from two cars.
. . . . . . . . . . . . . . .
Elsewhere, an explosion at a market in Baqubah, about 30 miles north of Baghdad late Thursday.
The market was struck by mortar bombs according to U.S. military spokesman Sgt. Danny Martin.
. . . . . . . . . . . . . . .
Figure 1: Sample news article that describes multiple events.
language text, extract the relevant arguments of the
event, and represent the extracted information into
a structured form (Grishman, 1997). The types of
events to extract are known in advance. For exam-
ple, ?Attack? and ?Death? are possible event types
to be extracted. Previous work in this area focuses
mainly on linguistic and statistical methods to ex-
tract the relevant arguments of a event type. Lin-
guistic methods attempt to capture linguists knowl-
edge in determining constraints for syntax, mor-
phology and the disambiguation of both. Statistical
methods generate models based in the internal struc-
tures of sentences, usually identifying dependency
structures using an already annotated corpus of sen-
tences. However, since an event can be expressed
by several sentences, our approach to event extrac-
tion is as follows: First, identify all the sentences in
a document that refer to the event in question. Sec-
ond, extract event arguments from these sentences
and finally represent the extracted information of the
event in a structured form.
Particularly, in this paper we focus on clustering
methods for grouping sentences in an article that dis-
cuss the same event. The task of clustering simi-
lar sentences is a problem that has been investigated
particularly in the area of text summarisation. In
SimFinder (Hatzivassiloglou et al, 2001), a flexible
clustering tool for summarisation, the task is defined
as finding text units (sentences or paragraphs) that
contain information about a specific subject. How-
ever, the text features used in their similarity metric
are selected using a Machine Learning model.
3 Identifying Events within Articles
We treat the task of grouping together sentences that
refer to the same event(s) as a clustering problem.
As a baseline, we generate sentence clusters us-
ing average-link, single-link and complete-link Hi-
erarchical Agglomerative Clustering. HAC initially
assigns each data point to a singleton cluster, and
repeatedly merges clusters until a specified termi-
nation criteria is satisfied (Manning and Schu?tze,
1999). These methods require a similarity metric
between two sentences. We use the standard co-
sine metric over a bag-of-words encoding of each
sentence. We remove stopwords and stem each re-
maining term using the Porter stemming algorithm
(Porter, 1997). Our algorithms begin by placing
each sentence in its own cluster. At each itera-
tion we merge the two closest clusters. A fully-
automated approach must use some termination cri-
teria to decide when to stop clustering. In exper-
iments presented here, we adopt two manually su-
pervised methods to set the desired number of clus-
ters (k): ?correct? k and ?best? k. ?Correct? sets k
to be the actual number of events. This value was
obtained during the annotation process (see Section
4.1). ?Best? tunes k so as to maximise the quality of
the resulting clusters.
3.1 Exploiting Article Structure
Our baseline ignores an important constraint on the
event associated with each sentence: the position
of the sentence within the document. Documents
consist of sentences arranged in a linear order and
nearby sentences in terms of this ordering typically
refer to the same topic (Zha, 2002). Similarly we as-
sume that adjacent sentences are more likely to refer
to the same event, later sentences are likely to intro-
duce new events, etc. In this Section, we describe an
algorithm that exploits this document structure dur-
ing the sentence clustering process.
32
The basic idea is to learn a model capable of cap-
turing document structure, i.e. the way events are
reported. Each document is treated as a sequence of
labels (1 label per sentence) where each label repre-
sents the event(s) discussed in that sentence. We de-
fine four generalised event label types: N, represents
a new event sentence; C, represents a continuing
event sentence (i.e. it discusses the same event as the
preceding sentence); B, represents a back-reference
to an earlier event; X, represents a sentence that does
not reference an event. This model takes the form of
a Finite State Automaton (FSA) where:
? States correspond to event labels.
? Transitions correspond to adjacent sentences
that mention the pair of events.
More formally, E = (S, s0, F, L, T) is a model
where S is the set of states, s0 ? S is the initial state,
F ? S is the set of final states, L is the set of edge
labels and T ? (S?L)?S is the set of transitions.
We note that it is the responsibility of the learning
algorithm to discover the correct number of states.
We treat the task of discovering an event model as
that of learning a regular grammar from a set of pos-
itive examples. Following Golds research on learn-
ing regular languages (Gold, 1967), the problem has
received significant attention. In our current experi-
ments, we use Thollard et als MDI algorithm (Thol-
lard et al, 2000) for learning the automaton. MDI
has been shown to be effective on a wide range of
tasks, but it must be noted that any grammar infer-
ence algorithm could be substituted.
To estimate how much sequential structure exists
in the sentence labels, the document collection was
randomly split into training and test sets. The au-
tomaton produced by MDI was learned using the
training data, and the probability that each test se-
quence was generated by the automaton was calcu-
lated. These probabilities were compared with those
of a set of random sequences (generated to have the
same distribution of length as the test data). The
probabilities of event sequences from our dataset
and the randomly generated sequences are shown
in Figure 2. The test and random sequences are
sorted by probability. The vertical axis shows the
rank in each sequence and the horizontal axis shows
the negative log probability of the sequence at each
Figure 2: Distribution in the probability that actual
and random event sequences are generated by the
automaton produced by MDI.
rank. The data suggests that the documents are in-
deed structured, as real document sequences tend to
be much more likely under the trained FSA than ran-
domly generated sequences.
We modify our baseline clustering algorithm to
utilise the structural information omitted by the au-
tomaton as follows: Let L(c1, c2) be a sequence
of labels induced by merging two clusters c1 and
c2. If P (L(c1, c2)) is the probability that sequence
L(c1, c2) is accepted by the automaton, and let
cos(c1, c2) be the cosine distance between c1 and c2.
We can measure the similarity between c1 and c2 as:
SIM(c1, c2) = cos(c1, c2)? P (L(c1, c2)) (1)
Let r be the number of clusters remaining. Then
there are r(r?1)2 pairs of clusters. For each pair of
clusters c1,c2 we generate the resulting sequence of
labels that would result if c1 and c2 were merged.
We then input each label sequence to our trained
FSA to obtain the probability that it is generated by
the automaton. At each iteration, the algorithm pro-
ceeds by merging the most similar pair according to
this metric. Figure 3 illustrates this process in more
detail. To terminate the clustering process, we adopt
either the ?correct? k or ?best? k halting criteria de-
scribed earlier.
4 Experiments
4.1 Experimental Setup
In our experiments, we used a corpus of news arti-
cles which is a subset of the Iraq Body Count (IBC)
33
Figure 3: The sequence-based clustering process.
dataset2. This is an independent public database of
media-reported civilian deaths in Iraq resulting di-
rectly from military attack by the U.S. forces. Casu-
alty figures for each event reported are derived solely
from a comprehensive manual survey of online me-
dia reports from various news sources. We obtained
a portion of their corpus which consists of 342 new
articles from 56 news sources. The articles are of
varying size (average sentence length per document
is 25.96). Most of the articles contain references to
multiple events. The average number of events per
document is 5.09. Excess HTML (image captions
etc.) was removed, and sentence boundaries were
identified using the Lingua::EN::Sentence perl mod-
ule available from CPAN3.
To evaluate our clustering methods, we use the
definition of precision and recall proposed by (Hess
and Kushmerick, 2003). We assign each pair of
sentences into one of four categories: (i) clustered
together (and annotated as referring to the same
event); (ii) not clustered together (but annotated as
referring to the same event); (iii) incorrectly clus-
tered together; (iv) correctly not clustered together.
Precision and recall are thus found to be computed
as P = aa+c and R =
a
a+b , and F1 =
2PR
P+R .
The corpus was annotated by a set of ten vol-
unteers. Within each article, events were uniquely
identified by integers. These values were then
mapped to one of the four label categories, namely
?N?, ?C?, ?X?, and ?B?. For instance, sentences de-
scribing previously unseen events were assigned a
new integer. This value was mapped to the label cat-
egory ?N? signifying a new event. Similarly, sen-
2http://iraqbodycount.org/
3http://cpan.org/
tences referring to events in a preceding sentence
were assigned the same integer identifier as that
assigned to the preceding sentence and mapped to
the label category ?C?. Sentences that referenced an
event mentioned earlier in the document but not in
the preceding sentence were assigned the same inte-
ger identifier as that sentence but mapped to the label
category ?B?. Furthermore, If a sentence did not re-
fer to any event, it was assigned the label 0 and was
mapped to the label category ?X?. Finally, each doc-
ument was also annotated with the distinct number
of events reported in it.
In order to approximate the level of inter-
annotation agreement, two annotators were asked to
annotate a disjoint set of 250 documents. Inter-rater
agreements were calculated using the kappa statis-
tic that was first proposed by (Cohen, 1960). This
measure calculates and removes from the agreement
rate the amount of agreement expected by chance.
Therefore, the results are more informative than a
simple agreement average (Cohen, 1960; Carletta,
1996). Some extensions were developed including
(Cohen, 1968; Fleiss, 1971; Everitt, 1968; Barlow et
al., 1991). In this paper the methodology proposed
by (Fleiss, 1981) was implemented. Each sentence
in the document set was rated by the two annotators
and the assigned values were mapped into one of the
four label categories (?N?, ?C?, ?X?, and ?B?). For
complete instructions on how kappa was calculated,
we refer the reader to (Fleiss, 1981). Using the an-
notated data, a kappa score of 0.67 was obtained.
This indicates that the annotations are somewhat in-
consistent, but nonetheless are useful for producing
tentative conclusions.
To determine why the annotators were having dif-
ficulty agreeing, we calculated the kappa score for
each category. For the ?N?, ?C? and ?X? categories,
reasonable scores of 0.69, 0.71 and 0.72 were ob-
tained respectively. For the ?B? category a relatively
poor score of 0.52 was achieved indicating that the
raters found it difficult to identify sentences that ref-
erenced events mentioned earlier in the document.
To illustrate the difficulty of the annotation task an
example where the raters disagreed is depicted in
Figure 4. The raters both agreed when assigning
labels to sentence 1 and 2 but disagreed when as-
signing a label to Sentence 23 . In order to correctly
annotate this sentence as referring to the event de-
34
Sentence 1: A suicide attacker set off a bomb that tore through a funeral tent jammed with Shiite mourners Thursday.
Rater 1: label=1. Rater 2: label=1
Sentence 2: The explosion, in a working class neighbourhood of Mosul, destroyed the tent killing nearly 50 people.
Rater 1: label=1. Rater 2: label=1.
. . . . . . . . .
Sentence 23: At the hospital of this northern city, doctor Saher Maher said that at least 47 people were killed.
Rater 1: label=1. Rater 2: label=2.
Figure 4: Sample sentences where the raters disagreed.
Algorithm a-link c-link s-link
BL(correct k) 40.5 % 39.2% 39.6%
SEQ(correct k) 47.6%* 45.5%* 44.9%*
BL(best k) 52.0% 48.2% 50.9%
SEQ(best k) 61.0%* 56.9%* 58.6%*
Table 1: % F1 achieved using average-link (a-link),
complete-link (c-link) and single-link (s-link) varia-
tions of the baseline and sequence-based algorithms
when the correct and best k halting criteria are used.
Scores marked with * are statistically significant to
a confidence level of 99%.
scribe in sentence 1 and 2, the rater have to resolve
that ?the northern city? is referring to ?Mosul? and
that ?nearly 50? equates to ?at least 47?. These and
similar ambiguities in written text make such an an-
notation task very difficult.
4.2 Results
We evaluated our clustering algorithms using the F1
metric. Results presented in Table 1 were obtained
using 50:50 randomly selected train/test splits aver-
aged over 5 runs. For each run, the automaton pro-
duced by MDI was generated using the training set
and the clustering algorithms were evaluated using
the test set. On average, the sequence-based clus-
tering approach achieves an 8% increase in F1 when
compared to the baseline. Specifically the average-
link variation exhibits the highest F1 score, achiev-
ing 62% when the ?best? k termination method is
used.
It is important to note that the inference produced
by the automaton depends on two values: the thresh-
old ? of the MDI algorithm and the amount of label
sequences used for learning. The closer ? is to 0,
the more general the inferred automaton becomes.
In an attempt to produce a more general automaton,
we chose ? = 0.1. Intuitively, as more training data
is used to train the automaton, more accurate infer-
ences are expected. To confirm this we calculated
the %F1 achieved by the average-link variation of
the method for varying levels of training data. Over-
all, an improvement of approx. 5% is observed as
the percentage training data used is increased from
10% to 90%.
5 Discussion
Accurately identifying events in unstructured text is
a very difficult task. This is partly because the de-
scription of an individual event can spread across
several sentences. In this paper, we investigated
the use of clustering for the task of grouping sen-
tences in a document that refer to the same event.
However, there are limitations to this approach that
need to be considered. Firstly, results presented
in Section 4.2 suggest that the performance of the
clusterer depends somewhat on the chosen value
of k (i.e. the number of events in the document).
This information is not readily available. However,
preliminary analysis presented in (Naughton et al,
2006) indicate that is possible to estimate this value
with reasonable accuracy. Furthermore, promising
results are observed when this estimated value is
used halt the clustering process. Secondly, labelled
data is required to train the automation used by our
novel clustering method. Evidence presented in Sec-
tion 4.1 suggests that reasonable inter-annotation
agreement for such an annotation task is difficult to
achieve. Nevertheless, clustering allows us to take
into account that the manner in which events are de-
scribed is not always linear. To assess exactly how
beneficial this is, we are currently treating this prob-
lem as a text segmentation task. Although this is a
35
crude treatment of the complexity of written text, it
will help us to approximate the benefit (if any) of
applying clustering-based techniques to this task.
In the future, we hope to further evaluate our
methods using a larger dataset containing more
event types. We also hope to examine the inter-
esting possibility that inherent structures learned
from documents originating from one news source
(e.g. Aljazeera) differ from structures learned us-
ing documents originating from another source (e.g.
Reuters). Finally, a single sentence often contains
references to multiple events. For example, consider
the sentence ?These two bombings have claimed the
lives of 23 Iraqi soldiers?. Our algorithms assume
that each sentence describes just one event. Future
work will focus on developing methods to automati-
cally recognise such sentences and techniques to in-
corporate them into the clustering process.
Acknowledgements. This research was supported
by the Irish Research Council for Science, Engineer-
ing & Technology (IRCSET) and IBM under grant
RS/2004/IBM/1. The author also wishes to thank
Dr. Joe Carthy and Dr. Nicholas Kushmerick for
their helpful discussions.
References
W. Barlow, N. Lai, and S. Azen. 1991. A comparison of
methods for calculating a stratified kappa. Statistics in
Medicine, 10:1465?1472.
Daniel Bikel, Richard Schwartz, and Ralph Weischedel.
1999. An algorithm that learns what?s in a name. Ma-
chine Learning, 34(1-3):211?231.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Linguis-
tics, 22:249?254.
Jacob Cohen. 1960. A coeficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
agreement with provision for scaled disagreement or
partial credit. Psychological Bulletin, 70.
B.S. Everitt. 1968. Moments of the statistics kappa and
the weighted kappa. The British Journal of Mathemat-
ical and Statistical Psychology, 21:97?103.
J.L. Fleiss. 1971. Measuring nominal scale agreement
among many raters. Psychological Bulletin, 76.
J.L. Fleiss, 1981. Statistical methods for rates and pro-
portions, pages 212?36. John Wiley & Sons.
E. Mark Gold. 1967. Grammar identification in the limit.
Information and Control, 10(5):447?474.
Ralph Grishman. 1997. Information extraction: Tech-
niques and challenges. In Proceedings of the sev-
enth International Message Understanding Confer-
ence, pages 10?27.
Vasileios Hatzivassiloglou, Judith Klavans, Melissa Hol-
combe, Regina Barzilay, Min-Yen Kan, and Kathleen
McKeown. 2001. SIMFINDER: A flexible clustering
tool for summarisation. In Proceedings of the NAACL
Workshop on Automatic Summarisation, Association
for Computational Linguistics, pages 41?49.
Andreas Hess and Nicholas Kushmerick. 2003. Learn-
ing to attach semantic metadata to web services. In
Proceedings of the International Semantic Web Con-
ference (ISWC 2003), pages 258?273. Springer.
Christopher Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press.
Martina Naughton, Nicholas Kushmerick, and Joseph
Carthy. 2006. Event extraction from heterogeneous
news sources. In Proceedings of the AAAI Workshop
Event Extraction and Synthesis, pages 1?6, Boston.
Martin Porter. 1997. An algorithm for suffix stripping.
Readings in Information Retrieval, pages 313?316.
Franck Thollard, Pierre Dupont, and Colin de la Higuera.
2000. Probabilistic DFA inference using Kullback-
Leibler divergence and minimality. In Proceedings of
the 17th International Conference on Machine Learn-
ing, pages 975?982. Morgan Kaufmann, San Fran-
cisco.
Feiyu Xu, Hans Uszkoreit, and Hong Li. 2006. Auto-
matic event and relation detection with seeds of vary-
ing complexity. In Proceedings of the AAAI Workshop
Event Extraction and Synthesis, pages 12?17, Boston.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement prin-
ciple and sentence clustering. In Proceedings of the
25th annual international ACM SIGIR conference on
Research and development in Information Retrieval,
pages 113?120, New York, NY. ACM Press.
36
