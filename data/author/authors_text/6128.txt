Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 182?190,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Phrase-Based Query Degradation Modeling for
Vocabulary-Independent Ranked Utterance Retrieval
J. Scott Olsson
HLT Center of Excellence
Johns Hopkins University
Baltimore, MD 21211, USA
solsson@jhu.edu
Douglas W. Oard
College of Information Studies
University of Maryland
College Park, MD 15213, USA
oard@umd.edu
Abstract
This paper introduces a new approach to rank-
ing speech utterances by a system?s confi-
dence that they contain a spoken word. Multi-
ple alternate pronunciations, or degradations,
of a query word?s phoneme sequence are hy-
pothesized and incorporated into the ranking
function. We consider two methods for hy-
pothesizing these degradations, the best of
which is constructed using factored phrase-
based statistical machine translation. We show
that this approach is able to significantly im-
prove upon a state-of-the-art baseline tech-
nique in an evaluation on held-out speech.
We evaluate our systems using three differ-
ent methods for indexing the speech utter-
ances (using phoneme, phoneme multigram,
and word recognition), and find that degrada-
tion modeling shows particular promise for lo-
cating out-of-vocabulary words when the un-
derlying indexing system is constructed with
standard word-based speech recognition.
1 Introduction
Our goal is to find short speech utterances which
contain a query word. We accomplish this goal
by ranking the set of utterances by our confidence
that they contain the query word, a task known as
Ranked Utterance Retrieval (RUR). In particular,
we are interested in the case when the user?s query
word can not be anticipated by a Large Vocabulary
Continuous Speech Recognizer?s (LVCSR) decod-
ing dictionary, so that the word is said to be Out-Of-
Vocabulary (OOV).
Rare words tend to be the most informative, but
are also most likely to be OOV. When words are
OOV, we must use vocabulary-independent tech-
niques to locate them. One popular approach is to
search for the words in output from a phoneme rec-
ognizer (Ng and Zue, 2000), although this suffers
from the low accuracy typical of phoneme recogni-
tion. We consider two methods for handling this in-
accuracy. First, we compare an RUR indexing sys-
tem using phonemes with two systems using longer
recognition units: words or phoneme multigrams.
Second, we consider several methods for handling
the recognition inaccuracy in the utterance rank-
ing function itself. Our baseline generative model
handles errorful recognition by estimating term fre-
quencies from smoothed language models trained
on phoneme lattices. Our new approach, which we
call query degradation, hypothesizes many alternate
?pronunciations? for the query word and incorpo-
rates them into the ranking function. These degra-
dations are translations of the lexical phoneme se-
quence into the errorful recognition language, which
we hypothesize using a factored phrase-based statis-
tical machine translation system.
Our speech collection is a set of oral history
interviews from the MALACH collection (Byrne
et al, 2004), which has previously been used for
ad hoc speech retrieval evaluations using one-best
word level transcripts (Pecina et al, 2007; Olsson,
2008a) and for vocabulary-independent RUR (Ols-
son, 2008b). The interviews were conducted with
survivors and witnesses of the Holocaust, who dis-
cuss their experiences before, during, and after the
Second World War. Their speech is predominately
spontaneous and conversational. It is often also
emotional and heavily accented. Because the speech
contains many words unlikely to occur within a gen-
eral purpose speech recognition lexicon, it repre-
182
sents an excellent collection for RUR evaluation.
We were graciously permitted to use BBN Tech-
nology?s speech recognition system Byblos (Prasad
et al, 2005; Matsoukas et al, 2005) for our speech
recognition experiments. We train on approximately
200 hours of transcribed audio excerpted from about
800 unique speakers in the MALACH collection. To
provide a realistic set of OOV query words, we use
an LVCSR dictionary previously constructed for a
different topic domain (broadcast news and conver-
sational telephone speech) and discard all utterances
in our acoustic training data which are not covered
by this dictionary. New acoustic and language mod-
els are trained for each of the phoneme, multigram
and word recognition systems.
The output of LVCSR is a lattice of recogni-
tion hypotheses for each test speech utterance. A
lattice is a directed acyclic graph that is used to
compactly represent the search space for a speech
recognition system. Each node represents a point in
time and arcs between nodes indicates a word oc-
curs between the connected nodes? times. Arcs are
weighted by the probability of the word occurring,
so that the so-called ?one-best? path through the lat-
tice (what a system might return as a transcription)
is the path through the lattice having highest proba-
bility under the acoustic and language models. Each
RUR model we consider is constructed using the ex-
pected counts of a query word?s phoneme sequences
in these recognition lattices. We consider three ap-
proaches to producing these phoneme lattices, using
standard word-based LVCSR, phoneme recognition,
and LVCSR using phoneme multigrams. Our word
system?s dictionary contains about 50,000 entries,
while the phoneme system contains 39 phonemes
from the ARPABET set.
Originally proposed by Deligne and Bimbot
(1997) to model variable length regularities in
streams of symbols (e.g., words, graphemes, or
phonemes), phoneme multigrams are short se-
quences of one or more phonemes. We produce a
set of ?phoneme transcripts? by replacing transcript
words with their lexical pronunciation. The set of
multigrams is learned by then choosing a maximum-
likelihood segmentation of these training phoneme
transcripts, where the segmentation is viewed as hid-
den data in an Expectation-Maximization algorithm.
The set of all continuous phonemes occurring be-
tween segment boundaries is then chosen as our
multigram dictionary. This multigram recognition
dictionary contains 16,409 entries.
After we have obtained each recognition lat-
tice, our indexing approach follows that of Olsson
(2008b). Namely, for the word and multigram sys-
tems, we first expand lattice arcs containing multi-
ple phones to produce a lattice having only single
phonemes on its arcs. Then, we compute the ex-
pected count of all phoneme n-grams n ? 5 in the
lattice. These n-grams and their counts are inserted
in our inverted index for retrieval.
This paper is organized as follows. In Section 2
we introduce our baseline RUR methods. In Sec-
tion 3 we introduce our query degradation approach.
We introduce our experimental validation in Sec-
tion 4 and our results in Section 5. We find that
using phrase-based query degradations can signifi-
cantly improve upon a strong RUR baseline. Finally,
in Section 6 we conclude and outline several direc-
tions for future work.
2 Generative Baseline
Each method we present in this paper ranks the ut-
terances by the term?s estimated frequency within
the corresponding phoneme lattice. This general
approach has previously been considered (Yu and
Seide, 2005; Saraclar and Sproat, 2004), on the ba-
sis that it provides a minimum Bayes-risk ranking
criterion (Yu et al, Sept 2005; Robertson, 1977) for
the utterances. What differs for each method is the
particular estimator of term frequency which is used.
We first outline our baseline approach, a generative
model for term frequency estimation.
Recall that our vocabulary-independent indices
contain the expected counts of phoneme sequences
from our recognition lattices. Yu and Seide (2005)
used these expected phoneme sequence counts to es-
timate term frequency in the following way. For a
query term Q and lattice L, term frequency t?fG is
estimated as t?fG(Q,L) = P (Q|L) ?NL, where NL
is an estimate for the number of words in the utter-
ance. The conditional P (Q|L) is modeled as an or-
der M phoneme level language model,
P? (Q|L) =
l?
i=1
P? (qi|qi?M+1, . . . , qi?1,L), (1)
183
so that t?fG(Q,L) ? P? (Q|L) ? NL. The probabil-
ity of a query phoneme qj being generated, given
that the phoneme sequence qj?M+1, . . . , qj?1 =
qj?1j?M+1 was observed, is estimated as
P? (qj |qj?1j?M+1,L) =
EPL [C(qjj?M+1)]
EPL [C(qj?1j?M+1)]
.
Here, EPL [C(qj?1j?M+1)] denotes the expected count
in lattice L of the phoneme sequence qj?1j?M+1. We
compute these counts using a variant of the forward-
backward algorithm, which is implemented by the
SRI language modeling toolkit (Stolcke, 2002).
In practice, because of data sparsity, the language
model in Equation 1 must be modified to include
smoothing for unseen phoneme sequences. We use a
backoff M -gram model with Witten-Bell discount-
ing (Witten and Bell, 1991). We set the phoneme
language model?s order to M = 5, which gave good
results in previous work (Yu and Seide, 2005).
3 Incorporating Query Degradations
One problem with the generative approach is that
recognition error is not modeled (apart from the un-
certainty captured in the phoneme lattice). The es-
sential problem is that while the method hopes to
model P (Q|L), it is in fact only able to model the
probability of one degradation H in the lattice, that
is P (H|L). We define a query degradation as any
phoneme sequence (including the lexical sequence)
which may, with some estimated probability, occur
in an errorful phonemic representation of the audio
(either a one-best or lattice hypothesis). Because of
speaker variation and because recognition is error-
ful, we ought to also consider non-lexical degrada-
tions of the query phoneme sequence. That is, we
should incorporate P (H|Q) in our ranking function.
It has previously been demonstrated that allow-
ing for phoneme confusability can significantly in-
crease spoken term detection performance on one-
best phoneme transcripts (Chaudhari and Picheny,
2007; Schone et al, 2005) and in phonemic lat-
tices (Foote et al, 1997). These methods work by
allowing weighted substitution costs in minimum-
edit-distance matching. Previously, these substitu-
tion costs have been maximum-likelihood estimates
of P (H|Q) for each phoneme, where P (H|Q) is
easily computed from a phoneme confusion matrix
after aligning the reference and one-best hypothesis
transcript under a minimum edit distance criterion.
Similar methods have also been used in other lan-
guage processing applications. For example, in (Ko-
lak, 2005), one-for-one character substitutions, in-
sertions and deletions were considered in a genera-
tive model of errors in OCR.
In this work, because we are focused on construct-
ing inverted indices of audio files (for speed and
to conserve space), we must generalize our method
of incorporating query degradations in the ranking
function. Given a degradation model P (H|Q), we
take as our ranking function the expectation of the
generative baseline estimate NL ? P? (H|L) with re-
spect to P (H|Q),
t?fG(Q,L) =
?
H?H
[
P? (H|L) ?NL
]
?P (H|Q), (2)
where H is the set of degradations. Note that, while
we consider the expected value of our baseline term
frequency estimator with respect to P (H|Q), this
general approach could be used with any other term
frequency estimator.
Our formulation is similar to approaches taken
in OCR document retrieval, using degradations of
character sequences (Darwish and Magdy, 2007;
Darwish, 2003). For vocabulary-independent spo-
ken term detection, perhaps the most closely re-
lated formulation is provided by (Mamou and Ram-
abhadran, 2008). In that work, they ranked ut-
terances by the weighted average of their match-
ing score, where the weights were confidences from
a grapheme to phoneme system?s first several hy-
potheses for a word?s pronunciation. The match-
ing scores were edit distances, where substitution
costs were weighted using phoneme confusability.
Accordingly, their formulation was not aimed at ac-
counting for errors in recognition per se, but rather
for errors in hypothesizing pronunciations. We ex-
pect this accounts for their lack of significant im-
provement using the method.
Since we don?t want to sum over all possible
recognition hypotheses H , we might instead sum
over the smallest setH such that?H?H P (H|Q) ?
?. That is, we could take the most probable degra-
dations until their cumulative probability exceeds
some threshold ?. In practice, however, because
184
degradation probabilities can be poorly scaled, we
instead take a fixed number of degradations and
normalize their scores. When a query is issued,
we apply a degradation model to learn the top few
phoneme sequences H that are most likely to have
been recognized, under the model. In the machine
translation literature, this process is commonly re-
ferred to as decoding.
We now turn to the modeling of query degrada-
tions H given a phoneme sequence Q, P (H|Q).
First, we consider a simple baseline approach in Sec-
tion 3.1. Then, in Section 3.2, we propose a more
powerful technique, using state-of-the-art machine
translation methods to hypothesize our degradations.
3.1 Baseline Query Degradations
Schone et al (2005) used phoneme confusion ma-
trices created by aligning hypothesized and refer-
ence phoneme transcripts to weight edit costs for a
minimum-edit distance based search in a one-best
phoneme transcript. Foote et al (1997) had previ-
ously used phoneme lattices, although with ad hoc
edit costs and without efficient indexing. In this
work, we do not want to linearly scan each phoneme
lattice for our query?s phoneme sequence, preferring
instead to look up sequences in the inverted indices
containing phoneme sequences.
Our baseline degradation approach is related to
the edit-cost approach taken by (Schone et al,
2005), although we generalize it so that it may be
applied within Equation 2 and we consider speech
recognition hypotheses beyond the one-best hypoth-
esis. First, we randomly generate N traversals of
each phonemic recognition lattice. These traver-
sals are random paths through the lattice (i.e., we
start at the beginning of the lattice and move to the
next node, where our choice is weighted by the out-
going arcs? probabilities). Then, we align each of
these traversals with its reference transcript using a
minimum-edit distance criterion. Phone confusion
matrices are then tabulated from the aggregated in-
sertion, substitution, and deletion counts across all
traversals of all lattices. From these confusion ma-
trices, we compute unsmoothed estimates of P (h|r),
the probability of a phoneme h being hypothesized
given a reference phoneme r.
Making an independence assumption, our base-
line degradation model for a query with m
AY K M AA N
Vowel Consonant Semi-vowel Vowel Semi-vowel
Dipthong Voiceless plosive Nasal Back vowel Nasal
Figure 1: Three levels of annotation used by the factored
phrase-based query degradation model.
phonemes is then P (H|Q) = ?mi=1 P (hi|ri). We
efficiently compute the most probable degradations
for a query Q using a lattice of possible degrada-
tions and the forward backward algorithm. We call
this baseline degradation approach CMQD (Confu-
sion Matrix based Query Degradation).
3.2 Phrase-Based Query Degradation
One problem with CMQD is that we only allow in-
sertions, deletions, and one-for-one substitutions. It
may be, however, that certain pairs of phonemes
are commonly hypothesized for a particular refer-
ence phoneme (in the language of statistical machine
translation, we might say that we should allow some
non-zero fertility). Second, there is nothing to dis-
courage query degradations which are unlikely un-
der an (errorful) language model?that is, degrada-
tions that are not observed in the speech hypothe-
ses. Finally, CMQD doesn?t account for similarities
between phoneme classes. While some of these de-
ficiencies could be addressed with an extension to
CMQD (e.g., by expanding the degradation lattices
to include language model scores), we can do bet-
ter using a more powerful modeling framework. In
particular, we adopt the approach of phrase-based
statistical machine translation (Koehn et al, 2003;
Koehn and Hoang, 2007). This approach allows
for multiple-phoneme to multiple-phoneme substi-
tutions, as well as the soft incorporation of addi-
tional linguistic knowledge (e.g., phoneme classes).
This is related to previous work allowing higher or-
der phoneme confusions in bigram or trigram con-
texts (Chaudhari and Picheny, 2007), although they
used a fuzzy edit distance measure and did not in-
corporate other evidence in their model (e.g., the
phoneme language model score). The reader is re-
ferred to (Koehn and Hoang, 2007; Koehn et al,
2007) for detailed information about phrase-based
statistical machine translation. We give a brief out-
line here, sufficient only to provide background for
our query degradation application.
Statistical machine translation systems work by
185
converting a source-language sentence into the most
probable target-language sentence, under a model
whose parameters are estimated using example sen-
tence pairs. Phrase-based machine translation is one
variant of this statistical approach, wherein multiple-
word phrases rather than isolated words are the
basic translation unit. These phrases are gener-
ally not linguistically motivated, but rather learned
from co-occurrences in the paired example transla-
tion sentences. We apply the same machinery to hy-
pothesize our pronunciation degradations, where we
now translate from the ?source-language? reference
phoneme sequence Q to the hypothesized ?target-
language? phoneme sequence H .
Phrase-based translation is based on the noisy
channel model, where Bayes rule is used to refor-
mulate the translation probability for translating a
reference query Q into a hypothesized phoneme se-
quence H as
argmax
H
P (H|Q) = argmax
H
P (Q|H)P (H).
Here, for example, P (H) is the language model
probability of a degradation H and P (Q|H) is the
conditional probability of the reference sequence Q
given H . More generally however, we can incorpo-
rate other feature functions of H and Q, hi(H,Q),
and with varying weights. This is implemented us-
ing a log-linear model for P (H|Q), where the model
covariates are the functions hi(H,Q), so that
P (H|Q) = 1Z exp
n?
i=1
?ihi(H,Q)
The parameters ?i are estimated by MLE and the
normalizing Z need not be computed (because we
will take the argmax). Example feature functions in-
clude the language model probability of the hypoth-
esis and a hypothesis length penalty.
In addition to feature functions being defined on
the surface level of the phonemes, they may also be
defined on non-surface annotation levels, called fac-
tors. In a word translation setting, the intuition is
that statistics from morphological variants of a lex-
ical form ought to contribute to statistics for other
variants. For example, if we have never seen the
word houses in language model training, but have
examples of house, we still can expect houses are to
be more probable than houses fly. In other words,
factors allow us to collect improved statistics on
sparse data. While sparsity might appear to be less
of a problem for phoneme degradation modeling
(because the token inventory is comparatively very
small), we nevertheless may benefit from this ap-
proach, particularly because we expect to rely on
higher order language models and because we have
rather little training data: only 22,810 transcribed
utterances (about 600k reference phonemes).
In our case, we use two additional annotation lay-
ers, based on a simple grouping of phonemes into
broad classes. We consider the phoneme itself, the
broad distinction of vowel and consonant, and a finer
grained set of classes (e.g., front vowels, central
vowels, voiceless and voiced fricatives). Figure 1
shows the three annotation layers we consider for an
example reference phoneme sequence. After map-
ping the reference and hypothesized phonemes to
each of these additional factor levels, we train lan-
guage models on each of the three factor levels of
the hypothesized phonemes. The language models
for each of these factor levels are then incorporated
as features in the translation model.
We use the open source toolkit Moses (Koehn
et al, 2007) as our phrase-based machine transla-
tion system. We used the SRI language model-
ing toolkit to estimate interpolated 5-gram language
models (for each factor level), and smoothed our
estimates with Witten-Bell discounting (Witten and
Bell, 1991). We used the default parameter settings
for Moses?s training, with the exception of modi-
fying GIZA++?s default maximum fertility from 10
to 4 (since we don?t expect one reference phoneme
to align to 10 degraded phonemes). We used default
decoding settings, apart from setting the distortion
penalty to prevent any reorderings (since alignments
are logically constrained to never cross). For the rest
of this chapter, we refer to our phrase-based query
degradation model as PBQD. We denote the phrase-
based model using factors as PBQD-Fac.
Figure 2 shows an example alignment learned
for a reference and one-best phonemic transcript.
The reference utterance ?snow white and the seven
dwarves? is recognized (approximately) as ?no
white a the second walks?. Note that the phrase-
based system is learning not only acoustically plau-
sible confusions, but critically, also confusions aris-
186
N OW W AY T AX DH AX S EH K AX N D W AO K S
S N OW W AY T AE N D DH AX S EH V AX N D W OW R F S
snow white and the seven dwarves
Figure 2: An alignment of hypothesized and reference phoneme transcripts from the multigram phoneme recognizer,
for the phrase-based query degradation model.
ing from the phonemic recognition system?s pe-
culiar construction. For example, while V and
K may not be acoustically similar, they are still
confusable?within the context of S EH?because
multigram language model data has many exam-
ples of the word second. Moreover, while the word
dwarves (D-W-OW-R-F-S) is not present in the
dictionary, the words dwarf (D-W-AO-R-F) and
dwarfed (D-W-AO-R-F-T) are present (N.B., the
change of vowel from AO to OW between the OOV
and in vocabulary pronunciations). While CMQD
would have to allow a deletion and two substitutions
(without any context) to obtain the correct degrada-
tion, the phrase-based system can align the complete
phrase pair from training and exploit context. Here,
for example, it is highly probable that the errorfully
hypothesized phonemes W AO will be followed by
K, because of the prevalence of walk in language
model data.
4 Experiments
An appropriate and commonly used measure for
RUR is Mean Average Precision (MAP). Given a
ranked list of utterances being searched through, we
define the precision at position i in the list as the pro-
portion of the top i utterances which actually contain
the corresponding query word. Average Precision
(AP) is the average of the precision values computed
for each position containing a relevant utterance. To
assess the effectiveness of a system across multi-
ple queries, Mean Average Precision is defined as
the arithmetic mean of per-query average precision,
MAP = 1n
?
n APn. Throughout this paper, when
we report statistically significant improvements in
MAP, we are comparing AP for paired queries us-
ing a Wilcoxon signed rank test at ? = 0.05.
Note, RUR is different than spoken term detec-
tion in two ways, and thus warrants an evaluation
measure (e.g., MAP) different than standard spoken
term detection measures (such as NIST?s actual term
weighted value (Fiscus et al, 2006)). First, STD
measures require locating a term with granularity
finer than that of an utterance. Second, STD mea-
sures are computed using a fixed detection thresh-
old. This latter requirement will be unnecessary in
many applications (e.g., where a user might prefer
to decide themselves when to stop reading down
the ranked list of retrieved utterances) and unlikely
to be helpful for downstream evidence combination
(where we may prefer to keep all putative hits and
weight them by some measure of confidence).
For our evaluation, we consider retrieving
short utterances from seventeen fully transcribed
MALACH interviews. Our query set contains all
single words occurring in these interviews that are
OOV with respect to the word dictionary. This
gives us a total of 261 query terms for evalua-
tion. Note, query words are also not present in
the multigram training transcripts, in any language
model training data, or in any transcripts used for
degradation modeling. Some example query words
include BUCHENWALD, KINDERTRANSPORT, and
SONDERKOMMANDO.
To train our degradation models, we used a held
out set of 22,810 manually transcribed utterances.
We run each recognition system (phoneme, multi-
gram, and word) on these utterances and, for each,
train separate degradation models using the aligned
reference and hypothesis transcripts. For CMQD,
we computed 100 random traversals on each lattice,
giving us a total of 2,281,000 hypothesis and refer-
ence pairs to align for our confusion matrices.
5 Results
We first consider an intrinsic measure of the three
speech recognition systems we consider, namely
Phoneme Error Rate (PER). Phoneme Error Rate
is calculated by first producing an alignment of
187
the hypothesis and reference phoneme transcripts.
The counts of each error type are used to compute
PER = 100 ? S+D+IN , where S,D, I are the num-ber of substitutions, insertions, and deletions respec-
tively, while N is the phoneme length of the refer-
ence. Results are shown in Table 1. First, we see that
the PER for the multigram system is roughly half
that of the phoneme-only system. Second, we find
that the word system achieves a considerably lower
PER than the multigram system. We note, however,
that since these are not true phonemes (but rather
phonemes copied over from pronunciation dictionar-
ies and word transcripts), we must cautiously inter-
pret these results. In particular, it seems reasonable
that this framework will overestimate the strength
of the word based system. For comparison, on the
same train/test partition, our word-level system had
a word error rate of 31.63. Note, however, that au-
tomatic word transcripts can not contain our OOV
query words, so word error rate is reported only to
give a sense of the difficulty of the recognition task.
Table 1 shows our baseline RUR evaluation re-
sults. First, we find that the generative model yields
statistically significantly higher MAP using words
or multigrams than phonemes. This is almost cer-
tainly due to the considerably improved phoneme
recognition afforded by longer recognition units.
Second, many more unique phoneme sequences typ-
ically occur in phoneme lattices than in their word
or multigram counterparts. We expect this will in-
crease the false alarm rate for the phoneme system,
thus decreasing MAP.
Surprisingly, while the word-based recognition
system achieved considerably lower phoneme er-
ror rates than the multigram system (see Table 1),
the word-based generative model was in fact in-
distinguishable from the same model using multi-
grams. We speculate that this is because the method,
as it is essentially a language modeling approach,
is sensitive to data sparsity and requires appropri-
ate smoothing. Because multigram lattices incor-
porate smaller recognition units, which are not con-
strained to be English words, they naturally produce
smoother phoneme language models than a word-
based system. On the other hand, the multigram
system is also not statistically significantly better
than the word-based generative model, suggesting
this may be a promising area for future work.
Table 1 shows results using our degradation mod-
els. Query degradation appears to help all sys-
tems with respect to the generative baseline. This
agrees with our intuition that, for RUR, low MAP on
OOV terms is predominately driven by low recall.1
Note that, at one degradation, CMQD has the same
MAP as the generative model, since the most prob-
able degradation under CMQD is almost always the
reference phoneme sequence. Because the CMQD
model can easily hypothesize implausible degrada-
tions, we see the MAP increases modestly with a
few degradations, but then MAP decreases. In con-
trast, the MAP of the phrase-based system (PBQD-
Fac) increases through to 500 query degradations us-
ing multigrams. The phonemic system appears to
achieve its peak MAP with fewer degradations, but
also has a considerably lower best value.
The non-factored phrase-based system PBQD
achieves a peak MAP considerably larger than the
peak CMQD approach. And, likewise, using addi-
tional factor levels (PBQD-Fac) also considerably
improves performance. Note especially that, using
multiple factor levels, we not only achieve a higher
MAP, but also a higher MAP when only a few degra-
dations are possible.
To account for errors in phonemic recognition, we
have taken two steps. First, we used longer recog-
nition units which we found significantly improved
MAP while using our baseline RUR technique. As
a second method for handling recognition errors,
we also considered variants of our ranking func-
tion. In particular, we incorporated query degrada-
tions hypothesized using factored phrase-based ma-
chine translation. Comparing the MAP for PBQD-
Fac with MAP using the generative baseline for the
most improved indexing system (the word system),
we find that this degradation approach again statisti-
cally significantly improved MAP. That is, these two
strategies for handling recognition errors in RUR ap-
pear to work well in combination.
Although we focused on vocabulary-independent
RUR, downstream tasks such as ad hoc speech
retrieval will also want to incorporate evidence
from in-vocabulary query words. This makes
1We note however that the preferred operating point in the
tradeoff between precision and recall will be task specific. For
example, it is known that precision errors become increasingly
important as collection size grows (Shao et al, 2008).
188
Query Degradations
Method Phone Source PER QD Model Baseline 1 5 50 500
Degraded Model Phonemes 64.4 PBQD-Fac 0.0387 0.0479 0.0581 0.0614 0.0612
Multigrams 32.1 CMQD 0.1258 0.1258 0.1272 0.1158 0.0991
Multigrams 32.1 PBQD 0.1258 0.1160 0.1283 0.1347 0.1317
Multigrams 32.1 PBQD-Fac 0.1258 0.1238 0.1399 0.1510 0.1527
Words 20.5 PBQD-Fac 0.1255 0.1162 0.1509 0.1787 0.1753
Table 1: PER and MAP results for baseline and degradation models. The best result for each indexing approach is
shown in bold.
our query degradation approach which indexed
phonemes from word-based LVCSR particularly at-
tractive. Not only did it achieve the best MAP in
our evaluation, but this approach also allows us to
construct recognition lattices for both in and out-of-
vocabulary query words without running a second,
costly, recognition step.
6 Conclusion
Our goal in this work was to rank utterances by our
confidence that they contained a previously unseen
query word. We proposed a new approach to this
task using hypothesized degradations of the query
word?s phoneme sequence, which we produced us-
ing a factored phrase-based machine translation
model. This approach was principally motivated by
the mismatch between the query?s phonemes and
the recognition phoneme sequences due to errorful
speech indexing. Our approach was constructed and
evaluated using phoneme-, multigram-, and word-
based indexing, and significant improvements in
MAP using each indexing system were achieved.
Critically, these significant improvements were in
addition to the significant gains we achieved by con-
structing our index with longer recognition units.
While PBQD-Fac outperformed CMQD averag-
ing over all queries in our evaluation, as expected,
there may be particular query words for which this
is not the case. Table 2 shows example degrada-
tions using both the CMQD and PBQD-Fac degra-
dation models for multigrams. The query word is
Mengele. We see that CMQD degradations are near
(in an edit distance sense) to the reference pronun-
ciation (M-EH-NX-EY-L-EH), while the phrase-
based degradations tend to sound like commonly oc-
CMQD Phrase-based
M-EH-NX-EY-L-EH M-EH-N-T-AX-L
M-EH-NX-EY-L M-EH-N-T-AX-L-AA-T
M-NX-EY-L-EH AH-AH-AH-AH-M-EH-N-T-AX-L
M-EH-NX-EY-EH M-EH-N-DH-EY-L-EH
M-EH-NX-L-EH M-EH-N-T-AX-L-IY
Table 2: The top five degradations and associated proba-
bilities using the CMQD and PBQD-Fac models, for the
term Mengele using multigram indexing.
curring words (mental, meant a lot, men they. . . ,
mentally). In this case, the lexical phoneme se-
quence does not occur in the PBQD-Fac degrada-
tions until degradation nineteen. Because delet-
ing EH has the same cost irrespective of context
for CMQD, both CMQD degradations 2 and 3 are
given the same pronunciation weight. Here, CMQD
performs considerably better, achieving an average
precision of 0.1707, while PBQD-Fac obtains only
0.0300. This suggests that occasionally the phrase-
based language model may exert too much influence
on the degradations, which is likely to increase the
incidence of false alarms. One solution, for future
work, might be to incorporate a false alarm model
(e.g., down-weighting putative occurrences which
look suspiciously like non-query words). Second,
we might consider training the degradation model
in a discriminative framework (e.g., training to op-
timize a measure that will penalize degradations
which cause false alarms, even if they are good can-
didates from the perspective of MLE). We hope that
the ideas presented in this paper will provide a solid
foundation for this future work.
189
References
W. Byrne et al 2004. Automatic Recognition of Spon-
taneous Speech for Access to Multilingual Oral His-
tory Archives. IEEE Transactions on Speech and Au-
dio Processing, Special Issue on Spontaneous Speech
Processing, 12(4):420?435, July.
U.V. Chaudhari and M. Picheny. 2007. Improvements in
phone based audio search via constrained match with
high order confusion estimates. Automatic Speech
Recognition & Understanding, 2007. ASRU. IEEE
Workshop on, pages 665?670, Dec.
Kareem Darwish and Walid Magdy. 2007. Error cor-
rection vs. query garbling for Arabic OCR document
retrieval. ACM Trans. Inf. Syst., 26(1):5.
Kareem M. Darwish. 2003. Probabilistic Methods for
Searching OCR-Degraded Arabic Text. Ph.D. thesis,
University of Maryland, College Park, MD, USA. Di-
rected by Bruce Jacob and Douglas W. Oard.
S. Deligne and F. Bimbot. 1997. Inference of Variable-
length Acoustic Units for Continuous Speech Recog-
nition. In ICASSP ?97: Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech, and Signal
Processing, pages 1731?1734, Munich, Germany.
Jonathan Fiscus et al 2006. English Spoken Term De-
tection 2006 Results. In Presentation at NIST?s 2006
STD Eval Workshop.
J.T. Foote et al 1997. Unconstrained keyword spot-
ting using phone lattices with application to spoken
document retrieval. Computer Speech and Language,
11:207?224.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In EMNLP ?07: Conference on Empiri-
cal Methods in Natural Language Processing, June.
Philipp Koehn et al 2003. Statistical phrase-based
translation. In NAACL ?03: Proceedings of the 2003
Conference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology, pages 48?54, Morristown, NJ,
USA. Association for Computational Linguistics.
Philipp Koehn et al 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In ACL ?07: Pro-
ceedings of the 2007 Conference of the Association
for Computational Linguistics, demonstration session,
June.
Okan Kolak. 2005. Rapid Resource Transfer for Mul-
tilingual Natural Language Processing. Ph.D. thesis,
University of Maryland, College Park, MD, USA. Di-
rected by Philip Resnik.
Jonathan Mamou and Bhuvana Ramabhadran. 2008.
Phonetic Query Expansion for Spoken Document Re-
trieval. In Interspeech ?08: Conference of the Interna-
tional Speech Communication Association.
Spyros Matsoukas et al 2005. The 2004 BBN 1xRT
Recognition Systems for English Broadcast News and
Conversational Telephone Speech. In Interspeech ?05:
Conference of the International Speech Communica-
tion Association, pages 1641?1644.
K. Ng and V.W. Zue. 2000. Subword-based approaches
for spoken document retrieval. Speech Commun.,
32(3):157?186.
J. Scott Olsson. 2008a. Combining Speech Retrieval Re-
sults with Generalized Additive Models. In ACL ?08:
Proceedings of the 2008 Conference of the Association
for Computational Linguistics.
J. Scott Olsson. 2008b. Vocabulary Independent Dis-
criminative Term Frequency Estimation. In Inter-
speech ?08: Conference of the International Speech
Communication Association.
Pavel Pecina, Petra Hoffmannova, Gareth J.F. Jones, Jian-
qiang Wang, and Douglas W. Oard. 2007. Overview
of the CLEF-2007 Cross-Language Speech Retrieval
Track. In Proceedings of the CLEF 2007 Workshop
on Cross-Language Information Retrieval and Evalu-
ation, September.
R. Prasad et al 2005. The 2004 BBN/LIMSI 20xRT En-
glish Conversational Telephone Speech Recognition
System. In Interspeech ?05: Conference of the Inter-
national Speech Communication Association.
S.E. Robertson. 1977. The Probability Ranking Princi-
ple in IR. Journal of Documentation, pages 281?286.
M. Saraclar and R. Sproat. 2004. Lattice-Based Search
for Spoken Utterance Retrieval. In NAACL ?04: Pro-
ceedings of the 2004 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology.
P. Schone et al 2005. Searching Conversational Tele-
phone Speech in Any of the World?s Languages.
Jian Shao et al 2008. Towards Vocabulary-Independent
Speech Indexing for Large-Scale Repositories. In In-
terspeech ?08: Conference of the International Speech
Communication Association.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In ICSLP ?02: Proceedings of 2002 In-
ternational Conference on Spoken Language Process-
ing.
I. H. Witten and T. C. Bell. 1991. The Zero-Frequency
Problem: Estimating the Probabilities of Novel Events
in Adaptive Text Compression. IEEE Trans. Informa-
tion Theory, 37(4):1085?1094.
Peng Yu and Frank Seide. 2005. Fast Two-
Stage Vocabulary-Independent Search In Spontaneous
Speech. In ICASSP ?05: Proceedings of the 2005
IEEE International Conference on Acoustics, Speech,
and Signal Processing.
P. Yu et al Sept. 2005. Vocabulary-Independent Index-
ing of Spontaneous Speech. IEEE Transactions on
Speech and Audio Processing, 13(5):635?643.
190
Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 33?36,
Rochester, April 2007. c?2007 Association for Computational Linguistics
Combining Evidence for Improved Speech Retrieval
J. Scott Olsson
Department of Mathematics
University of Maryland
College Park, MD 20742
olsson@math.umd.edu
Abstract
The goal of my dissertation research is
to investigate the combination of new ev-
idence sources for improving informa-
tion retrieval on speech collections. The
utility of these evidence sources is ex-
pected to vary depending on how well they
are matched to a collection?s domain. I
outline several new evidence sources for
speech retrieval, situate them in the con-
text of this domain dependency, and de-
tail several methods for their combination
with speech recognition output. Secondly,
I highlight completed and proposed work
for the production of this evidence.
1 Introduction and Goal
Early research in spoken document retrieval (SDR)
was spurred by a new way to overcome the high
cost of producing metadata (e.g., human assigned
topic labels) or manual transcripts for spoken doc-
uments: large vocabulary continuous speech recog-
nition. In this sense, SDR research has always been
about making do with the available evidence. With
the advent of automatic speech recognition (ASR),
this available evidence simply grew from being only
expensive human annotations to comparatively low-
cost machine producible transcripts.
But today even more evidence is available for re-
trieving speech: (1) Using ASR text as input fea-
tures, text classification can be applied to spoken
document collections to automatically produce topic
labels; (2) vocabulary independent spoken term de-
tection (STD) systems have been developed which
can search for query words falling outside of an
ASR system?s fixed vocabulary. These evidence
sources can be thought of as two bookends to the
spectrum of domain dependence and independence.
On one end, topic labels can significantly improve
retrieval performance but require the creation of
a (presumably domain-dependent) topic thesaurus
and training data. Furthermore, classification accu-
racy will be poor if the ASR system?s vocabulary is
badly matched to the collection?s speech (e.g., we
shouldn?t expect a classifier to sensibly hypothesize
automotive topics if the ASR system can not out-
put words about cars or driving). On the other end,
STD systems offer the most promise precisely when
the ASR system?s vocabulary is poorly matched to
the domain. If the ASR system?s vocabulary already
includes every word in the domain, after all, STD
can hardly be expected to help.
The primary goal of this dissertation is (1) to ex-
plore the combination of these new evidence sources
with the features available in ASR transcripts or
word lattices for SDR and (2) to determine their
suitability in various domain-matching conditions.
Secondarily, I?ll explore improving the production
of these new resources themselves (e.g., by classify-
ing with temporal domain knowledge or more robust
term detection methods).
Research in SDR has been inhibited by the ab-
sence of suitable test collections. The recently avail-
able MALACH collection of oral history data will,
in large part, make this dissertation research possible
(Oard et al, 2004). The MALACH test collection
33
contains about 1,000 hours of conversational speech
from 400 interviews with survivors of the Holo-
caust1. The interviews are segmented into 8,104
documents with topic labels manually assigned from
a thesaurus of roughly 40,000 descriptors. The
collection includes relevance assessments for more
than 100 topics and has been used for several years
in CLEF?s cross-language speech retrieval (CLSR)
track (Oard et al, 2006).
Participants in the CLEF CLSR evaluations have
already begun investigating evidence combination
for SDR, through the use of automatic topic labels?
although label texts are presently only used as an ad-
ditional field for indexing. In monolingual English
trials, this topic classification represents a significant
effort both in time and money (i.e., to produce train-
ing data), so that these evidence combination studies
have so far been rather domain dependent. Partici-
pants have also been using what are probably un-
naturally good ASR transcripts. The speech is emo-
tional, disfluent, heavily accented, and focused on a
somewhat rare topic, such that the ASR system re-
quired extensive tuning and adaptation to produce
the current word error rate of approximately 25%.
In this setting, we?d expect STD output and topic la-
bels to have low and high utility, respectively. To
investigate the domain mismatch case, I will apply
an off-the-shelf ASR system to produce new, com-
paratively poor, transcripts of the collection. In this
setting, we?d expect STD output and topic labels to
instead have high and low utility, respectively.
2 Proposed Combination Solutions
I will investigate improving SDR performance in
both the poorly and well matched domain conditions
through: (1) multiple approaches for utilizing auto-
matically produced topic labels and (2) the utiliza-
tion of STD output.
Throughout this paper, completed work will be
denoted with a ???, while proposed (non-complete,
future) work will be denoted with a ???.
1This is only a small subset of the entire MALACH col-
lection, which contains roughly 116,000 hours of speech from
52,000 interviews in 32 languages. This additional data also
provides training examples for classification.
2.1 Speech Classification for SDR
I outline three methods of incorporating evidence
from automatic classification for speech retrieval.
Creating Additional Indexable Text?
The simplest way to combine classification and
speech retrieval is to use the topic labels associ-
ated with the classes as indexable text. As a par-
ticipant on the MALACH project, I produced these
automatic topic labels (?keywords?) for the collec-
tion?s speech segments. These keywords were used
in this way in both years of the CLEF CLSR track.
For a top system in the track, using solely automat-
ically produced data (e.g., ASR transcripts and key-
word text), indexing keyword text gave a relative
improvement in mean average precision of 40.6%
over an identical run without keywords (Alzghool
and Inkpen, 2007).
Runtime Query Classification for SDR?
Simply using keyword text as an indexing field
is probably suboptimal because information seek-
ers don?t necessarily speak the same language as
the thesaurus constructors. An alternative is to clas-
sify the queries themselves at search time and to use
these label assignments to rank the documents. We
might expect this to be superior, insofar as infor-
mation seekers use language more like interviewees
(from which classification features are drawn) than
like thesaurus builders.
Class Guided Document Expansion?
A third option for using classification output is
as seed text for document expansion. The intuition
here is that ASR text may be a strong predictor for
a particular class label even if the ASR contains few
terms which a user might consider for a query. In
this sense, the class label text may represent a more
semantically dense representation of the segment?s
topical content. This denser representation may then
be a superior starting source for document centered
term expansion.
2.2 Unconstrained Term Detection for SDR?
It is not yet clear how best to combine a STD and
topical relevance IR system. One difficulty is that
IR systems count words (or putative occurrences of
words from an ASR system), while STD systems
34
report a score proportional to the confidence that a
word occurs in the audio. As a solution, I propose
normalizing the STD system?s score for OOV query
terms by a function of the STD system?s score on
putative occurrences of in-vocabulary terms. The
intuition here is that the ASR transcript is roughly
a ground truth representation of in-vocabulary term
occurrences and the score on OOV query terms
ought to reflect the STD system?s confidence in pre-
diction (which can be modeled from the STD sys-
tem?s score on ?ground truth? in-vocabulary term
occurrences). In this way, the presence or absence of
in-vocabulary terms and their associated STD confi-
dence scores can be used to learn a normalizer for
the STD system?s scores.
3 Producing the Evidence
In this section, I highlight both completed and pro-
posed work to improve the production of evidence
for combination.
3.1 Classifying with Temporal Evidence?
In spoken document collections, features beyond
merely the automatically transcribed words may ex-
ist. Consider, for example, the oral history data con-
tained in the MALACH collection. Each interview
in this collection can be thought of as a time ordered
set of spoken documents, produced by the guided
interview process. These documents naturally arise
in this context, and this temporal information can be
used to improve classification accuracy.
This work has so far focused on MALACH data,
although we expect the methods to be generally ap-
plicable to speech collections. For example, the top-
ical content of a television episode may often be
a good predictor of the subsequent episode?s topic.
Likewise, topics in radio, television, and podcasts
may tend to be seasonally dependent (based on Hol-
idays, recurring political or sporting events, etc.).
Time-shifted classification? One source of tem-
poral information in the MALACH data is the fea-
tures associated with temporally adjacent segments.
Terms may be class-predictive for not only their
own segment, but for the subsequent segments as
well. This intuition may be easily captured by a time
shifted classification (TSC) scheme. In TSC, each
training segment is labeled with the subsequent seg-
ment?s labels. During classification, each test seg-
ment is used to assign labels to its subsequent seg-
ment.
Temporal label weighting? We can also benefit
from non-local temporal information about a seg-
ment. For example, because interviewees were in-
structed to relate their story in chronological order,
we are more likely to find a discussion of childhood
at an interview?s beginning than at its end. We can
estimate the joint probability of labels and segment
times on held-out data and use this to bias new label
assignments. We call this approach temporal label
weighting (TLW).
In Olsson and Oard (2007), we showed that a
combined TSC and TLW approach on MALACH
data yields significant improvements on two sep-
arate label assignment tasks: conceptual and geo-
graphic thesaurus terms, with relative improvements
in mean average precision of 8.0% and 14.2% re-
spectively.
3.2 Classifying across languages?
In multilingual collections, training data for meta-
data creation may not be available for a particular
language?a good example of domain mismatch. If
however, training examples are available in a sec-
ond language, the metadata may still be produced
through cross-language text classification. In Ols-
son (2005), we used a probabilistic Czech-English
dictionary to transform Czech document vectors into
an English vector space before classifying them with
k-Nearest Neighbors and English training exam-
ples. In this study, the cross-language performance
achieved 73% of the monolingual English baseline
on conceptual topic assignment.
3.3 Vocabulary Independent Spoken Utterance
Retrieval?
In Olsson (2007), we examined a low resource ap-
proach to utterance retrieval using the expected pos-
terior count of n-grams in phonetic lattices as index-
ing units. A query?s phone subsequences are then
extracted and matched against the index to produce
a ranking on the lattices. Against a 1-best phone
sequence baseline, the approach was shown to sig-
nificantly improve the mean average precision of re-
trieved utterances on five human languages.
35
3.4 Improving Spoken Term Detection?
Phonetic lattices improve spoken term detection per-
formance by more accurately encoding the recog-
nizer?s uncertainty in prediction. Even so, a cor-
rect lattice may not always contain a path with
the query?s entire phone sequence. This is so not
only because of practical constraints on the size
(i.e., depth) of the lattice, but also because speak-
ers don?t always pronounce words with dictionary
precision. We?d like to allow approximate matching
of a query?s phone sequence with the phonetic lat-
tices, and to do this as quickly as possible. This time
requirement will prevent us from linearly scanning
through lattices for near matches. I am currently in-
vestigating two solutions to this problem: phonetic
query degradation and query expansion.
Phonetic query degradation? The idea in pho-
netic query degradation is to build an error model for
the phone recognition system and to then degrade
the query phone sequence such that it, hopefully,
will more closely resemble recognized sequences.
This approach incurs only a very slight cost in time
and is query independent (in the sense that any term
can be pushed through the degradation model?not,
for example, only terms for which we can find rec-
ognized examples).
Phonetic query expansion? The idea of phonetic
query expansion is, again, to transform the clean
phone sequence of the query into the degraded form
hypothesized by a recognizer. Instead of using a
degradation model however, we simply run a first
pass at STD with the non-degraded query term and
use the putative occurrences to learn new, alterna-
tive, degraded forms for a second search pass. This
can be thought of as blind relevance feedback or
query by (putative) example.
The advantage of this approach is that we are
not required to explicitly model the degradation pro-
cess. Disadvantages are that we (1) require exam-
ples which may not be available and (2) assume that
the degradation process is well represented by only
a few examples.
4 Contributions
This dissertation will significantly contribute to
speech retrieval research in several ways.
Can we improve SDR by evidence combination?
By exploring evidence combination, this dissertation
will advance the state of the art in speech retrieval
systems and their applicability to diverse domains. I
will investigate multiple methods for combining the
evidence presented by both STD and classification
systems with conventional ASR output (transcripts
or word lattices). This work will develop upon pre-
vious research which studied, in depth, the use of
only one evidence source, e.g., (Ng, 2000).
Can evidence combination decrease domain de-
pendency? I will investigate how combining evi-
dence sources can increase their applicability to new
content domains. This will include, for example, un-
derstanding how (vocabulary independent) STD sys-
tems can be paired with fixed vocabulary ASR.
How can these evidence sources be improved?
Lastly, I will explore how these new evidence
sources may themselves be improved. This will in-
clude utilizing temporal domain knowledge for clas-
sification and improving the robustness of phone-
based STD systems.
References
M. Alzghool and D. Inkpen. Experiments for the Cross
Language Spoken Retrieval Task at CLEF 2006. In
Not yet published.
K. Ng. 2000. Subword-based approaches for spoken
document retrieval. MIT dissertation.
D.W. Oard, et al Building an Information Retrieval Test
Collection for Spontaneous Conversational Speech. In
Proceedings of SIGIR?04.
D.W. Oard, et al 2006. Evaluation of Multilingual and
Multi-modal Information Retrieval. In Seventh Work-
shop of the Cross-Language Evaluation Forum, Ali-
cante, Spain. Selected Papers Series: Lecture Notes in
Computer Science.
J.S. Olsson and D.W. Oard. 2007. Improving Text Clas-
sification for Oral History Archives with Temporal Do-
main Knowledge. In Not yet published.
J.S. Olsson, et al Cross-language text classification. In
Proceedings of SIGIR?05.
J.S. Olsson, et al Fast Unconstrained Audio Search in
Numerous Human Languages. In ICASSP?07.
36
Proceedings of ACL-08: HLT, pages 461?469,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Combining Speech Retrieval Results with Generalized Additive Models
J. Scott Olsson? and Douglas W. Oard?
UMIACS Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park, MD 20742
Human Language Technology Center of Excellence
John Hopkins University, Baltimore, MD 21211
olsson@math.umd.edu, oard@umd.edu
Abstract
Rapid and inexpensive techniques for auto-
matic transcription of speech have the po-
tential to dramatically expand the types of
content to which information retrieval tech-
niques can be productively applied, but lim-
itations in accuracy and robustness must be
overcome before that promise can be fully
realized. Combining retrieval results from
systems built on various errorful representa-
tions of the same collection offers some po-
tential to address these challenges. This pa-
per explores that potential by applying Gener-
alized Additive Models to optimize the combi-
nation of ranked retrieval results obtained us-
ing transcripts produced automatically for the
same spoken content by substantially differ-
ent recognition systems. Topic-averaged re-
trieval effectiveness better than any previously
reported for the same collection was obtained,
and even larger gains are apparent when using
an alternative measure emphasizing results on
the most difficult topics.
1 Introduction
Speech retrieval, like other tasks that require trans-
forming the representation of language, suffers from
both random and systematic errors that are intro-
duced by the speech-to-text transducer. Limita-
tions in signal processing, acoustic modeling, pro-
nunciation, vocabulary, and language modeling can
be accommodated in several ways, each of which
make different trade-offs and thus induce different
? Dept. of Mathematics/AMSC, UMD
? College of Information Studies, UMD
error characteristics. Moreover, different applica-
tions produce different types of challenges and dif-
ferent opportunities. As a result, optimizing a sin-
gle recognition system for all transcription tasks is
well beyond the reach of present technology, and
even systems that are apparently similar on average
can make different mistakes on different sources. A
natural response to this challenge is to combine re-
trieval results from multiple systems, each imper-
fect, to achieve reasonably robust behavior over a
broader range of tasks. In this paper, we compare
alternative ways of combining these ranked lists.
Note, we do not assume access to the internal work-
ings of the recognition systems, or even to the tran-
scripts produced by those systems.
System combination has a long history in infor-
mation retrieval. Most often, the goal is to combine
results from systems that search different content
(?collection fusion?) or to combine results from dif-
ferent systems on the same content (?data fusion?).
When working with multiple transcriptions of the
same content, we are again presented with new op-
portunities. In this paper we compare some well
known techniques for combination of retrieval re-
sults with a new evidence combination technique
based on a general framework known as Gener-
alized Additive Models (GAMs). We show that
this new technique significantly outperforms sev-
eral well known information retrieval fusion tech-
niques, and we present evidence that it is the ability
of GAMs to combine inputs non-linearly that at least
partly explains our improvements.
The remainder of this paper is organized as fol-
lows. We first review prior work on evidence com-
461
bination in information retrieval in Section 2, and
then introduce Generalized Additive Models in Sec-
tion 3. Section 4 describes the design of our ex-
periments with a 589 hour collection of conversa-
tional speech for which information retrieval queries
and relevance judgments are available. Section 5
presents the results of our experiments, and we con-
clude in Section 6 with a brief discussion of implica-
tions of our results and the potential for future work
on this important problem.
2 Previous Work
One approach for combining ranked retrieval results
is to simply linearly combine the multiple system
scores for each topic and document. This approach
has been extensively applied in the literature (Bartell
et al, 1994; Callan et al, 1995; Powell et al, 2000;
Vogt and Cottrell, 1999), with varying degrees of
success, owing in part to the potential difficulty of
normalizing scores across retrieval systems. In this
study, we partially abstract away from this poten-
tial difficulty by using the same retrieval system on
both representations of the collection documents (so
that we don?t expect score distributions to be signif-
icantly different for the combination inputs).
Of course, many fusion techniques using more ad-
vanced score normalization methods have been pro-
posed. Shaw and Fox (1994) proposed a number
of such techniques, perhaps the most successful of
which is known as CombMNZ. CombMNZ has been
shown to achieve strong performance and has been
used in many subsequent studies (Lee, 1997; Mon-
tague and Aslam, 2002; Beitzel et al, 2004; Lillis et
al., 2006). In this study, we also use CombMNZ
as a baseline for comparison, and following Lil-
lis et al (2006) and Lee (1997), compute it in the
following way. First, we normalize each score si
as norm(si) =
si?min(s)
max(s)?min(s) , where max(s) and
min(s) are the maximum and minimum scores seen
in the input result list. After normalization, the
CombMNZ score for a document d is computed as
CombMNZd =
L?
`
Ns,d ? |Nd > 0|.
Here, L is the number of ranked lists to be com-
bined, N`,d is the normalized score of document d
in ranked list `, and |Nd > 0| is the number of non-
zero normalized scores given to d by any result set.
Manmatha et al (2001) showed that retrieval
scores from IR systems could be modeled using a
Normal distribution for relevant documents and ex-
ponential distribution for non-relevant documents.
However, in their study, fusion results using these
comparatively complex normalization approaches
achieved performance no better than the much sim-
pler CombMNZ.
A simple rank-based fusion technique is inter-
leaving (Voorhees et al, 1994). In this approach,
the highest ranked document from each list is taken
in turn (ignoring duplicates) and placed at the top of
the new, combined list.
Many probabilistic combination approaches have
also been developed, a recent example being Lillis
et al (2006). Perhaps the most closely related pro-
posal, using logistic regression, was made first by
Savoy et al (1988). Logistic regression is one exam-
ple from the broad class of models which GAMs en-
compass. Unlike GAMs in their full generality how-
ever, logistic regression imposes a comparatively
high degree of linearity in the model structure.
2.1 Combining speech retrieval results
Previous work on single-collection result fusion has
naturally focused on combining results from multi-
ple retrieval systems. In this case, the potential for
performance improvements depends critically on the
uniqueness of the different input systems being com-
bined. Accordingly, small variations in the same
system often do not combine to produce results bet-
ter than the best of their inputs (Beitzel et al, 2004).
Errorful document collections such as conversa-
tional speech introduce new difficulties and oppor-
tunities for data fusion. This is so, in particular,
because even the same system can produce drasti-
cally different retrieval results when multiple repre-
sentations of the documents (e.g., multiple transcript
hypotheses) are available. Consider, for example,
Figure 1 which shows, for each term in each of our
title queries, the proportion of relevant documents
containing that term in only one of our two tran-
script hypotheses. Critically, by plotting this propor-
tion against the term?s inverse document frequency,
we observe that the most discriminative query terms
are often not available in both document represen-
462
1
2
3
4
5
0.00.20.40.60.81.0
Inve
rse D
ocum
ent F
requ
ency
Proportion of relevant docs with term in only one transcript source
Figure 1: For each term in each query, the proportion of
relevant documents containing the term vs. inverse doc-
ument frequency. For increasingly discriminative terms
(higher idf ), we observe that the probability of only one
transcript containing the term increases dramatically.
tations. As these high-idf terms make large contri-
butions to retrieval scores, this suggests that even an
identical retrieval system may return a large score
using one transcript hypothesis, and yet a very low
score using another. Accordingly, a linear combina-
tion of scores is unlikely to be optimal.
A second example illustrates the difficulty. Sup-
pose recognition system A can recognize a particu-
lar high-idf query term, but system B never can. In
the extreme case, the term may simply be out of vo-
cabulary, although this may occur for various other
reasons (e.g., poor language modeling or pronuncia-
tion dictionaries). Here again, a linear combination
of scores will fail, as will rank-based interleaving.
In the latter case, we will alternate between taking a
plausible document from systemA and an inevitably
worse result from the crippled system B.
As a potential solution for these difficulties, we
consider the use of generalized additive models for
retrieval fusion.
3 Generalized Additive Models
Generalized Additive Models (GAMs) are a gen-
eralization of Generalized Linear Models (GLMs),
while GLMs are a generalization of the well known
linear model. In a GLM, the distribution of an ob-
served random variable Yi is related to the linear pre-
dictor ?i through a smooth monotonic link function
g,
g(?i) = ?i = Xi?.
Here, Xi is the ith row of the model matrix X (one
set of observations corresponding to one observed
yi) and ? is a vector of unknown parameters to be
learned from the data. If we constrain our link func-
tion g to be the identity transformation, and assume
Yi is Normal, then our GLM reduces to a simple lin-
ear model.
But GLMs are considerably more versatile than
linear models. First, rather than only the Normal dis-
tribution, the response Yi is free to have any distribu-
tion belonging to the exponential family of distribu-
tions. This family includes many useful distributions
such as the Binomial, Normal, Gamma, and Poisson.
Secondly, by allowing non-identity link functions g,
some degree of non-linearity may be incorporated in
the model structure.
A well known GLM in the NLP community is lo-
gistic regression (which may alternatively be derived
as a maximum entropy classifier). In logistic regres-
sion, the response is assumed to be Binomial and the
chosen link function is the logit transformation,
g(?i) = logit(?i) = log
(
?i
1? ?i
)
.
Generalized additive models allow for additional
model flexibility by allowing the linear predictor to
now also contain learned smooth functions fj of the
covariates xk. For example,
g(?i) = X?i ? + f1(x1i) + f2(x2i) + f3(x3i, x4i).
As in a GLM, ?i ? E(Yi) and Yi belongs to the
exponential family. Strictly parametric model com-
ponents are still permitted, which we represent as a
row of the model matrix X?i (with associated param-
eters ?).
GAMs may be thought of as GLMs where one
or more covariate has been transformed by a basis
expansion, f(x) =
?q
j bj(x)?j . Given a set of q
basis functions bj spanning a q-dimensional space
463
of smooth transformations, we are back to the lin-
ear problem of learning coefficients ?j which ?opti-
mally? fit the data. If we knew the appropriate trans-
formation of our covariates (say the logarithm), we
could simply apply it ourselves. GAMs allow us to
learn these transformations from the data, when we
expect some transformation to be useful but don?t
know it?s form a priori. In practice, these smooth
functions may be represented and the model pa-
rameters may be learned in various ways. In this
work, we use the excellent open source package
mgcv (Wood, 2006), which uses penalized likeli-
hood maximization to prevent arbitrarily ?wiggly?
smooth functions (i.e., overfitting). Smooths (in-
cluding multidimensional smooths) are represented
by thin plate regression splines (Wood, 2003).
3.1 Combining speech retrieval results with
GAMs
The chief difficulty introduced in combining ranked
speech retrieval results is the severe disagreement in-
troduced by differing document hypotheses. As we
saw in Figure 1, it is often the case that the most dis-
criminative query terms occur in only one transcript
source.
3.1.1 GLM with factors
Our first new approach for handling differences in
transcripts is an extension of the logistic regression
model previously used in data fusion work, (Savoy
et al, 1988). Specifically, we augment the model
with the first-order interaction of scores x1x2 and
the factor ?i, so that
logit{E(Ri)} = ?0+?i+x1?1+x2?2+x1x2?3,
where the relevance Ri ? Binomial. A factor is
essentially a learned intercept for different subsets
of the response. In this case,
?i =
?
?
?
?BOTH if both representations matched qi
?IBM only di,IBM matched qi
?BBN only di,BBN matched qi
where ?i corresponds to data row i, with associ-
ated document representations di,source and query
qi. The intuition is simply that we?d like our model
to have different biases for or against relevance
based on which transcript source retrieved the doc-
ument. This is a small-dimensional way of damp-
ening the effects of significant disagreements in the
document representations.
3.1.2 GAM with multidimensional smooth
If a document?s score is large in both systems, we
expect it to have high probability of relevance. How-
ever, as a document?s score increases linearly in one
source, we have no reason to expect its probability
of relevance to also increase linearly. Moreover, be-
cause the most discriminative terms are likely to be
found in only one transcript source, even an absent
score for a document does not ensure a document
is not relevant. It is clear then that the mapping
from document scores to probability of relevance is
in general a complex nonlinear surface. The limited
degree of nonlinear structure afforded to GLMs by
non-identity link functions is unlikely to sufficiently
capture this intuition.
Instead, we can model this non-linearity using a
generalized additive model with multidimensional
smooth f(xIBM , xBBN ), so that
logit{E(Ri)} = ?0 + f(xIBM , xBBN ).
Again, Ri ? Binomial and ?0 is a learned inter-
cept (which, alternatively, may be absorbed by the
smooth f ).
Figure 2 shows the smoothing transformation f
learned during our evaluation. Note the small de-
crease in predicted probability of relevance as the
retrieval score from one system decreases, while the
probability curves upward again as the disagreement
increases. This captures our intuition that systems
often disagree strongly because discriminative terms
are often not recognized in all transcript sources.
We can think of the probability of relevance map-
ping learned by the factor model of Section 3.1.1 as
also being a surface defined over the space of input
document scores. That model, however, was con-
strained to be linear. It may be visualized as a col-
lection of affine planes (with common normal vec-
tors, but each shifted upwards by their factor level?s
weight and the common intercept).
464
4 Experiments
4.1 Dataset
Our dataset is a collection of 272 oral history inter-
views from the MALACH collection. The task is
to retrieve short speech segments which were man-
ually designated as being topically coherent by pro-
fessional indexers. There are 8,104 such segments
(corresponding to roughly 589 hours of conversa-
tional speech) and 96 assessed topics. We follow the
topic partition used for the 2007 evaluation by the
Cross Language Evaluation Forum?s cross-language
speech retrieval track (Pecina et al, 2007). This
gives us 63 topics on which to train our combination
systems and 33 topics for evaluation.
4.2 Evaluation
4.2.1 Geometric Mean Average Precision
Average precision (AP) is the average of the pre-
cision values obtained after each document relevant
to a particular query is retrieved. To assess the
effectiveness of a system across multiple queries,
a commonly used measure is mean average preci-
sion (MAP). Mean average precision is defined as
the arithmetic mean of per-topic average precision,
MAP = 1n
?
n APn. A consequence of the arith-
metic mean is that, if a system improvement dou-
bles AP for one topic from 0.02 to 0.04, while si-
multaneously decreasing AP on another from 0.4 to
0.38, the MAP will be unchanged. If we prefer to
highlight performance differences on the lowest per-
forming topics, a widely used alternative is the geo-
metric mean of average precision (GMAP), first in-
troduced in the TREC 2004 robust track (Voorhees,
2006).
GMAP = n
?
?
n
APn
Robertson (2006) presents a justification and analy-
sis of GMAP and notes that it may alternatively be
computed as an arithmetic mean of logs,
GMAP = exp
1
n
?
n
log APn.
4.2.2 Significance Testing for GMAP
A standard way of measuring the significance of
system improvements in MAP is to compare aver-
age precision (AP) on each of the evaluation queries
using the Wilcoxon signed-rank test. This test, while
not requiring a particular distribution on the mea-
surements, does assume that they belong to an in-
terval scale. Similarly, the arithmetic mean of MAP
assumes AP has interval scale. As Robertson (2006)
has pointed out, it is in no sense clear that AP
(prior to any transformation) satisfies this assump-
tion. This becomes an argument for GMAP, since it
may also be defined using an arithmetic mean of log-
transformed average precisions. That is to say, the
logarithm is simply one possible monotonic trans-
formation which is arguably as good as any other,
including the identify transform, in terms of whether
the transformed value satisfies the interval assump-
tion. This log transform (and hence GMAP) is use-
ful simply because it highlights improvements on
the most difficult queries.
We apply the same reasoning to test for statistical
significance in GMAP improvements. That is, we
test for significant improvements in GMAP by ap-
plying the Wilcoxon signed rank test to the paired,
transformed average precisions, log AP. We handle
tied pairs and compute exact p-values using the Stre-
itberg & Ro?hmel Shift-Algorithm (1990). For topics
with AP = 0, we follow the Robust Track conven-
tion and add  = 0.00001. The authors are not aware
of significance tests having been previously reported
on GMAP.
4.3 Retrieval System
We use Okapi BM25 (Robertson et al, 1996) as
our basic retrieval system, which defines a document
D?s retrieval score for query Q as
s(D,Q) =
n?
i=1
idf(qi)
(k3+1)qfik3+qfi )f(qi, D)(k1 + 1)
f(qi, D) + k1(1? b+ b
|D|
avgdl )
,
where the inverse document frequency (idf ) is de-
fined as
idf(qi) = log
N ? n(qi) + 0.5
n(qi) + 0.5
,
N is the size of the collection, n(qi) is the docu-
ment frequency for term qi, qfi is the frequency of
term qi in query Q, f(qi, D) is the term frequency
of query term qi in document D, |D| is the length
of the matching document, and avgdl is the average
length of a document in the collection. We set the
465
BBN
 Sco
re
IBM Score
linear predictor
Figure 2: The two dimensional smooth f(sIBM, sBBN)
learned to predict relevance given input scores from IBM
and BBN transcripts.
parameters to k1 = 1, k3 = 1, b = .5, which gave
good results on a single transcript.
4.4 Speech Recognition Transcripts
Our first set of speech recognition transcripts was
produced by IBM for the MALACH project, and
used for several years in the CLEF cross-language
speech retrieval (CL-SR) track (Pecina et al, 2007).
The IBM recognizer was built using a manually
produced pronunciation dictionary and 200 hours
of transcribed audio. The resulting interview tran-
scripts have a reported mean word error rate (WER)
of approximately 25% on held out data, which was
obtained by priming the language model with meta-
data available from pre-interview questionnaires.
This represents significant improvements over IBM
transcripts used in earlier CL-SR evaluations, which
had a best reported WER of 39.6% (Byrne et al,
2004). This system is reported to have run at ap-
proximately 10 times real time.
4.4.1 New Transcripts for MALACH
We were graciously permitted to use BBN Tech-
nology?s speech recognition system to produce a
second set of ASR transcripts for our experiments
(Prasad et al, 2005; Matsoukas et al, 2005). We se-
lected the one side of the audio having largest RMS
amplitude for training and decoding. This channel
was down-sampled to 8kHz and segmented using an
available broadcast news segmenter. Because we did
not have a pronunciation dictionary which covered
the transcribed audio, we automatically generated
pronunciations for roughly 14k words using a rule-
based transliterator and the CMU lexicon. Using
the same 200 hours of transcribed audio, we trained
acoustic models as described in (Prasad et al, 2005).
We use a mixture of the training transcripts and var-
ious newswire sources for our language model train-
ing. We did not attempt to prime the language model
for particular interviewees or otherwise utilize any
interview metadata. For decoding, we ran a fast (ap-
proximately 1 times real time) system, as described
in (Matsoukas et al, 2005). Unfortunately, as we do
not have the same development set used by IBM, a
direct comparison of WER is not possible. Testing
on a small held out set of 4.3 hours, we observed our
system had a WER of 32.4%.
4.5 Combination Methods
For baseline comparisons, we ran our evaluation on
each of the two transcript sources (IBM and our new
transcripts), the linear combination chosen to opti-
mize MAP (LC-MAP), the linear combination cho-
sen to optimize GMAP (LC-GMAP), interleaving
(IL), and CombMNZ. We denote our additive fac-
tor model as Factor GLM, and our multidimensional
smooth GAM model as MD-GAM.
Linear combination parameters were chosen to
optimize performance on the training set, sweeping
the weight for each source at intervals of 0.01. For
the generalized additive models, we maximized the
penalized likelihood of the training examples under
our model, as described in Section 3.
5 Results
Table 1 shows our complete set of results. This
includes baseline scores from our new set of
transcripts, each of our baseline combination ap-
proaches, and results from our proposed combina-
tion models. Although we are chiefly interested in
improvements on difficult topics (i.e., GMAP), we
present MAP for comparison. Results in bold in-
dicate the largest mean value of the measure (ei-
ther AP or log AP), while daggers (?) indicate the
466
Type Model MAP GMAP
T IBM 0.0531 (-.2) 0.0134 (-11.8)
- BBN 0.0532 0.0152
- LC-MAP 0.0564 (+6.0) 0.0158 (+3.9)
- LC-GMAP 0.0587 (+10.3) 0.0154 (+1.3)
- IL 0.0592 (+11.3) 0.0165 (+8.6)
- CombMNZ 0.0550 (+3.4) 0.0150 (-1.3)
- Factor GLM 0.0611 (+14.9)? 0.0161 (+5.9)
- MD-GAM 0.0561 (+5.5)? 0.0180 (+18.4)?
TD IBM 0.0415 (-15.1) 0.0173 (-9.9)
- BBN 0.0489 0.0192
- LC-MAP 0.0519 (+6.1)? 0.0201 (+4.7)?
- LC-GMAP 0.0531 (+8.6)? 0.0200 (+4.2)
- IL 0.0507 (+3.7) 0.0210 (+9.4)
- CombMNZ 0.0495 (+1.2)? 0.0196 (+2.1)
- Factor GLM 0.0526 (+7.6)? 0.0198 (+3.1)
- MD-GAM 0.0529 (+8.2)? 0.0223 (+16.2)?
Table 1: MAP and GMAP for each combination ap-
proach, using the evaluation query set from the CLEF-
2007 CL-SR (MALACH) collection. Shown in paren-
theses is the relative improvement in score over the best
single transcripts results (i.e., using our new set of tran-
scripts). The best (mean) score for each condition is in
bold.
combination is a statistically significant improve-
ment (? = 0.05) over our new transcript set (that
is, over the best single transcript result). Tests for
statistically significant improvements in GMAP are
computed using our paired log AP test, as discussed
in Section 4.2.2.
First, we note that the GAM model with multi-
dimensional smooth gives the largest GMAP im-
provement for both title and title-description runs.
Secondly, it is the only combination approach able
to produce statistically significant relative improve-
ments on both measures for both conditions. For
GMAP, our measure of interest, these improve-
ments are 18.4% and 16.2% respectively.
One surprising observation from Table 1 is that
the mean improvement in log AP for interleaving is
fairly large and yet not statistically significant (it is
in fact a larger mean improvement than several other
baseline combination approaches which are signifi-
cant improvements. This may suggest that interleav-
ing suffers from a large disparity between its best
and worst performance on the query set.
0.001 0.002 0.005 0.010 0.020 0.050 0.100 0.200
0.00
1
0.00
2
0.00
5
0.01
0
0.02
0
0.05
0
Term recall in IBM transcripts
Term
 reca
ll in B
BN t
rans
cripts
impact guilt
attitudzionism
previou
assembl
Figure 3: The proportion of relevant documents returned
in IBM and BBN transcripts for discriminative title words
(title words occurring in less than .01 of the collection).
Point size is proportional to the improvement in average
precision using (1) the best linear combination chosen to
optimize GMAP (4) and (2) the combination using MD-
GAM (?).
Figure 3 examines whether our improvements
come systematically from only one of the transcript
sources. It shows the proportion of relevant docu-
ments in each transcript source containing the most
discriminative title words (words occurring in less
than .01 of the collection). Each point represents
one term for one topic. The size of the point is pro-
portional to the difference in AP observed on that
topic by using MD-GAM and by using LC-GMAP.
If the difference is positive (MD-GAM wins), we
plot ?, otherwise 4. First, we observe that, when
it wins, MD-GAM tends to increase AP much more
than when LC-GMAP wins. While there are many
wins also for LC-GMAP, the effects of the larger
MD-GAM improvements will dominate for many of
the most difficult queries. Secondly, there does not
appear to be any evidence that one transcript source
has much higher term-recall than the other.
5.1 Oracle linear combination
A chief advantage of our MD-GAM combination
model is that it is able to map input scores non-
linearly onto a probability of document relevance.
467
Type Model GMAP
T Oracle-LC-GMAP 0.0168
- MD-GAM 0.0180 (+7.1)
TD Oracle-LC-GMAP 0.0222
- MD-GAM 0.0223 (+0.5)
Table 2: GMAP results for an oracle experiment in
which MD-GAM was fairly trained and LC-GMAP was
unfairly optimized on the test queries.
To make an assessment of how much this capabil-
ity helps the system, we performed an oracle exper-
iment where we again constrained MD-GAM to be
fairly trained but allowed LC-GMAP to cheat and
choose the combination optimizing GMAP on the
test data. Table 2 lists the results. While the im-
provement with MD-GAM is now not statistically
significant (primarily because of our small query
set), we found it still out-performed the oracle linear
combination. For title-only queries, this improve-
ment was surprisingly large at 7.1% relative.
6 Conclusion
While speech retrieval is one example of retrieval
under errorful document representations, other sim-
ilar tasks may also benefit from these combination
models. This includes the task of cross-language re-
trieval, as well as the retrieval of documents obtained
by optical character recognition.
Within speech retrieval, further work also remains
to be done. For example, various other features are
likely to be useful in predicting optimal system com-
bination. These might include, for example, confi-
dence scores, acoustic confusability, or other strong
cues that one recognition system is unlikely to have
properly recognized a query term. We look forward
to investigating these possibilities in future work.
The question of how much a system should ex-
pose its internal workings (e.g., its document rep-
resentations) to external systems is a long standing
problem in meta-search. We?ve taken the rather nar-
row view that systems might only expose the list of
scores they assigned to retrieved documents, a plau-
sible scenario considering the many systems now
emerging which are effectively doing this already.
Some examples include EveryZing,1 the MIT Lec-
1http://www.everyzing.com/
ture Browser,2 and Comcast?s video search.3 This
trend is likely to continue as the underlying repre-
sentations of the content are themselves becoming
increasingly complex (e.g., word and subword level
lattices or confusion networks). The cost of expos-
ing such a vast quantity of such complex data rapidly
becomes difficult to justify.
But if the various representations of the con-
tent are available, there are almost certainly other
combination approaches worth investigating. Some
possible approaches include simple linear combi-
nations of the putative term frequencies, combina-
tions of one best transcript hypotheses (e.g., us-
ing ROVER (Fiscus, 1997)), or methods exploiting
word-lattice information (Evermann and Woodland,
2000).
Our planet?s 6.6 billion people speak many more
words every day than even the largest Web search
engines presently index. While much of this is
surely not worth hearing again (or even once!), some
of it is surely precious beyond measure. Separating
the wheat from the chaff in this cacophony is the rai-
son d?etre for information retrieval, and it is hard to
conceive of an information retrieval challenge with
greater scope or greater potential to impact our soci-
ety than improving our access to the spoken word.
Acknowledgements
The authors are grateful to BBN Technologies, who
generously provided access to their speech recogni-
tion system for this research.
References
Brian T. Bartell, Garrison W. Cottrell, and Richard K.
Belew. 1994. Automatic combination of multi-
ple ranked retrieval systems. In Proceedings of the
17th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 173?181.
Steven M. Beitzel, Eric C. Jensen, Abdur Chowdhury,
David Grossman, Ophir Frieder, and Nazli Goharian.
2004. Fusion of effective retrieval strategies in the
same information retrieval system. J. Am. Soc. Inf. Sci.
Technol., 55(10):859?868.
W. Byrne, D. Doermann, M. Franz, S. Gustman, J. Hajic,
D.W. Oard, M. Picheny, J. Psutka, B. Ramabhadran,
2http://web.sls.csail.mit.edu/lectures/
3http://videosearch.comcast.net
468
D. Soergel, T. Ward, and Wei-Jing Zhu. 2004. Au-
tomatic recognition of spontaneous speech for access
to multilingual oral history archives. IEEE Transac-
tions on Speech and Audio Processing, Special Issue
on Spontaneous Speech Processing, 12(4):420?435,
July.
J. P. Callan, Z. Lu, and W. Bruce Croft. 1995. Search-
ing Distributed Collections with Inference Networks .
In E. A. Fox, P. Ingwersen, and R. Fidel, editors, Pro-
ceedings of the 18th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 21?28, Seattle, Washington.
ACM Press.
G. Evermann and P.C. Woodland. 2000. Posterior prob-
ability decoding, confidence estimation and system
combination. In Proceedings of the Speech Transcrip-
tion Workshop, May.
Jonathan G. Fiscus. 1997. A Post-Processing System to
Yield Reduced Word Error Rates: Recogniser Output
Voting Error Reduction (ROVER). In Proceedings of
the IEEE ASRU Workshop, pages 347?352.
Jong-Hak Lee. 1997. Analyses of multiple evidence
combination. In SIGIR Forum, pages 267?276.
David Lillis, Fergus Toolan, Rem Collier, and John Dun-
nion. 2006. Probfuse: a probabilistic approach to data
fusion. In SIGIR ?06: Proceedings of the 29th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 139?146,
New York, NY, USA. ACM.
R. Manmatha, T. Rath, and F. Feng. 2001. Modeling
score distributions for combining the outputs of search
engines. In SIGIR ?01: Proceedings of the 24th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 267?275,
New York, NY, USA. ACM.
Spyros Matsoukas, Rohit Prasad, Srinivas Laxminarayan,
Bing Xiang, Long Nguyen, and Richard Schwartz.
2005. The 2004 BBN 1xRT Recognition Systems
for English Broadcast News and Conversational Tele-
phone Speech. In Interspeech 2005, pages 1641?1644.
Mark Montague and Javed A. Aslam. 2002. Condorcet
fusion for improved retrieval. In CIKM ?02: Proceed-
ings of the eleventh international conference on Infor-
mation and knowledge management, pages 538?548,
New York, NY, USA. ACM.
Pavel Pecina, Petra Hoffmannova, Gareth J.F. Jones, Jian-
qiang Wang, and Douglas W. Oard. 2007. Overview
of the CLEF-2007 Cross-Language Speech Retrieval
Track. In Proceedings of the CLEF 2007 Workshop
on Cross-Language Information Retrieval and Evalu-
ation, September.
Allison L. Powell, James C. French, James P. Callan,
Margaret E. Connell, and Charles L. Viles. 2000.
The impact of database selection on distributed search-
ing. In Research and Development in Information Re-
trieval, pages 232?239.
R. Prasad, S. Matsoukas, C.L. Kao, J. Ma, D.X. Xu,
T. Colthurst, O. Kimball, R. Schwartz, J.L. Gauvain,
L. Lamel, H. Schwenk, G. Adda, and F. Lefevre.
2005. The 2004 BBN/LIMSI 20xRT English Conver-
sational Telephone Speech Recognition System. In In-
terspeech 2005.
S. Robertson, S. Walker, S. Jones, and M. Hancock-
Beaulieu M. Gatford. 1996. Okapi at TREC-3. In
Text REtrieval Conference, pages 21?30.
Stephen Robertson. 2006. On GMAP: and other trans-
formations. In CIKM ?06: Proceedings of the 15th
ACM international conference on Information and
knowledge management, pages 78?83, New York, NY,
USA. ACM.
J. Savoy, A. Le Calve?, and D. Vrajitoru. 1988. Report on
the TREC-5 experiment: Data fusion and collection
fusion.
Joseph A. Shaw and Edward A. Fox. 1994. Combination
of multiple searches. In Proceedings of the 2nd Text
REtrieval Conference (TREC-2).
Bernd Streitberg and Joachim Ro?hmel. 1990. On tests
that are uniformly more powerful than the Wilcoxon-
Mann-Whitney test. Biometrics, 46(2):481?484.
Christopher C. Vogt and Garrison W. Cottrell. 1999. Fu-
sion via a linear combination of scores. Information
Retrieval, 1(3):151?173.
Ellen M. Voorhees, Narendra Kumar Gupta, and Ben
Johnson-Laird. 1994. The collection fusion problem.
In D. K. Harman, editor, The Third Text REtrieval Con-
ference (TREC-3), pages 500?225. National Institute
of Standards and Technology.
Ellen M. Voorhees. 2006. Overview of the TREC 2005
robust retrieval track. In Ellem M. Voorhees and L.P.
Buckland, editors, The Fourteenth Text REtrieval Con-
ference, (TREC 2005), Gaithersburg, MD: NIST.
Simon N. Wood. 2003. Thin plate regression splines.
Journal Of The Royal Statistical Society Series B,
65(1):95?114.
Simon Wood. 2006. Generalized Additive Models: An
Introduction with R. Chapman and Hall/CRC.
469
