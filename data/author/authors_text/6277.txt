Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 28?29,
Vancouver, October 2005.
THE MIT SPOKEN LECTURE PROCESSING PROJECT 
 
James R. Glass, Timothy J. Hazen, D. Scott Cyphers, Ken Schutte and Alex Park 
The MIT Computer Science and Artificial Intelligence Laboratory 
32 Vassar Street, Cambridge, Massachusetts, 02476, USA 
{hazen,jrg,cyphers}@csail.mit.edu 
 
Abstract 
We will demonstrate the MIT Spoken Lecture 
Processing Server and an accompanying lecture 
browser that students can use to quickly locate and 
browse lecture segments that apply to their query. 
We will show how lecturers can upload recorded 
lectures and companion text material to our server 
for automatic processing. The server automatically 
generates a time-aligned word transcript of the lec-
ture which can be downloaded for use within a 
browser. We will also demonstrate a browser we 
have created which allows students to quickly lo-
cate and browse audio segments that are relevant to 
their query. These tools can provide students with 
easier access to audio (or audio/visual) lectures, 
hopefully improving their educational experience. 
 
1 Introduction 
Over the past decade there has been increasing 
amounts of educational material being made avail-
able on-line. Projects such as MIT OpenCourse-
Ware provide continuous worldwide access to 
educational materials to help satisfy our collective 
thirst for knowledge. While the majority of such 
material is currently text-based, we are beginning 
to see dramatic increases in the amount of audio 
and visual recordings of lecture material. Unlike 
text materials, untranscribed audio data can be te-
dious to browse, making it difficult to utilize the 
information fully without time-consuming data 
preparation. Moreover, unlike some other forms of 
spoken communication such as telephone conver-
sations or television and radio broadcasts, lecture 
processing has until recently received little atten-
tion or benefit from the development of human 
language technology. The single biggest effort, to 
date, is on-going work in Japan using the Corpus 
of Spontaneous Japanese [1,3,4]. 
Lectures are particularly challenging for auto-
matic speech recognizers because the vocabulary 
used within a lecture can be very technical and 
specialized, yet the speaking style can be very 
spontaneous. As a result, even if parallel text mate-
rials are available in the form of textbooks or re-
lated papers, there are significant linguistic 
differences between written and oral communica-
tion styles. Thus, it is a challenge to predict how a 
written passage might be spoken, and vice versa. 
By helping to focus a research spotlight on spoken 
lecture material, we hope to begin to overcome 
these and many other fundamental issues. 
While audio-visual lecture processing will per-
haps be ultimately most useful, we have initially 
focused our attention on the problem of spoken 
lecture processing. Within this realm there are 
many challenging research issues pertaining to the 
development of effective automatic transcription, 
indexing, and summarization. For this project, our 
goals have been to a) help create a corpus of spo-
ken lecture material for the research community, b) 
analyze this corpus to better understand the lin-
guistic characteristics of spoken lectures, c) per-
form speech recognition and information retrieval 
experiments on these data to benchmark perform-
ance on these data, d) develop a prototype spoken 
lecture processing server that will allow educators 
to automatically annotate their recorded lecture 
data, and e) develop prototype software that will 
allow students to browse the resulting annotated 
lectures. 
2 Project Details 
As mentioned earlier, we have developed a web-
based Spoken Lecture Processing Server 
(http://groups.csail.mit.edu/sls/lectures) in which 
users can upload audio files for automatic tran-
scription and indexing. In our work, we have ex-
28
perimented with collecting audio data using a 
small personal digital audio recorder (an iRiver 
N10). To help the speech recognizer, users can 
provide their own supplemental text files, such as 
journal articles, book chapters, etc., which can be 
used to adapt the language model and vocabulary 
of the system.  Currently, the key steps of the tran-
scription process are as follows: a) adapt a topic-
independent vocabulary and language model using 
any supplemental text materials, b) automatically 
segment the audio file into short chunks of pause-
delineated speech, and c) automatically annotate 
these chunks using a speech recognition system. 
Language model adaptation is performed is two 
steps. First the vocabulary of any supplemental text 
material is extracted and added to an existing 
topic-independent vocabulary of nearly 17K 
words. Next, the recognizer merges topic-
independent word sequence statistics from an 
existing corpus of lecture material with the topic-
dependent statistics of the supplemental material to 
create a topic-adapted language model. 
The segmentation algorithm is performed in two 
steps. First the audio file is arbitrarily broken into 
10-second chunks for speech detection processing 
using an efficient speaker-independent phonetic 
recognizer. To help improve its speech detection 
accuracy, this recognizer contains models for non-
lexical artifacts such as laughs and coughs as well 
as a variety of other noises. Contiguous regions of 
speech are identified from the phonetic recognition 
output (typically 6 to 8 second segments of speech) 
and passed alone to our speech recognizer for 
automatic transcription. The speech segmentation 
and transcription steps are currently performed in a 
distributed fashion over a bank of computation 
servers. Once recognition is completed, the audio 
data is indexed (based on the recognition output) in 
preparation for browsing by the user. 
The lecture browser provides a graphical user in-
terface to one or more automatically transcribed 
lectures. A user can type a text query to the 
browser and receive a list of hits within the in-
dexed lectures. When a hit is selected, it is shown 
in the context of the lecture transcription. The user 
can adjust the duration of context preceding and 
following the hit, navigate to and from the preced-
ing and following parts of the lecture, and listen to 
the displayed segment. Orthographic segments are 
highlighted as they are played. 
 
3 Experimental Results 
To date we have collected and analyzed a corpus 
of approximately 300 hours of audio lectures in-
cluding 6 full MIT courses and 80 hours of semi-
nars from the MIT World web site [2]. We are 
currently in the process of expanding this corpus. 
From manual transcriptions we have generated and 
verified time-aligned transcriptions for 169 hours 
of our corpus, and we are in the process of time-
aligning transcriptions for the remainder of our 
corpus.  
We have performed initial speech recognition 
experiments using 10 computer science lectures. In 
these experiments we have discovered that, despite 
high word error rates (in the area of 40%), retrieval 
of short audio segments containing important key-
words and phrases can be performed with a high-
degree of reliability (over 90% F-measure when 
examining precision and recall results) [5]. These 
results are similar in nature to the findings in the 
SpeechBot project (which performs a similar ser-
vice for online broadcast news archives) [6]. 
References  
[1] S. Furui, ?Recent advances in spontaneous speech 
recognition and understanding,? in Proc. ISCA & IEEE 
Workshop on Spontaneous Speech Processing and Rec-
ognition (SSPR), pp. 1-6, Tokyo, April 2003. 
[2] J. Glass, T. Hazen, L. Hetherington, and C. Wang, 
?Analysis and Processing of Lecture Audio Data: Pre-
liminary Investigations,? in Proc. HLT/NAACL Speech 
Indexing Workshop, 9-12, Boston, May 2004. 
[3] T. Kawahara, H. Nanjo. And S. Furui,  ?Automatic 
transcription of spontaneous lecture speech,? in IEEE 
Workshop on Automatic Speech Recognition and Un-
derstanding, pp. 186-189, Trento, Italy, December 
2001. 
[4] H. Nanjo and T. Kawahara, ?Language model and 
speaking rate adaptation for spontaneous speech recog-
nition,? IEEE Transactions of Speech and Audio Proc-
essing, vol. 12, no. 4, pp. 391-400, July 2004. 
[5] A. Park, T. Hazen, and J. Glass, "Automatic 
Processing of Audio Lectures for Information Retrieval: 
Vocabulary Selection and Language Modeling," Proc. 
ICASSP, Philadelphia, PA, March 2005. 
[6] J.-M. Van Thong, et al ?SpeechBot: An experimen-
tal speech-based search engine for multimedia content 
on the web. IEEE Transactions of Multimedia, vol. 4, 
no. 1, pp. 88-96, March 2002.  
29
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 504?511,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Making Sense of Sound:
Unsupervised Topic Segmentation over Acoustic Input
Igor Malioutov, Alex Park, Regina Barzilay, and James Glass
Massachusetts Institute of Technology
{igorm,malex,regina,glass}@csail.mit.edu
Abstract
We address the task of unsupervised topic
segmentation of speech data operating over
raw acoustic information. In contrast to ex-
isting algorithms for topic segmentation of
speech, our approach does not require in-
put transcripts. Our method predicts topic
changes by analyzing the distribution of re-
occurring acoustic patterns in the speech sig-
nal corresponding to a single speaker. The
algorithm robustly handles noise inherent in
acoustic matching by intelligently aggregat-
ing information about the similarity profile
from multiple local comparisons. Our ex-
periments show that audio-based segmen-
tation compares favorably with transcript-
based segmentation computed over noisy
transcripts. These results demonstrate the
desirability of our method for applications
where a speech recognizer is not available,
or its output has a high word error rate.
1 Introduction
An important practical application of topic segmen-
tation is the analysis of spoken data. Paragraph
breaks, section markers and other structural cues
common in written documents are entirely missing
in spoken data. Insertion of these structural markers
can benefit multiple speech processing applications,
including audio browsing, retrieval, and summariza-
tion.
Not surprisingly, a variety of methods for
topic segmentation have been developed in the
past (Beeferman et al, 1999; Galley et al, 2003;
Dielmann and Renals, 2005). These methods typi-
cally assume that a segmentation algorithm has ac-
cess not only to acoustic input, but also to its tran-
script. This assumption is natural for applications
where the transcript has to be computed as part of the
system output, or it is readily available from other
system components. However, for some domains
and languages, the transcripts may not be available,
or the recognition performance may not be adequate
to achieve reliable segmentation. In order to process
such data, we need a method for topic segmentation
that does not require transcribed input.
In this paper, we explore a method for topic seg-
mentation that operates directly on a raw acoustic
speech signal, without using any input transcripts.
This method predicts topic changes by analyzing the
distribution of reoccurring acoustic patterns in the
speech signal corresponding to a single speaker. In
the same way that unsupervised segmentation algo-
rithms predict boundaries based on changes in lexi-
cal distribution, our algorithm is driven by changes
in the distribution of acoustic patterns. The central
hypothesis here is that similar sounding acoustic se-
quences produced by the same speaker correspond
to similar lexicographic sequences. Thus, by ana-
lyzing the distribution of acoustic patterns we could
approximate a traditional content analysis based on
the lexical distribution of words in a transcript.
Analyzing high-level content structure based on
low-level acoustic features poses interesting compu-
tational and linguistic challenges. For instance, we
need to handle the noise inherent in matching based
on acoustic similarity, because of possible varia-
504
tions in speaking rate or pronunciation. Moreover,
in the absence of higher-level knowledge, informa-
tion about word boundaries is not always discernible
from the raw acoustic input. This causes problems
because we have no obvious unit of comparison. Fi-
nally, noise inherent in the acoustic matching pro-
cedure complicates the detection of distributional
changes in the comparison matrix.
The algorithm presented in this paper demon-
strates the feasibility of topic segmentation over raw
acoustic input corresponding to a single speaker. We
first apply a variant of the dynamic time warping al-
gorithm to find similar fragments in the speech input
through alignment. Next, we construct a compari-
son matrix that aggregates the output of the align-
ment stage. Since aligned utterances are separated
by gaps and differ in duration, this representation
gives rise to sparse and irregular input. To obtain ro-
bust similarity change detection, we invoke a series
of transformations to smooth and refine the compar-
ison matrix. Finally, we apply the minimum-cut seg-
mentation algorithm to the transformed comparison
matrix to detect topic boundaries.
We compare the performance of our method
against traditional transcript-based segmentation al-
gorithms. As expected, the performance of the lat-
ter depends on the accuracy of the input transcript.
When a manual transcription is available, the gap
between audio-based segmentation and transcript-
based segmentation is substantial. However, in
a more realistic scenario when the transcripts are
fraught with recognition errors, the two approaches
exhibit similar performance. These results demon-
strate that audio-based algorithms are an effective
and efficient solution for applications where tran-
scripts are unavailable or highly errorful.
2 Related Work
Speech-based Topic Segmentation A variety of
supervised and unsupervised methods have been
employed to segment speech input. Some of these
algorithms have been originally developed for pro-
cessing written text (Beeferman et al, 1999). Others
are specifically adapted for processing speech input
by adding relevant acoustic features such as pause
length and speaker change (Galley et al, 2003; Diel-
mann and Renals, 2005). In parallel, researchers ex-
tensively study the relationship between discourse
structure and intonational variation (Hirschberg and
Nakatani, 1996; Shriberg et al, 2000). However,
all of the existing segmentation methods require as
input a speech transcript of reasonable quality. In
contrast, the method presented in this paper does
not assume the availability of transcripts, which pre-
vents us from using segmentation algorithms devel-
oped for written text.
At the same time, our work is closely related to
unsupervised approaches for text segmentation. The
central assumption here is that sharp changes in lex-
ical distribution signal the presence of topic bound-
aries (Hearst, 1994; Choi et al, 2001). These ap-
proaches determine segment boundaries by identi-
fying homogeneous regions within a similarity ma-
trix that encodes pairwise similarity between textual
units, such as sentences. Our segmentation algo-
rithm operates over a distortion matrix, but the unit
of comparison is the speech signal over a time in-
terval. This change in representation gives rise to
multiple challenges related to the inherent noise of
acoustic matching, and requires the development of
new methods for signal discretization, interval com-
parison and matrix analysis.
Pattern Induction in Acoustic Data Our work
is related to research on unsupervised lexical acqui-
sition from continuous speech. These methods aim
to infer vocabulary from unsegmented audio streams
by analyzing regularities in pattern distribution (de
Marcken, 1996; Brent, 1999; Venkataraman, 2001).
Traditionally, the speech signal is first converted into
a string-like representation such as phonemes and
syllables using a phonetic recognizer.
Park and Glass (2006) have recently shown the
feasibility of an audio-based approach for word dis-
covery. They induce the vocabulary from the au-
dio stream directly, avoiding the need for phonetic
transcription. Their method can accurately discover
words which appear with high frequency in the au-
dio stream. While the results obtained by Park and
Glass inspire our approach, we cannot directly use
their output as proxies for words in topic segmen-
tation. Many of the content words occurring only
a few times in the text are pruned away by this
method. Our results show that this data that is too
sparse and noisy for robustly discerning changes in
505
lexical distribution.
3 Algorithm
The audio-based segmentation algorithm identifies
topic boundaries by analyzing changes in the dis-
tribution of acoustic patterns. The analysis is per-
formed in three steps. First, we identify recurring
patterns in the audio stream and compute distortion
between them (Section 3.1). These acoustic patterns
correspond to high-frequency words and phrases,
but they only cover a fraction of the words that ap-
pear in the input. As a result, the distributional pro-
file obtained during this process is too sparse to de-
liver robust topic analysis. Second, we generate an
acoustic comparison matrix that aggregates infor-
mation from multiple pattern matches (Section 3.2).
Additional matrix transformations during this step
reduce the noise and irregularities inherent in acous-
tic matching. Third, we partition the matrix to iden-
tify segments with a homogeneous distribution of
acoustic patterns (Section 3.3).
3.1 Comparing Acoustic Patterns
Given a raw acoustic waveform, we extract a set of
acoustic patterns that occur frequently in the speech
document. Continuous speech includes many word
sequences that lack clear low-level acoustic cues to
denote word boundaries. Therefore, we cannot per-
form this task through simple counting of speech
segments separated by silence. Instead, we use a lo-
cal alignment algorithm to search for similar speech
segments and quantify the amount of distortion be-
tween them. In what follows, we first present a vec-
tor representation used in this computation, and then
specify the alignment algorithm that finds similar
segments.
MFCC Representation We start by transforming
the acoustic signal into a vector representation that
facilitates the comparison of acoustic sequences.
First, we perform silence detection on the original
waveform by registering a pause if the energy falls
below a certain threshold for a duration of 2s. This
enables us to break up the acoustic stream into con-
tinuous spoken utterances.
This step is necessary as it eliminates spurious
alignments between silent regions of the acoustic
waveform. Note that silence detection is not equiv-
alent to word boundary detection, as segmentation
by silence detection alone only accounts for 20% of
word boundaries in our corpus.
Next, we convert each utterance into a time se-
ries of vectors consisting of Mel-scale cepstral co-
efficients (MFCCs). This compact low-dimensional
representation is commonly used in speech process-
ing applications because it approximates human au-
ditory models.
The process of extracting MFCCs from the speech
signal can be summarized as follows. First, the 16
kHz digitized audio waveform is normalized by re-
moving the mean and scaling the peak amplitude.
Next, the short-time Fourier transform is taken at
a frame interval of 10 ms using a 25.6 ms Ham-
ming window. The spectral energy from the Fourier
transform is then weighted by Mel-frequency fil-
ters (Huang et al, 2001). Finally, the discrete cosine
transform of the log of these Mel-frequency spec-
tral coefficients is computed, yielding a series of 14-
dimensional MFCC vectors. We take the additional
step of whitening the feature vectors, which normal-
izes the variance and decorrelates the dimensions of
the feature vectors (Bishop, 1995). This whitened
spectral representation enables us to use the stan-
dard unweighted Euclidean distance metric. After
this transformation, the distances in each dimension
will be uncorrelated and have equal variance.
Alignment Now, our goal is to identify acoustic
patterns that occur multiple times in the audio wave-
form. The patterns may not be repeated exactly, but
will most likely reoccur in varied forms. We capture
this information by extracting pairs of patterns with
an associated distortion score. The computation is
performed using a sequence alignment algorithm.
Table 1 shows examples of alignments automati-
cally computed by our algorithm. The correspond-
ing phonetic transcriptions1 demonstrate that the
matching procedure can robustly handle variations
in pronunciations. For example, two instances of the
word ?direction? are matched to one another despite
different pronunciations, (?d ay? vs. ?d ax? in the
first syllable). At the same time, some aligned pairs
form erroneous matches, such as ?my prediction?
matching ?y direction? due to their high acoustic
1Phonetic transcriptions are not used by our algorithm and
are provided for illustrative purposes only.
506
Aligned Word(s) Phonetic Transcription
the x direction dh iy eh kcl k s dcl d ax r eh kcl sh ax n
D iy Ek^k s d^d @r Ek^S@n
the y direction dh ax w ay dcl d ay r eh kcl sh epi en
D @w ay d^ay r Ek^k S@n
of my prediction ax v m ay kcl k r iy l iy kcl k sh ax n
@v m ay k^k r iy l iy k^k S@n
acceleration eh kcl k s eh l ax r ey sh epi en
Ek^k s El @r Ey S- n
"
acceleration ax kcl k s ah n ax r eh n epi sh epi en
@k^k s 2n @r En - S- n
"
the derivation dcl d ih dx ih z dcl dh ey sh epi en
d^d IRIz d^D Ey S- n
"
a demonstration uh dcl d eh m ax n epi s tcl t r ey sh en
Ud^d Em @n - s t^t r Ey Sn
"
Table 1: Aligned Word Paths. Each group of rows
represents audio segments that were aligned to one
another, along with their corresponding phonetic
transcriptions using TIMIT conventions (Garofolo et
al., 1993) and their IPA equivalents.
similarity.
The alignment algorithm operates on the audio
waveform represented by a list of silence-free utter-
ances (u1, u2, . . . , un). Each utterance u? is a time
series of MFCC vectors ( ~x?1, ~x?2, . . . , ~x?m). Given
two input utterances u? and u??, the algorithm out-
puts a set of alignments between the corresponding
MFCC vectors. The alignment distortion score is
computed by summing the Euclidean distances of
matching vectors.
To compute the optimal alignment we use a vari-
ant of the dynamic time warping algorithm (Huang
et al, 2001). For every possible starting alignment
point, we optimize the following dynamic program-
ming objective:
D(ik, jk) = d(ik, jk) + min
?
?
?
?
?
D(ik ? 1, jk)
D(ik, jk ? 1)
D(ik ? 1, jk ? 1)
In the equation above, ik and jk are alignment end-
points in the k-th subproblem of dynamic program-
ming.
This objective corresponds to a descent through
a dynamic programming trellis by choosing right,
down, or diagonal steps at each stage.
During the search process, we consider not only
the alignment distortion score, but also the shape of
the alignment path. To limit the amount of temporal
warping, we enforce the following constraint:
?
?
(
ik ? i1
)
?
(
jk ? j1
)
?
? ? R,?k, (1)
ik ? Nx and jk ? Ny,
where Nx and Ny are the number of MFCC samples
in each utterance. The value 2R + 1 is the width of
the diagonal band that controls the extent of tempo-
ral warping. The parameter R is tuned on a develop-
ment set.
This alignment procedure may produce paths with
high distortion subpaths. Therefore, we trim each
path to retain the subpath with lowest average dis-
tortion and length at least L. More formally, given
an alignment of length N , we seek to find m and n
such that:
arg min
1?m?n?N
1
n ? m + 1
n
?
k=m
d(ik, jk) n?m ? L
We accomplish this by computing the length con-
strained minimum average distortion subsequence
of the path sequence using an O(N log(L)) algo-
rithm proposed by Lin et al(2002). The length
parameter, L, allows us to avoid overtrimming and
control the length of alignments that are found. Af-
ter trimming, the distortion of each alignment path
is normalized by the path length.
Alignments with a distortion exceeding a prespec-
ified threshold are pruned away to ensure that the
aligned phrasal units are close acoustic matches.
This parameter is tuned on a development set.
In the next section, we describe how to aggregate
information from multiple noisy matches into a rep-
resentation that facilitates boundary detection.
3.2 Construction of Acoustic Comparison
Matrix
The goal of this step is to construct an acoustic com-
parison matrix that will guide topic segmentation.
This matrix encodes variations in the distribution of
acoustic patterns for a given speech document. We
construct this matrix by first discretizing the acoustic
signal into constant-length blocks and then comput-
ing the distortion between pairs of blocks.
507
Figure 1: a) Similarity matrix for a Physics lecture constructed using a manual transcript. b) Similarity
matrix for the same lecture constructed from acoustic data. The intensity of a pixel indicates the degree of
block similarity. c) Acoustic comparison matrix after 2000 iterations of anisotropic diffusion. Vertical lines
correspond to the reference segmentation.
Unfortunately, the paths and distortions generated
during the alignment step (Section 3.1) cannot be
mapped directly to an acoustic comparison matrix.
Since we compare only commonly repeated acous-
tic patterns, some portions of the signal correspond
to gaps between alignment paths. In fact, in our cor-
pus only 67% of the data is covered by alignment
paths found during the alignment stage. Moreover,
many of these paths are not disjoint. For instance,
our experiments show that 74% of them overlap with
at least one additional alignment path. Finally, these
alignments vary significantly in duration, ranging
from 0.350 ms to 2.7 ms in our corpus.
Discretization and Distortion Computation To
compensate for the irregular distribution of align-
ment paths, we quantize the data by splitting the in-
put signal into uniform contiguous time blocks. A
time block does not necessarily correspond to any
one discovered alignment path. It may contain sev-
eral complete paths and also portions of other paths.
We compute the aggregate distortion score D(x, y)
of two blocks x and y by summing the distortions of
all alignment paths that fall within x and y.
Matrix Smoothing Equipped with a block dis-
tortion measure, we can now construct an acoustic
comparison matrix. In principle, this matrix can be
processed employing standard methods developed
for text segmentation. However, as Figure 1 illus-
trates, the structure of the acoustic matrix is quite
different from the one obtained from text. In a tran-
script similarity matrix shown in Figure 1 a), refer-
ence boundaries delimit homogeneous regions with
high internal similarity. On the other hand, looking
at the acoustic similarity matrix2 shown in Figure 1
b), it is difficult to observe any block structure cor-
responding to the reference segmentation.
This deficiency can be attributed to the sparsity of
acoustic alignments. Consider, for example, the case
when a segment is interspersed with blocks that con-
tain very few or no complete paths. Even though the
rest of the blocks in the segment could be closely
related, these path-free blocks dilute segment homo-
geneity. This is problematic because it is not always
possible to tell whether a sudden shift in scores sig-
nifies a transition or if it is just an artifact of irreg-
ularities in acoustic matching. Without additional
matrix processing, these irregularities will lead the
system astray.
We further refine the acoustic comparison matrix
using anisotropic diffusion. This technique has been
developed for enhancing edge detection accuracy in
image processing (Perona and Malik, 1990), and has
been shown to be an effective smoothing method in
text segmentation (Ji and Zha, 2003). When ap-
plied to a comparison matrix, anisotropic diffusion
reduces score variability within homogeneous re-
2We converted the original comparison distortion matrix to
the similarity matrix by subtracting the component distortions
from the maximum alignment distortion score.
508
gions of the matrix and makes edges between these
regions more pronounced. Consequently, this trans-
formation facilitates boundary detection, potentially
increasing segmentation accuracy. In Figure 1 c), we
can observe that the boundary structure in the dif-
fused comparison matrix becomes more salient and
corresponds more closely to the reference segmen-
tation.
3.3 Matrix Partitioning
Given a target number of segments k, the goal of
the partitioning step is to divide a matrix into k
square submatrices along the diagonal. This pro-
cess is guided by an optimization function that max-
imizes the homogeneity within a segment or mini-
mizes the homogeneity across segments. This opti-
mization problem can be solved using one of many
unsupervised segmentation approaches (Choi et al,
2001; Ji and Zha, 2003; Malioutov and Barzilay,
2006).
In our implementation, we employ the minimum-
cut segmentation algorithm (Shi and Malik, 2000;
Malioutov and Barzilay, 2006). In this graph-
theoretic framework, segmentation is cast as a prob-
lem of partitioning a weighted undirected graph
that minimizes the normalized-cut criterion. The
minimum-cut method achieves robust analysis by
jointly considering all possible partitionings of a
document, moving beyond localized decisions. This
allows us to aggregate comparisons from multiple
locations, thereby compensating for the noise of in-
dividual matches.
4 Evaluation Set-Up
Data We use a publicly available3 corpus of intro-
ductory Physics lectures described in our previous
work (Malioutov and Barzilay, 2006). This mate-
rial is a particularly appealing application area for an
audio-based segmentation algorithm ? many aca-
demic subjects lack transcribed data for training,
while a high ratio of in-domain technical terms lim-
its the use of out-of-domain transcripts. This corpus
is also challenging from the segmentation perspec-
tive because the lectures are long and transitions be-
tween topics are subtle.
3See http://www.csail.mit.edu/?igorm/
acl06.html
The corpus consists of 33 lectures, with an aver-
age length of 8500 words and an average duration
of 50 minutes. On average, a lecture was anno-
tated with six segments, and a typical segment cor-
responds to two pages of a transcript. Three lectures
from this set were used for development, and 30 lec-
tures were used for testing. The lectures were deliv-
ered by the same speaker.
To evaluate the performance of traditional
transcript-based segmentation algorithms on this
corpus, we also use several types of transcripts at
different levels of recognition accuracy. In addi-
tion to manual transcripts, our corpus contains two
types of automatic transcripts, one obtained using
speaker-dependent (SD) models and the other ob-
tained using speaker-independent (SI) models. The
speaker-independent model was trained on 85 hours
of out-of-domain general lecture material and con-
tained no speech from the speaker in the test set.
The speaker-dependent model was trained by us-
ing 38 hours of audio data from other lectures given
by the speaker. Both recognizers incorporated word
statistics from the accompanying class textbook into
the language model. The word error rates for the
speaker-independent and speaker-dependent models
are 44.9% and 19.4%, respectively.
Evaluation Metrics We use the Pk and WindowD-
iff measures to evaluate our system (Beeferman et
al., 1999; Pevzner and Hearst, 2002). The Pk mea-
sure estimates the probability that a randomly cho-
sen pair of words within a window of length k words
is inconsistently classified. The WindowDiff met-
ric is a variant of the Pk measure, which penalizes
false positives and near misses equally. For both of
these metrics, lower scores indicate better segmen-
tation accuracy.
Baseline We use the state-of-the-art mincut seg-
mentation system by Malioutov and Barzilay (2006)
as our point of comparison. This model is an appro-
priate baseline, because it has been shown to com-
pare favorably with other top-performing segmenta-
tion systems (Choi et al, 2001; Utiyama and Isa-
hara, 2001). We use the publicly available imple-
mentation of the system.
As additional points of comparison, we test the
uniform and random baselines. These correspond
to segmentations obtained by uniformly placing
509
Pk WindowDiff
MAN 0.298 0.311
SD 0.340 0.351
AUDIO 0.358 0.370
SI 0.378 0.390
RAND 0.472 0.497
UNI 0.476 0.484
Table 2: Segmentation accuracy for audio-based
segmentor (AUDIO), random (RAND), uniform
(UNI) and three transcript-based segmentation algo-
rithms that use manual (MAN), speaker-dependent
(SD) and speaker-independent (SI) transcripts. For
all of the algorithms, the target number of segments
is set to the reference number of segments.
boundaries along the span of the lecture and select-
ing random boundaries, respectively.
To control for segmentation granularity, we spec-
ify the number of segments in the reference segmen-
tation for both our system and the baselines.
Parameter Tuning We tuned the number of quan-
tized blocks, the edge cutoff parameter of the min-
imum cut algorithm, and the anisotropic diffusion
parameters on a heldout set of three development
lectures. We used the same development set for the
baseline segmentation systems.
5 Results
The goal of our evaluation experiments is two-fold.
First, we are interested in understanding the condi-
tions in which an audio-based segmentation is ad-
vantageous over a transcript-based one. Second, we
aim to analyze the impact of various design deci-
sions on the performance of our algorithm.
Comparison with Transcript-Based Segmenta-
tion Table 2 shows the segmentation accuracy
of the audio-based segmentation algorithm and three
transcript-based segmentors on the set of 30 Physics
lectures. Our algorithm yields an average Pk mea-
sure of 0.358 and an average WindowDiff mea-
sure of 0.370. This result is markedly better than
the scores attained by uniform and random seg-
mentations. As expected, the best segmentation re-
sults are obtained using manual transcripts. How-
ever, the gap between audio-based segmentation
and transcript-based segmentation narrows when the
recognition accuracy decreases. In fact, perfor-
mance of the audio-based segmentation beats the
transcript-based segmentation baseline obtained us-
ing speaker-independent (SI) models (0.358 for AU-
DIO versus Pk measurements of 0.378 for SI).
Analysis of Audio-based Segmentation A cen-
tral challenge in audio-based segmentation is how to
overcome the noise inherent in acoustic matching.
We addressed this issue by using anisotropic diffu-
sion to refine the comparison matrix. We can quan-
tify the effects of this smoothing technique by gener-
ating segmentations directly from the similarity ma-
trix. We obtain similarities from the distortions in
the comparison matrix by subtracting the distortion
scores from the maximum distortion:
S(x, y) = max
si,sj
[D(si, sj)] ? D(x, y)
Using this matrix with the min-cut algorithm, seg-
mentation accuracy drops to a Pk measure of 0.418
(0.450 WindowDiff). This difference in perfor-
mance shows that anisotropic diffusion compensates
for noise introduced during acoustic matching.
An alternative solution to the problem of irregu-
larities in audio-based matching is to compute clus-
ters of acoustically similar utterances. Each of the
derived clusters can be thought of as a unique word
type.4 We compute these clusters, employing a
method for unsupervised vocabulary induction de-
veloped by Park and Glass (2006). Using the out-
put of their algorithm, the continuous audio stream
is transformed into a sequence of word-like units,
which in turn can be segmented using any stan-
dard transcript-based segmentation algorithm, such
as the minimum-cut segmentor. On our corpus, this
method achieves disappointing results ? a Pk mea-
sure of 0.423 (0.424 WindowDiff). The result can
be attributed to the sparsity of clusters5 generated by
this method, which focuses primarily on discovering
the frequently occurring content words.
6 Conclusion and Future Work
We presented an unsupervised algorithm for audio-
based topic segmentation. In contrast to existing
4In practice, a cluster can correspond to a phrase, word, or
word fragment (See Table 1 for examples).
5We tuned the number of clusters on the development set.
510
algorithms for speech segmentation, our approach
does not require an input transcript. Thus, it can
be used in domains where a speech recognizer is
not available or its output is too noisy. Our ap-
proach approximates the distribution of cohesion
ties by considering the distribution of acoustic pat-
terns. Our experimental results demonstrate the util-
ity of this approach: audio-based segmentation com-
pares favorably with transcript-based segmentation
computed over noisy transcripts.
The segmentation algorithm presented in this pa-
per focuses on one source of linguistic information
for discourse analysis ? lexical cohesion. Multiple
studies of discourse structure, however, have shown
that prosodic cues are highly predictive of changes
in topic structure (Hirschberg and Nakatani, 1996;
Shriberg et al, 2000). In a supervised framework,
we can further enhance audio-based segmentation
by combining features derived from pattern analy-
sis with prosodic information. We can also explore
an unsupervised fusion of these two sources of in-
formation; for instance, we can induce informative
prosodic cues by using distributional evidence.
Another interesting direction for future research
lies in combining the results of noisy recogni-
tion with information obtained from distribution of
acoustic patterns. We hypothesize that these two
sources provide complementary information about
the audio stream, and therefore can compensate for
each other?s mistakes. This combination can be par-
ticularly fruitful when processing speech documents
with multiple speakers or background noise.
7 Acknowledgements
The authors acknowledge the support of the Microsoft Faculty
Fellowship and the National Science Foundation (CAREER
grant IIS-0448168, grant IIS-0415865, and the NSF Graduate
Fellowship). Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of the au-
thor(s) and do not necessarily reflect the views of the National
Science Foundation. We would like to thank T.J. Hazen for
his assistance with the speech recognizer and to acknowledge
Tara Sainath, Natasha Singh, Ben Snyder, Chao Wang, Luke
Zettlemoyer and the three anonymous reviewers for their valu-
able comments and suggestions.
References
D. Beeferman, A. Berger, J. D. Lafferty. 1999. Statistical mod-
els for text segmentation. Machine Learning, 34(1-3):177?
210.
C. Bishop, 1995. Neural Networks for Pattern Recognition,
pg. 38. Oxford University Press, New York, 1995.
M. R. Brent. 1999. An efficient, probabilistically sound algo-
rithm for segmentation and word discovery. Machine Learn-
ing, 34(1-3):71?105.
F. Choi, P. Wiemer-Hastings, J. Moore. 2001. Latent semantic
analysis for text segmentation. In Proceedings of EMNLP,
109?117.
C. G. de Marcken. 1996. Unsupervised Language Acquisition.
Ph.D. thesis, Massachusetts Institute of Technology.
A. Dielmann, S. Renals. 2005. Multistream dynamic Bayesian
network for meeting segmentation. In Proceedings Mul-
timodal Interaction and Related Machine Learning Algo-
rithms Workshop (MLMI?04), 76?86.
M. Galley, K. McKeown, E. Fosler-Lussier, H. Jing. 2003.
Discourse segmentation of multi-party conversation. In Pro-
ceedings of the ACL, 562?569.
J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallet,
N. Dahlgren, V. Zue. 1993. TIMIT Acoustic-Phonetic Con-
tinuous Speech Corpus. Linguistic Data Consortium, 1993.
M. Hearst. 1994. Multi-paragraph segmentation of expository
text. In Proceedings of the ACL, 9?16.
J. Hirschberg, C. H. Nakatani. 1996. A prosodic analysis of
discourse segments in direction-giving monologues. In Pro-
ceedings of the ACL, 286?293.
X. Huang, A. Acero, H.-W. Hon. 2001. Spoken Language Pro-
cessing. Prentice Hall.
X. Ji, H. Zha. 2003. Domain-independent text segmentation
using anisotropic diffusion and dynamic programming. In
Proceedings of SIGIR, 322?329.
Y.-L. Lin, T. Jiang, K.-M. Chao. 2002. Efficient algorithms
for locating the length-constrained heaviest segments with
applications to biomolecular sequence analysis. J. Computer
and System Sciences, 65(3):570?586.
I. Malioutov, R. Barzilay. 2006. Minimum cut model for
spoken lecture segmentation. In Proceedings of the COL-
ING/ACL, 25?32.
A. Park, J. R. Glass. 2006. Unsupervised word acquisition from
speech using pattern discovery. In Proceedings of ICASSP.
P. Perona, J. Malik. 1990. Scale-space and edge detection using
anisotropic diffusion. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 12(7):629?639.
L. Pevzner, M. Hearst. 2002. A critique and improvement of
an evaluation metric for text segmentation. Computational
Linguistics, 28(1):19?36.
J. Shi, J. Malik. 2000. Normalized cuts and image segmenta-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 22(8):888?905.
E. Shriberg, A. Stolcke, D. Hakkani-Tur, G. Tur. 2000.
Prosody-based automatic segmentation of speech into sen-
tences and topics. Speech Communication, 32(1-2):127?
154.
M. Utiyama, H. Isahara. 2001. A statistical model for domain-
independent text segmentation. In Proceedings of the ACL,
499?506.
A. Venkataraman. 2001. A statistical model for word dis-
covery in transcribed speech. Computational Linguistics,
27(3):353?372.
511
