143
144
145
146
Proceedings of the EACL 2009 Demonstrations Session, pages 53?56,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
A text-based search interface for Multimedia Dialectics
Katerina Pastra
Inst. for Language & Speech Processing
Athens, Greece
kpastra@ilsp.gr
Eirini Balta
Inst. for Language & Speech Processing
Athens, Greece
ebalta@ilsp.gr
Abstract
The growing popularity of multimedia
documents requires language technologies
to approach automatic language analysis
and generation from yet another perspec-
tive: that of its use in multimodal commu-
nication. In this paper, we present a sup-
port tool for COSMOROE, a theoretical
framework for modelling multimedia di-
alectics. The tool is a text-based search in-
terface that facilitates the exploration of a
corpus of audiovisual files, annotated with
the COSMOROE relations.
1 Introduction
Online multimedia content becomes more and
more accessible through digital TV, social net-
working sites and searchable digital libraries of
photographs and videos. People of different ages
and cultures attempt to make sense out of this data
and re-package it for their own needs, these being
informative, educational and entertainment ones.
Understanding and generation of multimedia dis-
course requires knowledge and skills related to the
nature of the interacting modalities and their se-
mantic interplay for formulating the multimedia
message.
Within such context, intelligent multimedia sys-
tems are expected to parse/generate such messages
or at least assist humans in these tasks. From an-
other perspective, everyday human communica-
tion is predominantly multimodal; as such, sim-
ilarly intuitive human-computer/robot interaction
demands that intelligent systems master ?among
others? the semantic interplay between differ-
ent media and modalities, i.e. they are able to
use/understand natural language and its reference
to objects and activities in the shared, situated
communication space.
It was more than a decade ago, when the lack
of a theory of how different media interact with
one another was indicated (Whittaker and Walker,
1991). Recently, such theoretical framework has
been developed and used for annotating a corpus
of audiovisual documents with the objective of us-
ing such corpus for developing multimedia infor-
mation processing tools (Pastra, 2008). In this pa-
per, we provide a brief overview of the theory and
the corresponding annotated corpus and present
a text-based search interface that has been devel-
oped for the exploration and the automatic expan-
sion/generalisation of the annotated semantic rela-
tions. This search interface is a support tool for
the theory and the related corpus and a first step
towards its computational exploitation.
2 COSMOROE
The CrOSs-Media inteRactiOn rElations (COS-
MOROE) framework describes multimedia di-
alectics, i.e. the semantic interplay between
images, language and body movements (Pastra,
2008). It uses an analogy to language discourse
analysis for ?talking? about multimedia dialectics.
It actually borrows terms that are widely used in
language analysis for describing a number of phe-
nomena (e.g. metonymy, adjuncts etc.) and adopts
a message-formation perspective which is remi-
niscent of structuralistic approaches in language
description. While doing so, inherent character-
istics of the different modalities (e.g. exhaustive
specificity of images) are taken into consideration.
COSMOROE is the result of a thorough,
inter-disciplinary review of image-language and
gesture-language interaction relations and charac-
teristics, as described across a number of disci-
plines from computational and semiotic perspec-
tives. It is also the result of observation and analy-
sis of different types of corpora for different tasks.
COSMOROE was tested for its coverage and de-
scriptive power through the annotation of a corpus
of TV travel documentaries. Figure 1 presents the
COSMOROE relations. There are three main rela-
53
tions: semantic equivalence, complementarity and
independence, each with each own subtypes.
Figure 1: The COSMOROE cross-media relations
For annotating a corpus with the COSMOROE
relations, a multi-faceted annotation scheme is
employed. COSMOROE relations link two or
more annotation facets, i.e. the modalities of
two or more different media. Time offsets
of the transcribed speech, subtitles, graphic-text
and scene text, body movements, gestures, shots
(with foreground and background distinction) and
keyframe-regions are identified and included in
COSMOROE relations. All visual data have been
labelled by the annotators with one or two-word
action or entity denoting tags. These labels have
resulted from a process of watching only the vi-
sual stream of the file. The labelling followed a
cognitive categorisation approach, that builds on
the ?basic level theory? of categorisation (Rosch,
1978). Currently, the annotated corpus consists
of 5 hours of TV travel documentaries in Greek
and 5 hours of TV travel documentaries in En-
glish. Three hours of the Greek files have under-
gone validation and a preliminary inter-annotator
agreement study has also been carried out (Pastra,
2008).
3 The COSMOROE Search Interface
Such rich semantically annotated multimedia cor-
pus requires a support tool that will serve the fol-
lowing:
? it will facilitate the active exploration and
presentation of the semantic interplay be-
tween different modalities for any user, illus-
trating the COSMOROE theory through spe-
cific examples from real audiovisual data
? it will serve as simple search interface for
general users, taking advantage of the rich se-
mantic annotation ?behind the scenes? for
more precise and intelligent retrieval of au-
diovisual files
? it will allow for observation and educated
decision-taking on how one could proceed
with mining the corpus or using it as train-
ing data for semantic multimedia processing
applications, and
? it will allow interfacing with semantic lexical
resources, computational lexicons, text pro-
cessing components and cross-lingual infor-
mation resources for automatically expand-
ing and generalising the data (semantic rela-
tions) one can mine from the corpus.
We have developed such tool, the COSMOROE
search interface. The interface itself is actually
a text-based search engine, that indexes and re-
trieves information from the COSMOROE anno-
tated corpus. The interface allows for both sim-
ple search and advanced search, depending on the
type and needs of the users. The advanced search
is designed for those who have a special interest
in multimedia semantics and/or ones who want to
develop systems that will be trained on the COS-
MOROE corpus. This advanced version allows
search in a text-based manner, in either of these
ways:
? Search using single or multiword query terms
(keywords) that are mentioned in the tran-
scribed speech (or other text) of the video
or in the visual labels set of its visual-units,
in order to find instantiations of different se-
mantic relations in which they participate;
? Search using a pair of single or multi-
word query terms (keywords) that are related
through (un)specified semantic relations;
? Search for specific types of relations and find
out how these are realized through actual in-
stances in a certain multimedia context;
? Search for specific modality types (e.g. spe-
cific types of gestures, image-regions etc.)
and find out all the different relations in
which they appear;
54
Figure 2 presents a search example, using the
advanced interface1. The user has opted to search
for all instances of the word ?bell? appearing in
the visual label of keyframe regions and/or video
shots and in particular ones in which the bell is
clearly shown either in the foreground or in the
background. In a similar way, the user can search
Figure 2: Search example
for concepts present in the audio part of the video,
through the use of the ?Transcribed Text? option or
make a multiple selection. Another possibility is
to use a ?Query 2? set, in conjunction, disjunction
or negation with ?Query 1?, in order to obtain the
relations through which two categories of concepts
are associated.
Multimedia relations can also be searched in-
dependently of their content, simply denoting the
desired type. Finally, the user can search for spe-
cial types of visual-units, such as body move-
ments, gestures, images, without defining the con-
cept they denote.
After executing the query, the user is presented
with the list of the results, grouped by the semantic
relation in which the visual labels ?in the exam-
ple case presented above? participate. Each hit
is accompanied by its transcribed speech. Indica-
tion of the number of results found is given and
the user has also the option to save the results of
the specific search session. By clicking on indi-
vidual hits in the result list, one may investigate
the corresponding relation particulars.
Figure 3 shows such detailed view of one of the
results of the query shown in Figure 2. All relation
1Only part of the advanced search interface is depicted for
the screenshot to be intelligible
Figure 3: Example result - relation template
components are presented, textual and visual ones.
There are links to the video file from which the re-
lation comes, at the specified time offsets. Also,
the user may watch the video clips of the modali-
ties that participate in the relation (e.g. a particu-
lar gesture) and/or a static image (keyframe) of a
participating image region (e.g. a specific object)
with the contour of the object highlighted.
In this example, one may see that the word
?monastery?, which was mentioned in the tran-
scribed speech of the file, is grounded to the video
sequence depicting a ?bell tower? in the back-
ground and to another image of a ?bell?, through
a metonymic relation of type ?part for whole?.
What is actually happening, from a semantic point
of view, is that although the video talks about a
?monastery?, it never actually shows the building,
it shows a characteristic part of it instead. In this
page, the option to save these relation elements as
a text file, is also provided.
Last, a user may get a quantified profile of the
contents of the database (the annotated corpus) in
terms of number of relations per type, per lan-
guage, per file, or even per file producer, number
of visual objects, gestures of different types, body
55
movements, word tokens, visual labels, frequen-
cies of such data per file/set of files, as well as co-
occurrence statistics on word-visual label pairs per
relation/file/language and other parameters.
For the novice or general user, a simple inter-
face is provided that allows the user to submit
a text query, with no other specifications. The
results consist of a hit list with thumbnails of
the video-clips related to the query and the cor-
responding transcribed utterance. Individual hits
lead to full viewing of the video clip. Further de-
tails on the hit, i.e. information an advanced user
would get, are available following the advance-
information link. The use of semantic relations in
multimedia data, in this case, is hidden in the way
results are sorted in the results list. The sorting fol-
lows a highly to less informative pattern relying
on whether the transcript words or visual labels
matched to the query participate in cross-media
relations or not, and in which relation. Automat-
ing the processing of audiovisual files for the ex-
traction of cross-media semantics, in order to get
this type of ?intelligence? in search and retrieval
within digital video archives, is the ultimate ob-
jective of the COSMOROE approach.
3.1 Technical Details
In developing the COSMOROE search interface,
specific application needs had to be taken into
consideration. The main goal was to develop a
text-based search engine module capable of han-
dling files in the .xml format and accessed by lo-
cal and remote users. The core implementation
is actually a web application, mainly based on
the Apache Lucene2 search engine library. This
choice is supported by Lucene?s intrinsic charac-
teristics, such as high-performance indexing and
searching, scalability and customization options
and open source, cross-platform implementation,
that render it one of the most suitable solutions for
text-based search.
In particular, we exploited and further devel-
oped the built-in features of Lucene, in order to
meet our design criteria:
? The relation specific .xml files were indexed
in a way that retained their internal tree
structure, while multilingual files can eas-
ily be handled during indexing and searching
phases;
2http://lucene.apache.org/
? The queries are formed in a text-like man-
ner by the user, but are treated in a combined
way by the system, that enables a relational
search, enhanced with linguistic capabilities;
? The results are shown using custom sorting
methods, making them more presentable and
easily browsed by the user;
? Since Lucene is written in Java the applica-
tion is basically platform-independent;
? The implementation of the Lucene search en-
gine as a web application makes it easily ac-
cessible to local and remote users, through a
simple web browser page.
During the results presentation phase, a special
issue had to be taken into consideration, that is
video sharing. Due to performance and security
reasons, the Red53 server is used, which is an open
source flash server, supporting secure streaming
video.
4 Conclusion: towards computational
modelling of multimedia dialectics
The COSMOROE search interface presented in
this paper is the first phase for supporting the
computational modelling of multimedia dialectics.
The tool aims at providing a user-friendly access
to the COSMOROE corpus, illustrating the theory
through specific examples and providing an inter-
face platform for reaching towards computational
linguistic resources and tools that will generalise
over the semantic information provided by the cor-
pus. Last, the tool illustrates the hidden intelli-
gence one could achieve with cross-media seman-
tics in search engines of the future.
References
K. Pastra. 2008. Cosmoroe: A cross-media rela-
tions framework for modelling multimedia dialec-
tics. Multimedia Systems, 14:299?323.
E. Rosch. 1978. Principles of categorization. In
E. Rosch and B. Lloyd, editors, Cognition and Cat-
egorization, chapter 2, pages 27?48. Lawrence Erl-
baum Associates.
S. Whittaker and M. Walker. 1991. Toward a theory
of multi-modal interaction. In Proceedings of the
National Conference on Artificial Intelligence Work-
shop on Multi-modal Interaction.
3http://osflash.org/red5/
56
Colouring Summaries BLEU
Katerina Pastra
Department of Computer Science
University of Sheffield
katerina@dcs.shef.ac.uk
Horacio Saggion
Department of Computing Science
University of Sheffield
saggion@dcs.shef.ac.uk
Abstract
In this paper we attempt to apply the
IBM algorithm, BLEU, to the output of
four different summarizers in order to
perform an intrinsic evaluation of their
output. The objective of this experiment
is to explore whether a metric, originally
developed for the evaluation of machine
translation output, could be used for as-
sessing another type of output reliably.
Changing the type of text to be evalu-
ated by BLEU into automatically gener-
ated extracts and setting the conditions
and parameters of the evaluation exper-
iment according to the idiosyncrasies
of the task, we put the feasibility of
porting BLEU in different Natural Lan-
guage Processing research areas under
test. Furthermore, some important con-
clusions relevant to the resources needed
for evaluating summaries have come up
as a side-effect of running the whole ex-
periment.
1 Introduction
Machine Translation and Automatic Summariza-
tion are two very different Natural Language Pro-
cessing (NLP) tasks with -among others- differ-
ent implementation needs and goals. They both
aim at generating text; however, the properties and
characteristics of these target texts vary consider-
ably. Simply put, in Machine Translation, the gen-
erated document should be an accurate and fluent
translation of the original document, in the target
language. In Summarization, the generated text
should be an informative, reduced version of the
original document (single-document summary), or
sets of documents (multi-document summary) in
the form of an abstract, or an extract. Abstracts
present an overview of the main points expressed
in the original document, while extracts consist of
a number of informative sentences taken directly
from the source document. The fact that, by their
very nature, automatically generated extracts carry
the single sentence qualities of the source docu-
ments1, may lead one to the conclusion that eval-
uating this type of text is trivial, as compared to
the evaluation of abstracts or even machine trans-
lation, since in the latter, one needs to be able to
evaluate the content of the generated translation in
terms of grammaticality, semantic equivalence to
the source document and other quality character-
istics (Hovy et al, 2002).
Though the evaluation of generated extracts is
not as demanding as the evaluation of Machine
Translation, it does have two critical idiosyncratic
aspects that render the evaluation task difficult:
? the compression level (word or sentence
level) and the compression rate of the source
document must be determined for the selec-
tion of the contents of the extract ; the val-
ues of these variables may greatly affect the
whole evaluation setup and the results ob-
tained
1Even if coherence issues may arise beyond the sentence
boundaries i.e. at the text level
? the very low agreement among human eval-
uators on what is considered to be ?impor-
tant information? for inclusion in the extract,
reaching sometimes the point of total dis-
agreement on the focus of the extract (Mani,
2001; Mani et al, 2001). The nature of this
disagreement on the adequacy of the extracts
is such that - by definition - cannot manifest
itself in Machine Translation; this is because
it refers to the adequacy of the contents cho-
sen to form the extract, rather than what con-
stitutes an adequate way of expressing all the
contents of the source document in a target
language.
The difference on the parameters to be taken
into consideration when performing evaluation
within these two NLP tasks presents a challenge
for porting evaluation metrics from the one re-
search area to the other. Given the relatively re-
cent success in achieving high correlations with
human judgement for Machine Translation evalua-
tion, using the IBM content-based evaluation met-
ric, BLEU (Papineni et al, 2001), we attempt to
run this same metric on system generated extracts;
this way we explore whether BLEU can be used
reliably in this research area and if so, which test-
ing parameters need to be taken into considera-
tion. First, we refer briefly to BLEU and its use
across different NLP areas, then we locate our ex-
periments relatively to this related work and we
describe the resources we used, the tools we de-
veloped and the parameters we set for running the
experiments. The description of these experiments
and the interpretation of the results follows. The
paper concludes with some preliminary observa-
tions we make as a result of this restricted, first
experimentation.
2 Using BLEU in NLP
Being an intrinsic evaluation measure
(Sparck Jones and Galliers, 1995), BLEU
compares the content of a machine translation
against an ?ideal? translation. It is based on
a ?weighted average of similar length phrase
matches? (n-grams), it is sensitive to longer
n-grams (the baseline being the use of up to 4-
grams) and it also includes a brevity penalty factor
for penalising shorter than the ?gold standard?
translations (Papineni et al, 2001; Doddington,
2002). The metric has been found to highly
correlate with human judgement, being at the
same time reliable even when run on different
documents and against different number of model
references. Experiments run by NIST (Dodding-
ton, 2002), checking the metric for consistency
and sensitivity, verified these findings and showed
that the metric distinguishes, indeed, between
quite similar systems. A slightly different version
of BLEU has been suggested by the same people,
which still needs to be put into comparative testing
with BLEU before any claims for its performance
are made.
BLEU has been used for evaluating different
types of NLP output to a small extent. In (Za-
jic et al, 2002), the algorithm has been used in
a specific Natural Language Generation applica-
tion: headline generation. The purpose of this
work was to use an automated metric for evalu-
ating a system generated headline against a hu-
man generated one, in order to draw conclusions
on the parameters that affect the performance of
a system and improve scoring similarity. In (Lin
and Hovy, 2002) BLEU has been applied on sum-
marization. The authors argue on the unstable
and unreliable nature of manual evaluation and
the low agreement among humans on the con-
tents of a reference summary. Lin and Hovy make
the case that automated metrics are necessary and
test their own modified recall metric, along with
BLEU itself, on single and multi-document sum-
maries and compare the results with human judge-
ment. Modified recall seems to reach very high
correlation scores, though direct comparative ex-
perimentation is needed for drawing conclusions
on its performance in relation to BLEU. The lat-
ter, has been shown to achieve 0.66 correlation
in single-document summaries at 100 words com-
pression rate and against a single reference sum-
mary. The correlation achieved by BLEU climbs
up to 0.82 when BLEU is run over and compared
against multiply judged document units, that could
be thought of as a sort of multiple reference sum-
maries. The correlation scores for multi-document
summaries are similar. Therefore, BLEU has been
found to correlate quite highly with human judge-
ment for the summarization task when multiple
judgement is involved, while -as Lin and Hovy
indicate- using a single reference is not adequate
for getting reliable results with high correlation
with the human evaluators.
It is this conclusion that Lin and Hovy have
drawn, that contradicts findings by the IBM and
NIST people for the importance of using multiple
references when using BLEU in Machine Trans-
lation. The use of either multiple references or
just a single reference has been proved not to af-
fect the reliability of the results provided by BLEU
(Papineni et al, 2001; Doddington, 2002), which
seems not to be the case in summarization. This
is not a surprise; comparisons of content-based
metrics for summarization in (Donaway et al,
2000) have led the authors to the conclusion that
such metrics correlate highly with human judge-
ment when the humans do not disagree substan-
tially. The fact that more than one reference sum-
maries are needed because of the low agreement
between human evaluators has been repeatedly
indicated in automatic summarization evaluation
(Mani, 2001).
We attempt to test BLEU?s reliability when
changing various evaluation parameters such as
the source documents, the reference summaries
used and even parameters unique to the evaluation
of summaries, such as the compression rate of the
extract. In doing so, we explore whether the met-
ric is indeed reliable only when using more than
a single reference and whether any other testing
parameter could compensate for lack of multiple
references, if used appropriately.
3 Evaluation Experiment
In this section, we will present a description of the
experiments themselves, along with the results ob-
tained and their analysis, preceded by information
on the corpus we used for our experiments and the
tools we developed for setting their parameters and
running them automatically.
3.1 Testing corpus
We make use of part of the language resources
(HKNews Corpus) developed during the 2001
Workshop on Automatic Summarization of Mul-
tiple (Multilingual) Documents (Saggion et al,
2002).
The documents of each cluster are all relevant
to a specific topic-query, so that they form, in fact,
thematic clusters. The texts are marked up on the
paragraph, sentence and word level. Annotations
with linguistic information (Part of speech tags
and morphological information), though marked
up on the documents have not been used in our
experiments at all. Three judges have assessed
the sentences in each cluster and have provided a
score on a scale from 0 to 10 (i.e. utility judge-
ment), expressing how important the sentence is
for the topic of the cluster (Radev et al, 2000).
In our experiments, we have used three document
clusters, each consisting of ten documents in En-
glish.
3.2 Summarizers
It is important to note, that our objective is not
to demonstrate how a particular summarization
methodology performs, but to analyse an evalua-
tion metric. The summaries used for the evalu-
ation were produced as extracts at different ?sen-
tence? (and not word) compression rates2. In or-
der to produce summarizers for our evaluation,
we use a robust summarisation system (Saggion,
2002) that makes use of components for seman-
tic tagging and coreference resolution developed
within the GATE architecture (Cunningham et al,
2002). The system combines GATE components
with well established statistical techniques devel-
oped for the purpose of text summarisation re-
search. The system supports ?generic? and query-
based summarisation addressing the need for user
adaptation3. For each sentence, the system com-
putes values for a number of ?shallow? summariza-
tion features: position of the sentence, term distri-
bution analysis, similarity of the sentence with the
document, similarity with the sentence at the lead-
ing part of the document, similarity of the sentence
with the query, named entity distribution analysis,
statistic cohesion, etc. The values of these features
are linearly combined to produce the sentence fi-
2We have to note that the level of compression i.e sentence
or word level, affects probably the evaluation of the summa-
rizers? output. Comparative testing could indicate whether
this is a crucial parameter for system evaluation.
3The software can be obtained from http://www.
dcs.shef.ac.uk/?saggion
nal score. Top-ranked sentences are annotated un-
til the target n% compression is achieved (an an-
notation set is produced for each summary that is
generated). Different summarization systems can
be deployed by setting-up the weights that par-
ticipate in the scoring formula. Note that as the
summarization components are not aware of the
compression parameter, one would expect specific
configurations to produce good extracts at differ-
ent compression rates and across documents.
We have configured four different summariz-
ers, namely, the ?query-based system? that com-
putes the similarity of each sentence of the source
document with the documents topic-query, in or-
der to decide whether to include a sentence in the
generated extract or not. We also have the ?Sim-
ple 1 system?, whose main feature is that it com-
putes the similarity of a sentence with the whole
document, the ?Simple 2 system? which is a lead
based summarizer and the ?Simple 3 system? that
blindly extracts the last part of the source docu-
ment.
3.3 Judge-based Summaries
Following the same methodology used in (Saggion
et al, 2002), we implemented a judge-based sum-
marization system that given a judge number (1,
2, 3, or all), it scores sentences based on a combi-
nation of the utility that the sentence has accord-
ing to the judge (or the sum of the utilities if ?all?)
and the position of the sentence (leading sentences
are preferred). These ?extracts? represent our gold-
standards for evaluation in our experiments. In
order to use the documents in a stand-alone way,
we have enriched the initial corpus mark-up and
added to each document information about cluster
number, cluster topic (or query) and all the infor-
mation about utility judgement (that information
was kept in separate files in the original HKNews
corpus).
3.4 Evaluation Software
We have developed a number of software compo-
nents to facilitate the evaluation and we make use
of the GATE development environment for testing
and processing. The evaluation package allows the
user to specify different reference extracts (judge-
based summarizers) and summarization systems to
be compared.
Co-selection comparison (i.e., precision and re-
call) is being done with modules obtained from
the GATE library (AnnotationDiff components).
Content-based comparison by the Bleu algorithm
was implemented as a Java class. The exact for-
mula provided by the developers of BLEU has
been implemented following the baseline config-
urations i.e use of 4-grams and uniform weights
summing to 1:
Bleu(S,R) = K(S,R) ? eBleu1(S,R)
Bleu1(S,R) =
?
i=1,2,...n
wi ? lg( |(Si
?Ri)|
|Si| )
K(S,R) =
{
1 if |S| > |R|
e(1?
|R|
|S| ) otherwise
wi = i?
j=1,2,...n j
for i = 1, 2, ..., n
where S and R are the system and reference
sets. Si and Ri are the ?bags? of i-grams for sys-
tem and reference. n is a parameter of our imple-
mentation, but for the purpose of our experiments
we have set n to 4.
3.5 Experiments
In our experiments we have treated compression
rates and clusters as variables each one being a
condition for the other and both dependent to a
third variable, the gold standard summary. We
ran BLEU in all different combinations in order to
see the main effects of each combination and the
interactions among them. In particular, we have
used three different text clusters, consisting of
texts that refer to the same topic: cluster 1197 on
?Museum exhibits and hours?, cluster 125 which
deals with ?Narcotics and rehabilitation? and clus-
ter 241 which refers to ?Fire safety and building
management?. For the texts of each cluster we
have three different reference summaries (created
according to the utility judgement score assigned
by human evaluators cf. 3.1 and 3.2). We will
refer to these as Reference1, Reference2 and Ref-
erence3. The judges behind these references are
all the same for the three text clusters with one ex-
ception: Reference1 in cluster 241 has not been
created by the same human evaluator as the Refer-
ence 1 summaries for the other two clusters. Last,
we ran the experiments at five different compres-
sion rates 4: 10%, 20%, 30%, 40% and 50%.
We first ran BLEU on the reference summaries
in order to check whether BLEU is consistent
in the data it produces concerning the agreement
among human evaluators. We tried all possible
combinations for comparing the reference sum-
maries; using at first Reference 1 as the gold stan-
dard, we ran BLEU over References 2 and 3 and
we did this for two clusters (since the third?s -241-
Reference 1 set of summaries had been created by
another judge - a fourth one). We did this for all
five compression rates separately. We repeated the
experiment changing the gold standard and the ref-
erences to be scored accordingly (i.e Reference 1
and 3 against 2, Reference 1 and 2 against 3). The
results we got were consistent neither across clus-
ters, nor within clusters across compression rates;
however the latter, did show a general tendency for
consistency which allows for some observations
to be made. In cluster 1197, References 1 and 2
are generally in higher agreement than with 3, a
fact verified regardless the reference chosen as a
gold standard. The fact that References 1 and 2 are
very close was also evident when both compared
against Reference 3; though the latter is generally
closer to Reference 2, the scores assigned to Ref-
erence 1 and 2 are extremely close. In cluster 125,
Reference 1 is consistently closer to 3, while 2 is
closer to 1 at some compression rates and closer
to 3 at others. These very close scores indicate
that all three references are similarly ?distant? one
from another, and no groupings of agreement can
actually be made. Agreement between reference
summaries augments as the compression rate also
increases, with the higher similarity scores always
found at the 50% compression rate and the lower
ones consistently found at 10%. Table 1 shows
a consistent ranking across compression rates in
cluster 1197 and an inconsistent one in cluster 125,
using in both cases Reference 2 as the gold stan-
dard. From this first experiment, the rankings of
4In our experiments compression is always performed at
the sentence level
the reference summaries seem to depend on the
different values of the variables used. If that is
the case, then one should use BLEU in summa-
rization only when determining specific values for
the evaluation experiment, that will guarantee re-
liable results; but how could one determine which
value(s) should be chosen? To explore things fur-
ther we decided to proceed with a second experi-
ment set up in a similar way.
In our second experiment we try to compare
the system generated extracts (and therefore the
performance of the four summarizers) against the
different human references. Again, the differ-
ent rounds of the experiment involve multiple pa-
rameters; the generated extracts of all three text
clusters are compared against each reference sum-
mary, against all reference summaries (integrated
summary) and at all five compression rates. Going
through the different stages of this experiment we
observe that:
? For Reference X within Cluster Y across
Compressions, the ranking of the systems is
not consistent
One does not get the same system ranking at dif-
ferent compression rates. The similarity of a gen-
erated extract to a specific reference summary is
the same at some compression rates, similar at oth-
ers (e.g the order of two of the systems swaps)
and totally different at other rates. No patterns
arise in the way that rankings are similar at spe-
cific compression rates; for example, in table 2,
there seems to be a prevailing ranking common in
four compression rates; however, the ranking pro-
vided at 10% is totally different, and no apparent
reason seems to justify this deviation (e.g. very
close scores). Furthermore, this agreement among
the four highest compression rates does not form
a pattern i.e it does not appear as such across clus-
ters or references.
? For Reference X at Compression Y across
Clusters, the ranking of the systems is not
consistent
In our experiments we were able to observe 15 dif-
ferent realisations of these testing configurations
and hardly did a case of consistency at a compres-
sion rate across clusters appeared.
Ref 2 - 1197 10% 20% 30% 40% 50%
Reference 1 0.50 - 1 0.67 - 1 0.73 - 1 0.73 - 1 0.79 - 1
Reference 3 0.34 - 2 0.51 - 2 0.52 - 2 0.63 - 2 0.69 - 2
Ref 2 - 125 10% 20% 30% 40% 50%
Reference 1 0.36 - 1 0.41 - 1 0.59 - 2 0.67 - 2 0.78 - 1
Reference 3 0.20 - 2 0.46 - 2 0.66 - 1 0.73 - 1 0.73 - 2
Table 1: Reference summary similarity scores and rankings across clusters and compression rates
Reference 3 10% 20% 30% 40% 50%
Query-based 0.44 - 2 0.50 - 1 0.58 - 1 0.66 - 1 0.71 - 1
Simple 1 0.10 - 3 0.23 - 3 0.48 - 3 0.57 - 3 0.64 - 3
Simple 2 0.52 - 1 0.45 - 2 0.53 - 2 0.62 - 2 0.68 - 2
Simple 3 0.03 - 4 0.07 - 4 0.08 - 4 0.11 - 4 0.11 - 4
Table 2: System scores and rankings for cluster 241, against Reference 3, at different compression rates
? For Reference All across Clusters at multiple
Compressions, the ranking of the systems is
consistent
Estimating similarity scores against Reference
All (use of multiple references cf. 3.2), proves to
provide reliable, consistent results across clusters
and compression rates. Table 3 presents the scores
and corresponding system rankings for two differ-
ent clusters and at the five different compression
rates. The prevailing system ranking is [1324],
which is what we would intuitively expect accord-
ing to the features of the summarizers we compare.
Some deviations from this ranking are due to very
small differences in the similarity scores assigned
to the systems5, which indicates the need for using
a larger testing corpus for the experiments.
So, the need for multiple references is evident;
BLEU is a consistent, reliable metric, but when
used in summarization, one has to apply it to mul-
tiple references in order to get reliable results.
This is not just a way to improve correlation with
human judgement (Lin and Hovy, 2002); it is a
crucial evaluation parameter that affects the qual-
ity of the automatic evaluation results. In our case
we had a balanced set of reference summaries to
work with, i.e none of them was too similar to an-
other. The more reference summaries one has and
the larger one?s testing corpus, the safer the con-
clusions drawn will be. However, what happens
when there is lack of such resources and especially
5For example, at the 10% compression rate, cluster 1197,
systems Simple 1 and Simple 2 swap places in the final rank-
ing with a 0.005 difference in their similarity scores
of multiple reference summaries? Is there a way
to use BLEU with a single reference summary and
still get reliable results back?
Looking at the results of our experiments, when
using each reference summary separately as a gold
standard, we realised that estimating the average
ranking of each system across multiple compres-
sion rates might lead to consistent rankings. Fol-
lowing the average rank aggregation techique (Ra-
jman and Hartley, 2001), we transfered the aver-
age scores each system got per text cluster at each
compression rate into ranks and computed the av-
erage rank of each system across all five compres-
sion rates per text cluster and against each refer-
ence summary. Table 4, shows the average system
rankings we got for each system at clusters 1197
and 125, using Reference 1, 2, and 3 separately.
[1324] is the average system ranking that is clearly
indicated in the vast majority of cases. The two
exceptions to this are due to extremely small dif-
ferences in average scores at specific compression
rates and indicate the need for scaling up our ex-
periment, a fact that has already been indicated by
the results of our experiment using multiple refer-
ences (Reference All).
4 Conclusions and Future Work
BLEU has been developed for measuring con-
tent similarity in terms of length and wording
between texts. For the evaluation of automati-
cally generated extracts, the metric is expected to
capture similarities between sentences not shared
by both the generated text and the model sum-
Ref All - 1197 10% 20% 30% 40% 50%
Query based 0.55 - 1 0.47 - 1 0.49 - 1 0.62 - 1 0.63 - 2
Simple 1 0.3184 - 2 0.32 - 3 0.40 - 3 0.49 - 3 0.62 - 3
Simple 2 0.3134 - 3 0.39 - 2 0.44 - 2 0.56 - 2 0.67 - 1
Simple 3 0.02 - 4 0.03 - 4 0.07 - 4 0.11 - 4 0.13 - 4
Ref All - 125 10% 20% 30% 40% 50%
Query based 0.44 - 1 0.43 - 1 0.57 - 1 0.72 - 1 0.7641 - 2
Simple 1 0.18 - 3 0.3684 - 2 0.54 - 2 0.60 - 3 0.68 - 3
Simple 2 0.32 - 2 0.3673 - 3 0.44 - 3 0.66 - 2 0.7691 - 1
Simple 3 0.03 - 4 0.06 - 4 0.07 - 4 0.10 - 4 0.14 - 4
Table 3: Systems? similarity scores and rankings using Reference All as gold standard
10% 20% 30% 40% 50% Average Rank
Ref 1 - 125 1324 1234 2134 1324 1234 1234
Ref 2 - 125 1324 1324 1324 1324 2314 1324
Ref 3 - 125 2314 2314 1324 1324 2314 2314
Ref 1 - 1197 1324 2314 1324 1324 2314 1324
Ref 2 - 1197 1324 1324 1324 1324 2314 1324
Ref 3 - 1197 1324 1324 1324 1324 2314 1324
Table 4: Systems? average rankings resulting from ranks at multiple compression rates in clusters 125 and
1197. (Systems assumed to be listed in alphabetical order: Query-based, Simple1, Simple2, Simple3)
mary. Going through the texts scored in the above
experiments, we found cases in which BLEU
does not actually capture content similarity to
such a granularity that a human would. Some-
times, this is because the order of the words
forming n-grams differs slightly but still conveys
the same meaning (e.g. ?...abusers reported...?
vs. ?...reported abusers...?) and most of the
times because there is no way to capture cases
of synonymy, paraphrasing (e.g. ?downward
tendency?/?falling trend?/?decrease?) and other
deeper semantic equivalence (e.g. ?number of X?
vs. ?9,000 of X?). Such phenomena are -of course-
expected from a statistical metric which involves
no linguistic knowledge at all. Our aim in this pa-
per was to shed some light on the conditions under
which the metric performs reliably within summa-
rization, given the different parameters that affect
evaluation in this NLP research area. From the re-
sults obtained by our preliminary experiments, we
have generally concluded that:
? Running BLEU over system generated sum-
maries using a single reference affects the re-
liability of the results provided by the metric.
The use of multiple references is a sine qua
non for reliable results
? Running BLEU over system generated sum-
maries at multiple compression rates and esti-
mating the average rank of each system might
yield consistent and reliable results even with
a single reference summary and therefore
compensate for lack of multiple reference
summaries
In order to draw more safe conclusions, we need
to scale our experiments considerably, and this is
already in progress. Many research questions need
still to be answered, such as how BLEU scores
correlate with results produced by other content-
based metrics used in summarization and else-
where. We hope that this preliminary, experimen-
tal work on porting evaluation metrics across dif-
ferent NLP research areas will function as a stim-
ulus for extensive and thorough research in this di-
rection.
References
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graph-
ical development environment for robust NLP tools
and applications. In ACL 2002.
G. Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurence
statistics. In Proceedings of HLT 2002, Human Lan-
guage Technology Conference, San Diego, CA.
R. Donaway, K. Drummey, and L. Mather. 2000. A
comparison of rankings produced by summariza-
tion evaluation measures. In Proceedings of the
ANLP-NAACL 2000 Workshop on Automatic Sum-
marization, Advanced Natural Language Processing
- North American of the Association for Computa-
tional Linguistics Conference, Seattle, DC.
E. Hovy, M. King, and A. Popescu-Belis. 2002. An in-
troduction to machine translation evaluation. In Pro-
ceedings of the LREC 2002 Workshop on Machine
Translation Evaluation: Human Evaluators Meet
Automated Metrics, Language Resources and Eval-
uation Conference. European Language Resources
Association (ELRA).
Ch. Lin and E. Hovy. 2002. Manual and automatic
evaluation of summariess. In Proceedings of the
ACL 2002 Workshop on Automatic Summarization,
Association for Computation Linguistics, Philadel-
phia, PA.
I. Mani, T. Firmin, and B. Sundheim. 2001. Summac:
A text summarization evaluation. Natural Language
Engineering.
I. Mani. 2001. Summarization evaluation: an
overview. In Proceedings of the NAACL 2001 Work-
shop on Automatic Summarization, North Chapter
of the Association for Computational Linguistics,
Pittsburgh, PA.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0 109-
022), IBM Research Division.
Dr. Radev, J. Hongyan, and M. Budzikowska. 2000.
Centroid-based summarization of multiple docu-
ments: sentence extraction, utility-based evaluation,
and user studies. In ANLP/NAACL Workshop on
Summarization, Seattle, WA, April.
M. Rajman and A. Hartley. 2001. Automatically pre-
dicting mt system rankings compatible with fluency,
adequacy or informativeness scores. In Proceed-
ings of the MT Summit 2001 Workshop on Machine
Translation Evaluation: Who did what to whom, Eu-
ropean Association for Machine Translation, Santi-
ago de Compostella, Spain.
H. Saggion, D. Radev., S. Teufel, L. Wai, and
S. Strassel. 2002. Developing infrastructure for
the evaluation of single and multi-document sum-
marization systems in a cross-lingual environment.
In 3rd International Conference on Language Re-
sources and Evaluation (LREC 2002), pages 747?
754, Las Palmas, Gran Canaria, Spain.
H. Saggion. 2002. Shallow-based Robust Summariza-
tion. In Automatic Summarization: Solutions and
Perspectives, ATALA, December, 14.
Karen Sparck Jones and Julia R. Galliers. 1995. Eval-
uating Natural Language Processing Systems: An
Analysis and Review. Number 1083 in Lecture
Notes in Artificial Intelligence. Springer.
D. Zajic, B. Dorr, and R. Schwartz. 2002. Automatic
headline generation for newspaper stories. In Pro-
ceedings of the ACL 2002 Workshop on Automatic
Summarization, Association for Computation Lin-
guistics, Philadelphia, PA.
