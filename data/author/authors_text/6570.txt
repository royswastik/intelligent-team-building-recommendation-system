Data-Or iented  Trans lat ion 
Arjen Poutsma 
Department ofComputational Linguistics 
University of Amsterdam 
the Nether lands 
pout sma@wins, uva. nl 
Abstract 
In this allicle, we present a statistical approach to 
machine translation that is based on Data-Oriented 
Parsing: l)ata-Oriented Translation (DOT). In DOT, 
we use linked subtree lmirs for creating a derivation 
of a source sentence. Each linked subhee pair has a 
certain probability, and consists of two trees: one 
in the source language and one in the target lan- 
guage. When a derbation has been formed with 
these subtree pairs, we can create a translation from 
this deriwition. Since there are typically many dif- 
ferent derivations of tile same sentence in the source 
language, there can be as many dilTemnt ranslations 
for it. The probability of a translation can be calcu- 
lated as the total probability of all tile derivations 
that form this translation. We give the computa- 
tional aspects for Ibis model, show tlmt we can con- 
vert each subtree imir into a productive rewrite rule, 
and that tile most probable translation can be com- 
imted by means of Monte Carlo disambiguation. H- 
nally, we discuss some pilot experiments with the 
Verbmobil COl\]mS. 
1 Introduction 
The Data-Oriented Parsing model has been pre- 
sented as a promising paradigm for natural hmguage 
processing (Scha, 1990; Bod, 1995; Bod, 1998). 
It has been shown that DOP has the ability to lo- 
cate syntactic and semantic dependencies, both of 
which are quite important for machine translation. 
We hope that, by basing our model on DOP, we can 
inherit these advantages, thus obtaining a new and 
interesting way to perform machine translation. 
In section 2, we describe this novel model by 
identifying its parameters, in section 3, we describe 
its comlmtational spects; in section 4, we discuss 
some pilot experiments with this model; and finally, 
in section 5, we give some issues open for filtum 
research. 
2 The Data-Oriented 35ranslation Model 
In this section, we will give the instantiation of a 
model that uses DOP for MT purposes, which we 
will call Data-Oriented Translation (DOT). t This 
model is largely based on DOPI (Bod, 1998, chapt. 
2). 
In DOT, we use linked subtree pairs as combi- 
national flagments, a Each linked subtree pair has 
a certain probability, and consists of a trec in the 
source language and a tree in the target language. 
By combining these fragments to form an an analy- 
sis of the soume sentence, we automatically gener- 
ate a translation, i.e. we form a derivation of both 
source sentence and target sentence. Since there 
am typically many different derivations which con- 
tain the same source sentence, there can be equally 
many different ranslations t\~r it. Tile probability of 
a translation can be calculated as the total probabil- 
ity of all the derivations that form this translation. 
Tile model presented here is capable of translat- 
ing between two hmguages only. This lilnitation is 
by no means a property of the model itself, but is 
chosen for simplicity and readability reasons only. 
The following parameters should be specified for 
a DOP-like approach to MT: 
1. tile representations of sentences that are as- 
sl imed, 
2. the fragments of these representations that can 
be used for generating new representations, 
3. the operator that is used to combine the flag- 
ments to form a translation, and 
I This is actually the second instantiation of such a frame- 
work. The original model (Poutsma, 1998; l)outsnm, 2000) had 
a major flaw, which resulted in translations that were simply in- 
correct, as pointed out by Way (1999). 
2Links between tree nodes were introduced for TAG trees, 
in (Schieber and Schabes, 1990), and put to use for Machine 
Translation by Abeilld et al (1990). 
635 
, I  f 
NP VP 
I 
Charles V NP 
I I 
likes Anne 
"S 
NP " -. VP 
\ Anne V -. PP 
I / '> .  
pla~t P NP 
I I 
a Charles 
Figure 1: A linked tree pair (T,., T~). 
4. the model that is used for determining the 
probability of a target sentence given a source 
sentence. 
In the explanation that follows, we will use a sub- 
script s to denote an element of the source language, 
and a subscript t to denote one of the target lan- 
guage. 
2.1 Representations 
In DOT, we basically use the same utterance- 
analysis as in DOPI (i.e. syntactically labeled 
phrase structure trees). To allow for translation ca- 
pabilities in tiffs model, we will use pairs of trees 
that incorporate semantic infonnation. The amounl 
of semantic information eed not be very detailed, 
since all we are interested in is semantic equiva- 
lence. Two trees 7\] and T2 are said to be semantic 
equivalents (denoted as TI "" 7~) iff TI can be re- 
placed with T2 without loss of meaning. 
We can now introduce the notion of links: a link 
symbolizes a semantic equivalence between two 
trees, or part of trees. It can occur at any level in 
the tree structure, except for the terminal level. 3 
The representation used in DOT is a 3-tuple 
(T,, Tt, ?), where ~ is a tree in the somce language, 
Tt is a tree in the target language, and ? is a function 
that maps between semantic equivalent parts in both 
trees. In the rest of this article, we will refer to this 
3-tuple as tile pair (T,, g) .  
Because of the semantic equivalence, a link nmst 
exist at the top level of the tree pair (Ts, Tt). Figure 1 
shows an example of two linked trees, the links are 
depicted graphically as dashed lines. 
3Links cannot occur at the terminal evel, since we map 
between semantic equivalent parts on the level of syntactic 
categories. 
2.2 Fragments 
Likewise, we will use linked subtrees as our flag- 
ments. Given a pair of linked trees (T~, Tt), a linked 
subtree pair of (T~, Tt) consists of two connected 
and linked subgraphs (t~, 6) of (77~, 7}) such that: 
1. for every pair of linked nodes in (t.,.,6), it holds 
that: 
(a) both nodes in (ts,lt} have either zero 
daughter nodes, 
or 
(b) both nodes have all the daughter nodes of 
the corresponding nodes in (T,, Tt) 
and 
2. every non-linked node in either t~. (or 6) has all 
the daughter nodes of the corresponding node 
in T, (T,), 
and 
3. both t, and ~ consist of more than one node. 
This definition has a number of consequences. 
First of all, it is morn restrictive than the DOPI def- 
inition for subtrees, thus resulting in a smaller or 
equal amount of subtrees per tree. Secondly, it de- 
fines a possible pair of linked subtl'ees. Typically, 
there are many pairs of linked subtrees for each set 
of linked trees. Thirdly, the linked tree pair itself 
is also a valid linked subtree pair. Finally, accord- 
ing to this definition, all the linked subtree pairs are 
semantic equivalents, since the semantic daughter 
nodes of the original tree are removed or retained si- 
multaneously (clause 1). The nodes for which a se- 
mantic equivalent does not exist are always retained 
(clause 2). 
We can now define the bag of linked subtree 
pailw, which we will use as a grammar. Given a 
corpus of linked trees C, the bag of linked subtree 
pairs of C is the bag in which linked subtree pairs 
occur exactly as often as they can be identified in 
C. 4 Figure 2 show the bag of linked subtree pairs 
for the linked tree pair (T,, Tt). 
2.3 Composition operator 
In DOT, we use the leftmost substitution opera- 
tor for forming combinations of grammar ules. 
The composition of tile linked tree pair {ts,6) and 
4The similarity between Example-based MT (Nagao, 1984) 
and DOT is clear: EBMT uses a database ofexamples toform 
a translation, whereas DOT uses a bag of structured trees. 
636 
,q~ . . . .  S 
NP VP NP ~ - VP 
I ~ ~"  I L?~ 
Charles V NP Anne V \ -. PP 
likes Anne l;Iaft P NP 
I I 
?~ Charles 
/ \ 
NP VP NP "- VP 
l ~ , "  >Q- - . .  
Charles V NP V \ PP 
likes plait P NP 
I I 
?t Charles 
? \ 
S - - -  S 
NP VP NP - VP 
~ ~ " I "v>~'-...\p 
V NP Anne P 
likes Anne pla) P NP I 
0 
/ \ 
NP VP NP " . .  VP 
/ 
V NP V \ PP 
likes" pla) P NP 
I 
gt 
NP NP NP NP 
I I I I 
Charles Charles Anne Anne 
Figure 2: The bag of liuked subtree pairs of (T,, 7~) 
(us,u,), written as (ts,tt)o (u.,.,u,), is deiined iff 
|he label of lhe leftmost nonterlninal \]inked fi'ontier 
uocle and the label of its linked counterpart are iden- 
tical to the labels of the root nodes of (u.~., ur). If this 
composition is defined, it yields a copy of (t,.,tt), in 
which a copy of u.,. has been substituted on t.,.'s left- 
most nonterminal linked frontier node, and a copy 
of ut has been substituted on the node's linked coun- 
terpart. The colnposition operation is illustrated in 
figure 3. 
Given a bag of linked subtree pairs B, a se- 
quence of compositions (ts~ , it, ) o . . .  o {t.~N, bN ), with 
(t.~i,b~) E B yielding a tree pair (T,,Tt) without non- 
terminal leaves is called a derivation D of (7~., 7~). 
2.4 Probabi l i ty  calculat ion 
To compute the probability of the target composi- 
tion, we make the same statistical assumptions as in 
DOPI with regard to independence and representa- 
tiou of the subtrees (Bed, 1998, p. 16). 
The probability of selecting a subtree pair (ts~bl 
is calculated by dividing the frequency of the sub- 
tree pair in the bag by the number of snbtrees that 
have the same root node labels in this bag. In other 
words, let I(t.,,t,)l be the number of times the sub- 
tree pair (t,.,tr} occurs in the bag of subtree pairs, 
and r(t) be the root node categories of t, then the 
probability assigned to (is,b) is 
p((t,,,t,)) - I(l.,,t,)l 
El,,,,,,,> :~(,,,.)=,-(,,.)~,-(,,,):,-(,,)I (",  ", )1 
(1) 
Given the assumptions that all subtree pairs 
are independent, Ihe probability of a derivation 
(ts~ ,hi) o . . .  o (GN,ttN) is equal to the product of the 
probabilities of the used subtree pairs. 
P(0.~,,t,,) o . . .o  (ts,,t,N>) = l-\[p((t,,,t, i)) 
i 
(2) 
The translation generated by a derivation is equal 
to the sentence yielded by the target trees of the 
derivation. Typically, a translation can be generated 
by a large number of different deriwltions, each of 
which has its own probability. Therefore, the prob- 
ability of a translation ws ~ wt is the sum of the 
probabilities of its derivations: 
P(w~, w, ) :  ~_P(D(ws,w,}) (3) 
637 
/ \ 
/ \ 
~ S ~ -  ~ S 
NP VP NP \ VP 
) 
V NP V \ PP 
likes plait P NP 
I 
71 
S S 
NP VP NP 
NP'NP I 
\] \] = Anne V NP' 
AtmeAnne \] 
likes 
VP 
V PP 
plaft P NP 
I I 
a Anne 
Figure 3: The composition operation 
The justification of this last equation is quite triv- 
ial. As in any statistical MT system, we wish to 
choose the target sentence w~ so as to maximize 
P(wtlw,) (Brown et al, 1990, p. 79). if we take the 
sum over all possible derivations that wele formed 
from Ws and derive wt, we can rewrite this as equa- 
tion 4, as seen below. Since both ws and wt are 
contained in Dlw,,w, ), we can remove them both and 
arrive at equation 5, which--as we maximize over 
wt--is equivalent to equation 3 above. 
maxP(wtlWs) = 
Wt 
= max E 
wr D{ws,wt) 
11qax E 
wt D(ws.Wt) 
P(w,, D<w,,w,)lw.d (4) 
P(D(w,,w,)) (5) 
3 Computational Aspects 
When translating using the DOT model, we can dis- 
tinguish between three computational stages: 
I. parsing: the formation of a derivation forest, 
2. translation: the transfer of the derivation for- 
est from the source language to the target lan- 
guage, 
3. disambiguation: the selection of the most 
probable translation from the derivation forest. 
3.1 Parsing 
In DOT, every subtlee pair (t~,tt) can be seen 
as a productive rewrite rule: (root(t~),root(tt)) 
(frontier(ts), frontier(tt)), where all linkage in the 
frontier nodes is retained. The linked non-terminals 
in the yield constitute the symbol pairs to which new 
roles (subtlee pairs) are applied. For instance, the 
rightmost subtree pair in tigure 3 can be rewritten as 
\ \[ 
(S, S) --+ ((Anne, likes, NP), (NP, pla~t, ~'t, Aune)) 
This rule can then be combined with nfles that have 
the root pair (NP, NP), and so on. 
If we only consider the left-side part of this rule, 
we can use algorithms that exist for context-free 
grammars, so that we can parse a sentence of n 
words with a time complexity which is polynomial 
in n. These algorithms give as output a chart-like 
derivation forest (Sima'an et al, 1994), which con- 
tains the tree pairs of all the derivations that can be 
formed. 
3.2 Translation 
Since every tree pair in the derivation forest contains 
a tree for the target language, the translation of this 
folest is trivial. 
3.3 Disambiguation 
In order to select he most probable translation, it is 
not efficient o compare all translations, ince there 
can be exponentially many of them. Furthermore, 
it has been shown that the Viterbi algorithm cannot 
be used to make the most probable selection from a 
DOP-like derivation forest (Sima'an, 1996). 
Instead, we use a random selection lnethod 
to generate derivations from the target derivation 
forest, otherwise known as Monte Carlo sam- 
pling (Bod, 1998, p. 4649).  In this method, 
the random choices of derivations ale based on the 
probabilities of the nnderlying subderivations. If we 
generate a large number of samples, we can esti- 
mate the most probable translation as the translation 
which results most often. The most probable trans- 
lation can be estimated as accurately as desired by 
making the number of random samples ufficiently 
large. 
4 Pilot Experiments 
In order to test the DOT-model, we did some pi- 
lot experiments with a small part of the Verbmo- 
bil corpus. This corpus consists of transliterated 
spoken appointment dialogues in German, English, 
638 
and Japanese. We only used the German and En- 
glish datasets, which were aligned at sentence level, 
and syntactically annotated using different annota- 
tion schemes. 5 
Naturally, the tree pairs in the corpus did not con- 
tain any links, so-- in order to make it useful for 
l )OT- -we had to analyze each tree pair, and place 
links where necessary. We also corrected tree pairs 
that were not aligned correctly. Figure 4 shows an 
example of a corrected and linked tree from our co l  
rection of the Verbmobil corpus. 
We used a blind testing method, dividing the 266 
trees of our corpus into an 85% training set of 226 
tree pairs, and a 15% test set of 40 tree pairs. We 
carried out three experiments, in both directions, 
each using a different split of training and test set. 
The 226 training set tree pairs were converted into 
fragments (i.e. subtree pairs), and were enriched 
with their corpus probabilities. The 40 sentences 
from the lest set served as input sentences: they 
were translated with the fragments from the train- 
ing set using a bottom-up chart parser, and disam- 
biguated by the Monte Carlo algorithm. The most 
probable translations were estinmted from probabil- 
ity distributions of 1500 sampled erivations, which 
accounts for a standard deviation ?5 < 0.013. Fi- 
nally, we compared the resulting trauslations wilh 
the original translation as given in the test set. We 
also fed tile tes! sentences inlo another MT-system: 
AltaVista's Babelfish, which is based on Systran. 6 
4.1 Evahmtion 
In a manner similar to (Brown et al, 1990, p. 83), 
we assigned each of the resulting sentences a cate- 
gory according to the following criteria. If the pro- 
duced sentence was exactly the stone as the actual 
Verbmobil translation, we assigned it the exact cat- 
ego W. If it was a legitimate translation of the source 
sentence but in different words, we assigned it the 
alternale category. If it made sense as a sentence, 
but could not be interpreted as a valid translation of 
the source sentence, we assigned it the wrong cat- 
egory. If the translation only yielded a part of the 
source sentence, we assigned it the partial category: 
either partial exact if it was a part of the actual Verb- 
mobil translation, or partial alternate if it was part 
of an alternate translation. Finally, if no translation 
5The Penn Treebank scheme for English; the Tiibingen 
schelne for Gernlan. 
6This service is available on the lnte,'net via ht tp : / /  
babel fish. al tavista, com. 
Exacl 
Vcrbmobil: 
Translated as: 
That woukl be very interesting. 
I)as wiire sehr inte,essant. 
l)as w~h'e sehr interessant. 
Alternale 
Verbmobil: 
Translated as: 
Wrong 
Verbmobil: 
Translated as: 
Parlial Exact 
Verbmobil: 
I will book the trains. 
ich buche die Zfige. 
Ich werdc (tie Ziige reservieren. 
Es ist ja keine Behgrde. 
It is not an administrative office 
you know. 
There is not an administrative office 
you know. 
Translated as: 
And as said 1 think the location of the 
branch office is posh. 
Und wit gesagt ich denke die Lage zur 
Filiale spricht Biinde ist. 
ich denke die Lage 
l'artial Alternate 
lch habe Preise veto Parkhotel 
ltannover da. 
Verbmobil: 1 have got prices for Hatmover 
Parkhotel here. 
Translated as: for Parkhotel Hannover 
Figure 5: Translation and classification examples. 
was given,  ? " we assigned it tile none category. Tile re- 
suits we obtained from Systran were also evaluated 
using this procedure. Figure 5 gives some classiIi- 
cation examples. 
The method of evaluation is very strict: even if 
ore" model generated a translation that had a better 
quality than the given Verbmobil translation, we still 
assigned it the (partial) alternate category. This can 
be seen in the second example in figure 5. 
4.2 Resu l ts  
The results that we obtained can be seen in table 1 
and 2. In both our experiments, the number of 
exact translations was somewhat higher tlmn Sys- 
trmfs, but Systran excelled at the number of al- 
ternate translations. This can be explained by the 
fact that Systran has a much larger lexicon, thus al- 
lowing it to form much more alternate translations. 
While it is meaningless to compare results obtained 
from different corpora, it may be interesting to note 
that Brown et al (1990) report a 5% exact match 
in experiments with the Hansard corpus, indicating 
that an exact match is very hard to achieve. 
The number of ungrammatical translations in our 
639 
J - .  
S S 
LK 1V~F" ~ _ -MD: -NP= - _ VP 
I - I 
machen NX NX ADVX / shall we VB NP ADVP 
I I I t I \[ 
wir es dann do it then 
Figure 4: Example of a linked tree pair in Verbmobil 
Corpus Categorical ccuracy 
Max. Size Correct Incorrect Partial 
Depth Exact Alternate Ungn Wrong Exact Alternate 
1 1263 16.22% 2 .70% t8.92% 18.92% 18.92% 24.32% 
2 2733 16.22% 2 .70% 32.43% 5.41% 27.03% 16.22% 
24.32% 13.51% 3 8228 
4 14192 
18.92% 5.41% 
18.92% 5.41% 
32.43% 5.41% 
32.43% 5.41% 24.32% 13.51% 
5 22147 18.92% 5 .41% 32.43% 5.41% 24.32% 13.51% 
6 27039 18.92% 5 .41% 32.43% 5.41% 27.03% 10.8t% 
18.92% 5.41% 33479 
Systran 
32.43% 5.41% 
18.92% 35.14% 8.11% 37.84% 
24.32% 13.51% 
0% 0% 
Table 1: Results of English to German translation experiments 
English to German experiment were much higher 
than Systran's (32% versus Systran's 19%); vice- 
versa it was much lower (13% versus Systran's 
21%). Since the German grammar is more complex 
than the English grammar, this result could be ex- 
pected. It is simpler to map a complex grammar to 
a simpler than vice-versa. 
The partial translations, which are quite useflfl for 
forming the basis of a post-edited, manual trans- 
lation, varied around 38% in our English to Ger- 
man experiments, and around 55% when translating 
from German to English. Systran is incapable of 
forming partial translations. 
As can be seen from the tables, we experimented 
with the maxinmm depth of the tree pairs used. We 
expected that the performance of the model would 
increase when we used deeper subtree pairs, since 
deeper structures allow for more complex struc- 
tures, and therefore better translations. Our exper- 
iments showed, however, that there was very little 
increase of performance as we increased the maxi- 
mum tree depth. A possible explanation is that the 
trees in our corpus contained a lot of lexical context 
(i.e. terminals) at very small tree depths. Instead 
of varying the maximum tree depth, we should ex- 
periment with varying the maximum tree width. We 
plan to perform such experiments in the future. 
5 Future work 
Though the findings presented in this article cover 
the most important issues regarding DOT, there are 
still some topics open for future research. 
As we stated in the previous section, we wish 
to see whether DOT's performance increases as we 
vary the maximum width of a tree. 
In the experiments it became clear that DOT lacks 
a large lexicon, thus resulting in less alternate trans- 
lations than Systran. By using an external lexicon, 
we can form a part-of-speech sequences fiom the 
source sentence, and use this sequence as input for 
DOT. The resulting target part-of-speech sequence 
can then be reformed into a target sentence. 
The experiments discussed in this article are pilot 
experiments, and do not account for much. In order 
to find more about DOT and its (dis)abilities, more 
experiments on larger corpora are required. 
6 Conclusion 
In this article, we have presented a new approach to 
machine translation: the Data-Oriented Translation 
model. This method uses linked subtree pairs for 
creating a derivation of a sentence. Each subtree- 
pair consists of two trees: one in the source lan- 
guage and one in the target language. Using these 
subtree pairs, we can form a derivation of a given 
source sentence, which can then be used to form a 
target sentence. The probability of a translation can 
640 
Corpus - Categorical ccuracy 
Max. Size Correct 
l)epth Exact Alternate 
1 1263 
2 2733 
3 8228 
15.38% 2.56% 
12.82% 7.69% 
12.82% 1{).26% 
Incorrect 
Ungr. Wrong 
12.82% 12.82% 
12.82% 12.82% 
l'artial 
Exact Alternate 
41.03% 15.38% 
35.90% 17.95% 
38.46% 17.95% 12.82% 7.69% 
4 14192 15.38% 7.69% 12.82% 10.26% 35.90% 17.95% 
5 22147 15.38% 5.13% 12.82% 12.82% 35.90% 17.95% 
6 27039 15.38% 5.13% 12.82% 10.26% 38.46% 17.95% 
oo 33479 15.38% 7.69% 12.82% 7.69% 38.46% 17.95% 
Systran 12.82% 25.64% 2(/.51% 41.03% 0% 1)% 
Table 2: Results of German to English translation experiments 
then be calculated as the total probability of all the 
derivations that form tiffs translation. 
The computational aspects of DOT have been dis- 
cussed, where we introduced a way to reform each 
subtree pair into a productive rewrite role so that 
well-known parsing algorithms can be used. We de- 
l:ermine the best translation by Monte Carlo sam- 
pling. 
We have discussed the results of some pilot ex- 
periments with a part of the Verbmobil corpus, and 
showed a method of evaluating them. The ewflua- 
tion showed that DOT produces less correct rans- 
lation than Systran, but also less incorrect ransla- 
tions. We expected to see an increase in perfor- 
mance as we increased the depth of subtree pairs 
used, but this was not the case. 
Finally, we supplied some topics which art open 
l'or future research. 
References 
A. Abeill6, Y. Schabes, and A.K. Joshi. 1990. Us- 
ing lexicalized tags for machine translation. In 
Proceedings of the 13th international col~\[erence 
on computational linguistics, volume 3, pages 1- 
6, Helsinki. 
R. Bod. 1995. Enriching linguistics with statistics: 
Pelformance models of natural language. Num- 
ber 1995-14 in ILLC Dissertation Series. Institute 
for Logic, Language and Computation, Amster- 
dam. 
R. Bod. 1998. Beyond grammar: an exl)erience- 
based theory of language. Number 88 in CSLI 
lecture notes. CSLI Publications, Stanford, Cali- 
fornia. 
J. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, 
E Jelinek, J. Lafferty, R. Mercer, and R Roossin. 
1990. A statistical approach to machine transla- 
tion. Computational Linguistics, 16(2):79-86. 
M. Nagao. 1984. A framework of a mechani- 
cal translation between Japanese and English by 
analogy principle. In A. Elithom and R. Banmji, 
editors, Artificial and Human Intelligence, chap- 
ter 11, pages 173-180. North-Holland, Amster- 
dam. 
A. Poulsma. 1998. Data-Oriented Translation. In 
Ninth Conference of Computational Linguistics 
in the Netherlands, Leuven, Belgium. Confer- 
ence presentation. 
A. Poutsma. 2000. Data-Oriented Translation: Us- 
ing the DOP framework for MT. Master's the- 
sis, Faculty of Mathematics, Computer Science, 
Physics and Astronomy, University of Amster- 
dam, the Netherlands. 
R. Scha. 1990. Taaltheorie n taaltechnologie; 
competence en performance. In Q.A.M. de Kort 
and G.L.J. Leerdam, editors, Coml)utertoel)assiu- 
gen in de Neerlandistiek. Landelijke Verenigmg 
van Neerlandici, Ahnere, the Netherlands. 
S.M. Schieber and Y. Sehabes. 1990. Synchronous 
tree-adjoining grammars. In Proceedings of the 
13th international cor(ference on computational 
linguistics, volume 3, pages 253-258, Helsinki. 
K. Sima'an, R. Bod, S. Krauwer, and R. Scha. 
1994. Efficient disambiguation by means of 
stochastic tree substitution grammars. In Pro- 
ceedings International Crmference ou New Meth- 
ods in Language Processing, Manchestel; UK. 
UMIST. 
K. Sima'an. 1996. Computational Complexity of 
Probabilistic Disambiguation by means of Tree 
Grammars. In Proceedings COLING-96, Copen- 
hagen, Denmark. 
A. Way. 1999. A hybrid amhitectum for robust MT 
using LFG-DOP. Journal of Experhnental and 
Theoretical Artificial Intelligence, 11(3). 
641 
