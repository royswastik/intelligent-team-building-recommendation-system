Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1005?1014, Prague, June 2007. c?2007 Association for Computational Linguistics
Learning to Merge Word Senses
Rion Snow Sushant Prakash
Computer Science Department
Stanford University
Stanford, CA 94305 USA
{rion,sprakash}@cs.stanford.edu
Daniel Jurafsky
Linguistics Department
Stanford University
Stanford, CA 94305 USA
jurafsky@stanford.edu
Andrew Y. Ng
Computer Science Department
Stanford University
Stanford, CA 94305 USA
ang@cs.stanford.edu
Abstract
It has been widely observed that different NLP appli-
cations require different sense granularities in order to
best exploit word sense distinctions, and that for many
applications WordNet senses are too fine-grained. In
contrast to previously proposed automatic methods for
sense clustering, we formulate sense merging as a su-
pervised learning problem, exploiting human-labeled
sense clusterings as training data. We train a discrimi-
native classifier over a wide variety of features derived
from WordNet structure, corpus-based evidence, and
evidence from other lexical resources. Our learned
similarity measure outperforms previously proposed
automatic methods for sense clustering on the task of
predicting human sense merging judgments, yielding
an absolute F-score improvement of 4.1% on nouns,
13.6% on verbs, and 4.0% on adjectives. Finally, we
propose a model for clustering sense taxonomies us-
ing the outputs of our classifier, and we make avail-
able several automatically sense-clustered WordNets
of various sense granularities.
1 Introduction
Defining a discrete inventory of senses for a word is
extremely difficult (Kilgarriff, 1997; Hanks, 2000;
Palmer et al, 2005). Perhaps the greatest obstacle is
the dynamic nature of sense definition: the correct
granularity for word senses depends on the appli-
cation. For language learners, a fine-grained set of
word senses may help in learning subtle distinctions,
while coarsely-defined senses are probably more
useful in NLP tasks like information retrieval (Gon-
zalo et al, 1998), query expansion (Moldovan and
Mihalcea, 2000), and WSD (Resnik and Yarowsky,
1999; Palmer et al, 2005).
Lexical resources such as WordNet (Fellbaum,
1998) use extremely fine-grained notions of word
sense, which carefully capture even minor distinc-
tions between different possible word senses (e.g.,
the 8 noun senses of bass shown in Figure 1). Pro-
ducing sense-clustered inventories of arbitrary sense
granularity is thus crucial for tasks which depend on
lexical resources like WordNet, and is also impor-
tant for the task of automatically constructing new
WordNet-like taxonomies. A solution to this prob-
lem must also deal with the constraints of the Word-
Net taxonomy itself; for example when clustering
two senses, we need to consider the transitive effects
of merging synsets.
The state of the art in sense clustering is insuffi-
cient to meet these needs. Current sense clustering
algorithms are generally unsupervised, each relying
on a different set of useful features or hand-built
rules. But hand-written rules have little flexibility
to produce clusterings of different granularities, and
previously proposed methods offer little in the di-
rection of intelligently combining and weighting the
many proposed features.
In response to these challenges, we propose a
new algorithm for clustering large-scale sense hier-
archies like WordNet. Our algorithm is based on a
supervised classifier that learns to make graduated
judgments corresponding to the estimated probabil-
ity that each particular sense pair should be merged.
This classifier is trained on gold standard sense clus-
tering judgments using a diverse feature space. We
are able to use the outputs of our classifier to produce
a ranked list of sense merge judgments by merge
probability, and from this create sense-clustered in-
ventories of arbitrary sense granularity.1
In Section 2 we discuss past work in sense cluster-
1We have made sense-clustered Wordnets using the al-
gorithms discussed in this paper available for download at
http://ai.stanford.edu/?rion/swn.
1005
INSTRUMENT 7: ...the lowest range of a family of musical instruments
FISH
4: the lean flesh of a saltwater fish of the family Serranidae
5: any of various North American freshwater fish with lean flesh
8: nontechnical name for any of numerous... fishes
SINGER
3: an adult male singer with the lowest voice
6: the lowest adult male singing voice
PITCH
1: the lowest part of the musical range
2: the lowest part in polyphonic music
Figure 1: Sense clusters for the noun bass; the eight
WordNet senses as clustered into four groups in the
SENSEVAL-2 coarse-grained evaluation data
ing, and the gold standard datasets that we use in our
work. In Section 3 we introduce our battery of fea-
tures; in Section 4 we show how to extend our sense-
merging model to cluster full taxonomies like Word-
Net. In Section 5 we evaluate our classifier against
thirteen previously proposed methods.
2 Background
A wide number of manual and automatic techniques
have been proposed for clustering sense inventories
and mapping between sense inventories of different
granularities. Much work has gone into methods for
measuring synset similarity; early work in this direc-
tion includes (Dolan, 1994), which attempted to dis-
cover sense similarities between dictionary senses.
A variety of synset similarity measures based on
properties of WordNet itself have been proposed;
nine such measures are discussed in (Pedersen et al,
2004), including gloss-based heuristics (Lesk, 1986;
Banerjee and Pedersen, 2003), information-content
based measures (Resnik, 1995; Lin, 1998; Jiang and
Conrath, 1997), and others. Other approaches have
used specific cues from WordNet structure to inform
the construction of semantic rules; for example, (Pe-
ters et al, 1998) suggest clustering two senses based
on a wide variety of structural cues from Word-
Net, including if they are twins (if two synsets share
more than one word in their synonym list) or if
they represent an example of autohyponymy (if one
sense is the direct descendant of the other). (Mihal-
cea and Moldovan, 2001) implements six semantic
rules, using twin and autohyponym features, in addi-
tion to other WordNet-structure-based rules such as
whether two synsets share a pertainym, antonym, or
are clustered together in the same verb group.
A large body of work has attempted to capture
corpus-based estimates of word similarity (Pereira
et al, 1993; Lin, 1998); however, the lack of
large sense-tagged corpora prevent most such tech-
niques from being used effectively to compare dif-
ferent senses of the same word. Some corpus-based
attempts that are capable of estimating similarity
between word senses include the topic signatures
method; here, (Agirre and Lopez, 2003) collect con-
texts for a polysemous word based either on sense-
tagged corpora or by using a weighted agglomera-
tion of contexts of a polysemous word?s monose-
mous relatives (i.e., single-sense synsets related by
hypernym, hyponym, or other relations) from some
large untagged corpus. Other corpus-based tech-
niques developed specifically for sense clustering
include (McCarthy, 2006), which uses a combina-
tion of word-to-word distributional similarity com-
bined with the JCN WordNet-based similarity mea-
sure, and work by (Chugur et al, 2002) in find-
ing co-occurrences of senses within documents in
sense-tagged corpora. Other attempts have exploited
disagreements between WSD systems (Agirre and
Lopez, 2003) or between human labelers (Chklovski
and Mihalcea, 2003) to create synset similarity
measures; while promising, these techniques are
severely limited by the performance of the WSD
systems or the amount of available labeled data.
Some approaches for clustering have made use of
regular patterns of polysemy among words. (Pe-
ters et al, 1998) uses the COUSIN relation defined
in WordNet 1.5 to cluster hyponyms of categorically
related noun synsets, e.g., ?container/quantity? (e.g.,
for clustering senses of ?cup? or ?barrel?) or ?or-
ganization/construction? (e.g., for the building and
institution senses of ?hospital? or ?school?); other
approaches based on systematic polysemy include
the hand-constructed CORELEX database (Buite-
laar, 1998), and automatic attempts to extract pat-
terns of systematic polysemy based on minimal de-
scription length principles (Tomuro, 2001).
Another family of approaches has been to
use either manually-annotated or automatically-
constructed mappings to coarser-grained sense in-
ventories; an attempt at providing coarse-grained
sense distinctions for the SENSEVAL-1 exercise in-
cluded a mapping between WordNet and the Hec-
tor lexicon (Palmer et al, 2005). Other attempts in
1006
this vein include mappings between WordNet and
PropBank (Palmer et al, 2004) and mappings to
Levin classes (Levin, 1993; Palmer et al, 2005).
(Navigli, 2006) presents an automatic approach for
mapping between sense inventories; here similari-
ties in gloss definition and structured relations be-
tween the two sense inventories are exploited in or-
der to map between WordNet senses and distinc-
tions made within the coarser-grained Oxford En-
glish Dictionary. Other work has attempted to ex-
ploit translational equivalences of WordNet senses
in other languages, for example using foreign lan-
guage WordNet interlingual indexes (Gonzalo et al,
1998; Chugur et al, 2002).
2.1 Gold standard sense clustering data
Our approach for learning how to merge senses
relies upon the availability of labeled judgments
of sense relatedness. In this work we focus on
two datasets of hand-labeled sense groupings for
WordNet: first, a dataset of sense groupings over
nouns, verbs, and adjectives provided as part of
the SENSEVAL-2 English lexical sample WSD task
(Kilgarriff, 2001), and second, a corpus-driven map-
ping of nouns and verbs in WordNet 2.1 to the
Omega Ontology (Philpot et al, 2005), produced as
part of the ONTONOTES project (Hovy et al, 2006).
A wide variety of semantic and syntactic criteria
were used to produce the SENSEVAL-2 groupings
(Palmer et al, 2004; Palmer et al, 2005); this data
covers all senses of 411 nouns, 519 verbs, and 257
adjectives, and has been used as gold standard sense
clustering data in previous work (Agirre and Lopez,
2003; McCarthy, 2006)2. The number of judgments
within this data (after mapping to WordNet 2.1) is
displayed in Table 1.
Due to a lack of interannotator agreement data for
this dataset, (McCarthy, 2006) performed an anno-
tation study using three labelers on a 20-noun sub-
set of the SENSEVAL-2 groupings; the three label-
ers were given the task of deciding whether the 351
potentially-related sense pairs were ?Related?, ?Un-
related?, or ?Don?t Know?.3 In this task the pair-
2In order to facilitate future work in this area, we
have made cleaned versions of these groupings available at
http://ai.stanford.edu/?rion/swn along with a ?diff? with the
original files.
3McCarthy?s gold standard data is available at
SENSEVAL-2
POS Total Pairs Merged Pairs Proportion
Nouns 16403 2593 0.1581
Verbs 30688 3373 0.1099
Adjectives 8368 2209 0.2640
ONTONOTES
POS Total Pairs Merged Pairs Proportion
Nouns 3552 347 0.0977
Verbs 4663 1225 0.2627
Table 1: Gold standard datasets for sense merging;
only sense pairs that share a word in common are
included; proportion refers to the fraction of synsets
sharing a word that have been merged
POS Overlap ON-True ON-False F-Score
S-T S-F S-T S-F
Nouns 2116 121 55 181 1759 0.5063
Verbs 3297 351 503 179 2264 0.5072
Table 2: Agreement data for gold standard datasets
wise interannotator F-scores were (0.4874, 0.5454,
0.7926), for an average F-score of 0.6084.
The ONTONOTES dataset4 covers a smaller set
of nouns and verbs, but it has been created with a
more rigorous corpus-based iterative annotation pro-
cess. For each of the nouns and verbs in question, a
50-sentence sample of instances is annotated using
a preliminary set of sense distinctions; if the word
sense interannotator agreement for the sample is less
than 90%, then the sense distinctions are revised and
the sample is re-annotated, and so forth, until an in-
terannotator agreement of at least 90% is reached.
We construct a combined gold standard set from
these SENSEVAL-2 and ONTONOTES groupings,
removing disagreements. The overlap and agree-
ment/disagreement data between the two groupings
is given in Table 2; here, for example, the column
with ON-True and S-F indicates the count of senses
that ONTONOTES judged as positive examples of
sense merging, but that SENSEVAL-2 data did not
merge. We also calculate the F-score achieved by
considering only one of the datasets as a gold stan-
dard, and computing precision and recall for the
other. Since the two datasets were created indepen-
dently, with different annotation guidelines, we can-
ftp://ftp.informatics.susx.ac.uk/pub/users/dianam/relateGS/.
4The OntoNotes groupings will be available through the
LDC at http://www.ldc.upenn.edu.
1007
not consider this as a valid estimate of interannota-
tor agreement; nonetheless the F-score for the two
datasets on the overlapping set of sense judgments
(50.6% for nouns and 50.7% for verbs) is roughly
in the same range as those observed in (McCarthy,
2006).
3 Learning to merge word senses
3.1 WordNet-based features
Here we describe the feature space we construct for
classifying whether or not a pair of synsets should be
merged; first, we employ a wide variety of linguistic
features based on information derived from Word-
Net. We use eight similarity measures implemented
within the WordNet::Similarity package5, described
in (Pedersen et al, 2004); these include three mea-
sures derived from the paths between the synsets
in WordNet: HSO (Hirst and St-Onge, 1998), LCH
(Leacock and Chodorow, 1998), and WUP (Wu and
Palmer, 1994); three measures based on information
content: RES (Resnik, 1995), LIN (Lin, 1998), and
JCN (Jiang and Conrath, 1997); the gloss-based Ex-
tended Lesk Measure LESK, (Banerjee and Peder-
sen, 2003), and finally the gloss vector similarity
measure VECTOR (Patwardan, 2003). We imple-
ment the TWIN feature (Peters et al, 1998), which
counts the number of shared synonyms between
the two synsets. Additionally we produce pair-
wise features indicating whether two senses share an
ANTONYM, PERTAINYM, or derivationally-related
forms (DERIV). We also create the verb-specific
features of whether two verb synsets are linked in
a VERBGROUP (indicating semantic similarity) or
share a VERBFRAME, indicating syntactic similar-
ity. Also, we encode a generalized notion of sib-
linghood in the MN features, recording the distance
of the synset pair?s nearest least common subsumer
(i.e., closest shared hypernym) from the two synsets,
and, separately, the maximum of those distances (in
the MAXMN feature.
Previous attempts at categorizing systematic pol-
ysemy patterns within WordNet has resulted in the
COUSIN feature6; we create binary features which
5We choose not to use the PATH measure due to its negligible
difference from the LCH measure.
6This data is included in the WordNet 1.6 distribution as the
?cousin.tops? file.
indicate whether a synset pair belong to hypernym
ancestries indicated by one or more of these COUSIN
features, and the specific cousin pair(s) involved.
Finally we create sense-specific features, including
SENSECOUNT, the total number of senses associ-
ated with the shared word between the two synsets
with the highest number of senses, and SENSENUM,
the specific pairing of senses for the shared word
with the highest number of senses (which might al-
low us to learn whether the most frequent sense of a
word has a higher chance of having similar deriva-
tive senses with lower frequency).
3.2 Features derived from corpora and other
lexical resources
In addition to WordNet-based features, we use
a number of features derived from corpora and
other lexical resources. We use the publicly avail-
able topic signature data7 described in (Agirre and
Lopez, 2004), yielding representative contexts for
all nominal synsets from WordNet 1.6. These topic
signatures were obtained by weighting the contexts
of monosemous relatives of each noun synset (i.e.,
single-sense synsets related by hypernym, hyponym,
or other relations); the text for these contexts were
extracted from snippets using the Google search en-
gine. We then create a sense similarity feature by
taking a thresholded cosine similarity between pairs
of topic signatures for these noun synsets.
Additionally, we use the WordNet domain dataset
described in (Magnini and Cavaglia, 2000; Ben-
tivogli et al, 2004). This dataset contains one or
more labels indicating of 164 hierarchically orga-
nized ?domains? or ?subject fields? for each noun,
verb, and adjective synset in WordNet; we derive a
set of binary features from this data, with a single
feature indicating whether or not two synsets share
a domain, and one indicator feature per pair of do-
mains indicating respective membership of the sense
pair within those domains.
Finally, we use as a feature the mappings pro-
duced in (Navigli, 2006) of WordNet senses to Ox-
ford English Dictionary senses. This OED dataset
was used as the coarse-grained sense inventory in the
Coarse-grained English all-words task of SemEval-
7The topic signature data is available for download at
http://ixa.si.ehu.es/Ixa/resources/sensecorpus.
1008
20078; we specify a single binary feature for each
pair of synsets from this data; this feature is true if
the words are clustered in the OED mapping, and
false otherwise.
3.3 Classifier, training, and feature selection
For each part of speech, we split the merged gold
standard data into a part-of-speech-specific train-
ing set (70%) and a held-out test set (30%). For
every synset pair we use the binary ?merged? or
?not-merged? labels to train a support vector ma-
chine classifier9 (Joachims, 2002) for each POS-
specific training set. We perform feature selection
and regularization parameter optimization using 10-
fold cross-validation.
4 Clustering Senses in WordNet
The previous section describes a classifier which
predicts whether two synsets should be merged; we
would like to use the pairwise judgments of this
classifier to cluster the senses within a sense hierar-
chy. In this section we present the challenge implicit
in applying sense merging to full taxonomies, and
present our model for clustering within a taxonomy.
4.1 Challenges of clustering a sense taxonomy
The task of clustering a sense taxonomy presents
certain challenges not present in the problem of clus-
tering the senses of a word; in order to create a
consistent clustering of a sense hierarchy an algo-
rithm must consider the transitive effects of merging
synsets. This problem is compounded in sense tax-
onomies like WordNet, where each synset may have
additional structured relations, e.g., hypernym (IS-
A) or holonym (is-part-of) links. In order to consis-
tently merge two noun senses with different hyper-
nym ancestries within WordNet, for example, an al-
gorithm must decide whether to have the new sense
inherit both hypernym ancestries, or whether to in-
herit only one, and if so it must decide which ances-
try is more relevant for the merged sense.
Without strict checking, human labelers will
likely find it difficult to label a sense inventory with
8http://lcl.di.uniroma1.it/coarse-grained-aw/index.html
9We use the SV Mperf package, freely available for non-
commercial use from http://svmlight.joachims.org; we use the
default settings in v2.00, except for the regularization parameter
(set in 10-fold cross-validation).
Clusering based on ??need??
Clustering based on ??require??
need#v#1
require#v#1 require as useful, just, or proper
need#v#2
require#v#4 have need of
need#v#3 have or feel a need for
require#v#1
need#v#1 require as useful, just, or proper
require#v#4
need#v#2 have need of
require#v#2 consider obligatory; request and expect
require#v#3 make someone do something
Figure 2: Inconsistent sense clusters for the verbs
require and need from SENSEVAL-2 judgments
transitively-consistent judgments. As an example,
consider the SENSEVAL-2 clusterings of the verbs
require and need, as shown in Figure 2. In WN 2.1
require has four verb senses, of which the first has
synonyms {necessitate, ask, postulate, need, take,
involve, call for, demand}, and gloss ?require as use-
ful, just, or proper?; and the fourth has synonyms
{want, need}, and gloss ?have need of.?
Within the word require, the SENSEVAL-2 dataset
clusters senses 1 and 4, leaving the rest unclustered.
In order to make a consistent clustering with respect
to the sense inventory, however, we must enforce
the transitive closure by merging the synset corre-
sponding to the first sense (necessitate, ask, need
etc.), with the senses of want and need in the fourth
sense. In particular, these two senses correspond
to WordNet 2.1 senses need#v#1 and need#v#2, re-
spectively, which are not clustered according to
the SENSEVAL-2 word-specific labeling for need ?
need#v#1 is listed as a singleton (i.e., unclustered)
sense, though need#v#2 is clustered with need#v#3,
?have or feel a need for.?
While one might hope that such disagreements
between sense clusterings are rare, we found
178 such transitive closure disagreements in the
SENSEVAL-2 data. The ONTONOTES data is much
cleaner in this respect, most likely due to the
stricter annotation standard (Hovy et al, 2006);
we found only one transitive closure disagreement
1009
in the OntoNotes data, specifically WordNet 2.1
synsets (head#n#2, lead#n#7: ?be in charge of?) and
(head#n#3, lead#v#4: ?travel in front of?) are clus-
tered under head but not under lead.
4.2 Sense clustering within a taxonomy
As a solution to the previously mentioned chal-
lenges, in order to produce taxonomies of different
sense granularities with consistent sense distinctions
we propose to apply agglomerative clustering over
all synsets in WordNet 2.1. While one might con-
sider recalculating synset similarity features after
each synset merge operation, depending on the fea-
ture set this could be prohibitively expensive; for our
purposes we use average-link agglomerative cluster-
ing, in effect approximating the the pairwise similar-
ity score between a given synset and a merged sense
as the average of the similarity scores between the
given synset and the clustered sense?s component
synsets. Further, for the purpose of sense cluster-
ing we assume a zero sense similarity score between
synsets with no intersecting words.
Without exploiting additional hypernym or
coordinate-term evidence, our algorithm does
not distinguish between judgments about which
hypernym ancestry or other structured relationships
to keep or remove upon merging two synsets. In
lieu of additional evidence, for our experiments
we choose to retain only the hypernym ancestry of
the sense with the highest frequency in SEMCOR,
breaking frequency ties by choosing the first-listed
sense in WordNet. We add every other relationship
(meronyms, entailments, etc.) to the new merged
sense (except in the rare case where adding a
relation would cause a cycle in acyclic relations like
hypernymy or holonymy, in which case we omit
it). Using this clustering method we have produced
several sense-clustered WordNets of varying sense
granularity, which we evaluate in Section 5.3.
5 Evaluation
We evaluate our classifier in a comparison with thir-
teen previously proposed similarity measures and
automatic methods for sense clustering. We conduct
a feature ablation study to explore the relevance of
the different features in our system. Finally, we eval-
uate the sense-clustered taxonomies we create on
the problem of providing improved coarse-grained
sense distinctions for WSD evaluation.
5.1 Evaluation of automatic sense merging
We evaluate our classifier on two held-out test
sets; first, a 30% sample of the sense judgments
from the merged gold standard dataset consisting
of both the SENSEVAL-2 and ONTONOTES sense
judgments; and, second, a test set consisting of only
the ONTONOTES subset of our first held-out test set.
For comparison we implement thirteen of the meth-
ods discussed in Section 2. First, we evaluate each
of the eight WordNet::Similarity measures individu-
ally. Next, we implement cosine similarity of topic
signatures (TOPSIG) built from monosemous rela-
tives (Agirre and Lopez, 2003), which provides a
real-valued similarity score for noun synset pairs.
Additionally, we implement the two methods
proposed in (Peters et al, 1998), namely using
metonymy clusters (MetClust) and generalization
clusters (GenClust) based on the COUSIN relation-
ship in WordNet. While (Peters et al, 1998) only
considers four cousin pairs, we re-implement their
method for general purpose sense clustering by us-
ing all 226 cousin pairs defined in WordNet 1.6,
mapped to WordNet 2.1 synsets. These methods
each provide a single clustering of noun synsets.
Next, we implement the set of semantic rules de-
scribed in (Mihalcea and Moldovan, 2001) (MIMO);
this algorithm for merging senses is based on 6 se-
mantic rules, in effect using a subset of the TWIN,
MAXMN, PERTAINYM, ANTONYM, and VERB-
GROUP features; in our implementation we set the
parameter for when to cluster based on number of
twins to K = 2; this results in a single clustering
for each of nouns, verbs, and adjectives. Finally, we
compare against the mapping from WordNet to the
Oxford English Dictionary constructed in (Navigli,
2006), equivalent to clustering based solely on the
OED feature.
Considering merging senses as a binary classifi-
cation task, Table 3 gives the F-score performance
of our classifier vs. the thirteen other classifiers and
an uninformed ?merge all synsets? baseline on our
held-out gold standard test set. This table shows that
our SVM classifier outperforms all implemented
methods on the basis of F-score on both datasets
1010
SENSEVAL-2 + ONTONOTES
ONTONOTES
Method Nouns Verbs Adj Nouns Verbs
SVM 0.4228 0.4319 0.4727 0.3698 0.4545
RES 0.3817 0.2703 ? 0.2807 0.3156
WUP 0.3763 0.2782 ? 0.3036 0.3451
LCH 0.3700 0.2440 ? 0.2857 0.3396
OED 0.3310 0.2878 0.3712 0.2183 0.3962
LESK 0.3174 0.2956 0.4323 0.2914 0.3774
HSO 0.3090 0.2784 0.4312 0.3025 0.3156
TOPSIG 0.3072 ? ? 0.2581 ?
VEC 0.2960 0.2315 0.4321 0.2454 0.3420
JCN 0.2818 0.2292 ? 0.2222 0.3156
LIN 0.2759 0.2464 ? 0.2056 0.3471
Baseline 0.2587 0.2072 0.4312 0.1488 0.3156
MIMO 0.0989 0.2142 0.0759 0.1833 0.2157
GenClust 0.0973 ? ? 0.0264 ?
MetClust 0.0876 ? ? 0.0377 ?
Table 3: F-score sense merging evaluation on hand-
labeled testsets
for all parts of speech. In Figure 3 we give a pre-
cision/recall plot for noun sense merge judgments
for the SENSEVAL-2 + ONTONOTES dataset. For
sake of simplicity we plot only the two best mea-
sures (RES and WUP) of the eight WordNet-based
similarity measures; we see that our classifier, RES,
and WUP each have higher precision all levels of
recall compared to the other tested measures.
Of the methods we compare against, only the
WordNet-based similarity measures, (Mihalcea and
Moldovan, 2001), and (Navigli, 2006) provide a
method for predicting verb similarities; our learned
measure widely outperforms these methods, achiev-
ing a 13.6% F-score improvement over the LESK
similarity measure. In Figure 4 we give a pre-
cision/recall plot for verb sense merge judgments,
plotting the performance of the three best WordNet-
based similarity measures; here we see that our clas-
sifier has significantly higher precision than all other
tested measures at nearly every level of recall.
Only the measures provided by LESK, HSO,
VEC, (Mihalcea and Moldovan, 2001), and (Nav-
igli, 2006) provide a method for predicting adjective
similarities; of these, only LESK and VEC outper-
form the uninformed baseline on adjectives, while
our learned measure achieves a 4.0% improvement
over the LESK measure on adjectives.
5.2 Feature analysis
Next we analyze our feature space. Table 4 gives the
ablation analysis for all features used in our system
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pr
ec
is
io
n
Precision/Recall for Merging Nouns
SVM Classifier
Resnik Measure
Wu & Palmer Measure
Topic Signatures
OED Mapping
Generalization Clusters
Metonym Clusters
Semantic Rules
Figure 3: Precision/Recall plot for noun sense merge
judgments
as evaluated on our held-out test set; here the quan-
tity listed in the table is the F-score loss obtained by
removing that single feature from our feature space,
and retraining and retesting our classifiers, keeping
everything else the same. Here negative scores cor-
respond to an improvement in classifier performance
with the removal of the feature.
For noun classification, the three features that
yield the highest gain in testset F-score are the
topic signature, OED, and derivational link features,
yielding a 4.0%, 3.6%, and 3.5% gain, respectively.
For verb classification, we find that three features
yield more than a 5% F-score gain; by far the largest
single-feature performance gain for verb classifica-
tion found in our ablation study was the DERIV fea-
ture, i.e., the count of shared derivational links be-
tween the two synsets; this single feature improves
our maximum F-score by 9.8% on the testset. This
is a particularly interesting discovery, as none of the
referenced automatic techniques for sense clustering
presently make use of this very useful feature. We
also achieve large gains with the LIN and LESK sim-
ilarity features, with F-score improvement of 7.4%
and 5.4% gain respectively.
For adjective classification again the DERIV fea-
ture proved very helpful, with a 3.5% gain on the
testset. Interestingly, only the DERIV feature and
the SENSECNT features helped across all parts of
speech; in many cases a feature which proved to be
1011
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Recall
Pr
ec
is
io
n
Precision/Recall for Merging Verbs
SVM Classifier
Lesk Measure
Hirst & St?Onge
Wu & Palmer
OED Mapping
Semantic Rules
Figure 4: Precision/Recall plot for verb sense merge
judgments
very helpful for one part of speech actually hurt per-
formance on another part of speech (e.g., LIN on
nouns and OED on adjectives).
5.3 Evaluation of sense-clustered Wordnets
Our goal in clustering a sense taxonomy is to pro-
duce fully sense-clustered WordNets, and to be able
to produce coarse-grained Wordnets at many differ-
ent levels of resolution. In order to evaluate the en-
tire sense-clustered taxonomy, we have employed an
evaluation method inspired by Word Sense Disam-
biguation (this is similar to an evaluation used in
Navigli, 2006, however we do not remove monose-
mous clusters). Given past system responses in the
SENSEVAL-3 English all-words task, we can eval-
uate past systems on the same corpus, but using
the coarse-grained sense hierarchy provided by our
sense-clustered taxonomy. We may then compare
the scores of each system on the coarse-grained task
against their scores given a random clustering at the
same resolution. Our expectation is that, if our sense
clustering is much better than a random sense clus-
tering (and, of course, that the WSD algorithms per-
form better than random guessing), we will see a
marked improvement in the performance of WSD
algorithms using our coarse-grained sense hierarchy.
We consider the outputs of the top 3 all-
words WSD systems that participated in Senseval-3:
Gambl (Decadt et al, 2004), SenseLearner (Mihal-
cea and Faruque, 2004), and KOC University (Yuret,
Nouns Verbs Adjectives
F-SCORE 0.4228 0.4319 0.4727
Feature F-Score Ablation Difference
TOPSIG 0.0403 ? ?
OED 0.0355 0.0126 -0.0124
DERIV 0.0351 0.0977 0.0352
RES 0.0287 0.0147 ?
TWIN 0.0285 0.0109 -0.0130
MN 0.0188 0.0358 ?
LESK 0.0183 0.0541 -0.0250
SENSENUM 0.0155 0.0146 -0.0147
SENSECNT 0.0121 0.0160 0.0168
DOMAIN 0.0119 0.0082 -0.0265
LCH 0.0099 0.0068 ?
WUP 0.0036 0.0168 ?
JCN 0.0025 0.0190 ?
ANTONYM 0.0000 0.0295 0.0000
MAXMN -0.0013 0.0179 ?
VEC -0.0024 0.0371 -0.0062
HSO -0.0073 0.0112 -0.0246
LIN -0.0086 0.0742 ?
COUSIN -0.0094 ? ?
VERBGRP ? 0.0327 ?
VERBFRM ? 0.0102 ?
PERTAINYM ? ? -0.0029
Table 4: Feature ablation study; F-score difference
obtained by removal of the single feature
2004). A guess by a system is given full credit if it
was either the correct answer or if it was in the same
cluster as the correct answer.
Clearly any amount of clustering will only in-
crease WSD performance. Therefore, to account for
this natural improvement and consider only the ef-
fect of our particular clustering, we also calculate
the expected score for a random clustering of the
same granularity, as follows: Let C represent the set
of clusters over the possible N synsets containing a
given word; we then calculate the expectation that an
incorrectly-chosen sense and the actual correct sense
would be clustered together in the random clustering
as
P
c?C |c|(|c|?1)
N(N?1) .
Our sense clustering algorithm provides little im-
provement over random clustering when too few or
too many clusters are chosen; however, with an ap-
propriate threshold for average-link clustering we
find a maximum of 3.55% F-score improvement in
WSD over random clustering (averaged over the de-
cisions of the top 3 WSD algorithms). Table 5 shows
the improvement of the three top WSD algorithms
given a sense clustering created by our algorithm vs.
a random clustering at the same granularity.
1012
0 0.5 1 1.5 2 2.5 3 3.5
x 104
0.65
0.7
0.75
0.8
Sense Merge Iterations
W
SD
 F
?S
co
re
Group Average Agglomerative Clustering
Random Clustering
Figure 5: WSD Improvement with coarse-grained
sense hierarchies
System F-score Avg-link Random Impr.
Gambl 0.6516 0.7702 0.7346 0.0356
SenseLearner 0.6458 0.7536 0.7195 0.0341
KOC Univ. 0.6414 0.7521 0.7153 0.0368
Table 5: Improvement in SENSEVAL-3 WSD perfor-
mance using our average-link agglomerative cluster-
ing vs. random clustering at the same granularity
6 Conclusion
We have presented a classifier for automatic sense
merging that significantly outperforms previously
proposed automatic methods. In addition to its novel
use of supervised learning and the integration of
many previously proposed features, it is interest-
ing that one of our new features, the DERIV count
of shared derivational links between two synsets,
proved an extraordinarily useful new cue for sense-
merging, particularly for verbs.
We also show how to integrate this sense-merging
algorithm into a model for sense clustering full sense
taxonomies like WordNet, incorporating taxonomic
constraints such as the transitive effects of merging
synsets. Using this model, we have produced several
WordNet taxonomies of various sense granularities;
we hope these new lexical resources will be useful
for NLP applications that require a coarser-grained
sense hierarchy than that already found in WordNet.
Acknowledgments
Thanks to Marie-Catherine de Marneffe, Mona
Diab, Christiane Fellbaum, Thad Hughes, and Ben-
jamin Packer for useful discussions. Rion Snow is
supported by an NSF Fellowship. This work was
supported in part by the Disruptive Technology Of-
fice (DTO)?s Advanced Question Answering for In-
telligence (AQUAINT) Phase III Program.
References
Eneko Agirre and Oier Lopez de Lacalle. 2003. Cluster-
ing WordNet word senses. In Proceedings of RANLP
2003.
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nomi-
nal senses. In Proceedings of LREC 2004.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
Gloss Overlaps as a Measure of Semantic Relatedness.
In Proceedings of IJCAI 2003.
Lisa Bentivogli, Pamela Forner, Bernardo Magnini, and
Emanuele Pianta. 2004. Revising the WordNet Do-
mains Hierarchy: Semantics, Coverage, and Balanc-
ing. In Proceedings of COLING Workshop on Multi-
lingual Linguistic Resources, 2004.
Timothy Chklovski and Rada Mihalcea. 2003. Exploit-
ing Agreement and Disagreement of Human Annota-
tors for Word Sense Disambiguation. In Proceedings
of RANLP 2003.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and Sense Proximity in the Senseval-2 Test
Suite. In Proceedings of ACL 2002 WSD Workshop.
Bart Decadt, Veronique Hoste, Walter Daelemans, and
Antal van den Bosch. 2004. Gamble, genetic algo-
rithm optimization of memory-based wsd. In Proceed-
ings of ACL/SIGLEX Senseval-3.
William Dolan. 1994. Word Sense Ambiguation: Clus-
tering Related Senses. In Proceedings of ACL 1994.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
Julio Gonzalo, Felia Verdejo, Irina Chugur, and Juan
Cigarran. 1998. Indexing with WordNet synsets can
improve text retrieval. In Proceedings of COLING-
ACL 1998 Workshop on WordNet in NLP Systems.
Patrick Hanks. 2000. Do word meanings exist? Com-
puters and the Humanities, 34(1-2): 171-177.
1013
Graeme Hirst and David St-Onge. 1998. Lexical chains
as representations of context for the detection and cor-
rection of malapropisms. In WordNet: An Electronic
Lexical Database.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. Proceedings of HLT-NAACL 2006.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on Re-
search in Computational Linguistics, 19-33.
Thorsten Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines. Dissertation, Kluwer,
2002.
Adam Kilgariff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31(1-2): 1-13.
Adam Kilgarriff. 2001. English lexical sample task de-
scription. In Proceedings of the SENSEVAL-2 work-
shop, 17-20.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for word
sense identification. In WordNet: An Electronic Lexi-
cal Database.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of SIG-
DOC 1986.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago, IL.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML 1998.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL 1998.
Bernardo Magnini and Gabriela Cavaglia. 2000. Inte-
grating Subject Field Codes into WordNet. In Pro-
ceedings of LREC 2000.
Diana McCarthy. 2006. Relating WordNet Senses for
Word Sense Disambiguation. In Proceedings of ACL
Workshop on Making Sense of Sense, 2006.
Rada Mihalcea and Dan I. Moldovan. 2001. Automatic
Generation of a Coarse Grained WordNet. In Proceed-
ings of NAACL Workshop on WordNet and Other Lex-
ical Resources.
Rada Mihalcea and Ehsanul Faruque. 2004. Sense-
learner: Minimally supervised word sense disam-
biguation for all words in open text. In Proceedings
of ACL/SIGLEX Senseval-3.
Dan I. Moldovan and Rada Mihalcea. 2000. Using
WordNet and lexical operators to improve Internet
searches. IEEE Internet Computing, 4(1):34-43.
Roberto Navigli. 2006. Meaningful Clustering of
Senses Helps Boost Word Sense Disambiguation Per-
formance. In Proceedings of COLING-ACL 2006.
Martha Palmer, Olga Babko-Malaya, Hoa Trang Dang.
2004. Different Sense Granularities for Different Ap-
plications. In Proceedings of Workshop on Scalable
Natural Language Understanding.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2005. Making fine-grained and coarse-grained
sense distinctions. Journal of Natural Language Engi-
neering.
Siddharth Patwardhan. 2003. Incorporating dictionary
and corpus information into a context vector measure
of semantic relatedness. Master?s thesis, Univ. of Min-
nesota, Duluth.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the
Relatedness of Concepts. In Proceedings of NAACL
2004.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional Clustering of English Words. In Pro-
ceedings of ACL 1993.
Wim Peters, Ivonne Peters, and Piek Vossen. 1998. Au-
tomatic Sense Clustering in EuroWordNet. In Pro-
ceedings of LREC 1998.
Andrew Philpot, Eduard Hovy, and Patrick Pantel. 2005.
The Omega Ontology. In Proceedings of the ON-
TOLEX Workshop at IJCNLP 2005.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the IJCAI 1995, 448-453.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: new evaluation
methods for word sense disambiguation. Natural Lan-
guage Engineering, 5(2):113-134.
Noriko Tomuro. 2001. Tree-cut and A Lexicon based
on Systematic Polysemy. In Proceedings of NAACL
2001.
Zhibiao Wu and Martha Palmer. 1994. Verb Semantics
and Lexical Selection. In Proceedings of ACL 1994.
Deniz Yuret. 2004. Some experiments with a naive
bayes wsd system. In Proceedings of ACL/SIGLEX
Senseval-3.
1014
