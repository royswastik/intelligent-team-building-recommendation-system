Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 12?21,
Athens, Greece, 30 March, 2009. c?2009 Association for Computational Linguistics
Computational Linguistics and Generative Linguistics:
The Triumph of Hope over Experience
Geoffrey K. Pullum
School of Philosophy, Psychology, and Language Sciences
University of Edinburgh
gpullum@ling.ed.ac.ul
Abstract
It is remarkable if any relationship at all
persists between computational linguists
(CL) and that part of general linguis-
tics comprising the mainstream of MIT
transformational-generative (TG) theoreti-
cal syntax. If the lines are still open, it rep-
resents something of a tribute to CL prac-
titioners? tolerance ? a triumph of hope
and goodwill over the experience of abuse
? because the TG community has shown
considerable hostility toward CL and ev-
erything it stands for over the past fifty
years. I offer some brief historical notes,
and hint at prospects for a better basis for
collaboration in the future.
1 Introduction
The theme of this workshop is the interaction be-
tween computational linguistics (CL) and general
linguistics. The organizers ask whether it has
it been virtuous, vicious, or vacuous. They use
only three of the rather extraordinary number of
v-initial adjectives. Is the relationship vital, valu-
able, venturesome, visionary, versatile, and vi-
brant? Or vague, variable, verbose, and sometimes
vexatious? Has it perhaps been merely vestigial
and vicarious, with hardly any general linguists
really participating? Or vain, venal, vaporous, vir-
ginal, volatile, and voguish, yet vulnerable, a re-
lationship at risk? Or would the best description
use adjectives like vengeful, venomous, vilifica-
tory, villainous, vindictive, violent, vitriolic, vo-
ciferous, and vulpine?
I will argue that at least with respect to that part
of general linguistics comprising the mainstream
of American theoretical syntax, it would be quite
remarkable if any relationship with computational
linguistics (CL) had thrived. It would represent (as
Samuel Johnson remarked cynically, and wrongly,
about second marriages) a triumph of hope over
experience. It seems to me that the relation-
ship that could have been was at least somewhat
blighted by the negative and defensive stance that
MIT-influenced transformational-generative (TG)
syntacticians have adopted on a diverse array of
topics highly relevant to CL.
There was never any need for such attitudes.
And at the conclusion of these brief remarks I will
suggest a basis for thinking that relations could be
much more satisfactory in the future. But I think
it is worth taking a sober look at the half-century
of history from 1959 to 2009, during which al-
most everything about the course of theoretical
syntax, at least in the USA, where I worked dur-
ing the latter half of the period, has been tacitly
guided by a single line of thinking. ?Generative
grammar? is commonly used to denote it, but that
will not do. First, ?generative grammar? is often
used to mean ?MIT-influenced transformational-
generative grammar?. For that I will use the ab-
breviation TG. And second, it is sometimes (in-
correctly) claimed that ?generative? means noth-
ing more or less than ?explicit? (see Chomsky
1966, 12: ?a generative grammar (that is, an ex-
plicit grammar that makes no appeal to the reader?s
?facule? de langage? but rather attempts to incorpo-
rate the mechanisms of this faculty)?).
We need more precise terminology in order to
home in on what I am talking about. As Seuren
(2004) has stressed, the relevant vision of what a
grammar is like, built into most linguistic theoriza-
tion today at a level so deep that most linguists are
incapable of seeing past it or out of it, is not just
that it is explicit, but that a grammar is and must be
a syntax-centered random generator. I will there-
fore refer to language specification by random
generation (LSRG).
The definitive technical paper defining gram-
mars in LSRG terms is Chomsky (1959). This
was a fine paper, which would have earned its
12
writer tenure in any department of linguistics,
logic, computer science, or mathematics that knew
what it was doing and could see the possibilities.
But it brought into linguistics two things that were
not going to go away for half a century. One
was the notion that any formally precise linguis-
tics had to be limited to LSRG. And the other was
the combative and insular personality of the pa-
per?s author, which had such a great influence on
the personality of his extraordinarily important de-
partment at MIT.
2 Obsession with random generation
The sense of ?generate? relevant to LSRG goes
back to the work of the great mathematical lo-
gician Emil Post (as acknowledged by Chomsky
1959, 137n). Post?s project was initially to for-
malize the notion of proof in logical systems ?
originally, the propositional logic that was infor-
mally used but not formally defined in Whitehead
and Russell?s Principia Mathematica. He repre-
sented well-formed formulae (?enunciations?, in-
cluding the ?primitive assertions?, i.e. axioms, and
the ?assertions? i.e. theorems) to be simply strings
over a finite set of symbols, and rules of inference
(?productions?) as instructions for deriving a new
string (the conclusion) from a set of strings already
in hand (the premises). He then studied the ques-
tion of what kinds of sets of strings could be gen-
erated if a set of initial strings were closed under
the operation of applying inference rules. Post?s
rather ungainly general presentation of the general
concept of rules of inference, or in his terms pro-
ductions, looks like this:
g11 Pi?1 g12 Pi?2 ? ? ? g1m1 Pi?m1 g1m1+1
g21 Pi??1 g22 Pi??2 ? ? ? g2m2 Pi??m2 g2m2+1
? ? ? ? ? ? ? ? ? ? ? ?
gk1 Pi?k1
gk2 Pi?k2
? ? ? gkmk Pi?kmk
gkmk+1
produce
g1 Pi1 g2 Pi2 ? ? ? gm Pim gm+1
In specific instances of productions the g metavari-
ables in this schema are replaced by actual strings
over what is now known as the terminal vocabu-
lary. The P metavariables function as cover sym-
bols for arbitrary stretches of material ? they are
string variables, some of which may be repeated
to copy material into the conclusion. A produc-
tion provides a license, given a set of strings that
match patterns of the form g0P1g1P2 ? ? ?Pkgk, to
produce a certain other string composed in some
way out of the various gi and Pi.
Post defined a class of canonical systems, each
consisting of a set of initial strings and a finite
set of productions. Sets generated by canonical
production systems he called canonical sets. Post
had realized early on that the canonical sets were
nothing more or less than the sets definable by re-
cursive functions or Turing machines; that is, they
were just the recursively enumerable (r. e.) sets.
He proceeded to prove that even if you restrict
the number and distribution of the gi and Pi ex-
tremely tightly, expressive power may not be re-
duced. Specifically, he proved that no reduction
in the definable sets is obtained even if you set the
number of P variables and the number of premises
at 1, and require that every production has the form
?g0P produces Pg1?. Such very restricted sys-
tems were called normal systems. Normal sys-
tems can still derive every canonical set, provided
you are allowed to use extra symbols that appear
in derivations but not in the ultimately generated
strings (these extra symbols are what would be-
come known to formal language theorists within
computer science as variables and to linguists as
nonterminals).
In a notation more familiar to linguists, the re-
sult amounts to showing that every r. e. subset of
?+ can be generated by some generative grammar
using a symbol vocabulary V = ? ? N in which
all rules have the form ?xW ? Wy? for speci-
fied strings x, y ? V ? and some fixed W ? V ?.
This was the first weak generative capacity result:
normal systems are equivalent in weak generative
capacity to full canonical systems.
In a later paper, settling a conjecture of Thue,
Post showed (1947) that you can derive every
canonical set if your productions all have the
form ?P1giP2 produces P1gjP2?. This amounts
to showing that every canonical subset of ?+ can
be generated by (what would later be called) a
generative grammar using a symbol vocabulary
V = ? ? N in which all rules have the form
?WxZ ? WyZ? for specified strings x, y ? V ?
and fixed W,Z ? V ?.
Hence the first demonstration that unrestricted
rewriting systems (Chomsky?s ?type-0? grammars)
can derive any r. e. set was not original with Chom-
sky (1959). It had been published twelve years
earlier by Post.
Post had in effect invented what could be called
13
top-down random generators. These randomly
generate r. e. sets of symbols by expanding an ini-
tial axiomatic string, which can be just a single
symbol. Their equivalence to Turing machines is
obvious (Kozen 1997, 256?257).
Between the time of Post?s doctoral work in
1920 and the 1943 paper in which he published
his result on canonical systems (already present
in compressed form in his thesis), Ajdukiewicz
(1935) had proposed a different style of genera-
tive grammar, also motivated by the development
of a better understanding of proof. Adjukiewicz?s
invention was categorial grammar, the first kind of
bottom-up random generators. It composes ex-
pressions of the generated language by combining
parts ? initially primitive categorized symbols,
and then previously composed subparts.
When Chomsky and Lasnik (1977) start talk-
ing about the ?computational system? of human
language (a mode of speaking that rapidly caught
on, and persists in current ?minimalist? work), the
?computation? of which they spoke was one that
takes place nowhere: no such computations are
ever done, except perhaps using pencil and paper
as a syntactic theorist tries to figure out how or
whether a certain string can be derived. This ?com-
putational system? attempts randomly and nonde-
terministically to find some way to apply rules in
order to build a particular structure, starting from
an arbitrary syntactic starting point.
In the case of pre-1990 work the starting point
was apparently a start symbol; in post-1990 ?min-
imalist? work it is a numeration: a randomly cho-
sen multiset of categorized items from the lexi-
con. The concept of a ?numeration? is a reflection
of how firmly embedded the random-generation
idea is. The numeration serves no real purpose.
It would be possible to formalize a grammar as a
set of combinatory principles for putting together
words in a string as encountered, from first to last,
so that it was in effect a parser. Categorial gram-
mars seem ideally suited to that role (Steedman,
2000), and minimalist grammars are really just a
variety of categorial grammar, stripped of some of
the formal coherence and links to logic and seman-
tics.
Chomsky has often written as if it were a neces-
sary truth that a grammar must be a random gener-
ator. For example: ?Clearly, a grammar must con-
tain . . . a ?syntactic component? that generates an
infinite number of strings representing grammat-
ical sentences . . . This is the classical model for
grammar? (Chomsky 1962, 539). This says that a
grammar must be a random generator. But this is
not true. A grammar could in principle be formu-
lated as, say a transducer mapping phonetic rep-
resentation inputs to corresponding sets of logical
forms. (Presumably this must be possible, given
what human beings do.)
It is particularly strange to see Chomsky ignor-
ing this possibility and yet asserting in Knowl-
edge of Language (Chomsky, 1986b) that a per-
son?s internalized grammar ?assigns a status to ev-
ery relevant physical event, say, every sound wave?
(p. 26). The claim is false, simply because ran-
dom generators are not transducers or functions:
they do not take inputs. A random generator only
?assigns a status? to a string by generating it with
a derivation that associates it with certain prop-
erties. And surely it is not a sensible hypothesis
about human linguistic competence to posit that in
the brain of every human being there is an inter-
nalized random generator generating every phys-
ically possible sequence of sounds, from a ship?s
foghorn to Mahler?s ninth symphony.
3 Downplaying expressive power
Perhaps the most centrally important reason for
linguists? concern with the possibility of excess
expressive power in grammar formalisms was
their sense that it should be guaranteed by the gen-
eral theory of grammar that linguistic behaviors
such as understanding a sentence should be repre-
sented as at least possible. This meant that gram-
mars had to be defined in a way that at least made
the general membership problem (?Given gram-
mar G, is string w grammatical??) decidable.
It was in Chomsky?s 1959 paper that progress
was first made toward restricting the expressive
power of production systems in ways that achieved
this, and the early work on topics like pushdown
automata and finite state machines shows that
those topics were of interest.
As is well known, Chomsky showed that if
productions of the general form X?Z ? X?Z
(where X,?,Z are strings in V ? and ? ? V +) are
limited by the condition that ? is no shorter than
?, we are no longer able to derive every r. e. set of
strings over the alphabet; we get only the context-
sensitive stringsets. If the further limitation that
? ? N is imposed, we get only the context-free
stringsets. And if on top of that the requirement
14
that ? ? (? ? ?N) is imposed, we get only the
regular sets.
Chomsky?s 1959 position was that the set of
all grammatical English word sequences was not
a regular stringset over the set of English words,
and that if any context-free grammar for English
could be constructed, it would not be an elegant or
revealing one. The search for intuitively adequate
grammars therefore had to range over the class of
grammars generating context-sensitive stringsets.
This is a large class of grammars, but at least it is
a proper subset of the class of grammars for which
the membership recognition problem is decidable.
Casting around outside that range was probably
not sensible, since natural languages surely had to
be decidable (it was taken to be quite obvious that
native speakers could rapidly recognize whether or
not a string of words was a sentence in their lan-
guage).
As I have detailed elsewhere in somewhat
tongue-in-cheek fashion (Pullum, 1989), Chom-
sky pulled back sharply from his initial interest
in mathematical study of linguistic formalisms as
it became clear that TG theories were being criti-
cized for their Turing-equivalence, and began dis-
missing precise studies of the generative capac-
ity of grammars as trivial and ridiculous. This, it
seems to me, was one more clear sign of distanc-
ing from the concerns of CL. It was mainly com-
putational linguists who showed interest in Gaz-
dar?s observation that a theory limited to gener-
ating context-free languages could guarantee not
just recognition but recognition in polynomial (in-
deed, better than cubic) time, and in the related
observation that none of the arguments for non-
context-free characteristics in human languages
seemed to be good ones (Pullum and Gazdar,
1982).
The MIT reaction to Gazdar?s suggestion was
to mount a major effort to find intractability
in Gazdar-style (GPSG) grammars ? to repre-
sent the recognition problem as NP-hard even
for context-free-equivalent theories of grammar
(Barton et al, 1987). This was something of
a confidence trick. First, the results depended
on switching attention from the fixed-grammar
arbitrary-string recognition problem (the analog
of what Vardi (1982) calls data complexity) to
the variable-grammar arbitrary-string recognition
problem (what Vardi calls combined complexity).
Second, it seemed to be vaguely assumed that only
GPSG had any charges to answer, and that the
GB theory of that time (Chomsky, 1981) would
not suffer from similar computational complexity
problems, but GB eventually turned out to be, in-
sofar as it was well defined, strongly equivalent to
Gazdar?s framework (Rogers, 1998).
For pre-GB varieties of TG, however, the prob-
lem had mainly been not that recognition was NP-
hard but that it was not computable at all: trans-
formational grammars from 1957 on kept proving
to be Turing-equivalent. That was what seems to
have driven the denigration of mathematical lin-
guistics, and the downplaying of the relevance of
decidability to such an extreme degree (see e.g.
Chomsky 1980: 120ff, where the very idea that
recognition is decidable is dismissed as an unim-
portant detail, and not necessarily even a true
claim).
4 Hostility to machine testing
With many versions of TG offering no guaran-
tee that there was any parser for the language
even in principle, it was not clear that machine
testing of grammatical theories by algorithmic
checking of claims made about grammaticality of
selected strings was a plausible idea. Perhaps
machine theorem-proving algorithms could have
been adapted to showing that a certain grammar
could indeed derive a certain string, but in prac-
tice early transformational grammar was vastly too
complex to permit the building of tools for gram-
mar testing, and later transformational grammar
far too vague.
I know of only one success story in grammar
evaluation by implementing random generation,
in fact. Ed Stabler (1992) coded up a Prolog
grammaticality-proving system based on the Bar-
riers theory of transformational grammar (Chom-
sky, 1986a), which (Pullum, 1989) had mocked for
sloppiness of statement. The Barriers system had
in particular abandoned the usual practice of defin-
ing trees in a way that had dominance as a reflexive
relation. Chomsky casually asserted that he would
take it to be irreflexive. Moreover, Stabler?s care-
ful and sympathetic reconstruction of Chomsky?s
intent defines the notion of ?exclusion? in such a
way that every node excludes itself (Chomsky?s
definition said that ?? excludes ? if [and only if]
no segment of ? dominates ??, and of course a
given ? never dominates itself). And sure enough,
the Stabler implementation revealed that this sys-
15
tem of definitions had a problem: unbounded de-
pendency constructions that Chomsky took to be
allowed were in fact blocked by his theoretical ma-
chinery.
Stabler concluded from his discovery ?that the
project of implementing GB theories transparently
is both manageable and worthwhile?. But his pa-
per has essentially never been referred to by any
mainstream syntacticians. It was not exactly what
they wanted to hear. Nor has anyone, to my knowl-
edge, utilized Stabler?s experience in doing syn-
tactic research using the Barriers framework.
There has in any case traditionally been con-
siderable resistance to machine testing of theo-
ries. I have heard a story told by MIT linguists
of how one early graduate student devised a com-
puter program to test the rule system of SPE, and
told Morris Halle about some of the bugs he had
thereby found, but Halle had already noticed all of
them. The moral of the story is clearly supposed
to be that machine testing is unneeded and of no
value.
Mark Johnson as an undergraduate did some
work showing that the Unix stream editor sed
could serve as an excellent tool for implementing
systems of ordered historical sound changes for
the assistance of comparative-historical linguists;
but this very sensible idea never led to widespread
testing of synchronic phonological ordered-rule
analyses.
In short, computational testbeds, however en-
thusiastically developed in some areas of science
(chemistry, astrophysics, ecology, molecular biol-
ogy), simply never (yet) took off in linguistic sci-
ence.
5 Loathing of corpora
There has traditionally been hostility even to
machine data-hunting or language study through
computer-searchable corpora. This is fading away
as a new generation of young linguists who do ev-
erything by searching the web do their data by web
search too; but it held back collaboration for a long
time. Early proposals for amassing computer cor-
pora were treated with contempt by TG grammar-
ians (?I?m a native speaker, I have intuitions; why
do I need your arbitrary collection of computer-
searchable text??).
And quite often evidence from attested sen-
tences is simply dismissed. To take a random ex-
ample, on page 48 of Postal (1971) the sentences
I am annoying to myself is prefixed with an as-
terisk to show that it is ungrammatical. Searching
for this exact strings using Google, as we can do
today, reveals that it gets 229 hits. I take this multi-
ple attestation to shift the burden overwhelmingly
against the linguist who claims that it is barred by
the grammar of the language. But anyone who has
experience (as I do) with trying to talk TG lin-
guists out of their beliefs by citing attested sen-
tences will know that it is between the difficult and
the impossible. From ?There are many errors in
published works? to ?It may be OK for him, but
it?s not for me?, there are many ways in which the
linguist can escape from the conclusion that a ma-
chine has proved superior in assessing the data.
Hostility to corpus work has probably to some
extent paved the way for the present situation,
where the machine translation teams at Google?s
research labs has no linguists, the work depending
entirely on heavily numerical tracking of statisti-
cal parallels seen in aligned bilingual texts.
And an unwholesome split is visible in the lin-
guistics community between those who broadly
want nothing to do with corpora and think personal
intuitions are fine as a basis for data gathering, and
the people that I have called corpus fetishists who
treat all facts as unclean and unholy unless they
come direct and unedited out of a corpus. At the
extremes, we get a divide between dreamers and
token-counters ? on the one hand, people who
think that speculations on how universal principles
might account for subtle shades of their own inner
reactions to particular sentences, and on the other,
people who think that counting the different pro-
nouns in ten million words of text and tabulating
the results is a contribution to science.
6 Aversion to the stochastic
Mention of statistics reminds us that stochastic
methods have revolutionized CL since the 1980s,
but have made few inroads into general linguis-
tics, and none into TG linguistics. This is despite
the excellent introduction to probabilistic genera-
tive grammars provided in Levelt?s excellent and
far-sighted introduction to mathematical linguis-
tics (Levelt, 1974), the first volume of which has
now been republished separately (Levelt, 2008).
The reason for the extraordinarily low profile of
probabilistic grammars within the ranks of TG lin-
guists has to do with the very successful attack on
the very possibility of their relevance in Syntac-
16
tic Structures (Chomsky, 1957). Insisting that any
statistical model for grammaticality would have
to treat Colorless green ideas sleep furiously and
Furiously sleep ideas green colorless in exactly
the same terms, as they are word strings with the
same (pre-1957) frequency of zero, Chomsky ar-
gued that probability of a string had no conceiv-
able relevance to its grammaticality.
Unfortunately he had made a mistake. He was
tacitly assuming that the probability of an event
type that has not yet occurred must be zero. Max-
imum likelihood estimation (MLE) does indeed
yield that result; but Chomsky was not obliged to
adopt MLE. The technique now known as smooth-
ing had been developed during the Second World
War by Alan Turing and I. J. Good, and although
it took a while to become known, Good had pub-
lished on it by 1953. Chomsky was simply not
acquainted with the statistical literature and not in-
terested in applying statistical methods to linguis-
tic material. Most linguists for the next forty years
followed him in his disdain for such work. But
when Pereira (2000) finally applied Good-Turing
estimation (smoothing) to the question of how dif-
ferent the probabilities of the two famous word se-
quences are from normal English text, he found
that the first (the syntactically well-formed one)
had a probability 200,000 times that of the second.
7 Contempt for applications
Theoretical linguists have tended to have an al-
most total lack of interest in anything that might
offer a practical application for their theories.
Most kinds of science tend eventually to support
some sort of engineering or practical techniques:
physics led to jet planes; geology gave us oil lo-
cation methods; biology brought forth gene splic-
ing; even logic and psychology have applications
in factories and other workplaces. But not main-
stream theoretical linguistics. Its theories do not
seem to yield applications of any sort.
Very early on, Chomsky found that he had to
distance himself from computers altogether: note
the remark in Chomsky (1966, 9) that ?Quite a
few commentators have assumed that recent work
in generative grammar is somehow an outgrowth
of an interest in the use of computers for one
or another purpose, or that it has some engineer-
ing motivation?, and note that he calls such views
both ?incomprehensible? and ?entirely false?. Be-
ing taken to have ambitions relating to natural lan-
guage processing was at that time clearly anath-
ema for the leader of the TG community.
What takes the place of application of theories
to practical domains today, since nothing has come
of any computational TG linguistics, is an attempt
to derive conclusions about human brain organiza-
tion and mental anatomy. Linguists claim to be bi-
ologists rather than psychologists (psycholinguis-
tics developed its own experimental paradigms
and began its own steady progress away from in-
teraction with TG linguistics). There is a journal
called Biolinguistics now, and much talk about in-
terfaces and evolution and perfection. Linguists
somehow live with the fact that the real biologists
and neurophysiologists are not getting involved.
It is probably this pretense at uncovering deep
principles of structure in a putative mental organ
(and pretense is what it is) that is responsible for
the dramatic falling off of interest in precise de-
scription of languages. Getting the details right ?
what was described as ?observational adequacy? in
Aspects (Chomsky, 1965) ? is taken to be a low-
prestige occupation when compared to one that is
alleged to offer glimpses of universal principles
that hold the key to language acquisition and the
innate cognitive abilities of the species.
Yet these universal principles are never actually
presented for examination in the way that genuine
results in science are. It is as if what is impor-
tant to the hunter after universal principles is the
hunt itself, the call of the horn and the thrill of the
chase, but not the grubby business of examining
and weighing the kill. The fact is that no really
robust and carefully formulated universals of lan-
guage have been discovered, described, promul-
gated, confirmed, and widely accepted as correct
in the fifty years that universals have been sought.
The notion that linguists have discovered innate
principles that solve the mystery of first language
acquisition (Scholz and Pullum, 2006) is partic-
ularly pernicious. The position generally advo-
cated by TG linguists is widely known as linguis-
tic nativism, and it says that some significant as-
pects of knowledge of language are not derived
from any experience but are innately known. But
when pressed on the question of what the evidence
shows about linguistic nativism, about whether
it can really be defended against its plausible ri-
vals, nativists tend to react by drawing back very
sharply into a trivial form of the thesis: of course
linguistic nativism must be true, they insist, be-
17
cause when you raise a baby and a kitten in the
same household under the same conditions it is
only the baby ends up with knowledge of lan-
guage. They therefore differ in some respect, in-
nately. ?Universal grammar? is simply one name
that linguists use for that which separates them:
whatever it is that human infants have but kittens
and monkeys and bricks don?t.
But of course, that makes the thesis trivial: it
is true in virtue of being merely a restatement of
the observation that led to linguistic nativism be-
ing put forward. We know that it is only human
neonates who accomplish the language acquisition
task, and that is why we are seeking an explanatory
theory of how humans accomplish the task. To say
that there must be something special about them is
certainly true, but that does not count as a scien-
tific discovery. We need specifics. Serious scien-
tists are like the private sector as characterized in
the immortal line uttered by Ray Stantz (played
by Dan Ackroyd) in Ghostsbusters: ?I?ve worked
in the private sector. They expect results!?
8 Hope for the future
It is absolutely not the case that general and the-
oretical linguistics should continue to act as if the
main object were to prevent any interaction with
CL. Let me point to a few hopeful developments.
Over the period from about 1989 to 2001, a
team of linguists worked on and completed a truly
comprehensive informal grammar of the English
language. It was published as Huddleston and Pul-
lum et al (2002), henceforth CGEL. It is an infor-
mal grammar, intended for serious academic users
but not limited to those with a linguistics back-
ground. And it comes close to being fully exhaus-
tive in its coverage of Standard English grammat-
ical constructions and morphology.
It should not be forgotten that the era of TG,
though it produced (in my view) no theories that
are really worth having, an enormous number of
interesting data discoveries about English were
made. CGEL profited greatly from those, as the
Further Reading section makes clear. But does not
attempt to develop theoretical conclusions or par-
ticipate in theoretical disputes. Wherever possible,
CGEL takes a largely pretheoretic or at least basi-
cally neutral stance.
Where theoretical commitments have to be
made explicit, they are, but they are then imple-
mented in consistent terms across the entire book.
Although more than a dozen linguists were in-
volved, it is not an anthology; Huddleston and Pul-
lum provide a unitary authorial voice for the book
and rewrote every part of the book at least once.
When disputes about analyses arose between the
authors who drafted different chapters, they were
settled one way or the other by recourse to evi-
dence, and not permitted to create departures from
consistency in the book as a whole.
CGEL was preceded by large-scale 3-volume
grammars for Italian (Renzi et al, 2001) and for
Spanish (Bosque and Demonte, 1999), and now a
grammar of French on a similar scale, the Grande
Grammaire du franc?ais is being written by a team
of linguists in Paris under the leadership of Anne
Abeille? (Paris 7), Annie Delaveau (Paris 10), and
Danie`le Godard (CNRS). In 2006 I visited Paris at
the request of that team to give a workshop on the
making of CGEL. Work continues, and the book is
now planned for publication by Editions Bayard in
2010. If anything the scope of this work is broader
than CGEL?s, since CGEL did not aim to cover
uncontroversially non-standard dialects of English
(for example, those that have negative concord),
whereas the Grande Grammaire explicitly aims
to cover regional and non-standard varieties of
French. Additionally, an effort to produce a com-
parable grammar of Mandarin Chinese is now be-
ing mounted in Hong Kong under the directorship
of Professor Chu-Ren Huang, the dean of the new
Faculty of Humanities at Hong Kong Polytechnic
University. I gave a workshop on CGEL there (in
March 2009) too.
The importance of these projects is simply that
they bear witness to the fact that, at least in some
areas, there are linguists ? and not just isolated
individuals but teams of experienced linguists ?
who are prepared to get involved in detailed lan-
guage description of the type that will be a prereq-
uisite to any future computational linguistics that
relies on details of syntax and semantics (rather
than probabilistic number-crunching on n-grams
and raw text, which has its own interest but does
not involve input from linguistics or even a rudi-
mentary knowledge of the language being pro-
cessed). Among them are both traditional general
linguists like Huddleston and people with serious
CL experience like Abeille? and Huang.
But there is more. I have made a prelimi-
nary analysis of the inventory of syntactic cate-
gories used in the tagging for labelling trees in the
18
Penn Treebank (Marcus et al, 1993), comparing
them to the categories used in CGEL. I would de-
scribe the fit as not perfect, but within negotiating
range. In some ways the fit is remarkable, given
the complete independence of the two projects
(the Treebank under Mitch Marcus in Philadelphia
was largely complete by 1992, when the CGEL
project under the direction of Rodney Huddleston
in Australia was only just getting up to speed, but
Huddleston and Marcus did not know about each
other?s work).
The biggest discrepancy in categorization is in
the problematic area of prepositions, adverbs, and
subordinating conjunctions, where the Treebank
has remained much too close to the confused older
tradition (where many prepositions are claimed to
have second lives as adverbs and quite a few are
also included on the list of subordinating conjunc-
tions, so that a word like since has one mean-
ing but three grammatical categories). The heart
of the problem is that the sage counsel of Jes-
persen (1924, 87?90) and the cogent arguments
of Emonds (1972) were not taken under consid-
eration by the devisers of the Treebank?s tagging
categories. But fixing that would involve nothing
more than undoing some unmotivated partitioning
of the preposition category.
Since there are few if any significant disagree-
ments about bracketing, and the category systems
could be brought into alignment, I believe it would
not be a major project to convert the entire Penn
Treebank into an alternate form where it was to-
tally compatible with CGEL in the syntactic anal-
yses it presupposed. There could be considerable
value in a complex of reference tools that included
a treebank of some 4.5 million words that is fully
compatible in its syntactic assumptions with an
1,860-page reference grammar of high reliability
and consistency.
And there is yet more. Here I will be brief,
and things will get slightly technical. The question
naturally arises of how one might formalize CGEL
to get it in a form where it was explicit enough for
use as a database that natural language process-
ing systems could in principle make use of. James
Rogers and I have recently considered that ques-
tion (Pullum and Rogers, 2008) within the con-
text of model-theoretic syntax, a line of work that
first began to receive sophisticated formulations
here at the EACL in various papers of the early
1990s (e.g. Blackburn et al (1993), Kracht (1993),
Blackburn & Gardent (1995); see Pullum (2007)
for a brief historical survey, and Pullum & Scholz
(2001) for a deeper treatment of relevant theoreti-
cal issues).
One thing that might appear to be a stumbling-
block to formalizing CGEL, and an obstacle to the
relationship with treebanks as well, is that strictly
speaking CGEL?s assumed syntactic representa-
tions are not (or not all) trees. They are graphs that
depart from being ordinary constituent-structure
trees in at least two respects.
First, they are annotated not just with cate-
gories labelling the nodes, but also with syntactic
functions (grammatical relations like Subject-of,
Determiner-of, Head-of, Complement-of, etc.)
that are perhaps best conceptualized as labelling
the edges of the graph (the lines between the nodes
in the diagrams).
Second, and perhaps more seriously, there is oc-
casional downward convergence of branches: it
is permitted for a given constituent, under certain
conditions, to bear two different grammatical rela-
tions to two different superordinate nodes. (A de-
terminative like some, for example, may be both
the Determiner of an NP and the Head of the
Nominal that is the phrasal head of that NP.) Often
(as in HPSG work) the introduction of re-entrancy
had dramatic consequences for key properties like
decidability of satisfiability for descriptions, or
even for model-checking. (I take it that the for-
mal issues around HPSG are very well known to
the EACL community. In this short paper I do not
try to deal with HPSG at all. There is plenty to be
said, but also plenty of excellent HPSG specialists
in Europe who are more competent than I am to
treat the topic.)
Pullum & Rogers (2008) shows, however, that
given certain very weak conditions, which seem
almost certainly to be satisfied by the kinds of
grammatical analysis posited in grammars of the
CGEL sort, there is a way of constructing a com-
patible directed ordered spanning tree for any
CGEL-style syntactic structure in such a way
that no information is lost and reachability via
edge chains is preserved. Moreover, the map-
ping between CGEL structures and spanning trees
is definable in weak monadic second-order logic
(wMSO).
Put this together with the results of Rogers
(1998) on definability of trees in wMSO, and there
is a clear prospect of the CGEL analysis of En-
19
glish syntax being reconstructible in terms of the
wMSO theory of trees. And what that means for
parsing is clear from results of nearly 40 years
ago (Doner, 1970): there is a strong equivalence
via tree automata to context-free grammars, which
means that all the technology of context-free pars-
ing can potentially be brought to bear on process-
ing them.
This does not mean it would be a crisis if some
language of interest is found to be non-context-
free, incidentally. By the results of Rogers (2003),
wMSO theories interpreted on tree-like structures
of higher dimensionality than 2 could be em-
ployed. For example, where the structures are 3-
dimensional (so that individual nodes are allowed
to bear the parent-of relation to all of the nodes
in entire 2-dimensional trees), the string yield of
the set of all structures satisfying a given wMSO
sentence is always a tree-adjoining language, and
for every tree-adjoining language there is such a
characterizing wMSO sentence.
Notice, by the way, that the theoretical tools
of use here are coming out of currently very ac-
tive subdisciplines of computational logic and au-
tomata theory, such as finite model theory, descrip-
tive complexity theory, and database theory. The
very tools that linguistics needs in order to for-
malize syntactic theories in a revealing way are
the ones that theoretical computer science is in-
tensively working on because their investigation is
intrinsically interesting.
To sum up, what this is all telling us is that
there is no reason for anyone to continue be-
ing guided by the TG bias toward isolating the-
oretical linguistics from CL. There is not neces-
sarily a major gulf between (i) cutting-edge cur-
rent theoretical developments like model-theoretic
syntax, (ii) large-scale descriptive grammars like
CGEL, and (iii) feasible computational natural-
language engineering. Given the excellent per-
sonal relations between general linguists and com-
putational linguists in some European locations
(Edinburgh being an excellent example), it seems
to me that developments in interdisciplinary rela-
tions that would integrate the two disciplines quite
thoroughly could probably happen quite fast. Per-
haps it is happening already.
Acknowledgments
I am very grateful to Barbara Scholz for her de-
tailed criticisms of a draft of this paper. I have
taken account of many of her helpful suggestions,
but since she still does not agree with what I say
here, none of the failings or errors above should be
blamed on her.
References
Kazimierz Ajdukiewicz. 1935. Die syntaktische kon-
nexita?t. Studia Philosophica, 1:1?27. Reprinted in
Storrs McCall, ed., Polish Logic 1920?1939, 207?
231. Oxford: Oxford University Press.
G. Edward Barton, Robert C. Berwick, and Eric Sven
Ristad. 1987. Computational Complexity and Natu-
ral Language. MIT Press, Cambridge, MA.
Patrick Blackburn and Claire Gardent. 1995. A spec-
ification language for lexical functional grammars.
In Seventh Conference of the European Chapter of
the Association for Computational Linguistics: Pro-
ceedings of the Conference, pages 39?44, Morris-
town, NJ. European Association for Computational
Linguistics.
Patrick Blackburn, Claire Gardent, and Wilfried
Meyer-Viol. 1993. Talking about trees. In Sixth
Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Proceedings of
the Conference, pages 21?29, Morristown, NJ. Eu-
ropean Association for Computational Linguistics.
Ignacio Bosque and Violeta Demonte, editors. 1999.
Grama?tica Descriptiva de La Lengua Espan?ola.
Real Academia Espan?ola / Espasa Calpe, Madrid. 3
volumes.
Noam Chomsky and Howard Lasnik. 1977. Filters and
control. Linguistic Inquiry, 8:425?504.
Noam Chomsky. 1957. Syntactic Structures. Mouton,
The Hague.
Noam Chomsky. 1959. On certain formal properties
of grammars. Information and Control, 2:137?167.
Reprinted in Readings in Mathematical Psychology,
Volume II, ed. by R. Duncan Luce, Robert R. Bush,
and Eugene Galanter, 125?155, New York: John Wi-
ley & Sons, 1965 (citation to the original on p. 125
of this reprinting is incorrect).
Noam Chomsky. 1962. Explanatory models in linguis-
tics. In Ernest Nagel, Patrick Suppes, and Alfred
Tarski, editors, Logic, Methodology and Philosophy
of Science: Proceedings of the 1960 International
Congress, pages 528?550, Stanford, CA. Stanford
University Press.
Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax. MIT Press, Cambridge, MA.
Noam Chomsky. 1966. Topics in the Theory of Gener-
ative Grammar. Mouton, The Hague.
Noam Chomsky. 1980. Rules and Representations.
Basil Blackwell, Oxford.
20
Noam Chomsky. 1981. Lectures on Government and
Binding. Foris, Dordrecht.
Noam Chomsky. 1986a. Barriers. MIT Press, Cam-
bridge, MA.
Noam Chomsky. 1986b. Knowledge of Language: Its
Origins, Nature, and Use. Praeger, New York.
John Doner. 1970. Tree acceptors and some of their
applications. Journal of Computer and System Sci-
ences, 4:406?451.
Joseph E. Emonds. 1972. Evidence that indirect ob-
ject movement is a structure-preserving rule. Foun-
dations of Language, 8:546?561.
Rodney Huddleston, Geoffrey K. Pullum, et al 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, Cambridge.
Otto Jespersen. 1924. The Philosophy of Grammar.
Holt, New York.
Dexter Kozen. 1997. Automata and Computability.
Springer, Berlin.
Marcus Kracht. 1993. Mathematical aspects of com-
mand relations. In Sixth Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Proceedings of the Conference, pages
240?249, Morristown, NJ. Association for Compu-
tational Linguistics.
W. J. M. Levelt. 1974. Formal Grammars in Lin-
guistics and Psycholinguistics. Volume I: An Intro-
duction to the Theory of Formal Languages and
Automata; Volume II: Applications in Linguistic
Theory; Volume III: Psycholinguistic Applications.
Mouton, The Hague.
W. J. M. Levelt. 2008. An Introduction to the Theory of
Formal Languages and Automata. John Benjamins,
Amsterdam.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Emil Post. 1947. Recursive unsolvability of a problem
of thue. Journal of Symbolic Logic, 12:1?11.
Paul M. Postal. 1971. Crossover Phenomena. Holt,
Rinehart and Winston, New York.
Geoffrey K. Pullum and Gerald Gazdar. 1982. Natural
languages and context-free languages. Linguistics
and Philosophy, 4:471?504.
Geoffrey K. Pullum and James Rogers. 2008. Ex-
pressive power of the syntactic theory implicit in the
cambridge grammar of the english language. Pa-
per presented at the annual meeting of the Linguis-
tics Assocition of Great Britain, University of Es-
sex, September 2008. Online at http://ling.ed.ac.
uk/?gpullum/EssexLAGB.pdf.
Geoffrey K. Pullum and Barbara C. Scholz. 2001.
On the distinction between model-theoretic and
generative-enumerative syntactic frameworks. In
Philippe de Groote, Glyn Morrill, and Christian
Retore?, editors, Logical Aspects of Computational
Linguistics: 4th International Conference, num-
ber 2099 in Lecture Notes in Artificial Intelligence,
pages 17?43, Berlin and New York. Springer.
Geoffrey K. Pullum. 1989. Formal linguistics meets
the Boojum. Natural Language & Linguistic The-
ory, 7:137?143.
Geoffrey K. Pullum. 2007. The evolution of model-
theoretic frameworks in linguistics. In James Rogers
and Stephan Kepser, editors, Model-Theoretic Syn-
tax at 10: ESSLLI 2007Workshop, pages 1?10, Trin-
ity College Dublin, Ireland. Association for Logic,
Language and Information.
Lorenzo Renzi, Giampaolo Salvi, and Anna Cardi-
naletti. 2001. Grande grammatica italiana di con-
sultazione. Il Mulino, Bologna. 3 volumes.
James Rogers. 1998. A Descriptive Approach to
Language-Theoretic Complexity. CSLI Publica-
tions, Stanford, CA.
James Rogers. 2003. wMSO theories as grammar for-
malisms. Theoretical Computer Science, 293:291?
320.
Barbara C. Scholz and Geoffrey K. Pullum. 2006. Ir-
rational nativist exuberance. In Robert Stainton, ed-
itor, Contemporary Debates in Cognitive Science,
pages 59?80. Basil Blackwell, Oxford.
Pieter A. M. Seuren. 2004. Chomsky?s Minimalism.
Oxford University Press, Oxford.
Edward P. Stabler, Jr. 1992. Implementing government
binding theories. In Robert Levine, editor, Formal
Grammar: Theory and Implementation, pages 243?
289. Oxford University Press, New York.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA.
Moshe Y. Vardi. 1982. The complexity of relational
query languages. In Proceedings of the 14th ACM
Symposium on Theory of Computing, pages 137?
146, New York. Association for Computing Machin-
ery.
21
