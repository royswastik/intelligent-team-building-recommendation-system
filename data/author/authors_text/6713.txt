A Novel Use of Statistical Parsing to Extract Information from 
Text 
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph Weischedel 
BBN Technologies 
70 Fawcett Street, Cambridge, MA 02138 
szmiller@bbn.com 
Abstract 
Since 1995, a few statistical parsing 
algorithms have demonstrated a 
breakthrough in parsing accuracy, as 
measured against the UPenn TREEBANK 
as a gold standard. In this paper we report 
adapting a lexicalized, probabilistic 
context-free parser to information 
extraction and evaluate this new technique 
on MUC-7 template elements and template 
relations. 
1 Introduction 
Since 1995, a few statistical parsing 
algorithms (Magerman, 1995; Collins, 1996 
and 1997; Charniak, 1997; Rathnaparki, 1997) 
demonstrated a breakthrough in parsing 
accuracy, as measured against he University 
of Pennsylvania TREEBANK as a gold 
standard. Yet, relatively few have embedded 
one of these algorithms in a task. Chiba, 
(1999) was able to use such a parsing 
algorithm to reduce perplexity with the long 
term goal of improved speech recognition. 
In this paper, we report adapting a lexicalized, 
probabilistic context-free parser with head 
rules (LPCFG-HR) to information extraction. 
The technique was benchmarked in the 
Seventh Message Understanding Conference 
(MUC-7) in 1998. 
Several technical challenges confronted us and 
were solved: 
? How could the limited semantic 
interpretation required in information 
extraction be integrated into the statistical 
learning algorithm? We were able to integrate 
both syntactic and semantic information into 
the parsing process, thus avoiding potential 
errors of syntax first followed by semantics. 
? Would TREEBANKing of the variety of 
news sources in MUC-7 be required? Or 
could the University of Pennsylvania's 
TREEBANK on Wall Street Journal 
adequately train the algorithm for New York 
Times newswire, which includes dozens of 
newspapers? Manually creating source- 
specific training data for syntax was not 
required. Instead, our parsing algorithm, 
trained on the UPenn TREEBANK, was run 
on the New York Times source to create 
unsupervised syntactic training which was 
constrained to be consistent with semantic 
annotation. 
* Would semantic annotation require 
computational linguists? We were able to 
specify relatively simple guidelines that 
students with no training in computational 
linguistics could annotate. 
2 Information Extraction Tasks 
We evaluated the new approach to information 
extraction on two of the tasks of the Seventh 
Message Understanding Conference (MUC-7) 
and reported in (Marsh, 1998). The Template 
Element (TE) task identifies organizations, 
persons, locations, and some artifacts (rocket 
and airplane-related artifacts). For each 
organization i an article, one must identify all 
of its names as used in the article, its type 
(corporation, government, or other), and any 
significant description of it. For each person, 
one must find all of the person's names within 
the document, his/her type (civilian or 
military), and any significant descriptions 
(e.g., titles). For each location, one must also 
give its type (city, province, county, body of 
water, etc.). For the following example, the 
226 
template element in  Figure I was to be 
generated: "...according to the report by 
Edwin Dorn, under secretary of defense for 
personnel and readiness . . . .  Dorn's conclusion 
that Washington..." 
<ENTITY-9601020516-13> := 
ENT_NAME: "Edwin Dorn" 
"Dorn" 
ENT_TYPE: PERSON 
ENT_DESCRIPTOR: "under secretary of 
defense for personnel and readiness" 
ENT_CATEGORY: PER_CIV 
Figure 1: An example of the information to be 
extracted for TE. 
The Template Relations (TR) task involves 
identifying instances of three relations in the 
text: 
? the products made by each company 
? the employees ofeach organization, 
? the (headquarters) location of each 
organization. 
TR builds on TE in that TR reports binary 
relations between elements of TE. For the 
following example, the template relation in 
Figure 2 was to be generated: "Donald M. 
Goldstein, a historian at the University of 
Pittsburgh who helped write..." 
<EMPLOYEE_OF-9601020516-5> := 
PERSON: <ENTITY-9601020516-18> 
ORGANIZATION: <ENTITY- 
9601020516-9> 
<ENTITY-9601020516-9> := 
ENT_NAME: "University of Pittsburgh" 
ENT_TYPE: ORGANIZATION 
ENT_CATEGORY: ORG_CO 
<ENTITY-9601020516-18> := 
ENT_NAME: "Donald M. Goldstein" 
ENT_TYPE: PERSON 
ENT_DESCRIPTOR: "a historian at the 
University of Pittsburgh" 
Figure 2: An example of information to be 
extracted for TR 
3 Integrated Sentential Processing 
Almost all approaches to information 
extraction - even at the sentence level - are 
based on the divide-and-conquer st ategy of 
reducing acomplex problem to a set of simpler 
ones. Currently, the prevailing architecture for 
dividing sentential processing is a four-stage 
pipeline consisting of: 
1. part-of-speech tagging 
2. name finding 
3. syntactic analysis, often limited to noun 
and verb group chunking 
4. semantic interpretation, usually based on 
pattern matching 
Since we were interested in exploiting recent 
advances in parsing, replacing the syntactic 
analysis tage of the standard pipeline with a 
modem statistical parser was an obvious 
possibility. However, pipelined architectures 
suffer from a serious disadvantage: rrors 
accumulate as they propagate through the 
pipeline. For example, an error made during 
part-of-speech-tagging may cause a future 
error in syntactic analysis, which may in turn 
cause a semantic interpretation failure. There 
is no opportunity for a later stage, such as 
parsing, to influence or correct an earlier stage 
such as part-of-speech tagging. 
An integrated model can limit the propagation 
of errors by making all decisions jointly. For 
this reason, we focused on designing an 
integrated model in which tagging, name- 
finding, parsing, and semantic interpretation 
decisions all have the opportunity to mutually 
influence ach other. 
A second consideration influenced our 
decision toward an integrated model. We were 
already using a generative statistical model for 
part-of-speech tagging (Weischedel et al 
1993), and more recently, had begun using a 
generative statistical model for name finding 
(Bikel et al 1997). Finally, our newly 
constructed parser, like that of (Collins 1997), 
was based on a generative statistical model. 
Thus, each component of what would be the 
first three stages of our pipeline was based on 
227 
the same general class of statistical model. 
Although each model differed in its detailed 
probability structure, we believed that the 
essential elements of all three models could be 
generalized in a single probability model. 
If the single generalized model could then be 
extended to semantic anal);sis, all necessary 
sentence level processing would be contained 
in that model. Because generative statistical 
models had already proven successful for each 
of the first three stages, we were optimistic 
that some of their properties - especially their 
ability to learn from large amounts of data, and 
their robustness when presented with 
unexpected inputs - would also benefit 
semantic analysis. 
4 Representing Syntax and Semantics 
Jointly 
Our integrated model represents yntax and 
semantics jointly using augmented parse trees. 
In these trees, the standard TREEBANK 
structures are augmented to convey semantic 
information, that is, entities and relations. An 
example of an augmented parse tree is shown 
in Figure 3. The five key facts in this example 
are: 
? "Nance" is the name of a person. 
? "A paid consultant to ABC News" 
describes a person. 
t "ABC News" is the name of an 
organization. 
? The person described as "a paid consultant 
to ABC News" is employed by ABC News. 
? The person named "Nance" and the person 
described as "a paid consultant to ABC News" 
are the same person. 
Here, each "reportable" name or description is 
identified by a "-r" suffix attached to its 
semantic label. For example, "per-r" identifies 
"Nance" as a named person, and "per-desc-r" 
identifies "a paid consultant to ABC News" as 
a person description. Other labels indicate 
relations among entities. For example, the co- 
reference relation between "Nance" and "a 
paid consultant to ABC News" is indicated by 
"per-desc-of." In this case, because the 
argument does not connect directly to the 
relation, the intervening nodes are labeled with 
semantics "-ptr" to indicate the connection. 
Further details are discussed in the section 
Tree Augmentation. 
5 Creating the Training Data 
To train our integrated model, we required a 
large corpus of augmented parse trees. Since it 
was known that the MUC-7 evaluation data 
would be drawn from a variety of newswire 
sources, and that the articles would focus on 
rocket launches, it was important that our 
training corpus be drawn from similar sources 
and that it cover similar events. Thus, we did 
not consider simply adding semantic labels to 
the existing Penn TREEBANK, which is 
drawn from a single source - the Wall Street 
Journal - and is impoverished in articles about 
rocket launches. 
Instead, we applied an information retrieval 
system to select a large number of articles 
from the desired sources, yielding a corpus 
rich in the desired types of events. The 
retrieved articles would then be annotated with 
augmented tree structures to serve as a training 
corpus. 
Initially, we tried to annotate the training 
corpus by hand marking, for each sentence, the 
entire augmented tree. It soon became 
painfully obvious that this task could not be 
performed in the available time. Our 
annotation staff found syntactic analysis 
particularly complex and slow going. By 
necessity, we adopted the strategy of hand 
marking only the semantics. 
Figure 4 shows an example of the semantic 
annotation, which was the only type of manual 
annotation we performed. 
To produce a corpus of augmented parse trees, 
we used the following multi-step training 
procedure which exploited the Penn 
TREEBANK 
228 
S 
per/np vp 
per-r/np 
I 
per/nnp 
I 
Nance , who is also a paid consultant to 
/ ~~-~1~p \ /// \ 
/ / I / o rg~ \ 
, wp vbz rb det vbn per-desc/nn to org'/nnporg/nnp , vbd 
I I I I I I I I I I I I 
ABe News , said ... 
Figure 3: An example of an augmented parse tree. 
1. The model (see Section 7) was first trained 
on purely syntactic parse trees from the 
TREEBANK, producing a model capable 
of broad-coverage syntactic parsing. 
parses that were consistent with the 
semantic annotation. A parse was 
considered consistent if no syntactic 
constituents crossed an annotated entity or 
description boundary. 
2. Next, for each sentence in the semantically 
annotated corpus: 
a. The model was applied to parse the 
sentence, constrained to produce only 
b. The resulting parse tree was then 
augmented to reflect semantic structure in 
addition to syntactic structure. 
/ 
F.?rso?l 
Nance 
coreference ~ employee  .ation 
person-descriptor -. 
Iorganization 1
, who is also a paid consultant to ABC News said ... 
Figure 4: An example of semantic annotation. 
229 
Applying this procedure yielded a new version 
of the semantically annotated corpus, now 
annotated with complete augmented trees like 
that in Figure 3. 
6 Tree Augmentation 
In this section, we describe the algorithm that 
was used to automatically produce augmented 
trees, starting with a) human-generated 
semantic annotations and b) machine- 
generated syntactic parse trees. For each 
sentence, combining these two sources 
involved five steps. These steps are given 
below: 
Tree Augmentation Algorithm 
. Nodes are inserted into the parse tree to 
distinguish names and descriptors that are 
not bracketed in the parse. For example, 
the parser produces a single noun phrase 
with no internal structure for "Lt. Cmdr. 
David Edwin Lewis". Additional nodes 
must be inserted to distinguish the 
description, "Lt. Cmdr.," and the name, 
"David Edwin Lewis." 
. Semantic labels are attached to all nodes 
that correspond to names or descriptors. 
These labels reflect he entity type, such as 
person, organization, or location, as well 
as whether the node is a proper name or a 
descriptor. 
. For relations between entities, where one 
entity is not a syntactic modifier of the 
other, the lowermost parse node that spans 
both entities is identified. A semantic tag 
is then added to that node denoting the 
relationship. For example, in the sentence 
"Mary Fackler Schiavo is the inspector 
general of the U.S. Department of 
Transportation," a co-reference semantic 
label is added to the S node spanning the 
name, "Mary Fackler Schiavo," and the 
descriptor, "the inspector general of the 
U.S. Department of Transportation." 
. Nodes are inserted into the parse tree to 
distinguish the arguments to each relation. 
In cases where there is a relation between 
two entities, and one of the entities is a 
syntactic modifier of the other, the inserted 
node serves to indicate the relation as well 
as the argument. For example, in the 
phrase "Lt. Cmdr. David Edwin Lewis," a 
node is inserted to indicate that "Lt. 
Cmdr." is a descriptor for "David Edwin 
Lewis." 
. Whenever a relation involves an entity that 
is not a direct descendant of that relation 
in the parse tree, semantic pointer labels 
are attached to all of the intermediate 
nodes. These labels serve to form a 
continuous chain between the relation and 
its argument. 
7 Model Structure 
In our statistical model, trees are generated 
according to a process imilar to that described 
in (Collins 1996, 1997). The detailed 
probability structure differs, however, in that it 
was designed to jointly perform part-of-speech 
tagging, name finding, syntactic parsing, and 
relation finding in a single process. 
For each constituent, the head is generated 
first, followed by the modifiers, which are 
generated from the head outward. Head 
words, along with their part-of-speech tags and 
features, are generated for each modifier as 
soon as the modifier is created. Word features 
are introduced primarily to help with unknown 
words, as in (Weischedel et al 1993). 
We illustrate the generation process by 
walking through a few of the steps of the parse 
shown in Figure 3. At each step in the 
process, a choice is made from a statistical 
distribution, with the probability of each 
possible selection dependent on particular 
features of previously generated elements. We 
pick up the derivation just after the topmost S 
and its head word, said, have been produced. 
The next steps are to generate in order: 
1. A head constituent for the S, in this case a 
VP. 
2. Pre-modifier constituents for the S. In this 
case, there is only one: a PER/NP. 
3. A head part-of-speech tag for the PER/NP, 
in this case PER/NNP. 
230 
4. A head word for the PER/NP, in this case 
nance. 
5. Word features for the head word of the 
PER/NP, in this case capitalized. 
6. A head constituent for the PER/NP, in this 
case a PER-R/NP. 
7. Pre-modifier constituents for the PER/NP. 
In this case, there are none. 
. Post-modifier constituents for the 
PER/NP. First a comma, then an SBAR 
structure, and then a second comma are 
each generated in turn. 
This generation process is continued until the 
entire tree has been produced. 
We now briefly summarize the probability 
structure of the model. The categories for 
head constituents, ch, are predicted based 
solely on the category of the parent node, cp: 
e(c h Icp), e.g. P(vpls )
Modifier constituent categories, Cm, are 
predicted based on their parent node, cp, the 
head constituent of their parent node, Chp, the 
previously generated modifier, Cm-1, and the 
head word of their parent, wp. Separate 
probabilities are maintained for left (pre) and 
right (post) modifiers: 
PL (Cm I Cp,Chp,Cm_l,Wp), e.g. 
PL ( per I np I s, vp, null, said) 
PR(c~ I Ce,Ch~,Cm-l, Wp), e.g. 
PR(null \[ s, vp, null, said) 
Part-of-speech tags, tin, for modifiers are 
predicted based on the modifier, Cm, the part- 
of-speech tag of the head word, th, and the 
head word itself, wh: 
P(t m ICm,th,wh), e.g. 
P(per / nnp \[ per /np, vbd, said) 
Head words, win, for modifiers are predicted 
based on the modifier, cm, the part-of-speech 
tag of the modifier word , t,,, the part-of- 
speech tag of the head word, th, and the head 
word itself, Wh: 
P(W m ICm,tmth,Wh),  e.g. 
P(nance I per / np, per / nnp, vbd, said) 
Finally, word features, fro, for modifiers are 
predicted based on the modifier, cm, the part- 
of-speech tag of the modifier word , tin, the 
part-of-speech tag of the head word , th, the 
head word itself, Wh, and whether or not the 
modifier head word, w,,, is known or unknown. 
P(fm \[Cm,tm,th,Wh,known(Wm)), e.g. 
P( cap I per I np, per / nnp, vbd, said, true) 
The probability of a complete tree is the 
product of the probabilities of generating each 
element in the tree. If we generalize the tree 
components (constituent labels, words, tags, 
etc.) and treat them all as simply elements, e, 
and treat all the conditioning factors as the 
history, h, we can write: 
P(tree) = H e(e I h) 
e~tree 
8 Training the Model 
Maximum likelihood estimates for the model 
probabilities can be obtained by observing 
frequencies in the training corpus. However, 
because these estimates are too sparse to be 
relied upon, we use interpolated estimates 
consisting of mixtures of successively lower- 
order estimates (as in Placeway et al 1993). 
For modifier constituents, 
components are: 
P'(cm I cp, chp, Cm_ l , w p) = 
21 P(c,, ICp,Chp,C,,_I,W,) 
+22 P(cm I%,chp,Cm_,) 
the mixture 
For part-of-speech tags, 
components are: 
P'(t m ICm,th,Wh)=21 P(t m Icm,wh) 
+'~2 e(tm I cm, th) 
+~3 P(t,, I C~,) 
the mixture 
For head words, the mixture components are: 
P'(w m I Cm,tm,th, wh) = JL 1 P(w m I Cm,tm, Wh) 
+22 P(wm Icm,tm,th) 
+23 P(w m I Cm,t,,) 
+~4 P(w, It,,) 
Finally, for word features, the mixture 
components are: 
231 
P'(f,, \[c,,,t~,t h, w h, known(w,,)) = 
21 P(f,, )c,,,t,,,wh,known(w,,)) 
+)\[2 e(f., \[c~,t,,,th,kn?wn(w,,)) 
+A3 e(L, \[c,,,t ,,known(w,,)) 
+As P(fm \[t,,,known(w,,)) 
9 Searching the Model 
Given a sentence to be analyzed, the search 
program must find the most likely semantic 
and syntactic interpretation. More precisely, it
must find the most likely augmented parse 
tree. Although mathematically the model 
predicts tree elements in a top-down fashion, 
we search the space bottom-up using a chart- 
based search. The search is kept tractable 
through a combination of CKY-style dynamic 
programming and pruning of low probability 
elements. 
9.1 Dynamic Programming 
Whenever two or more constituents are 
equivalent relative to all possible later parsing 
decisions, we apply dynamic programming, 
keeping only the most likely constituent in the 
chart. Two constituents are considered 
equivalent if: 
1. They have identical category labels. 
2. Their head constituents have identical 
labels. 
3. They have the same head word. 
4. Their leftmost modifiers have identical 
labels. 
. Their rightmost modifiers have identical 
labels. 
9.2 Pruning 
Given multiple constituents that cover 
identical spans in the chart, only those 
constituents with probabilities within a 
threshold of the highest scoring constituent are 
maintained; all others are pruned. For 
purposes of pruning, and only for purposes of 
pruning, the prior probability of each 
constituent category is multiplied by the 
generative probability of that constituent 
(Goodman, 1997). We can think of this prior 
probability as an estimate of the probability of 
generating a subtree with the constituent 
category, starting at the topmost node. Thus, 
the scores used in pruning can be considered 
as the product of: 
. The probability of generating a constituent 
of the specified category, starting at the 
topmost node. 
. The probability of generating the structure 
beneath that constituent, having already 
generated a constituent ofthat category. 
Given a new sentence, the outcome of this 
search process is a tree structure that encodes 
both the syntactic and semantic structure of the 
sentence. The semantics - that is, the entities 
and relations - can then be directly extracted 
from these sentential trees. 
10 Experimental Results 
Our system for MUC-7 consisted of the 
sentential model described in this paper, 
coupled with a simple probability model for 
cross-sentence merging. The evaluation 
results are summarized in Table 1. 
In both Template Entity (TE) and Template 
Relation (TR), our system finished in second 
place among all entrants. Nearly all of the 
work was done by the sentential model; 
disabling the cross-sentence model entirely 
reduced our overall F-Score by only 2 points. 
Task Recall Precision 
Entities (TE) 83% 84% 
Relations (TR) 64% 81% 
Table 1:MUC-7 scores. 
F-Score 
83.49% 
71.23% 
232 
Task Score 
Part-of-Speech Tagging 95.99 (% correct) 
Parsing (sentences <40 words) 85.06 (F-Score) 
Name Finding 92.28 (F-Score) 
Table 2: Component task performance. 
While our focus throughout the project was on 
TE and TR, we became curious about how 
well the model did at part-of-speech tagging, 
syntactic parsing, and at name finding. We 
evaluated part-of-speech tagging and parsing 
accuracy on the Wall Street Journal using a 
now standard procedure (see Collins 97), and 
evaluated name finding accuracy on the MUC- 
7 named entity test. The results are 
summarized in Table 2. 
While performance did not quite match the 
best previously reported results for any of 
these three tasks, we were pleased to observe 
that the scores were at or near state-of-the-art 
levels for all cases. 
11 Conclusions 
We have demonstrated, at least for one 
problem, that a lexicalized, probabilistic 
context-free parser with head rules (LPCFG- 
HR) can be used effectively for information 
extraction. A single model proved capable of 
performing all necessary sentential processing, 
both syntactic and semantic. We were able to 
use the Penn TREEBANK to estimate the 
syntactic parameters; no additional syntactic 
training was required. The semantic training 
corpus was produced by students according to 
a simple set of guidelines. This simple 
semantic annotation was the only source of 
task knowledge used to configure the model. 
Acknowledgements 
The work reported here was supported in part 
by the Defense Advanced Research Projects 
Agency. Technical agents for part of this work 
were Fort Huachucha and AFRL under 
contract numbers DABT63-94-C-0062, 
F30602-97-C-0096, and 4132-BBN-001. The 
views and conclusions contained in this 
document are those of the authors and should 
not be interpreted as necessarily representing 
the official policies, either expressed or 
implied, of the Defense Advanced Research 
Projects Agency or the United States 
Government. 
We thank Michael Collins of the University of 
Pennsylvania for his valuable suggestions. 
References 
Bikel, Dan; S. Miller; R. Schwartz; and R. 
Weischedel. (1997) "NYMBLE: A High- 
Performance Learning Name-finder." In 
Proceedings of the Fifth Conference on Applied 
Natural Language Processing, Association for 
Computational Linguistics, pp. 194-201. 
Collins, Michael. (1996) "A New Statistical Parser 
Based on Bigram Lexical Dependencies." In
Proceedings of the 34th Annual Meeting of the 
Association for Computational Linguistics, pp. 
184-191. 
Collins, Michael. (1997) "Three Generative, 
Lexicalised Models for Statistical Parsing." In 
Proceedings of the 35th Annual Meeting of the 
Association for Computational Linguistics, pp. 
16-23. 
Marcus, M.; B. Santorini; and M. Marcinkiewicz. 
(1993) "Building a Large Annotated Corpus of 
English: the Penn Treebank." Computational 
Linguistics, 19(2):313-330. 
Goodman, Joshua. (1997) "Global Thresholding 
and Multiple-Pass Parsing." In Proceedings of 
the Second Conference on Empirical Methods in 
Natural Language Processing, Association for 
Computational Linguistics, pp. 11-25. 
Placeway, P., R. Schwartz, et al (1993). "The 
Estimation of Powerful Language Models from 
Small and Large Corpora." IEEE ICASSP 
Weischedel, Ralph; Marie Meteer; Richard 
Schwartz; Lance Ramshaw; and Jeff Palmucci. 
(1993) "Coping with Ambiguity and Unknown 
Words through Probabilistic Models." 
Computational Linguistics, 19(2):359-382. 
233 
 	
	
-
 	



ffProceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 57?60,
New York, June 2006. c?2006 Association for Computational Linguistics
OntoNotes: The 90% Solution 
 
 
Eduard Hovy Mitchell Marcus Martha Palmer Lance Ramshaw Ralph Weischedel 
USC/ICI Comp & Info Science ICS and Linguistics BBN Technologies BBN Technologies 
4676 Admiralty U. of Pennsylvania U. of Colorado 10 Moulton St. 10 Moulton St. 
Marina d. R., CA Philadelphia, PA Boulder, CO Cambridge, MA Cambridge, MA 
hovy 
@isi.edu 
mitch  
@cis.upenn.edu 
martha.palmer 
@colorado.edu 
lance.ramshaw 
@bbn.com 
weischedel 
@bbn.com 
 
 
 
 
Abstract* 
We describe the OntoNotes methodology and its 
result, a large multilingual richly-annotated corpus 
constructed at 90% interannotator agreement. An 
initial portion (300K words of English newswire 
and 250K words of Chinese newswire) will be 
made available to the community during 2007. 
1 Introduction 
Many natural language processing applications 
could benefit from a richer model of text meaning 
than the bag-of-words and n-gram models that cur-
rently predominate. Until now, however, no such 
model has been identified that can be annotated 
dependably and rapidly. We have developed a 
methodology for producing such a corpus at 90% 
inter-annotator agreement, and will release com-
pleted segments beginning in early 2007. 
The OntoNotes project focuses on a domain in-
dependent representation of literal meaning that 
includes predicate structure, word sense, ontology 
linking, and coreference. Pilot studies have shown 
that these can all be annotated rapidly and with 
better than 90% consistency. Once a substantial 
and accurate training corpus is available, trained 
algorithms can be developed to predict these struc-
tures in new documents. 
                                                        
*
 This work was supported under the GALE program of the 
Defense Advanced Research Projects Agency, Contract No. 
HR0011-06-C-0022. 
This process begins with parse (TreeBank) and 
propositional (PropBank) structures, which provide 
normalization over predicates and their arguments.  
Word sense ambiguities are then resolved, with 
each word sense also linked to the appropriate 
node in the Omega ontology. Coreference is also 
annotated, allowing the entity mentions that are 
propositional arguments to be resolved in context. 
Annotation will cover multiple languages (Eng-
lish, Chinese, and Arabic) and multiple genres 
(newswire, broadcast news, news groups, weblogs, 
etc.), to create a resource that is broadly applicable. 
2 Treebanking 
The Penn Treebank (Marcus et al, 1993) is anno-
tated with information to make predicate-argument 
structure easy to decode, including function tags 
and markers of ?empty? categories that represent 
displaced constituents.  To expedite later stages of 
annotation, we have developed a parsing system 
(Gabbard et al, 2006) that recovers both of these 
latter annotations, the first we know of.  A first-
stage parser matches the Collins (2003) parser on 
which it is based on the Parseval metric, while si-
multaneously achieving near state-of-the-art per-
formance on recovering function tags (F-measure 
89.0). A second stage, a seven stage pipeline of 
maximum entropy learners and voted perceptrons, 
achieves state-of-the-art performance (F-measure 
74.7) on the recovery of empty categories by com-
bining a linguistically-informed architecture and a 
rich feature set with the power of modern machine 
learning methods. 
57
3 PropBanking  
The Penn Proposition Bank, funded by ACE 
(DOD), focuses on the argument structure of verbs, 
and provides a corpus annotated with semantic 
roles, including participants traditionally viewed as 
arguments and adjuncts.  The 1M word Penn Tree-
bank II Wall Street Journal corpus has been suc-
cessfully annotated with semantic argument 
structures for verbs and is now available via the 
Penn Linguistic Data Consortium as PropBank I 
(Palmer et al, 2005).   Links from the argument 
labels in the Frames Files to FrameNet frame ele-
ments and VerbNet thematic roles are being added.  
This style of annotation has also been successfully 
applied to other genres and languages. 
4 Word Sense  
Word sense ambiguity is a continuing major ob-
stacle to accurate information extraction, summari-
zation and machine translation.  The subtle fine-
grained sense distinctions in WordNet have not 
lent themselves to high agreement between human 
annotators or high automatic tagging performance. 
Building on results in grouping fine-grained 
WordNet senses into more coarse-grained senses 
that led to improved inter-annotator agreement 
(ITA) and system performance (Palmer et al, 
2004; Palmer et al, 2006), we have  developed a 
process for rapid sense inventory creation and an-
notation that includes critical links between the 
grouped word senses and the Omega ontology 
(Philpot et al, 2005; see Section 5 below). 
This process is based on recognizing that sense 
distinctions can be represented by linguists in an 
hierarchical structure, similar to a decision tree, 
that is rooted in very coarse-grained distinctions 
which become increasingly fine-grained until 
reaching WordNet senses at the leaves.  Sets of 
senses under specific nodes of the tree are grouped 
together into single entries, along with the syntac-
tic and semantic criteria for their groupings, to be 
presented to the annotators.   
As shown in Figure 1, a 50-sentence sample of 
instances is annotated and immediately checked for 
inter-annotator agreement.  ITA scores below 90% 
lead to a revision and clarification of the groupings 
by the linguist. It is only after the groupings have 
passed the ITA hurdle that each individual group is 
linked to a conceptual node in the ontology. In ad-
dition to higher accuracy, we find at least a three-
fold increase in annotator productivity. 
 
Figure 1. Annotation Procedure 
As part of OntoNotes we are annotating the 
most frequent noun and verb senses in a 300K 
subset of the PropBank, and will have this data 
available for release in early 2007.  
4.1 Verbs 
Our initial goal is to annotate the 700 most fre-
quently occurring verbs in our data, which are 
typically also the most polysemous; so far 300 
verbs have been grouped and 150 double anno-
tated. Subcategorization frames and semantic 
classes of arguments play major roles in determin-
ing the groupings, as illustrated by the grouping for 
the 22 WN 2.1 senses for drive in Figure 2.  In ad-
word
Check against ontology (1 person)
not OK
Annotate test (2 people)
Results: agreement 
and confusion matrix
Sense partitioning, creating definitions, 
commentary, etc. (2 or 3 people)
Adjudication (1 person)
OK 
not OK
Sa
ve
 
fo
r 
fu
ll
a
n
n
o
ta
tio
n
GI: operating or traveling via a vehi-
cle 
NP (Agent) drive NP, NP drive PP 
WN1: ?Can you drive a truck??, WN2: ?drive to school,?, WN3: ?drive her to 
school,?, WN12: ?this truck drives well,? WN13: ?he drives a taxi,?,WN14: ?The car 
drove around the corner,?, WN:16: ?drive the turnpike to work,?  
G2: force to a position or stance 
NP drive NP/PP/infinitival 
WN4: ?He drives me mad.,? WN6: ?drive back the invaders,? WN7: ?She finally 
drove him to change jobs,? WN8: ?drive a nail,? WN15: ?drive the herd,? WN22: 
?drive the game.? 
G3:  to exert energy on behalf of 
something NP drive NP/infinitival 
WN5: ?Her passion drives her,? WN10: ?He is driving away at his thesis.? 
G4: cause object to move rapidly by 
striking it NP drive NP 
WN9: ?drive the ball into the outfield ,? WN17 ?drive a golf ball,? WN18 ?drive a 
ball? 
Figure 2. A Portion of the Grouping of WordNet Senses for "drive? 
58
dition to improved annotator productivity and ac-
curacy, we predict a corresponding improvement 
in word sense disambiguation performance.  Train-
ing on this new data, Chen and Palmer (2005) re-
port 86.3% accuracy for verbs using a smoothed 
maximum entropy model and rich linguistic fea-
tures, which is 10% higher than their earlier, state-
of-the art performance on ungrouped, fine-grained 
senses. 
4.2 Nouns 
We follow a similar procedure for the annotation 
of nouns.  The same individual who groups Word-
Net verb senses also creates noun senses, starting 
with WordNet and other dictionaries.  We aim to 
double-annotate the 1100 most frequent polyse-
mous nouns in the initial corpus by the end of 
2006, while maximizing overlap with the sentences 
containing annotated verbs.   
Certain nouns carry predicate structure; these 
include nominalizations (whose structure obvi-
ously is derived from their verbal form) and vari-
ous types of relational nouns (like father, 
President, and believer, that express relations be-
tween entities, often stated using of).  We have 
identified a limited set of these whose structural 
relations can be semi-automatically annotated with 
high accuracy.   
5 Ontology  
In standard dictionaries, the senses for each word 
are simply listed.   In order to allow access to addi-
tional useful information, such as subsumption, 
property inheritance, predicate frames from other 
sources, links to instances, and so on, our goal is to 
link the senses to an ontology.  This requires de-
composing the hierarchical structure into subtrees 
which can then be inserted at the appropriate con-
ceptual node in the ontology. 
The OntoNotes terms are represented in the 
110,000-node Omega ontology (Philpot et al, 
2005), under continued construction and extension 
at ISI.  Omega, which has been used for MT, 
summarization, and database alignment, has been 
assembled semi-automatically by merging a vari-
ety of sources, including Princeton?s WordNet, 
New Mexico State University?s Mikrokosmos, and 
a variety of Upper Models, including DOLCE 
(Gangemi et al, 2002), SUMO (Niles and Pease, 
2001), and ISI?s Upper Model, which are in the 
process of being reconciled.  The verb frames from 
PropBank, FrameNet, WordNet, and Lexical Con-
ceptual Structures (Dorr and Habash, 2001) have 
all been included and cross-linked.   
In work planned for later this year, verb and 
noun sense groupings will be manually inserted 
into Omega, replacing the current (primarily 
WordNet-derived) contents. For example, of the 
verb groups for drive in the table above, G1 and 
G4 will be placed into the area of ?controlled mo-
tion?, while G2 will then sort with ?attitudes?.   
6 Coreference  
The coreference annotation in OntoNotes connects 
coreferring instances of specific referring expres-
sions, meaning primarily NPs that introduce or 
access a discourse entity. For example, ?Elco In-
dustries, Inc.?, ?the Rockford, Ill. Maker of fasten-
ers?, and ?it? could all corefer. (Non-specific 
references like ?officials? in ?Later, officials re-
ported?? are not included, since coreference for 
them is frequently unclear.) In addition, proper 
premodifiers and verb phrases can be marked when 
coreferent with an NP, such as linking, ?when the 
company withdrew from the bidding? to ?the with-
drawal of New England Electric?.  
Unlike the coreference task as defined in the 
ACE program, attributives are not generally 
marked. For example, the ?veterinarian? NP would 
not be marked in ?Baxter Black is a large animal 
veterinarian?. Adjectival modifiers like ?Ameri-
can? in ?the American embassy? are also not sub-
ject to coreference. 
Appositives are annotated as a special kind of 
coreference, so that later processing will be able to 
supply and interpret the implicit copula link. 
All of the coreference annotation is being dou-
bly annotated and adjudicated. In our initial Eng-
lish batch, the average agreement scores between 
each annotator and the adjudicated results were 
91.8% for normal coreference and 94.2% for ap-
positives. 
7 Related and Future Work  
PropBank I (Palmer et al, 2005), developed at 
UPenn, captures predicate argument structure for 
verbs; NomBank provides predicate argument 
structure for nominalizations and other noun predi-
cates (Meyers et al, 2004).  PropBank II annota-
59
tion (eventuality ID?s, coarse-grained sense tags, 
nominal coreference and selected discourse con-
nectives) is being applied to a small (100K) paral-
lel Chinese/English corpus (Babko-Malaya et al, 
2004).  The OntoNotes representation extends 
these annotations, and allows eventual inclusion of 
additional shallow semantic representations for 
other phenomena, including temporal and spatial 
relations, numerical expressions, deixis, etc. One 
of the principal aims of OntoNotes is to enable 
automated semantic analysis.  The best current al-
gorithm for semantic role labeling for PropBank 
style annotation (Pradhan et al, 2005) achieves an 
F-measure of 81.0 using an SVM. OntoNotes will 
provide a large amount of new training data for 
similar efforts.   
Existing work in the same realm falls into two 
classes: the development of resources for specific 
phenomena or the annotation of corpora. An ex-
ample of the former is Berkeley?s FrameNet pro-
ject (Baker et al, 1998), which produces rich 
semantic frames, annotating a set of examples for 
each predicator (including verbs, nouns and adjec-
tives), and describing the network of relations 
among the semantic frames.  An example of the 
latter type is the Salsa project (Burchardt et al, 
2004), which produced a German lexicon based on 
the FrameNet semantic frames and annotated a 
large German newswire corpus.  A second exam-
ple, the Prague Dependency Treebank (Hajic et al, 
2001), has annotated a large Czech corpus with 
several levels of (tectogrammatical) representation, 
including parts of speech, syntax, and topic/focus 
information structure. Finally, the IL-Annotation 
project (Reeder et al, 2004) focused on the repre-
sentations required to support a series of increas-
ingly semantic phenomena across seven languages 
(Arabic, Hindi, English, Spanish, Korean, Japanese  
and French). In intent and in many details, 
OntoNotes is compatible with all these efforts, 
which may one day all participate in a larger multi-
lingual corpus integration effort.   
References  
O. Babko-Malaya, M. Palmer, N. Xue, A. Joshi, and S. Ku-
lick. 2004. Proposition Bank II: Delving Deeper, Frontiers 
in Corpus Annotation, Workshop, HLT/NAACL  
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berke-
ley FrameNet Project. In Proceedings of COLING/ACL, 
pages 86-90. 
J. Chen and M. Palmer.  2005.  Towards Robust High Per-
formance Word Sense Disambiguation of English Verbs 
Using Rich Linguistic Features. In Proceedings of 
IJCNLP2005, pp. 933-944. 
B. Dorr and N. Habash.  2001.  Lexical Conceptual Structure 
Lexicons. In Calzolari et al ISLE-IST-1999-10647-WP2-
WP3, Survey of Major Approaches Towards Bilin-
gual/Multilingual Lexicons.  
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado, and M. 
Pinkal. 2006. Consistency and Coverage: Challenges for 
exhaustive semantic annotation. In Proceedings of DGfS-
06. 
C. Fellbaum (ed.). 1998. WordNet: An On-line Lexical Data-
base and Some of its Applications. MIT Press. 
R. Gabbard, M. Marcus, and S. Kulick. Fully Parsing the Penn 
Treebank. In Proceedings of HLT/NAACL 2006.  
A. Gangemi, N. Guarino, C. Masolo, A. Oltramari, and L. 
Schneider. 2002. Sweetening Ontologies with DOLCE. In 
Proceedings of EKAW  pp. 166-181. 
J. Hajic, B. Vidov?-Hladk?, and P. Pajas.  2001: The Prague 
Dependency Treebank: Annotation Structure and Support. 
Proceeding of the IRCS Workshop on Linguistic Data-
bases, pp. 105?114. 
M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. 
Building a Large Annotated Corpus of English: The Penn 
Treebank. Computational Linguistics 19: 313-330. 
A. Meyers, R. Reeves, C Macleod, R. Szekely, V. Zielinska, 
B. Young, and R. Grishman. 2004. The NomBank Project: 
An Interim Report. Frontiers in Corpus Annotation, Work-
shop in conjunction with HLT/NAACL. 
I. Niles and A. Pease.  2001.  Towards a Standard Upper On-
tology.  Proceedings of the International Conference on 
Formal Ontology in Information Systems (FOIS-2001). 
M. Palmer, O. Babko-Malaya, and H. T. Dang. 2004. Differ-
ent Sense Granularities for Different Applications, 2nd 
Workshop on Scalable Natural Language Understanding 
Systems, at HLT/NAACL-04,  
M. Palmer, H. Dang and C. Fellbaum. 2006. Making Fine-
grained and Coarse-grained Sense Distinctions, Both 
Manually and Automatically, Journal of Natural Language 
Engineering, to appear. 
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposi-
tion Bank: A Corpus Annotated with Semantic Roles, 
Computational Linguistics, 31(1). 
A. Philpot, E.. Hovy, and P. Pantel. 2005. The Omega Ontol-
ogy. Proceedings of the ONTOLEX Workshop at IJCNLP 
 S. Pradhan, W. Ward, K. Hacioglu, J. Martin, D. Jurafsky.  
2005.  Semantic Role Labeling Using Different Syntactic 
Views.  Proceedings of the ACL.  
F. Reeder, B. Dorr, D. Farwell, N. Habash, S. Helmreich, E.H. 
Hovy, L. Levin, T. Mitamura, K. Miller, O. Rambow, A. 
Siddharthan. 2004.  Interlingual Annotation for MT Devel-
opment. Proceedings of AMTA.  
60
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1437?1446,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Extreme Extraction -- Machine Reading in a Week 
 
Marjorie Freedman, Lance Ramshaw, Elizabeth Boschee, Ryan Gabbard,  
Gary Kratkiewicz, Nicolas Ward, Ralph Weischedel 
Raytheon BBN Technologies 
10 Moulton St. 
Cambridge, MA 02138 
mfreedma,lramshaw,eboschee,rgabbard,kratkiewicz, 
nward,weischedel@bbn.com 
 
The views expressed are those of the author and do not reflect the official policy or position of the Depart-
ment of Defense or the U.S. Government. This is in accordance with DoDI 5230.29, January 8, 2009.   
  
 
Abstract 
We report on empirical results in extreme 
extraction. It is extreme in that (1) from re-
ceipt of the ontology specifying the target 
concepts and relations, development is li-
mited to one week and that (2) relatively 
little training data is assumed. We are able 
to surpass human recall and achieve an F1 
of 0.51 on a question-answering task with 
less than 50 hours of effort using a hybrid 
approach that mixes active learning, boot-
strapping, and limited (5 hours) manual 
rule writing. We compare the performance 
of three systems: extraction with handwrit-
ten rules, bootstrapped extraction, and a 
combination. We show that while the recall 
of the handwritten rules surpasses that of 
the learned system, the learned system is 
able to improve the overall recall and F1.      
1 Introduction 
Throughout the Automatic Content Extraction 1 
(ACE) evaluations and the Message Understanding 
Conferences2 (MUC), teams typically had a year or 
more from release of the target to submitting sys-
tem results. One exception was MUC-6 (Grishman 
& Sundheim, 1996), in which scenario templates 
for changing positions were extracted given only 
one month. Our goal was to confine development 
to a calendar week, in fact, <50 person hours. This 
                                                          
1 http://www.nist.gov/speech/tests/ace/ 
2 http://www-nlpir.nist.gov/related_projects/muc/ 
is significant in two ways: the less effort it takes to 
bring up a new domain, (1) the more broadly ap-
plicable the technology is and (2) the less effort 
required to run a diagnostic research experiment. 
Our second goal concerned minimizing training 
data. Rather than approximately 250k words of 
entity and relation annotation as in ACE, only ~20 
example pairs per relation-type were provided as 
training. Reducing the training requirements has 
the same two desirable outcomes: demonstrating 
that the technology can be broadly applicable and 
reducing the overhead for running experiments. 
The system achieved recall of 0.49 and precision 
of 0.53 (for an F1 of 0.51) on a blind test set of 60 
queries of the form Ri(arg1, arg2), where Ri is one 
of the 5 new relations and exactly one of arg1 or 
arg2 is a free variable for each query. 
Key to this achievement was a hybrid of:   
? a variant of (Miller, et al, 2004) to learn two 
new classes of entities via automatically induced 
word classes and active learning (6 hours) 
? bootstrap relation learning (Freedman et al 
2010) to learn 5 new relation classes (2.5 hours),  
? handwritten patterns over predicate-argument 
structure (5 hours), and 
? coreference (20 hours) 
Our bootstrap learner is initialized with relation 
tuples (not annotated text) and uses LDC?s Giga-
word and Wikipedia as a background corpus to 
learn patterns for relation detection that are based 
on normalized predicate argument structure as well 
as surface strings.  
These early empirical results suggest the follow-
ing: (1) It is possible to specify a domain, adapt 
our system, and complete manual scoring, includ-
1437
ing human performance, within a month. Experi-
ments in machine reading (and in extraction) can 
be performed much more quickly and cheaply than 
ever before. (2) Through machine learning and 
limited human pattern writing (6 hours), we 
adapted a machine reading system within a week 
(using less than 50 person hours), achieving ques-
tion answering performance with an F1 of 0.5 and 
with recall 11% higher (relative) to a human read-
er. (3) Unfortunately, machine learning, though 
achieving 80% precision,3 significantly lags behind 
a gifted human pattern writer in recall. Thus, boot-
strap learning with much higher recall at minimal 
sacrifice in precision is highly desirable. 
2 Related Work 
This effort is evaluated extrinsically via formal 
questions expressed as a binary relation with one 
free variable. This contrasts with TREC Question 
Answering, 4  where the questions are in natural 
language, and not restricted to a single binary rela-
tion. Like the ?list? queries of TREC QA, the re-
quirement is to find all answers, not just one. 
Though question interpretation is not required in 
our work, interpretation of the text corpus is. 
The goal of rapid adaptation has been tested in 
other contexts. In 2003, a series of experiments in 
adapting to a new language in less than month 
tested system performance on Cebuano and Hindi. 
The primary goal was to adapt to a new language, 
rather than a new domain. The extraction partici-
pants focused on named-entity recognition, not 
relation extraction (May, et al 2003; Sekine & 
Grishman, 2003; Li & McCallum, 2003; Maynard 
et al 2003). The scenario templates of MUC-6 
(Grishman & Sundheim, 1996) are more similar to 
our relation extraction task, although the domain is 
quite different. Our experiment allowed for 1 week 
of development time, while MUC-6 allowed a 
month. The core entities in the MUC-6 task 
(people and organizations) had been worked on 
previously. In contrast all of our relations included 
at least one novel class. While MUC-6 systems 
tended to use finite-state patterns, they did not in-
corporate bootstrapping or patterns based on the 
output of a statistical parser.   
                                                          
3 Handwritten patterns achieved 52% precision. 
4 http://trec.nist.gov/data/qamain.html 
For learning entity classes, we follow Miller, et 
al., (2004), using word clustering and active learn-
ing to train a perceptron model, but unlike that 
work we apply the technique not just to names but 
also to descriptions. An alternative approach to 
learning classes, applying structural patterns to 
bootstrap description recognition without active 
learning, is seen in Riloff (1996) and Kozareva et 
al., (2008)   
Much research (e.g. Ramshaw 2001) has fo-
cused on learning relation extractors using large 
amounts of supervised training, as in ACE. The 
obvious weakness of such approaches is the result-
ing reliance on manually annotated examples, 
which are expensive and time-consuming to create.  
Others have explored bootstrap relation learn-
ing from seed examples. Agichtein & Gravano 
(2000) and Ravichandran & Hovy (2002) reported 
results for generating surface patterns for relation 
identification; others have explored similar ap-
proaches (e.g. Pantel & Pennacchiotti, 2006). Mit-
chell et al (2009) showed that for macro-reading, 
precision and recall can be improved by learning a 
large set of interconnected relations and concepts 
simultaneously. None use coreference to find train-
ing examples; all use surface (word) patterns. 
Freedman et. al (2010) report improved perfor-
mance from using predicate structure for boot-
strappped relation learning.  
Most approaches to automatic pattern genera-
tion have focused on precision, e.g., Ravichandran 
and Hovy (2002) report results in TREC QA, 
where extracting one instance of a relation can be 
sufficient, rather than detecting all instances. Mit-
chell et al (2009), while demonstrating high preci-
sion, do not measure recall. 
By contrast, our work emphasizes recall, not 
just precision. Our question answering task asks 
list-like questions that require multiple answers.  
We also include the results of a secondary, extrac-
tion evaluation which requires that the system 
identify every mention of the relations in a small 
set of documents. This evaluation is loosely based 
on the relation mention detection task in ACE.  
3 Task Set-Up and Evaluation 
Our effort was divided into four phases. During the 
first phase, a third party produced an ontology and 
the resources, which included: brief (~1 paragraph) 
guidelines for each relation and class in the ontolo-
1438
gy; ~20 examples for each relation in the ontology; 
2K documents that are rich in domain relations. 
Table 1 lists the 5 new relations and number of ex-
amples provided for each. Arguments in italics 
were known by the system prior to the evaluation.  
Relation Ex. 
possibleTreatment(Substance, Condition) 23 
expectedDateOnMarket(Substance, Date) 11 
responsibleForTreatment(Substance, Agent) 19 
studiesDisease(Agent, Condition) 16 
hasSideEffect(Substance, Condition) 27 
Table 1: New Relations and Number of Examples 
In phase two, we spent one week extending our 
extraction system for the new ontology. During the 
third phase, we ran our system over 10K docu-
ments to extract all instances of domain relations 
from those documents. In the fourth phase, our 
question answering system used the extracted in-
formation to answer queries.  
4 Approach to Domain Specialization 
Our approach to extracting domain relations inte-
grated novel relation and class detectors into an 
existing extraction system, designed primarily 
around the ACE tasks. The existing system uses a 
discriminatively trained classifier to detect the enti-
ty and value types of ACE. It also produces a syn-
tactic parse for each sentence; normalizes these 
parses to find logical predicate argument structure; 
and detects and coreferences pronominal, nominal, 
and name mentions for each of the 7 ACE entity 
types (Person, Organization, Geopolitical Entity, 
Location, Facility, Weapon, and Vehicle).5  
The extraction system has three components that 
allow for rapid adaptation to a new domain:  
? Class detectors trained using word classes de-
rived from unsupervised clustering and sentence-
selected training data. 
? A bootstrap relation learner which given a few 
seed examples learns patterns that indicate the 
presence of relations.  
? An expressive pattern language which allows a 
developer to express rules for relation extraction 
in a simple, but fast manner.  
 
 
Component Approach Effort 
Class Recognizer Active Learning 6 hrs 
                                                          
5 The extraction system detects relations and events in the 
ACE ontology, but these were not used in the current work.  
 
Class Recognizer Web-Mined List 1 hrs 
Relation Recognizer 
Semi-supervised 
Bootstrapping 
8.5 hrs 
Relation Recognizer Manual Patterns 5 hrs 
Coreference Heuristics 20 hrs 
Table 2: Effort and Approach for New Domain 
4.1 Class Extraction  
Each of the relations in the new domain included at 
least one argument that was new. While question 
answering requires the system to identify the 
classes only when they appear in a relation, know-
ledge of when a class is present provides important 
information for relation extraction. For example in 
our ontology, Y is a treatment for X only if Y is a 
substance. Thus, ?Group counseling sessions are 
effective treatments for depression? does not con-
tain an instance of possibleTreatment(), while 
?SSRIs are effective treatments for depression? 
does. The bootstrap learner allows constraints 
based on argument type. To use this capability, we 
trained the recognizer at the beginning of the week 
of domain adaptation and used the predicted 
classes during learning.  
We annotated 1064 sentences (~31K words) us-
ing active learning combined with unsupervised 
word clusters (Miller,et al, 2004) for the following 
classes: Substance-Name, Substance-Description, 
Condition-Name, and Condition-Description. Ge-
neric noun-phrases like new drugs, the illness, etc 
were labeled as descriptors. Because of the time 
frame, we did not develop extensive guidelines nor 
measure inter-annotator agreement. Annotation 
took 6 hours. We supplemented our annotation 
with lists of substances and treatments from the 
web, which took 1 hour.  
4.2 Coreference 
Providing a name reference is generally preferable 
to a non-specific string (e.g. the drugs), but not 
always feasible; for instance, reports of new re-
search may appear without a name for the drug. 
Our existing system?s coreference algorithms op-
erate only on mentions of ACE entity types (per-
sons, organizations, GPEs, other locations, 
facilities, vehicles, and weapons). During the week 
of domain adaption we developed new heuristics 
for coreference over non-ACE types.  Most of our 
heuristics are domain independent (e.g. linking the 
parts of an appositive). Our decision to annotate 
names and descriptions separately was driven par-
1439
tially by the need to select the best reference (i.e. 
name) for co-referent clusters. Adding coreference 
heuristics for the two new entity types was the sin-
gle most time-consuming activity, taking 20 of the 
total 43 hours. 
4.3 Relation Extraction  
For relation extraction, we used both pattern 
learning and handwritten patterns. We initialized 
our bootstrap relation learner with the example 
instances provided with the domain ontology; Ta-
ble 3 includes examples of the instances provided 
to the system as training. Our bootstrap relation 
learner finds instances of the relation argument 
pairs in text and then proposes both predicate-
argument structure and word-based connections 
between the arguments as possible new patterns for 
the relation. The learner automatically prunes po-
tential patterns using information about the number 
of known-to-be true and novel instances matched 
by a proposed pattern. By running the pattern ex-
tractor over a large corpus, the proposed patterns 
generate new seeds which are in turn are used to 
propose new patterns. For this experiment, we in-
corporated a small amount of supervision during 
the bootstrapping process (roughly 1 hour total per 
relation); we also performed ~30 minutes total in 
pruning domain patterns at the end of learning.  
 Relation Arg-1 Arg-2 
possTreatmnt AZT AIDS 
studyDisease Dr Henri Joyeux cancer 
studyDisease Samir Khleif cancer 
Table 3: Sample Instances for Initializing Learner 
We also used a small amount of human effort 
creating rules for detecting the relations. The pat-
tern writer was given the guidelines, the examples, 
and a 2K document background corpus and spent 1 
hour per relation writing rules.  
The learned patterns use a subset of the full pat-
tern language used by the pattern-writer. The lan-
guage operates over surface-strings as well as 
predicate-argument structure. Figure 1 illustrates 
learned and handwritten patterns for the possible-
TreatmentRelation(). The patterns in rectangles 
match surface-string patterns; the tree-like patterns 
match normalized predicate argument structure.  
The ?WORD- token indicates a wild card of 1-3 
words.  The blue rectangles at the root of the trees 
in the handwritten patterns are sets of predicates 
that can be matched by the pattern. 
5 Evaluation  
Our question answering evaluation was inspired 
by the evaluation in DARPA?s machine reading 
program, which requires systems to map the in-
formation in text into a formal ontology and an-
swer questions based on that ontology. Unlike 
ACE, this allows evaluators to measure perfor-
mance without exhaustively annotating documents, 
allows for balance between rare and common rela-
tions, and implicitly measures coreference without 
requiring explicit annotation of answer keys for 
coreference. However because the evaluation only 
measures performance on the set of queries, many 
relation instances will be unscored. Furthermore, 
the system is not rewarded for finding the same 
relation multiple times; finding 100 instances of 
isPossibleTreatment(Penicillin, Strep Throat) is 
the same as finding 1 (or 10) instances.  
 
Figure 1: Sample Patterns for possibleTreatment() 
 
The evaluation included only queries of the type 
Find all instances for which the relation P(X, Z) is 
true where one of X or Z is constant. For example, 
Find possible treatments for diabetes; or What is 
expected date to market for Abilify? There were 60 
queries in the evaluation set to be answered from a 
10K document corpus. To produce a preliminary 
answer key, annotators were given the queries and 
corpus indexed by Google Desktop. Annotators 
were given 1 hour to find potential answers to each 
query. If no answers were found after 1 hour, the 
annotators were given a second hour to look for 
answers. For two queries, both of the form Find 
treatments with an expected date to market of MM-
YYYY, even after two hours of searching the anno-
tators were unable to find any answers.6  
Annotator answers served as the initial gold-
standard. Given this initial answer key, annotators 
reviewed system answers and aligned them with 
gold-standard answers. System output not aligned 
with the initial gold standard was assessed as cor-
rect or incorrect. We assume that the final gold-
standard constitutes a complete answer key, and 
                                                          
6 Evaluators wanted some queries with no answers. 
1440
are thus able to calculate recall for our system and 
for humans7. Because we had only one annotator 
for each query and because we assumed that any 
answer found by an annotator was correct, we 
could not estimate human precision on this task.  
Answers can be specific named concepts (e.g. 
Penicillin) or generic descriptions (e.g. drug, ill-
ness). Given the sentence, ACME produces a wide 
range of drugs including treatments for malaria 
and athletes foot,? our reading system would ex-
tract the relations responsibleForTreatment(drugs, 
ACME), possibleTreatment(drugs, malaria), pos-
sibleTreatment(drugs, athletes foot). When a name 
was available in the document, annotators marked 
the answer as correct, but underspecified. We cal-
culated precision and recall treating underspecified 
answers as incorrect and separately calculated pre-
cision and recall counting underspecified answers 
as correct. When treated as correct, there was less 
than a 0.05 absolute increase in both precision and 
recall. Unless otherwise specified, all scores re-
ported here use the stricter condition which treats 
underspecified answers as incorrect.  
We also evaluated extracting all information in a 
small document collection (here human search of 
the 10k documents does not play a role in finding 
answers). Individuals were asked to annotate every 
instance of the 5 relations in a set of 102 docu-
ments. Recall, Precision, and F were calculated by 
aligning system responses to the answer key. Sys-
tem answers that aligned are correct; those that did 
not are incorrect; and answers in the key that were 
not found by the system are misses. Unlike the 
question answering evaluation, this evaluation 
measures the ability to find every instance of a 
fact. If the gold standard includes 100 instances of 
isPossibleTreatment(Penicillin, Strep Throat), re-
call will decrease for each instance missed. The 
?extraction? evaluation does not penalize systems 
for missing coreference.  
6 Results 
6.1 Class Detection 
                                                          
7 The answer key may contain some answers that were found 
neither by the annotator nor by the systems described here, 
since the answer key includes answers pooled from other sys-
tems not reported in this paper. The system reported here was 
the highest performing of all those participating in the experi-
ment. Furthermore, if a system answer is marked as correct, 
but underspecified, the specific  answer is put in the key. 
The recall, precision, and F1 for class detection 
using 10-fold cross validation of the ~1K anno-
tated sentences appear in the 3-5th columns of Table 
4. Given the amount of training, our results are 
lower than in Miller et al(2004) (an F1 of 90 with 
less than 25K words of training). Several factors 
could explain this: Finding boundaries and types 
for descriptions is more complex than for names in 
English. 8  Our classes, pharmaceutical substances 
and physiological conditions, may have been more 
difficult to learn. Our classes are less common in 
news reporting; as such, both word-class clusters 
and active learning may have been less effective. 
Finally, our evaluation was done on a 10-fold split 
of the active-learning selected data; bias in select-
ing the data could explain at least a part of our 
lower performance.  
Type 
# in 
GS 
Without Lists With Lists 
R P F R P F 
Subst-D 789 77 85 80.8 78 85 81.3 
Subst-N 410 70 82 75.5 77 81 78.9 
Cond-D 427 72 78 74.9 72 77 74.4 
Cond-N 963 80 87 83.4 84 83 83.5 
Table 4: Cross Validation:  Condition & Substance 
We noticed that the system frequently reported 
country names to be substance-names. Surprising-
ly, we found that our well-trained name finder 
made the opposite mistake, occasionally reporting 
drugs as geo-political entities.  
We incorporated lists of known substances and 
conditions to improve recall. Performance on the 
same cross-validation split is shown in the final 
three columns of Table 4. Incorporating the lists led 
to recall gains for both substance-name and condi-
tion-name. Because a false-alarm in class recogni-
tion only leads to an incorrect relation extraction if 
it appears in a context indicating a domain relation, 
false alarms of classes may be less important in the 
question answering and extraction evaluations.   
6.2 Question Answering and Extraction 
Figure 2 and Table 6 show system performance 
using only handwritten rules (HW), only learned 
patterns (L), and combining both (C). Figure 2  
includes scores calculated with all of the systems? 
answers (in the dotted boxes), and with just those 
answers that were deemed useful (discussed be-
                                                          
8 English names are capitalized; person names have a typical 
form and are frequently signaled by titles; organization names 
frequently have clear signal words, such as Corp. 
1441
low). We include annotator recall. Handwritten 
patterns outperform learned patterns consistently 
with much higher recall. Encouragingly, however, 
1. The combined system?s recall and F-Score 
are noticeably higher for 3 of the relations.  
2. The learned patterns generate answers not 
found by handwritten patterns.  
3. The learned patterns have high precision.9 
There is variation across the different relations. 
The two best performing relations possibleTreat-
ment() and studiesDisease() have F1 more than 
twice as high as the two worst performing rela-
tions, expectedDateToMarket() and hasSideEf-
fect(). This is primarily due to differences in recall.  
 
Figure 2: Overall Q/A Performance: All answers in  
dotted boxes; 'Useful Answers' unboxed 
The combined system?s recall (0.49), while low, 
is higher than that of the annotators (0.44). While 
hardly surprising that a machine can process in-
formation much more quickly than a person, it is 
encouraging that higher recall is achieved even 
with only one week?s effort. In the context of our 
pooled answer-key, the relatively low recall of 
both the system and the annotator suggests that 
there was little overlap between the answers found 
by the annotator and those found by the system.  
As already described, the system answers can 
include both specific references (e.g. Prozac) and 
more generic references (the drug). When a more 
specific answer is present in the document, generic 
references have been treated as incorrect. Howev-
er, sometimes there is not a more specific refer-
ence; for example an article written before a drug 
has been released may never name the drug. Scores 
reported thus far treat such answers as correct. 
These answers would be useful when answering 
more complex queries. For example, given the sen-
                                                          
9 The learned patterns' high precision is to be expected for two 
reasons. First, a few bad patterns were manually removed for 
each relation. More importantly, the learning algorithm strong-
ly favors high precision patterns because it needs to maintain a 
seed set with low noise in order to learn effectively.  
tence ?ACME spent 5 years developing a pill to 
treat the flu which it will release next week,? ex-
tracting relations involving ?the pill?  would allow 
a system to answer questions that use multiple rela-
tions in the ontology to for example ask about  or-
ganizations developing treatments for the flu, or 
the expected date of release for ACME?s drugs. 
However, in our simple question answering 
framework such generic answers never convey 
novel information and thus were probably ignored 
by human annotators.  
 To measure the impact of treating these generic 
references as correct,10 we did additional annota-
tion on the correct answers, marking answers as 
?useful? (specific) and ?not-useful? (generic). The 
unboxed bars in Figure 2 show performance when 
?not-useful? answers are removed from the answer-
key and the responses. For the four relations where 
there was a change Table 5 provides the relative 
change performance when only ?useful? answers 
are considered. The annotator?s recall increases 
noticeably while the combined system?s drops. 
This results in the overall recall of annotators sur-
passing that of the combined system.   
Relation 
Recall Precision 
A C H L C H L 
possTreat 12 10 10 14 -10 -11 -3 
respTreat 9 0 -5 8 -4 -4 -1 
studyDis 12 -6 -9 13 -11 -13 0 
hasSidEff 3 4 4 4 0 0 0 
Total 11 -2 -4 6 -9 -10 -2 
Table 5: Relative Change in Recall and Precision When 
Non-Useful Answers are Removed 
Table 7 shows the total number of answers pro-
duced by annotators and by each system, as well as 
the percentage of queries with at least one correct 
answer for each system. For one relation expec-
tedDateOnMarket(), the learned system did not 
find any answers. This relation had far fewer an-
swers found by annotators and occurred far more 
rarely in the fully annotated extraction set (see Ta-
ble 8). Anecdotally, extracting this relation fre-
quently required co-referencing ?it? (e.g. ?It will be 
released in March 2011?). Our heuristics for core-
ference of the new classes did not account for pro-
nouns. Learning from such examples would 
require coreference during bootstrapping. Most 
likely, the learned system was unable to generate 
enough novel instances to continue bootstrapping 
                                                          
10 Generic answers were treated as correct only if a more spe-
cific reference was not available in the document.  
1442
and was thus unable to learn the relation.  
Relation Type 
(# Queries; # Correct Ans.) 
Recall Precision F 
A C HW L C HW L C HW L 
possTreatment (10;247) 0.27 0.63 0.50 0.34 0.51 0.47 0.83 0.56 0.48 0.48 
respForTreat (15;134) 0.73 0.33 0.24 0.22 0.66 0.78 0.73 0.44 0.37 0.33 
expectDateMarkt (11;60) 0.90 0.17 0.17 0.00 0.77 0.83 0.00 0.27 0.28 0 
studiesDisease (13;292) 0.23 0.67 0.59 0.09 0.51 0.50 0.79 0.58 0.54 0.16 
hasSideEffect (11;104) 0.80 0.10 0.13 0.02 0.83 0.70 1.00 0.17 0.23 0.04 
Total (60;837) 0.44 0.49 0.42 0.17 0.53 0.52 0.80 0.51 0.46 0.28 
Table 6: Question Answering Results by Relation Type 
Relation Type 
 
Total Number of Answers % Queries with At Least 1 Corr. Ans 
A C HW L A C HW L 
possTreatment  66 303 261 100 100.0% 90.0% 90.0% 90.0% 
respForTreat  98 67 41 40 100.0% 66.7% 60.0% 60.0% 
expectDateMarkt  54 13 12 0 72.7% 45.5% 45.5% 0.0% 
studiesDisease  68 379 347 33 100.0% 61.5% 46.2% 46.2% 
hasSideEffect  83 12 20 2 72.7% 36.4% 45.5% 18.2% 
Total  369 774 681 175 90.0% 60.0% 56.7% 43.3% 
Table 7: Number of Answers and Number of Queries Answered 
Overall, the system did better on relations hav-
ing more correct answers. Bootstrap learning has 
an easier time discovering new instances and new 
patterns when there are more examples to work 
with. Even a human pattern writer will have more 
examples to generalize from for common relations.  
While possibleTreatment() and hasSideEffect() 
have similar F-scores, their performance is very 
different at the query level. The system was able to 
find at least one correct answer to every possible-
Treatment() query; however only 72.7% of the stu-
diesDisease() queries were answered.  
Table 8 presents results from the extraction 
evaluation where a set of ~100 documents were 
annotated for all mentions of the 5 relations. Be-
cause every mention in the document set must be 
found, the system cannot rely on finding the easiest 
answers for common relations. The results in Table 
8 are significantly lower than for the question ans-
wering tasks; yet some of the same trends are 
present. Handwritten rules outperform learned pat-
terns. For at least some relations, the combination 
of the two improves performance. The three rela-
tions for which the learned system has the lowest 
performance on the question-answering task have 
the fewest instances annotated in the document set. 
Fewer instance in the large corpus make bootstrap-
ping more difficult?the learner is less able to gen-
erate novel instances to expand its pattern set.  
7 Discussion 
7.1 Sources of Error 
The most common source of error is pattern cover-
age. In the following figure, the system identified 
responsibleForTreatment(Janssen Pharmaceutical, 
Sporanox), but missed the corresponding relation 
between Novartis and Lamisil.  
 
 
 
 
 
Relation Type # Relations Found Recall Precision F 
GS C HW L C HW L C HW L C HW L 
possibleTreatment 518 225 187 68 0.15 0.10 0.09 0.34 0.28 0.66 0.21 0.15 0.15 
respForTreatment 387 101 77 36 0.10 0.08 0.05 0.41 0.40 0.50 0.17 0.13 0.08 
expDateOnMarket 66 13 13 0 0.06 0.06 0.00 0.31 0.31 0.00 0.10 0.10 0.00 
studiesDisease 136 95 91 4 0.08 0.09 0.00 0.12 0.13 0.00 0.10 0.11 0.00 
hasSideEffect 256 26 25 2 0.04 0.04 0.00 0.39 0.40 0.50 0.07 0.07 0.01 
Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the Relations 
Sporanox is made by Janssen Pharmaceutica Inc., 
of Titusville, N.J. Lamisil is a product of Novartis 
Pharmaceuticals of East Hanover, N.J. 
 
 
1443
Missed class instances contribute to errors, some-
times originating in errors in tokenization (e.g. not 
removing the ?_? in each drug name in a bulleted 
list of the form ?_Trovan, an antibiotic...; etc.) 
However, many drug-names are simply missed: 
 
The system correctly identifies Rebif and Aricept 
as drugs, but misses Pregabalin and Serono. In 
both misses, the immediately preceding and fol-
lowing words provide little evidence that the word 
refers to a drug rather than some other product. 
Substance detection might be better served with a 
web-scale, list-learning approach like the doubly 
anchored patterns described in (Kozareva et al, 
2008). Alternatively, our approach may need to be 
extended to include a larger context window. 
7.2 Learned Patterns  
One of the ways in which learned patterns supple-
ment handwritten ones is learning highly specific 
surface-string patterns that are insensitive to errors 
in parsing. Figure 3 illustrates two examples of 
what appear to be easy cases of possibleTreat-
ment(). Because the handwritten patterns are not 
exhaustive and make extensive use of syntactic 
structure, parse errors prevented the system based 
on handwritten rules from firing. Learned surface-
string patterns were able to find these relations.  
Even when the syntactic structure is correct, 
learned patterns capture expressions not common 
enough to have been noticed by the rule writer. For 
example, while the handwritten patterns included 
?withdrew? as a predicate indicating a company 
was responsible for a drug, they did not include 
?pulled.? By including ?pulled?, learned patterns 
extracted responsibleForTreatment() from ?Ameri-
can Home Products pulled Duract, a painkiller.? 
Similarly, the learned patterns include an explicit 
pattern ?CONDITION drug called SUBSTANCE?, 
and thus extracted a possibleTreatment() relation 
from ?newly approved narcolepsy drug called 
modafinil? without relying on the coreference 
component to link drug to modafinil.  
Handwritten Patterns 
Despite the examples above of successfully learned 
patterns, handwritten patterns perform significantly 
better. In the active-learning context used for these 
experiments, the handwritten rules also required 
less manual effort. This comparison is not entirely 
fair-- while learned patterns required more hours, 
supervising the bootstrapping algorithm requires 
no training. The handwritten patterns, in contrast, 
require a trained expert.  
 
Figure 3: Extractions Missed by Handwritten Rules & 
the Erroneous Parses that Hid them 
While handwritten rules and learned patterns use 
the same language, they make use of it differently. 
The handwritten patterns group similar concepts 
together. A human pattern writer adds relevant 
synonyms, as well as words that are not synonym-
ous but in the pattern context can be used inter-
changeably. In Figure 4, the handwritten patterns 
include three word-sets: (patient*, people, partici-
pant*); (given, taken, took, using); and (report*, 
experience*, develop*, suffer*). The ?*? serves as a 
wild-card to further generalize a pattern. The word-
sets in Figure 4 illustrate challenges for a learned 
system: the words are not synonyms, but rather are 
words that can be used to imply the relation.  
A human pattern writer frequently generates 
new classes not in the domain ontology. In Figure 
4, the circled patterns form a class of ?people tak-
ing a substance.? The handwritten patterns for stu-
diesDisease() include classes targeting scientists 
and researchers. These classes are not necessarily 
triggered by nouns. Such classes allow the pattern 
writer to include complex patterns as in Figure 4 
and to write relatively precise, but open-ended pat-
terns such as: if there is a single named-drug and a 
named, non-side-effect disease in the same sen-
tence, the drug is a treatment for the disease.  
Pfizer also hopes to introduce Pregabalin next 
year for treatment of neuropathic pain, epilepsy 
and anxiety?Other deals include co-promoting 
Rebif for multiple sclerosis with its discoverer, 
Serono, and marketing Aricept for Alzheimer's 
disease with its developer, Eisai Co. 
1444
 
Figure 4: Learned and Handwritten Patterns for  
hasSideEffect() 
A final difference between handwritten and 
learned patterns is the level of predicate-argument 
complexity used. In general, handwritten patterns 
account for larger spans of predicate argument 
structure while learned patterns tend to limit them-
selves to the connections between the arguments of 
the relation with minor extensions.  
8 Conclusions and Lessons Learned 
First, it is encouraging that the synthesis of learn-
ing algorithms and handwritten algorithms can 
achieve an F1 of 0.51 in a new domain in a week 
(<50 hours of effort). Second, it is exciting that so 
little training data is required: ~20 relation pairs 
out of context (~2.5 hours of effort) and ~6 hours 
of active learning for the new classes.  
Third, the effectiveness of learning algorithms is 
still not competitive with handwritten patterns 
based on predicate-argument structure (~5 hours of 
effort on top of active learning for entities). 
Though the learned patterns have high precision 
(0.80 on average), recall is low (0.17) and varied 
greatly across the relations. Though the dominant 
factor in missing relations is pattern coverage, 
missing instances of classes contributed to low re-
call. Comparing learned patterns to manually writ-
ten patterns, (1) synonyms or other lexical 
alternatives that a human pattern writer would in-
clude, (2) the creation of subclasses for argument 
types, and (3) the scope of patterns11 are each ma-
jor sources of the disparity in coverage. Research 
on learning approaches to raise recall without sig-
nificant sacrifice in precision seems essential.  
Fourth, despite the disparity in performance of 
learned versus manual patterns, and despite the low 
                                                          
11 Learned patterns tend to focus on the structure that appears 
between the two arguments, rather than structure surrounding 
the left and right arguments. 
recall of learned patterns, the combined system?s 
recall and F-Score are higher for three of the rela-
tions because the learned patterns generated an-
swers not found by handwritten patterns. We found 
examples where highly specific, learned, surface-
level patterns (lexical patterns) occasionally found 
information missed by handwritten patterns due to 
parsing errors or general low coverage. 
Fifth, the effort for coreference was the most 
time-consuming, given that every new relation 
contained at least one of the new argument types. 
While we included this in our estimate of domain 
adaptation, the infrastructure we built is domain 
generic. Improving generic coreference will reduce 
domain specific effort in future.  
Perhaps most significant of all, running a com-
plete experiment from definition of the domain 
through creation of training data and measurement 
of end-to-end performance of the system can be 
completed in a month. The ability to rapidly, 
cheaply, and empirically measure the impact of 
extraction research could prove a significant spur 
to research across the board. 
These experiments suggest three possible direc-
tions for improving the ability to quickly develop 
information extraction technology for a new set of 
relations: (1) reducing the amount of supervision 
provided to the bootstrap-learner; (2) improving 
the bootstrapping approach to reach the level of 
recall achieved by the human pattern writer elimi-
nating the need for a trained expert during domain 
adaptation; and (3) focusing improvements to the 
bootstrapping approach on techniques that allow it 
to find more of the instances missed by the pattern 
writer, thus improving the accuracy of the hybrid 
system.   
Acknowledgments 
This work was supported, in part, by DARPA un-
der AFRL Contract FA8750-09-C-179. Distribu-
tion Statement ?A? (Approved for Public Release, 
Distribution Unlimited) Thank you to the review-
ers for your insightful comments and to Michelle 
Franchini for coordinating the assessment effort. 
References 
E. Agichtein and L. Gravano. Snowball: extracting rela-
tions from large plain-text collections. In Proceed-
ings of the ACM Conference on Digital Libraries, pp. 
85-94, 2000.  
1445
A. Blum and T. Mitchell. Combining Labeled and Un-
labeled Data with Co-Training. In Proceedings of the 
1998 Conference on Computational Learning 
Theory, July 1998.  
E. Boschee, V. Punyakanok, R. Weischedel. An Explo-
ratory Study Towards ?Machines that Learn to Read?. 
Proceedings of AAAI BICA Fall Symposium, No-
vember 2008. 
J. Chen, D. Ji, C. Tan and Z. Niu. (2006). Relation ex-
traction using label propagation based semi-
supervised learning. COLING-ACL 2006: 129-136. 
July 2006. 
M. Freedman, E. Loper, E. Boschee, and R. Weischedel. 
Empirical Studies in Learning to Read. Proceedings 
of NAACL 2010 Workshop on Formalisms and Me-
thodology for Learning by Reading, pp. 61-69, June 
2010. 
W. Li and A. McCallum.  Rapid development of Hindi 
named entity recognition using conditional random 
fields and feature induction. Transactions on Asian 
Language Information Processing (TALIP), Volume 
2 Issue 3  September, 2003. 
R Grishman and B. Sundheim. Message Understanding 
Conference-6 : A Brief History", in COLING-96, 
Proc . of the Int'l Conj. on Computational Linguis-
tics, 1996.  
Z. Kozareva and E. Hovy. Not All Seeds Are Equal: 
Measuring the Quality of Text Mining Seeds. Human 
Language Technologies: The 2010 Annual Confe-
rence of the North American Chapter of the Associa-
tion for Computational Linguistics, June, 2010, pp. 
618-626. 
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic 
class learning from the web with hyponym pattern 
linkage graphs. In Proceedings of ACL-08: HLT, 
pages 1048?1056.  
J. May, A. Brunstein, P. Natarajan,  and R. Weischedel.  
Surprise! What's in a Cebuano or Hindi Name? 
Transactions on Asian Language Information 
Processing (TALIP), Volume 2 Issue 3  September, 
2003. 
D. Maynard, V. Tablan, K. Bontcheva, and H. Cun-
ningham. Rapid customization of an information ex-
traction system for a surprise language. Transactions 
on Asian Language Information Processing (TALIP), 
Volume 2 Issue 3  September, 2003. 
S. Miller, J. Guinness, and A. Zamanian, ?Name Tag-
ging with Word Cluster and Discriminative Train-
ing?, Proceedings of HLT/NAACL 2004, pp. 337-
342, 2004 
T. Mitchell, J. Betteridge, A. Carlson, E. Hruschka, and 
R. Wang. ?Populating the Semantic Web by Macro-
Reading Internet Text. Invited paper, Proceedings of 
the 8th International Semantic Web Conference 
(ISWC 2009).  
NIST, ACE 2007: 
http://www.itl.nist.gov/iad/mig/tests/ace/2007/softwa
re.html 
P. Pantel and M. Pennacchiotti. Espresso: Leveraging 
Generic Patterns for Automatically Harvesting Se-
mantic Relations. In Proceedings of Conference on 
Computational Linguistics / Association for Compu-
tational Linguistics (COLING/ACL-06). pp. 113-120. 
Sydney, Australia, 2006.  
L. Ramshaw , E. Boschee, S. Bratus, S. Miller, R. 
Stone, R. Weischedel, A. Zamanian, ?Experiments in 
multi-modal automatic content extraction?, Proceed-
ings of Human Technology Conference, March 2001.  
D. Ravichandran and E. Hovy. Learning surface text 
patterns for a question answering system. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2002), 
pages 41?47, Philadelphia, PA, 2002.  
E. Riloff. Automatically generating extraction patterns 
from untagged text. In Proceedings of the Thirteenth 
National Conference on Artificial Intelligence, pages 
1044-1049, 1996.  
S. Sekine and R. Grishman.  Hindi-English cross-lingual 
question-answering  system. Transactions on Asian 
Language Information Processing (TALIP), Volume 
2 Issue 3  September, 2003. 
G. Zhou, J. Li, L. Qian, Q. Zhu. Semi-Supervised 
Learning for Relation Extraction. Proceedings of the 
Third International Joint Conference on Natural 
Language Processing: Volume-I. 2008. 
 
1446
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 1?27,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
CoNLL-2011 Shared Task:
Modeling Unrestricted Coreference in OntoNotes
Sameer Pradhan
BBN Technologies,
Cambridge, MA 02138
pradhan@bbn.com
Lance Ramshaw
BBN Technologies,
Cambridge, MA 02138
lramshaw@bbn.com
Mitchell Marcus
University of Pennsylvania,
Philadelphia, 19104
mitch@linc.cis.upenn.edu
Martha Palmer
University of Colorado,
Boulder, CO 80309
martha.palmer@colorado.edu
Ralph Weischedel
BBN Technologies,
Cambridge, MA 02138
weischedel@bbn.com
Nianwen Xue
Brandeis University,
Waltham, MA 02453
xuen@cs.brandeis.edu
Abstract
The CoNLL-2011 shared task involved pre-
dicting coreference using OntoNotes data. Re-
sources in this field have tended to be lim-
ited to noun phrase coreference, often on a
restricted set of entities, such as ACE enti-
ties. OntoNotes provides a large-scale corpus
of general anaphoric coreference not restricted
to noun phrases or to a specified set of en-
tity types. OntoNotes also provides additional
layers of integrated annotation, capturing ad-
ditional shallow semantic structure. This pa-
per briefly describes the OntoNotes annota-
tion (coreference and other layers) and then
describes the parameters of the shared task
including the format, pre-processing informa-
tion, and evaluation criteria, and presents and
discusses the results achieved by the partic-
ipating systems. Having a standard test set
and evaluation parameters, all based on a new
resource that provides multiple integrated an-
notation layers (parses, semantic roles, word
senses, named entities and coreference) that
could support joint models, should help to en-
ergize ongoing research in the task of entity
and event coreference.
1 Introduction
The importance of coreference resolution for the
entity/event detection task, namely identifying all
mentions of entities and events in text and clustering
them into equivalence classes, has been well recog-
nized in the natural language processing community.
Automatic identification of coreferring entities and
events in text has been an uphill battle for several
decades, partly because it can require world knowl-
edge which is not well-defined and partly owing to
the lack of substantial annotated data. Early work
on corpus-based coreference resolution dates back
to the mid-90s by McCarthy and Lenhert (1995)
where they experimented with using decision trees
and hand-written rules. A systematic study was
then conducted using decision trees by Soon et al
(2001). Significant improvements have been made
in the field of language processing in general, and
improved learning techniques have been developed
to push the state of the art in coreference resolu-
tion forward (Morton, 2000; Harabagiu et al, 2001;
McCallum and Wellner, 2004; Culotta et al, 2007;
Denis and Baldridge, 2007; Rahman and Ng, 2009;
Haghighi and Klein, 2010). Various different knowl-
edge sources from shallow semantics to encyclo-
pedic knowledge are being exploited (Ponzetto and
Strube, 2005; Ponzetto and Strube, 2006; Versley,
2007; Ng, 2007). Researchers continued finding
novel ways of exploiting ontologies such as Word-
Net. Given that WordNet is a static ontology and
as such has limitation on coverage, more recently,
there have been successful attempts to utilize in-
formation from much larger, collaboratively built
resources such as Wikipedia (Ponzetto and Strube,
2006). In spite of all the progress, current techniques
still rely primarily on surface level features such as
string match, proximity, and edit distance; syntac-
tic features such as apposition; and shallow seman-
tic features such as number, gender, named entities,
semantic class, Hobbs? distance, etc. A better idea
of the progress in the field can be obtained by read-
ing recent survey articles (Ng, 2010) and tutorials
(Ponzetto and Poesio, 2009) dedicated to this sub-
ject.
Corpora to support supervised learning of this
task date back to the Message Understanding Con-
ferences (MUC). These corpora were tagged with
coreferring entities identified by noun phrases in the
text. The de facto standard datasets for current coref-
erence studies are the MUC (Hirschman and Chin-
1
chor, 1997; Chinchor, 2001; Chinchor and Sund-
heim, 2003) and the ACE1 (G. Doddington et al,
2000) corpora. The MUC corpora cover all noun
phrases in text, but represent small training and test
sets. The ACE corpora, on the other hand, have much
more annotation, but are restricted to a small subset
of entities. They are also less consistent, in terms of
inter-annotator agreement (ITA) (Hirschman et al,
1998). This lessens the reliability of statistical ev-
idence in the form of lexical coverage and seman-
tic relatedness that could be derived from the data
and used by a classifier to generate better predic-
tive models. The importance of a well-defined tag-
ging scheme and consistent ITA has been well rec-
ognized and studied in the past (Poesio, 2004; Poe-
sio and Artstein, 2005; Passonneau, 2004). There
is a growing consensus that in order for these to be
most useful for language understanding applications
such as question answering or distillation ? both of
which seek to take information access technology
to the next level ? we need more consistent anno-
tation of larger amounts of broad coverage data for
training better automatic techniques for entity and
event identification. Identification and encoding of
richer knowledge ? possibly linked to knowledge
sources ? and development of learning algorithms
that would effectively incorporate them is a neces-
sary next step towards improving the current state
of the art. The computational learning community,
in general, is also witnessing a move towards eval-
uations based on joint inference, with the two pre-
vious CoNLL tasks (Surdeanu et al, 2008; Hajic? et
al., 2009) devoted to joint learning of syntactic and
semantic dependencies. A principle ingredient for
joint learning is the presence of multiple layers of
semantic information.
One fundamental question still remains, and that
is ? what would it take to improve the state of the art
in coreference resolution that has not been attempted
so far? Many different algorithms have been tried in
the past 15 years, but one thing that is still lacking
is a corpus comprehensively tagged on a large scale
with consistent, multiple layers of semantic infor-
mation. One of the many goals of the OntoNotes
project2 (Hovy et al, 2006; Weischedel et al, 2011)
is to explore whether it can fill this void and help
push the progress further ? not only in coreference,
but with the various layers of semantics that it tries
to capture. As one of its layers, it has created a
corpus for general anaphoric coreference that cov-
1http://projects.ldc.upenn.edu/ace/data/
2http://www.bbn.com/nlp/ontonotes
ers entities and events not limited to noun phrases
or a limited set of entity types. A small portion of
this corpus from the newswire and broadcast news
genres (?120k) was recently used for a SEMEVAL
task (Recasens et al, 2010). As mentioned earlier,
the coreference layer in OntoNotes constitutes just
one part of a multi-layered, integrated annotation of
shallow semantic structure in text with high inter-
annotator agreement, which also provides a unique
opportunity for performing joint inference over a
substantial body of data.
The remainder of this paper is organized as
follows. Section 2 presents an overview of the
OntoNotes corpus. Section 3 describes the coref-
erence annotation in OntoNotes. Section 4 then de-
scribes the shared task, including the data provided
and the evaluation criteria. Sections 5 and 6 then de-
scribe the participating system results and analyze
the approaches, and Section 7 concludes.
2 The OntoNotes Corpus
The OntoNotes project has created a corpus of large-
scale, accurate, and integrated annotation of multi-
ple levels of the shallow semantic structure in text.
The idea is that this rich, integrated annotation cov-
ering many layers will allow for richer, cross-layer
models enabling significantly better automatic se-
mantic analysis. In addition to coreference, this
data is also tagged with syntactic trees, high cov-
erage verb and some noun propositions, partial verb
and noun word senses, and 18 named entity types.
However, such multi-layer annotations, with com-
plex, cross-layer dependencies, demands a robust,
efficient, scalable mechanism for storing them while
providing efficient, convenient, integrated access to
the the underlying structure. To this effect, it uses a
relational database representation that captures both
the inter- and intra-layer dependencies and also pro-
vides an object-oriented API for efficient, multi-
tiered access to this data (Pradhan et al, 2007a).
This should facilitate the creation of cross-layer fea-
tures in integrated predictive models that will make
use of these annotations.
Although OntoNotes is a multi-lingual resource
with all layers of annotation covering three lan-
guages: English, Chinese and Arabic, for the scope
of this paper, we will just look at the English por-
tion. Over the years of the development of this cor-
pus, there were various priorities that came into play,
and therefore not all the data in the English portion is
annotated with all the different layers of annotation.
There is a core portion, however, which is roughly
2
1.3M words which has been annotated with all the
layers. It comprises ?450k words from newswire,
?150k from magazine articles, ?200k from broad-
cast news, ?200k from broadcast conversations and
?200k web data.
OntoNotes comprises the following layers of an-
notation:
? Syntax ? A syntactic layer representing a re-
vised Penn Treebank (Marcus et al, 1993;
Babko-Malaya et al, 2006).
? Propositions ? The proposition structure of
verbs in the form of a revised PropBank(Palmer
et al, 2005; Babko-Malaya et al, 2006).
? Word Sense ? Coarse grained word senses
are tagged for the most frequent polysemous
verbs and nouns, in order to maximize cov-
erage. The word sense granularity is tailored
to achieve 90% inter-annotator agreement as
demonstrated by Palmer et al (2007). These
senses are defined in the sense inventory files
and each individual sense has been connected
to multiple WordNet senses. This provides a
direct access to the WordNet semantic struc-
ture for users to make use of. There is also a
mapping from the word senses to the PropBank
frames and to VerbNet (Kipper et al, 2000) and
FrameNet (Fillmore et al, 2003).
? Named Entities ? The corpus was tagged with
a set of 18 proper named entity types that
were well-defined and well-tested for inter-
annotator agreement by Weischedel and Burn-
stein (2005).
? Coreference ? This layer captures general
anaphoric coreference that covers entities and
events not limited to noun phrases or a limited
set of entity types (Pradhan et al, 2007b). We
will take a look at this in detail in the next sec-
tion.
3 Coreference in OntoNotes
General anaphoric coreference that spans a rich set
of entities and events ? not restricted to a few types,
as has been characteristic of most coreference data
available until now ? has been tagged with a high
degree of consistency. Attributive coreference is
tagged separately from the more common identity
coreference.
Two different types of coreference are distin-
guished in the OntoNotes data: Identical (IDENT),
and Appositive (APPOS). Appositives are treated
separately because they function as attributions, as
described further below. The IDENT type is used
for anaphoric coreference, meaning links between
pronominal, nominal, and named mentions of spe-
cific referents. It does not include mentions of
generic, underspecified, or abstract entities.
Coreference is annotated for all specific entities
and events. There is no limit on the semantic types
of NP entities that can be considered for coreference,
and in particular, coreference is not limited to ACE
types.
The mentions over which IDENT coreference ap-
plies are typically pronominal, named, or definite
nominal. The annotation process begins by auto-
matically extracting all of the NP mentions from the
Penn Treebank, though the annotators can also add
additional mentions when appropriate. In the fol-
lowing two examples (and later ones), the phrases
notated in bold form the links of an IDENT chain.
(1) She had a good suggestion and it was unani-
mously accepted by all.
(2) Elco Industries Inc. said it expects net income
in the year ending June 30, 1990, to fall below a
recent analyst?s estimate of $ 1.65 a share. The
Rockford, Ill. maker of fasteners also said it
expects to post sales in the current fiscal year
that are ?slightly above? fiscal 1989 sales of $
155 million.
3.1 Verbs
Verbs are added as single-word spans if they can
be coreferenced with a noun phrase or with an-
other verb. The intent is to annotate the VP, but we
mark the single-word head for convenience. This in-
cludes morphologically related nominalizations (3)
and noun phrases that refer to the same event, even
if they are lexically distinct from the verb (4). In the
following two examples, only the chains related to
the growth event are shown.
(3) Sales of passenger cars grew 22%. The strong
growth followed year-to-year increases.
(4) Japan?s domestic sales of cars, trucks and buses
in October rose 18% from a year earlier to
500,004 units, a record for the month, the Japan
Automobile Dealers? Association said. The
strong growth followed year-to-year increases
of 21% in August and 12% in September.
3
3.2 Pronouns
All pronouns and demonstratives are linked to any-
thing that they refer to, and pronouns in quoted
speech are also marked. Expletive or pleonastic pro-
nouns (it, there) are not considered for tagging, and
generic you is not marked. In the following exam-
ple, the pronoun you and it would not be marked. (In
this and following examples, an asterisk (*) before a
boldface phrase identifies entity/event mentions that
would not be tagged as coreferent.)
(5) Senate majority leader Bill Frist likes to tell a
story from his days as a pioneering heart sur-
geon back in Tennessee. A lot of times, Frist re-
calls, *you?d have a critical patient lying there
waiting for a new heart, and *you?d want to
cut, but *you couldn?t start unless *you knew
that the replacement heart would make *it to
the operating room.
3.3 Generic mentions
Generic nominal mentions can be linked with refer-
ring pronouns and other definite mentions, but are
not linked to other generic nominal mentions. This
would allow linking of the bracketed mentions in (6)
and (7), but not (8).
(6) Officials said they are tired of making the same
statements.
(7) Meetings are most productive when they are
held in the morning. Those meetings, however,
generally have the worst attendance.
(8) Allergan Inc. said it received approval to
sell the PhacoFlex intraocular lens, the first
foldable silicone lens available for *cataract
surgery. The lens? foldability enables it to be
inserted in smaller incisions than are now pos-
sible for *cataract surgery.
Bare plurals, as in (6) and (7), are always consid-
ered generic. In example (9) below, there are two
generic instances of parents. These are marked as
distinct IDENT chains (with separate chains distin-
guished by subscripts X, Y and Z), each containing
a generic and the related referring pronouns.
(9) ParentsX should be involved with theirX chil-
dren?s education at home, not in school. TheyX
should see to it that theirX kids don?t play tru-
ant; theyX should make certain that the children
spend enough time doing homework; theyX
should scrutinize the report card. ParentsY are
too likely to blame schools for the educational
limitations of theirY children. If parentsZ are
dissatisfied with a school, theyZ should have
the option of switching to another.
In (10) below, the verb ?halve? cannot be linked
to ?a reduction of 50%?, since ?a reduction? is in-
definite.
(10) Argentina said it will ask creditor banks to
*halve its foreign debt of $64 billion ? the
third-highest in the developing world . Ar-
gentina aspires to reach *a reduction of 50%
in the value of its external debt.
3.4 Pre-modifiers
Proper pre-modifiers can be coreferenced, but
proper nouns that are in a morphologically adjecti-
val form are treated as adjectives, and not corefer-
enced. For example, adjectival forms of GPEs such
as Chinese in ?the Chinese leader?, would not be
linked. Thus we could coreference United States in
?the United States policy? with another referent, but
not American ?the American policy.? GPEs and Na-
tionality acronyms (e.g. U.S.S.R. or U.S.). are also
considered adjectival. Pre-modifier acronyms can be
coreferenced unless they refer to a nationality. Thus
in the examples below, FBI can be coreferenced to
other mentions, but U.S. cannot.
(11) FBI spokesman
(12) *U.S. spokesman
Dates and monetary amounts can be considered
part of a coreference chain even when they occur as
pre-modifiers.
(13) The current account deficit on France?s balance
of payments narrowed to 1.48 billion French
francs ($236.8 million) in August from a re-
vised 2.1 billion francs in July, the Finance
Ministry said. Previously, the July figure was
estimated at a deficit of 613 million francs.
(14) The company?s $150 offer was unexpected.
The firm balked at the price.
3.5 Copular verbs
Attributes signaled by copular structures are not
marked; these are attributes of the referent they mod-
ify, and their relationship to that referent will be
captured through word sense and propositional ar-
gument tagging.
4
(15) JohnX is a linguist. PeopleY are nervous
around JohnX, because heX always corrects
theirY grammar.
Copular (or ?linking?) verbs are those verbs that
function as a copula and are followed by a sub-
ject complement. Some common copular verbs are:
be, appear, feel, look, seem, remain, stay, become,
end up, get. Subject complements following such
verbs are considered attributes, and not linked. Since
Called is copular, neither IDENT nor APPOS corefer-
ence is marked in the following case.
(16) Called Otto?s Original Oat Bran Beer, the brew
costs about $12.75 a case.
3.6 Small clauses
Like copulas, small clause constructions are not
marked. The following example is treated as if the
copula were present (?John considers Fred to be an
idiot?):
(17) John considers *Fred *an idiot.
3.7 Temporal expressions
Temporal expressions such as the following are
linked:
(18) John spent three years in jail. In that time...
Deictic expressions such as now, then, today, to-
morrow, yesterday, etc. can be linked, as well as
other temporal expressions that are relative to the
time of the writing of the article, and which may
therefore require knowledge of the time of the writ-
ing to resolve the coreference. Annotators were al-
lowed to use knowledge from outside the text in re-
solving these cases. In the following example, the
end of this period and that time can be coreferenced,
as can this period and from three years to seven
years.
(19) The limit could range from three years to
seven yearsX, depending on the composition
of the management team and the nature of its
strategic plan. At (the end of (this period)X)Y,
the poison pill would be eliminated automati-
cally, unless a new poison pill were approved
by the then-current shareholders, who would
have an opportunity to evaluate the corpora-
tion?s strategy and management team at that
timeY.
In multi-date temporal expressions, embedded
dates are not separately connected to to other men-
tions of that date. For example in Nov. 2, 1999, Nov.
would not be linked to another instance of November
later in the text.
3.8 Appositives
Because they logically represent attributions, appos-
itives are tagged separately from Identity corefer-
ence. They consist of a head, or referent (a noun
phrase that points to a specific object/concept in the
world), and one or more attributes of that referent.
An appositive construction contains a noun phrase
that modifies an immediately-adjacent noun phrase
(separated only by a comma, colon, dash, or paren-
thesis). It often serves to rename or further define
the first mention. Marking appositive constructions
allows us to capture the attributed property even
though there is no explicit copula.
(20) Johnhead, a linguistattribute
The head of each appositive construction is distin-
guished from the attribute according to the following
heuristic specificity scale, in a decreasing order from
top to bottom:
Type Example
Proper noun John
Pronoun He
Definite NP the man
Indefinite specific NP a man I know
Non-specific NP man
This leads to the following cases:
(21) Johnhead, a linguistattribute
(22) A famous linguistattribute, hehead studied at ...
(23) a principal of the firmattribute, J. Smithhead
In cases where the two members of the appositive
are equivalent in specificity, the left-most member of
the appositive is marked as the head/referent. Defi-
nite NPs include NPs with a definite marker (the) as
well as NPs with a possessive adjective (his). Thus
the first element is the head in all of the following
cases:
(24) The chairman, the man who never gives up
(25) The sheriff, his friend
(26) His friend, the sheriff
In the specificity scale, specific names of diseases
and technologies are classified as proper names,
whether they are capitalized or not.
(27) A dangerous bacteria, bacillium, is found
5
Type Description
Annotator Error An annotator error. This is a catch-all category for cases of errors that do not fit in the other
categories.
Genuine Ambiguity This is just genuinely ambiguous. Often the case with pronouns that have no clear an-
tecedent (especially this & that)
Generics One person thought this was a generic mention, and the other person didn?t
Guidelines The guidelines need to be clear about this example
Callisto Layout Something to do with the usage/design of Callisto
Referents Each annotator thought this was referring to two completely different things
Possessives One person did not mark this possessive
Verb One person did not mark this verb
Pre Modifiers One person did not mark this Pre Modifier
Appositive One person did not mark this appositive
Extent Both people marked the same entity, but one person?s mention was longer
Copula Disagreement arose because this mention is part of a copular structure
a) Either each annotator marked a different half of the copula
b) Or one annotator unnecessarily marked both
Figure 1: Description of various disagreement types
Figure 1: The distribution of disagreements across the various types in Table 2
Sheet1
Page 1
Copulae 2%Appositives 3%Pre Modifiers 3%Verbs 3%Possessives 4%Refer nts 7%Callisto Layout 8%Guidelines 8%Generics 11%Genuine Ambiguity 25%Annotator Error 26%
Copulae
Appositives
Pre Modifiers
Verbs
Possessives
Referents
Callisto Layout
Guidelines
Generics
Genuine Ambiguity
Annotator Error
0% 5% 10% 15% 20% 25% 30%
Figure 2: The distribution of disagreements across the various types in Table 1
When the entity to which an appositive refers is
also mentioned elsewhere, only the single span con-
taining the entire appositive construction is included
in the larger IDENT chain. None of the nested NP
spans are linked. In the example below, the en-
tire span can be linked to later mentions to Richard
Godown. The sub-spans are not included separately
in the IDENT chain.
(28) Richard Godown, president of the Indus-
trial Biotechnology Association
Ages are tagged as attributes (as if they were el-
lipses of, for example, a 42-year-old):
(29) Mr.Smithhead, 42attribute,
3.9 Special Issues
In addition to the ones above, there are some special
cases such as:
? No coreference is marked between an organi-
zation and its members.
Genre ANN1-ANN2 ANN1-ADJ ANN2-ADJ
Newswire 80.9 85.2 88.3
Broadcast News 78.6 83.5 89.4
Broadcast Conversation 86.7 91.6 93.7
Magazine 78.4 83.2 88.8
Web 85.9 92.2 91.2
Table 1: Inter Annotator and Adjudicator agreement for
the Coreference Layer in OntoNotes measured in terms
of the MUC score.
? GPEs are linked to references to their govern-
ments, even when the references are nested
NPs, or the modifier and head of a single NP.
3.10 Annotator Agreement and Analysis
Table 1 shows the inter-annotator and annotator-
adjudicator agreement on all the genres of
OntoNotes. We also analyzed about 15K dis-
agreements in various parts of the data, and grouped
them into one of the categories shown in Figure 1.
Figure 2 shows the distribution of these different
types that were found in that sample. It can be
6
seen that genuine ambiguity and annotator error
are the biggest contributors ? the latter of which is
usually captured during adjudication, thus showing
the increased agreement between the adjudicated
version and the individual annotator version.
4 CoNLL-2011 Coreference Task
This section describes the CoNLL-2011 Corefer-
ence task, including its closed and open track ver-
sions, and characterizes the data used for the task
and how it was prepared.
4.1 Why a Coreference Task?
Despite close to a two-decade history of evaluations
on coreference tasks, variation in the evaluation cri-
teria and in the training data used have made it dif-
ficult for researchers to be clear about the state of
the art or to determine which particular areas require
further attention. There are many different parame-
ters involved in defining a coreference task. Looking
at various numbers reported in literature can greatly
affect the perceived difficulty of the task. It can seem
to be a very hard problem (Soon et al, 2001) or one
that is somewhat easier (Culotta et al, 2007). Given
the space constraints, we refer the reader to Stoy-
anov et al (2009) for a detailed treatment of the
issue.
Limitations in the size and scope of the available
datasets have also constrained research progress.
The MUC and ACE corpora are the two that have
been used most for reporting comparative results,
but they differ in the types of entities and corefer-
ence annotated. The ACE corpus is also one that
evolved over a period of almost five years, with dif-
ferent incarnations of the task definition and dif-
ferent corpus cross-sections on which performance
numbers have been reported, making it hard to un-
tangle and interpret the results.
The availability of the OntoNotes data offered an
opportunity to define a coreference task based on a
larger, more broad-coverage corpus. We have tried
to design the task so that it not only can support the
current evaluation, but also can provide an ongoing
resource for comparing different coreference algo-
rithms and approaches.
4.2 Task Description
The CoNLL-2011 shared task was based on the En-
glish portion of the OntoNotes 4.0 data. The task
was to automatically identify mentions of entities
and events in text and to link the coreferring men-
tions together to form entity/event chains. The target
coreference decisions could be made using automat-
ically predicted information on the other structural
layers including the parses, semantic roles, word
senses, and named entities.
As is customary for CoNLL tasks, there were two
tracks, closed and open. For the closed track, sys-
tems were limited to using the distributed resources,
in order to allow a fair comparison of algorithm per-
formance, while the open track allowed for almost
unrestricted use of external resources in addition to
the provided data.
4.2.1 Closed Track
In the closed track, systems were limited to the pro-
vided data, plus the use of two pre-specified external
resources: i) WordNet and ii) a pre-computed num-
ber and gender table by Bergsma and Lin (2006).
For the training and test data, in addition to the
underlying text, predicted versions of all the supple-
mentary layers of annotation were provided, where
those predictions were derived using off-the-shelf
tools (parsers, semantic role labelers, named entity
taggers, etc.) as described in Section 4.4.2. For the
training data, however, in addition to predicted val-
ues for the other layers, we also provided manual
gold-standard annotations for all the layers. Partici-
pants were allowed to use either the gold-standard or
predicted annotation for training their systems. They
were also free to use the gold-standard data to train
their own models for the various layers of annota-
tion, if they judged that those would either provide
more accurate predictions or alternative predictions
for use as multiple views, or wished to use a lattice
of predictions.
More so than previous CoNLL tasks, corefer-
ence predictions depend on world knowledge, and
many state-of-the-art systems use information from
external resources such as WordNet, which can
add a layer that helps the system to recognize se-
mantic connections between the various lexical-
ized mentions in the text. Therefore, the use of
WordNet was allowed, even for the closed track.
Since word senses in OntoNotes are predominantly3
coarse-grained groupings of WordNet senses, sys-
tems could also map from the predicted or gold-
standard word senses provided to the sets of under-
lying WordNet senses. Another significant piece of
knowledge that is particularly useful for coreference
but that is not available in the layers of OntoNotes is
that of number and gender. There are many different
3There are a few instances of novel senses introduced in
OntoNotes which were not present in WordNet, and so lack a
mapping back to the WordNet senses
7
ways of predicting these values, with differing accu-
racies, so in order to ensure that participants in the
closed track were working from the same data, thus
allowing clearer algorithmic comparisons, we spec-
ified a particular table of number and gender predic-
tions generated by Bergsma and Lin (2006), for use
during both training and testing.
Following the recent CoNLL tradition, partici-
pants were allowed to use both the training and the
development data for training the final model.
4.2.2 Open Track
In addition to resources available in the closed track,
the open track, systems were allowed to use external
resources such as Wikipedia, gazetteers etc. This
track is mainly to get an idea of a performance ceil-
ing on the task at the cost of not getting a compar-
ison across all systems. Another advantage of the
open track is that it might reduce the barriers to par-
ticipation by allowing participants to field existing
research systems that already depend on external re-
sources ? especially if there were hard dependen-
cies on these resources. They can participate in the
task with minimal or no modification to their exist-
ing system.
4.3 Coreference Task Data
Since there are no previously reported numbers on
the full version of OntoNotes, we had to create
a train/development/test partition. The only por-
tion of OntoNotes that has a previously determined,
widely used, standard split is the WSJ portion of the
newswire data. For that subcorpus, we maintained
the same partition. For all the other portions we cre-
ated stratified training, development and test parti-
tions over all the sources in OntoNotes using the pro-
cedure shown in Algorithm 1. The list of training,
development and test document IDs can be found on
the task webpage.4
4.4 Data Preparation
This section gives details of the different annota-
tion layers including the automatic models that were
used to predict them, and describes the formats in
which the data were provided to the participants.
4.4.1 Manual Annotation Gold Layers
We will take a look at the manually annotated, or
gold layers of information that were made available
for the training data.
4http://conll.bbn.com/download/conll-train.id
http://conll.bbn.com/download/conll-dev.id
http://conll.bbn.com/download/conll-test.id
Algorithm 1 Procedure used to create OntoNotes
training, development and test partitions.
Procedure: GENERATE PARTITIONS(ONTONOTES) returns TRAIN,
DEV, TEST
1: TRAIN? ?
2: DEV? ?
3: TEST? ?
4: for all SOURCE ? ONTONOTES do
5: if SOURCE = WALL STREET JOURNAL then
6: TRAIN? TRAIN ? SECTIONS 02 ? 21
7: DEV? DEV ? SECTIONS 00, 01, 22, 24
8: TEST? TEST ? SECTION 23
9: else
10: if Number of files in SOURCE ? 10 then
11: TRAIN? TRAIN ? FILE IDS ending in 1 ? 8
12: DEV? DEV ? FILE IDS ending in 0
13: TEST? TEST ? FILE IDS ending in 9
14: else
15: DEV? DEV ? FILE IDS ending in 0
16: TEST? TEST ? FILE ID ending in the highest number
17: TRAIN? TRAIN ? Remaining FILE IDS for the
SOURCE
18: end if
19: end if
20: end for
21: return TRAIN, DEV, TEST
Coreference The manual coreference annotation
is stored as chains of linked mentions connecting
multiple mentions of the same entity. Coreference is
the only document-level phenomenon in OntoNotes,
and the complexity of annotation increases non-
linearly with the length of a document. Unfortu-
nately, some of the documents ? especially ones in
the broadcast conversation, weblogs, and telephone
conversation genre ? are very long which prohib-
ited us from efficiently annotating them in entirety.
These had to be split into smaller parts. We con-
ducted a few passes to join some adjacent parts, but
since some documents had as many as 17 parts, there
are still multi-part documents in the corpus. Since
the coreference chains are coherent only within each
of these document parts, for this task, each such part
is treated as a separate document. Another thing
to note is that there were some cases of sub-token
annotation in the corpus owing to the fact that to-
kens were not split at hyphens. Cases such as pro-
WalMart had the sub-span WalMart linked with another
instance of the same. The recent Treebank revision
which split tokens at most hyphens, made a majority
of these sub-token annotations go away. There were
still some residual sub-token annotations. Since
subtoken annotations cannot be represented in the
CoNLL format, and they were a very small quantity
? much less than even half a percent ? we decided to
ignore them.
For various reasons, not all the documents in
OntoNotes have been annotated with all the differ-
8
Corpora Words Documents
Total Train Dev Test Total Train Dev Test
MUC-6 25K 12K 13K 60 30 30
MUC-7 40K 19K 21K 67 30 37
ACE (2000-2004) 1M 775K 235K - - -
OntoNotes5 1.3M 1M 136K 142K 2,083(2,999) 1,674(2,374) 202(303) 207(322)
Table 2: Number of documents in the OntoNotes data, and some comparison with the MUC and ACE data sets. The
numbers in parenthesis for the OntoNotes corpus indicate the total number of parts that correspond to the documents.
Each part was considered a separate document for evaluation purposes.
Syntactic category Train Development Test
Count % Count % Count %
NP 60,345 59.71 8,463 59.31 8,629 53.09
PRP 25,472 25.21 3,535 24.78 5,012 30.84
PRP$ 8,889 8.80 1,208 8.47 1,466 9.02
NNP 2,643 2.62 468 3.28 475 2.92
NML 900 0.89 151 1.06 118 0.73
Vx 1,915 1.89 317 2.22 314 1.93
Other 893 0.88 126 0.88 239 1.47
Overall 101,057 100.00 14,268 100.00 16,253 100.00
Table 3: Distribution of mentions in the data by their syn-
tactic category.
Train Development Test
Entities/Chains 26,612 3,752 3,926
Links 74,652 10,539 12,365
Mentions 101,264 14,291 16,291
Table 4: Number of entities, links and mentions in the
OntoNotes 4.0 data.
ent layers of annotation, with full coverage.6 There
is a core portion, however, which is roughly 1.3M
words which has been annotated with all the layers.
This is the portion that we used for the shared task.
The number of documents in the corpus for this
task, for each of the different genres, are shown in
Table 2. Tables 3 and 4 shows the distribution of
mentions by the syntactic categories, and the counts
of entities, links and mentions in the corpus respec-
tively. All of this data has been Treebanked and
PropBanked either as part of the OntoNotes effort
or some preceding effort.
For comparison purposes, Table 2 also lists the
number of documents in the MUC-6, MUC-7, and
ACE (2000-2004) corpora. The MUC-6 data was
taken from the Wall Street Journal, whereas the
MUC-7 data was from the New York Times. The
ACE data spanned many different genres similar to
6Given the nature of word sense annotation, and changes in
project priorities, we could not annotate all the low frequency
verbs and nouns in the corpus. Furthermore, PropBank annota-
tion currently only covers verb predicates.
the ones in OntoNotes.
Parse Trees This represents the syntactic layer
that is a revised version of the Penn Treebank. For
purposes of this task, traces were removed from the
syntactic trees, since the CoNLL-style data format,
being indexed by tokens, does not provide any good
means of conveying that information. Function tags
were also removed, since the parsers that we used
for the predicted syntax layer did not provide them.
One thing that needs to be dealt with in conversa-
tional data is the presence of disfluencies (restarts,
etc.). In the original OntoNotes parses, these are
marked using a special EDITED7 phrase tag ? as was
the case for the Switchboard Treebank. Given the
frequency of disfluencies and the performance with
which one can identify them automatically,8 a prob-
able processing pipeline would filter them out be-
fore parsing. Since we did not have a readily avail-
able tagger for tagging disfluencies, we decided to
remove them using oracle information available in
the Treebank.
Propositions The propositions in OntoNotes con-
stitute PropBank semantic roles. Most of the verb
predicates in the corpus have been annotated with
their arguments. Recent enhancements to the Prop-
Bank to make it synchronize better with the Tree-
bank (Babko-Malaya et al, 2006) have enhanced
the information in the proposition by the addition of
two types of LINKs that represent pragmatic corefer-
ence (LINK-PCR) and selectional preferences (LINK-
SLC). More details can be found in the addendum to
the PropBank guidelines9 in the OntoNotes 4.0 re-
7There is another phrase type ? EMBED in the telephone con-
versation genre which is similar to the EDITED phrase type, and
sometimes identifies insertions, but sometimes contains logical
continuation of phrases, so we decided not to remove that from
the data.
8A study by Charniak and Johnson (2001) shows that one
can identify and remove edits from transcribed conversational
speech with an F-score of about 78, with roughly 95 Precision
and 67 recall.
9doc/propbank/english-propbank.pdf
9
lease. Since the community is not used to this rep-
resentation which relies heavily on the trace struc-
ture in the Treebank which we are excluding, we de-
cided to unfold the LINKs back to their original rep-
resentation as in the Release 1.0 of the Proposition
Bank. This functionality is part of the OntoNotes
DB Tool.10
Word Sense Gold word sense annotation was
supplied using sense numbers as specified in
the OntoNotes list of senses for each lemma.11
The sense inventories that were provided in the
OntoNotes 4.0 release were not all mapped to the lat-
est version 3.0 of WordNet, so we provided a revised
version of the sense inventories, containing mapping
to WordNet 3.0, on the task page for the participants.
Named Entities Named Entities in OntoNotes
data are specified using a catalog of 18 Name types.
Other Layers Discourse plays a vital role in
coreference resolution. In the case of broadcast con-
versation, or telephone conversation data, it partially
manifests in the form of speakers of a given utter-
ance, whereas in weblogs or newsgroups it does so
as the writer, or commenter of a particular article
or thread. This information provides an important
clue for correctly linking anaphoric pronouns with
the right antecedents. This information could be au-
tomatically deduced, but since it would add addi-
tional complexity to the already complex task, we
decided to provide oracle information of this meta-
data both during training and testing. In other words,
speaker and author identification was not treated
as an annotation layer that needed to be predicted.
This information was provided in the form of an-
other column in the .conll table. There were some
cases of interruptions and interjections that ideally
would associate parts of a sentence to two different
speakers, but since the frequency of this was quite
small, we decided to make an assumption of one
speaker/writer per sentence.
4.4.2 Predicted Annotation Layers
The predicted annotation layers were derived using
automatic models trained using cross-validation on
other portions of OntoNotes data. As mentioned ear-
lier, there are some portions of the OntoNotes corpus
that have not been annotated for coreference but that
have been annotated for other layers. For training
10http://cemantix.org/ontonotes.html
11It should be noted that word sense annotation in OntoNotes
is note complete, so only some of the verbs and nouns have
word sense tags specified.
Senses Lemmas
1 1,506
2 1,046
> 2 1,016
Table 6: Word sense polysemy over verb and noun lem-
mas in OntoNotes
models for each of the layers, where feasible, we
used all the data that we could for that layer from
the training portion of the entire OntoNotes release.
Parse Trees Predicted parse trees were produced
using the Charniak parser (Charniak and Johnson,
2005).12 Some additional tag types used in the
OntoNotes trees were added to the parser?s tagset,
including the NML tag that has recently been added
to capture internal NP structure, and the rules used to
determine head words were appropriately extended.
The parser was then re-trained on the training por-
tion of the release 4.0 data using 10-fold cross-
validation. Table 5 shows the performance of the
re-trained Charniak parser on the CoNLL-2011 test
set. We did not get a chance to re-train the re-ranker,
and since the stock re-ranker crashes when run on n-
best parses containing NMLs, because it has not seen
that tag in training, we could not make use of it.
Word Sense We trained a word sense tagger us-
ing a SVM classifier and contextual word and part
of speech features on all the training portion of the
OntoNotes data. The OntoNotes 4.0 corpus com-
prises a total of 14,662 sense definitions across 4877
verb and noun lemmas13. The distribution of senses
per lemma is as shown in Table 6. Table 7 shows
the performance of this classifier over both the verbs
and nouns in the CoNLL-2011 test set. Again this
performance is not directly comparable to any re-
ported in the literature before, and it seems lower
then performances reported on previous versions
of OntoNotes because this is over all the genres
of OntoNotes, and aggregated over both verbs and
nouns in the CoNLL-2011 test set.
Propositions To predict propositional structure,
ASSERT14 (Pradhan et al, 2005) was used, re-
trained also on all the training portion of the release
12http://bllip.cs.brown.edu/download/reranking-
parserAug06.tar.gz
13The number of lemmas in Table 6 do not add up to this
number because not all of them have examples in the training
data, where the total number of instantiated senses amounts to
7933.
14http://cemantix.org/assert.html
10
All Sentences Sentence len < 40
N POS R P F N R P F
Broadcast Conversation (BC) 2,194 95.93 84.30 84.46 84.38 2124 85.83 85.97 85.90
Broadcast News (BN) 1,344 96.50 84.19 84.28 84.24 1278 85.93 86.04 85.98
Magazine (MZ) 780 95.14 87.11 87.46 87.28 736 87.71 88.04 87.87
Newswire (NW) 2,273 96.95 87.05 87.45 87.25 2082 88.95 89.27 89.11
Telephone Conversation (TC) 1,366 93.52 79.73 80.83 80.28 1359 79.88 80.98 80.43
Weblogs and Newsgroups (WB) 1,658 94.67 83.32 83.20 83.26 1566 85.14 85.07 85.11
Overall 9,615 96.03 85.25 85.43 85.34 9145 86.86 87.02 86.94
Table 5: Parser performance on the CoNLL-2011 test set
Frameset Total Total % Perfect Argument ID + Class
Accuracy Sentences Propositions Propositions P R F
Broadcast Conversation (BC) 0.92 2,037 5,021 52.18 82.55 64.84 72.63
Broadcast News (BN) 0.91 1,252 3,310 53.66 81.64 64.46 72.04
Magazine (MZ) 0.89 780 2,373 47.16 79.98 61.66 69.64
Newswire (NW) 0.93 1,898 4,758 39.72 80.53 62.68 70.49
Weblogs and Newsgroups (WB) 0.92 929 2,174 39.19 81.01 60.65 69.37
Overall 0.91 6,896 17,636 46.82 81.28 63.17 71.09
Table 8: Performance on the propositions and framesets in the CoNLL-2011 test set.
Accuracy
Broadcast Conversation (BC) 0.70
Broadcast News (BN) 0.68
Magazine (MZ) 0.60
Newswire (NW) 0.62
Weblogs and Newsgroups (WB) 0.63
Overall 0.65
Table 7: Word sense performance over both verbs and
nouns in the CoNLL-2011 test set
4.0 data. Given time constraints, we had to per-
form two modifications: i) Instead of a single model
that predicts all arguments including NULL argu-
ments, we had to use the two-stage mode where the
NULL arguments are first filtered out and the remain-
ing NON-NULL arguments are classified into one of
the argument types, and ii) The argument identifi-
cation module used an ensemble of ten classifiers
? each trained on a tenth of the training data and
performed an unweighted voting among them. This
should still give a close to state of the art perfor-
mance given that the argument identification perfor-
mance tends to start to be asymptotic around 10k
training instances. At first glance, the performance
on the newswire genre is much lower than what has
been reported for WSJ Section 23. This could be
attributed to two factors: i) the fact that we had to
compromise on the training method, but more im-
portantly because ii) the newswire in OntoNotes not
only contains WSJ data, but also Xinhua news. One
could try to verify using just the WSJ portion of the
data, but it would be hard as it is not only a sub-
set of the documents that the performance has been
reported on previously, but also the annotation has
been significantly revised; it includes propositions
for be verbs missing from the original PropBank,
and the training data is a subset of the original data
as well. Table 8 shows the detailed performance
numbers.
In addition to automatically predicting the argu-
ments, we also trained a classifier to tag PropBank
frameset IDs in the data using the same word sense
module as mentioned earlier. OntoNotes 4.0 con-
tains a total of 7337 framesets across 5433 verb
lemmas.15 An overwhelming number of them are
monosemous, but the more frequent verbs tend to be
polysemous. Table 9 gives the distribution of num-
ber of framesets per lemma in the PropBank layer of
the OntoNotes 4.0 data.
During automatic processing of the data, we
tagged all the tokens that were tagged with a part
of speech VBx. This means that there would be cases
where the wrong token would be tagged with propo-
sitions. The CoNLL-2005 scorer was used to gener-
ate the scores.
Named Entities BBN?s IdentiFinderTMsystem
was used to predict the named entities. Given the
15The number of lemmas in Table 9 do not add up to this
number because not all of them have examples in the training
data, where the total number of instantiated senses amounts to
4229.
11
Framesets Lemmas
1 2,722
2 321
> 2 181
Table 9: Frameset polysemy across lemmas
Overall BC BN MZ NW TC WB
F F F F F F F
ALL Named Entities 71.8 64.8 72.2 61.5 84.3 39.5 55.2
Cardinal 68.7 51.8 71.1 66.1 82.8 34.0 68.7
Date 76.1 63.7 77.9 66.7 83.7 60.5 56.0
Event 27.6 00.0 34.8 30.8 47.6 - 13.3
Facility 41.9 55.0 16.7 23.1 66.7 00.0 22.9
GPE 87.9 87.5 90.3 73.7 92.9 65.9 88.7
Language 41.2 - 50.0 50.0 00.0 20.0 75.0
Law 63.0 00.0 85.7 00.0 67.9 00.0 50.0
Location 58.4 59.1 59.6 53.3 68.0 00.0 23.5
Money 74.6 16.7 66.7 73.2 79.4 30.8 61.5
NORP 00.0 00.0 00.0 00.0 00.0 00.0 00.0
Ordinal 73.4 73.8 73.4 78.1 78.4 88.9 37.0
Organization 71.0 57.8 67.1 52.9 86.9 21.2 32.1
Percent 71.2 88.9 76.9 69.6 92.1 01.2 71.6
Person 79.6 78.9 87.7 66.7 91.6 65.1 64.8
Product 46.9 00.0 43.8 00.0 81.8 00.0 00.0
Quantity 47.5 25.3 58.3 61.1 71.9 00.0 22.2
Time 58.6 56.9 64.1 42.9 80.0 23.8 51.7
Work of Art 41.9 26.9 37.1 16.0 77.9 00.0 05.6
Table 10: Named Entity performance on the CoNLL-
2011 test set
time constraints, we could not re-train it on the
OntoNotes data and so an existing, pre-trained
model was used, therefore the results are not a
good indicator of the model?s best performance.
The pre-trained model had also used a somewhat
different catalog of name types, which did not
include the OntoNotes NORP type (for nationalities,
organizations, religions, and political parties),
so that category was never predicted. Table 10
shows the overall performance of the tagger on the
CoNLL-2011 test set, as well as the performance
broken down by individual name types. IdentiFinder
performance has been reported to be in the low 90?s
on WSJ test set.
Other Layers As noted above, systems were al-
lowed to make use of gender and number predic-
tions for NPs using the table from Bergsma and Lin
(Bergsma and Lin, 2006).
4.4.3 Data Format
In order to organize the multiple, rich layers of anno-
tation, the OntoNotes project has created a database
representation for the raw annotation layers along
with a Python API to manipulate them (Pradhan et
al., 2007a). In the OntoNotes distribution the data is
organized as one file per layer, per document. The
API requires a certain hierarchical structure with
documents at the leaves inside a hierarchy of lan-
guage, genre, source and section. It comes with var-
ious ways of cleanly querying and manipulating the
data and allows convenient access to the sense in-
ventory and propbank frame files instead of having
to interpret the raw .xml versions. However, main-
taining format consistency with earlier CoNLL tasks
was deemed convenient for sites that already had
tools configured to deal with that format. Therefore,
in order to distribute the data so that one could make
the best of both worlds, we created a new file type
called .conll which logically served as another layer
in addition to the .parse, .prop, .name and .coref
layers. Each .conll file contained a merged repre-
sentation of all the OntoNotes layers in the CoNLL-
style tabular format with one line per token, and with
multiple columns for each token specifying the input
annotation layers relevant to that token, with the fi-
nal column specifying the target coreference layer.
Because OntoNotes is not authorized to distribute
the underlying text, and many of the layers contain
inline annotation, we had to provide a skeletal form
(.skel of the .conll file which was essentially the
.conll file, but with the word column replaced with
a dummy string. We provided an assembly script
that participants could use to create a .conll file tak-
ing as input the .skel file and the top-level directory
of the OntoNotes distribution that they had sepa-
rately downloaded from the LDC16 Once the .conll
file is created, it can be used to create the individual
layers such as .parse, .name, .coref etc. using an-
other set of scripts. Since the propositions and word
sense layers are inherently standoff annotation, they
were provided as is, and did not require that extra
merging step. One thing thing that made this data
creation process a bit tricky was the fact that we had
dissected some of the trees for the conversation data
to remove the EDITED phrases. Table 11 describes
the data provided in each of the column of the .conll
format. Figure 3 shows a sample from a .conll file.
4.5 Evaluation
This section describes the evaluation criteria used.
Unlike for propositions, word sense and named en-
tities, where it is simply a matter of counting the
correct answers, or for parsing, where there are sev-
eral established metrics, evaluating the accuracy of
coreference continues to be contentious. Various al-
16OntoNotes is deeply grateful to the Linguistic Data Con-
sortium for making the source data freely available to the task
participants.
12
Column Type Description
1 Document ID This is a variation on the document filename
2 Part number Some files are divided into multiple parts numbered as 000, 001, 002, ... etc.
3 Word number This is the word index in the sentence
4 Word The word itself
5 Part of Speech Part of Speech of the word
6 Parse bit This is the bracketed structure broken before the first open parenthesis in the parse, and the
word/part-of-speech leaf replaced with a *. The full parse can be created by substituting
the asterix with the ([pos] [word]) string (or leaf) and concatenating the items in the
rows of that column.
7 Predicate lemma The predicate lemma is mentioned for the rows for which we have semantic role informa-
tion. All other rows are marked with a -
8 Predicate Frameset ID This is the PropBank frameset ID of the predicate in Column 7.
9 Word sense This is the word sense of the word in Column 3.
10 Speaker/Author This is the speaker or author name where available. Mostly in Broadcast Conversation and
Web Log data.
11 Named Entities These columns identifies the spans representing various named entities.
12:N Predicate Arguments There is one column each of predicate argument structure information for the predicate
mentioned in Column 7.
N Coreference Coreference chain information encoded in a parenthesis structure.
Table 11: Format of the .conll file used on the shared task
#begin document (nw/wsj/07/wsj_0771); part 000
...
...
nw/wsj/07/wsj_0771 0 0 ?? ?? (TOP(S(S* - - - - * * (ARG1* * * -
nw/wsj/07/wsj_0771 0 1 Vandenberg NNP (NP* - - - - (PERSON) (ARG1* * * * (8|(0)
nw/wsj/07/wsj_0771 0 2 and CC * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 3 Rayburn NNP *) - - - - (PERSON) *) * * *(23)|8)
nw/wsj/07/wsj_0771 0 4 are VBP (VP* be 01 1 - * (V*) * * * -
nw/wsj/07/wsj_0771 0 5 heroes NNS (NP(NP*) - - - - * (ARG2* * * * -
nw/wsj/07/wsj_0771 0 6 of IN (PP* - - - - * * * * * -
nw/wsj/07/wsj_0771 0 7 mine NN (NP*)))) - - 5 - * *) * * * (15)
nw/wsj/07/wsj_0771 0 8 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 9 ?? ?? *) - - - - * * *) * * -
nw/wsj/07/wsj_0771 0 10 Mr. NNP (NP* - - - - * * (ARG0* (ARG0* * (15
nw/wsj/07/wsj_0771 0 11 Boren NNP *) - - - - (PERSON) * *) *) * 15)
nw/wsj/07/wsj_0771 0 12 says VBZ (VP* say 01 1 - * * (V*) * * -
nw/wsj/07/wsj_0771 0 13 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 14 referring VBG (S(VP* refer 01 2 - * * (ARGM-ADV* (V*) * -
nw/wsj/07/wsj_0771 0 15 as RB (ADVP* - - - - * * * (ARGM-DIS* * -
nw/wsj/07/wsj_0771 0 16 well RB *) - - - - * * * *) * -
nw/wsj/07/wsj_0771 0 17 to IN (PP* - - - - * * * (ARG1* * -
nw/wsj/07/wsj_0771 0 18 Sam NNP (NP(NP* - - - - (PERSON* * * * * (23
nw/wsj/07/wsj_0771 0 19 Rayburn NNP *) - - - - *) * * * * -
nw/wsj/07/wsj_0771 0 20 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 21 the DT (NP(NP* - - - - * * * * (ARG0* -
nw/wsj/07/wsj_0771 0 22 Democratic JJ * - - - - (NORP) * * * * -
nw/wsj/07/wsj_0771 0 23 House NNP * - - - - (ORG) * * * * -
nw/wsj/07/wsj_0771 0 24 speaker NN *) - - - - * * * * *) -
nw/wsj/07/wsj_0771 0 25 who WP (SBAR(WHNP*) - - - - * * * * (R-ARG0*) -
nw/wsj/07/wsj_0771 0 26 cooperated VBD (S(VP* cooperate 01 1 - * * * * (V*) -
nw/wsj/07/wsj_0771 0 27 with IN (PP* - - - - * * * * (ARG1* -
nw/wsj/07/wsj_0771 0 28 President NNP (NP* - - - - * * * * * -
nw/wsj/07/wsj_0771 0 29 Eisenhower NNP *))))))))))) - - - - (PERSON) * *) *) *) 23)
nw/wsj/07/wsj_0771 0 30 . . *)) - - - - * * * * * -
nw/wsj/07/wsj_0771 0 0 ?? ?? (TOP(S* - - - - * * * -
nw/wsj/07/wsj_0771 0 1 They PRP (NP*) - - - - * (ARG0*) * (8)
nw/wsj/07/wsj_0771 0 2 allowed VBD (VP* allow 01 1 - * (V*) * -
nw/wsj/07/wsj_0771 0 3 this DT (S(NP* - - - - * (ARG1* (ARG1* (6
nw/wsj/07/wsj_0771 0 4 country NN *) - - 3 - * * *) 6)
nw/wsj/07/wsj_0771 0 5 to TO (VP* - - - - * * * -
nw/wsj/07/wsj_0771 0 6 be VB (VP* be 01 1 - * * (V*) (16)
nw/wsj/07/wsj_0771 0 7 credible JJ (ADJP*))))) - - - - * *) (ARG2*) -
nw/wsj/07/wsj_0771 0 8 . . *)) - - - - * * * -
#end document
Figure 3: Sample portion of the .conll file.
13
ternative metrics have been proposed, as mentioned
below, which weight different features of a proposed
coreference pattern differently. The choice is not
clear in part because the value of a particular set of
coreference predictions is integrally tied to the con-
suming application.
A further issue in defining a coreference metric
concerns the granularity of the mentions, and how
closely the predicted mentions are required to match
those in the gold standard for a coreference predic-
tion to be counted as correct.
Our evaluation criterion was in part driven by the
OntoNotes data structures. OntoNotes coreference
distinguishes between identity coreference and ap-
positive coreference, treating the latter separately
because it is already captured explicitly by other lay-
ers of the OntoNotes annotation. Thus we evaluated
systems only on the identity coreference task, which
links all categories of entities and events together
into equivalent classes.
The situation with mentions for OntoNotes is also
different than it was for MUC or ACE. OntoNotes
data does not explicitly identify the minimum ex-
tents of an entity mention, but it does include hand-
tagged syntactic parses. Thus for the official evalua-
tion, we decided to use the exact spans of mentions
for determining correctness. The NP boundaries
for the test data were pre-extracted from the hand-
tagged Treebank for annotation, and events trig-
gered by verb phrases were tagged using the verbs
themselves. This choice means that scores for the
CoNLL-2011 coreference task are likely to be lower
than for coref evaluations based on MUC, where the
mention spans are specified in the input,17 or those
based on ACE data, where an approximate match is
often allowed based on the specified head of the NP
mention.
4.5.1 Metrics
As noted above, the choice of an evaluation met-
ric for coreference has been a tricky issue and there
does not appear to be any silver bullet approach that
addresses all the concerns. Three metrics have been
proposed for evaluating coreference performance
over an unrestricted set of entity types: i) The link
based MUC metric (Vilain et al, 1995), ii) The men-
tion based B-CUBED metric (Bagga and Baldwin,
1998) and iii) The entity based CEAF (Constrained
Entity Aligned F-measure) metric (Luo, 2005). Very
recently BLANC (BiLateral Assessment of Noun-
Phrase Coreference) measure (Recasens and Hovy,
17as is the case in this evaluation with Gold Mentions
2011) has been proposed as well. Each of the met-
ric tries to address the shortcomings or biases of the
earlier metrics. Given a set of key entities K, and
a set of response entities R, with each entity com-
prising one or more mentions, each metric generates
its variation of a precision and recall measure. The
MUC measure if the oldest and most widely used. It
focuses on the links (or, pairs of mentions) in the
data.18 The number of common links between en-
tities in K and R divided by the number of links
in K represents the recall, whereas, precision is the
number of common links between entities in K and
R divided by the number of links in R. This met-
ric prefers systems that have more mentions per en-
tity ? a system that creates a single entity of all
the mentions will get a 100% recall without signifi-
cant degradation in its precision. And, it ignores re-
call for singleton entities, or entities with only one
mention. The B-CUBED metric tries to addresses
MUCS?s shortcomings, by focusing on the mentions
and computes recall and precision scores for each
mention. If K is the key entity containing mention M,
and R is the response entity containing mention M,
then recall for the mention M is computed as |K?R||K|
and precision for the same is is computed as |K?R||R| .
Overall recall and precision are the average of the
individual mention scores. CEAF aligns every re-
sponse entity with at most one key entity by finding
the best one-to-one mapping between the entities us-
ing an entity similarity metric. This is a maximum
bipartite matching problem and can be solved by
the Kuhn-Munkres algorithm. This is thus a entity
based measure. Depending on the similarity, there
are two variations ? entity based CEAF ? CEAFe and
a mention based CEAF ? CEAFe. Recall is the total
similarity divided by the number of mentions in K,
and precision is the total similarity divided by the
number of mentions in R. Finally, BLANC uses a
variation on the Rand index (Rand, 1971) suitable
for evaluating coreference. There are a few other
measures ? one being the ACE value, but since this
is specific to a restricted set of entities (ACE types),
we did not consider it.
4.5.2 Official Evaluation Metric
In order to determine the best performing system
in the shared task, we needed to associate a single
number with each system. This could have been
one of the metrics above, or some combination of
more than one of them. The choice was not sim-
ple, and while we consulted various researchers in
18The MUC corpora did not tag single mention entities.
14
the field, hoping for a strong consensus, their con-
clusion seemed to be that each metric had its pros
and cons. We settled on the MELA metric by Denis
and Baldridge (2009), which takes a weighted av-
erage of three metrics: MUC, B-CUBED, and CEAF.
The rationale for the combination is that each of the
three metrics represents a different important dimen-
sion, the MUC measure being based on links, the
B-CUBED based on mentions, and the CEAF based
on entities. For a given task, a weighted average
of the three might be optimal, but since we don?t
have an end task in mind, we decided to use the un-
weighted mean of the three metrics as the score on
which the winning system was judged. We decided
to use CEAFe instead of CEAFm.
4.5.3 Scoring Metrics Implementation
We used the same core scorer implementation19 that
was used for the SEMEVAL-2010 task, and which
implemented all the different metrics. There were a
couple of modifications done to this scorer after it
was used for the SEMEVAL-2010 task.
1. Only exact matches were considered cor-
rect. Previously, for SEMEVAL-2010 non-exact
matches were judged partially correct with a
0.5 score if the heads were the same and the
mention extent did not exceed the gold men-
tion.
2. The modifications suggested by Cai and Strube
(2010) were incorporated in the scorer.
Since there are differences in the version used for
CoNLL and the one available on the download site,
and it is possible that the latter would be revised in
the future, we have archived the version of the scorer
on the CoNLL-2011 task webpage.20
5 Systems and Results
About 65 different groups demonstrated interest in
the shared task by registering on the task webpage.
Of these, 23 groups submitted system outputs on the
test set during the evaluation week. 18 groups sub-
mitted only closed track results, 3 groups only open
track results, and 2 groups submitted both closed and
open track results. 2 participants in the closed track,
did not write system papers, so we don?t use their re-
sults in the discussion. Their results will be reported
on the task webpage.
19http://www.lsi.upc.edu/ esapena/downloads/index.php?id=3
20http://conll.bbn.com/download/scorer.v4.tar.gz
The official results for the 18 systems that submit-
ted closed track outputs are shown in Table 12, with
those for the 5 systems that submitted open track
results in Table 13. The official ranking score, the
arithmetic mean of the F-scores of MUC, B-CUBED
and CEAFe, is shown in the rightmost column. For
convenience, systems will be referred to here using
the first portion of the full name, which is unique
within each table.
For completeness, the tables include the raw pre-
cision and recall scores from which the F-scores
were derived. The tables also include two additional
scores (BLANC and CEAFm) that did not factor into
the official ranking score. Useful further analysis
may be possible based on these results beyond the
preliminary results presented here.
As discussed previously in the task description,
we will consider three different test input conditions:
i) Predicted only (Official), ii) Predicted plus gold
mention boundaries, and iii) Predicted plus gold
mentions
5.1 Predicted only (Official)
For the official test, beyond the raw source text,
coreference systems were provided only with the
predictions from automatic engines as to the other
annotation layers (parses, semantic roles, word
senses, and named entities).
In this evaluation it is important to note that the
mention detection score cannot be considered in iso-
lation of the coreference task as has usually been the
case. This is mainly owing to the fact that there are
no singleton entities in the OntoNotes data. Most
systems removed singletons from the response as a
post-processing step, so not only will they not get
credit for the singleton entities that they correctly re-
moved from the data, but they will be penalized for
the ones that they accidentally linked with another
mention. What this number does indicate is the ceil-
ing on recall that a system would have got in absence
of being penalized for making mistakes in corefer-
ence resolution. A close look at the Table 12 indi-
cates a possible outlier in case of the sapena system.
The recall for this system is very high, and precision
way lower than any other system. Further investi-
gations uncovered that the reason for this aberrant
behavior was that fact that this system opted to keep
singletons in the response. By design, the scorer re-
moves singletons that might be still present in the
system, but it does so after the mention detection
accuracy is computed.
The official scores top out in the high 50?s. While
this is lower than the figures cited in previous coref-
15
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
75
.07
66
.81
70
.7
0
61
.76
57
.53
59
.57
68
.40
68
.23
68
.31
56
.37
56
.37
56
.3
7
43
.41
47
.75
45
.4
8
70
.63
76
.21
73
.02
57
.7
9
sap
en
a
92
.39
28
.19
43
.20
56
.32
63
.16
59
.55
62
.75
72
.08
67
.09
53
.51
53
.51
53
.51
44
.75
38
.38
41
.32
69
.50
73
.07
71
.10
55
.99
ch
an
g
68
.08
61
.96
64
.88
57
.15
57
.15
57
.15
67
.14
70
.53
68
.7
9
54
.40
54
.40
54
.40
41
.94
41
.94
41
.94
71
.19
77
.09
73
.7
1
55
.96
nu
gu
es
69
.87
68
.08
68
.96
60
.20
57
.10
58
.61
66
.74
64
.23
65
.46
51
.45
51
.45
51
.45
38
.09
41
.06
39
.52
71
.99
70
.31
71
.11
54
.53
san
tos
67
.80
63
.25
65
.45
59
.21
54
.30
56
.65
68
.79
62
.81
65
.66
49
.54
49
.54
49
.54
35
.86
40
.21
37
.91
73
.37
66
.91
69
.46
53
.41
son
g
57
.81
80
.41
67
.26
53
.73
67
.79
59
.9
5
60
.65
66
.05
63
.23
46
.29
46
.29
46
.29
43
.37
30
.71
35
.96
69
.49
59
.71
61
.47
53
.05
sto
ya
no
v
70
.84
64
.98
67
.78
63
.61
54
.04
58
.43
72
.58
53
.27
61
.44
46
.08
46
.08
46
.08
32
.00
40
.82
35
.88
73
.21
58
.93
60
.88
51
.92
sob
ha
67
.82
62
.09
64
.83
51
.08
49
.88
50
.48
62
.63
65
.43
64
.00
49
.48
49
.48
49
.48
40
.65
41
.82
41
.23
61
.40
68
.35
63
.88
51
.90
ko
bd
an
i
62
.06
60
.04
61
.03
55
.64
51
.50
53
.49
69
.66
62
.43
65
.85
42
.70
42
.70
42
.70
32
.33
35
.40
33
.79
61
.86
63
.51
62
.61
51
.04
zh
ou
61
.08
63
.59
62
.31
45
.65
52
.79
48
.96
57
.14
72
.91
64
.07
47
.53
47
.53
47
.53
43
.19
36
.79
39
.74
61
.10
73
.94
64
.72
50
.92
ch
art
on
65
.90
62
.77
64
.30
55
.09
50
.05
52
.45
66
.26
58
.44
62
.10
46
.82
46
.82
46
.82
34
.33
39
.05
36
.54
69
.94
62
.23
64
.80
50
.36
ya
ng
71
.92
57
.53
63
.93
59
.91
46
.43
52
.31
71
.64
55
.14
62
.32
46
.55
46
.55
46
.55
30
.28
42
.39
35
.33
71
.11
61
.75
64
.63
49
.99
ha
o
64
.50
64
.11
64
.30
57
.89
51
.42
54
.47
67
.83
55
.43
61
.01
45
.07
45
.07
45
.07
30
.08
35
.76
32
.67
72
.61
62
.37
65
.35
49
.38
xin
xin
65
.49
58
.71
61
.92
48
.54
44
.85
46
.62
61
.59
62
.28
61
.93
44
.75
44
.75
44
.75
35
.19
38
.62
36
.83
63
.04
65
.83
64
.27
48
.46
zh
an
g
55
.35
68
.25
61
.13
42
.03
55
.62
47
.88
52
.57
73
.05
61
.14
44
.46
44
.46
44
.46
42
.00
30
.28
35
.19
62
.84
69
.22
65
.21
48
.07
ku
mm
erf
eld
69
.77
56
.97
62
.72
46
.39
39
.56
42
.70
63
.60
57
.30
60
.29
45
.35
45
.35
45
.35
35
.05
42
.26
38
.32
58
.74
61
.58
59
.91
47
.10
zh
ek
ov
a
67
.49
37
.60
48
.29
28
.87
20
.66
24
.08
67
.14
56
.67
61
.46
40
.43
40
.43
40
.43
31
.57
41
.21
35
.75
52
.77
57
.05
53
.77
40
.43
irw
in
17
.06
61
.09
26
.67
12
.45
50
.60
19
.98
35
.07
89
.90
50
.46
31
.68
31
.68
31
.68
45
.84
17
.38
25
.21
51
.48
56
.83
51
.12
31
.88
Ta
ble
12
:P
erf
orm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
cl
os
ed
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
74
.31
67
.87
70
.9
4
62
.83
59
.34
61
.0
3
68
.85
69
.01
68
.9
3
56
.70
56
.70
56
.7
0
43
.29
46
.80
44
.9
8
71
.90
76
.55
73
.96
58
.3
1
cai
67
.15
67
.64
67
.40
56
.73
58
.90
57
.80
64
.60
71
.03
67
.66
53
.37
53
.37
53
.37
42
.71
40
.68
41
.67
69
.77
73
.96
71
.62
55
.71
ury
up
ina
70
.60
66
.31
68
.39
59
.70
55
.70
57
.63
66
.29
64
.12
65
.18
51
.42
51
.42
51
.42
38
.34
42
.17
40
.16
69
.23
68
.54
68
.88
54
.32
kle
nn
er
64
.41
60
.28
62
.28
49
.04
50
.71
49
.86
61
.70
68
.61
64
.97
50
.03
50
.03
50
.03
41
.28
39
.70
40
.48
66
.05
73
.90
69
.05
51
.77
irw
in
24
.60
62
.27
35
.27
18
.56
51
.01
27
.21
38
.97
85
.57
53
.55
33
.86
33
.86
33
.86
43
.33
19
.36
26
.76
51
.62
52
.91
51
.76
35
.84
Ta
ble
13
:P
erf
orm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
op
en
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
79
.52
71
.25
75
.1
6
65
.87
62
.05
63
.9
0
69
.52
70
.55
70
.0
3
59
.26
59
.26
59
.2
6
46
.29
50
.48
48
.3
0
72
.00
78
.55
74
.7
7
60
.7
4
nu
gu
es
74
.18
70
.74
72
.42
64
.33
60
.05
62
.12
68
.26
65
.17
66
.68
53
.84
53
.84
53
.84
39
.86
44
.23
41
.93
72
.53
71
.04
71
.75
56
.91
ch
an
g
63
.37
73
.18
67
.92
55
.00
65
.50
59
.79
62
.16
76
.65
68
.65
54
.95
54
.95
54
.95
46
.77
37
.17
41
.42
70
.97
79
.30
74
.29
56
.62
san
tos
65
.82
69
.90
67
.80
57
.76
61
.39
59
.52
64
.49
70
.27
67
.26
51
.87
51
.87
51
.87
41
.42
38
.16
39
.72
72
.72
71
.97
72
.34
55
.50
ko
bd
an
i
67
.11
65
.09
66
.08
62
.63
56
.80
59
.57
73
.20
62
.22
67
.27
44
.49
44
.49
44
.49
32
.87
37
.25
34
.92
64
.07
64
.13
64
.10
53
.92
sto
ya
no
v
76
.90
64
.73
70
.29
69
.81
55
.01
61
.54
77
.07
52
.54
62
.48
48
.08
48
.08
48
.08
30
.97
44
.84
36
.64
76
.57
60
.33
62
.96
53
.55
zh
an
g
59
.62
71
.19
64
.89
46
.06
58
.75
51
.64
53
.89
73
.41
62
.16
46
.62
46
.62
46
.62
43
.49
32
.11
36
.95
64
.11
70
.47
66
.54
50
.25
son
g
58
.43
77
.64
66
.68
46
.66
68
.40
55
.48
54
.40
70
.19
61
.29
43
.62
43
.62
43
.62
43
.77
25
.88
32
.53
66
.29
58
.76
60
.22
49
.77
zh
ek
ov
a
69
.19
57
.27
62
.67
33
.48
37
.15
35
.22
55
.47
68
.23
61
.20
41
.31
41
.31
41
.31
38
.29
34
.65
36
.38
53
.45
63
.33
54
.79
44
.27
Ta
ble
14
:P
erf
orm
an
ce
of
sys
tem
sin
the
sup
ple
me
nta
ry
cl
os
ed
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
bo
un
da
ri
es
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
78
.71
72
.33
75
.39
66
.93
63
.91
65
.39
70
.09
71
.49
70
.78
59
.78
59
.78
59
.78
46
.34
49
.62
47
.92
73
.38
79
.00
75
.83
61
.36
Ta
ble
15
:P
erf
orm
an
ce
of
sys
tem
sin
the
sup
ple
me
nta
ry
op
en
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
bo
un
da
ri
es
16
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
ch
an
g
10
0
10
0
10
0
80
.46
84
.75
82
.55
72
.84
74
.57
73
.70
69
.71
69
.71
69
.71
70
.45
60
.75
65
.24
78
.01
76
.57
77
.26
73
.83
Ta
ble
16
:P
erf
orm
an
ce
of
sys
tem
sin
the
su
pp
le
m
en
ta
ry
,c
lo
se
d
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
m
en
ti
on
s
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
83
.37
10
0
90
.93
74
.79
89
.68
81
.56
67
.46
86
.88
75
.95
70
.73
70
.73
70
.73
77
.75
51
.05
61
.64
76
.65
85
.85
80
.35
73
.05
Ta
ble
17
:P
erf
orm
an
ce
of
sys
tem
sin
the
su
pp
le
m
en
ta
ry
,o
pe
n
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
m
en
ti
on
s
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
76
.79
68
.34
72
.3
2
63
.29
58
.96
61
.05
68
.84
68
.72
68
.78
57
.28
57
.28
57
.2
8
44
.19
48
.75
46
.3
6
70
.93
76
.58
73
.36
58
.7
3
sap
en
a
95
.27
29
.07
44
.55
56
.99
63
.91
60
.25
62
.89
72
.31
67
.27
53
.90
53
.90
53
.90
45
.22
38
.70
41
.71
69
.71
73
.32
71
.32
56
.41
ch
an
g
69
.88
63
.61
66
.60
58
.48
58
.48
58
.48
67
.42
70
.91
69
.1
2
55
.21
55
.21
55
.21
42
.66
42
.66
42
.66
71
.42
77
.36
73
.9
6
56
.75
nu
gu
es
72
.96
71
.08
72
.01
62
.68
59
.46
61
.03
67
.24
64
.89
66
.04
52
.82
52
.82
52
.82
39
.25
42
.50
40
.81
72
.57
70
.86
71
.68
55
.96
san
tos
70
.39
65
.67
67
.95
61
.28
56
.20
58
.63
69
.25
63
.16
66
.07
50
.47
50
.47
50
.47
36
.51
41
.15
38
.69
73
.92
67
.32
69
.93
54
.46
son
g
59
.24
82
.39
68
.92
54
.92
69
.29
61
.27
60
.89
66
.27
63
.46
46
.97
46
.97
46
.97
44
.49
31
.15
36
.65
69
.73
59
.87
61
.61
53
.79
sto
ya
no
v
74
.43
68
.28
71
.22
67
.18
57
.08
61
.7
2
74
.06
53
.45
62
.09
47
.40
47
.40
47
.40
32
.78
42
.52
37
.02
74
.10
59
.34
61
.31
53
.61
sob
ha
71
.06
65
.06
67
.93
53
.91
52
.64
53
.27
63
.17
66
.14
64
.62
50
.80
50
.80
50
.80
41
.77
43
.03
42
.39
61
.91
69
.15
64
.49
53
.43
ko
bd
an
i
65
.98
63
.83
64
.89
59
.22
54
.81
56
.93
70
.49
63
.12
66
.60
44
.17
44
.14
44
.15
33
.19
36
.50
34
.77
62
.52
64
.25
63
.32
52
.77
zh
ou
64
.11
66
.74
65
.40
48
.00
55
.51
51
.48
57
.18
73
.71
64
.40
48
.40
48
.40
48
.40
44
.18
37
.35
40
.48
61
.54
74
.86
65
.30
52
.12
ch
art
on
71
.01
67
.64
69
.28
59
.24
53
.82
56
.40
67
.10
59
.02
62
.80
48
.91
48
.91
48
.91
35
.96
41
.39
38
.48
70
.65
62
.71
65
.34
52
.56
ya
ng
73
.73
58
.97
65
.53
61
.23
47
.45
53
.47
71
.88
55
.13
62
.40
47
.05
47
.05
47
.05
30
.54
43
.16
35
.77
71
.39
61
.92
64
.83
50
.55
ha
o
66
.79
66
.38
66
.59
59
.55
52
.89
56
.02
68
.27
55
.46
61
.20
45
.95
45
.95
45
.95
30
.76
36
.81
33
.51
73
.22
62
.73
65
.78
50
.24
xin
xin
69
.05
61
.91
65
.28
50
.99
47
.11
48
.97
61
.59
62
.70
62
.14
45
.64
45
.64
45
.64
35
.86
39
.57
37
.62
63
.42
66
.29
64
.68
49
.58
zh
an
g
57
.41
70
.78
63
.40
43
.48
57
.53
49
.53
52
.44
73
.60
61
.24
44
.97
44
.97
44
.97
42
.71
30
.44
35
.55
63
.12
69
.63
65
.53
48
.77
ku
mm
erf
eld
71
.05
58
.01
63
.87
47
.42
40
.44
43
.65
63
.73
57
.39
60
.39
45
.76
45
.76
45
.76
35
.30
42
.72
38
.66
58
.89
61
.77
60
.07
47
.57
zh
ek
ov
a
72
.65
40
.48
51
.99
31
.73
22
.70
26
.46
66
.92
56
.68
61
.37
41
.04
41
.04
41
.04
31
.93
42
.17
36
.34
53
.09
57
.86
54
.22
41
.39
irw
in
17
.58
62
.96
27
.49
12
.69
51
.59
20
.37
34
.88
89
.98
50
.27
31
.71
31
.71
31
.71
46
.13
17
.33
25
.20
51
.51
56
.93
51
.14
31
.95
Ta
ble
18
:H
ea
d
w
or
d
ba
se
d
pe
rfo
rm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
cl
os
ed
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
76
.01
69
.43
72
.5
7
64
.40
60
.83
62
.5
7
69
.34
69
.57
69
.4
5
57
.68
57
.68
57
.6
8
44
.15
47
.85
45
.9
2
72
.23
76
.94
74
.3
2
59
.3
1
cai
69
.32
69
.82
69
.57
58
.39
60
.63
59
.49
64
.88
71
.53
68
.04
54
.36
54
.36
54
.36
43
.74
41
.58
42
.64
70
.13
74
.39
72
.01
56
.72
ury
up
ina
72
.10
67
.72
69
.84
60
.74
56
.68
58
.64
66
.43
64
.25
65
.32
52
.00
52
.00
52
.00
38
.87
42
.85
40
.76
69
.43
68
.73
69
.07
54
.91
kle
nn
er
71
.73
67
.14
69
.36
55
.17
57
.04
56
.09
62
.67
70
.69
66
.44
53
.25
53
.25
53
.25
44
.27
42
.39
43
.31
67
.45
75
.92
70
.68
55
.28
irw
in
25
.24
63
.87
36
.18
18
.90
51
.94
27
.71
38
.79
85
.64
53
.40
33
.89
33
.89
33
.89
43
.59
19
.31
26
.76
51
.66
52
.98
51
.80
35
.96
Ta
ble
19
:H
ea
d
w
or
d
ba
se
d
pe
rfo
rm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
op
en
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
17
erence evaluations, that is as expected, given that the
task here includes predicting the underlying men-
tions and mention boundaries, the insistence on ex-
act match, and given that the relatively easier appos-
itive coreference cases are not included in this mea-
sure. The top-performing system (lee) had a score
of 57.79 which is about 1.8 points higher than that
of the second (sapena) and third (chang) ranking
systems, which scored 55.99 and 55.96 respectively.
Another 1.5 points separates them from the fourth
best score of 54.53 (nugues). Thus the performance
differences between the better-scoring systems were
not large, with only about three points separating the
top four systems.
This becomes even clearer if we merge in the re-
sults of systems that participated only in the open
track but that made relatively limited use of outside
resources.21 Comparing that way, the cai system
scores in the same ball park as the second rank sys-
tems (sapena and chang). The uryupina system sim-
ilarly scores very close to nugues?s 54.53
Given that our choice of the official metric was
somewhat arbitrary, if is also useful to look at
the individual metrics, including the mention-based
CEAFm and BLANC metrics that were not part of
the official metric. The lee system which scored
the best using the official metric does slightly worse
than song on the MUC metric, and also does slightly
worse than chang on the B-CUBED and BLANC met-
rics. However, it does much better than every other
group on the entity-based CEAFe, and this is the pri-
mary reason for its 1.8 point advantage in the offi-
cial score. If the CEAFe measure does indicate the
accuracy of entities in the response, this suggests
that the lee system is doing better on getting coher-
ent entities than any other system. This could be
partly due to the fact that that system is primarily
a precision-based system that would tend to create
purer entities. The CEAFe measure also seems to pe-
nalize other systems more harshly than do the other
measures.
We cannot compare these results to the ones ob-
tained in the SEMEVAL-2010 coreference task using
a small portion of OntoNotes data because it was
only using nominal entities, and had heuristically
added singleton mentions to the OntoNotes data22
21The cai system specifically mentions that, and the only re-
source that the uryupina system used outside of the closed track
setting was the Stanford named entity tagger.
22The documentation that comes with the SEMEVAL data
package from LDC (LDC2011T01) states: ?Only nominal
mentions and identical (IDENT) types were taken from the
OntoNotes coreference annotation, thus excluding coreference
5.2 Predicted plus gold mention boundaries
We also explored performance when the systems
were provided with the gold mention boundaries,
that is, with the exact spans (expressed in terms of
token offsets) for all of the NP constituents in the
human-annotated parse trees for the test data. Sys-
tems could use this additional data to ensure that the
output mention spans in their entity chains would not
clash with those in the answer set. Since this was
a secondary evaluation, it was an optional element,
and not all participants ran their systems on this task
variation. The results for those systems that did par-
ticipate in this optional task are shown in Tables 14
(closed track) and 15 (open track).
Most of the better scoring systems did supply
these results. While all systems did slightly better
here in terms of raw scores, the performance was
not much different from the official task, indicating
that mention boundary errors resulting from prob-
lems in parsing do not contribute significantly to the
final output.23
One side benefit of performing this supplemental
evaluation was that it revealed a subtle bug in the
automatic scoring routine that we were using that
could double-count duplicate correct mentions in a
given entity chain. These can occur, for example, if
the system considers a unit-production NP-PRP com-
bination as two mentions that identify the exact same
token in the text, and reports them as separate men-
tions. Most systems had a filter in their processing
that selected only one of these duplicate mentions,
but the kobdani system considered both as potential
mentions, and its developers tuned their algorithm
using that flawed version of the scorer.
When we fixed the scorer and re-evaluated all of
the systems, the kobdani system was the only one
whose score was affected significantly, dropping by
about 8 points, which lowered that system?s rank
from second to ninth. It is not clear how much of
this was owing to the fact that the system?s param-
relations with verbs and appositives. Since OntoNotes is only
annotated with multi-mention entities, singleton referential ele-
ments were identified heuristically: all NPs and possessive de-
terminers were annotated as singletons excluding those func-
tioning as appositives or as pre-modifiers but for NPs in the
possessive case. In coordinated NPs, single constituents as well
as the entire NPs were considered to be mentions. There is no
reliable heuristic to automatically detect English expletive pro-
nouns, thus they were (although inaccurately) also annotated as
singletons.?
23It would be interesting to measure the overlap between the
entity clusters for these two cases, to see whether there was
any substantial difference in the mention chains, besides the ex-
pected differences in boundaries for individual mentions.
18
eters had been tuned using the scorer with the bug,
which double-credited duplicate mentions. To find
out for sure, one would have to re-tune the system
using the modified scorer.
One difficulty with this supplementary evaluation
using gold mention boundaries is that those bound-
aries alone provide only very partial information.
For the roughly 10% of mentions that the automatic
parser did not correctly identify, while the systems
knew the correct boundaries, they had no hierarchi-
cal parser or semantic role label information, and
they also had to further approximate the already
heuristic head word identification. This incomplete
data complicated the systems? task and also compli-
cates interpretation of the results.
5.3 Predicted plus gold mentions
The final supplementary condition that we explored
was if the systems were supplied with the manually-
annotated spans for exactly those mentions that did
participate in the gold standard coreference chains.
This supplies significantly more information than
the previous case, where exact spans were supplied
for all NPs, since the gold mentions list here will also
include verb headwords that are linked to event NPs,
but will not include singleton mentions, which do
not end up as part of any chain. The latter constraint
makes this test seem somewhat artificial, since it di-
rectly reveals part of what the systems are designed
to determine, but it still has some value in quanti-
fying the impact that mention detection has on the
overall task and what the results are if the mention
detection is perfect.
Since this was a logical extension of the task and
since the data was available to the participants for
the development set, a few of the sites did run ex-
periments of this type. Therefore we decided to pro-
vide the gold mentions data to a few sites who had
reported these scores, so that we could compute the
performance on the test set. The results of these ex-
periments are shown in Tables 16 and 17. The results
show that performance does go up significantly, in-
dicating that it is markedly easier for the systems
to generate better entities given gold mentions. Al-
though, ideally, one would expect a perfect mention
detection score, it is the case that one of the two sys-
tems ? lee ? did not get a 100% Recall. This could
possibly be owing to unlinked singletons that were
removed in post-processing.
The lee system developers also ran a further ex-
periment where both gold mentions for the elements
of the coreference chains and also gold annota-
tions for all the other layers were available to the
system. Surprisingly, the improvement in corefer-
ence performance from having gold annotation of
the other layers was almost negligible. This sug-
gests that either: i) the automatic models are pre-
dicting those layers well enough that switching to
gold doesn?t make much difference; ii) information
from the other layers does not provide much lever-
age for coreference resolution; or iii) current coref-
erence models are not capable of utilizing the infor-
mation from these other layers effectively. Given
the performance numbers on the individual layers
cited earlier, (i) seems unlikely, and we hope that
further research in how best to leverage these lay-
ers will result in models that can benefit from them
more definitively.
5.4 Head word based scoring
In order to check how stringent the official, exact
match scoring is, we also performed a relaxed scor-
ing. Unlike ACE and MUC, the OntoNotes data does
not have manually annotated minimum spans that
a mention must contain to be considered correct.
However, OntoNotes does have manual syntactic
analysis in the form of the Treebank. Therefore, we
decided to approximate the minimum spans by using
the head words of the mentions using the gold stan-
dard syntax tree. If the response mention contained
the head word and did not exceed the true mention
boundary, then it was considered correct ? both from
the point of view of mention detection, and corefer-
ence resolution. The scores using this relaxed strat-
egy for the open and closed track submissions using
predicted data are shown in Tables 18 and 19. It
can be observed that the relaxed, head word based,
scoring does not improve performance very much.
The only exception was the klenner system whose
performance increased from 51.77 to 55.28. Over-
all, the ranking remained quite stable, though it did
change for some adjacent systems which had very
close exact match scores.
5.5 Genre variation
In order to check how the systems did on various
genres, we scored their performance per genre as
well. Tables 20 and 21 summarize genre based per-
formance for the closed and open track participants
respectively. System performance does not seem
to vary as much across the different genres as is
normally the case with language processing tasks,
which could suggest that coreference is relatively
genre insensitive, or it is possible that scores are
two low for the difference to be apparent. Compar-
isons are difficult, however, because the spoken gen-
19
MD MUC BCUB Cm Ce BLANC O MD MUC BCUB Cm Ce BLANC O
F F F F F F F F F F F F F F
lee GENRE zhou GENRE
BC 72.2 60.0 66.2 53.9 43.7 71.7 56.7 BC 64.1 49.5 62.1 45.3 38.8 61.8 50.1
BN 72.0 59.0 68.7 57.6 48.7 68.8 58.8 BN 60.8 45.9 64.4 49.5 41.2 66.8 50.5
MZ 70.1 58.0 72.2 61.6 50.9 75.0 60.4 MZ 58.8 44.4 66.9 50.1 41.8 64.6 51.0
NW 65.4 54.3 69.4 56.5 45.5 70.4 56.4 NW 57.7 44.8 65.7 48.7 40.3 63.1 50.2
TC 75.9 66.8 69.5 59.3 41.3 81.6 59.2 TC 69.2 58.1 60.8 43.1 35.7 62.6 51.5
WB 73.0 63.9 65.7 54.2 42.7 73.4 57.5 WB 67.4 55.4 62.8 47.9 39.2 69.1 52.5
sapena charton
BC 48.7 58.8 64.6 50.8 39.4 70.4 54.3 BC 65.8 53.1 59.1 44.6 35.2 64.4 49.1
BN 47.1 60.0 69.1 57.4 45.0 74.3 58.0 BN 65.5 52.0 64.0 50.0 39.6 65.9 51.9
MZ 35.3 59.2 72.3 60.4 48.2 75.0 59.9 MZ 61.7 46.3 64.6 49.7 39.9 64.1 50.3
NW 35.2 57.9 69.7 55.3 41.9 73.8 56.5 NW 57.6 44.6 64.5 48.2 37.7 67.0 48.9
TC 60.4 64.3 63.3 48.3 35.1 68.8 54.2 TC 73.1 66.8 56.2 42.8 29.9 58.1 51.0
WB 46.3 60.1 62.5 49.1 37.4 67.4 53.3 WB 67.6 57.6 59.3 45.1 33.3 66.6 50.0
chang yang
BC 65.5 56.4 67.1 51.5 39.8 71.6 54.4 BC 65.7 53.8 62.3 46.8 35.0 67.5 50.3
BN 66.6 57.4 69.1 56.0 45.6 70.5 57.4 BN 66.0 53.1 63.8 49.1 40.0 63.1 52.3
MZ 61.6 52.7 71.3 57.6 46.4 72.9 56.8 MZ 58.8 43.9 59.7 42.6 32.8 55.5 45.5
NW 61.0 53.3 69.1 54.1 42.1 71.9 54.8 NW 57.2 44.7 62.9 45.3 35.0 62.7 47.6
TC 72.2 68.5 71.4 59.6 37.7 81.7 59.2 TC 74.2 66.8 66.3 55.3 36.0 76.1 56.4
WB 66.4 59.7 66.7 52.7 39.4 74.7 55.3 WB 67.6 57.6 57.0 42.6 32.1 60.1 48.9
nugues hao
BC 71.4 59.2 62.4 48.2 37.2 68.4 52.9 BC 68.9 58.7 58.9 44.8 31.7 64.9 49.8
BN 70.0 58.5 67.4 54.5 43.1 73.1 56.3 BN 62.0 51.1 63.0 46.2 35.5 64.1 49.9
MZ 65.4 53.6 68.6 54.2 42.2 70.1 54.8 MZ 60.3 46.7 61.5 46.3 34.3 61.9 47.5
NW 61.8 51.9 67.0 51.3 39.2 69.4 52.7 NW 57.2 47.7 63.3 45.5 32.9 66.0 48.0
TC 77.2 69.2 63.9 53.0 37.9 72.2 57.0 TC 67.9 60.4 58.8 44.7 30.3 68.3 49.8
WB 72.9 64.2 63.4 51.1 38.5 74.3 55.4 WB 71.4 61.8 55.7 42.6 30.0 64.4 49.2
santos xinxin
BC 66.6 57.2 64.8 48.5 37.2 68.6 53.0 BC 64.8 47.8 60.2 43.9 35.5 65.1 47.9
BN 66.9 57.3 66.9 52.3 41.0 71.8 55.1 BN 61.5 44.7 63.2 47.0 38.9 65.8 48.9
MZ 62.7 51.0 65.9 48.9 37.8 64.5 51.6 MZ 54.6 35.5 64.5 45.7 37.7 61.0 45.9
NW 58.4 49.5 66.2 48.1 37.4 66.9 51.0 NW 54.3 39.5 64.0 45.0 37.5 61.1 47.0
TC 74.2 66.9 65.9 52.5 35.5 72.5 56.1 TC 74.2 62.0 57.9 45.4 33.4 66.5 51.1
WB 70.4 63.2 63.4 49.5 38.2 70.3 55.0 WB 66.9 52.6 58.5 42.2 35.9 63.4 49.0
song zhang
BC 68.9 61.4 61.0 44.1 34.3 59.5 52.2 BC 65.8 50.6 61.1 45.3 35.5 67.3 49.1
BN 66.2 58.4 64.8 49.0 38.2 65.2 53.8 BN 56.3 43.9 61.0 45.8 35.8 66.8 46.9
MZ 63.7 53.4 65.5 49.9 39.0 63.4 52.6 MZ 57.1 35.1 62.2 44.4 36.1 59.4 44.5
NW 62.4 53.6 64.3 48.0 37.2 62.7 51.7 NW 49.9 37.8 61.8 43.2 35.2 59.8 44.9
TC 76.9 74.4 62.0 43.3 33.2 58.1 56.5 TC 75.4 65.9 60.2 46.0 32.1 67.1 52.7
WB 70.0 63.0 60.1 43.3 31.8 60.8 51.6 WB 69.2 55.4 57.4 42.5 34.6 64.7 49.1
stoyanov kummerfield
BC 69.5 59.1 57.6 43.5 34.0 58.7 50.2 BC 66.4 41.5 55.6 41.7 36.2 57.9 44.4
BN 69.2 59.1 65.4 50.4 40.0 65.5 54.8 BN 68.3 48.2 63.4 51.7 44.7 61.6 52.1
MZ 66.7 55.1 65.5 51.0 39.9 63.7 53.5 MZ 58.0 39.9 65.8 51.0 43.4 64.1 49.7
NW 61.8 52.0 63.3 46.2 36.1 62.0 50.5 NW 55.2 41.3 64.7 46.8 37.0 63.5 47.6
TC 72.6 66.6 57.6 42.3 31.0 57.6 51.7 TC 61.8 34.5 51.5 34.7 30.0 54.1 38.7
WB 71.5 63.9 58.3 44.8 33.1 61.1 51.8 WB 68.2 48.1 56.0 44.4 38.6 59.6 47.6
sobha zhekova
BC 68.3 51.7 61.4 47.8 40.4 62.9 51.2 BC 50.5 23.8 60.6 39.4 35.1 53.4 39.8
BN 66.5 51.9 66.5 53.7 45.5 66.3 54.6 BN 51.2 26.0 62.4 42.5 37.5 54.3 42.0
MZ 68.8 54.9 70.3 58.9 49.3 69.8 58.1 MZ 44.0 22.6 63.4 43.3 37.3 56.0 41.1
NW 55.1 43.1 65.8 48.6 39.0 64.9 49.3 NW 39.7 19.4 62.8 41.0 35.8 53.7 39.3
TC 71.5 55.1 57.5 44.2 36.7 60.5 49.7 TC 59.4 31.6 58.2 37.7 33.6 54.1 41.1
WB 70.5 55.7 59.2 46.6 39.8 62.6 51.6 WB 54.1 27.8 58.7 38.5 34.7 53.0 40.4
kobdani irwin
BC 63.2 56.3 65.8 40.6 32.4 61.9 51.5 BC 23.5 16.1 46.0 29.4 23.6 49.8 28.6
BN 63.5 55.7 68.5 46.9 37.5 64.6 53.9 BN 24.9 20.0 49.7 34.2 27.1 52.9 32.3
MZ 57.5 52.2 69.8 45.7 36.4 61.7 52.8 MZ 23.2 17.9 55.9 36.2 28.5 53.0 34.1
NW 52.2 41.7 64.4 43.2 33.7 62.6 46.6 NW 27.5 21.6 56.4 33.9 27.3 52.6 35.1
TC 67.7 60.2 65.3 36.6 28.5 57.6 51.3 TC 28.0 19.3 38.2 24.5 18.7 49.0 25.4
WB 68.7 62.8 62.4 42.5 32.9 64.0 52.7 WB 33.6 24.8 47.6 29.7 23.0 50.2 31.8
Table 20: Detailed look at the performance per genre for the official, closed track using automatic performance. MD
represents MENTION DETECTION; BCUB represents B-CUBED; Cm represents CEAFm; Ce represents CEAFe and O
represents the OFFICIAL score.
20
res were treated here with perfect speech recognition
accuracy and perfect speaker turn information. Un-
der more realistic application conditions, the spread
in performance between genres might be greater.
MD MUC BCUB Cm Ce BLANC O
F F F F F F F
lee GENRE
BC 72.7 61.7 67.0 54.5 43.6 72.7 57.4
BN 72.0 60.6 69.4 57.9 48.1 70.3 59.3
MZ 69.9 58.4 72.1 61.2 50.1 75.2 60.2
NW 65.3 55.8 70.0 56.7 44.9 71.7 56.9
TC 76.6 68.4 70.4 59.6 40.8 82.1 59.9
WB 73.8 65.5 66.2 54.5 42.1 74.2 57.9
cai
BC 69.7 59.1 66.0 50.5 39.9 69.2 55.0
BN 68.6 57.6 67.8 55.4 45.5 68.2 56.9
MZ 64.0 51.1 69.5 55.9 45.6 71.2 55.4
NW 60.3 49.9 67.8 52.7 41.2 69.1 53.0
TC 75.6 70.5 72.2 59.6 38.0 80.3 60.2
WB 71.7 63.9 65.0 51.8 39.8 72.8 56.2
uryupina
BC 70.2 58.3 62.7 48.7 38.0 68.7 53.0
BN 69.0 57.6 66.8 53.6 43.1 69.2 55.8
MZ 65.7 52.4 68.3 54.3 43.6 68.8 54.8
NW 62.6 52.1 68.3 53.2 41.2 71.3 53.9
TC 75.7 67.1 61.0 50.7 34.6 67.1 54.2
WB 72.0 61.7 60.9 48.8 38.3 67.6 53.6
klenner
BC 63.2 50.3 63.4 48.2 38.9 66.8 50.8
BN 63.1 48.6 65.0 51.0 42.6 66.0 52.1
MZ 59.1 43.7 67.1 52.9 45.3 65.0 52.0
NW 55.3 41.3 65.0 48.0 39.6 64.5 48.7
TC 73.9 64.9 67.9 56.4 39.0 78.0 57.3
WB 66.8 58.1 64.0 50.1 39.6 72.7 53.9
irwin
BC 36.6 27.6 50.9 32.0 25.5 50.2 34.7
BN 30.8 24.6 51.9 36.4 28.6 54.8 35.0
MZ 26.1 20.0 57.3 37.6 29.4 54.3 35.6
NW 32.3 24.7 58.4 34.7 27.9 51.1 37.0
TC 46.4 34.3 44.6 29.4 21.9 51.7 33.6
WB 41.7 32.9 50.5 32.9 25.1 53.2 36.2
Table 21: Detailed look at the performance per genre for
the official, open track using predicted information. MD
represents MENTION DETECTION; BCUB represents B-
CUBED; Cm represents CEAFm; Ce represents CEAFe and
O represents the OFFICIAL score.
6 Approaches
Tables 22 and 23 summarize the approaches of the
participating systems along with some of the impor-
tant dimensions.
Most of the systems broke the problem into two
phases, first identifying the potential mentions in the
text and then linking the mentions to form corefer-
ence chains. Most participants also used rule-based
approaches for mention detection, though two did
use trained models. While trained morels seem able
to better balance precision and recall, and thus to
achieve a higher F-score on the mention task itself,
their recall tends to be quite a bit lower than that
achievable by rule-based systems designed to fa-
vor recall. This impacts coreference scores because
the full coreference system has no way to recover
if the mention detection stage misses a potentially
anaphoric mention.
Only one of the participating systems cai at-
tempted to do joint mention detection and corefer-
ence resolution. While it did not happen to be among
the top-performing systems, the difference in perfor-
mance could be due to the richer features used by
other systems rather than to the use of a joint model.
Most systems represented the markable mentions
internally in terms of the parse tree NP constituent
span, but some systems used shared attribute mod-
els, where the attributes of the merged entity are
determined collectively by heuristically merging the
attribute types and values of the different constituent
mentions.
Various types of trained models were used for pre-
dicting coreference. It is interesting to note that
some of the systems, including the best-performing
one, used a completely rule-based approach even for
this component.
Most participants appear not to have focused
much on eventive coreference, those coreference
chains that build off verbs in the data. This usu-
ally meant that mentions that should have linked to
the eventive verb were instead linked in with some
other entity. Participants may have chosen not to fo-
cus on events because they pose unique challenges
while making up only a small portion of the data.
Roughly 91% of mentions in the data are NPs and
pronouns.
In the systems that used trained models, many
systems used the approach described in Soon et al
(2001) for selecting the positive and negative train-
ing examples, while others used some of the al-
ternative approaches that have been introduced in
the research literature more recently. Many of the
trained systems also were able to improve their per-
formance by using feature selection, though things
varied some depending on the example selection
strategy and the classifier used. Almost half of the
trained systems used the feature selection strategy
from Soon et al (2001) and found it beneficial. It is
not clear whether the other systems did not explore
this path, or whether it just did not prove as useful in
their case.
7 Conclusions
In this paper we described the anaphoric coreference
information and other layers of annotation in the
21
Ta
sk
Sy
nta
x
Le
arn
ing
Fra
me
wo
rk
Ma
rka
ble
Ide
nti
fic
ati
on
Ma
rka
ble
Ve
rb
Fe
atu
re
Se
lec
tio
n
#F
eat
ure
s
Tra
ini
ng
lee
C+
O
P
Ru
le-
ba
sed
Ru
les
to
ex
clu
de
Co
pu
lar
co
nst
ruc
tio
n,
Ap
po
sit
ive
s,P
leo
na
sti
ci
t,e
tc.
Fe
atu
re
de
pe
nd
en
t
wi
th
sha
red
att
rib
ute
s
?
?
?
sap
en
a
C
P
De
cis
ion
Tre
e+
Re
lax
ati
on
La
be
lin
g
NP
(m
ax
im
al
spa
n)
+P
RP
+N
E
+C
ap
ita
liz
ed
no
un
he
uri
sti
c
Fu
llp
hra
se
?
?
Tra
in
+D
ev
ch
an
g
C
P
Le
arn
ing
Ba
sed
Jav
a
NP
,N
E,
PR
P,
PR
P$
Fu
llp
hra
se
?
?
Tra
in
+D
ev
cai
O
P
Co
mp
ute
hy
pe
red
ge
we
igh
tso
n
30
%
of
tra
ini
ng
da
ta
NP
,P
RP
,P
RP
$,
Ba
se
ph
ras
ec
hu
nk
s,
Ple
on
ast
ic
it
filt
er
Fu
llp
hra
se
?
?
?
nu
gu
es
C
D
Lo
gis
tic
Re
gre
ssi
on
(LI
BL
IN
EA
R)
NP
,P
RP
$a
nd
seq
ue
nc
eo
fN
NP
(s)
in
po
st
pro
ces
sin
gu
sin
gA
LI
AS
an
dS
TR
IN
GM
AT
CH
He
ad
wo
rd
?
Fo
rw
ard
+B
ack
wa
rd
sta
rtin
gf
rom
So
on
fea
tur
es
et
24
Tra
in
+D
ev
ury
up
ina
O
P
De
cis
ion
Tre
e.
Di
ffe
ren
t
cla
ssi
fie
rs
for
Pro
no
mi
na
la
nd
no
n-P
ron
om
ina
lm
en
tio
ns
NP
,N
E,
PR
P,
PR
P$
,a
nd
rul
es
to
ex
clu
de
som
es
pe
cifi
cc
ase
s
Fu
llp
hra
se
?
Mu
lti-
Ob
jec
tiv
e
Op
tim
iza
tio
no
nt
hre
e
spl
its
.N
SG
A-
II
46
Tra
in
+D
ev
san
tos
C
P
ET
L
(E
ntr
op
yg
uid
ed
Tra
nsf
orm
ati
on
al
Le
arn
ing
)
co
mm
itte
ea
nd
Ra
nd
om
Fo
res
t
(W
EK
A)
Al
lN
Pa
nd
all
pro
no
un
sa
nd
PE
R,
OR
G,
GP
E
in
NP
Fu
llp
hra
se
?
Inh
ere
nt
to
the
cla
ssi
fie
rs
Tra
in
+D
ev
son
g
C
P
Ma
xE
nt
(O
pe
nN
LP
)
Me
nti
on
de
tec
tio
nc
las
sifi
er
Fu
llp
hra
se
?
Sa
me
fea
tur
es
et,
bu
t
pe
rc
las
sifi
er
40
Tra
in
sto
ya
no
v
C
P
Av
era
ge
dp
erc
ep
tro
n
NE
an
dp
oss
ess
ive
sin
ad
dit
ion
to
AC
E
ba
sed
sys
tem
Fu
llp
hra
se
?
?
76
?
sob
ha
C
P
CR
Ff
or
no
n-p
ron
om
ina
la
nd
sal
ien
ce
fac
tor
for
pro
no
mi
na
l
res
olu
tio
n
Ma
ch
ine
lea
rne
dp
leo
na
sti
ci
t,p
lus
NP
,P
RP
,
PR
P$
an
dN
E
Mi
nim
al
(C
hu
nk
/N
E)
an
dM
ax
im
um
spa
n
?
?
Tra
in
kle
nn
er
O
D
Ru
le-
ba
sed
.S
ali
en
ce
me
asu
re
usi
ng
de
pe
nd
en
cie
sg
en
era
ted
fro
m
tra
ini
ng
da
ta
NP
,N
E,
PR
P,
PR
P$
Sh
are
d
att
rib
ute
d/t
ran
sit
ivi
ty
by
usi
ng
av
irtu
al
pro
tot
yp
e
?
?
?
ko
bd
an
i
C
P
De
cis
ion
Tre
e
NP
(no
me
nti
on
of
PR
P$
)
Sta
rtw
ord
,E
nd
wo
rd
an
dH
ead
of
NP
?
Inf
orm
ati
on
ga
in
rat
io
Tra
in
zh
ou
C
P
SV
M
tre
ek
ern
el
usi
ng
BC
po
rtio
n
of
the
da
ta
Ru
le-
ba
sed
;F
ive
rul
es:
PR
P$
,P
RP
,N
E,
sm
all
est
NP
sub
sum
ing
NE
an
dD
ET
+N
P
Fu
llp
hra
se
?
?
17
Tra
in
+D
ev
ch
art
on
C
P
Mu
lti-
lay
er
pe
rce
ptr
on
Ru
les
ba
sed
on
PO
S,
NE
an
dfi
lte
ro
ut
ple
on
ast
ic
it
usi
ng
rul
e-b
ase
dfi
lte
r
Fu
llp
hra
se
?
?
22
Tra
in
ya
ng
C
P
Ma
xE
nt
(M
AL
LE
T)
NP
,P
RP
,P
RP
$,
pre
-m
od
ifie
rs
an
dv
erb
s
Fu
llp
hra
se
?
?
40
Tra
in
+D
ev
ha
o
C
P
Ma
xE
nt
NP
,P
RP
,P
RP
$,
VB
D
ful
lp
hra
se
?
?
Tra
in
+D
ev
xin
xin
C
P
IL
P/I
nfo
rm
ati
on
ga
in
NP
,P
RP
,P
RP
$
Fu
llp
hra
se
?
Inf
orm
ati
on
ga
in
rat
io
65
?
zh
an
g
C
P
SV
M
IO
B
cla
ssi
fic
ati
on
Fu
llp
hra
se
?
?
?
ku
mm
erfi
eld
C
P
Un
sup
erv
ise
dg
en
era
tiv
em
od
el
NP
,P
RP
,P
RP
$w
ith
ma
xim
al
spa
n
Fu
llp
hra
se
?
?
?
zh
ek
ov
a
C
P
TI
M
BL
me
mo
ry
ba
sed
lea
rne
r
NP
,P
rop
er
no
un
s,P
RP
,P
RP
$,
plu
sv
erb
wi
th
pre
dic
ate
lem
ma
He
ad
wo
rd
?
?
Tra
in
+D
ev
irw
in
C+
O
P
Cl
ass
ific
ati
on
-ba
sed
ran
ke
r
NP
,P
RP
,P
RP
$
Sh
are
da
ttri
bu
tes
?
?
?
Ta
ble
22
:P
art
ici
pa
tin
gs
yst
em
pro
file
s?
Pa
rt
I.I
nt
he
Ta
sk
co
lum
n,
C/
O
rep
res
en
ts
wh
eth
er
the
sys
tem
pa
rtic
ipa
ted
in
the
cl
os
ed
,o
pe
n
or
bo
th
tra
ck
s.
In
the
Sy
nta
xc
olu
mn
,a
Pr
ep
res
en
ts
tha
tth
es
yst
em
su
sed
ap
hra
se
str
uc
tur
eg
ram
ma
rr
ep
res
en
tat
ion
of
syn
tax
,w
he
rea
sa
D
rep
res
en
ts
tha
tth
ey
use
da
de
pe
nd
en
cy
rep
res
en
tat
ion
.
22
Po
sit
ive
Tra
ini
ng
Ex
am
ple
s
Ne
ga
tiv
eT
rai
nin
gE
xa
mp
les
De
co
din
g
Pa
rse
Co
nfi
gu
rat
ion
lee
?
?
Mu
lti-
pa
ss
Sie
ve
s
sap
en
a
Al
lm
en
tio
np
air
sa
nd
lon
ge
ro
fn
est
ed
me
nti
on
sw
ith
co
mm
on
he
ad
ke
pt
Me
nti
on
pa
irs
wi
th
les
sth
an
thr
esh
old
(5)
nu
mb
er
of
dif
fer
en
ta
ttri
bu
te
val
ue
sa
re
co
nsi
de
red
(22
%
ou
to
f9
9%
ori
gin
al
are
dis
car
de
d)
Ite
rat
ive
1-b
est
ch
an
g
Cl
ose
sta
nte
ced
en
t
Al
lp
rec
ed
ing
me
nti
on
sin
au
nio
no
fo
fg
ol
d
an
dp
re
di
ct
ed
me
nti
on
s.
Me
nti
on
sw
he
re
the
firs
tis
pro
no
un
an
do
the
rn
ot
are
no
t
co
nsi
de
red
Be
stl
ink
an
dA
lll
ink
ss
tra
teg
y;
wi
th
an
d
wi
tho
ut
co
nst
rai
nts
?B
est
lin
kw
ith
ou
t
co
nst
rai
nts
wa
ss
ele
cte
df
or
the
offi
cia
lru
n
cai
We
igh
tsa
re
tra
ine
do
np
art
of
the
tra
ini
ng
da
ta
Re
cu
rsi
ve
2-w
ay
Sp
ect
ral
clu
ste
rin
g
(A
ga
rw
al,
20
05
)
nu
gu
es
Cl
ose
stA
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Cl
ose
st-
firs
tc
lus
ter
ing
for
pro
no
un
sa
nd
Be
st-
firs
tc
lus
ter
ing
for
no
n-p
ron
ou
ns
1-b
est
ury
up
ina
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
me
nti
on
pa
irm
od
el
wi
tho
ut
ran
kin
ga
sin
So
on
20
01
san
tos
Ex
ten
de
dv
ers
ion
of
So
on
(20
01
)w
he
re
in
ad
dit
ion
to
the
irs
tra
teg
y,
po
sit
ive
an
dn
ega
tiv
e
ex
am
ple
sf
rom
me
nti
on
sin
the
sen
ten
ce
of
the
clo
ses
tp
rec
ed
ing
an
tec
ed
en
ta
re
co
nsi
de
red
Lim
ite
dn
um
be
ro
fp
rec
ed
ing
me
nti
on
s6
0
for
au
tom
ati
ca
nd
40
giv
en
go
ld
bo
un
da
rie
s;
Ag
gre
ssi
ve
-m
erg
ec
lus
ter
ing
(M
cca
rth
ya
nd
Le
nh
ert
,1
99
5)
son
g
Pre
-cl
ust
er
pa
irm
od
els
sep
ara
te
for
eac
hp
air
NP
-N
P,
NP
-PR
Pa
nd
PR
P-P
RP
Pre
-cl
ust
ers
,w
ith
sin
gle
ton
pro
no
un
pre
-cl
ust
ers
,a
nd
use
clo
ses
t-fi
rst
clu
ste
rin
g.
Di
ffe
ren
tli
nk
mo
de
lsb
ase
do
nt
he
typ
eo
f
lin
kin
gm
en
tio
ns
?N
P-P
RP
,P
RP
-PR
Pa
nd
NP
-N
P
sto
ya
no
v
Sm
art
Pa
irG
en
era
tio
n(
Sm
art
PG
)w
he
re
the
typ
eo
fa
nte
ced
en
tis
de
ter
mi
ne
db
yt
he
typ
eo
f
an
ap
ho
ru
sin
ga
set
of
rul
es
Sin
gle
-lin
kc
lus
ter
ing
by
co
mp
uti
ng
tra
nsi
tiv
ec
los
ure
be
tw
een
pa
irw
ise
po
sit
ive
s.
sob
ha
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Pro
no
mi
na
l:a
llp
rec
ed
ing
NP
sin
the
sen
ten
ce
an
dp
rec
ed
ing
4s
en
ten
ces
kle
nn
er
?
?
Inc
rem
en
tal
en
tity
cre
ati
on
ko
bd
an
i
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
st-
firs
tc
lus
ter
ing
.T
hre
sho
ld
of
10
0w
ord
s
use
df
or
lon
gd
oc
um
en
ts
1-b
est
zh
ou
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
?
ch
art
on
Fro
m
the
en
do
fth
ed
oc
um
en
t,u
nti
la
n
an
tec
ed
en
tis
fou
nd
,o
r1
0m
en
tio
ns
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t
ML
Pw
ith
sco
re
of
0.5
use
df
or
lin
kin
ga
nd
10
me
nti
on
s
ya
ng
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Ma
xim
um
23
sen
ten
ces
to
the
lef
t;
Co
nst
rai
ne
dc
lus
ter
ing
ha
o
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
am
sea
rch
(L
uo
,2
00
4)
Pa
ck
ed
for
est
xin
xin
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
st-
firs
tc
lus
ter
ing
fol
low
ed
by
IL
P
op
tim
iza
tio
n
zh
an
g
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Wi
nd
ow
of
10
0m
ark
ab
les
ku
mm
erfi
eld
?
?
Pre
-a
nd
po
st-
res
olu
tio
nfi
lte
rs
Gi
ve
n+
Be
rke
ley
pa
rse
rp
ars
es;
pa
rse
s
wi
tho
ut
NM
Ls
im
pro
ve
dp
erf
orm
an
ce
sli
gh
tly
;re
-tr
ain
ed
Be
rke
ley
pa
rse
r
zh
ek
ov
a
Ex
am
ple
sin
the
pa
stt
hre
es
en
ten
ces
Fro
m
las
tp
oss
ibl
em
en
tio
ni
nd
oc
um
en
t
irw
in
Cl
ust
er
qu
ery
wi
th
NU
LL
clu
ste
rfo
rd
isc
ou
rse
new
me
nti
on
s
Cl
ust
er-
ran
kin
ga
pp
roa
ch
(ra
hm
an
,2
00
9)
Ta
ble
23
:P
art
ici
pa
tin
gs
yst
em
pro
file
s?
Pa
rtI
I.T
his
foc
use
so
nt
he
wa
yp
osi
tiv
ea
nd
ne
ga
tiv
ee
xa
mp
les
we
re
ge
ne
rat
ed
an
dt
he
de
co
din
gs
tra
teg
yu
sed
.
23
OntoNotes corpus, and presented the results from an
evaluation on learning such unrestricted entities and
events in text. The following represent our conclu-
sions on reviewing the results:
? Perhaps the most surprising finding was that the
best-performing system (lee) was completely
rule-based, rather than trained. This suggests
that their rule-based approach was able to do
a more effective job of combining the multiple
sources of evidence than the trained systems.
The features for coreference prediction are cer-
tainly more complex than for many other lan-
guage processing tasks, which makes it more
challenging to generate effective feature com-
binations. The rule-based approach used by
the best-performing system seemed to benefit
from a heuristic that captured the most con-
fident links before considering less confident
ones, and also made use of the information in
the guidelines in a slightly more refined man-
ner than other systems. They also included ap-
positives and copular constructions in their cal-
culations. Although OntoNotes does not count
those as instances of IDENT coreference, using
that information may have helped their system
discover additional useful links.
? It is interesting to note that the developers of
the lee system also did the experiment of run-
ning their system using gold standard informa-
tion on the individual layers, rather than auto-
matic model predictions. The somewhat sur-
prising result was that using perfect informa-
tion for the other layers did not end up improv-
ing coreference performance much, if at all. It
is not clear whether this means that: i) Auto-
matic predictors for the individual layers are
accurate enough already; ii) Information cap-
tured by those supplementary layers actually
does not provide much leverage for resolving
coreference; or iii) researchers have yet have
found an effective way of capturing and utiliz-
ing the extra information provided by these lay-
ers.
? It does seem that collecting information about
an entity by merging information across the
various attributes of the mentions that comprise
it can be useful, though not all systems that at-
tempted this achieved a benefit.
? System performance did not seem to vary as
much across the different genres as is nor-
mally the case with language processing tasks,
which could suggest that coreference is rela-
tively genre insensitive, or it is possible that
scores are two low for the difference to be ap-
parent. Comparisons are difficult, however, be-
cause the spoken genres were treated here with
perfect speech recognition accuracy and perfect
speaker turn information. Under more realis-
tic application conditions, the spread in perfor-
mance between genres might be greater.
? It is noteworthy that systems did not seem to
attempt the kind of joint inference that could
make use of the full potential of various layers
available in OntoNotes, but this could well have
been owing to the limited time available for the
shared task.
? We had expected to see more attention paid to
event coreference, which is a novel feature in
this data, but again, given the time constraints
and given that events represent only a small
portion of the total, it is not surprising that most
systems chose not to focus on it.
? Scoring coreference seems to remain a signif-
icant challenge. There does not seem to be an
objective way to establish one metric in prefer-
ence to another in the absence of a specific ap-
plication. On the other hand, the system rank-
ings do not seem terribly sensitive to the par-
ticular metric chosen. It is interesting that both
versions of the CEAF metric ? which tries to
capture the goodness of the entities in the out-
put ? seem much lower than the other metric,
though it is not clear whether that means that
our systems are doing a poor job of creating
coherent entities or whether that metric is just
especially harsh.
Finally, it is interesting to note that the problem of
coreference does not seem to be following the same
kind of learning curve that we are used to with other
problems of this sort. While performance has im-
proved somewhat, it is not clear how far we will be
able to go given the strategies at hand, or whether
new techniques will be needed to capture additional
information from the texts or from world knowl-
edge. We hope that this corpus and task will provide
a useful resource for continued experimentation to
help resolve this issue.
Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
24
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022.
We would like to thank all the participants. Without
their hard work, patience and perseverance this eval-
uation would not have been a success. We would
also like to thank the Linguistic Data Consortium
for making the OntoNotes 4.0 corpus freely and
timely available to the participants. Emili Sapena,
who graciously allowed the use of his scorer
implementation, and made available enhancements
and immediately fixed issues that were uncovered
during the evaluation. Finally, we offer our special
thanks to Llu??s Ma`rquez and Joakim Nivre for their
wonderful support and guidance without which this
task would not have been successful.
References
Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Li-
bin Shen. 2006. Issues in synchronizing the English
treebank and propbank. In Workshop on Frontiers in
Linguistically Annotated Corpora 2006, July.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 33?40, Sydney,
Australia, July.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In Pro-
ceedings of the 11th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue, SIGDIAL
?10, pages 28?36.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the Second Meeting of North American Chapter
of the Association of Computational Linguistics, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL), Ann
Arbor, MI, June.
Nancy Chinchor and Beth Sundheim. 2003. Message
understanding conference (MUC) 6. In LDC2003T13.
Nancy Chinchor. 2001. Message understanding confer-
ence (MUC) 7. In LDC2001T02.
Aron Culotta, Michael Wick, Robert Hall, and Andrew
McCallum. 2007. First-order probabilistic models for
coreference resolution. In HLT/NAACL, pages 81?88.
Pascal Denis and Jason Baldridge. 2007. Joint de-
termination of anaphoricity and coreference resolu-
tion using integer programming. In Proceedings of
HLT/NAACL.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
(42):87?96.
Charles Fillmore, Christopher Johnson, and Miriam R. L.
Petruck. 2003. Background to framenet. Interna-
tional Journal of Lexicography, 16(3).
G. G. Doddington, A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassell, and R. Weischedel.
2000. The automatic content extraction (ACE)
program-tasks, data, and evaluation. In Proceedings
of LREC.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393, Los An-
geles, California, June.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2009): Shared Task, pages
1?18, Boulder, Colorado, June.
Sanda M. Harabagiu, Razvan C. Bunescu, and Steven J.
Maiorano. 2001. Text and knowledge mining for
coreference resolution. In NAACL.
L. Hirschman and N. Chinchor. 1997. Coreference task
definition (v3.0, 13 jul 97). In Proceedings of the Sev-
enth Message Understanding Conference.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% solution. In Proceedings of HLT/NAACL,
pages 57?60, New York City, USA, June. Association
for Computational Linguistics.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2000. A large-scale classification of
english verbs. Language Resources and Evaluation,
42(1):21 ? 40.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
25
Methods in Natural Language Processing, pages 25?
32, Vancouver, British Columbia, Canada, October.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313?330, June.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems (NIPS).
Joseph McCarthy and Wendy Lehnert. 1995. Using de-
cision trees for coreference resolution. In Proceedings
of the Fourteenth International Conference on Artifi-
cial Intelligence, pages 1050?1055.
Thomas S. Morton. 2000. Coreference for nlp applica-
tions. In Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics, Octo-
ber.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of the IJCAI.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396?1411, Uppsala, Swe-
den, July.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2007. Making fine-grained and coarse-grained
sense distinctions, both manually and automatically.
R. Passonneau. 2004. Computing reliability for corefer-
ence annotation. In Proceedings of LREC.
Massimo Poesio and Ron Artstein. 2005. The reliability
of anaphoric annotation, reconsidered: Taking ambi-
guity into account. In Proceedings of the Workshop on
Frontiers in Corpus Annotations II: Pie in the Sky.
Massimo Poesio. 2004. The mate/gnome scheme for
anaphoric annotation, revisited. In Proceedings of
SIGDIAL.
Simone Paolo Ponzetto and Massimo Poesio. 2009.
State-of-the-art nlp approaches to coreference resolu-
tion: Theory and practical recipes. In Tutorial Ab-
stracts of ACL-IJCNLP 2009, page 6, Suntec, Singa-
pore, August.
Simone Paolo Ponzetto and Michael Strube. 2005. Se-
mantic role labeling for coreference resolution. In
Companion Volume of the Proceedings of the 11th
Meeting of the European Chapter of the Associa-
tion for Computational Linguistics, pages 143?146,
Trento, Italy, April.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of the HLT/NAACL, pages 192?199, New York City,
N.Y., June.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Dan Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning Journal, 60(1):11?39.
Sameer Pradhan, Eduard Hovy, Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph Weischedel.
2007a. OntoNotes: A Unified Relational Semantic
Representation. International Journal of Semantic
Computing, 1(4):405?419.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007b.
Unrestricted Coreference: Indentifying Entities and
Events in OntoNotes. In in Proceedings of the
IEEE International Conference on Semantic Comput-
ing (ICSC), September 17-19.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977, Singapore, Au-
gust. Association for Computational Linguistics.
W. M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. Journal of the American Statis-
tical Association, 66(336).
Marta Recasens and Eduard Hovy. 2011. Blanc: Im-
plementing the rand index for coreference evaluation.
Natural Language Engineering.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 1?8,
Uppsala, Sweden, July.
W. Soon, H. Ng, and D. Lim. 2001. A machine learn-
ing approach to coreference resolution of noun phrase.
Computational Linguistics, 27(4):521?544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 656?664, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
26
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159?177, Manchester,
England, August.
Yannick Versley. 2007. Antecedent selection techniques
for high-recall coreference resolution. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model theoretic coreference
scoring scheme. In Proceedings of the Sixth Message
Undersatnding Conference (MUC-6), pages 45?52.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus LDC catalog
no.: LDC2005T33. BBN Technologies.
Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch
Marcus, Robert Belvin, Sameer Pradhan, Lance
Ramshaw, and Nianwen Xue. 2011. OntoNotes: A
Large Training Corpus for Enhanced Processing. In
Joseph Olive, Caitlin Christianson, and John McCary,
editors, Handbook of Natural Language Processing
and Machine Translation. Springer.
27
