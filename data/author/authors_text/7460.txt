Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 41?48, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Error Detection Using Linguistic Features
Yongmei Shi
Department of Computer Science and
Electrical Engineering
University of Maryland Baltimore County
Baltimore, MD 21250
yshi1@umbc.edu
Lina Zhou
Information Systems Department
University of Maryland Baltimore County
Baltimore, MD 21250
zhoul@umbc.edu
Abstract
Recognition errors hinder the prolifera-
tion of speech recognition (SR) systems.
Based on the observation that recogni-
tion errors may result in ungrammatical
sentences, especially in dictation appli-
cation where an acceptable level of ac-
curacy of generated documents is indis-
pensable, we propose to incorporate two
kinds of linguistic features into error de-
tection: lexical features of words, and syn-
tactic features from a robust lexicalized
parser. Transformation-based learning is
chosen to predict recognition errors by in-
tegrating word confidence scores with lin-
guistic features. The experimental results
on a dictation data corpus show that lin-
guistic features alone are not as useful as
word confidence scores in detecting er-
rors. However, linguistic features provide
complementary information when com-
bined with word confidence scores, which
collectively reduce the classification error
rate by 12.30% and improve the F measure
by 53.62%.
1 Introduction
The proliferation of speech recognition (SR) sys-
tems is hampered by the ever-presence of recogni-
tion errors and the significant amount of effort in-
volved in error correction. A user study (Sears et al,
2001) showed that users spent one-third of their time
finding and locating errors and another one-third of
the time correcting errors in a hand-free dictation
task. Successfully detecting SR errors can speed up
the entire process of error correction. Therefore, we
focus on error detection in this study.
A common approach to detecting SR errors is an-
notating confidence at the word level. The major-
ity of confidence annotation methods are based on
feature combination, which follows two steps: (i)
extract useful features characteristics of the correct-
ness of words either from the inner components of
an SR system (SR-dependent features) or from the
recognition output (SR-independent features); and
(ii) develop a binary classifier to separate words into
two groups: correct recognitions and errors.
Various features extracted from different compo-
nents of an SR system, such as the acoustic model,
the language model, and the decoder, have been
proven useful to detecting recognition errors (Chase,
1997; Pao et al, 1998; San-Segundo et al, 2001).
Nonetheless, merely using these features is inade-
quate, because the information conveyed by these
features has already been considered when SR sys-
tems generate the output. A common observation is
that the combination of SR-dependent features can
only marginally improve the performance achieved
by using only the best single feature (Zhang and
Rudnicky, 2001; Sarikaya et al, 2003). Hence in-
formation sources beyond the SR system are desired
in error detection.
High-level linguistic knowledge is a good candi-
date for additional information sources. It can be
extracted from the SR output via natural language
processing, which compensates for the lack of high-
41
level linguistic knowledge in a typical SR system.
A user study (Brill et al, 1998) showed that hu-
mans can utilize linguistic knowledge at various lev-
els to improve the SR output by selecting the best
utterance hypotheses from N-best lists. Linguistic
features from syntactic, semantic, and dialogue dis-
course analyses have proven their values in error de-
tection in domain specific spoken dialogue systems,
e.g. (Rayner et al, 1994; Carpenter et al, 2001;
Sarikaya et al, 2003). However, few studies have in-
vestigated the merit of linguistic knowledge for error
detection in dictation, a domain-independent appli-
cation.
Transformation-based learning (TBL) is a rule-
based learning method. It has been used in error
correction (Mangu and Padmanabhan, 2001) and er-
ror detection (Skantze and Edlund, 2004). The rules
learned by TBL show good interpretability as well
as good performance. Although statistical learning
methods have been widely used in confidence an-
notation (Carpenter et al, 2001; Pao et al, 1998;
Chase, 1997), their results are difficult to interpret.
Therefore, we select TBL to derive error patterns
from the SR output in this study.
The rest of the paper is organized as follows. In
Section 2, we review the extant work on utilizing lin-
guistic features in error detection. In Section 3, we
introduce linguistic features used in this study. In
Section 4, we describe transformation-based learn-
ing and define the transformations, followed with
reporting the experimental results in Section 5. Fi-
nally, we summarize the findings of this study and
suggest directions for further research in Section 6.
2 Related Work
When the output of an SR system is processed, the
entire utterance is available and thus utterance-level
contextual information can be utilized. Features
generated from high-level language processing such
as syntactic and semantic analyses may complement
the low-level language knowledge (usually n-gram)
used in the SR systems.
Most of the previous work on utilizing linguis-
tic features in error detection focused on utterance-
level confidence measures. Most of features were
extracted from the output of syntactic or semantic
parsers, including full/robust/no parse, number of
words parsed, gap number, slot number, grammar
rule used, and so on (Rayner et al, 1994; Pao et
al., 1998; Carpenter et al, 2001; San-Segundo et al,
2001). Some discourse-level features were also em-
ployed in spoken dialogue systems such as number
of turns, and dialog state (Carpenter et al, 2001).
Several studies incorporated linguistic features
into word-level confidence measures. Zhang and
Rudnicky (2001) selected two features, i.e., pars-
ing mode and slot backoff mode, extracted from the
parsing result of Phoenix, a semantic parser. The
above two features were combined with several SR-
dependent features using SVM, which achieved a
7.6% relative classification error rate reduction over
SR-dependent features on the data from CMU Com-
municator system.
Sarikaya et al (2003) explored two sets of seman-
tic features: one set from a statistical classer/parser,
and the other set from a maximum entropy based
semantic-structured language model. When com-
bined with the posterior probability using the deci-
sion tree, both sets achieved about 13-14% absolute
improvement on correct acceptance at 5% false ac-
ceptance over the baseline posterior probability on
the data from IBM Communicator system.
Skantze and Edlund (2004) focused on lexical
features (e.g., part-of-speech, syllables, and con-
tent words) and dialogue discourse features (e.g.,
previous dialogue act, and mentioned word), but
did not consider parser-based features. They em-
ployed transformation-based learning and instance-
based learning as classifiers. When combined with
confidence scores, the linguistic features achieved
7.8% absolute improvement in classification accu-
racy over confidence scores on one of their dialogue
corpora.
It is shown from the related work that linguis-
tic features have merit in judging the correctness
of words and/or utterances. However, such features
have only been discussed in the context of conver-
sational dialogue in specific domains such as ATIS
(Rayner et al, 1994), JUPITER (Pao et al, 1998),
and Communicator (Carpenter et al, 2001; San-
Segundo et al, 2001; Zhang and Rudnicky, 2001;
Sarikaya et al, 2003).
In an early study, we investigated the usefulness
of linguistic features in detecting word errors in dic-
tation recognition (Zhou et al, 2005). The linguis-
42
tic features were extracted from the parsing result
of the link grammar. The combination of linguis-
tic features with various confidence score based fea-
tures using SVM can improve F measure for error
detection from 42.2% to 55.3%, and classification
accuracy from 80.91% to 83.53%. However, parser-
based features used were limited to the number of
links that a word has.
3 Linguistic Features
For each output word, two sets of linguistic features
are extracted: lexical features and syntactic features.
3.1 Lexical Features
For each word w, the following lexical features are
extracted:
? word: w itself
? pos: part-of-speech tag from Brill?s tagger
(Brill, 1995)
? syllables: number of syllables in w, estimated
based on the distribution patterns of vowels and
consonants
? position: the position of w in the sentence: be-
ginning, end, and middle
3.2 Syntactic Features
Speech recognition errors may result in ungrammat-
ical sentences under the assumption that the speaker
follows grammar rules while speaking. Such an as-
sumption holds true especially for dictation appli-
cation because the general purpose of dictation is
to create understandable documents for communi-
cation.
Syntactic parsers are considered as the closest ap-
proximation to this intuition since there is still a lack
of semantic parsers for the general domain. More-
over, robust parsers are preferred so that an error
in a recognized sentence does not lead to failure in
parsing the entire sentence. Furthermore, lexicalized
parsers are desired to support error detection at the
word level. As a result, we select Link Grammar1 to
generate syntactic features.
1Available via http://www.link.cs.cmu.edu/link/
3.2.1 Link Grammar
Link Grammar is a context-free lexicalized gram-
mar without explicit constituents (Sleator and Tem-
perley, 1993). In link grammar, rules are expressed
as link requirements associated with words. A link
requirement is a set of disjuncts, each of which rep-
resents a possible usage of the word. A sequence of
words belongs to the grammar if the result linkage is
a planar, connected graph in which at most one link
is between each word pair and no cross link exists.
Link grammar supports robust parsing by incorpo-
rating null links (Grinberg et al, 1995).
3.2.2 Features from Link Grammar
We hypothesize that a word without any link in
a linkage of the sentence is a good indicator of
the occurrence of errors. Either the word itself
or words around it are likely to be erroneous. It
has been shown that null links can successfully ig-
nore false starts and connect grammatical phrases in
ungrammatical utterances, which are randomly se-
lected from the Switchboard corpus (Grinberg et al,
1995).
A word with links may still be an error, and
its correctness may affect the correctness of words
linked to it, especially those words connected with
the shortest links that indicate the closest connec-
tions.
Accordingly, for each word w, the following fea-
tures are extracted from the parsing result:
? haslink: whether w has left links, right links, or
no link
? llinkto/rlinkto: the word to which w links via
the shortest left/right link
An example of parsing results is illustrated in Fig-
ure 1. Links are represented with dotted lines which
are annotated with labels (e.g., Wd, Xp) represent-
ing link types. In Figure 1, word ?since? has no
link, and word ?around? has one left link and one
right link. The word that has the shortest left link to
?world? is ?the?.
43
LEFT-WALL [since] people.p will.v come.v from around the world.n .
Wd Sp I MVp FM Ds
Js
Xp
Figure 1: An Example of Parsing Results of Link Grammar
4 Error Detection based on
Transformation-Based Learning
4.1 Transformation-Based Learning
Transformation-Based Learning is a rule-based ap-
proach, in which rules are automatically learned
from the data corpus. It has been successfully used
in many natural language applications such as part-
of-speech tagging (Brill, 1995). Three prerequisites
for using TBL are: an initial state annotator, a set of
possible transformations, and an objective function
for choosing the best transformations.
Before learning, the initial state annotator adds la-
bels to the training data. The learning goes through
the following steps iteratively until no improvement
can be achieved: (i) try each possible transformation
on the training data, (ii) score each transformation
with the objective function and choose the one with
the highest score, and (iii) apply the selected trans-
formation to update the training data and append it
to the learned transformation list.
4.2 Error Detection Based on TBL
Pre-defined transformation templates are the rules
allowed to be used, which play a vital role in TBL.
The transformation templates are defined in the fol-
lowing format:
Change the word label of a word w from X to Y , if
condition C is satisfied
where, X and Y take binary values: 1 (correct
recognition) and -1 (error). Each condition C is the
conjunction of sub-conditions in form of f op v,
where f represents a feature, v is a possible cate-
gorical value of f , and op is the possible operations
such as <, > and =.
In addition to the linguistic features introduced in
Section 3, two other features are used:
? word confidence score (CS): an SR dependent
feature generated by an SR system.
? word label (label): the target of the transfor-
mation rules. Using it as a feature enables the
propagation of the effect of preceding rules.
As shown in Table 1, conditions are classified into
three categories based on the incrementally enlarged
context from which features are extracted: word
alone, local context, and sentence context. The three
categories are further split into seven groups accord-
ing to the features they used.
? L: the correctness of w depends solely on itself.
Conditions only include lexical features of w.
? Local: the correctness of w depends not only
on itself but also on its surrounding words.
Conditions incorporate lexical features of sur-
rounding words as well as those of w. Fur-
thermore, word labels of surrounding words are
also employed as a feature to capture the effect
of the correctness of surrounding words of w.
? Long: the scope of conditions for the correct-
ness of w is expanded to include syntactic fea-
tures. Syntactic features of w and its surround-
ing words as well as the features in Local are
incorporated into conditions. In addition, the
lexical features and word labels of words that
have the shortest links to w are also incorpo-
rated.
? CS: the group in which conditions only include
confidence scores of w.
? LCS, CSLocal, CSLong: these three groups
are generated by combining the features from
L, Local, and Long with the confidence scores
of w as an additional feature respectively.
lrHaslink and llinkLabel are combinations of
basic features. lrHaslink represents whether the
preceding word and the following word have links,
44
Category Group Example
Word CS cs(wi) < ci
Alone L position(wi) = ti & syllables(wi) = si
LCS cs(wi) < ci & pos(wi) = pi
Local Local position(wi) = ti & label(wi?1) = li?1 & word(wi) = di
Context CSLocal cs(wi) < ci & position(wi) = ti & label(wi?1) = li?1 & label(wi+1) =
li+1
Sentence Long position(wi) = ti & lrHaslink(wi) = hi & haslink(wi) = hli
Context CSLong cs(wi) < ci & position(wi) = ti & llinkLabel(wi) = lli & pos(wi) = pi
Table 1: Condition Categories and Examples
and llinkLabel represents the label of the word to
which w has the shortest left link. ci, ti, si, pi, li, di,
hi, hli, and lli are possible values of the correspond-
ing features.
The initial state annotator initializes all the words
as correct words. A Prolog based TBL tool, ?-
TBL (Lager, 1999) 2 is used in this study. Classi-
fication accuracy is adopted as the objective func-
tion. For each transformation, its positive effect
(PE) is the number of words whose labels are cor-
rectly updated by applying it, and its negative ef-
fect (NE) is the number of words wrongly updated.
Two cut-off thresholds are used to select transfor-
mations with strong positive effects: net positive ef-
fect (PE ? NE), and the ratio of positive effect
(PE/(PE +NE)).
5 Experimental Results and Discussion
Experiments were conducted at several levels. Start-
ing with transformation rules with word alone con-
ditions, additional rules with local context and sen-
tence context conditions were incorporated incre-
mentally by enlarging the scope of the context. As
such, the results help us not only identify the ad-
ditional contribution of each condition group to the
task of error detection but also reveal the importance
of enriching contextual information to error detec-
tion.
5.1 Data Corpus
The data corpus was collected from a user study
on a composition dictation task (Feng et al, 2003).
A total of 12 participants were native speakers and
2Available via http://www.ling.gu.se/?lager/mutbl.html
none of them used their voice for professional pur-
poses. Participants spoke to IBM ViaVoice (Millen-
nium edition), which contains a general vocabulary
of 64,000 words. The dictation task was completed
in a quiet lab environment with high quality micro-
phones.
During the study, participants were given one pre-
designed topic and instructed to compose a docu-
ment of around 400 words on that topic. Before
starting the dictation, they completed enrollments to
build personal profiles and received training on fin-
ishing the task with a different topic. They were
asked to make corrections only after they finished
composing a certain length of text. The data cor-
pus consists of the recognition output of their dicta-
tions excluding corrections. Word recognition errors
were first marked by the participants themselves and
then validated by researchers via cross-referencing
the recorded audios. The data corpus contains 4,804
words.
5.2 Evaluation Metrics
To evaluate the overall performance of the error de-
tection, classification error rate (CER) (Equation 1),
commonly used metric to evaluate classifiers, is
used. CER is the percentage of words that are
wrongly classified.
CER = # of wrongly classified wordstotal# of words (1)
The baseline CER is derived by assuming all the
words are correct, and it has the value as the ratio of
the total number of insertion and substitution errors
to the total number of output words.
Precision (PRE) and recall (REC) on errors are
used to measure the performance of identifying er-
45
rors. PRE is the percentage of words classified as er-
rors that are in fact recognition errors. REC denotes
the proportion of actual recognition errors that are
categorized as errors by the classifier. In addition,
F measure (Equation 2), a single-valued metric re-
flecting the trade-off between PRE and REC, is also
used. The baselines of PRE, REC, and F for error
are zeros, for all of the output words are assumed
correct.
F = 2 ? PRE ?RECPRE +REC (2)
5.3 Results
3-fold cross-validation was used to test the system.
When dividing the data corpus, sentence is treated
as an atomic unit. The 3-fold cross-validation was
run 9 times, and the average performance is reported
in Table 2. The labels of rule combinations are de-
fined by the connections of several symbols defined
in Section 4.2. For each rule combination, the types
of rules can be included are decided by all the possi-
ble combinations of those symbols which are in Ta-
ble 1. For example, L-CS-Local-Long includes rules
with conditions L, CS, Local, Long, LCS, CSLocal
and CSLong.
The threshold of net positive effect is set to 5 to
ensure that enough evidence has been observed, and
that of the ratio of the positive effect is set to 0.5 to
ensure that selected transformations have the posi-
tive effects.
For the combinations without CS, L-Local-Long
achieves the best performance in terms of both CER
and F measure. A relative improvement of 4.85% is
achieved over the baseline CER, which is relatively
small. One possible explanation concerns the large
vocabulary size in the data set. Although the par-
ticipants were asked to compose the documents on
the same topic, the word usage was greatly diversi-
fied. An analysis of the data corpus shows that the
vocabulary size is 993.
Despite its best performance in linguistic feature
groups, L-Local-Long produces worse performance
than CS in both CER and F measure. Therefore, lin-
guistic features by themselves are not as useful as
confidence scores.
When linguistic features are combined with
CS, they provide additional improvement. L-CS
achieves a 4.58% relative improvement on CER and
a 31.37% relative improvement on F measure over
CS. L-CS-Local only achieves marginal improve-
ment on CER and a 7.54% relative improvement on
F measure over L-CS.
The best performance is generated by L-CS-
Local-Long. In particular, it boosts CER by a rel-
ative improvement of 12.30% over CS and a relative
improvement of 7.02% over L-CS-Local. In addi-
tion, it improves F measure by 53.62% and 8.74%
in comparison with CS and L-CS-Local respectively.
Therefore, enlarging the scope of context can lead to
improved performance on error detection.
It is revealed from Table 2 that the improvement
on F measure is due to the improvement on re-
call without hurting the precision. After combining
linguistic features with CS, L-CS and L-CS-Local-
Long achieve 43.77% and 75.57% relative improve-
ments on recall over CS separately. Hence, the
linguistic features can improve the system?s ability
in finding more errors. Additionally, L-CS-Local-
Long achieves a 7.32% relative improvement on pre-
cision over CS.
The average numbers of learned rules are shown
in Table 2. With the increased number of possible
used pre-defined rules, the number of learned rules
increases moderately. L-CS-Local-Long and L-CS-
Local have the largest number of rules, 14, which is
rather a small set of rules. As discussed above, these
rules are straightforward and easy to understand.
Figure 2 shows CERs when the learned rules are
incrementally applied in one run for L-CS-Local-
Long. Three lines represent each of the three folds
separately, and the number of learned rules differs
among folds.
10
11
12
13
14
15
16
17
18
19
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
Number of Rules
CE
R(
%)
fold1
fold2
fold3
Figure 2: Relations of CERs with Number of Rules
46
Combination Mean Std. Mean Mean Mean Mean #
CER (%) Dev PRE (%) REC (%) F (%) of rules
Baseline 15.66 0.06 - - - -
L 15.55 0.11 61.85 2.04 3.88 3
L-Local 15.58 0.14 60.88 2.19 4.17 4
L-Local-Long 14.90 0.10 61.67 13.83 22.37 8
CS 14.64 0.09 61.03 21.98 31.50 1
L-CS 13.97 0.15 61.48 31.60 41.38 8
L-CS-Local 13.81 0.18 61.28 35.52 44.50 14
L-CS-Local-Long 12.84 0.21 65.50 38.59 48.39 14
Table 2: Performance of Transformation Rule Combinations
After the first several rules are applied, CERs drop
significantly. Then the changes in CERs become
marginal as additional rules are applied. The fold
1 and 3 reach the lowest CER after the last rule is
applied, and fold 2 reaches the lowest CERs in the
middle. Thus, the top ranked rules are mostly useful.
One advantage of TBL is that the learning result
can be easily interpreted. The following is the top
six rules learned in fold 3 in Figure 2.
Mark a word as an error, if :
? its confidence score is less than 0; it is in the
middle of a sentence; and it is a null-link word.
? its confidence score is less than -5; it is in the
middle of a sentence; and it has links to preced-
ing words.
? its confidence score is less than 0; it is the first
word of a sentence; and it is a null-link word.
? its confidence score is less than 2; it is in the
middle of a sentence; it has 1 syllable; and the
word following it also has 1 syllable and is an
error.
? its confidence score is less than -1; and both its
preceding and following words are errors.
Mark a word as a correct word, if :
? its confidence score is greater than -1; and both
its preceding and following words are correct
words.
All of the above six rules include word confidence
score as a feature. Rule 1 and rule 3 suggest that
null-link words are good indicators of errors, which
confirms our hypothesis. Rule 2 shows that a word
with low confidence score may also be an error even
if it is part of the linkage of the sentence. Rule 4
shows continuous short words are possible errors.
Rule 5 indicates that a word with low confidence
score may be an error if its surrounding words are er-
rors. Rule 6 is a rule to compensate for the wrongly
labeled words by previous rules.
6 Conclusion and Future Works
We introduced an error detection method based on
feature combinations. Transformation-based learn-
ing was used as the classifier to combine linguistic
features with word confidence scores. Two kinds
of linguistic features were selected: lexical fea-
tures extracted from words themselves, and syntac-
tic features from the parsing result of link grammar.
Transformation templates were defined by varying
scope of the context. Experimental results on a dic-
tation corpus showed that although linguistic fea-
tures alone were not as useful as word confidence
scores to error detection, they provided complemen-
tary information when combined with word confi-
dence score. Moreover, the performance of error de-
tection was improved incrementally as the scope of
context was enlarged, and the best performance was
achieved when sentence context was considered. In
particular, enlarging the context modeled by linguis-
tic features improved the capability of error detec-
tion by finding more errors without deteriorating and
even improving the precision.
The proposed method has been tested using a dic-
tation corpus on a topic related to office environ-
47
ment. We are working on evaluating the method
on spontaneous dictation utterances from the CSR-II
corpus, and other monologue corpora such as Broad-
cast News. The method can be extended by incorpo-
rating lexical semantic features from the semantic
analysis of recognition output to detect semantic er-
rors that are likely overlooked by syntactic analysis.
Acknowledgement
This work is supported by the National Science
Foundation under Grant# 0328391. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation (NSF).
References
Eric Brill, Radu Florian, John C. Henderson, and Lidia
Mangu. 1998. Beyond n-grams: Can linguistic so-
phistication improve language modeling? In Proceed-
ings of COLING/ACL, pages 186?190.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational Lin-
guistics, 21(4):543?565.
Paul Carpenter, Chun Jin, Daniel Wilson, Rong Zhang,
Dan Bohus, and Alex Rudnicky. 2001. Is this conver-
sation on track? In Proceedings of Eurospeech, pages
2121?2124.
Lin L. Chase. 1997. Error-Responsive Feedback Mecha-
nisms for Speech Recognizers. Ph.D. thesis, School of
Computer Science, CMU, April.
Jinjuan Feng, Andrew Sears, and Clare-Marie Karat.
2003. A longitudinal investigation of hands-free
speech based navigation during dictation. Technical
report, UMBC.
Dennis Grinberg, John Lafferty, and Daniel Sleator.
1995. A robust parsing algorithm for link grammars.
Technical Report CMU-CS-95-125, Carnegie Mellon
University.
Torbjo?rn Lager. 1999. The ?-tbl system: Logic program-
ming tools for transformation-based learning. In Pro-
ceedings of the third international workshop on com-
putational natural language learning.
Lidia Mangu and Mukund Padmanabhan. 2001. Error
corrective mechanisms for speech recognition. In Pro-
ceedings of ICASSP, volume 1, pages 29?32.
Christine Pao, Philipp Schmid, and James Glass. 1998.
Confidence scoring for speech understanding systems.
In Proceedings of ICSLP, pages 815?818.
Manny Rayner, David Carter, Vassilios Digalakis, and
Patti Price. 1994. Combining knowledge sources to
reorder n-best speech hypothesis lists. In Proceedings
of the ARPA Workshop on Human Language Technol-
ogy, pages 212?217.
Rube?n San-Segundo, Bryan Pellom, Kadri Hacioglu, and
Wayne Ward. 2001. Confidence measures for spo-
ken dialogue systems. In Proceedings of ICASSP, vol-
ume 1, pages 393?396.
Ruhi Sarikaya, Yuqing Gao, and Michael Picheny. 2003.
Word level confidence measurement using semantic
features. In Proceedings of ICASSP, volume 1, pages
604?607.
Andrew Sears, Clare-Marie Karat, Kwesi Oseitutu, Az-
far S. Karimullah, and Jinjuan Feng. 2001. Productiv-
ity, satisfaction, and interaction strategies of individ-
uals with spinal cord injuries and traditional users in-
teracting with speech recognition software. Universal
Access in the Information Society, 1(1):4?15, June.
Gabriel Skantze and Jens Edlund. 2004. Early error de-
tection on word level. In Proceedings of Robust.
Daniel Sleator and Davy Temperley. 1993. Parsing eng-
lish with a link grammar. In Proceedings of the third
international workshop on parsing technologies.
Rong Zhang and Alexander I. Rudnicky. 2001. Word
level confidence annotation using combinations of fea-
tures. In Proceedings of Eurospeech, pages 2105?
2108.
Lina Zhou, Yongmei Shi, Jinjuan Feng, and Andrew
Sears. 2005. Data mining for detecting errors in dicta-
tion speech recognition. IEEE Transactions on Speech
and Audio Processing, Special Issues on Data Mining
of Speech, Audio and Dialog, 13(5), September.
48
Proceedings of NAACL HLT 2007, Companion Volume, pages 157?160,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Are Some Speech Recognition Errors
Easier to Detect than Others?
Yongmei Shi
Department of Computer Science
and Electrical Engineering
University of Maryland, Baltimore County
Baltimore, MD 21250
yshi1@umbc.edu
Lina Zhou
Department of Information Systems
University of Maryland, Baltimore County
Baltimore, MD 21250
zhoul@umbc.edu
Abstract
This study investigates whether some
speech recognition (SR) errors are eas-
ier to detect and what patterns can be
identified from those errors. Specifically,
SR errors were examined from both non-
linguistic and linguistic perspectives. The
analyses of non-linguistic properties re-
vealed that high error ratios and consecu-
tive errors lowered the ease of error detec-
tion. The analyses of linguistic properties
showed that ease of error detection was as-
sociated with changing parts-of-speech of
reference words in SR errors. Addition-
ally, syntactic relations themselves and the
change of syntactic relations had impact
on the ease of error detection.
1 Introduction
Speech recognition (SR) errors remain as one of the
main impediment factors to the wide adoption of
speech technology, especially for continuous large-
vocabulary SR applications. As a result, lowering
word error rate is the focus of SR research which
can benefit from analyzing SR errors. SR errors
have been examined from various perspectives: lin-
guistic regularity of errors (McKoskey and Boley,
2000), the relationships between linguistic factors
and SR performance (Greenberg and Chang, 2000),
and the associations of prosodic features with SR er-
rors (Hirschberg et al, 2004). However, little is un-
derstood about patterns of errors with regard to ease
of detection.
Analyzing SR errors can be helpful to error detec-
tion. Skantze and Edlund (2004) conducted a user
study to evaluate the effects of various features on
error detection. Our study is different in that it in-
vestigates the relationships between the characteris-
tics of SR errors and their ease of detection through
an empirical user study. Given two SR systems with
the same word error rates, the output of one system
could be more useful if its errors are easier to detect
than the other. Accordingly, SR and its error de-
tection research could focus on addressing difficult
errors by developing automatic solutions or by pro-
viding decision support to manual error detection.
2 Experiment
A laboratory experiment was carried out to evaluate
humans? performance in SR error detection.
2.1 Experimental Data
Speech transcripts were extracted from a dictation
corpus on daily correspondence in office environ-
ment generated using IBM ViaVoice under high-
quality condition (Zhou et al, 2006).
Eight paragraphs were randomly selected from
the transcripts of two task scenarios based on two
criteria: recognition accuracy and paragraph length
(measured by # of words). Specifically, the over-
all recognition accuracy (84%) and the length of a
medium-sized paragraph (90 words) of the corpus
were used as references.
The selected paragraphs consist of 36 sentences.
Sentence lengths range from 9 to 38 words, with an
average of 20. For error detection, SR output in-
stead of references is a better base for computing
157
error rates because SR output but not reference tran-
scripts are accessible during error detection. This
may result in fewer number of deletion errors be-
cause when one SR error maps to several reference
words, it is counted as one substitution error. Based
on this method, there are totally 140 errors in the
selected data: 104 substitution, 31 insertion, and 5
deletion errors. The error ratio, defined as the ratio
of the number of errors to the number of words in
output sentence, ranges from 4.76% to 61.54%.
2.2 Task and Procedure
Participants were required to read error annotation
schema and sample transcripts prior to the exper-
iment, and could attend the experiment only af-
ter they passed the test on their knowledge of the
schema and SR output.
Each participant was asked to detect errors in all
eight paragraphs. All sentences in the same para-
graphs were presented all at once. The paragraphs
were presented with different methods, including
three with no additional information, three with al-
ternative hypotheses, and two with both dictation
scenario and alternative hypotheses. The sequence
of paragraphs and their presentation methods were
randomized for each participant.
Ten participants from a mid-sized university in
the U.S. completed the study. They were all native
speakers and none of them was professional editor.
3 Analysis and Discussion
In this section, we analyze the relationship between
characteristics of SR errors and ease of error detec-
tion. We characterize errors with non-linguistic and
linguistic properties and further break down the lat-
ter into parts-of-speech and syntactic relations.
3.1 Ease of Error Detection
The ease of detecting an error was defined as the
number of participants who successfully detected
the error. When computing the ease of error detec-
tion, we merged all the data by ignoring the presen-
tation methods. The decision was made because a
repeated measure ANOVA of recall failed to yield
a significant effect of presentation methods (p =
n.s.). The recall was selected because it measures
the percentage of actual errors being detected and
the focal interest of this study was actual errors. The
average recalls of error detection of three presenta-
tion methods were very close, ranging from 72% to
75%.
The ease values fell between 0 and 10, with 0 be-
ing the least ease when all participants missed the er-
ror and 10 being the most ease when everyone found
the error. To improve the power of statistical analy-
ses, errors were separated into 3 groups using equal-
height binning based on their ease values, namely 1
for low, 2 for medium, and 3 for high (see Table 1).
The overall average ease value was 2.15.
Level of Ease Ease Values # of Errors
Low (1) 0-5 39
Medium (2) 6-8 41
High (3) 9-10 60
Table 1: Grouping of ease values
3.2 Non-linguistic Error Properties
Three non-linguistic error properties, including error
ratio, word error type, and error sequence (in isola-
tion or next to other errors) were selected to examine
their relationships with ease of error detection.
Two-tailed correlation analyses of error ratio and
ease of detection showed that the Pearson correla-
tion coefficient was -0.477 (p < 0.01), which sug-
gests that it is easier to detect errors in sentences
with lower error ratios.
One way ANOVA failed to yield a significant ef-
fect of error type on ease of detection (p = n.s.).
Nonetheless, mean comparisons showed that inser-
tion errors were less easy to detect (mean = 2.03)
than deletion errors (mean = 2.20) and substitution
errors (mean = 2.18). Users may have difficulty in
judging extra words.
Among the 140 errors, about half of them (i.e., 71)
were next to some other errors. One way ANOVA
revealed a significant effect of error sequence on
ease of detection, p < 0.05. Specifically, isolated
errors (mean = 2.33) are easier to detect than con-
secutive errors (mean = 1.97).
3.3 Part-Of-Speech(POS)
SR output and reference transcripts were analyzed
using Brill?s part-of-speech tagger (Brill, 1995). To
alleviate data sparsity problem, we adopted second-
158
level tags such as NN and VB. The POSes of SR
errors as well as POS change patterns between ref-
erence words and SR errors were analyzed.
Table 2 reports the average eases of detection for
difference POSes on all the errors, substitution er-
rors only, and insertion errors only. Deletion errors
were not included because they did not appear in SR
output. Only those POSes with frequency of at least
10 in all the errors were selected.
POS All Substitution Insertion
NN 2.03 2.00 2.25
VB 2.30 2.41 1.67
CC 2.21 2.38 1.83
IN 2.22 2.27 2.00
DT 1.80 2.25 1.50
Table 2: Ease of detection for different POSes
It was easier to detect verbs that were misplaced
than verbs that were inserted mistakenly (p < 0.1
in one-tailed results). This is because an additional
verb may change syntactic and semantic structures
of entire sentence. Similar patterns held for both CC
and DT (p < 0.1 in one-tailed results). The less
ease in detecting DT and CC when they were in-
serted than replaced is due in part to the fact that
they play significant syntactic roles in constructing
a grammatical sentence. Further, ease of detecting
DT was lower than the average ease of all errors
(p < 0.1 in one-tailed results).
Only substitution errors were applicable in POS
change analysis. POS change was set to ?Y? when
the POSes of an SR error and its corresponding ref-
erence word were different, and ?N? when otherwise.
This resulted in 69 Ys and 35 Ns. One way ANOVA
results yielded a significant effect of POS change
on ease of detection (p < 0.05). Specifically, it
was easier to detect errors that had different POSes
(mean = 2.32) from their references than those that
shared the same POSes (mean = 1.91). This is
partly due to the requirements of semantic and even
discourse information in detecting errors from the
same POSes.
3.4 Syntactic Relations
Both SR output and reference transcripts were
parsed using minipar (Lin, 1998), a principle-based
parser that can generate a dependency parse tree for
each sentence. The dependency relations between
SR errors and other words in the same sentence were
extracted as the syntactic relations of SR errors. The
same kinds of relations were also extracted for cor-
responding reference words.
Three types of properties of syntactic relations
were analyzed, including the number of syntactic
relations, syntactic relation change, and errors? pat-
terns of syntactic relations.
Table 3 reports descriptive statistics of ease of
detection for SR errors with varying numbers of
syntactic relations. The average number of syntac-
tic relations for all errors was 1.64. Analysis re-
sults showed that it was easier to detect errors with
no syntactic relations than those with one relation
(p < 0.05). The analysis of correlation between the
number of syntactic relations and the ease of detec-
tion yielded a very small Pearson correlation coef-
ficient (p = n.s.). They suggest that errors that do
not fit into a sentence are easy to detect. However,
increasing the number of syntactic relations does not
lower the ease of detection.
# of Syntactic Mean Std Frequency
Relations Deviation
0 2.40 0.695 35
1 1.98 0.883 51
2 2.21 0.918 19
3 2.00 0.791 17
> 3 2.22 0.808 18
Table 3: Ease of detection for numbers of relations
Same as POS change, only substitution errors
were considered in syntactic relation change anal-
ysis, and the values of the syntactic changes were
set similarly. By dividing the syntactic relations into
head and modifier according to whether the words
served as heads in the relations, we also derived syn-
tactic changes for head and modifier relations, re-
spectively.
Two-way ANOVA analyses of head and modifier
syntactic relation changes yielded a significant in-
teraction effect (p < 0.05). A post-hoc analysis
revealed that, when the modifier syntactic relations
were the same, it was easier to detect errors that did
not cause the change of head syntactic relations than
159
those causing such changes (p < 0.05).
Table 4 reports descriptive statistics of ease of de-
tection in terms of syntactic relations of SR errors
that occurred at least 5 times. Two relations were
presented in the ?Syntactic Relations? column. The
first one is the relation in which errors played the
head role, and the second one is the relation that er-
rors served as a modifier. ?None? indicates no such
relations exist.
Syntactic Mean Std Frequency
Relations Deviation
none none 2.40 0.695 35
none subj 2.70 0.675 10
none det 1.78 0.833 9
none punc 2.00 0.926 8
none nn 2.00 1.000 7
none pcomp-n 2.33 1.033 6
mod pcomp-n 1.20 0.447 5
none obj 1.80 0.837 5
Table 4: Ease of detection for syntactic relations
It is shown in Table 4 that it is easier to detect if
an error is the subject of a verb (subj). A typical ex-
ample is the ?summary? in sentence ?summary will
have to make my travel arrangement ... ?. All the
participants successfully detected ?summary? as an
error. In contrast, ?mod pcomp-n? was difficult to
detect. Manual scrutinizing of the data showed that
such errors were nouns that both have some other
words/phrases as modifier (mod) and are nominal
complements of a preposition (pcomp-n). For exam-
ple, for ?transaction? in sentence ?I?m particularly
interested in signal transaction in ... ?, 80% partic-
ipants failed to detect the error. It requires domain
knowledge to determine the error.
4 Conclusion and Future Work
This study revealed that both high error ratio and
consecutive errors increased the difficulty of error
detection, which highlights the importance of SR
performance. In addition, it was easier to detect
SR errors when they had different POSes from cor-
responding reference words. Further, SR errors
lacking syntactic relations were easy to detect, and
changes in syntactic relations of reference words in
SR errors had impact on the ease of error detection.
The extracted patterns could advance SR and auto-
matic error detection research by accounting for the
ease of error detection. They could also guide the
development of support systems for manual SR er-
ror correction.
This study brings up many interesting issues for
future study. We plan to replicate the study with au-
tomatic error detection experiment. Additional ex-
periments would be conducted on a larger data set to
extract more robust patterns.
Acknowledgement
This work was supported by the National Science
Foundation (NSF) under Grant 0328391. Any opin-
ions, findings and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the views of NSF.
References
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational Lin-
guistics, 21(4):543?565.
Steven Greenberg and Shuangyu Chang. 2000. Linguis-
tic dissection of switchboard-corpus automatic speech
recognition systems. In Proceedings of the ISCA
Workshop on Automatic Speech Recognition: Chal-
lenges for the New Millennium.
Julia Hirschberg, Diane Litman, and Marc Swerts. 2004.
Prosodic and other cues to speech recognition failures.
Speech Communication, 43:155?175.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on the Eval-
uation of Parsing Systems.
David McKoskey and Daniel Boley. 2000. Error analysis
of automatic speech recognition using principal direc-
tion divisive partitioning. In Proceedings of ECML,
pages 263?270.
Gabriel Skantze and Jens Edlund. 2004. Early error de-
tection on word level. In Proceedings of Robustness.
Lina Zhou, Yongmei Shi, Dongsong Zhang, and An-
drew Sears. 2006. Discovering cues to error detec-
tion in speech recogntion output: A user-centered ap-
proach. Journal of Management of Information Sys-
tems, 22(4):237?270.
160
