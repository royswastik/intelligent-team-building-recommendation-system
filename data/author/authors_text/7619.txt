Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 57?64,
Prague, June 2007. c?2007 Association for Computational Linguistics
Phonological Reconstruction of a Dead Language
Using the Gradual Learning Algorithm
Eric J. M. Smith
Department of Linguistics
University of Toronto
130 St. George Street, Room 6076
Toronto, Ont. M5S 3H1
Canada
eric.smith@utoronto.ca
Abstract
This paper discusses the reconstruction of
the Elamite language?s phonology from its
orthography using the Gradual Learning Al-
gorithm, which was re-purposed to ?learn?
underlying phonological forms from surface
orthography. Practical issues are raised re-
garding the difficulty of mapping between
orthography and phonology, and Optimal-
ity Theory?s neglected Lexicon Optimiza-
tion module is highlighted.
1 Introduction
The purpose of this paper is to reconstruct the
phonology of Elamite, an extinct language known
only from written sources, whose phonology is cur-
rently poorly understood. Given that the mecha-
nisms provided by Optimality Theory are powerful
enough for a language learner to acquire a natural
language given only overt forms, it should be possi-
ble to apply the same mechanisms to ?learn? Elamite
phonology given only its orthography.
The research described here was carried out with
the aid of a piece of software, nicknamed Grote-
fend, which was developed as part of a larger re-
search project into Elamite.1 The data used in this
paper consisted of the contents of the Elamisches
Wo?rterbuch (Hinz and Koch, 1987) marked up as
XML with attributes such as morphology, cognates,
1Grotefend was written in C++ using Trolltech?s Qt toolkit,
and runs under Mac OS X. The portions that implement the
Gradual Learning Algorithm (?4.3) were adapted from Paul
Boersma?s Visual Basic source code for the OTSoft program,
which was kindly provided by Bruce Hayes.
semantics, corpus frequency, and chronology. The
Wo?rterbuch was used because it is the only source
that incorporates Elamite data from all historical pe-
riods. It also has the virtue of containing every sin-
gle attested form known to the authors, which is par-
ticularly useful for this project, since we have spe-
cial interest in alternative spellings of given words.
2 Elamite Language
2.1 Historical and geographical context
Elamite is an extinct language spoken in what is now
southwestern and central Iran. Elamite-language
texts dating from 2400 BCE until 360 BCE are
attested, written in the cuneiform script borrowed
from the Sumerians and Akkadians.2 Elamite has
no known linguistic affiliations, although a connec-
tion to the Dravidian family has been proposed by
McAlpin (1982) and others.
Since both the language and scribal practices are
certain to have changed over such a long time-span,
this study will restrict itself to text from a single era.
The Ach?menid Elamite period (539 BCE to 360
BCE) was chosen, because this period contains the
largest volume of texts, and also because those texts
are particularly rich in Old Persian names and loan-
words that provide a useful starting point for esti-
mating the phonology.
2.2 The cuneiform writing system
As part of their adaptation of cuneiform, the
Elamites abandoned most of the logographic ele-
2Early texts from Elam using two other indigenous writing
systems are not well-enough understood to provide useful lin-
guistic information.
57
ments found in Sumerian and Akkadian usage, mov-
ing to an almost completely phonetic system, which
would be a ?core syllabary? in Sproat?s (2000) typol-
ogy. That is, each grapheme represents a syllable,
but the system lacks graphemes to represent all of
the language?s syllables. This is particularly the case
for many ?CVC? graphemes, which must be writ-
ten using ?syllable telescoping?, where a ?CV-VC?
combination is written, with the internal vowel being
repeated. For example, lacking a ?lan? grapheme,
the syllable /lan/ would have to be written ?la-an?3.
Even when the ?CVC? grapheme does exist, the
?CV-VC? writing is often preferred.
2.3 Hypotheses to be tested
The strategy of this research is to use the techniques
of Optimality Theory to reconstruct the language?s
phonology. We will take the various hypotheses pre-
sented by earlier authors and attempt to encapsulate
each in the form of an OT constraint.
Altogether 30 separate hypotheses were evalu-
ated, arranged into 11 major groupings. Within
each grouping, the sub-hypotheses (which may or
may not be mutually exclusive) refer to a related
context or related orthographic phenomenon. This
paper will largely restrict its discussion to only two
of the groupings: H3 (Geminate consonants) and
H4 (Nasal vowels).4
H3 Geminate consonants
H3a Geminate orthographies represent underlying
geminate phonologies.
H3b Geminate orthographies indicate voicelessness
(Reiner, 1969).
H3c Certain geminate spellings indicate a distinc-
tion other than voicing, such as retroflex/alveolar
(McAlpin, 1982).
H4 Nasal vowels
H4a Alternations in the writing of nasals indicate
3Or ?7?, but since the readership of this paper is un-
likely to be familiar with cuneiform, all graphemes will be pre-
sented in the traditional transliterated form used in Assyriology.
4The full list of hypothesis groups also includes: H1 (In-
terpretation of broken ?CV1-V2C? writings), H2 (Voicing of
stops), H5 (Word-final vowels), H6 (Sibilants), H7 (Existence
of an /h/ phoneme), H8 (Existence of an /f/ or /v/ phoneme), H9
(Existence of a /j/ phoneme), H10 (Existence of a /w/ phoneme),
and H11 (Existence of an /e/ phoneme). Full discussion of the
results for these hypotheses can be found in Smith (2004).
the presence of nasal vowels (e.g. /hu?ban/ ?
?hu-um-ban?, ?hu-ban?).
H4b Alternations in the writing of nasals can be
explained by underlying nasal consonants (e.g.
/humban/ ? ?hu-um-ban?, ?hu-ban?).
3 Theory of Writing Systems
The discussion of Elamite orthography will be
framed within the theory of writing systems pro-
posed by Sproat (2000), whose core claim that ?par-
ticular (sets of) linguistic elements license the oc-
currence of (sets of) orthographic elements?. The
details of which linguistic elements license which
orthographic ones are specific to any given combi-
nation of spoken language and writing system.
The licensing is implemented by a mapping func-
tion, MORL??, whose input is the Orthographically
Relevant Level (ORL), and whose output is the
orthography(?). In the case of Elamite, the rele-
vant level is the surface phonology after the applica-
tion of phonological processes such as assimilation
and cluster simplification, so in Sproat?s schema,
Elamite is classified as having a ?shallow? ORL.5
4 Applying OT to Orthography
In the normal application of Optimality Theory, the
input and the output are both the same type of lin-
guistic entity. However, in the problem dealt with
here, the relationship is between an input that is
phonological and an output that is orthographic. The
comparison of phonological apples to orthographic
oranges leads to complications that will be discussed
in ?6.1. All the modules of Optimality Theory must
be adapted for use with orthography.
4.1 Background
As originally formulated, Optimality Theory can be
considered as a set of three interconnected modules:
GEN, H-EVAL, and Lexicon Optimization (Prince
and Smolensky, 1993). Together GEN and H-EVAL
comprise the grammar proper. For any given in-
put, GEN generates a set of output candidates, and
these candidates are then evaluated against a set of
constraints by the H-EVAL module. Lexicon Opti-
mization is not part of the grammar, but it provides
5For instance, the noun kittin ?length? is spelled ?ki-it-ti-im-
ma? when followed by the locative suffix -ma, while the 3SG
object prefix in- is written ?id? before a verb like dunih ?I gave?.
58
a mechanism by which language learners can use
that grammar to determine underlying forms based
on the overt forms that are presented to them.
Optimality Theory can be seen as a model for how
a language learner acquires a natural language, pre-
sented only with overt forms (Tesar and Smolensky,
2000). At first, the learner?s constraint rankings and
underlying forms will be inaccurate, but as more in-
formation is presented the estimates of the under-
lying forms become more accurate, which in turn
improves the constraint rankings, which further im-
proves the estimates of the underlying forms, and so
on. In this study, the ?learner? is the Grotefend soft-
ware, which is presented with surface orthography
and attempts to deduce the phonology.
4.2 Adaptation for orthography
The GEN module must be adapted to generate plau-
sible overt forms (i.e. rival orthographies). The gen-
eral strategy for GEN is described in ?5, with the
specific details given in ?5.2.
The constraints are used by a ranking algorithm
(?4.3) that compares the rival orthographies from
GEN against underlying phonological forms in or-
der to determine the number of constraint violations.
In order to start the process, those underlying forms
have to be seeded with reasonable initial estimates.
If the word is a loanword, the initial estimate is
based on the Old Persian or Akkadian phonology.
If there is no available loanword phonology, the ini-
tial estimate is a direct transcription of the grapheme
values as if the word were being read in Akkadian.
Once the constraints have been ranked, Lexical
Optimization takes the orthographic forms and the
newly-ranked constraints, and calculates an esti-
mated phonology for each of the forms. At this
point, the process can stop, or else it can proceed
through another iteration of the ranking algorithm,
using the new improved estimated phonologies as
underlying forms.
4.3 Gradual Learning Algorithm
The Gradual Learning Algorithm (Boersma, 1997;
Boersma and Hayes, 2001) is an evolution of the
Constraint Demotion algorithm (Tesar, 1995), but
avoids the infinite-looping which can arise in Con-
straint Demotion if underlying forms have more than
one overt form. This limitation of Constraint Demo-
tion is a serious one given the data from Elamite or-
thography; not only are the orthographic forms sub-
ject to considerable variation, but also this variation
is a key piece of information in attempting to recon-
struct the phonology.
In the GLA, constraints each have a numeric rank-
ing value associated with them. It is no longer the
case that Constraint A consistently outranks Con-
straint B; whenever a constraint is evaluated, a ran-
dom ?noise? factor is added to each of the ranking
values, and an instantaneous constraint ordering is
determined based on these adjusted values. If the
ranking values for two constraints are far apart, the
noise is unlikely to alter the ordering, and the re-
sults will be effectively the same as ordinary OT. If
the ranking values for two constraints are close to-
gether, the noise could put either constraint on top,
but ties are avoided.
In the GLA implementation within Grotefend, all
constraints start with ranking values of 100.00. With
each iteration of the algorithm, one of the observed
forms is selected as an exemplar, and rivals (pro-
duced by GEN) are compared against the observed
exemplar form. Whenever a rival beats the exemplar
form, the constraint ranking values must be adjusted:
all constraints that picked the wrong winner are pe-
nalized (adjusted downwards), and all constraints
that picked the right winner are rewarded (adjusted
upwards). The size of this adjustment is determined
by a variable called ?plasticity?, which starts at 2.00
and is reduced gradually to 0.002 as the algorithm
proceeds through its iterations.
5 Implementation of GEN
The purpose of GEN is to generate a set of plausi-
ble overt forms consisting of the real form and a set
of rivals which will lose out to the real form. In
this problem the overt forms are orthographies, so
for any underlying form the challenge is to gener-
ate orthographic strings that compete with the real
orthography, but which are ?wrong? with respect to
one or more of the constraints.
5.1 Background
There have been a number of computational imple-
mentations of GEN, but the most promising one for
our purposes was that of Heiberg (1999). Heiberg?s
59
algorithm proceeds by choosing a starting point and
then adding constraints to the system. As each con-
straint is added, new candidates are generated using
what she calls ?relevant? GEN operations. A GEN
operation is considered to be relevant for the current
constraint if the operation could affect a candidate?s
harmony relative to that constraint. So for instance,
if the constraint being added evaluates the [+back]
feature, the only GEN operations that are relevant
are those which affect [+back] or its associations.
The candidates at each stage of the algorithm are
not fully formed, and are slowly refined as the con-
straints are added to the system. One advantage of
Heiberg?s algorithm is that it functions even if the
relative rankings of the constraints are not known. If
the constraint rankings are known, the algorithm can
operate more efficiently, by culling known losers,
but knowing the rankings is not essential.
5.2 Adaptation of GEN for orthography
Generating all ?plausible? orthographic candidates
for a given form is computationally prohibitive.6
So, borrowing Heiberg?s notion of ?relevant? oper-
ations, our approach is to generate candidates that
specifically exercise one of the constraints in the
constraint system.
Each of the hypothesis groups described in ?2.3
refers to a particular orthographic context, and each
context has a miniature version of GEN to generate
appropriately test-worthy rivals. For example, the
mini-GEN function for context H3 is as follows:
GEN H3 (Geminate consonants)
Rule Whenever a geminate consonant is found
in the orthography, generate a rival with the non-
geminate equivalent.
Example ?hu-ut-ta? ? { ?hu-ta? }
6 Implementation of H-EVAL
The H-EVAL module is responsible for the actual
evaluations of the various candidates. It takes any
output candidate produced by GEN, and counts the
violation marks for each of the constraints.
6Initial experiments indicated that a moderately long string
of four graphemes would generate in the neighbourhood of
18000 rivals. It seemed unrealistic to evaluate tens of thousands
of rivals for each of the 8000+ forms in the database.
um
PHON
SEM
divinity: 
d  
a h u
ORTH u mas ? da
mar a z d h
ri
a?
Figure 1: Annotation graph for Ahuramazda?h ?
?du-ri-um-mas?-da?
Each constraint is implemented as a function
which takes two inputs (an underlying form and a
surface form), and produces as an output the num-
ber of violations incurred by the comparing the two
inputs. For full generality, both inputs are anno-
tation graphs (Bird and Liberman, 1999; Sproat,
2000) such as the one shown in Figure 1. As imple-
mented in Grotefend, the comparison involves only
the PHON tier of the underlying form?s graph and the
ORTH tier of the surface form?s graph.
Constraints were written to test each of the hy-
potheses described in ?2.3. Since there is no prior
art in the area of constraints involving orthography
and phonology, they were developed in the most
straightforward way possible. The implementation
of these functions is described in ?6.2.
6.1 Implementation of alignment
In order to count violations, the two inputs must
be properly aligned. For an annotation graph to
be ?aligned?, every grapheme must be licensed by
some part of the phonology, and every phoneme
must be represented in the orthography. Without
such a licensing relationship, it is impossible to
make the comparisons needed to count constraint vi-
olations.
There is considerable previous work in the area of
alignment, most recently summarized by Kondrak
and Sherif (2006). The algorithm used in this study
is a similarity-based approach, not unlike ALINE
(Kondrak, 2000). It differs in some significant re-
spects, notably the use of binary features.
Determining the eligibility of two phonemes for
matching requires a distance function. The approach
taken was to assign a weight to each phonological
feature, and to calculate the distance as the sum of
the weights of all features that differ between the
two phonemes. The full listing of feature weights is
shown in Table 1. The weighting values were deter-
60
Table 1: Feature weights for computing distance
Phonological Feature Weight
delayed release, voice, labio-dental,
anterior, distributed, strident
1
approximant, continuant, nasal, lateral,
round, low, pharyngeal
2
syllabic, consonantal, constricted glot-
tis, spread glottis, high, front, back
4
sonorant, place of articulation 8
mined empirically, selecting the weightings that did
the best job of aligning the orthography for the Old
Persian loanwords given by Hinz and Koch (1987).
For the actual alignment, several approaches
were tried, but the most effective one was simply
to line up the consonants and let the vowels fall
where they may. For instance, the licensing of
?du-ri-um-mas?-da? in Figure 1 used the Old Persian
phonology as the best available initial estimate for
the Elamite phonology, and proceeded as follows:
?d? is the divine determinative, and is licensed
by the semantic tier of the annotation graph, so it
does not need to be anchored to the phonology.
?u? is anchored at the left edge of the phonology.
?ri? starts at /r/, but has no clear right edge. The
anchoring of /r/ sets a right boundary on the ?u?,
which must therefore be licensed by the initial /ahu/
of /ahuramazda?h/.
?um? right edge at phoneme /m/; since the ?um? has
no clear left edge, the second /a/ of /ahuramazda?h/
is left floating between the ?ri? and the ?um?. Since
there is no clear choice between the two locations,
the /a/ will be shared by ?ri? and ?um?.
?mas?? starts at /m/ and ends at /z/, which is suffi-
ciently similar to match s?. The /m/ will be shared by
?um? and ?mas??.
?da? starts at /d/; since ?da? is the last grapheme, it
must be licensed by the remainder of the phonology.
The general strategy of aligning consonants
proved to be an effective one. In the working dataset
of Ach?menid Elamite words, there were 3045 that
used Old Persian or Akkadian data to provide an ini-
tial estimate of the underlying phonology. The algo-
rithm successfully aligned 2902 of those words, for
a success rate of over 95%.
6.2 Implementation of constraints
Once the orthography has been successfully aligned
with the underlying phonology, it is possible to
evaluate the forms for violations against all the con-
straints in the system. In terms of the tiers shown in
annotation graphs like Figure 1, the constraints are
performing comparisons between the underlying
forms in the PHON tier and overt forms in the ORTH
tier. For example, the rules for calculating constraint
violations for H3 are as follows:
H3a Geminate spellings indicate geminate pronun-
ciations.
Rule Count a violation if the orthography contains
a geminate consonant not matched by a geminate in
the phonology.7
Violation /ata/ ? ?at-ta?
Non-violations /atta/ ? ?at-ta?, /atta/ ? ?a-ta?
H3b Geminate spellings indicate voicelessness.
Rule Count a violation if 1) the orthography con-
tains an intervocalic geminate stop not matched by a
voiceless stop in the phonology, or 2) the orthogra-
phy contains an intervocalic non-geminate stop not
matched by a voiced stop in the phonology, or 3) the
phonology contains an intervocalic voiceless stop
not matched by a geminate in the orthography, or 4)
the phonology contains an intervocalic voiced stop
not matched by a non-geminate in the orthography.8
Violations /duba:la/ ? ?du-ib-ba-la?, /garmapada/
? ?dkar-ma-ba-tas??
Non-violations /gauma:ta/ ? ?kam-ma-ad-da?,
/babili/ ? ?ba-pi-li?
H3c Certain geminate spellings indicate a distinc-
tion other than voicing, such as retroflex/alveolar.
Rule Count a violation if 1) the orthography con-
tains a ?Vl-lV? or ?Vr-rV? sequence not matched by
a retroflex in the phonology, or 2) the phonology
contains a /?/ or /?/ not matched by a ?Vl-lV? or
7The claim made by Grillot-Susini and Roche (1988) was
only that a geminate orthography represents a geminate phonol-
ogy; a non-geminate orthography could still conceal a geminate
phonology.
8Reiner (1969) restricted her claim about gemination repre-
senting voicelessness to intervocalic stops. Word-initial stops
and intervocalic non-stops were not relevant here.
61
?Vr-rV? in the orthography.
Violations /talu/ ? ?ta-al-lu?, /ta?u/ ? ?ta-lu?
Non-violation /ta?u/ ? ?ta-al-lu?
7 Implementation of Lexicon Optimization
Given the set of constraints provided in ?6.2 and
rankings determined by the Gradual Learning Algo-
rithm (?4.3), it is now possible to move on to the
final stage of the ?learning? process: Lexicon Opti-
mization, which is responsible for choosing the most
harmonic input form for any given output form.
There has been surprisingly little literature de-
voted to Lexicon Optimization, and discussions of
how it might be implemented have been restricted to
toy algorithms such as Ito? et als (1995) ?tableau des
tableaux?. Hence, a novel approach was devised,
based on the observation that Lexicon Optimization
is a sort of mirror image of H-EVAL. For H-EVAL,
there exists a separate GEN module whose task is
to generate the possible output candidates. Clearly,
Lexicon Optimization needs an equivalent module,
but one that would generate a range of rival input
forms. Since the GEN algorithm described in ?5.2
uses a constraint-driven technique for generating
output candidates, it seems appropriate to also use
a constraint-driven technique for generating input
candidates. Accordingly, this anti-GEN is imple-
mented as a set of miniature anti-GENs, each of
which is responsible for generating ?relevant? input
candidates for one of the hypothesis groupings. For
example, the anti-GEN function for H3 is as follows:
Anti-GEN H3 (Geminate consonants)
Rule Whenever a geminate consonant is found
in the orthography, create input candidates with
the geminate phonology and the equivalent non-
geminate phonology. If the geminate orthography is
a ?Vl-lV? or ?Vr-rV?, also create an input candidate
with a ?retroflex? phonology.9
Example ?ta-al-lu? ? { /tallu/, /talu/, /ta?u/ }
8 Results and Discussion
We ran 40000 iterations of the Gradual Learning
Algorithm against the Ach?menid Elamite forms
9McAlpin (1982) hedges on whether the phonology repre-
sented by these geminates actually represents retroflexion, but
he then proceeds to discuss Proto-Elamo-Dravidian cognates as
if this orthography actually did represent a retroflex articulation.
Table 2: Final constraint rankings for H3 and H4
Hypo-
thesis
Constraint Ranking
Value
H4b NasalConsonants ?136.93
H3b ?Geminate?=/Voiceless/ ?283.70
H4a NasalVowels ?1434.77
H3c ?Geminate?=/Retroflex/ ?1629.74
H3a ?Geminate?=/Geminate/ ?3189.11
found in the Elamisches Wo?rterbuch. The final con-
straint rankings for hypothesis groups H3 and H4 are
shown in Table 2.10 The combination of constraints,
GEN, and anti-GEN functions used by Grotefend
tends to penalize constraints much more often than
it rewards them. The absolute ranking values are not
significant; what matters is their relative values.
8.1 Results for H3 (Geminate consonants)
The results for H3 strongly support the hypothesis
(Reiner, 1969) that geminate orthographies are an
attempt to indicate voicelessness; the opposing hy-
pothesis (Grillot-Susini and Roche, 1988) that gem-
inate orthographies represent geminate phonologies
ended up being very heavily penalized.
What was surprising was that hypothesis H3c, that
?Vl-lV? and ?Vr-rV? geminates represent a sepa-
rate phoneme from the non-geminate orthographies,
ranked so poorly. The problem here is a side-effect
of the process for generating input candidates.
Consider the Akkadian name Nabu?-kudurri-us. ur;
the ?ur-ri? sequence that occurs in the various
spellings of this name would appear to be an ideal
context for evaluating H3c. However, when gen-
erating input candidates for Nabu?-kudurri-us. ur, the
various anti-GEN functions create 238 permutations
(mostly permutations of voicing), but only four of
those input candidates contain an /?/ phoneme, with
the rest having an /rr/ or an /r/. Since the anti-GEN
function produces so few /?/ input candidates for the
?ur-ri? orthography, it is likely that the software will
find an /r/ in the underlying phonology, and will
count a violation against this constraint.
The prejudice against /?/ and /?/ highlights the im-
portance of having a fair and balanced anti-GEN
10Results for the other nine groups are in Smith (2004).
62
PHON h ?) d u ?
ORTH
hi is?in du
PHON h ?) d u ?
ORTH
hi is?in du
1 H4a (nasal vowel) violation
0 H4b (nasal consonant) violations
0 H4a (nasal vowel) violations
1 H4b (nasal consonant) violation
Figure 2: Licensing of ?hi-in-du-is?? ?Indian?
function. The proposal to be discussed in ?8.3 for
cross-permuting the results of the constraint-specific
anti-GEN functions would probably also improve
the results for this hypothesis.
8.2 Results for H4 (Nasal vowels)
The effectiveness of the constraints for evaluating
nasals was undermined by choices made in the align-
ment algorithm. Although constraint H4b (Nasal-
Consonants) is ranked significantly higher than H4a
(NasalVowels), this may be merely a side-effect of
the alignment algorithm.
Consider the word for ?Indian?, which shows up
as ?hi-du-is??, ?hi-in-du-is??, or ?in-du-is??. It is not
unreasonable to postulate an underlying phonol-
ogy of /h??duS/, based both on the range of written
forms, and on the Old Persian phonology. However,
when the alignment algorithm attempts to deter-
mine which phoneme sequences are licensing which
graphemes, it has a difficult choice to make for the
?in? grapheme. Licensing the vowel portion of ?in?
is straightforward, but what should be done for the
consonant? If the software assumes that the most
salient features are [+consonantal] and [?syllabic],
we get the first annotation graph shown in Figure
2, but if the most salient features are [+nasal] and
[+sonorant], we get the second graph.
The choice of how to license the ?in? grapheme
makes a difference for how the H4a and H4b con-
straints are evaluated. Using the weightings given
in Table 1, the software will align ?in? with /??d/, be-
cause the distance between /n/ and /d/ is less than
that between /n/ and /??/. Hence, the alignment algo-
rithm chooses the first of the two annotation graphs
given in Figure 2. This has the result of prejudic-
ing the learning algorithm in favour of H4b instead
of H4a. Ideally, the alignment algorithm should be
neutral with respect to the various constraints.
The licensing of the ?in? sign in this example
is one case of several where it appears that us-
ing phonological segments as the basis for licensing
may be the wrong thing to do. It would be better to
think of the second portion of the ?in? sign in ?hi-in-
du-is?? as being licensed by a [+nasal] feature, with-
out attempting to tie the feature down to either the
/i/ or the /d/ segment.11
8.3 Discussion of Lexicon Optimization
The generation of useful input candidates is limited
by the information that is available to us. For all
we know, Elamite had an /W/ vowel, and Grotefend
could even generate input candidates that contained
an /W/. However, none of the constraints would
weigh either for or against it, so there is no point in
generating such an input candidate. Consequently,
the correct underlying form may well be inaccessi-
ble to Lexicon Optimization. At best, Lexicon Opti-
mization can produce an estimated underlying form
that leaves as underspecified any features that can-
not be verified by a corresponding constraint. This
is a limitation of Lexicon Optimization in general,
not just of the implementation in Grotefend.
One problem specific to our constraint-based gen-
eration of input candidates is that the anti-GEN func-
tions work in isolation from each other. For exam-
ple, when processing ?da-is??, the H1 (broken-vowel)
anti-GEN produces /daiS/, /dajS/, /dES/, and /daS/.
Separately, the H6 (sibilant) anti-GEN will produce
/dais/, /daiS/, /daiz/, /daitS/, and /daits/. Since the
two functions operate independently, the software
fails to generate a whole range of candidates. If
the actual underlying phonology were /dEtS/, Grote-
fendwould never find it, since that particular phonol-
ogy will never be generated and presented to Lexi-
con Optimization as a possible input candidate. A
more sophisticated anti-GEN implementation would
allow for the input candidates produced by one con-
straint?s anti-GEN function to be further permuted
11Sproat (2000) uses phonological segments to describe li-
censing, but there is nothing in his theory that requires this; in
fact, he says that his use of segments is merely a ?shorthand?
for a set of overlapping gestures.
63
by the anti-GEN function of another constraint.
9 Conclusions
This project represented an expedition into three
largely unexplored territories: the application of Op-
timality Theory to orthography, the implementation
of Lexicon Optimization in software, and the mass
analysis of Elamite phonology. All three presented
unanticipated challenges.
The problem of implementing GEN algorithmi-
cally appears to be at an early stage even in the pro-
cessing of phonological data. The constraint-driven
GEN adopted from Heiberg (1999) does appear to be
a useful starting point for working with orthography.
The determination of the mapping between
phonology and orthography can have unexpected
consequences for the evaluation of constraints. Even
when properly aligned, implementing meaningful
constraints to evaluate the mismatches between
phonology and orthography proved to be surpris-
ingly complex. An alternative representation, licens-
ing graphemes based on bundles of features rather
than phonemes, might be more effective.
The whole area of Lexicon Optimization has re-
ceived surprisingly little mention in the literature of
Optimality Theory. The notion that there must be
some form of anti-GEN module to produce suitable
input candidates appears never to have been raised
at all. The existence of anti-GEN is hardly specific
to the study of orthography, but would seem to be an
omission from Optimality Theory in general.
The constraint-driven implementation of the anti-
GEN function does seem like a promising strategy,
although the details need work. In particular, there
is a need for the outputs of the various constraint-
specific anti-GENs to be permuted together in order
to produce all plausible input candidates.
Elamite has always been problematic both due to
its status as an isolate and because the available clues
end up being obscured by the writing system. So
far, we can claim that this computational analysis
of the body of Elamite vocabulary has succeeded in
duplicating some of the tentative conclusions drawn
from a century of hard work ?by hand?.
References
Steven Bird and Mark Liberman. 1999. A formal frame-
work for linguistic annotation. Technical Report Tech-
nical Report MS-CIS-99-01, Department of Computer
and Information Science, University of Pennsylvania.
Paul Boersma and Bruce Hayes. 2001. Empirical tests
of the gradual learning algorithm. Linguistic Inquiry,
32:45?86.
Paul Boersma. 1997. How we learn variation, option-
ality, and probability. Proceedings of the Institute
of Phonetic Sciences of the University of Amsterdam,
21:43?58.
Franc?oise Grillot-Susini and Claude Roche. 1988.
Ele?ments de grammaire e?lamite. Etudes elamites. Edi-
tions Recherche sur les civilisations, Paris.
Andrea Heiberg. 1999. Features in Optimality Theory:
A computational model. Ph.D. thesis, University of
Arizona.
Walther Hinz and Heidemarie Koch. 1987. Elamisches
Wo?rterbuch. D. Reimer, Berlin.
Junko Ito?, Armin Mester, and Jaye Padgett. 1995. NC:
Licensing and underspecification in optimality theory.
Linguistic Inquiry, 26(4):571?613.
Grzegorz Kondrak and Tarek Sherif. 2006. Evaluation
of several phonetic similarity algorithms on the task
of cognate identification. In Proceedings of the Work-
shop on Linguistic Distances, pages 43?50.
Grzegorz Kondrak. 2000. A new algorithm for the align-
ment of phonetic sequences. In Proceedings of the
First Meeting of the NAACL, pages 288?295.
David W. McAlpin. 1982. Proto-Elamo-Dravidian: The
evidence and its implications. Transactions of the
American Philosophical Society, 71(3):1?155.
Alan Prince and Paul Smolensky. 1993. Optimality the-
ory. Rutgers Optimality Archive, #537.
Erica Reiner. 1969. The Elamite language. Handbuch
der Orientalistik I/II/1/2/2, pages 54?118.
Eric J. M. Smith. 2004. Optimality Theory and Orthog-
raphy: Using OT to Reconstruct Elamite Phonology.
M.A. forum paper, University of Toronto.
Richard Sproat. 2000. A Computational Theory of Writ-
ing Systems. Cambridge University Press.
Bruce Tesar and Paul Smolensky. 2000. Learnability in
Optimality Theory. MIT Press, Cambridge, Mass.
Bruce Tesar. 1995. Computational Optimality Theory.
Ph.D. thesis, University of Colorado.
64
Book Review
Machine-Aided Linguistic Discovery: An Introduction
and Some Examples
Vladimir Pericliev
(Bulgarian Academy of Sciences)
London: Equinox, 2010, ix+330 pp; hardbound, ISBN 978-1-84553-660-2, $90.00, ?60.00
Reviewed by
Eric J. M. Smith
University of Toronto
The subtitle of Vladimir Pericliev?s book,An Introduction and Some Examples, is a succinct
and accurate description of its contents. Pericliev argues briefly for the usefulness of
computer-aided techniques in linguistic discovery, contrasting it with the intuitionist
approach which has characterized linguistic discovery throughout much of its history.
The bulk of the book is devoted to examples of software-aided linguistic discovery
drawn from his own work.
Chapter 1 starts by sketching out the current state of discovery techniques in lin-
guistic theory, categorizing scientific discovery into three main approaches: the intui-
tionist approach, the chance approach, and the problem-solving approach. Discoveries
by intuition and by chance remain the purview of humans, but clearly the problem-
solving approach can benefit from the application of computational techniques.
Chapter 2 presents the KINSHIP program, which performs ?parsimonious discrimi-
nation? in order to determine the minimal set of features which are necessary to dis-
criminate all of a language?s kinship terms. The program is used to discover feature
geometries, superior to existing human-discovered ones, which describe the kinship
terminology of languages like English and Bulgarian.
Chapter 3 extends the ideas used in KINSHIP to a program called MPD (maximal
parsimonious discrimination), which is then applied to a variety of other tasks, some
of which are unconnected to linguistics. Of these applications, the most interesting is
the use of MPD to determine the segment profiles which uniquely identify languages
in the UPSID-451 database (consisting of segment inventories from 451 languages,
selected to provide broad coverage of the world?s language families) (Maddieson and
Precoda 1991). Although Pericliev discusses his results at considerable length, it is not
clear what the theoretical usefulness of these profiles might be. What does it really
tell us about French to know that it is the only language in the database to contain
the phoneme [??]? Of more practical interest was Pericliev?s discussion of the process
of converting the UPSID data into a featural representation to make it amenable to
processing, describing how to represent underspecified segments and how to deal with
transcription variations. This sort of necessary preprocessing constitutes an important
and underemphasized part of the process of machine-aided linguistic discovery. The
study of UPSID does produce some interesting, though not unexpected, results. For
instance, when a profile contains more than one unique segment, the majority of these
segments share a common feature, and 85.8% of the unique segments have some sort of
secondary articulation.
Chapters 4 and 5 present twomore pieces of software developed by Pericliev: UNIV
and AUTO. The UNIV software is inspired by Greenberg?s universals (Greenberg 1966),
Computational Linguistics Volume 36, Number 4
and automates the haphazard process by which universals have been identified in the
past. Given a vector of features for each language being studied, UNIV identifies all
universal patterns which hold above a user-specified threshold. Such universals can
be unrestricted or statistical and they can be stand-alone or implicational. Once a set
of universals has been identified, the results are fed through AUTO (for "AUthoring
TOol"), which assembles boilerplate text into a journal article; given the vast number
of (often trivial) universals which UNIV discovers, this can be useful. UNIV is first
applied to two data sets: one of kinship terms, and the other the word-order data used
by Greenberg himself. The most interesting result is that Greenberg?s set of word-order
universals was neither complete nor fully supported by the data.
UNIV is then applied to the UPSID-451 database. UNIV identifies a large number
of previously unnoted universals, most of which are rather low-level and of little in-
herent theoretical interest. However, the low-level machine-discovered generalizations
can then be used as the basis for more interesting manually created generalizations.
The UNIV analysis also serves to refine earlier claims made by Maddieson (1984) and
Gamkrelidze (1978).
Chapter 6 is devoted to MINTYP, which is a program for determining the minimum
typology to account for an observed set of universals. The search for such typologies
is discussed by Greenberg (1966) and by Hawkins (1983). MINTYP takes a system of
universals and a set of logically admissible types, and eliminates any superfluous uni-
versals which can be implied by stronger universals in order to determine the smallest
set of universals which still accounts for the observed data. This approach is able to
distill Greenberg?s set of universals into as few as four composite universals. Like UNIV
and KINSHIP, MINTYP follows Pericliev?s basic approach: Reduce the data to a set of
features, and then find the patterns whichmost economically cover the observed feature
distribution.
Chapter 7 turns this featural approach to the problem of genetic language classifi-
cation with the RECLASS software. Pericliev extends the featural approach to include
Swadesh-type word-lists, for which he describes a method for calculating a similarity
metric based on phonological features of words in the list. A set of languages from
different families is selected for study, a similarity metric is calculated for pairs of lan-
guages, and unrelated languages whose similarity is significantly greater than expected
are given further attention. In Pericliev?s test case, the initial feature data consists of
kinship terminology, which revealed an unexpected similarity between the Kaingang
languages of Brazil and various Polynesian languages. He pursues this similarity first
by using features based on word-list similarities and then by looking at other structural
features, arguing at length for the plausibility of a genetic connection. Of all the results
described in the book, this is probably the most interesting, because it represents a
discovery made by Pericliev?s machine-aided approach which is unlikely ever to have
been found by the haphazard manual process of discovery.
The main weakness of the book is that all the software described in the book
was developed or co-developed by Pericliev himself, so the various programs all risk
seeming like variations on a single theme. The book would have benefited by including
examples of software from others working in the field, which might differ from the
feature-coverage approach favored by Pericliev. That being said, Pericliev?s essential
point is a valid one: Machine-aided discovery has a tremendous untapped potential
for analyzing data sets which are too large to be amenable to human inspection.
The success of this approach is best exemplified by his machine-aided discovery of a
possible genetic relationship which would otherwise have eluded human discovery.
786
Book Review
References
Gamkrelidze, T. V. 1978. On the correlation
of stops and fricatives in a phonological
system. In J. H. Greenberg, editor,
Universals of Human Language. Stanford
University Press, Stanford, CA,
pages 2:9?46.
Greenberg, J. H. 1966. Some universals of
grammar with particular reference to the
order of meaningful elements. In J. H.
Greenberg, editor, Universals of Language.
Mouton & Co., The Hague, pages 73?113.
Hawkins, J. 1983.Word Order Universals.
Academic Press, New York.
Maddieson, Ian. 1984. Patterns of Sounds.
Cambridge University Press,
Cambridge, UK.
Maddieson, I. and K. Precoda. 1991.
Updating UPSID. UCLA Working Papers
in Phonetics, 74:104?114.
Eric Smith?s primary research at the University of Toronto is centered on corpus approaches to
the study of Sumerian syntax. Earlier machine-aided discoveries included the reconstruction of
Elamite phonology using Optimality Theory. His e-mail address is eric.smith@utoronto.ca.
787

