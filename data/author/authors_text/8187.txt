Proceedings of the 6th Workshop on Statistical Machine Translation, pages 303?308,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Personal Translator at WMT2011 - A rule-based MTsystem with hybrid components - 
  Vera Aleksi? Gregor Thurmair Linguatec Gmbh Linguatec Gmbh Gottfried-Keller-Str. 12 Gottfried-Keller-Str. 12 Munich, Germany Munich, Germany v.aleksic@linguatec.de g.thurmair@linguatec.de      Abstract This paper presents the Linguatec submission to the WMT 2011 sixth workshop on statistical machine translation. It describes the architecture of our machine translation system ?Personal Translator? (hereinafter also referred to as PT), developed by Linguatec, which is a rule-based translation system, enriched by statistical approaches. We participate for the German-English translation direction. For the current submission we have chosen the latest commercial version of the system, PT14. The translation quality improvement for the submission was done mainly by lexicon tuning:  detection of unknown words, extracting of possible translations, partly from the wmt11 training corpora, and enlarging the lexicon by manually coding the chosen transfer candidates. 1 Introduction The origin of the PT technology dates back to the 80?s when a translation system based on logic programming and slot grammars was developed by Michael McCord at IBM T.J. Watson Research Center. In many years of development the translation engine has been driven forward and enhanced. Most recently we have added statistical 
approaches for tasks such as erroneous input correction, subject area recognition and word disambiguation. Today ?Personal Translator? is one of the leading programs in the translation technology field. It is a commercial MT system whose product range includes 7 language pairs, i.e. 14 translation directions, for single users and networks. Linguatec is a leading provider of language-technology software for office use in Germany. In addition to machine translation, we develop and provide commercial products in the fields of speech recognition and speech synthesis. Linguatec is the only company to have won the European Information Technology Prize three times. 2 System fundamentals Personal Translator is implemented as a modular system which basically consists of the following components: ? the grammar, written in Prolog, based on the concept of slot grammar ? the lexicon, administrated in the data base internally called TransLexis ? additional morphological analysers written partly in C and C++  ? hybrid (rule-based and statistical) methods for word disambiguation, subject area recognition and spell-checking ? a range of pre- and post-processing components such as format converters  for 
303
html, pdf, doc, txt and rtf formats, sentence splitter, tokeniser, lemmatizer. As Personal Translator is a commercial system, aiming at providing a complete translator work bench and creating added value for users, it integrates a wide range of advanced features such as:  ? Translation memory system for management, creation, analysis and maintenance of  TMs, as well as large system modules, containing tens of thousands of sentence pairs ? Translation project management tool, enabling the user to save and administer all important translation settings and project relevant options ? Text to speech functionality to support  editing and learning processes such as text revision/correction in the language(s) mastered by the user, or   getting a feeling for the correct pronunciation in a foreign language, to name just a few. 2.1 LMT and Slot Grammar Personal Translator is based on the LMT (Logic programming based Machine Translation). The core of LMT uses the principles of slot grammar, a grammatical description system developed originally by Michael McCord1 at IBM. Slot grammar is based on the concept of word valence. It is dependency oriented, i.e. each phrase has a head word. Each (head) word is characterised by slots which represent empty places in its grammatical surroundings such as subject, object, modifier etc. which can be realised in text or not. The slots represent either complements of the head word which have to be defined in the lexicon or adjuncts which are rather associated with the part of speech and defined more generally in the grammar rules. The possible slot fillers are typified by their morphological, syntactic or semantic properties. The analysis of a word is finished and the phrase is considered as satisfied if the appropriate fillers are found in the text and all (obligatory) slots of the word are filled 
                                                           1 McCord (1989); McCord, Vernth (1992) 
3 Advanced translation features There are some well-known restrictions concerning the automatic translation process. One of them is the ability of most MT systems to operate on only one sentence at a time. The same is also true for the PT but only to a limited degree. PT integrates several methods for semantic and context analysis on multi-sentence level and for the identification of concepts which are repeated throughout the text.  This applies in particular to the recognition of pronoun references and coreference analysis of proper names, as well as subject area recognition and neural transfer which are described further below. 3.1 Recognition of pronoun reference Pronouns can refer to other words (their antecedents) which had occurred in the previous text. When translating from German into English and vice versa the fact that e.g. the English personal pronouns he/she apply only to humans and it to all other things, whereas in German er/sie/es can refer to any noun, has to be considered when searching for appropriate translation: This is a desk. It is new.  Dies ist ein Schreibtisch. Er ist neu. versus: This is a bag. It is new. Dies ist eine Tasche. Sie ist neu. The user can either select the translation option ?Automatic recognition of pronoun reference?, when translating a continuous text, or deselect it in case of translating lists of independent sentences (as we did for the current submission). If this option is deselected, the PT output for the sentences above reads as follows: Dies ist ein Schreibtisch. Es ist neu.  Dies ist eine Tasche. Es ist neu.  Also the translation of other words in the context can benefit from correct pronoun reference recognition:  The dogs found biscuits. They ate them.  Die Hunde fanden Kekse. Sie fra?en sie. versus: The children found biscuits. They ate them.  Die Kinder fanden Kekse. Sie a?en sie. The last example demonstrates an improvement in the translation of the verb eat which is to be translated into German with fressen if its subject is 
304
an animal or with essen if the subject is a human. The pronoun they in the first sentence refers to dogs (animals), in the second to children (humans) respectively. 3.2 Named entity recognition The treatment of proper names is a real challenge for machine translation. There is a huge number of proper names, even growing constantly if e.g. the companies and product names are considered. Furthermore, person names are constantly changing in their degree of topicality, so it is not of much use to have Kohl and Fischer in the lexicon when the texts to be translated speak about Merkel and R?sler. As such, the proper names are unsuitable to be primarily stored in the lexicon. The second problem is homography: If a proper name is spelled in the same way as a common word, it is very likely to be translated by an MT system (Brown => Braun; Metzger => Butcher).  Personal Translator integrates a named entity recognition component which runs both: ? as a pre-processing tool: It puts mark-ups on the proper names to exclude them of other pre-processing components such as e.g. spell checker ? as part of the translation process, integrated into the lexicon and the complete analysis-transfer-generation process: Morphological and syntactic analysis/generation bases among other things on semantic roles (person, place?), as the proper names have special inflection patterns  and specific syntactic behaviour (preposition  slots, appositions etc.). By this, we could achieve an increase in translation quality of about 30% for sentences containing proper names.2 3.3 Word sense disambiguation Another important issue is the treatment of ambiguous words. Most glossaries contain several million translations, among them large amounts of words with multiple meanings. Traditionally, ?Personal Translator? uses several ways to disambiguate ambiguous words and select the most proper translation: ? Interpretion of gender/number and other morphosyntactic information:                                                            2 cf. Thurmair (2005) 
der Kiefer (m) = jaw die Kiefer (f) = pine minute (sg)  = Minute minutes (pl) = Protokoll ? Analysis of slot fillers: anmachen (Licht) = turn on (light) anmachen (Salat) = prepare (salad) anmachen (jmd.) = chat (s.o.) up  bestehen (auf ) =  insist (on) bestehen (aus) = be made (of) ? Use of orthographic information: fest (lower case) = stable, firm Fest (capitalised) = celebration ? Definition of different subject area codes for the translations: die Mutter (general) = mother die Mutter (techn.) = nut  4 Hybrid technology All these disambiguation methods are labour-intensive in terms of manual coding efforts, and they require, to a certain extent, user interaction (e.g. selecting appropriate options such as subject area) that in turn needs reliable knowledge of the contents to be translated which is often not the case. And not at least, manual setting of the disambiguation contexts is not only inefficient but also prone to errors.  For these reasons Linguatec continually tests new, innovative solutions to reduce manual coding efforts and increase translation quality. Therefore it seemed obvious to try to draw statistical significant, reliable, and empirically-sound information from the immense Linguatec corpus and enrich the RMT with this knowledge. Thus an innovative hybrid component, which has been filed as patent3, has been developed. 4.1 Neural transfer We as humans rarely have problems to distinguish between two or more different meanings of a word. The decision happens automatically, supported by accessing the world knowledge stored in our brains. Many efforts have been made to artificially imitate these processes. In linguistics, traditionally ontologies have been created which aim at 
                                                           3 cf. Linguatec Patent ?Hybrid transfer selection in Machine Translation?  US: 11/885.688, EPA: Nr. 05715789.3 
305
reflecting the relations and the hierarchy in the nature. In information technology, artificial neural networks try to approximate the operation of the human brain. Linguatec?s hybrid disambiguation model tries to single out the best translation for a word by identifying its semantic network.  We call it ?neural transfer?. The disambiguation model for the neural transfer has been trained on a significant amount of different contexts for each lexicon entry with multiple translations, where this method could be considered as appropriate. Clusters of different meanings of words were built manually and statistical methods were applied on them in order to identify the most distinctive terms in their surroundings and represent the results in neural networks.  The neural transfer technology has been integrated into the PT by modifying the affected lexicon entries, and by adding a pre-processing component which assigns a semantic net to the affected text passage. The neural transfer enables the PT to ?understand? the context beyond sentence boundaries. Thus it is possible to deliver two different translations for the word Gericht (court, dish) in absolutely identical sentences, depending on the textual context: Ich kann mich noch an dieses Gericht erinnern. Es hat die Klage meiner Firma auf Entsch?digung abgewiesen. I can still remember this court. It has rejected the complaint of my company on reimbursement. versus: Ich kann mich noch an dieses Gericht erinnern. Es war eines dieser Gerichte aus der K?che der Balkanl?nder, mit Gem?se und Knoblauch. I can still remember this dish. It was one of these dishes from the kitchen of the Balkan States with vegetables and garlic. The test results showed an improvement of the translation quality by about 40% for texts containing the affected concepts. 4.2 Automatic subject area recognition In order to overcome the problems mentioned above (manual coding effort, required user interaction), a component for automatic topic identification has been developed and integrated into the PT. Its principle works in a similar way to neural transfer. The most important difference is that the automatic topic identifier assigns the 
recognised subject area to the whole text to be translated, whereas the neural transfer can operate on the single paragraph level. 4.3 SmartCorrect Regarding the enormous amount of texts to be translated, most of which are from internet or other unscanned sources, it is not reasonable to expect from MT users to keep control of correct spelling. Nevertheless, a MT system is only able to translate correctly spelled words. For these reasons most MT systems, as well as text processing programmes, include a spellchecker. The problem is that they mostly just identify the typos/spelling errors and leave it up to the user to choose the correct form from a list of suggestions. This is process which requires intensive user interaction and experience has taught us, that users are not always ready to invest their time. In addition, this can only be expected if the text to be corrected belongs to the language mastered by the user. Therefore Linguatec developed SmartCorrect which not only recognises spelling errors in the text but also corrects them automatically. Trained on very large corpora, the model is likely to detect the best variant in nearly all cases. Clever enough, it cooperates with the named entities recogniser and thus does not identify unknown proper names as spelling errors. Entries from the user lexicons are also save from SmartCorrect intervention. However, a major part of the misspelling corrections is already performed in a pre-processing step, which adopts some proven methods4 to identify and correct frequent errors, such as letter deletion, insertion, substitution, inversion and duplication. 5 WMT2011 Submission  We participate for the German-English translation direction. Linguatec has not used the training corpus because we wanted to submit the results of our general purpose MT system. The only system tuning consisted of lexicon coding. Unknown words were detected automatically by analysing the test set. Appropriate translations were found, some of them from the training corpus. About 200 terms were manually coded or imported into the PT lexicon.                                                             4 cf. Habash (2008) 
306
Furthermore, we have observed that the test set contained some spelling errors which have been corrected by SmartCorrect (ca. 150 misspelling corrections were done), for example:  offiziel => offiziell Sympatie => Sympathie enh?llten => enth?llten bessseren => besseren unbwohnbar => unbewohnbar zwiwchen => zwischen  Thus, for comparison purposes we translated the test set three times: ? Out-of-the-box PT, without SmartCorrect ? Out-of-the-box PT, with SmartCorrect ? Out-of-the-box PT, with SmartCorrect plus lexicon adaptation The BLEU score in the first run was 17,0. Interestingly, the BLEU score of the second run did not reflect any improvements caused by correction of typos; on the contrary, it declined by 0,2  from 17,0 to 16,8. However, by manual evaluation of sample sentences we gained a more positive impression of the results. With the third run, after the lexicon coding, a BLEU of 17,1, i.e. a minimal increase compared with the firs run, was achieved. Here again, the manual inspection of random sentences, containing the coded terms, left an impression of some more significant improvements than measured by BLEU. 5.1 Conclusion Automatic metrics have shown a minimal improvement of translation quality. However, the manual inspection suggested much more significant influences of spelling correction and lexicon coding on the translation adequacy and sentence structure and consequently on the readability of the output than the BLEU score did. 5.2 Combined system submission by DFKI At WMT 2011 our PT will also participate in the combined translation task in a combination of rule-based and SMT systems submitted by the DFKI5.  
                                                           5 Xu et al(2011) 
6 Outlook Simultaneously with the current submission a ?hybrid experiment? was performed: An attempt at using SMT methods to improve the transfer selection for coding new entries in PT. An existing (crawled) parallel corpus in the automotive domain was cleaned, segmented by Liguatec sentence splitter, sentence-aligned by Hunalign (supported by using the Linguatec dictionary), word-aligned by GIZA++ and finally phrase tables were produced by using Moses. The objective was to extract meaningful phrases and their translations which are particularly suitable for import into the PT lexicon and thus generate a glossary. First a phrase table filter, based on frequency, was applied. Then part of speech information was added to both source and target entries as a basis for filtering linguistically motivated phrases. A glossary was generated. For testing purposes a very small set of about 250 terms, namely those which were unknown in the PT lexicon, was chosen to be imported. On a test corpus of about 320 sentences from the automotive domain the translation quality improvement, measured by BLEU, turned out to be about 3.1% (before coding: 14.87, after coding: 17.97). We will continue researching in that field. References  Bogdan Babych, Anthony Hartley. 2003. Improving Machine Translation Quality with Automatic Named Entity Recognition. Proc. EACL-EAMT, Budapest. Arendse Bernth. 1992. The LMT Book. IBM Deutschland Informationssysteme GmbH Scientific Center Institute for Logic and Linguistics. Nazar Habash. 2008. Four techniqes for Online Handling of Out-of-Vocabulary Words in Arabic-English Statistical Machine Translation. Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 57-60, Columbus, Ohio, USA. Roland Kaplan and Joan Bresnan. 1982. Lexical functional grammar: A formal system for grammatical representation. In Joan Bresnan (Ed.) The mental representation of Grammatical Relations. MIT Press. Michael McCord. 1989. A new version of slot grammar. Research report RC 14506, IBM research division, Yorktown Heights. 
307
Michael McCord and Arendse Bernth. 1992. Using Slot Grammar. IBM Deutschland Informationssysteme GmbH Scientific Center Institute for Logic and Linguistics. Gregor Thurmair. 2004. Using corpus information to improve MT quality. In Yuste Rodrigo, Elia (ed) Paris: ELRA (European Language Resources Association): Proceedings of the Third International Workshop on Language Resources for Translation Work, Reseach & Training (LR4Trans-III) Gregor Thurmair. 2005. Improving MT Quality: Towards a Hybrid MT Architecture in the Linguatec ?Personal Translator?. International MT Summit X, Phuket. Invited paper. Gregor Thurmair. 2009. Comparing different architectures of hybrid Machine Translation systems. Proceedings of the Twelft Machine Translation Summit. Ottawa, Canada. p.340-348 Jia Xu, Xiaojun Zhang, David Vilar, Casey Kennington and Hans Uszkoreit. 2011. The DFKI Hybrid Machine Translation System for WMT 2011 - On the Integration of SMT and RBMT. Submission paper for WMT 2011 sixth workshop on statistical machine translation. Edinburgh.  
308
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 43?51,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
A modular open-source focused crawler for mining monolingual and
bilingual corpora from the web
Vassilis Papavassiliou
Institute for Language and Speech Processing
Athena Research and Innovation Center
Athens, Greece
{vpapa, prokopis}@ilsp.gr
Prokopis Prokopidis Gregor Thurmair
Linguatec
Gottfried-Keller-Str. 12, 81245
Munich, Germany
g.thurmair@linguatec.de
Abstract
This paper discusses a modular and open-
source focused crawler (ILSP-FC) for the
automatic acquisition of domain-specific
monolingual and bilingual corpora from
the Web. Besides describing the main
modules integrated in the crawler (dealing
with page fetching, normalization, clean-
ing, text classification, de-duplication and
document pair detection), we evaluate sev-
eral of the system functionalities in an ex-
periment for the acquisition of pairs of par-
allel documents in German and Italian for
the "Health & Safety at work" domain.
1 Introduction and motivation
There is a growing literature on using the Web for
constructing various types of text collections, in-
cluding monolingual, comparable, parallel and/or
domain-specific corpora. Such resources can
be used by linguists studying language use and
change (Kilgarriff and Grefenstette, 2003), and at
the same time they can be exploited in applied re-
search fields like machine translation and multi-
lingual information extraction. Moreover, these
collections of raw data can be automatically an-
notated and used to produce, by means of induc-
tion tools, a second order or synthesized deriva-
tives: rich lexica (with morphological, syntactic
and lexico-semantic information), large bilingual
dictionaries (word andmultiword based) and trans-
fer grammars.
To this end, several tools (i.e. web crawlers,
HTMLparsers, language identifiers, HTML clean-
ers, etc.) have been developed and combined in
order to produce corpora useful for specific tasks.
However, to the best of our knowledge, most of
the available systems either omit some processing
tasks or require access to the results of a search en-
gine. For instance, the BootCaT toolkit (Baroni et
al., 2006), a well-known suite of Perl scripts for
bootstrapping specialized language corpora from
the web, uses the Bing search engine and allows
up to 5,000 queries per month.
In this paper, we present ILSP-FC, a modular
system that includes components and methods for
all the tasks required to acquire domain-specific
corpora from the Web. The system is available as
an open-source Java project1 and due to its modu-
lar architecture, each of its components can be eas-
ily substituted by alternatives with the same func-
tionalities. Depending on user-defined configura-
tion, the crawler employs processing workflows
for the creation of either monolingual or bilingual
collections. For users wishing to try the system be-
fore downloading it, two web services2 allow them
to experiment with different configuration settings
for the construction of monolingual and bilingual
domain-specific corpora.
The organization of the rest of the paper is as fol-
lows. In Section 2, we refer to recent related work.
In Section 3, we describe in detail the workflow
of the proposed system. A solution for bootstrap-
ping the focused crawler input is presented in Sec-
tion 4. Then, an experiment on acquiring parallel
documents in German and Italian for the "Health
& Safety at work" domain (H&S) is described in
Section 5, which also includes evaluation results
on a set of criteria including parallelness and do-
main specificity. We conclude and mention future
work in Section 6.
2 Related work
Web crawling for building domain-specific mono-
lingual and/or parallel data involves several tasks
(e.g. link ranking, cleaning, text classification,
near-duplicates removal) that remain open issues.
Even though there are several proposed methods
1http://nlp.ilsp.gr/redmine/projects/
ilsp-fc
2http://nlp.ilsp.gr/ws/
43
for each of these tasks, in this section we refer only
to a few indicative approaches.
Olston and Najork (2010) outline the funda-
mental challenges and describe the state-of-the-art
models and solutions for web crawling. A gen-
eral framework to fairly evaluate focused crawling
algorithms under a number of performance met-
rics is proposed by Srinivasan et al (2005). A
short overview of cleaning methods is presented in
Spousta et al (2008) and the comparison of such
methods is discussed in Baroni et al (2008). Sev-
eral algorithms (Qi and Davison, 2009) exploit the
main content and the HTML tags of a web page
in order to classify a page as relevant to a targeted
domain or not. Methods for the detection and re-
moval of near-duplicates (i.e. acquired web pages
that have almost the same content) are reviewed
and compared in Theobald et al (2008).
Efficient focused web crawlers can be built
by adapting existing open-source frameworks like
Heritrix3, Nutch4 and Bixo5. For instance, Com-
bine6 is an open-source focused crawler that is
based on a combination of a general web crawler
and a text classifier. Other approaches make use
of search engines APIs to identify in-domain web
pages (Hong et al, 2010) or multilingual web sites
(Resnik and Smith, 2003). Starting from these
pages, Almeida and Sim?es (2010) try to detect
which links point to translations, while Shi et al
(2006) harvest multilingual web sites and extract
parallel content from them. Bitextor (Espl?-Gomis
and Forcada, 2010) combines language identifica-
tion with shallow features that represent HTML
structures to mine parallel pages.
Besides structure similarity, systems like PT-
Miner (Nie et al, 1999) and WeBiText (D?silets
et al, 2008) filtered fetched web pages by keep-
ing only those containing languagemarkers in their
URLs. Chen et al (2004) proposed the Parallel
Text Identification System, which incorporated a
content analysis module using a predefined bilin-
gual wordlist. Similarly, Zhang et al (2006) and
Utiyama et al (2009) adopted the use of aligners
in order to estimate the content similarity of candi-
date parallel web pages or mixed languages pages.
Barbosa et al (2012) proposed the use of bilin-
gual dictionaries and generated translations (e.g.
byGoogle Translate andMicrosoft Bing) to extract
3http://crawler.archive.org/
4http://nutch.apache.org
5http://openbixo.org/
6http://combine.it.lth.se/
parallel content from multilingual sites.
3 System architecture
In this section, we describe the main modules inte-
grated in ILSP-FC. In general, the crawler initial-
izes its frontier (i.e. the list of pages to be visited)
from a seed URL list provided by the user (or con-
structed semi-automatically, see Section 4), clas-
sifies fetched pages as relevant to the targeted do-
main, extracts links from fetched web pages and
adds them to the list of pages to be visited.
Focused crawler
page fetching
normalization
cleaning
language
identification
text classification
link extraction
exporting
deduplication
seed
URL list
domain
definition
in-domain
pages
detection of
parallel documents
document pairs
Figure 1: System architecture
In order to ensure modularity and scalability, the
crawler is built using Bixo, an open source web
mining toolkit that allows easy configuration of
workflows and runs on top of the Hadoop7 frame-
work for distributed data processing.
3.1 Page Fetcher
The first module concerns page fetching. A
multithreaded crawling implementation has been
adopted in order to ensure concurrent visiting of
multiple hosts. Users can configure several set-
tings that determine the fetching process, includ-
ing number of concurrent harvesters and filtering
out specific document types. The crawler always
respects standard robots.txt files, while politeness
can also be affected with the use of settings re-
garding time intervals for revisiting URLs from the
same website, maximum number of URLs from a
specific host per iteration, maximum number of at-
tempts to fetch a web page etc.
7http://hadoop.apache.org
44
3.2 Normalizer
The normalizer module uses the Apache Tika
toolkit 8 to parse the structure of each fetched web
page and extract its metadata. Extracted metadata
are exported at a later stage (see Subsection 3.7)
if the web document is considered relevant to the
domain. The text encoding of the web page is also
detected based on the HTTP Content-Encoding
header and the charset part of the Content-Type
header, and if needed, the content is converted into
UTF-8. Besides default conversion, special care is
taken for normalization of specific characters like
no break space, narrow no-break space, three-per-
em space, etc.
3.3 Cleaner
Apart from its textual content, a typical web page
also contains boilerplate, i.e. "noisy" elements like
navigation headers, advertisements, disclaimers,
etc., which are of only limited or no use for the pro-
duction of good-quality language resources. For
removing boileplate, we use a modified version of
Boilerpipe 9 (Kohlsch?tter et al 2010) that also
extracts structural information like title, heading
and list item. At this stage, text is also segmented
in paragraphs on the basis of specific HTML tags
like<p>, <br> and<li>. Paragraphs judged to be
boilerplate and/or detected as titles, etc. are prop-
erly annotated (see Subsection 3.7).
3.4 Language Identifier
The next processing module deals with language
identification. We use the Cybozu10 language
identification library that considers n-grams as fea-
tures and exploits a Naive Bayes classifier for lan-
guage identification. If a web page is not in the
targeted language, its only further use is in extrac-
tion of new links. Even though the main content of
a web page is in the targeted language, it is likely
that the web page includes a few paragraphs that
are not in this language.Thus, the language iden-
tifier is also applied on each paragraph and marks
them properly (see Subsection 3.7).
3.5 Text Classifier
The aim of this module is to identify if a page
that is normalized and in the targeted language
contains data relevant to the targeted domain. To
8http://tika.apache.org
9http://code.google.com/p/boilerpipe/
10http://code.google.com/p/language-detection/
this end, the content of the page is compared
to a user-provided domain definition. Following
the string-matching method adopted by the Com-
bine web crawler, the definition consists of term
triplets (<relevance weight, (multi-word) term,
subdomain>) that describe a domain and, option-
ally, subcategories of this domain. Language-
dependent stemmers from the Lucene11 project
are used to stem user-provided terms and docu-
ment content. Based on the number of terms? oc-
currences, their location in the web page and the
weights of found terms, a page relevance score p
is calculated as follows:
p =
N
?
i=1
4
?
j=1
nij ? wti ? wlj ,
whereN is the amount of terms in the domain def-
inition, wti is the weight of term i, wlj is the weight
of location j and nij denotes the number of occur-
rences of term i in location j. The four discrete
locations in a web page are title, metadata, key-
words, and plain text, with respective weights of
10, 4, 2, and 1.
Moreover, the amount of unique domain terms
found in the main content of the page,m, is calcu-
lated. Then, the values p andm are compared with
two predefined thresholds (t1 and t2) and if both
values are higher than the thresholds, the web page
is categorized as relevant to the domain and stored.
It is worth mentioning that the user can affect the
strictness of the classifier by setting the values of
both thresholds in the crawler's configuration file.
3.6 Link Extractor
Even when a web page is not stored (because it
was deemed irrelevant to the domain, or not in
the targeted language), its links are extracted and
added to the list of links scheduled to be visited.
Since the crawling strategy is a critical issue for
a focused crawler, the links should be ranked and
the most promising links (i.e. links that point to
"in-domain" web pages or candidate translations)
should be followed first. To this end, a score sl is
calculated for each link l as follows:
sl = c+ p/L+
N
?
i=1
ni ? wi
where L is the amount of links originating from
the source page, N is the amount of terms in the
domain definition, ni denotes the number of occur-
rences of the i-th term in the link's surrounding text
and wi is the weight of the i-th term. By using this
11http://lucene.apache.org/
45
formulation, the score link is mainly influenced by
the "domainess" of its surrounding text.
The parameter c is only added in case the
crawler is used for building bilingual collections.
It gets a high positive value if the link under con-
sideration originates from a web page in L1 and
"points" to a web page that is probably in L2.
This is the case when, for example, L2 is Ger-
man and the anchor text contains strings like "de",
"Deutsch", etc. The insertion of this parameter
forces the crawler to visit candidate translations
before following other links.
3.7 Exporter
The Exporter module generates an XML file for
each stored web document. Each file contains
metadata (e.g. language, domain, URL, etc.)
about the corresponding document inside a header
element. Moreover, a <body> element contains
the content of the document segmented in para-
graphs. Apart from normalized text, each para-
graph element <p> is enriched with attributes pro-
viding more information about the process out-
come. Specifically, <p> elements in the XML
files may contain the following attributes: i)
crawlinfo with possible values boilerplate, mean-
ing that the paragraph has been considered boil-
erplate (see Subsection 3.3), or ooi-lang, meaning
that the paragraph is not in the targeted language;
ii) typewith possible values: title, heading and lis-
titem; and iii) topicwith a string value including all
terms from the domain definition detected in this
paragraph.
3.8 De-duplicator
Ignoring the fact12 that the web contains many
near-duplicate documents could have a negative
effect in creating a representative corpus. Thus,
the crawler includes a de-duplicator module that
represents each document as a list containing the
MD5 hashes of the main content's paragraphs, i.e.
paragraphs without the crawlinfo attribute. Each
document list is checked against all other docu-
ment lists, and for each candidate pair, the inter-
section of the lists is calculated. If the ratio of
the intersection cardinality to the cardinality of the
shortest list is more than 0.8, the documents are
considered near-duplicates and the shortest is dis-
carded.
12Baroni et al (2009) reported that during building of the
Wacky corpora, the amount of collected documents was re-
duced by more than 50% after de-duplication.
3.9 Pair Detector
After in-domain pages are downloaded, the Pair
Detector module uses two complementary meth-
ods to identify pairs of pages that could be con-
sidered parallel. The first method is based on co-
occurrences, in two documents, of images with the
same filename, while the second takes into account
structural similarity.
In order to explain the workflow of the pair de-
tection module, we will use the multilingual web-
site http://www.suva.ch as a running exam-
ple. Crawling this website using the processes de-
scribed in previous subsections provides a pool of
707 HTML files (and their exported XML counter-
parts) that are found relevant to the H&S domain
and in the targeted DE and IT languages (376 and
331 files, respectively).
Each XML file is parsed and the following
features are extracted: i) the document lan-
guage; ii) the depth of the original source page,
(e.g. for http://domain.org/d1/d2/d3/
page.html, depth is 4); iii) the amount of para-
graphs; iv) the length (in terms of tokens) of the
clean text; and v) the fingerprint of the main con-
tent, which is a sequence of integers that represent
the structural information of the page, in a way
similar to the approach described by Espl?-Gomis
and Forcada (2010). For instance, the fingerprint
of the extract in Figure 2 is [-2, 28, 145, -4, 9, -3,
48, -5, 740] with boilerplate paragraphs ignored; -
2, -3 and -4 denote that the type attributes of corre-
sponding<p> elements have title, heading and lis-
titem values, respectively; -5 denotes the existence
of the topic attribute in the last <p>; and positive
integers are paragraph lengths in characters.
The language feature is used to filter out pairs of
files that are in the same language. Pages that have
a depth difference above 1 are also filtered out, on
the assumption that it is very likely that translations
are found at the same or neighbouring depths of the
web site tree.
Next, we extract the filenames of the images
from HTML source and each document is repre-
sented as a list of image filenames. Since it is very
likely that some images appear inmanyweb pages,
we count the occurrence frequency of each image
and discard relatively frequent images (i.e. Face-
book and Twitter icons, logos etc.) from the lists.
In order to classify images into "critical" or
"common" (see Figure 3) we need to calcu-
late a threshold. In principle, one should ex-
46
<p type="title">Strategia degli investimenti</p> <!-- -2, 28-->
<p >I ricavi degli investimenti sono un elemento essenziale per finanziare le
rendite e mantenere il potere d'acquisto dei beneficiari delle rendite.</p>
<!--145-->
<p type="listitem">Document:</p> <!-- -4, 9 -->
<p crawlinfo="boilerplate" type="listitem">Factsheet "La strategia d'investimento
della Suva in sintesi" (Il link viene aperto in una nuova finestra) </p> <!--
ignored -->
<p type="heading">Perch? la Suva effettua investimenti finanziari?</p> <!-- -3,
48-->
<p topic="prevenzione degli infortuni;infortunio sul lavoro">Nonostante i molti
sforzi compiuti nella prevenzione degli infortuni sul lavoro e nel tempo libero
ogni anno accadono oltre 2500 infortuni con conseguenze invalidanti o mortali.
In questi casi si versa una rendita per invalidit? agli infortunati oppure una
rendita per orfani o vedovile ai superstiti. Nello stesso anno in cui
attribuisce una rendita, la Suva provvede ad accantonare i mezzi necessari a
pagare le rendite future. La maggior parte del patrimonio investito dalla Suva ?
rappresentato proprio da questi mezzi, ossia dal capitale di copertura delle
rendite. La restante parte del patrimonio ? costituta da accantonamenti per
prestazioni assicurative a breve termine come le spese di cura, le indennit?
giornaliere e le riserve.</p> <!-- -5, 740-->
Figure 2: An extract of an XML file for an Italian web page relevant to the H&S domain.
Figure 3: Critical (dashed) and common (dotted) images in a multilingual (EN/DE) site.
pect that low/high frequencies correspond to "crit-
ical"/"common" images. We employ a non-
parametric approach for estimating the probabil-
ity density function (Alpaydin, 2010) of the image
frequencies using the following formula:
p?(x) = 1Mh
M
?
t=1
K(x?xth )
where the random variable x defines the positions
(i.e. images frequencies) at which the p?(x) will be
estimated, M is the amount of images, xt denotes
the values of data samples in the region of width
h around the variable x, and K(?) is the normal
kernel that defines the influence of values xt in the
estimation of p?(x). The optimal value for h, the
optimal bandwidth of the kernel smoothing win-
dow, was calculated as described in Bowman and
Azzalini (1997).
Figure 4 illustrates the normalized histogram of
image frequencies in the example collection and
the estimated probability density function. One
can identify a main lobe in the low values, which
corresponds to "critical" images. Thus, the thresh-
old is chosen to be equal to the minimum just af-
ter this lobe. The underlining assumption is that if
a web page in L1 contains image(s) then the web
page with its translation in L2 will contain more or
less the same images. In case this assumption is not
valid for a multilingual site (i.e. there are only im-
ages that appear in all pages, e.g. template icons),
probably all images will be included. To eliminate
this, we discard images that exist in more than 10%
of the total HTML files.
Following this step, each document is exam-
ined against all others and two documents are con-
sidered parallel if a) the ratio of their paragraph
amounts (the ratio of their lengths in terms of para-
47
0 2 4 6 8 10 12 14 16 180
0.1
0.2
0.3
0.4
0.5
log2(frequency)
den
sity
Figure 4: The normalized histogram and the esti-
mated pdf of the image frequencies.
graphs), b) the ratio of their clean text lengths (in
terms of tokens), and c) the Jaccard similarity co-
efficient of their image lists, are higher than em-
pirically predefined thresholds.
More pairs are detected by examining structure
similarity. Since the XML files contain informa-
tion about structure, content (i.e. titles, headings,
list items) and domain specificity (i.e. paragraphs
with the topic attribute), we use these files instead
of examining the similarity of the HTML source.
A 3-dimensional feature vector is constructed for
each candidate pair of parallel documents. The
first element in this vector is the ratio of their fin-
gerprint lengths, the second is the ratio of their
sizes in paragraphs, and the third is the ratio of the
edit distance of the fingerprints of the two docu-
ments to the maximum fingerprint length. Clas-
sification of a pair as parallel is achieved using a
soft-margin polynomial Support Vector Machine
trained with the positive and negative examples
collected during our previous work (Pecina et al,
2012). Note that the dataset included only candi-
date pairs that met the criteria regarding the ratio
of paragraphs amounts and the ratio of text lengths,
mentioned above. As a result, negative instances
(i.e. pairs of documents that have similar structure
but are not real pairs) did not heavily outnumber
positive ones and thus the training was not imbal-
anced (Akbani et al, 2004).
4 Bootstrapping the input of the focused
crawler
In the work presented in previous sections, we as-
sumed that users had access to already existing
lists of seed terms and URLs for the initializa-
tion of the frontier and the classifier. But what if
manually compiled resources for a particular do-
main/language(s) combination (e.g. ES/FR termi-
nology for endocrinology or lists of EN/DE web
sites related to floriculture) are impossible or diffi-
cult to find? Can we bootstrap such resources and
provide them to users for post-editing? In this sec-
tion, we present ongoing work towards this goal
using the category graph and the external links of
multilingual editions of Wikipedia.
We initialize the bootstrapping process by
searching for a term defining the domain of in-
terest (e.g. "ballet", "automotive accessories") in
the category graph of the EN wikipedia. If a cat-
egory is found, we recursively collect all pages in
this category and its subcategories for a predefined
depth. For each page we extract its title and we
consider it a term that can participate in a list of
domain-related seed terms. We use a set of pattern
matching rules that exclude certain titles like those
of disambiguation and redirect pages. Other rules
exclude titles that refer to lists of related pages or
titles that use upper case or title case and are proba-
bly abbreviations and named entities, respectively.
Obviously, in a different setting where, for exam-
ple, a user is interested in discovering named enti-
ties related to a domain, these titles should be han-
dled differently.
The next step involves utilizing the links from
each EN page to articles in wikipedias written in
other languages. Based on which languages we are
interested in, we again consider each title a seed
term in language LANG, this time also storing the
information that the term is also a LANG transla-
tion of the EN term.
During traversing the EN category graph and
visiting corresponding articles in other languages,
we also populate a list of seed URLs for the fo-
cused crawler, by keeping record of all links to
URLs outside wikipedia.org. At this stage,
we have all necessary resources to initiate mono-
lingual focused crawls in each language we are in-
terested in.
An optional last stage targets the automatic dis-
covery of sites with multilingual content where
parallel documents can be extracted from. During
this stage, we visit each of the external links we
collected and detect the language of the web page
this link points to. From this web page, we extract
its links and examine whether the anchor text of
each link matches a set of patterns indicating that
48
this link points to a translation (in a way similar to
the process described in Subsection 3.6). If trans-
lation links are found, we store the site as a candi-
date for bilingual focused crawling. Also, since it
is common that links to multilingual editions of a
web site are not present in all of its pages, we re-
peat the same process for the home page of the site.
Notice that it is a task for the FC to detect whether
these sites (or one of their sections) also contain
parallel documents in the targeted domain.
In a first set of experiments following this ap-
proach, we used September 2012 snapshots13 for
English, French, German, Greek, Portuguese and
Spanish wikipedias (EN, FR, DE, EL, PT and ES,
respectively). Although we leave detailed eval-
uation of created resources for future work, we
present as example output a list of terms related to
"Flowers" in Table 1. Notice that, since the num-
ber of articles of multilingual wikipedias varies
considerably, the term list extracted for languages
like EL is, as expected, smaller compared, for ex-
ample, to the 547 and 293 terms collected for EN
and ES, respectively. Finally, using the URLs ex-
tracted from the articles on the "Flowers" domain,
Table 2 contains a sample of web sites detected for
containing relevant multilingual content.
5 Evaluation Results
In order to assess the quality of the resources that
ILSP-FC can produce, we evaluated it in a task of
acquiring pairs of parallel documents in German
and Italian for the "Health & Safety at work" (Ar-
beitsschutz/Sicurezza sul lavoro) domain. We as-
sume that this task is relatively difficult, i.e. that
the number of documents in this domain and pair of
languages is relatively small in the web. Overall,
our system delivered 807 document pairs for H&S,
containing 1.40 and 1.21 million tokens for IT and
DE, respectively. Numbers refer to tokens in the
main content of the acquired web pages, i.e. to to-
kens in paragraphs without the attribute crawlinfo
(see Subsection 3.7).
A sample of the acquired corpora were evalu-
ated against a set of criteria discussed in the fol-
lowing subsections. We randomly selected 103
document pairs for manual inspection. The sample
size was calculated according to a 95% confidence
level and an at most 10% confidence interval.
13We use the JavaWikipedia Library (Zesch et al, 2008) to
convert each snapshot into a database that allows structured
access to several aspects of categories, articles, sections etc.
5.1 Parallelness
The number of the correctly identified parallel doc-
ument pairs was obviously critical in this particular
evaluation setting. We focused on the precision of
the pair detector module, since it is not feasible to
count how many pairs were missed. In the subset
examined, 94 and 4 document pairs were judged as
parallel and not parallel, respectively. The other
5 pairs were considered borderline cases, where
more than 20% of the sentences in one document
were translated in the other. Since about 95% of
the crawled data are of good or sufficiently good
quality, this shows that they are usable for further
processing, e.g. for sentence alignment.
5.2 Domain specificity
We next evaluated how many documents in the se-
lected data fit the targeted domain in both the IT
and the DE partitions. The overall precision was
about 77%, with 79 IT documents and 80DE docu-
ments found relevant to the narrow domain chosen
for evaluation.
Reported results on text-to-topic classification
sometimes score higher; however they neglect a
critical factor of influence, namely the distance be-
tween training and prediction datasets. In the "real
world", scores between 75% and 85% are realistic
to assume. It should be mentioned that the preci-
sion of the topic classifier strongly depends on the
quality of the seed terms: by inspecting results,
modifying the seed term list and re-crawling, re-
sults could easily be improved further.
5.3 Language identification
Since the language identifier is applied on every
paragraph of the main content of each web page,
we examined how many of the paragraphs have
been marked correctly. Overall, 5223 and 4814
paragraphs of IT and DE documents were checked
and only 13 and 65wrong assignments were found,
respectively.
Most errors (about 80%) were found in a sin-
gle document with a lot of tokens denoting chem-
ical substances that seem to confuse the language
identifier. When excluding this document, figures
rise to 99,67% and 99,95% for the DE and IT par-
titions, respectively. The rest of the errors mainly
occurred in paragraphs containing sentences in dif-
ferent languages.
49
EN: 547 DE: 255 EL: 22 ES: 293 FR: 286 IT: 143 PT: 164
Gardenia Gardenien ???????? Gardenia Gard?nia Gardenia Gardenia
Calendula Ringelblumen ?????????? Calendula Calendula Calendula Calendula
Lilium Lilien ????? Lilium Lys Lilium L?rio
Peony Pfingstrosen ??????? Paeoniaceae Pivoine Paeonia Paeoniaceae
Tulip Tulpen ??????? Tulipa Tulipe Tulipa Tulipa
Flower Bl?te ????? Flor Fleur Fiore Flor
Crocus Krokusse ?????? Crocus Crocus Crocus Crocus
Anemone Windr?schen ??????? Anemone An?mone Anemone Anemone
Table 1: Sample seed terms for the "Flowers" domain in 7 languages, collected automatically frommulti-
lingual editions ofWikipedia. The header of the table refers to the total terms collected for each language.
Wikipedia article Seed URL WebSite Langs
EN: Omphalodes_verna http://goo.gl/msyIc http://www.luontoportti.com de,en,es,fr
ES: Tropaeolum http://goo.gl/Ec5uK http://www.chileflora.com de,en,es
EN: Erythronium americanum http://goo.gl/nEP2L http://wildaboutgardening.org en,fr
DE: Nickendes_Leimkraut http://goo.gl/nuHNe http://www.wildblumen.at de,en,pt
DE: Titanenwurz http://goo.gl/rLl9W http://www.wilhelma.de de,en
Table 2: Automatically detected web sites with multilingual content related to the "Flowers" domain.
Column 1 presents the original LANG.wikipedia.org article from which the (shortened for readability
purposes) seed URLs in column 2 were extracted. The seed URLs led to the 3rd column web sites, in
which content in the languages of the 4th column was found.
5.4 Boilerplate removal
For this evaluation aspect, we evaluated howmany
"good" paragraphs were judged to be boilerplate,
and how many "bad" paragraphs were missed. We
examined 23178 and 23176 paragraphs of IT and
DE documents and found 2326 and 2591 errors
with an overall error rate around 10%. It should
be noted that different strategies for boilerplate re-
moval can be followed. One "classical" option is to
remove everything that does not belong to the text,
i.e. headers, advertisements etc. that "frame" real
content. Another option is to attempt to remove
everything which is irrelevant for MT sentence
alignment; this goes beyond the first approach as it
also removes short textual chunks, copyright dis-
claimers, etc. Most of the errors reported herewere
mainly due to this difference; i.e. they were para-
graphs that were deemed not usable for MT align-
ment.
6 Conclusions and future work
In this paper we described and evaluated ILSP-FC,
a system for mining domain-specific monolingual
and bilingual corpora from the web. The system
is available as open-source and is modular in the
sense that each of its components can be easily sub-
stituted with similar software performing the same
functionalities. The crawler can also be tested via
web services that allow the user to perform exper-
iments without the need to install it.
We have already used the crawler in producing
monolingual and parallel corpora and other deriva-
tive resources. Evaluation has shown that the sys-
tem can be used effectively in collecting resources
of high quality, provided that the user can initial-
ize it with lists of seed terms and URLs that can be
easily found on the web. For domains for which
no similar lists are available, we presented ongo-
ing work for bootstrapping them frommultilingual
editions of Wikipedia. Future work includes eval-
uation and improvement of the bootstrapping com-
ponent, more sophisticated methods for text clas-
sification, and grouping of collected data based on
genre.
Acknowledgments
Work by the first two authors was partially funded
by the European Union QTLaunchPad (FP7,
Grant 296347) and Abu-MaTran (FP7-People-
IAPP, Grant 324414) projects. An initial version
of this work was produced during the EU Panacea
project (FP7-ICT, Grant 248064).
50
References
Rehan Akbani, Stephen Kwek, and Nathalie Japkow-
icz. 2004. Applying support vector machines to
imbalanced datasets. In In Proceedings of the 15th
European Conference onMachine Learning (ECML,
pages 39--50.
Jos? Jo?o Almeida and Alberto Sim?es. 2010. Auto-
matic parallel corpora and bilingual terminology ex-
traction from parallel websites. In 3rd Workshop on
Building and Using Comparable Corpora .
Ethem Alpaydin. 2010. Introduction to Machine
Learning. The MIT Press, 2nd edition.
Luciano Barbosa, Vivek Kumar Rangarajan Sridhar,
Mahsa Yarmohammadi, and Srinivas Bangalore.
2012. Harvesting parallel text in multiple languages
with limited supervision. In COLING, pages 201--
214.
Marco Baroni, Adam Kilgarriff, Jan Pomik?lek, and
Pavel Rychl?. 2006. WebBootCaT: Instant Domain-
Specific Corpora to Support Human Translators.
In Proceedings of the 11th Annual Conference of
EAMT, pages 47--252, Norway.
Marco Baroni, Francis Chantree, Adam Kilgarriff, and
Serge Sharoff. 2008. Cleaneval: a competition for
cleaning web pages. In LREC'08.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. TheWaCkyWideWeb: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209--226.
Adrian W. Bowman and Adelchi Azzalini. 1997.
Applied smoothing techniques for data analysis:
the kernel approach with S-Plus illustrations, vol-
ume 18. Oxford University Press.
Jisong Chen, Rowena Chau, and Chung-Hsing Yeh.
2004. Discovering parallel text from theWorldWide
Web. In Proceedings of ACSW Frontiers '04, vol-
ume 32, pages 157--161, Darlinghurst, Australia.
Alain D?silets, Benoit Farley, Marta Stojanovic, and
Genevi?ve Patenaude. 2008. WeBiText: Building
Large Heterogeneous Translation Memories from
Parallel Web Content. In Proceedings of Translat-
ing and the Computer (30), London, UK.
Miquel Espl?-Gomis and Mikel L. Forcada. 2010.
Combining Content-Based and URL-Based Heuris-
tics to Harvest Aligned Bitexts from Multilingual
Sites with Bitextor. The Prague Bulletin of Math-
emathical Lingustics, 93:77--86.
Gumwon Hong, Chi-Ho Li, Ming Zhou, and Hae-
Chang Rim. 2010. An empirical study on web min-
ing of parallel data. In Proceedings of the 23rd COL-
ING, pages 474--482.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistics, 29(3):333--348.
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and
Richard Durand. 1999. Cross-language information
retrieval based on parallel texts and automatic min-
ing of parallel texts from the Web. In Proceedings
of the 22nd annual international ACM SIGIR con-
ference on research and development in information
retrieval, pages 74--81, New York.
Christopher Olston and Marc Najork. 2010. Web
crawling. Found. Trends Inf. Retr., 4(3):175--246.
Pavel Pecina, Antonio Toral, Vassilis Papavassiliou,
Prokopis Prokopidis, and Josef van Genabith. 2012.
Domain adaptation of statistical machine translation
using web-crawled resources: a case study. In Pro-
ceedings of the 16th Annual Conference of EAMT,
pages 145--152, Trento, Italy.
Xiaoguang Qi and Brian D. Davison. 2009. Web page
classification: Features and algorithms. ACM Com-
puting Surveys, 41:11--31.
Philip Resnik and Noah A. Smith. 2003. The Web as a
parallel corpus. Computational Linguistics, 29:349-
-380.
Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.
2006. A dom tree alignment model for mining paral-
lel data from the web. In COLING/ACL-2006, pages
489--496.
Miroslav Spousta, Michal Marek, and Pavel Pecina.
2008. Victor: the Web-Page Cleaning Tool. In Pro-
ceedings of the 4th Web as Corpus Workshop - Can
we beat Google?, pages 12--17, Marrakech.
Padmini Srinivasan, Filippo Menczer, and Gautam
Pant. 2005. A General Evaluation Framework for
Topical Crawlers. Information Retrieval, 8:417--
447.
Martin Theobald, Jonathan Siddharth, and Andreas
Paepcke. 2008. Spotsigs: robust and efficient near
duplicate detection in large web collections. In Pro-
ceedings of the 31st annual international ACM SI-
GIR conference on research and development in in-
formation retrieval, pages 563--570.
Masao Utiyama, Daisuke Kawahara, Keiji Yasuda, and
Eiichiro Sumita. 2009. Mining parallel texts from
mixed-language web pages. InMT Summit.
Torsten Zesch, Christof M?ller, and Iryna Gurevych.
2008. Extracting lexical semantic knowledge from
wikipedia and wiktionary. In Proceedings of the 6th
International Conference on Language Resources
and Evaluation, Marrakech.
Ying Zhang, Ke Wu, Jianfeng Gao, and Phil Vines.
2006. Automatic Acquisition of Chinese-English
Parallel Corpus from the Web. In Proceedings of
the 28th European Conference on Information Re-
trieval, pages 420--431.
51
