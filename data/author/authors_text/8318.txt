Non-Factoid Japanese Question Answering through Passage Retrieval
that Is Weighted Based on Types of Answers
Masaki Murata and Sachiyo Tsukawaki
National Institute of Information and
Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
{murata,tsuka}@nict.go.jp
Qing Ma
Ryukoku University
Otsu, Shiga, 520-2194, Japan
qma@math.ryukoku.ac.jp
Toshiyuki Kanamaru
Kyoto University
Yoshida-Nihonmatsu-Cho, Sakyo
Kyoto, 606-8501 Japan
kanamaru@hi.h.kyoto-u.ac.jp
Hitoshi Isahara
National Institute of Information and
Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
isahara@nict.go.jp
Abstract
We constructed a system for answering non-
factoid Japanese questions. We used var-
ious methods of passage retrieval for the
system. We extracted paragraphs based on
terms from an input question and output
them as the preferred answers. We classified
the non-factoid questions into six categories.
We used a particular method for each cate-
gory. For example, we increased the scores
of paragraphs including the word ?reason?
for questions including the word ?why.? We
participated at NTCIR-6 QAC-4, where our
system obtained the most correct answers
out of all the eight participating teams. The
rate of accuracy was 0.77, which indicates
that our methods were effective.
1 Introduction
A question-answering system is an application de-
signed to produce the correct answer to a question
given as input. For example, when ?What is the
capital of Japan?? is given as input, a question-
answering system may retrieve text containing sen-
tences like ?Tokyo is Japan?s capital and the coun-
try?s largest and most important city?, and ?Tokyo
is also one of Japan?s 47 prefectures?, from Web-
sites, newspaper articles, or encyclopedias. The sys-
tem then outputs ?Tokyo? as the correct answer.
We believe question-answering systems will become
a more convenient alternative to other systems de-
signed for information retrieval and a basic compo-
nent of future artificial intelligence systems. Numer-
ous researchers have recently been attracted to this
important topic. These researchers have produced
many interesting studies on question-answering sys-
tems (Kupiec, 1993; Ittycheriah et al, 2001; Clarke
et al, 2001; Dumis et al, 2002; Magnini et al, 2002;
Moldovan et al, 2003). Evaluation conferences and
contests on question-answering systems have also
been held. In particular, the U.S.A. has held the Text
REtrieval Conferences (TREC) (TREC-10 commit-
tee, 2001), and Japan has hosted the Question-
Answering Challenges (QAC) (National Institute of
Informatics, 2002) at NTCIR (NII Test Collection
for IR Systems ) 3. These conferences and contests
have aimed at improving question-answering sys-
tems. The researchers who participate in these create
question-answering systems that they then use to an-
swer the same questions, and each system?s perfor-
mance is then evaluated to yield possible improve-
ments.
We addressed non-factoid question answering in
NTCIR-6 QAC-4. For example, when the question
was ?Why are people opposed to the Private Infor-
mation Protection Law?? the system retrieved sen-
tences based on terms appearing in the question and
output an answer using the retrieved sentences. Nu-
merous studies have addressed issues that are in-
volved in the answering of non-factoid questions
(Berger et al, 2000; Blair-Goldensohn et al, 2003;
727
Xu et al, 2003; Soricut and Brill, 2004; Han et al,
2005; Morooka and Fukumoto, 2006; Maehara et
al., 2006; Asada, 2006).
We constructed a system for answering non-
factoid Japanese questions for QAC-4. We used
methods of passage retrieval for the system. We
extracted paragraphs based on terms from an input
question and output them as the preferred answers.
We classified the non-factoid questions into six cat-
egories. We used a particular method for each cate-
gory. For example, we increased the scores of para-
graphs including the word ?reason? for questions
including the word ?why.? We performed exper-
iments using the NTCIR-6 QAC-4 data collection
and tested the effectiveness of our methods.
2 Categories of Non-Factoid Questions
We used six categories of non-factoid questions in
this study. We constructed the categories by con-
sulting the dry run data in QAC-4.
1. Definition-oriented questions (Questions that
require a definition to be given in response.)
e.g., K-1 to wa nandesuka? (What is K-1?)
2. Reason-oriented questions (Questions that re-
quire a reason to be given in response.)
e.g., kojin jouhou hokogou ni hantai shiteiru
hito wa doushite hantai shiteiru no desuka?
(Why are people opposed to the Private Infor-
mation Protection Law?)
3. Method-oriented questions (Questions that re-
quire an explanation of a method to be given in
response.)
e.g., sekai isan wa donoyouni shite kimeru no
desuka?? (How is a World Heritage Site deter-
mined?)
4. Degree-oriented questions (Questions that re-
quire an explanation of the degree of something
to be given in response.)
5. Change-oriented questions (Questions that re-
quire a description of things that change to be
given in response.)
e.g., shounen hou wa dou kawari mashitaka?
(How was the juvenile law changed?)
6. Detail-oriented questions (Questions that re-
quire a description of the particulars or details
surrounding a sequence of events to be given in
response.)
e.g., donoyouna keii de ryuukyuu oukoku wa ni-
hon no ichibu ni natta no desuka? (How did
Ryukyu come to belong to Japan?)
3 Question-answering Systems in this
Study
The system has three basic components:
1. Prediction of type of answer
The system predicts the answer to be a partic-
ular type of expression based on whether the
input question is indicated by an interrogative
pronoun, an adjective, or an adverb. For exam-
ple, if the input question is ?Why are people
opposed to the Private Information Protection
Law??, the word ?why? suggests that the an-
swer will be an expression that describes a rea-
son.
2. Document retrieval
The system extracts terms from the input ques-
tion and retrieves documents by using these
terms. Documents that are likely to contain
the correct answer are thus gathered during the
retrieval process. For example, for the input
question ?Why are people opposed to the Pri-
vate Information Protection Law??, the system
extracts ?people,? ?opposed,? ?Private,? ?Infor-
mation,? ?Protection,? and ?Law? as terms and
retrieves the appropriate documents based on
these.
3. Answer detection
The system separates the retrieved documents
into paragraphs and retrieves those that contain
terms from the input question and a clue ex-
pression (e.g., ?to wa? (copula sentence) for the
definition sentence). The system outputs the re-
trieved paragraphs as the preferred answer.
3.1 Prediction of type of answer
We used the following rules for predicting the type
of answer. We constructed the rules by consulting
the dry run data in QAC-4.
728
1. Definition-oriented questions Questions in-
cluding expressions such as ?to wa nani,?
?donna,? ?douiu,? ?douitta,? ?nanimono,?
?donoyouna mono,? ?donna mono,? and ?douiu
koto? (which all mean ?what is?) are rec-
ognized by the system as being definition-
oriented questions.
2. Reason-oriented questions Questions including
expressions such as ?naze? (why), ?naniyue?
(why), ?doushite? (why), ?nani ga riyuu de?
(what is the reason), and ?donna riyuu de?
(what reason), are recognized by the system as
being reason-oriented questions.
3. Method-oriented questions Questions includ-
ing expressions such as ?dou,? ?dousureba,?
?douyatte,? ?dono youni shite,? ?ikani shite,?
?ikani,? and ?donnna houhou de? (which all
mean ?how?) are recognized by the system as
being method-oriented questions.
4. Degree-oriented questions Questions including
expressions such as ?dorekurai? (how much),
?dorekurai no? (to what extent), and ?dono
teido? (to what extent), are recognized by the
system as being degree-oriented questions.
5. Change-oriented questions Questions includ-
ing expressions such as ?naniga chigau? (What
is different), ?donoyuni kawaru? (How is ...
changed), and ?dokoga kotonaru? (What is dif-
ferent), are recognized by the system as being
change-oriented questions.
6. Detail-oriented questions Questions including
expressions such as ?dono you na keii,? ?dono
you na ikisatsu,? and ?dono you na nariyuki?
(which all mean ?how was?) are recognized by
the system as being detail-oriented questions.
3.2 Document retrieval
Our system extracts terms from a question by using
the morphological analyzer, ChaSen (Matsumoto et
al., 1999). The analyzer first eliminates preposi-
tions, articles, and similar parts of speech. It then
retrieves documents by using the extracted terms.
The documents are retrieved as follows:
We first retrieve the top k
dr1
documents with the
highest scores calculated using the equation
Score(d)
=
?
term t
?
?
?
tf(d, t)
tf(d, t) + kt
length(d) + k
+
? + k
+
? log
N
df(t)
?
?
?
,
(1)
where d is a document, t is a term extracted from
a question, and tf(d, t) is the frequency of t oc-
curring in d. Here, df(t) is the number of docu-
ments in which t appears, N is the total number
of documents, length(d) is the length of d, and ?
is the average length of all documents. Constants
k
t
and k
+
are defined based on experimental re-
sults. We based this equation on Robertson?s equa-
tion (Robertson and Walker, 1994; Robertson et al,
1994). This approach is very effective, and we have
used it extensively for information retrieval (Murata
et al, 2000; Murata et al, 2001; Murata et al, 2002).
The question-answering system uses a large number
for k
t
.
We extracted the top 300 documents and used
them in the next procedure.
3.3 Answer detection
In detecting answers, our system first generates can-
didate expressions for them from the extracted docu-
ments. We use two methods for extracting candidate
expressions. Method 1 uses a paragraph as a candi-
date expression. Method 2 uses a paragraph, two
continuous paragraphs, or three continuous para-
graphs as candidate expressions.
We award each candidate expression the follow-
ing score.
Score(d)
= ?mint1?T log
?
t2?T3
(2dist(t1, t2)df(t2)
N
)
+ 0.00000001 ? length(d)
= maxt1?T
?
t2?T3
log
N
2dist(t1, t2) ? df(t2)
+ 0.00000001 ? length(d)
(2)
729
T3 = {t|t ? T, 2dist(t1, t)df(t)
N
? 1}, (3)
where d is a candidate expression, T is the set of
terms in the question, dist(t1, t2) is the distance
between t1 and t2 (defined as the number of char-
acters between them with dist(t1, t2) = 0.5 when
t1 = t2), and length(d) is the number of charac-
ters in a candidate expression. The numerical term,
0.00000001 ? length(d), is used for increasing the
scores of long paragraphs.
For reason-oriented questions, our system uses
some reason terms such as ?riyuu? (reason),
?gen?in? (cause), and ?nazenara? (because) as terms
for Eq. 2 in addition to terms from the input ques-
tion. This is because we would like to increase the
score of a document that includes reason terms for
reason-oriented questions.
For method-oriented questions, our system uses
some method terms such as ?houhou? (method),
?tejun? (procedure), and ?kotoniyori? (by doing) as
terms for second document retrieval (re-ranking) in
addition to terms from the input question.
For detail-oriented questions, our system uses
some method terms such as ?keii? (a detail, or a se-
quence of events), ?haikei? (background), and ?rek-
ishi? (history) as terms for second document re-
trieval (re-ranking) in addition to terms from the in-
put question.
For degree-oriented questions, when candidate
paragraphs include numerical expressions, the score
(Score(d)) is multiplied by 1.1.
For definition-oriented questions, the system first
extracts focus expressions. When the question in-
cludes expressions such as ?X-wa?, ?X-towa?, ?X-
toiunowa?, and ?X-tte?, X is extracted as a fo-
cus expression. The system multiplies the score,
(Score(d)), of the candidate paragraph having ?X-
wa?, ?X-towa or something by 1.1. When the can-
didate expression includes focus expressions having
modifiers (including modifier clauses and modifier
phrases), the modifiers are used as candidate expres-
sions, and the scores of the candidate expressions are
multiplied by 1.1.
Below is an example of a candidate expression
that is a modifier clause in a sentence.
Table 1: Results
Method Correct A B C D
Method 1 57 18 42 10 89
Method 2 77 5 67 19 90
(There were a total of 100 questions.)
Question sentence: sekai isan jouyaku to
wa dono youna jouyaku desu ka?
(What is the Convention concerning the
Protection of the World Cultural and Nat-
ural Heritage?)
Sentence including answers:
1972 nen no dai 17 kai yunesuko soukai de
saitaku sareta sekai isan jouyaku ....
(Convention concerning the Pro-
tection of the World Cultural
and Natural Heritage, which
was adopted in 1972 in the 17th gen-
eral assembly meeting of the UN Educational,
Scientific and Cultural Organization.)
Finally, our system extracts candidate expressions
having high scores, (Score(d)s), as the preferred
output. Our system extracts candidate expressions
having scores that are no less than the highest score
multiplied by 0.9 as the preferred output.
We constructed the methods for answer detection
by consulting the dry run data in QAC-4.
4 Experiments
The experimental results are listed in Table 1. One
hundred non-factoid questions were used in the ex-
periment. The questions, which were generated by
the QAC-4 organizers, were natural and not gener-
ated by using target documents. The QAC-4 orga-
nizers checked four or fewer outputs for each ques-
tion. Methods 1 and 2 were used to determine what
we used as answer candidate expressions (Method 1
uses one paragraph as a candidate answer. Method
2 uses one paragraph, two paragraphs, or three para-
graphs as candidate answers.).
?A,? ?B,? ?C,? and ?D? are the evaluation criteria.
?A? indicates output that describes the same content
as that in the answer. Even if there is a supplemen-
tary expression in the output, which does not change
730
the content, the output is judged to be ?A.? ?B? in-
dicates output that contains some content similar to
that in the answer but contains different overall con-
tent. ?C? indicates output that contains part of the
same content as that in the answer. ?D? indicates
output does not contain any of the same content as
that in the answer. The numbers for ?A,? ?B,? ?C,?
and ?D? in Table 1 indicate the number of questions
where an output belongs to ?A,? ?B,? ?C,? and ?D?.
?Correct? indicates the number of questions where
an output belongs to ?A,? ?B,? or ?C?. The evalu-
ation criteria ?Correct? was also used officially at
NTCIR-6 QAC-4.
We found the following.
? Method 1 obtained higher scores in evaluation
A than Method 2. This indicates that Method 1
can extract a completely relevant answer more
accurately than Method 2.
? Method 2 obtained higher scores in evaluation
?Correct? than Method 1. The rate of accuracy
for Method 2 was 0.77 according to evaluation
?Correct?. This indicates that Method 2 can ex-
tract more partly relevant answers than Method
1. When we want to extract completely relevant
answers, we should use Method 1. When we
want to extract more answers, including partly
relevant answers, we should use Method 2.
? Method 2 was the most accurate (0.77) of those
used by all eight participating teams. We could
detect paragraphs as answers including input
terms and the key terms related to answer types
based the methods discussed in Section 3.3.
Our system obtained the best results because
our method of detecting answers was the most
effective.
Below is an example of the output of Method 1,
which was judged to be ?A.?
Question sentence:
jusei ran shindan wa douiu baai ni okon-
awareru noka?
(When is amniocentesis performed on a
pregnant woman?)
System output:
omoi idenbyou no kodono ga umareru no
wo fusegu.
(To prevent the birth of children with seri-
ous genetic disorders )
Examples of answers given by organizers:
omoi idenbyou
(A serious genetic disorder)
omoi idenbyou no kodomo ga umareru
kanousei ga takai baai
(To prevent the birth of children with seri-
ous genetic disorders.)
5 Conclusion
We constructed a system for answering non-factoid
Japanese questions. An example of a non-factoid
question is ?Why are people opposed to the Pri-
vate Information Protection Law?? We used vari-
ous methods of passage retrieval for the system. We
extracted paragraphs based on terms from an input
question and output them as the preferred answers.
We classified the non-factoid questions into six cat-
egories. We used a particular method for each cate-
gory. For example, we increased the scores of para-
graphs including the word ?reason? for questions in-
cluding the word ?why.? We participated at NTCIR-
6 QAC-4, where our system obtained the most cor-
rect answers out of all the eight participating teams.
The rate of accuracy was 0.77, which indicates that
our methods were effective.
We would like to apply our method and system to
Web data in the future. We would like to construct a
sophisticated system that can answer many kinds of
complicated queries such as non-factoid questions
based on a large amount of Web data.
Acknowledgements
We are grateful to all the organizers of NTCIR-6
who gave us the chance to participate in their con-
test to evaluate and improve our question-answering
system. We greatly appreciate the kindness of all
those who helped us.
731
References
Yoshiaki Asada. 2006. Processing of definition type
questions in a question answering system. Master?s
thesis, Yokohama National University. (in Japanese).
AdamBerger, Rich Caruana, David Cohn, Dayne Freitag,
and Vibhu Mittal. 2000. Bridging the lexical chasm:
Statistical approaches to answer-finding. In Proceed-
ings of the 23rd annual international ACM SIGIR con-
ference on Research and development in information
retrieval (SIGIR-2000), pages 192?199.
Sasha Blair-Goldensohn, Kathleen R. McKeown, and
Andrew Hazen Schlaikjer. 2003. A hybrid approach
for qa track definitional questions. In Proceedings
of the 12th Text Retrieval Conference (TREC-2003),
pages 185?192.
Charles L. A. Clarke, Gordon V. Cormack, and
Thomas R. Lynam. 2001. Exploiting redundancy
in question answering. In Proceedings of the 24th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Susan Dumis, Michele Banko, Eric Brill, Jimmy Lin, and
Andrew Ng. 2002. Web question answering: Is more
always better? In Proceedings of the 25th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval.
Kyoung-Soo Han, Young-In Song, Sang-Bum Kim, and
Hae-Chang Rim. 2005. Phrase-based definitional
question answering using definition terminology. In
Lecture Notes in Computer Science 3689, pages 246?
259.
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and
Adwait Ratnaparkhi. 2001. IBM?s Statistical Ques-
tion Answering System. In TREC-9 Proceedings.
Julian Kupiec. 1993. MURAX: A robust linguistic ap-
proach for question answering using an on-line ency-
clopedia. In Proceedings of the Sixteenth Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval.
Hideyuki Maehara, Jun?ichi Fukumoto, and Noriko
Kando. 2006. A BE-based automated evaluation
for question-answering system. IEICE-WGNLC2005-
109, pages 19?24. (in Japanese).
Bernardo Magnini, Matto Negri, Roberto Prevete, and
Hristo Tanev. 2002. Is it the right answer? Exploiting
web redundancy for answer validation. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, and Masayuki
Asahara. 1999. Japanese morphological analysis sys-
tem ChaSen version 2.0 manual 2nd edition.
Dan Moldovan, Marius Pasca, Sanda Harabagiu, and Mi-
hai Surdeanu. 2003. Performance issues and er-
ror analysis in an open-domain question answering
system. ACM Transactions on Information Systems,
21(2):133?154.
Kokoro Morooka and Jun?ichi Fukumoto. 2006. Answer
extraction method for why-type question answering
system. IEICE-WGNLC2005-107, pages 7?12. (in
Japanese).
Masaki Murata, Kiyotaka Uchimoto, Hiromi Ozaku,
Qing Ma, Masao Utiyama, and Hitoshi Isahara. 2000.
Japanese probabilistic information retrieval using lo-
cation and category information. The Fifth Interna-
tional Workshop on Information Retrieval with Asian
Languages, pages 81?88.
Masaki Murata, Masao Utiyama, Qing Ma, Hiromi
Ozaku, and Hitoshi Isahara. 2001. CRL at NTCIR2.
Proceedings of the Second NTCIR Workshop Meeting
on Evaluation of Chinese & Japanese Text Retrieval
and Text Summarization, pages 5?21?5?31.
Masaki Murata, Qing Ma, and Hitoshi Isahara. 2002.
High performance information retrieval using many
characteristics and many techniques. Proceedings of
the Third NTCIR Workshop (CLIR).
National Institute of Informatics. 2002. Proceedings of
the Third NTCIR Workshop (QAC).
S. E. Robertson and S. Walker. 1994. Some simple
effective approximations to the 2-Poisson model for
probabilistic weighted retrieval. In Proceedings of the
Seventeenth Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval.
S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M. Gatford. 1994. Okapi at TREC-3.
In TREC-3.
Radu Soricut and Eric Brill. 2004. Automatic question
answering: Beyond the factoid. In In Proceedings
of the Human Language Technology and Conference
of the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL-2004), pages
57?64.
TREC-10 committee. 2001. The tenth text retrieval con-
ference. http://trec.nist.gov/pubs/trec10/t10 proceed-
ings.html.
Jinxi Xu, Ana Licuanan, and Ralph Weischedel. 2003.
TREC 2003 QA at BBN: answering definitional ques-
tions. In Proceedings of the 12th Text Retrieval Con-
ference (TREC-2003), pages 98?106.
732
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 1?11,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Development of an automatic trend exploration system
using the MuST data collection
Masaki Murata1
murata@nict.go.jp
Qing Ma3,1
3qma@math.ryukoku.ac.jp
Toshiyuki Kanamaru1,4
1kanamaru@nict.go.jp
Hitoshi Isahara1
isahara@nict.go.jp
1National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
3Ryukoku University
Otsu, Shiga, 520-2194, Japan
Koji Ichii2
ichiikoji@hiroshima-u.ac.jp
Tamotsu Shirado1
shirado@nict.go.jp
Sachiyo Tsukawaki1
tsuka@nict.go.jp
2Hiroshima University
1-4-1 Kagamiyama, Higashi-hiroshima,
Hiroshima 739-8527, Japan
4Kyoto University
Yoshida-nihonmatsu-cho, Sakyo-ku,
Kyoto, 606-8501, Japan
Abstract
The automatic extraction of trend informa-
tion from text documents such as news-
paper articles would be useful for explor-
ing and examining trends. To enable this,
we used data sets provided by a workshop
on multimodal summarization for trend in-
formation (the MuST Workshop) to con-
struct an automatic trend exploration sys-
tem. This system first extracts units, tem-
porals, and item expressions from news-
paper articles, then it extracts sets of ex-
pressions as trend information, and finally
it arranges the sets and displays them in
graphs. For example, when documents
concerning the politics are given, the sys-
tem extracts ?%? and ?Cabinet approval
rating? as a unit and an item expression in-
cluding temporal expressions. It next ex-
tracts values related to ?%?. Finally, it
makes a graph where temporal expressions
are used for the horizontal axis and the
value of percentage is shown on the ver-
tical axis. This graph indicates the trend
of Cabinet approval rating and is useful
for investigating Cabinet approval rating.
Graphs are obviously easy to recognize
and useful for understanding information
described in documents. In experiments,
when we judged the extraction of a correct
graph as the top output to be correct, the
system accuracy was 0.2500 in evaluation
A and 0.3334 in evaluation B. (In evalua-
tion A, a graph where 75% or more of the
points were correct was judged to be cor-
rect; in evaluation B, a graph where 50%
or more of the points were correct was
judged to be correct.) When we judged
the extraction of a correct graph in the top
five outputs to be correct, accuracy rose to
0.4167 in evaluation A and 0.6250 in eval-
uation B. Our system is convenient and ef-
fective because it can output a graph that
includes trend information at these levels
of accuracy when given only a set of doc-
uments as input.
1 Introduction
We have studied ways to automatically extract
trend information from text documents, such as
newspaper articles, because such a capability will
be useful for exploring and examining trends. In
this work, we used data sets provided by a work-
shop on multimodal summarization for trend in-
formation (the MuST Workshop) to construct an
automatic trend exploration system. This system
firsts extract units, temporals, and item expres-
sions from newspaper articles, then it extract sets
of expressions as trend information, and finally it
arranges the sets and displays them in graphs. For
example, when documents concerning the politics
1
are given, the system extracts ?%? and ?Cabinet
approval rating? as a unit and an item expression
including temporal expressions. It next extracts
values related to ?%?. Finally, it makes a graph
where temporal expressions are used for the hor-
izontal axis and the value of percentage is shown
on the vertical axis. This graph indicates the trend
of Cabinet approval rating and is useful for inves-
tigating Cabinet approval rating. Graphs are obvi-
ously easy to recognize and useful for understand-
ing information described in documents.
2 The MuST Workshop
Kato et al organized the workshop on multimodal
summarization for trend information (the MuST
Workshop) (Kato et al, 2005). In this work-
shop, participants were given data sets consisting
of newspaper documents (editions of the Mainichi
newspaper from 1998 and 1999 (Japanese docu-
ments)) that included trend information for vari-
ous domains. In the data, tags for important ex-
pressions (e.g. temporals, numerical expressions,
and item expressions) were tagged manually.1 The
20 topics of the data sets (e.g., the 1998 home-run
race to break the all-time Major League record,
the approval rating for the Japanese Cabinet, and
news on typhoons) were provided. Trend infor-
mation was defined as information regarding the
change in a value for a certain item. A change in
the number of home runs hit by a certain player or
a change in the approval rating for the Cabinet are
examples of trend information. In the workshop,
participants could freely use the data sets for any
study they chose to do.
3 System
3.1 Structure of the system
Our automatic trend exploration system consists
of the following components.
1. Component to extract important expressions
First, documents related to a certain topic are
given to the system, which then extracts im-
portant expressions that will be used to ex-
tract and merge trend information. The sys-
tem extracts item units, temporal units, and
item expressions as important expressions.
1We do not use manually provided tags for important ex-
pressions because our system automatically extracts impor-
tant expressions.
Here, important expressions are defined as
expressions that play important roles in a
given document set. Item expressions are de-
fined as expressions that are strongly related
to the content of a given document set.
1a. Component to extract important item
units
The system extracts item units that will
be used to extract and merge trend infor-
mation.
For example, when documents concern-
ing the home-run race are given, ?hon?
or ?gou? (the Japanese item units for the
number of home runs) such as in ?54
hon? (54th home run) are extracted.
1b. Component to extract important tempo-
ral units
The system extracts temporal units that
will also be used to extract and merge
trend information.
For example, the system extracts tempo-
ral units such as ?nichi? (day), ?gatsu?
(month), and ?nen? (year). In Japanese,
temporal units are used to express dates,
such as in ?2006 nen, 3 gatsu, 27 nichi?
for March 27th, 2006.
1c. Component to extract important item
expressions
The system extracts item expressions
that will also be used to extract and
merge trend information.
For example, the system extracts expres-
sions that are objects for trend explo-
ration, such as ?McGwire? and ?Sosa?
as item expressions in the case of docu-
ments concerning the home-run race.
2. Component to extract trend information sets
The system identifies the locations in sen-
tences where a temporal unit, an item unit,
and an item expression that was extracted by
the component to extract important expres-
sions appear in similar sentences and extracts
sets of important expressions described by
the sentences as a trend information set. The
system also extracts numerical values appear-
ing with item units or temporal units, and
uses the connection of the numerical values
and the item units or temporal units as nu-
merical expressions or temporal expressions.
2
For example, in the case of documents con-
cerning the home-run race, the system ex-
tracts a set consisting of ?item expression:
McGwire?, ?temporal expression: 11 day?
(the 11th), and ?numerical expression: 47
gou? (47th home run) as a trend information
set.
3. Component to extract and display important
trend information sets
The system gathers the extracted trend infor-
mation sets and displays them as graphs or by
highlighting text displays.
For example, for documents concerning
the home-run race, the system displays as
graphs the extracted trend information sets
for ?McGwire? . In these graphs, temporal
expressions are used for the horizontal axis
and the number of home runs is shown on the
vertical axis.
3.2 Component to extract important
expressions
The system extracts important expressions that
will be used to extract trend information sets. Im-
portant expressions belong to one of the following
categories.
? item units
? temporal units
? item expressions
We use ChaSen (Matsumoto et al, 1999), a
Japanese morphological analyzer, to extract ex-
pressions. Specifically, we use the parts of
speeches in the ChaSen outputs to extract the ex-
pressions.
The system extracts item units, temporal units,
and item expressions by using manually con-
structed rules using the parts of speeches. The
system extracts a sequence of nouns adjacent to
numerical values as item units. It then extracts
expressions from among the item units which in-
clude an expression regarding time or date (e.g.,
?year?, ?month?, ?day?, ?hour?, or ?second?) as
temporal units. The system extracts a sequence of
nouns as item expressions.
The system next extracts important item units,
temporal units, and item expressions that play im-
portant roles in the target documents.
The following three methods can be used to ex-
tract important expressions. The system uses one
of them. The system judges that an expression
producing a high value from the following equa-
tions is an important expression.
? Equation for the TF numerical term in Okapi
(Robertson et al, 1994)
Score =
?
i?Docs
TF
i
TF
i
+
l
i
?
(1)
? Use of total word frequency
Score =
?
i?Docs
TF
i
(2)
? Use of total frequency of documents where a
word appears
Score =
?
i?Docs
1 (3)
In these equations, i is the ID (identification
number) of a document, Docs is a set of document
IDs, TF
i
is the occurrence number of an expres-
sion in document i, l is the length of document i,
and ? is the average length of documents inDocs.
To extract item expressions, we also applied a
method that uses the product of the occurrence
number of an expression in document i and the
length of the expression as TF
i
, so that we could
extract longer expressions.
3.3 Component to extract trend information
sets
The system identifies the locations in sentences
where a temporal unit, an item unit, and an item
expression extracted by the component to extract
important expressions appears in similar sentences
and extracts sets of important expressions de-
scribed by the sentences as a trend information
set. When more than one trend information set
appears in a document, the system extracts the one
that appears first. This is because important and
new things are often described in the beginning of
a document in the case of newspaper articles.
3.4 Component to extract and display
important trend information sets
The system gathers the extracted trend informa-
tion sets and displays them in graphs or as high-
lighted text. In the graphs, temporal expressions
3
are used for the horizontal axis and numerical ex-
pressions are used for the vertical axis. The system
also displays sentences used to extract trend infor-
mation sets and highlights important expressions
in the sentences.
The system extracts multiple item units, tempo-
ral units, and item expressions (through the com-
ponent to extract important expressions) and uses
these to make all possible combinations of the
three kinds of expression. The system extracts
trend information sets for each combination and
calculates the value of one of the following equa-
tions for each combination. The system judges
that the combination producing a higher value rep-
resents more useful trend information. The fol-
lowing four equations can be used for this purpose,
and the system uses one of them.
? Method 1 ? Use both the frequency of trend
information sets and the scores of important
expressions
M = Freq ? S
1
? S
2
? S
3
(4)
? Method 2 ? Use both the frequency of trend
information sets and the scores of important
expressions
M = Freq ? (S
1
? S
2
? S
3
)
1
3 (5)
? Method 3 ? Use the frequency of trend in-
formation sets
M = Freq (6)
? Method 4 ? Use the scores of important ex-
pressions
M = S
1
? S
2
? S
3
(7)
In these equations, Freq is the number of trend
information sets extracted as described in Section
3.3, and S1, S2, and S3 are the values of Score as
calculated by the corresponding equation in Sec-
tion 3.2.
The system extracts the top five item units, the
top five item expressions, and the top three tem-
poral units through the component to extract im-
portant expressions and forms all possible combi-
nations of these (75 combinations). The system
then calculates the value of the above equations for
these 75 combinations and judges that a combina-
tion having a larger value represents more useful
trend information.
4 Experiments and Discussion
We describe some examples of the output of our
system in Sections 4.1, 4.2, and 4.3, and the re-
sults from our system evaluation in Section 4.4.
We made experiments using Japanese newspaper
articles.
4.1 Extracting important expressions
To extract important expressions we applied the
equation for the TF numerical term in Okapi and
the method using the product of the occurrence
number for an expression and the length of the
expression as TF
i
for item expressions. We did
experiments using the three document sets for ty-
phoons, the Major Leagues, and political trends.
The results are shown in Table 1.
We found that appropriate important expres-
sions were extracted for each domain. For ex-
ample, in the data set for typhoons, ?typhoon?
was extracted as an important item expression and
an item unit ?gou? (No.), indicating the ID num-
ber of each typhoon, was extracted as an im-
portant item unit. In the data set for the Major
Leagues, the MuST data included documents de-
scribing the home-run race between Mark McG-
wire and Sammy Sosa in 1998. ?McGwire? and
?Sosa? were properly extracted among the higher
ranks. ?gou? (No.) and ?hon? (home run(s)), im-
portant item units for the home-run race, were
properly extracted. In the data set for political
trends, ?naikaku shiji ritsu? (cabinet approval rat-
ing) was properly extracted as an item expression
and ?%? was extracted as an item unit.
4.2 Graphs representing trend information
We next tested how well our system graphed the
trend information obtained from the MuST data
sets. We used the same three document sets as in
the previous section. As important expressions in
the experiments, we used the item unit, the tempo-
ral unit, and the item expression with the highest
scores (the top ranked ones) which were extracted
by the component to extract important expressions
using the method described in the previous sec-
tion. The system made the graphs using the com-
ponent to extract trend information sets and the
component to extract and display important trend
information sets. The graphs thus produced are
shown in Figs. 1, 2, and 3. (We used Excel to draw
these graphs.) Here, we made a temporal axis for
each temporal expression. However, we can also
4
Table 1: Examples of extracting important expressions
Typhoon
item units temporal units item expressions
gou nichi taihuu
(No.) (day) (typhoon)
me-toru ji gogo
(meter(s)) (o?clock) (afternoon)
nin jigoro higai
(people) (around x o?clock) (damage)
kiro fun shashin setsumei
(kilometer(s)) (minute(s)) (photo caption)
miri jisoku chuushin
(millimeter(s)) (per hour) (center)
Major League
item units temporal units item expressions
gou nichi Maguwaia
(No.) (day) (McGwire)
hon nen honruida
(home run(s)) (year) (home run)
kai gatsu Ka-jinarusu
(inning(s)) (month) (Cardinals)
honruida nen buri Ma-ku Maguwaia ichiruishu
(home run(s)) (after x year(s) interval) (Mark McGwire, the first baseman)
shiai fun So-sa
(game(s)) (minute(s)) (Sosa)
Political Trend
item units temporal units item expressions
% gatsu naikaku shiji ritsu
(%) (month) (cabinet approval rating)
pointo gen nichi Obuchi naikaku
(decrease of x point(s)) (day) (Obuchi Cabinet)
pointo zou nen Obuchi shushou
(increase of x point(s)) (year) (Prime Minister Obuchi)
dai kagetu shijiritsu
(generation) (month(s)) (approval rating)
pointo bun no kitai
(point(s)) (divided) (expectation)
5
Figure 1: Trend graph for the typhoon data set
Figure 2: Trend graph for the Major Leagues data
set
display a graph where regular temporal intervals
are used in the temporal axis.
For the typhoon data set, gou (No.), nichi (day),
and taihuu (typhoon) were respectively extracted
as the top ranked item unit, temporal unit, and
item expression. The system extracted trend in-
formation sets using these, and then made a graph
where the temporal expression (day) was used for
the horizontal axis and the ID numbers of the ty-
phoons were shown on the vertical axis. The
MuST data included data for September and Octo-
ber of 1998 and 1999. Figure 1 is useful for seeing
when each typhoon hit Japan during the typhoon
season each year. Comparing the 1998 data with
that of 1999 reveals that the number of typhoons
increased in 1999.
For the Major Leagues data set, gou (No.), nichi
(day), and Maguwaia (McGwire) were extracted
with the top rank. The system used these to make
a graph where the temporal expression (day) was
used for the horizontal axis and the cumulative
number of home runs hit by McGwire was shown
on the vertical axis (Fig. 2). The MuST data
included data beginning in August, 1998. The
graph shows some points where the cumulative
number of home runs decreased (e.g., September
Figure 3: Trend graph for the political trends data
set
4th), which was obviously incorrect. This was be-
cause our system wrongly extracted the number of
home runs hit by Sosa when this was given close
to McGwire?s total.
In the political trends data set, %, gatsu
(month), and naikaku shiji ritsu (cabinet approval
rating) were extracted with the top rankings. The
system used these to make a graph where the
temporal expression (month) was used for the
horizontal axis and the Cabinet approval rating
(Japanese Cabinet) was shown as a percentage on
the vertical axis. The MuST data covered 1998
and 1999. Figure 2 shows the cabinet approval
rating of the Obuchi Cabinet. We found that the
overall approval rating trend was upwards. Again,
there were some errors in the extracted trend infor-
mation sets. For example, although June was han-
dled correctly, the system wrongly extracted May
as a temporal expression from the sentence ?in
comparison to the previous investigation in May?.
4.3 Sentence extraction and highlighting
display
We then tested the sentence extraction and high-
lighting display with respect to trend information
using the MuST data set; in this case, we used
the typhoon data set. As important expressions,
we used the item unit, the temporal unit, and the
item expression extracted with the highest scores
(the top ranked ones) by the component to extract
important expressions using the method described
in the previous section. Gou (No.), nichi (day),
and taihuu (typhoon) were respectively extracted
as an item unit, a temporal unit, and an item ex-
pression. The system extracted sentences includ-
ing the three expressions and highlighted these ex-
pressions in the sentences. The results are shown
in Figure 4. The first trend information sets to ap-
6
Sept. 16, 1998 No. 5
Large-scale and medium-strength Typhoon No. 5 made landfall near Omaezaki in Shizuoka Pre-
fecture before dawn on the 16th, and then moved to the northeast involving the Koshin, Kantou,
and Touhoku areas in the storm.
Sept. 21, 1998 No. 8
Small-scale Typhoon No. 8 made landfall near Tanabe City in Wakayama Prefecture around 4:00
p.m. on the 21st, and weakened while tracking to the northward across Kinki district.
Sept. 22, 1998 No. 7
Typhoon No. 7 made landfall near Wakayama City in the afternoon on the 22nd, and will hit the
Kinki district.
Sept. 21, 1998 No. 8
The two-day consecutive landfall of Typhoon No. 8 on the 21st and Typhoon No. 7 on the 22nd
caused nine deaths and many injuries in a total of six prefectures including Nara, Fukui, Shiga,
and so on.
Oct. 17, 1998 No. 10
Medium-scale and medium-strength Typhoon No. 10 made landfall on Makurazaki City in
Kagoshima Prefecture around 4:30 p.m. on the 17th, and then moved across the West Japan area
after making another landfall near Sukumo City in Kochi Prefecture in the evening.
Aug. 20, 1999 No. 11
The Meteorological Office announced on the 20th that Typhoon No. 11 developed 120 kilometers
off the south-southwest coast of Midway.
Sept. 14, 1999 No. 16
Typhoon No. 16, which developed off the south coast in Miyazaki Prefecture, made landfall near
Kushima City in the prefecture around 5:00 p.m. on the 14th.
Sept. 15, 1999 No. 16
Small-scale and weak Typhoon No. 16 became extratropical in Nagano Prefecture and moved out
to sea off Ibaraki Prefecture on the 15th.
Sept. 24, 1999 No. 18
Medium-scale and strong Typhoon No. 18 made landfall in the north of Kumamoto Prefecture
around 6:00 a.m. on the 24th, and after moving to Suo-Nada made another landfall at Ube City
in Yamaguchi Prefecture before 9:00 p.m., tracked through the Chugoku district, and then moved
into the Japan Sea after 10:00 p.m.
Sept. 25, 1999 No. 18
Typhoon No. 18, which caused significant damage in the Kyushu and Chugoku districts, weakened
and made another landfall before moving into the Sea of Okhotsk around 10:00 a.m. on the 25th.
Figure 4: Sentence extraction and highlighting display for the typhoon data set
7
pear are underlined twice and the other sets are
underlined once. (In the actual system, color is
used to make this distinction.) The extracted tem-
poral expressions and numerical expressions are
presented in the upper part of the extracted sen-
tence. The graphs shown in the previous section
were made by using these temporal expressions
and numerical expressions.
The extracted sentences plainly described the
state of affairs regarding the typhoons and were
important sentences. For the research being done
on summarization techniques, this can be consid-
ered a useful means of extracting important sen-
tences. The extracted sentences typically describe
the places affected by each typhoon and whether
there was any damage. They contain important
descriptions about each typhoon. This confirmed
that a simple method of extracting sentences con-
taining an item unit, a temporal unit, and an item
expression can be used to extract important sen-
tences.
The fourth sentence in the figure includes infor-
mation on both typhoon no.7 and typhoon no.8.
We can see that there is a trend information set
other than the extracted trend information set (un-
derlined twice) from the expressions that are un-
derlined once. Since the system sometimes ex-
tracts incorrect trend information sets, the high-
lighting is useful for identifying such sets.
4.4 Evaluation
We used a closed data set and an open data set
to evaluate our system. The closed data set was
the data set provided by the MuST workshop or-
ganizer and contained 20 domain document sets.
The data sets were separated for each domain.
We made the open data set based on the MuST
data set using newspaper articles (editions of the
Mainichi newspaper from 2000 and 2001). We
made 24 document sets using information retrieval
by term query. We used documents retrieved by
term query as the document set of the domain for
each query term.
We used the closed data set to adjust our system
and used the open data set to calculate the evalua-
tion scores of our system for evaluation.
We judged whether a document set included the
information needed to make trend graphs by con-
sulting the top 30 combinations of three kinds of
important expression having the 30 highest values
as in the method of Section 3.4. There were 19
documents including such information in the open
data. We used these 19 documents for the follow-
ing evaluation.
In the evaluation, we examined how accurately
trend graphs could be output when using the top
ranked expressions. The results are shown in Table
2. The best scores are described using bold fonts
for each evaluation score.
We used five evaluation scores. MRR is the av-
erage of the score where 1/r is given as the score
when the rank of the first correct output is r (Mu-
rata et al, 2005b). TP1 is the average of the pre-
cision in the first output. TP5 is the average of
the precision where the system includes a correct
output in the first five outputs. RP is the average
of the r-precision and AP is the average of the av-
erage precision. (Here, the average means that the
evaluation score is calculated for each domain data
set and the summation of these scores divided by
the number of the domain data sets is the average.)
R-precision is the precision of the r outputs where
r is the number of correct answers. Average pre-
cision is the average of the precision when each
correct answer is output (Murata et al, 2000). The
r-precision indicates the precision where the recall
and the precision have the same value. The preci-
sion is the ratio of correct answers in the system
output. The recall is the ratio of correct answers
in the system output to the total number of correct
answers.
Methods 1 to 4 in Table 2 are the methods used
to extract useful trend information described in
Section 3.4. Use of the expression length means
the product of the occurrence number for an ex-
pression and the length of the expression was used
to calculate the score for an important item ex-
pression. No use of the expression length means
this product was not used and only the occurrence
number was used.
To calculate the r-precision and average preci-
sion, we needed correct answer sets. We made the
correct answer sets by manually examining the top
30 outputs for the 24 (= 4? 6) methods (the com-
binations of methods 1 to 4 and the use of Equa-
tions 1 to 3 with or without the expression length)
and defining the useful trend information among
them as the correct answer sets.
In evaluation A, a graph where 75% or more of
the points were correct was judged to be correct.
In evaluation B, a graph where 50% or more of the
points were correct was judged to be correct.
8
Table 2: Experimental results for the open data
Evaluation A Evaluation B
MRR TP1 TP5 RP AP MRR TP1 TP5 RP AP
Use of Equation 1 and the expression length
Method 1 0.3855 0.3158 0.4737 0.1360 0.1162 0.5522 0.4211 0.7368 0.1968 0.1565
Method 2 0.3847 0.3158 0.4211 0.1360 0.1150 0.5343 0.4211 0.6316 0.1880 0.1559
Method 3 0.3557 0.2632 0.4211 0.1360 0.1131 0.5053 0.3684 0.6316 0.1805 0.1541
Method 4 0.3189 0.2632 0.4211 0.1125 0.0973 0.4492 0.3158 0.6316 0.1645 0.1247
Use of Equation 2 and the expression length
Method 1 0.3904 0.3158 0.4737 0.1422 0.1154 0.5746 0.4211 0.7368 0.2127 0.1674
Method 2 0.3877 0.3158 0.4737 0.1422 0.1196 0.5544 0.4211 0.7368 0.2127 0.1723
Method 3 0.3895 0.3158 0.5263 0.1422 0.1202 0.5491 0.4211 0.7895 0.2127 0.1705
Method 4 0.2216 0.1053 0.3684 0.0846 0.0738 0.3765 0.2105 0.5789 0.1328 0.1043
Use of Equation 3 and the expression length
Method 1 0.3855 0.3158 0.4737 0.1335 0.1155 0.5452 0.4211 0.7368 0.1943 0.1577
Method 2 0.3847 0.3158 0.4211 0.1335 0.1141 0.5256 0.4211 0.6316 0.1855 0.1555
Method 3 0.3570 0.2632 0.4737 0.1335 0.1124 0.4979 0.3684 0.6842 0.1780 0.1524
Method 4 0.3173 0.2632 0.4737 0.1256 0.0962 0.4652 0.3684 0.6316 0.1777 0.1293
Use of Equation 1 and no use of the expression length
Method 1 0.3789 0.3158 0.4737 0.1294 0.1152 0.5456 0.4211 0.7368 0.2002 0.1627
Method 2 0.3750 0.3158 0.4211 0.1294 0.1137 0.5215 0.4211 0.6842 0.2002 0.1621
Method 3 0.3333 0.2632 0.4211 0.1119 0.1072 0.4798 0.3684 0.6842 0.1763 0.1552
Method 4 0.2588 0.1053 0.4737 0.1269 0.0872 0.3882 0.1579 0.6842 0.1833 0.1189
Use of Equation 2 and no use of the expression length
Method 1 0.3277 0.2105 0.4737 0.1134 0.0952 0.4900 0.2632 0.7895 0.1779 0.1410
Method 2 0.3662 0.2632 0.4737 0.1187 0.1104 0.5417 0.3684 0.7368 0.1831 0.1594
Method 3 0.3504 0.2632 0.4737 0.1187 0.1116 0.5167 0.3684 0.7368 0.1884 0.1647
Method 4 0.1877 0.0526 0.3684 0.0775 0.0510 0.3131 0.1053 0.5263 0.1300 0.0879
Use of Equation 3 and no use of the expression length
Method 1 0.3855 0.3158 0.4737 0.1335 0.1155 0.5452 0.4211 0.7368 0.1943 0.1577
Method 2 0.3847 0.3158 0.4211 0.1335 0.1141 0.5256 0.4211 0.6316 0.1855 0.1555
Method 3 0.3570 0.2632 0.4737 0.1335 0.1124 0.4979 0.3684 0.6842 0.1780 0.1524
Method 4 0.3173 0.2632 0.4737 0.1256 0.0962 0.4652 0.3684 0.6316 0.1777 0.1293
9
From the experimental results, we found that
the method using the total frequency for a word
(Equation 2) and the length of an expression was
best for calculating the scores of important expres-
sions.
Using the length of an expression was impor-
tant. (The way of using the length of an expres-
sion was described in the last part of Section 3.2.)
For example, when ?Cabinet approval rating? ap-
pears in documents, a method without expression
lengths extracts ?rating?. When the system ex-
tracts trend information sets using ?rating?, it ex-
tracts wrong information related to types of ?rat-
ing? other than ?Cabinet approval rating?. This
hinders the extraction of coherent trend informa-
tion. Thus, it is beneficial to use the length of an
expression when extracting important item expres-
sions.
We also found that method 1 (using both the fre-
quency of the trend information sets and the scores
of important expressions) was generally the best.
When we judged the extraction of a correct
graph as the top output in the experiments to be
correct, our best system accuracy was 0.3158 in
evaluation A and 0.4211 in evaluation B.When we
judged the extraction of a correct graph in the top
five outputs to be correct, the best accuracy rose to
0.5263 in evaluation A and 0.7895 in evaluation B.
In terms of the evaluation scores for the 24 original
data sets (these evaluation scores were multiplied
by 19/24), when we judged the extraction of a cor-
rect graph as the top output in the experiments to
be correct, our best system accuracy was 0.3158 in
evaluation A and 0.4211 in evaluation B.When we
judged the extraction of a correct graph in the top
five outputs to be correct, the best accuracy rose to
0.5263 in evaluation A and 0.7895 in evaluation B.
Our system is convenient and effective because it
can output a graph that includes trend information
at these levels of accuracy when given only a set
of documents as input.
As shown in Table 2, the best values for RP
(which indicates the precision where the recall and
the precision have the same value) and AP were
0.2127 and 0.1705, respectively, in evaluation B.
This RP value indicates that our system could
extract about one out of five graphs among the cor-
rect answers when the recall and the precision had
the same value.
5 Related studies
Fujihata et al (Fujihata et al, 2001) developed a
system to extract numerical expressions and their
related item expressions by using syntactic infor-
mation and patterns. However, they did not deal
with the extraction of important expressions or
gather trend information sets. In addition, they did
not make a graph from the extracted expressions.
Nanba et al (Nanba et al, 2005) took an
approach of judging whether the sentence rela-
tionship indicates transition (trend information)
or renovation (revision of information) and used
the judgment results to extract trend information.
They also constructed a system to extract nu-
merical information from input numerical units
and make a graph that includes trend information.
However, they did not consider ways to extract
item numerical units and item expressions auto-
matically.
In contrast to these systems, our system auto-
matically extracts item numerical units and item
expressions that each play an important role in a
given document set. When a document set for
a certain domain is given, our system automati-
cally extracts item numerical units and item ex-
pressions, then extracts numerical expressions re-
lated to these, and finally makes a graph based
on the extracted numerical expressions. When a
document set is given, the system automatically
makes a graph that includes trend information.
Our system also uses an original method of pro-
ducing more than one graphs and selecting an ap-
propriate graph among them using Methods 1 to 4,
which Fujihata et al and Namba et al did not use.
6 Conclusion
We have studied the automatic extraction of trend
information from text documents such as newspa-
per articles. Such extraction will be useful for ex-
ploring and examining trends. We used data sets
provided by a workshop on multimodal summa-
rization for trend information (the MuST Work-
shop) to construct our automatic trend exploration
system. This system first extracts units, tempo-
rals, and item expressions from newspaper arti-
cles, then it extracts sets of expressions as trend
information, and finally it arranges the sets and
displays them in graphs.
In our experiments, when we judged the extrac-
tion of a correct graph as the top output to be cor-
rect, the system accuracy was 0.2500 in evaluation
10
A and 0.3334 in evaluation B. (In evaluation A, a
graph where 75% or more of the points were cor-
rect was judged to be correct; in evaluation B, a
graph where 50% or more of the points were cor-
rect was judged to be correct.) When we judged
the extraction of a correct graph in the top five out-
puts to be correct, we obtained accuracy of 0.4167
in evaluation A and 0.6250 in evaluation B. Our
system is convenient and effective because it can
output a graph that includes trend information at
these levels of accuracy when only a set of docu-
ments is provided as input.
In the future, we plan to continue this line of
study and improve our system. We also hope to
apply the method of using term frequency in doc-
uments to extract trend information as reported by
Murata et al (Murata et al, 2005a).
References
Katsuyuki Fujihata, Masahiro Shiga, and Tatsunori
Mori. 2001. Extracting of numerical expressions
by constraints and default rules of dependency struc-
ture. Information Processing Society of Japan,
WGNL 145.
Tsuneaki Kato, Mitsunori Matsushita, and Noriko
Kando. 2005. MuST: A workshop on multimodal
summarization for trend information. Proceedings
of the Fifth NTCIR WorkshopMeeting on Evaluation
of Information Access Technologies: Information
Retrieval, Question Answering and Cross-Lingual
Information Access.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, and Masayuki
Asahara. 1999. Japanese morphological analysis
system ChaSen version 2.0 manual 2nd edition.
Masaki Murata, Kiyotaka Uchimoto, Hiromi Ozaku,
Qing Ma, Masao Utiyama, and Hitoshi Isahara.
2000. Japanese probabilistic information retrieval
using location and category information. The Fifth
International Workshop on Information Retrieval
with Asian Languages, pages 81?88.
Masaki Murata, Koji Ichii, Qing Ma, Tamotsu Shirado,
Toshiyuki Kanamaru, and Hitoshi Isahara. 2005a.
Trend survey on Japanese natural language process-
ing studies over the last decade. In The Second In-
ternational Joint Conference on Natural Language
Processing, Companion Volume to the Proceedings
of Conference including Posters/Demos and Tutorial
Abstracts.
Masaki Murata, Masao Utiyama, and Hitoshi Isahara.
2005b. Use of multiple documents as evidence with
decreased adding in a Japanese question-answering
system. Journal of Natural Language Processing,
12(2).
Hidetsugu Nanba, Yoshinobu Kunimasa, Shiho
Fukushima, Teruaki Aizawa, and Manabu Oku-
mura. 2005. Extraction and visualization of trend
information based on the cross-document structure.
Information Processing Society of Japan, WGNL
168, pages 67?74.
S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M. Gatford. 1994. Okapi at TREC-3.
In TREC-3.
11
