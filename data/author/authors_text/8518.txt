Creative Discovery in Lexical Ontologies
Tony VEALE
Dept. of Computer Science 
Belfield, 
Dublin, Ireland, D4
Tony.Veale@UCD.ie
Nuno SECO, Jer HAYES
Dept. of Computer Science 
Belfield, 
Dublin, Ireland, D4
{nuno.seco, jer.hayes}@UCD.ie
Abstract
Compound terms play a surprisingly key role 
in the organization of lexical ontologies. 
However, their inclusion forces one to address 
the issues of completeness and consistency 
that naturally arise from this organizational 
role. In this paper we show how creative 
exploration in the space of literal compounds 
can reveal not only additional compound 
terms to systematically balance an ontology, 
but can also discover new and potentially 
innovative concepts in their own right.
1 Introduction
Broad-coverage lexical knowledge-bases like 
WordNet (Miller et al, 1990) generally contain a 
large number of compound terms, many of which 
are literal in composition. These compounds are 
undoubtedly included for a reason, yet the idea that 
literal compounds might actually be essential to 
WordNet?s usefulness may strike some as heretical 
on at least two fronts: first, the lexicon is a finite 
resource, while the space of compounds is 
potentially infinite; and at any rate, literal 
compounds can be created as needed from purely 
compositional principles (Hanks, 2004). However, 
these retorts are valid only if we view WordNet as 
a dictionary, but of course it is much more than 
this. WordNet is a lexical ontology, and ultimately, 
ontologies derive a large part of their functionality 
from their structure.
So, while the meaning of literal compounds like 
Greek-deity and animal-product may well be 
predictable from compositional principles alone, 
such concepts still serve an important 
organizational role in WordNet by adding much 
needed structure to the middle ontology. Having 
conceded the importance of such compounds, one 
is forced to address the issues of completeness and 
consistency that then arise from their inclusion. 
Completeness suggests that we strive to include as 
many literal compounds as are sensible, if they 
enhance the organization of the ontology or if there 
is evidence that they are in common usage in the 
language. Systematicity is a related issue that 
arises when a group of existing compounds 
suggests that another should also exist for the 
ontology to be consistent. For instance, the 
existence of Greek-deity, Greek-alphabet and 
Hebrew-alphabet leads to the hypothesis that 
Hebrew-deity should also exist if WordNet is to be 
both consistent and symmetric in its treatment of 
different cultural groupings.
Indeed, because literal compounds like these 
arise from the yoking together of two different 
ontological branches into one, compounding 
represents an important contextualization device in 
the design of ontologies, allowing lexical elements 
to be logically grouped into clusters or families 
that share important dimensions of meaning. This 
clustering facilitates both automated reasoning by 
machines (such as the determination of semantic 
similarity based on taxonomic distance) and 
effective browsing by humans. Sometimes this 
yoking results in a compound that, following 
Boden (1990) and Wiggins (2003), deserves to be 
called ?creative?, because it exhibits both novelty 
and value. Novelty can be measured along either a 
psychological or a historical dimension, while 
utility is a reflection of the uses to which a 
compound can be put. For instance, a new 
compound may have utility as a clustering node 
when added to the middle ontology if its 
appropriate hyponyms can be identified. 
Alternately, a new compound may represent an 
alternate nominalization of an existing concept 
(e.g., see Vendler?s (1967) insights about 
nominalization, and Lynott and Keane?s (2003) 
application of these insights to compound 
generation). 
In this paper we present a process of ontological 
exploration to identify those areas of the lexicon 
that can contribute to, and may in turn benefit 
from, the invention of new compound terms. Since 
the discovery of new compound terms is 
essentially a process of creative exploration, we 
frame our discussion within the theoretical 
framework of creative computation. Within this 
framework two approaches to validating new 
compounds are presented: internal validation 
determines whether the ontology itself provides 
evidence for the sensibility of a new compound, 
while external validation uses web-search to find 
evidence that the compound already exists outside 
the ontology. We then go on to show how these 
different strategies create a validation gap that can 
be exploited to identify the small number of truly 
creative compounds that arise.
2 Exploring the Space of LMH Concepts
Creative discovery requires that we give 
structure to the space of possible concepts that we 
plan to explore. This is made somewhat easier if 
we consider the meaning of conceptual structures 
to be grounded in a semiotic system of meaning-
creating oppositions. Given a starting structure, 
knowledge of allowable oppositions can then be 
used to transform this starting point into a variety 
of different conceivable structures, some of which 
may be novel and possess value on a particular 
utility scale.
The notion of opposition employed here is much 
broader than that of antonymy. For our purposes, 
contextual oppositions exist between terms that 
compete to fill a given dimension of the same 
concept. For instance, Greek1 and Hindu can each 
be used to differentiate the concept deity along a 
culture dimension, and so, in the context of deity, 
both are opposed. However, this is a contextual 
opposition that, unlike the role of antonymy, does 
not constitute part of the meaning of either 
concept. WordNet is a rich source of explicit 
antonymous oppositions, but contextual 
oppositions must be inferred from the structure of 
the ontology itself and from existing compounds.
Fortunately, WordNet contains many instances 
of literal modifier-head terms, such as "pastry 
crust" and "Greek alphabet". The concepts denoted 
by these compound terms, or LMH concepts for 
short, have the lexical form M-H (such as pizza-pie
or prairie-dog) and express their literality in two 
ways. First, they must be stored in the WordNet 
ontology under an existing sense of the lexeme H; 
for instance, pizza-pie is actually stored under the 
hypernym pie. Secondly, the gloss for the concept 
M-H should actually contain the lexeme M or 
some synonym of it. Thus, while Greek-alphabet is 
a LMH (it literally is a kind of alphabet, and it is 
literally Greek), neither monkey-bread (which is 
not literally a kind of bread) nor Dutch-courage
(which is not literally Dutch) is a LMH concept.
2.1 A Framework for Creativity
We use the terminology of Wiggins (2003) to 
frame our discussion of creative exploration. 
Wiggins, following earlier work by Boden (1990), 
                                                  
1
 To avoid later confusion with set notion, we denote 
WordNet senses not as synsets but as italicized terms .
formalizes the creative exploration process using 
the following abstractions:
C  -  the realm of concepts that is being explored
R - the set of rules for forming concepts and 
conversely, deconstructing existing ones
T - the transformational rules that generate new 
concepts via R
E -  the evaluation mechanism that ascribes value 
or utility to these new concepts
In applying these terms to creativity in WordNet, 
we introduce the following refinements:
Cw  - the subset of C described explicitly in 
WordNet as synsets
C* - the set of LMH concepts in Cw considered 
as a starting point for creative exploration
R* - the subset of R needed to construct and 
deconstruct LMH compounds in C*
T* - the subset of T needed to hypothesize new 
LMH concepts for R* to construct
So for our current purposes, we define C* as the 
set of LMH concepts in WordNet, and R* as the 
compositional criteria used to identify and 
decompose existing LMH entries and to construct 
new ones by concatenating an appropriate M and H 
term pair. However, to define T*, we first need to 
consider how taxonomic differentiation is used to 
create LMH concepts in the first place.
3 Domain Differentiation
LMH concepts exist in WordNet to differentiate 
more general concepts in meaningful taxonomic 
ways. For instance, the LMH concepts Greek-
alphabet, Hebrew-alphabet and Roman-alphabet
each serve to differentiate the concept alphabet. 
This is a useful ontological distinction that 
contributes to the definition of individual letter 
concepts like Alpha, Beta and Gimel. Since we can 
represent this specialization pattern via a 
differentiation set Dalphabet as follows:
   Dalphabet  = {Greek, Hebrew, Roman}
More generally, the differentiation set of a concept 
H comprises the set of all concepts M such that the 
LMH concept M-H is in C*. Thus we have:
   Ddeity        = {Hindu, Roman, Greek,  ...}
   Darchitecture =  {Greek, Roman, ...}
   Dcalendar =  {Muslim, Jewish, Hebrew, ...}
We use D to denote the set of all differentiation 
sets that are implied by C*, allowing us to define 
the absolute affinity between two modifier terms c1
and c2 in terms of differentiation as follows:
 Aabs(c1, c2) =   |{x ? D: c1 ? x ? c2 ? x}|       (1) (1)
This simply counts the number of base concepts 
that c1 and c2 can both differentiate. We thus 
define the relative affinity between two modifier 
terms c1 and c2 as follows:
Arel(c1, c2) =     |{x ? D : c1 ? x ? c2 ? x}|      (2) 
        /  |{x ? D : c1 ? x ? c2 ? x}| 
A relative affinity of 1.0 means that both terms 
differentiate exactly the same concepts in 
WordNet. It follows that the higher the relative 
affinity between c1 and c2, then the greater the 
likelihood that a concept differentiated by c1 can 
also be differentiated by c2, while the higher the 
absolute affinity, the more reliable this likelihood 
estimate becomes. Affinity thus provides an 
effective basis for formulating the transformation 
rules in T*. 
We should naturally expect near-synonymous 
modifiers to have a strong affinity for each other. 
For instance, Jewish and Hebrew are near-
synonyms because WordNet compounds Jewish-
Calendar and Hebrew-Calendar are themselves 
synonymous. This is clearly a form of contextual 
synonymy, since Jewish and Hebrew do not mean 
the same thing. Nonetheless, their affinity can be 
used to generate new compounds that add value to 
WordNet as synonyms of existing terms, such as 
Jewish-alphabet, Hebrew-Religion, and so on.
Recall that literal compounds represent a yoking 
together of two or more ontological branches. In 
exploring the space of novel compounds, it will be 
important to recognize which branches most 
naturally form the strongest bonds. Another variant 
of affinity can be formulated for this purpose:
Adomain(x, y) =     |Dx ? Dx|                   (3)
For instance,  Adomain(sauce, pizza) = 2, since in 
WordNet the modifier overlap between the pizza
and sauce domains is {anchovy, cheese}.
4 Creative Exploration in the LMH Space
We consider as an exploratory starting point any 
LMH concept M-H in C*. We can transform this 
into another concept M'-H by replacing M with 
any M' for which:
M' ? {x | x ? D - {DH} ? M ? x}             (4)
This formulation may suggest a large range of 
values of M'. However, these candidates can be 
sorted by Arel(M, M'), which estimates the 
probability that a given M'-H will later be 
validated as useful. One rule in T* can now be 
formulated for our further consideration:
T*:   M1-H1 ? M1-H2 ? M2-H1   ?   M2-H2     (5)
This rule allows the LMH space to be explored 
via a process of modifier modulation. Suppose we 
choose Greek-deity as a starting point. Since M = 
Greek and H = Deity, we can choose M' from any 
set other than Ddeity that contains Greek:
Dalphabet      = {Hebrew, Greek, Roman}
Ddeity          = {Greek, Roman, Norse, Hindu, ?}
These differentiation patterns suggest that new 
compounds can meaningfully be created by yoking 
the ontological branches of alphabet and deity
together. Thus, from Dalphabet we can choose M' 
to be either Hebrew or Roman, leading to the 
creation of the LMH concepts Hebrew-deity and 
Roman-deity. One of these, Roman-deity, already 
exists in C*, but another, Hebrew-deity is novel in 
a way that Boden terms psychologically or P-
Creative, inasmuch as it is neither in Cw nor C*. It 
may thus be of some value as a hypernym for 
existing WordNet concepts like Yahwe and 
Jehovah.
Rule (5) is a general principle for ontological 
exploration in the space of compound terms. 
Consider the compound software-engineering, 
which, following (5), is suggested by the joint 
existence in WordNet of the concepts software-
engineer, automotive-engineer and automotive-
engineering. While this particular addition could 
be predicted from the application of simple 
morphology rules, the point here is that a single 
exploration principle like (5) can obviate the need 
for a patchwork of such simple rules.
Of course, one can imagine rules other than (5) 
to exploit the regularities inherent in WordNet 
definitions. For instance, consider the sense 
gasoline-bomb, which WordNet glosses as: ?a 
crude incendiary bomb made of a bottle filled with 
flammable liquid and fitted with a rag wick?. By 
determining which definite description in the gloss 
conforms to the modifier ? in this case it is 
?flammable liquid? ? other modifiers can be found 
that also match this description. Thus, the new 
concepts methanol-bomb and butanol-bomb can be 
generated, and from this the creative concept 
alcohol-bomb can be generalized. However, each 
strategy raises its own unique issues, so for now 
we consider a T* comprising (5) only.
4.1 The Evaluation Mechanism E
For purposes of ascribing value or ontological 
utility to a new LMH concept M'-H, the concept 
must first be placed into one of the following 
categories:
a) M'-H already exists in C* is thus ascribed zero 
value as an addition to C*.
b)  M'-H does not exist in C* but does exist in Cw, 
and thus corresponds to an existing non-literal 
concept (such as monkey-bread). While it may 
have value if given a purely literal reading, it 
cannot be added to Cw without creating 
ambiguity, and so has zero value.
c)  Using R*, M'-H can be seen to describe a non-
empty class of existing concepts in Cw, and 
would thus have value as either a synonym 
(when this set is a singleton) or as a new 
organizing super-type (when this set is a 
severalton). In this case, we say that M'-H has 
been internally validated against Cw.
d) Using a textual analysis of a large corpus such 
as the World-Wide-Web, M'-H is recognized 
to have a conventional meaning in C even if it 
is not described in Cw. In this case, we say that 
M'-H has been externally validated for 
inclusion in Cw. The fact that M'-H is novel to 
the system but not to the historical context of 
the web suggests that it is merely a 
psychologically or P-Creative invention in the 
sense of Boden (1990). 
e) M'-H is recognized to have a hypothetical or 
metaphoric value within a comprehension 
framework such as conceptual blending theory 
(e.g., see Veale et al 2000), mental space 
theory, etc. In this case, M'-H may truly be a 
historically or H-Creative invention in the 
sense of Boden (1990).
In general, a new compound has value if its 
existence is suggested by, but not recognized by, 
the lexical ontology. As noted in the introduction, 
this value can be realized in a variety of ways, e.g., 
by automatically suggesting new knowledge-base 
additions to the lexical ontologist, or by providing 
potentially creative expansions for a user query in 
an information retrieval system (see Veale, 2004). 
4.2 Validating New Concepts
The evaluation strategies (c) and (d) above 
suggest two ways of validating the results of new 
compound creation: a WordNet-internal approach 
that uses the structure of the ontology itself to 
provide evidence for a compound?s utility, and a 
WordNet-external approach that instead looks to 
an unstructured archive like the web. In both cases, 
a new compound is validated by assembling a 
support set of precedent terms that argue for its 
meaningfulness.
4.2.1 Internal Validation
The internal support-set for a new compound M-H 
is the set of all WordNet words w that have: (i) at 
least one sense that is a hyponym of a sense of H; 
and (ii) a sense that contains M or some variant of 
it in its gloss. For instance, the novel compound 
?rain god? is internally validated by the word set 
{?Thor?, ?Parjanya?, ?Rain giver?}. 
When w is polysemous, two distinct senses may 
be used, reflecting the fact that M-H may be 
metonymic in construction. For instance, the 
compound ?raisin-wine? can be validated 
internally by the polysemous word ?muscatel?, 
since one sense of ?muscatel? is a kind of wine, 
and another, a kind of grape, has a WordNet gloss 
containing the word ?raisin?. From this 
perspective, a ?raisin wine? can be a wine made 
from the same grapes that raisins are made from. 
Likewise, the compound ?Jewish robot? can be 
validated by simultaneously employing both senses 
of ?Golem? in WordNet, which defines ?Golem? 
as either a Jewish mythical being or as a robotic 
automaton.
Creative products arise when conceptual 
ingredients from different domains are effectively 
blended (see Veale and O?Donoghue, 2000). It 
follows that a creative product can be validated by 
deblending it into its constituent parts and 
determining whether there is a precedent for 
combining elements of these types, if not these 
specific elements. We can thus exploit this notion 
of deblending to provide internal validation for 
new compounds. For instance the WordNet gloss 
for pizza lists ?tomato sauce? as an ingredient. 
This suggests we can meaningfully understand a 
compound of the form  M-pizza if there exists a 
compound M-sauce that can be viewed as a 
replacement for this ingredient. Generalizing from 
this, we can consider a new compound M1-H1 to 
be internally validated if H has a sense whose gloss 
contains the compound M2-H2, and if the ontology 
additionally contains the concept M1-H2. It follows 
then that the novel compounds apple-pizza, 
chocolate-pizza, taco-pizza, and curry-pizza will 
all  be internally validated as meaningful (if not 
necessarily enjoyable) varieties of pizza.
4.2.2 External Validation
In contrast, the external validation set for a 
compound M-H is the set of distinct documents 
that contain the compound term ?M H?, as 
acquired using a web search engine. For instance, 
given the WordNet concepts naval-engineer, 
software-engineer and naval-academy, rule (5) 
generates the hypothesis software-academy, which 
cannot be validated internally yet which retrieves 
over 1000 web documents to atest to its validity.
This web strategy is motivated by Keller and 
Lapata?s (2003) finding that the number of 
documents containing a novel compound reliably 
predicts the human plausibility scores for the 
compound. 
Nonetheless, external validation in this way is 
by no means a robust process. Since web 
documents are not sense tagged, one cannot be 
sure that a compound occurs with the sense that it 
is hypothesized to have. Indeed, it may not even 
occur as a compound at all, but as a coincidental 
juxtaposition of terms from different phrases or 
sentences. Finally, even if found with the correct 
syntactic and semantic form, one cannot be sure 
that the usage is not that of a non-native, second 
language learner.
These possibilities can be diminished by 
seeking a large enough sample set, but this has the 
effect of setting the evidential bar too high for truly 
creative compounds.  However, another solution 
lies in the way that the results of external 
validation are actually used, as we shall later see.
4.2.3 Validating New Synonyms
Many of the compounds that are validated either 
by internal or external means will be synonyms of 
existing WordNet terms. As such, their creative 
value will not represent an innovative combination 
of ideas, but rather a creative use of paraphrasing. 
The nature of (5) makes it straightforward to 
determine which is the case.
In general, when M1-H1 and M2-H1 are 
themselves synonyms, then M2-H2 will be a 
synonym of M1-H2. For instance, from the 
combination of applied-science, engineering-
science and applied-mathematics, we can generate 
from (5) the new compound engineering-
mathematics. This compound cannot be validated 
internally, but since it retrieves more than 300,000 
documents from the web, this is enough to 
adequately atest to its meaningfulness. Now, since 
applied-science and engineering-science are 
synonymous in WordNet, we can conclude that 
engineering-mathematics and applied-mathematics
are themselves synonymous also.         
4.3 Creativity in the Validation Gap
The difference between internal and external 
validation strategies can be illuminating. Internal 
validation verifies a compound on the basis that it 
could meaningfully exist, while external validation 
verifies it on the basis that it does actually exist in 
a large corpus. Therefore, if a compound can be 
validated externally but not internally, it suggests 
that the concept may by P-Creative. In contrast, if 
the compound can be validated internally but not 
externally, it suggests that the compound may be 
H-Creative and represent a genuine historical 
innovation (if only a lexical one, and of minor 
proportions).
For instance, the new compound ?sea dance?
(analogous to ?rain dance?) cannot be validated 
internally, yet can be found in over 700 internet 
documents. It thus denotes a P-Creative concept. In 
contrast, the compound ?cranial vein? yields no 
documents from a web query (on AltaVista), yet 
can be internally validated by WordNet via the 
word-concept Diploic-Vein, a blood vessel that 
serves the soft tissue of the cranial bones. 
Likewise, the compounds ?chocolate pizza?, ?taco 
pizza? and many more from the yoking of Dpizza
and Dsauce can all be validated externally via 
hundreds of different web occurrences, and so 
represent P-Creative varieties of pizza. However, 
compounds like ?Newburg pizza? (a pizza made 
with lobster sauce) and ?wine pizza? (a pizza made 
with wine sauce) can only be validated internally 
and are thus candidates for H-Creative innovation.
5 Large-Scale Evaluation
A large scale evaluation of these ideas was 
conducted by exhaustively applying the T* rule of 
(5) to the noun taxonomy of WordNet 1.7. To 
better see the effect of affinity between modifiers, 
Table 1 ranks the results according to the measure 
Aabs from (1).
Aabs 1 2 3
No. compounds 
generated 
941,841 22,727 2,175
% H-Creative 0.49% 0.63% 1.38%
% P-Creative 35.65% 33.77% 34.57%
% Conflations 0.10% 0.10% 0.05%
%Indeterminate 63.76% 65.49% 64.00%
Table 1: Number of compounds created, and their 
assessment, for each affinity level.
Conflations are terms that exist both as compounds 
and as conflated lexical atoms. For instance, while 
the compound ?bull dog? may not exist in 
WordNet, its conflation ?bulldog? does. 
Compound discovery is thus a useful means of re-
expanding these conflations when it is meaningful 
to do so. 
As one might expect, lower affinity levels allow 
greater numbers of new compounds to be created. 
Interestingly, however, Table 1 suggests that as the 
affinity threshold is raised and the number of 
compounds lowered, the creativity of these 
compounds increases, as measured by the relative 
proportion of H-Creative terms that are generated.
Generating compound terms in a lexical 
ontology is a creative process that demands 
rigorous validation if the ontology is not to be 
corrupted. Of the two strategies discussed here, 
external validation is undoubtedly the weaker of 
the two, as one should be loathe to add new 
compounds to WordNet on the basis of web 
evidence alone. However,  external validation does 
serve to illustrate the soundness of internal 
validation, since 99.51% of internally validated 
concepts (at Aabs = 1) are shown to exist on the 
web. It follows then that the absence of external 
validation yields a very conservative basis for 
assessing H-Creativity. Web validation is perhaps 
better used therefore as a means of rejecting 
creative products than as a means of discovering 
them. In fact, when used as a reverse barometer in 
this way, the inevitable errors that arise from web-
based validation serve only to make the creative 
process more selective.
6 Conclusions and Future Work
We are currently considering ways of 
broadening the scope of internal validation while 
maintaining its conceptual rigour. This should 
counter-balance the high rejection rate caused  by 
an overly conservative external validation process, 
and thereby allow us to identify a higher 
percentage of H-creative products. As shown with 
the ?pizza? examples of section 4.3, we have 
already begun to explore the possibilities of 
validation latent in the WordNet ontology itself. So 
while the use of web content for external validation 
suggests that creative discovery has a role to play 
in producing and expanding web queries, internal 
validation remains our central thrust, leading to 
what we hope will be a new, more creative model 
of the thesaurus. 
In grounding our discussion in the creative 
framework of Boden (1990) and its formalization 
by Wiggins (2003), we have placed particular 
emphasis on the labels P-Creative and H-Creative. 
However, the empirical results of section 5 suggest 
that this binary categorization may be overly 
reductive, and that a more gradated system of 
labels is needed. For instance, the novel 
compounds computer-consultant and handwriting-
consultant are both created from a yoking of the 
domains expert and consultant, and because each is 
externally validated, each is considered P-Creative. 
However, while only a handful of documents can 
be marshalled to support handwriting-consultant,
the amount of web evidence available to support 
computer-consultant is vast. So it is wrongheaded 
to consider both as equally P-Creative and lacking 
in H-Creativity, since the dearth of existing uses 
suggests handwriting-consultant has far greater 
novelty. Perhaps what is needed then is not a 
binary categorization but a continuous one, a 
numeric scale with P- and H-Creativity as its poles. 
This scale would function much like the continuum 
used by (MacCormac, 1985) to separate banal 
metaphors (which he dubbed epiphors) from 
creative ones (or diaphors).
References 
M. A. Boden. 1990. The Creative Mind: Myths and 
Mechanisms. New York: Basic Books.
P. Hanks. 2004. WordNet: What is to be done? In 
the proceedings of GWC?2004, the 2nd Global 
WordNet conference, Masaryk University, Brno.
F. Keller, and M. Lapata. 2003. Using the web to 
obtain frequencies for unseen bigrams. 
Computational Linguistics.
Lynott, Dermot and Mark Keane, 2003. The role of 
knowledge support in creating noun-noun 
compounds. In the proceedings of the 25th
Conference of the Cognitive Science Society.
E. R. MacCormac. 1985. A Cognitive Theory of 
Metaphor. Cambridge, MA: MIT Press.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross  
and K.J. Miller. 1990. Introduction to WordNet: 
an on-line lexical database. International Journal 
of Lexicography, 3(4):235 ?  244.
G. Wiggins. 2003. Categorizing Creative Systems. 
In the proceedings of the 3rd Workshop on 
Creative Systems, IJCAI?03, Acapulco, Mexico.
T. Veale and D. O?Donoghue. 2000. Computation 
and Blending. Cognitive Linguistics, 11(3/4):  
253? 281.
T. Veale. 2004. Creative Information Retrieval. In 
the proceedings of CICLing 2004, A. Gelbukh, 
ed. LNCS 2945, Springer: Berlin.
Z. Vendler. 1967. Linguistics and Philosophy. 
Ithaca, New York: Cornell University Press.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 81?88
Manchester, August 2008
A Concept-Centered Approach to Noun-Compound Interpretation 
Cristina Butnariu 
School of Computer Science and Informatics 
University College Dublin 
Belfield, Dublin 4 
Ioana.Butnariu@ucd.ie 
Tony Veale 
School of Computer Science and Informatics 
University College Dublin 
Belfield, Dublin 4 
Tony.Veale@ucd.ie 
 Abstract 
A noun-compound is a compressed 
proposition that requires an audience to 
recover the implicit relationship between 
two concepts that are expressed as nouns. 
Listeners recover this relationship by 
considering the most typical relations 
afforded by each concept. These 
relational possibilities are evident at a 
linguistic level in the syntagmatic 
patterns that connect nouns to the verbal 
actions that act upon, or are facilitated 
by, these nouns. We present a model of 
noun-compound interpretation that first 
learns the relational possibilities for 
individual nouns from corpora, and 
which then uses these to hypothesize 
about the most likely relationship that 
underpins a noun compound. 
1 Introduction 
Noun compounds hide a remarkable depth of 
conceptual machinery behind a simple syntactic 
form, Noun-Noun, and thus pose a considerable 
problem for the computational processing of 
language (Johnston and Busa, 1996). It is not just 
that compounds are commonplace in language, 
or that their interpretation requires a synthesis of 
lexical, semantic, and pragmatic information 
sources (Finin, 1980); compounds provide a 
highly compressed picture of the workings of 
concept combination, so there are as many ways 
of interpreting a noun compound as there are 
ways of combining the underlying concepts 
(Gagn?, 2002). Linguists have thus attempted to 
                                                 
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
understand noun-compounds as full propositions 
in which a phrase with two nouns connected by 
an explicit relation ? usually expressed as a verb 
and a preposition ? is compressed into a pair of 
nouns (Levi, 1978). Since these noun-pairs must 
allow an audience to reconstruct the 
decompressed proposition, there must be some 
systematic means by which the missing relation 
can easily be inferred. 
This framing of the problem as a search for a 
missing relation suggests two broad strategies for 
the interpretation of compounds. In the first, the 
top-down strategy, we assume that there are only 
so many ways of combining two concepts; by 
enumerating these ways, we can view the 
problem of interpretation as a problem of 
classification, in which compounds are placed 
into separate classes that each correspond to a 
single manner of concept connection (Kim and 
Baldwin, 2006), (Nastase and Szpakowicz, 
2003). This strategy explicitly shaped the 
SemEval task on classifying semantic relations 
between nominals (Girju et al, 2007) and so is 
employed by all of the systems that participated 
in that task. In the second, the bottom-up 
strategy, we assume that it is futile to try and 
enumerate the many ways in which concepts can 
relationally combine, but look instead to large 
corpora to discover the ways in which different 
word combinations are explicitly framed by 
language (Nakov, 2006), (Turney, 2006a). 
In this paper we describe an approach that 
employs the bottom-up strategy with an open- 
rather than closed-inventory of inter-concept 
relations. These relations are acquired from the 
analysis of large corpora, such as the Web IT 
corpus of Google n-grams (Brants and Franz, 
2006). We argue that an understanding of noun-
compounds requires an understanding of 
lexicalized concept combination, which in turn 
requires an understanding of how lexical 
concepts can be used and connected to others. As 
such, we do not use corpora as a means of 
81
characterizing noun-compounds themselves, but 
as a means of characterizing the action 
possibilities of the individual nouns that can 
participate in a compound. In other words, we 
attempt to characterize those properties of 
different concepts denoted by nouns to help 
predict how those nouns will combine with 
others, and through which relations. For instance, 
?diamonds? can be used to cover and encrust 
jewelry or to provide a sharp tip for various 
tools; we see the former usage in ?diamond 
bracelet? and the latter in ?diamond saw?. 
Likewise, ?cheese? is a solid substance which 
can be cut, as in ?cheese knife?, an edible 
substance that can used as a filling, as in ?cheese 
sandwich?, and a substance that can be melted as 
a topping, as in ?cheese pizza?. It follows that a 
sandwich can be filled, a pizza can be topped, 
knives can cut and a bracelet can have a covering 
of gems. We use relational possibilities as a 
general term for what are sometimes called the 
qualia of a word (Pustejovsky, 1995), and learn 
the linguistic relational possibilities of nouns by 
seeking out specific textual patterns in corpora. 
In section 2 we consider the most currently 
relevant elements of the substantial body of past 
work in this area. In section 3 we describe how 
corpus analysis is used to identify the most 
common lexico-semantic relational possibilities 
of nouns, while in section 4 we describe how 
these relations are used, in conjunction with web-
based validation, to interpret specific noun-
compounds. We present an evaluation of this 
approach in section 5 and conclude the paper 
with some final remarks in section 6. 
2 Related Work 
Machine-learning and example-based approaches 
to noun-compounds generally favor the top-down 
strategy for defining relations, since it allows 
training data and exemplars/cases to be labeled 
using a fixed inventory of relational classes. As 
noted earlier, this strategy is characteristic of the 
systems that participated in the SemEval task on 
classifying semantic relations between nominals 
(Girju et al, 2007), such as Butnariu and Veale 
(2007). Though the inventory is fixed in size, it 
can be defined using varying levels of 
abstraction; for instance, Nastase and 
Szpakowicz (2003) use an inventory of 35 
relations, 5 of which are top level relations with 
the remaining 30 at the lower level. The top-
down strategy pre-dates these computational 
approaches, and is a key aspect of the 
foundational work of Levi (1978) and of 
subsequent work by Gagn? and Shoben (1997), 
both of whom posit a small set of semantic 
relations as underpinning all noun compounds. 
More recently, Kim and Baldwin (2005) use a 
fixed inventory of semantic relations to annotate 
a case-base of examples; new compounds are 
understood by determining their lexical similarity 
to the closest annotated compounds in the case-
base. Kim and Baldwin (2006) link their 
relations to specific seed verbs that linguistically 
convey these relations, and then train a classifier 
to recognize which semantic relation is implied 
by a pair of nouns connected by a given 
intervening verb. This approach appears to be 
sensitive to the number of seed verbs; on a test 
involving 453 noun compounds, an accuracy of 
52.6% is achieved with 84 seed verbs, but just 
46.6% with 57 seed verbs. 
Verbs understandably play a key role in the 
interpretation of compounds, since some kind of 
predicate must be recovered to link both nouns. 
For instance, Levi (1978) uses verbs to make 
explicit the implicit relation between the nouns 
of a compound, while Finin (1980) characterizes 
the relation in a noun-noun compound using an 
inventory of all the possible verbs that can link 
both nouns; thus, e.g. salt water is interpreted 
using a relation like dissolved_in. Nakov (2006) 
takes a similar approach, and uses verb-centred 
paraphrases to express the semantic relations 
between the nouns of a compound. He argues 
that the meaning of a compound is best 
expressed via a collection of appropriate verbs 
rather than via the abstract relations (such as 
Cause, Location) that are used in more 
traditional approaches, such as those of Levi 
(1978) and Gagn? (2002).  
Nakov (2006) pursues a bottom-up strategy in 
which an open-ended inventory of relations is 
discovered using linguistic evidence. Turney 
(2006a, 2006b) similarly pursues a bottom-up, 
data-driven approach, in which semantic 
relations are expressed via representative lexico-
syntactic patterns that are mined from large text 
corpora. Turney (2006a) sorts these relational 
patterns by pertinence, a measure that reflects the 
similarity of the noun pairs in the corpus in 
which each pattern is observed to occur. Patterns 
which are relatively unambiguous and which 
serve to cluster noun pairs with similar meanings 
have higher pertinence than those that do not.  
The approach described here is similarly corpus-
based and verb-centric, but it is also noun-centric 
rather than pair-centric, which is to say, we use 
82
corpus analysis to learn about the relational 
behavior of individual nouns rather than pairs of 
nouns. Like many other authors, from Finin 
(1980) to Nakov (2006), we see the problem of 
compound interpretation as a problem of 
paraphrase generation, in which a suitable verb 
(with an optional preposition) is used to 
linguistically re-frame the compound as a 
complete proposition. This linguistic frame is a 
relational possibilities of one of the nouns that is 
apt for the other. Following Gagn? and Shoben 
(1997), this relational possibilities is frequently 
suggested by the modifier noun, but as we now 
describe, it may also be suggested by the head. 
3 Acquisition of Relational Possibilities  
The meaning of a noun compound can be 
paraphrased in a variety of ways. For instance, 
consider the compound ?headache pill?, which 
might be paraphrased as follows: 
 
P1: headache-inducing pill 
P2: headache prevention pill 
P3: pill for treating headaches 
P4: pill that causes headaches 
P5: pill that is prescribed for 
headaches 
P6: pill that prevents headaches 
 
Some paraphrases are syntactic variants of others 
(e.g., P2 and P6), others employ lexical variation 
(e.g., P1 and P4) and others are co-descriptions 
of the same event (e.g., P3 and P5 or P5 and P6). 
It thus seems unreasonable to try and reduce 
these meanings to a single semantic relation, 
since the compound can be used to mean several 
of P1 ? P6 simultaneously. Rather than try to 
construct an inventory of logical relations, closed 
or otherwise, we shall instead treat linguistic 
frames like ?for treating X?, ?that prevents X?. 
etc. as proxies for the relations themselves, while 
retaining the capacity to treat syntactic variants 
as proxies for the same relation. Moreover, these 
linguistic frames are relational possibilities of 
specific words, so that ?-inducing X? is a 
relational possibility of ?headache? while ?for 
treating X? is a relational possibility of ?pill?. 
Thus, a compound of the form ?headache X? 
might be re-framed as ?headache-inducing X? 
and a compound of the form ?X pill? might be 
re-framed as ?pill that prevents X?, ?pill that 
causes X? or ?pill for treating X?. 
The relational possibilities of individual words 
can be acquired from a large n-gram corpus like 
that of Brants and Franz (2006), as derived from 
Google?s web index. Table 1 summarizes the 
linguistic relational possibilities that can be 
derived from specific n-gram patterns. 
Google n-gram 
pattern 
Relational  
possibilities 
Logical Form 
X ? Verb+ing X Verb+ing Y verb(X, Y) 
X ? Verb+ed X Verb+ed Y verb(X, Y) 
Verb+ed prep X Y Verb+ed prep X verb_prep(Y, X)
X Verb+ed X Verb+ed prep Y verb_prep(X, Y)
for Verb+ing X Y for Verb+ing X verb(Y, X) 
X for Verb+ing X for Verb+ing Y verb(X, Y) 
that Verb+s X Y that Verb+s X verb(Y, X) 
X that Verb+s X that Verb+s Y verb(X, Y) 
 
Table 1. For an anchor noun X, the n-gram 
(left) suggests relational possibilities (middle) to 
link to a generic noun Y; different linguistic 
relational possibilities can have the same logical 
form (right). 
 
For example, we extract the following 
linguistic relational possibilities for the noun 
?diamond?, where Google frequencies are given 
in parentheses:  
accented_with_diamonds(4224), 
encrusted_with_diamonds(3990), 
decorated_with_diamonds(2616), 
based_on_diamond(2148), 
covered_with_diamonds(2018), 
filled_with_diamonds(1942), 
adorned_with_diamonds(1462), 
coated_with_diamond(1150), 
for_buying_diamonds(618), 
for_grading_diamonds(342), 
for_cutting_diamonds(430), 
dusted_in_diamond(168), 
bedecked_with_diamonds(140), 
tipped_with_diamond(108), 
crowned_with_diamonds(98), 
for_exporting_diamonds(98), 
embossed_with_diamond(90), 
edged_with_diamonds(82), 
drilled_with_diamond(86), 
that_sells_diamonds(44)?
A hat may be crowned with diamonds, a watch 
decorated with diamonds, a bracelet covered 
with diamonds, a throne encrusted with 
diamonds and a king bedecked with diamonds ? 
each is an elaboration of a basic covering 
relation, but each adds nuances of its own that 
we do not want to lose in an interpretation that is 
83
maximally specific to the nouns concerned. We 
therefore take the view that relations should be as 
open-ended and nuanced as the linguistic 
evidence suggests; if one needs to see two 
different relations as broadly equivalent, 
resources like WordNet (Fellbaum, 1998) can be 
used to make the generalization. 
4 Interpreting Noun Compounds 
We see interpretation of a compound M-H as a 
two-stage process of divergent generation 
followed by convergent validation. The 
generation process simply considers the 
relational possibilities associated either with the 
modifier M or the head H and generates a 
paraphrase from each. Consider the compound 
?yeast bread? where M = ?yeast? and H = 
?bread?; the relational possibilities for ?yeast? 
and ?bread? and used to generate a set of 
potential paraphrases as shown in Table 2. For 
clarity, ?M? and ?H? denote the parts of each 
paraphrase frame that will be filled with the 
modifier and head respectively. 
 
Relational possibilities 
for M 
Paraphrases for M-
H 
H Verb+ed prep M 
e.g., H derived from yeast 
H Verb+ed prep M 
e.g., bread derived 
from yeast 
H that Verb+s M 
e.g., H that contains yeast 
H that Verb+s M 
e.g., bread that 
contains yeast 
Relational possibilities 
for H 
Paraphrase for M-H 
H Verb+ed prep M 
e.g., bread prepared with 
M 
H Verb+ed prep M 
e.g., bread prepared 
with yeast 
H that Verb+s M 
e.g., bread that has M 
H that Verb+s M 
e.g., bread that has 
yeast 
Table 2. Relational possibilities of the head (H) 
and modifier (M) nouns used for paraphrasing. 
 
The relational possibilities for the head noun 
?bread? yield the following paraphrases, where 
numbers in parentheses are Google frequencies 
for the original n-grams on which each 
paraphrase is based: 
 
?bread made from yeast? (6335), 
?bread topped with yeast? 
(6043), ?bread made with yeast? 
(4726), ?bread stuffed with 
yeast? (3871), ?bread baked in 
yeast? (3341), bread made of 
yeast? (3064), ?bread served 
with yeast? (3012), ?bread 
soaked in yeast? (2975), ?bread 
dipped in yeast? (2873), ?bread 
filled with yeast? (2783), ... 
 
Similarly, the relational possibilities for the 
modifier noun ?yeast? yield the following 
paraphrases: 
 
?bread expressed in yeast? 
(14058), ?bread leavened with 
yeast? (10816), ?bread derived 
from yeast? (2562), ?bread 
based on yeast? (1200), ?bread 
fermented with yeast? (842), 
?bread raised with yeast? 
(736), ?bread induced in yeast? 
(342) , ?bread infected with 
yeast? (262), ?bread filled 
with yeast? (120), ? 
 
While these two sets of relational possibilities 
capture the most salient activities in which 
?yeast? and  ?bread? participate, many of the 
paraphrases listed here are inappropriate for 
?yeast bread?. Candidate paraphrases for a noun 
compound are useful only when one has a means 
of determining the degree to which paraphrases 
are meaningful and apt and of rejecting those 
which are not. This process typically assumes 
that a meaningful paraphrase is one for which 
evidence of prior usage can be found in a large 
corpus (like the web); the greater this evidence, 
the more favored a given paraphrase should be. 
This assumption is central to Nakov (2006), who 
uses templates to find paraphrases for a noun 
compound on the web. These templates use the 
Google wildcard * to indicate the position of a 
verb so that the specific verbs at the heart of a 
paraphrase can be mined from the snippets that 
are returned. Nakov (2007) uses the schematic 
patterns ?N1 that * N2?,  ?N2 that * N1?,  ?N1 * 
N2? and ?N2 * N1?, where the wildcard can 
stand for as many a eight contiguous words. 
Relational possibilities allow us, in the first 
divergent stage of interpretation, to generate 
fully-formed paraphrases that do not require 
wildcards, so the second convergent stage of 
interpretation simply needs to validate these 
paraphrases by finding one or more instances of 
each on the web. Indeed, an especially 
compelling paraphrase may be found in the 
Google n-grams themselves, without recourse to 
the web. For instance, the paraphrase ?bread 
leavened with yeast? has a frequency of 56 in the 
database of Google 4-grams, while the 
84
paraphrase ?bread based on yeast? has such a 
low web frequency of 2 hits that it can be 
validated only by going to the web.  
But web-based validation has its limitations: it 
cannot account for novel and creative 
compounds, nor can it account for conventional 
compounds whose meaning is not echoed in an 
expanded paraphrase-form on the web. Thus, we 
also consider an alternate validation procedure 
for those paraphrases that can  be generated both 
from a modifier noun relational possibility and 
from a head noun relational possibility. For 
example, ?bread filled with yeast? can be derived 
from the head relational possibility ?bread filled 
with X? which has a frequency of 2783, and 
from the modifier relational possibility ?X filled 
with yeast? which has a frequency of just 120. 
This dual basis for generation provides evidence 
that the paraphrase is meaningful without the 
need to actually find the paraphrase on the web. 
We refer to the validation of paraphrases in this 
way as validation by matching relational 
possibilities of the modifier and head nouns. 
This matching relational possibilities 
procedure does not require web validation, and 
so does not produce a web frequency for each 
paraphrase. We thus need to assign a score to a 
paraphrase based on the web frequencies of the 
matching relational possibilities that give rise to 
it. For simplicity, we add the web frequency of 
the head relational possibility (e.g., 2783 from 
?bread filled with X?) to the frequency of the 
modifier relational possibility (e.g., 120 from ?X 
filled with yeast?) to obtain an invented 
frequency for the generated paraphrase (e.g., 
2903 for ?bread filled with yeast?). 
The third and more restricted validation 
procedure we employ is a hybrid one, based on 
the intersection of the two procedures above: we 
require web-validation of paraphrases that are 
already validated by virtue of arising from 
matching head and modifier relational 
possibilities. In this case, we rank the 
paraphrases by their actual web frequency. The 
set of paraphrases validated by the hybrid 
approach will be a subset of the paraphrases 
validated by the other two validation methods; 
the size of this subset will be informative about 
the relative utility of each procedure. 
5 Empirical Evaluation 
To evaluate the relational possibility approach to 
noun-noun interpretation, we perform two 
experiments: one to consider how well the set of 
validated paraphrases can be mapped to the 
abstract relations used by (Nastase and 
Szpakowicz, 2003) to annotate their noun-noun 
compounds, and one to consider how well these 
paraphrases match the paraphrases offered by 
humans for the same noun compounds. To 
understand the role of different validation 
strategies, we use three variants of the model that 
correspond to the three means of validating 
paraphrases: model-1 uses the presence of the 
relational possibility on the web as the mark of a 
valid paraphrase; model-2 uses the matching 
relational possibilities procedure to validate a 
paraphrase (i.e., the paraphrase must arise from 
both an relational possibility of the modifier and 
of the head); a third model, model-3 intersects 
both validation procedures. In each case, 
validated paraphrases are ranked by their 
frequency scores, as found explicitly on the web 
in the case of model-1 and model-3, or as 
invented in model-2.   
5.1 Mapping compounds to abstract relations 
In the first experiment, we test the relational 
possibility model on a set of noun-noun 
compounds from Nastase and Szpakowicz 
(2003), whose data is pre-classified into abstract 
classes of semantic relations (i.e., Agent, 
Instrument, Location). We perform a manual 
analysis on the paraphrases that are generated 
and validated for each noun pair, to measure how 
accurately each paraphrase matches the pre-
classified abstract semantic relation. The Nastase 
and Szpakowicz (2003) dataset comprises 600 
word pairs of the form adj-noun, adv-noun and 
noun-noun; for this experiment we use only the 
329 noun-noun pairs, which are each pre-labeled 
with one of 28 different semantic relations. 
We consider and quantify two eventualities here: 
those situations in which the relational possibility 
model generates and validates a paraphrase that 
closely corresponds to the semantic relation 
assigned by Nastase and Szpakowicz (2003); and 
those situations in which the relational possibility 
model generates and validates an interpretation 
that a human judge considers a plausible and 
sensible interpretation of a compound regardless 
of Nastase and Szpakowicz (2003)?s 
interpretation. Table 3 presents validated 
relational possibilities for the compound ?olive 
oil?, where those that match the pre-classified 
relation are in bold, and those that are otherwise 
plausible are italicized. 
 
85
Paraphrases generated by 
web-based validation 
Paraphrases generated by 
matching relational possibilities 
Paraphrases generated by 
Nakov (2007) 
extracted from (189), obtained from 
(132), mixed with (87), made from 
(75), produced from (38), pressed 
from (35), colored (25), infused (20), 
enriched with (16), made of (14), 
flavored (13), made with (12), derived 
(10), based (10), produced by (9), 
blended with (8), coloured (7), based 
on (7), combined with (6), found in 
(6), dissolved in (6), served with (6), 
contained in (5), replaced by (4), 
flavoured (3), come from (3)?  
used in (25839), obtained from 
(15352), extracted from (14561), 
made from (11627), found in 
(11524), used for (9919), mixed with 
(9781), produced from (7794), 
produced by (6776), made with 
(5423), used as (4880),  are in (4577), 
contained in (4551), come from 
(4241), based on (4135), combined 
with (4029), added to (3848), made in 
(3608) ? 
come from (13), be 
obtained from (11), be 
extracted from (10), be 
made from (9), be 
produced from (7), be 
released from (4), taste 
like (4), be beaten from 
(3), be produced with (3) , 
emerge from (3) 
Table 3. Validated paraphrases for ?olive oil?; matches with Nastase and Szpakowicz are in 
bold; other sensible interpretations are italicized. 
 
We also consider the rank of the paraphrases 
that match the relations assigned by Nastase and 
Szpakowicz (2003) to this data set. Figure 1 
graphs the F-measure for the relational 
possibilities approach when this relation is the 
top-ranked validated paraphrase, when it is in the 
top two validated paraphrases, and  more 
generally, when  it is in the top n validated 
paraphrases, n <= 20. Model-1 (web-based 
validation) out-performs Model-2 (matching 
relational possibilities, with no web validation) 
when we consider just a small window of top 
ranked paraphrases, but this situation reverses as 
the window (whose size is given on the x-axis) is 
enlarged.  
F-measure (%) for top ranked paraphrases
0
10
20
30
40
50
60
70
80
90
1 3 5 7 9 11 13 15 17 19
Model 1
Model 2
Model 3
  
Figure 1. F-measure for target semantic 
relations of top n ranked paraphrases generated 
with Model-1, Model-2 and Model-3. 
Model 3 (which requires both matching 
relational possibilities and web validation) shows 
similar results to Model 2 (web validation only), 
which suggests that the matching relational 
possibilities criterion is strongly predictive of 
web-validation. This further suggests that 
matching relational possibilities alone can 
reliably validate a paraphrase even when web 
evidence is lacking, as will be the case in 
creative noun compounds. 
During this evaluation process, we observe a 
tendency for specific paraphrases to co-occur 
when conveying a certain relation. For instance, 
Y obtained from X typically co-occurs with Y 
produced from X to indicate Nastase and 
Szpakowicz?s Source relation, while Y caused by 
X co-occurs with Y induced by X to convey their 
Effect relation, and Y owned by X co-occur with 
Y held by X to indicate their Possessor relation. 
This observation is similar to that of Nakov 
(2007), who performs a manual analysis of 
paraphrases obtains from web-mining. The 
results he reports are similar to those obtained 
using the relational possibilities approach, as 
shown in Table 3. 
5.2 Comparing human-generated paraphrases 
In the second experiment, we compared the 
paraphrases validated by the relational 
possibilities approach to human-generated 
paraphrases reported by Nakov (2007) and to the 
paraphrases generated by Nakov?s own web-
mining approach to this task. Nakov (2007) 
collected human paraphrases for each noun-
compound in his data-set (250 noun compounds 
listed in the appendix of Levi, 1978) by asking 
subjects to rephrase a noun-compound using a 
relative-clause centred around a single verb with 
an optional preposition. This rephrasing elicited 
human-generated paraphrases like the following: 
 
'neck vein is a vein that comes 
from the neck'  
'neck vein is a vein that drains 
the neck' 
 
86
Nakov then extracted normalized verbs and 
prepositions from these paraphrases to obtain a 
reduced verb-based form for each, e.g., to obtain 
the reduced forms come from and drain from the 
above examples. He used 174 subjects for this 
task, to generate around 17,000 reduced forms, 
or 71 forms per compound.  
For each of his 250 noun pairs we constructed 
three vectors h, w, and a, using human-generated 
paraphrase verbs and their frequencies (h), 
Nakov's web-extracted verbs and their 
frequencies (w) and the verbs of the paraphrases 
obtained using the relational possibilities 
approach and their frequencies (a). Following 
Nakov (2007), we then calculated the cosine 
correlation between two frequency vectors using 
the formula: 
simcos(h,w) = ?hiwi   ? ??hi2 ?? wi2 
For ease of comparison, the a vector is 
populated with verbs and frequencies from just 
two patterns, Y Verb+ed Prep X and Y that 
Verb+s X. In Table 4 we report the average 
cosine correlation across the vectors for all 250 
noun pairs, to compare for the three validation 
models the relational possibilities-based and 
Nakov?s web-generated paraphrases and the 
relational possibilities -based and human-elicited 
paraphrases. Also shown, in the last row, is the 
average cosine correlation  between Nakov?s 
web-mined paraphrases and human-elicited 
paraphrases, as reported in Nakov (2007). 
 
Model 1 (web-validation)  
correlation to humans 26.8 % 
correlation to web-mined approach 27.1 % 
Model 2 (matching relational possibilities) 
correlation to humans 17 % 
correlation to web-mined approach 14.25 % 
Model 3 (intersection of Model-1 and Model-2) 
correlation to humans 27.9 % 
correlation to web-mined approach 28 % 
Web-mining (Nakov, 2007) 
correlation to humans 31.8% 
Table 4. Average correlation between web-
mined paraphrases and relational possibilities-
based paraphrases with human elicitations. 
 
The results show the difference in quality of 
the paraphrases validated by each of our models. 
The matching-relational possibilities model 
(Model 2) yields the largest number of 
paraphrases. In the first experiment we showed 
that this model outperforms the other two when 
we consider just top-ranked paraphrases, but here 
it appears that this wider range of potentially 
creative interpretations diminishes the cosine 
correlation with human-elicited interpretations. 
But the most plausible paraphrases come to the 
fore in the hybrid model (Model 3), whose 
paraphrases are a subset of those of Models 1 and 
2. This hybrid approach also outperforms Model 
1 and compares well with the results obtained by 
web mining. 
The difference in cosine correlation between 
human-elicited and relational possibitlies-based 
paraphrases in Model-3 (27.9%) and Nakov?s 
web-mined and human-elicited paraphrases 
(31.8%) can be justified both by the type of 
patterns used in the comparison, and by the type 
of patterns used to validate paraphrases. For one, 
we consider paraphrases generated using just two 
forms of relational possibilities, Y Verb+ed Prep 
X and Y that Verb+s X, since these can be 
directly compared to the type of relative-clause 
paraphrases used in this experiment. 
Furthermore, relational possibilities are derived 
from Google n-grams where n < 6, we allow up 
to four words to intervene between the modifier 
and the head in a paraphrase, while the web-
mining paraphrases benefit from a larger window 
of intervening words (up to 8). Nonetheless, in 
88 out of the 250 pairs, the correlation between 
relational possibilities-based and human-elicited 
paraphrases is larger than that observed for the 
web-mining approach. 
6 Conclusions 
Since the meaning of noun compounds arises 
from a combination of individual noun meanings, 
it follows that the key input to the process of 
compound interpretation is detailed linguistic 
knowledge about how these nouns are 
conventionally used in language. This point may 
seem obvious, but a model of compounding can 
place so much emphasis on the behavior of noun-
pairs that the linguistic behavior of nouns in 
isolation is easily over-looked.  
We have presented a model of noun-
compounding that places nouns and their specific 
linguistic relational possibilities at the centre of 
processing. When one considers that linguistic 
relational possibilities capture aspects of noun 
meaning such as purpose, constitution and 
agency, their realization here can be viewed as a 
87
generalized and lexicalized aspect of qualia 
structure in the sense of Pustejovsky (1995) and 
Johnston and Busa (1996). Indeed, the n-gram 
patterns used to extract these relational 
possibilities from corpora are not unlike the 
patterns used by Cimiano and Wenderoth (2007) 
to harvest qualia structures from the web. 
We conclude from the empirical observation 
that the hybrid model outperforms the web-based 
model (albeit slimly) in experiment 2 while both 
perform equally well in experiment 1, is that the 
modifier and head are of comparable 
performance when paraphrasing the 
interpretations of noun compounds. Recall that 
the web-validation approach (Model-1) generates 
interpretations from either the modifier or the 
head, while the matching-relational possibilities 
and hybrid models require both to contribute 
equally. 
Necessary extensions to the approach include 
the acquisition of more relational possibilities of 
greater linguistic complexity, the ability to 
organize relational possibilities hierarchically 
according to their underlying semantic meanings, 
and the ability to recognize an implication 
structure among different but related relational 
possibilities. 
Acknowledgement 
We would like to thank Preslav Nakov for 
providing us the data used in the second 
experiment. 
References 
Brants, T., and Franz, A. 2006. Web 1t 5-gram 
version 1, Linguistic Data Consortium. 
Butnariu, C., and Veale T. 2007. A hybrid model for 
detecting semantic relations between noun pairs in 
text. In Proc. of the Fourth International Workshop 
on Semantic Evaluations (SemEval-2007), Prague,. 
Association for Computational Linguistics. 
Cimiano, P., and Wenderoth, J. 2007. Automatic 
Acquisition of Ranked Qualia Structures from the 
Web. In Proc. of the 45th Annual Meeting of the 
ACL, pp 888-895. 
Fellbaum, C. 1998. WordNet, an electronic lexical 
database. Cambridge: MIT Press. 
Finin, T. 1980. The semantic interpretation of 
compound nominals. Urbana, Illinois: University of 
Illinois dissertation. 
Gagn?, C. L., and Shoben, E. J. 1997. Influence of 
thematic relations on the comprehension of 
modifier-noun combinations. Journal of 
Experimental Psychology: Learning, Memory, and 
Cognition, 23, 71?87  
Gagn?, C. L. 2002. Lexical and Relational Influences 
on the Processing of Novel Compounds. Brain and 
Language 81(1-3), pp 723-735. 
Girju, R., Nakov, P. Nastase, V., Szpakowicz, S., 
Turney, P., and Yuret, D. 2007. Semeval-2007 task 
04: Classification of semantic relations between 
nominals. In Proc. of the Fourth International 
Workshop on Semantic Evaluations (SemEval-
2007), 13?18, Prague, Czech Republic. ACL. 
Johnston, M., and Busa, F. 1996. Qualia structure and 
the compositional interpretation of compounds. In 
Proc. of the ACL SIGLEX workshop on breadth 
and depth of semantic lexicons, Santa Cruz, CA. 
Kim, S. N., and Baldwin, T. 2006. Interpreting 
semantic relations in noun compounds via verb 
semantics. In Proc. of the 21st International 
Conference on Computational Linguistics and the 
44th annual meeting of the ACL, pp 491?498, NJ, 
USA. 
Kim, S. N., and Baldwin, T. 2005. Automatic 
interpretation of noun compounds using WordNet 
similarity. In Proc. of the 2nd International Joint 
Conference On Natural Language Processing, pp 
945?956, Cheju, Korea. 
Levi, J. 1978. The syntax and semantics of complex 
nominals. NY: Academic Press. 
Nakov, P., and Hearst, M. A. 2006. Using verbs to 
characterize noun-noun relations. In AIMSA, 
Jerome Euzenat and John Domingue (eds.), vol. 
4183 of Lecture Notes in Computer Science, pp 
233?244. Springer. 
Nakov, P. 2007. Using the Web as an Implicit 
Training Set: Application to Noun Compound 
Syntax and Semantics. Ph.D. Dissertation, 
University of California at Berkeley. 
Nastase, V., and Szpakowicz, S. 2003. Exploring 
noun-modifier semantic relations. In Proc. of the 
5th International Workshop on Computational 
Semantics (IWCS-5), pp 285?301, Tilburg, The 
Netherlands. 
Pustejovsky, J. 1995. The Generative Lexicon. The 
MIT Press, Cambridge, MA. 
Turney, P. 2006a. Expressing implicit semantic 
relations without supervision. In Proc. of the 21st 
International Conference on Computational 
Linguistics and the 44th annual meeting of the 
ACL, pp 313?320, NJ, USA. 
Turney, P. D. 2006b. Similarity of semantic relations. 
Computational Linguistics, 32, pp 379?416. 
88
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 945?952
Manchester, August 2008
A Fluid Knowledge Representation for Understanding and Generating
Creative Metaphors
Tony Veale
School of Computer Science
University College Dublin
Ireland
tony.veale@ucd.ie
Yanfen Hao
School of Computer Science
University College Dublin
Ireland
yanfen.hao@ucd.ie
Abstract
Creative metaphor is a phenomenon that
stretches and bends the conventions of se-
mantic description, often to humorous and
poetic extremes. The computational mod-
eling of metaphor thus requires a knowl-
edge representation that is just as stretch-
able and semantically accommodating. We
present here a flexible knowledge repre-
sentation for metaphor interpretation and
generation, called Talking Points, and de-
scribe how talking points can be acquired
on a large scale from WordNet (Fellbaum,
1998) and from the web. We show how
talking points can be fluidly connected to
form a slipnet, and demonstrate that talk-
ing points provide an especially concise
representation for concepts in general.
1 Introduction
Metaphor serves two important roles in language.
The first of these is to make the unfamiliar and the
strange seem more familiar and understandable
(Indurkhya, 1992). For instance, one might
describe a burqa (a full body covering for Muslim
women) as a suit of armor, as a shield against
prying eyes or, depending on one?s communi-
cation goal, as a wearable cage. The other role
of metaphor is most often associated with the
poetic and fanciful use of language, but is no less
important: to make the familiar and mundane
seem strange and unfamiliar. In this latter guise,
metaphor allows us to view a commonplace idea
from a new and revealing category perspective
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
(Camac and Glucksberg, 1984). For instance,
one might describe makeup as ?the Western
burqa?, to communicate not just the idea that each
involves a covering of the female form, but that
each reflects a society-imposed expectation on
the public presentation of women. Each of these
roles is a manifestation of the same underlying
mechanism for combining concepts, for under-
standing how they interact (Black, 1962) and for
determining how they are connected (Fauconnier
and Turner, 1998), even if those connections are
tenuous, hidden or not always obvious (Collins
and Loftus, 1975). For example, consider the
connections needed to make and understand the
above metaphors:
Burqa ?
? for concealing a Muslim woman
? for protecting a Muslim woman
? protecting a woman
? for protecting a person
Armor ?
Make-up ?
? typically worn by women
? expected to be worn by women
? must be worn by women
? must be worn by Muslim women
Burqa ?
In each case we see how metaphor draws out
and highlights, in a modified or exaggerated form,
an existing aspect of each target concept. In
other words, metaphor does not indiscriminately
transplant arbitrary aspects of a source concept
onto a target, but accommodates a selective
graft of the most salient aspects of this source
concept onto those aspects of the target that can
945
be highlighted by the comparison (Ortony, 1979).
This connection between concepts requires a
flexible knowledge representation, one that allows
the connections between non-identical source and
target aspects to be recognized, reconciled and
even compressed (Fauconnier and Turner, 1998).
This fluid representation (Hofstadter et al, 1995)
defines the search space in which the processes of
metaphor generation and metaphor understanding
are cognitively situated (Veale and O?Donoghue,
2000): for generation, fluid connectivity allows a
system to search outwards from a given target to
find those potential source concepts that offer a
new yet appropriate perspective; for understanding
purposes, connectivity allows an agent to focus
on those key aspects of a source concept that are
most apt for a target because they can be linked to
that target.
In this paper we describe the construction
of a fluid knowledge representation for creative
metaphor processing, one that is acquired automat-
ically from WordNet (Fellbaum, 1998) and from
the texts of the web. In section 2 we summarize
related work in the field of metaphor as it pertains
to flexible knowledge representation. In section 3
we describe two complementary means of acquir-
ing the basic elements of this representation, from
WordNet and from the web, before describing how
these elements can be placed into a fluid network
of connections - what Hofstadter (ibid) calls a slip-
net - in section 4. We then present in section 5
some empirical evaluation of the acquired repre-
sentation on an objective test of term categoriza-
tion, before concluding with some consideration of
future work in section 6.
2 Related Work
Since metaphor can be viewed as a stretching
of linguistic conventions to cover new conceptual
ground, the interpretation of metaphor crucially
hinges on a systems ability to recognize these con-
ventions and accommodate the exceptional mean-
ing conveyed by each figurative expression. In-
deed, most computational approaches embody a
sense of what it means to be literal, and accom-
modate metaphoric meanings within this conven-
tional scheme through a form of relaxation, map-
ping or translation. Wilks (1978) advocates that
the typically hard constraints that define a literal
semantics should instead be modeled as soft pref-
erences that can accommodate the violations that
arise in metaphoric utterances, while Fass (1991)
builds on this view to show how these violations
can be repaired to thus capture the literal intent be-
hind each metaphor. This repair process in turn
relies on the availability of a concept taxonomy
through which metaphoric uses can be mapped
onto their literal counterparts; a car that ?drinks
gasoline? would thus be understood as a car that
?consumes gasoline?. Way (1991) emphasizes
the importance of this taxonomy by positing a
central role for a dynamic type hierarchy (DTH)
in metaphor, one that can create new and com-
plex taxonyms on the fly. For instance, Way?s
DTH would understand the ?make-up as Western
burqa? metaphor via a dynamically created tax-
onym like things-women-are-expected-to-wear-in-
public, though Way offers no algorithmic basis
for the workings of such a remarkable taxonomy.
Another family of computational approaches com-
bines explicit knowledge about certain metaphors
with knowledge about the domains connected by
these metaphors. Martin?s (1990) Midas sys-
tem encodes schematic knowledge about conven-
tionalized metaphors such as ?to kill a process?
and ?to open a program?, and uses this knowl-
edge to fit novel variations of these metaphors
into the most appropriate schemas. Barnden and
Lee (2002) focus on the role of inference in a
metaphorically-structured domain, and describe a
system called ATTMeta that contains sufficient
knowledge about e.g., conventional metaphors of
mind to reason about the mental states implied by
these metaphors. Each of these approaches sees
metaphor interpretation as a process of fitting what
is said to what can meaningfully be represented
and reasoned about. This fitting process is most ex-
plicitly modelled by Hofstadter et al (1995), who
focus on the slippage processes that are required to
understand analogies in abstract domains that e.g.,
involve the mapping of letter sequences or the mir-
roring of actions in a highly stylized tabletop en-
vironment. Though simplified and toy-like, these
are non-deterministic problem domains that are
nonetheless shaped by a wide range of pragmatic
pressures. Hofstadter and Mitchell (1994) model
these pressures using a slipnet, a probabilistic net-
work in which concepts are linked to others into
which they can slip or be substituted with. In this
view, deeply embedded concepts that are further
removed from direct observation are less likely to
engage in slippage than more superficial concepts.
946
To take a linguistic example, word choice in natu-
ral language generation is more susceptible to slip-
page (as influenced by synonym availability) than
the concepts underlying the meaning of a sentence.
Slippage can be seen as a lossy form of con-
ceptual re-representation: the greater the slippage,
the more dramatic the re-representation and the
greater the potential for loss of accuracy. For
instance, a recent magazine cover proclaims the
governor of California, Arnold Schwarzenegger,
as ?president of 12% of the United States?. This
conceptualization can be viewed as an interme-
diate stage in a slippage path from Governor to
President as follows:
Governor of California ?
? governor of 12% of the United States
? leader of 12% of the United States
? president of 12% of the United States
? president of 100% of the United States
President of the U.S. ?
This labeling is creative enough to grace a
magazine cover because it involves an ambitious
level of re-conceptualization, at least from a
computational perspective. The pivotal insight
comes from California = 12% of the United
States, an ad-hoc synonym that one is unlikely to
find in a dictionary or general-purpose resource
like WordNet. While ultimately aiming for this
kind of creative transformation, our goal in this
paper is more modest: to build a slippage network
of concepts that are connected via their most
salient features, one that combines the principled
flexibility of a Hofstadter-style slipnet with the
comprehensive scale of a resource like WordNet.
3 Acquiring Conceptual Talking Points
We refer to the knowledge elements connected
by this slipnet as conceptual talking points. We
first describe the form of these talking points
and how they are acquired, before describing in
section 4 how slippage operates between these
talking points. We discuss two complementary
kinds of talking point here: objective descriptions,
extracted from WordNet glosses, and informal,
stereotypical descriptions, harvested from the text
of the web via a search engine like Google.
3.1 Objective Talking Points
Objective talking points are aspects of conceptual
description that contribute to the consensus defini-
tional view of a concept. Though WordNet does
not provide explicit semantic criteria for the defi-
nition of each lexical concept, many of these cri-
teria can be gleaned from a shallow parse of the
pithy dictionary gloss it associates with each (e.g.,
see Ahlswede and Evans, 1988). Thus, whenever
the head phrase of a concept?s gloss has the form
?ADJ
+
NOUN? where NOUN can denote a hyper-
nym of the concept, we can associate the talking
point is ADJ:NOUN with that concept. For exam-
ple, the gloss of {Hamas} is ?a militant Islamic
fundamentalist political movement that ...?, which
yields the talking points is militant:movement,
is islamic:movement, is fundamentalist:movement
and is political:movement for Hamas. When a
WordNet concept has a hypernym of the form
{ADJ NOUN}, where NOUN can denote a hy-
pernym of this concept, we likewise associate
the talking point is ADJ:NOUN with that con-
cept. For example, {Taliban, Taleban} has
{religious movement} as a hypernym, which
yields is religious:movement as a talking point for
Taliban.
Objective talking points can also be gleaned
from the subject-verb-object structure of a Word-
Net gloss. For instance, the gloss for synset
{conductor, music director} is ?the person who
leads a musical group?, which yields the talking
point leads:musical group. The hypernym of
this concept, {musician}, has the gloss ?artist
who composes or conducts music ...?, which
yields the talking points composes:music and con-
ducts:music that are then inherited by {conductor,
...} and other sub-types of musician in WordNet.
A shallow parse will generally not lead to a
complete understanding of a concept, but will
typically produce some interesting talking points
of the predicate:object variety that can be used
to relate a concept to others that are analogically
or metaphorically similar. Using WordNet?s
noun and verb taxonomies, we can identify the
following slippage paths between talking points:
composes:music ? composes:speech ?
writes:speech ? writes:oration ? writes:sermon
? writes:law ? writes:philosophy ?
writes:theorem ? writes:plan ? ...
947
In all, we extract talking points of the form
is adj:noun for over 40,000 WordNet concepts,
and talking points of the form verb:noun for over
50,000 concepts. However, the real power of
talking points emerges when they are connected to
form a slipnet, as we discuss in section 4.
3.2 Stereotypical Talking Points
The talking points we harvest from the web do not
have the authoritative, definitional character we
find in hand-crafted resources like WordNet, but
they do reflect how people typically speak of (and,
perhaps, actually think of) the world. Veale and
Hao (2007) argue that similes present the clear-
est window into the stereotypical talking points
that underpin everyday conversations, and collect
from the web instances of the pattern ?as ADJ as a
*? for thousands of WordNet adjectives. Though
the simile frame is somewhat leaky in English,
and prone to subversion by irony, Veale and Hao
construct a comprehensive database of more than
12,000 highly stereotypical adjective:noun asso-
ciations, such as precise:surgeon, straight:arrow,
balanced:pyramid and sharp:knife. We use their
data here, as the basis of an additional web harvest-
ing process to gather stereotypical talking points
of the form has ADJ:facet. For every stereotypi-
cal association ADJ:NOUN in their database, we
send the query ?the ADJ * of a|an|the NOUN? to
Google and collect noun values for the wildcard *
from the first 200 hits returned for each query.
This pattern allows us to determine the con-
ceptual attributes that are implicit in each stereo-
typical adjective:noun pairing. For instance, ?the
delicate hands of a surgeon? and ?the inspiring
voice of a preacher? reveal that hand is a salient
attribute of surgeons while voice is a salient at-
tribute of preachers. The frequency with which
we find these attributes on the web also allows
us to build a textured representation for each con-
cept. So while these expanded web patterns also
reveal that surgeons have a thorough eye and
steady nerves, ?the hands of a surgeon? are men-
tioned far more frequently and are thus far more
salient to our understanding of surgeons. To avoid
noise, the set of allowable attribute nouns, such
as hands, soul, heart, voice, etc., is limited to the
nouns in WordNet that denote a kind of trait, body
part, quality, activity, ability or faculty. This al-
lows us to acquire meaningful talking points like
has magical:skill for Wizard, has brave:spirit for
Lion and has enduring:beauty for Diamond, while
avoiding dubious or misleading talking points like
has proud:owner for Peacock that lack either rep-
resentational value or insight. In all, this pro-
cess acquires 18,794 stereotypical talking points
for 2032 different WordNet noun senses, for an av-
erage of 9 facet:feature pairs per sense. Specific
senses are identified automatically, by exploiting
WordNet?s network of hypernymy and synonymy
relations to connect talking points that describe
variations of the same concept.
4 Building a Slipnet of Talking Points
To construct a slipnet in the style of Hofstadter
and Mitchell (1994), but on the scale of WordNet,
we need to connect those talking points that ex-
press similar but different meanings, and to quan-
tify the difference between these meanings. Is-
sues of scale mean that we need only connect
talking points that are close in meaning, since
greater slippage can be achieved by following
longer paths through the slipnet. This slippage can
be based on semantic or pragmatic criteria. Thus,
the talking points has sacred:authority (for Pope)
and has sacred:power (for God) are semantically
similar since the potency sense of ?authority? is
a specialization of the control sense of ?power?
in WordNet. Likewise, writes:speech and com-
poses:speech are similar because ?compose? and
?write? are synonymous in the context of literary
creation, and it is this particular linkage that sup-
ports a slippage pathway from composes:music to
writes:poetry. In contrast, is political:movement
(for Hamas) and is religious:movement (for Tal-
iban) are pragmatically similar since movements
that are religious often have a political agenda also.
We can use WordNet to construct the semantic
links of the slipnet, but pragmatic links like these
require not just word senses but a sense of the
world, of a kind we can distil from the text of the
web.
Two talking points is ADJ
1
:OBJ
1
and
is ADJ
2
:OBJ
2
should be connected in the
slipnet if: OBJ
1
and OBJ
2
are semantically close
(i.e., synonymous, or semantic siblings in Word-
Net); and ADJ
1
and ADJ
2
are synonymous, or
ADJ
1
frequently implies ADJ
2
or ADJ
2
frequently
implies ADJ
1
. These implications are recog-
nized and quantified using another web trawling
process, in which the query ?as * and * as? is
used to harvest pairs of adjectives that are seen to
948
mutually reinforce each other in web comparisons.
This search reveals that ?religious? reinforces
?superstitious? (5 times), ?moral? (4), ?political?
(3), ?conservative? (3), ?intolerant? (2) and
?irrational? (1). These slippage connections link
is religious:movement to is political:movement
(pragmatic) to is political:campaign (semantic)
to is military:campaign (pragmatic), thereby
connecting Taliban (is religious:movement) to
Crusade (is military:campaign).
4.1 The Slipnet in Action
Slippage is a phenomenon best explained with an
example, so consider again the task of creating
metaphors for the concept Pope. We have al-
ready seen that slippage among talking points al-
lows Pope to be linked to the concept God via Pope
? has sacred:authority ? has sacred:power ?
God. Pope can also be linked to Rabbi via the
path Pope ? has sacred:words ? has wise:words
? Rabbi and to Judge by extending this pathway:
Pope ? has sacred:words ? has wise:words ?
has solemn:words ? Judge. Black (1962) saw
metaphor as an interaction between concepts, in
which the interpretation of a particular source con-
cept depends crucially on how it is able to inter-
act with a specific target concept. This concept-
sensitive interplay is clearly on display here. The
Pope can be metaphorically viewed as a warrior
not by considering what it means for a generic
person to be a warrior, but by considering how
the concept Pope actually interacts with the con-
cept Warrior, e.g., Pope ? has infallible:voice ?
has powerful:voice ? Warrior.
Consider the potential for slippage between ob-
jective talking points from WordNet:
Pope ?
? leads:Roman Catholic Church
? leads:congregation
? leads:flock
? leads:mob
? leads:organized crime
Don (Crime Father) ?
Pope ?
? leads:Roman Catholic Church
? leads:congregation
? leads:political movement
? leads:gang
? leads:military force
Warlord (Military Leader) ?
One can typically terminate a slippage path at
any point, to produce different metaphors with
varying semantic similarity to the starting con-
cept. Thus, at leads:flock one can reach Shepherd,
and from leads:political movement, one can reach
Civil rights leader.
A lexicon alone, like WordNet, is generally in-
sufficient for metaphor processing, but such a re-
source can still reveal useful lexical resonances
that may enrich an interpretation. In the exam-
ple above, we see a resonance between the Pope,
which WordNet alo lexicalizes as ?holy father?,
and a mafia Don, which WordNet alo lexicalizes
as ?father?. Indeed, since WordNet conceptualizes
Roman Catholic Church as a specialization of Or-
ganized religion, the metaphor establishes a par-
allelism between crime and religion as organized
activities.
5 Empirical Evaluation
To understand whether talking points are suffi-
ciently descriptive of the concepts they are ac-
quired for, we replicate here the clustering ex-
periments of Almuhareb and Poesio (2004, 2005)
which are designed to measure the effectiveness of
web-acquired conceptual descriptions. Since Al-
muhareb and Poesio use WordNet as a semantic
gold-standard, we consider here the effectiveness
of stereotypical talking points alone; it would be
circular to consider objective talking points, since
these are extracted from WordNet.
Almuhareb and Poesio describe two different
clustering experiments. In the first, they choose
214 English nouns from 13 of WordNet?s upper-
level semantic categories, and proceed to harvest
property values for these concepts from the web
using the pattern ?a|an|the * C is|was?. This pat-
tern yields a combined total of 51,045 values for
all 214 nouns; these values are primarily adjec-
tives, such as hot, black, etc., but noun-modifiers
of C are also allowed, such as fruit for cake. They
also harvest 8934 attribute nouns, such as temper-
ature and color, using the query pattern ?the * of
the C is|was?. These values and attributes are then
used as the basis of a clustering algorithm to parti-
tion the 214 nouns back into their original 13 cate-
gories. Comparing these clusters with the original
WordNet-based groupings, Almuhareb and Poesio
report a cluster accuracy of 71.96% using just val-
ues like hot (all 51,045), an accuracy of 64.02% us-
ing just attributes like temperature (all 8934), and
949
Table 1: Experiment 1, accuracy for 214 nouns
Approach Values Attr?s All
only only (V + A)
Almu. + Poesio 71.96% 64.02% 85.51%
(51045 (8934 (59979
vals) attr) v+a)
Talking Points 70.2% 78.7% 90.2%
(2209 (4974 (7183
vals) attr) v+a)
an accuracy of 85.5% using both together (59979
features).
In a second, larger experiment, Almuhareb and
Poesio select 402 nouns from 21 different seman-
tic classes in WordNet, and proceed to harvest
94,989 property values (again mostly adjectives)
and 24,178 attribute nouns from the web using
the same retrieval patterns. They then applied
the repeated bisections clustering algorithm to this
larger data set, and report an initial cluster purity
measure of 56.7% using only property values like
hot, 65.7% using only attributes like temperature,
and 67.7% using both together. Suspecting that
noisy features contribute to the perceived drop in
performance, those authors then applied a variety
of noise filters to reduce the value set to just 51,345
values and the attribute set to just 12,345 attributes,
for a size reduction of about 50% in each case.
This in turn leads to an improved cluster purity
measure of 62.7% using property values only and
70.9% using attributes only. Surprisingly, filtering
actually appears to reduce the clustering perfor-
mance of both data-sets used together, to 66.4%.
We replicate here both of these experiments us-
ing the same data-sets of 214 and 402 nouns re-
spectively. For fairness, we collect raw descrip-
tions for each of these nouns directly from the web,
and use no filtering (manual or otherwise) to re-
move poor or ill-formed descriptions. We thus use
the pattern ?as * as a|an|the C? to collect 2209 raw
adjectival values for the 214 nouns of experiment
1, and 5547 raw adjectival values for the 402 nouns
of experiment 2. We then use the pattern ?the ADJ
* of a|an|the C? to collect 4974 attributes for the
214 nouns of experiment 1, and 3952 for the 402
nouns of experiment 2; in each case, ADJ is bound
to the raw adjectival values that were acquired us-
ing ?as * as a|an|the C?. A comparison of cluster-
ing results is given in Tables 1 and 2. These tables
illustrate that clustering is most effective when it
Table 2: Experiment 2, accuracy for 402 nouns
Approach Values Attr?s All
only only (V + A)
Almu. + Poesio 56.7% 65.7% 67.7%
(no filtering) (94989 (24178 (119167
vals) attr) v+a)
Almu. + Poesio 62.7% 70.9% 66.4%
(with filtering) (51345 (12345 (63690
vals) attr) v+a)
Talking Points 64.3% 54.7% 69.85%
(5547 (3952 (9499
vals) attr) v+a)
is performed on the basis of both values and at-
tributes (yielding the highest scores, 90.2% and
69.85%, in each experiment respectively). These
results thus support the combination of conceptual
attributes with specific adjectival values into inte-
grated talking points which reflect how people ac-
tually talk about the concepts concerned.
6 Conclusions
Metaphor is a knowledge-hungry phenomenon, so
any computational treatment of metaphor will only
be as good as the knowledge representation that
supports it. Moreover, from a computational per-
spective, any theory of metaphor ? cognitive, lin-
guistic, or otherwise ? is only as good as the al-
gorithmic and representational insights that it pro-
vides, and the scale of the implementation that it
ultimately allows us to realize. In this paper we
have given computational form to some of the key
insights in the metaphor literature, from the in-
teraction theory of Black (1962) to the salience
imbalance theory of Ortony (1979) to the theory
of conceptual blending of Fauconnier and Turner
(1998). We also employ a key insight from the
work of Hofstadter and his fluid analogies group
(1995), that robust reasoning on a conceptual level
requires a degree of slippage that must be sup-
ported by the underlying knowledge representa-
tion.
Our knowledge base of talking points is derived
from two complementary information sources: the
objective definitions contained in WordNet (Fell-
baum, 1998) and the stereotypical comparisons
that pepper the texts of the web. These sources
yield a knowledge-base that is neither small nor
hand-crafted. While the knowledge-base needs
to grow by at least an order of magnitude, slip-
950
page means that non-identical talking points can
be treated as equivalent for purposes of robust
processing, which in turn extends the halo of
talking points that surrounds each concept in the
knowledge-base (Hofstadter et al, 1995). The
experiments of section 5 also indicate that, in a
pinch, new talking points for a previously under-
represented concept can be acquired dynamically
from the web with reasonable accuracy. As it
currently stands, the talking points approach to
metaphor is robust enough and scalable enough to
generate simple but imaginative metaphors on de-
mand for a wide range of user inputs.
But what does it mean to state, at a knowledge-
representation level, that lions and knights both
have a brave heart, that wolves and tyrants both
have a cruel face, or that eagles and warriors
have a fierce expression? Stereotypical talking
points such as these can be poetic or metaphor-
ical, and may express a viewpoint that is overly
simplistic, subjective or even technically inaccu-
rate. Nonetheless, our experiments suggest that
the linguistic insights we acquire from non-literal
descriptions strongly reflect our ontological intu-
itions about concepts and are more than mere lin-
guistic decorations. Most significantly, we see
from these experiments that stereotypical talking
points yield an especially concise representation,
since with no filtering of any kind, this approach
achieves comparable clustering results with feature
sets that are many times smaller than those used in
previous work. We anticipate therefore that stereo-
typical descriptions will be a key growth area for
the development of the talking points knowledge-
base.
The Pope examples of section 4.1. exem-
plify the competence of the system as it is cur-
rently implemented, while the Burqa and Gover-
nor/President examples of sections 1 and 2 mark
out our future directions. The Burqa examples
demonstrate the need for a more complex repre-
sentation of talking points that can accommodate
nested propositions, while the Governor example
demonstrates the need for more radical and ad-hoc
slippage patterns in creative metaphors. Rather
than add special rules to handle such individual
cases (which are creative because of their one-
off disposal nature), our ambition is to develop a
general corpus-grounded mechanism for explain-
ing all metaphor-related slippage. We remain a
considerable distance from this goal, yet believe
it is best attained using the kind of robust and scal-
able approach described here.
References
Almuhareb, A. and M. Poesio. 2004. Attribute-
Based and Value-Based Clustering: An Evaluation
In proceedings of EMNLP, the Conference on Em-
pirical Methods on Natural Language Processing.
Barcelona.
Almuhareb, A. and M. Poesio. 2005. Concept Learn-
ing and Categorization from the Web. In proceed-
ings of the annual meeting of the Cognitive Science
society. Italy.
Ahlswede, T. and M. Evans. 1998. Parsing vs. Text
Processing in the analysis of dictionary definitions.
In proceedings of the 26th Annual Meeting of the As-
sociation for Computational Linguistics, 217?224.
Barnden, J. A. and M. G. Lee. 2002. An Artificial
Intelligence Approach to Metaphor Understanding.
Theoria et Historia Scientiarum, 6(1):399?412.
Black, M. 1962. Models and Metaphor: studies in
language and philosophy. Ithaca, NY: Cornell Uni-
versity Press.
Camac, K. and S. Glucksberg. 1984. Metaphors
do not use associations between concepts, they are
used to create them. Journal of Psycholinguistic Re-
search,13(6).
Collins, A. and E. F. Loftus. 1975. A Spreading-
Activation Theory of Semantic Processing. Psycho-
logical Review 82, 407?428.
Fauconnier, G. and M. Turner. 1998. Conceptual In-
tegration Networks. Cognitive Science, 22(2):133?
187.
Fellbaum, C. (ed.). 1998. WordNet: An electronic lex-
ical database. The MIT Press. 1985 A comprehen-
sive grammar of the English.
Hofstadter, D. R. and M. Mitchell. 1994. The Copy-
cat Project: A model of mental fluidity and analogy-
making. In Holyoak, K.J. & Barnden, J. A. (Eds.)
Advances in Connectionist and Neural Computation
Theory, Vol. 2. Norwood, NJ: Ablex.
Hofstadter, D. R. and the Fluid Analogy Research
Group. 1995. Fluid Concepts and Creative Analo-
gies: Computer Models of the Fundamental Mecha-
nisms of Thought. NY: Basic Books.
Fass, D. 1991. Met*: a method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49?90.
Indurkhya, B. 1992. Metaphor and Cognition: Studies
in Cognitive Systems. Kluwer Academic Publishers,
Dordrecht: The Netherlands.
951
Martin, J. H. 1990. A Computational Model of
Metaphor Interpretation. NY: Academic Press.
Ortony, A. 1979. Beyond literal similarity. Psycholog-
ical Review, 86, 161?180.
Veale, T. and Y. Hao. 2007. Making WordNet Func-
tional and Context-Sensitive. In proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics. Czech Republic.
Veale, T. and O?Donoghue. 2000. Computation and
Blending. Cognitive Linguistics, 11(3?4), special is-
sue on Conceptual Blending.
Way, E. C. 1991. Knowledge Representation and
Metaphor. Studies in Cognitive systems. Holland:
Kluwer.
Wilks, Y. 1978. Making Preferences More Active,
Studies in Cognitive systems. Artificial Intelligence
11(3), 197?223.
952
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 835?842,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Growing Finely-Discriminating Taxonomies from Seeds
of Varying Quality and Size 
Tony Veale
School of Computer Science
University College Dublin
Ireland
tony.veale@ucd.ie
Guofu Li
School of Computer Science
University College Dublin
Ireland
guofu.li@ucd.ie
Yanfen Hao
School of Computer Science
University College Dublin
Ireland
yanfen.hao@ucd.ie
Abstract
Concept taxonomies offer a powerful means 
for organizing knowledge, but this organiza-
tion  must  allow  for  many  overlapping  and 
fine-grained perspectives if a general-purpose 
taxonomy is  to  reflect  concepts  as  they are 
actually employed and reasoned about in ev-
eryday  usage.  We present  here  a  means  of 
bootstrapping  finely-discriminating  tax-
onomies from a variety of different  starting 
points, or seeds, that are acquired from three 
different sources: WordNet, ConceptNet and 
the web at large.  
1 Introduction
Taxonomies  provide  a  natural  and  intuitive 
means of organizing information, from the bio-
logical taxonomies of the Linnaean system to the 
layout of supermarkets and bookstores to the or-
ganizational structure of companies. Taxonomies 
also provide the structural backbone for ontolo-
gies  in  computer  science,  from common-sense 
ontologies like Cyc (Lenat and Guha, 1990) and 
SUMO (Niles and Pease, 2001) to lexical ontolo-
gies like WordNet (Miller  et al, 1990). Each of 
these uses is based on the same root-branch-leaf 
metaphor:  the  broadest  terms  with  the  widest 
scope occupy the highest positions of a taxono-
my, near the root, while specific terms with the 
most local concerns are located lower in the hier-
archy,  nearest  the  leaves.  The  more  interior 
nodes that  a taxonomy possesses,  the finer  the 
conceptual distinctions and the more gradated the 
similarity judgments it can make (e.g., Budanit-
sky and Hirst, 2006).
General-purpose  computational  taxonomies 
are called upon to perform both coarse-grained 
and  fine-grained  judgments.  In  NLP,  for  in-
stance,  the  semantics  of  ?eat?  requires  just 
enough  knowledge  to  discriminate  foods  like 
tofu and cheese from non-foods like  wool  and 
steel, while specific applications in the domain of 
cooking  and  recipes  (e.g.,  Hammond?s  (1986) 
CHEF)  require  enough  discrimination  to  know 
that tofu can be replaced with clotted cheese in 
many recipes because each is a soft, white and 
bland food.  
So while much depends on the domain of us-
age, it remains an open question as to how many 
nodes a good taxonomy should possess. Prince-
ton WordNet,  for  instance,  strives for  as many 
nodes as there are word senses in English, yet it 
also contains a substantial number of composite 
nodes  that  are  lexicalized not  as  single  words, 
but as complex phrases. Print dictionaries intend-
ed for human consumption aim for some econo-
my of structure, and typically do not include the 
meaning  of  phrases  that  can  be  understood  as 
straightforward compositions of the meaning of 
their  parts  (Hanks,  2004).  But  WordNet  also 
serves another purpose, as a lexical knowledge-
base  for  computers,  not  humans,  a  context  in 
which concerns about space seem quaint. When 
space is not a issue, there seems no good reason 
to exclude nodes from a concept taxonomy mere-
ly for being composites of other ideas; the real 
test of entry is whether a given node adds value 
to a taxonomy, by increasing its level of internal 
organization through the systematic dissection of 
overly broad categories into finer, more intuitive 
and manageable clusters.
In this paper we describe a means by which 
finely-discriminating  taxonomies  can  be  grown 
from  a  variety  of  different  knowledge  seeds. 
These taxonomies comprise composite categories 
that  can  be  lexicalized  as  phrases  of  the  form 
?ADJ NOUN?, such as Sharp-Instrument, which 
represents the set of all instruments that are typi-
cally considered sharp, such as knives, scissors, 
chisels and can-openers. While WordNet aleady 
contains  an  equivalent  category,  named  Edge-
835
Tool, which it defines with the gloss ?any cutting 
tool  with a sharp cutting edge?,  it  provides  no 
structural basis for inferring that any member of 
this  category can be considered  sharp.  For  the 
most part, if two ideas (word senses) belong to 
the same semantic category X in WordNet, the 
most we can infer is that both possess the trivial 
property  X-ness.  Our  goal  here  is  to  construct 
taxonomies whose form makes explicit the actual 
properties that accrue from membership in a cat-
egory. 
Past work on related approaches to taxonomy 
creation are discussed in section 2, while section 
3  describes  the  different  knowledge  seeds  that 
serve as the starting point for our bootstrapping 
process. In section 4 we describe the bootstrap-
ping process in more detail;  such processes are 
prone to noise, so we also discuss how the ac-
quired categorizations are validated and filtered 
after each bootstrapping cycle. An evaluation of 
the key ideas is then presented in section 5, to 
determine which seed yields the highest quality 
taxonomy once bootstrapping is completed. The 
paper then concludes with some final remarks in 
section 6.
2 Related Work
Simple pattern-matching techniques can be sur-
prisingly effective for the extraction of lexico-se-
mantic relations from text when those relations 
are expressed using relatively stable and unam-
biguous  syntagmatic  patterns  (Ahlswede  and 
Evens, 1988). For instance, the work of Hearst 
(1992) typifies this surgical approach to relation 
extraction,  in  which a system fishes in  a large 
text for particular word sequences that strongly 
suggest  a  semantic  relationship  such  as  hyper-
nymy  or,  in  the  case  of  Charniak and Berland 
(1999), the part-whole relation. Such efforts offer 
high precision but can exhibit low recall on mod-
erate-sized corpora, and extract just a tiny (but 
very useful) subset of the semantic content of a 
text.  The  KnowItAll system  of  Etzioni  et  al. 
(2004)  employs  the  same  generic  patterns  as 
Hearst (e.g., ?NPs such as NP1, NP2, ??),  and 
more besides, to extract a whole range of facts 
that can be exploited for web-based question-an-
swering.  Cimiano  and  Wenderoth  (2007)  also 
use a range of Hearst-like patterns to find text se-
quences in web-text that are indicative of the lex-
ico-semantic  properties  of  words;  in  particular, 
these  authors  use  phrases  like  ?to  *  a  new 
NOUN? and ?the purpose of NOUN is to *? to 
identify the formal (isa), agentive (made by) and 
telic (used for) roles of nouns.
Snow, Jurafsky and Ng (2004) use supervised 
learning techniques to acquire those syntagmatic 
patterns that prove most useful for extracting hy-
pernym relations from text. They train their sys-
tem using pairs of WordNet terms that exemplify 
the hypernym relation; these are used to identify 
specific sentences in corpora that are most likely 
to express the relation in lexical terms. A binary 
classifier is then trained on lexico-syntactic fea-
tures that are extracted from a dependency-struc-
ture  parse  of  these  sentences.  Kashyap  et  al., 
(2005) experiment with a bootstrapping approach 
to  growing concept  taxonomies  in  the  medical 
domain.  A  gold  standard  taxonomy  provides 
terms that are used to retrieve documents which 
are  then  hierarchically  clustered;  cohesiveness 
measures are used to yield a taxonomy of terms 
that can then further drive the retrieval and clus-
tering cycle. Kozareva  et al (2008) use a boot-
strapping approach that extends the fixed-pattern 
approach  of  Hearst  (1992)  in  two  intriguing 
ways. First, they use a doubly-anchored retrieval 
pattern of the form ?NOUNcat such as NOUNexam-
ple and  *?  to  ground the  retrieval  relative  to  a 
known example of hypernymy,  so that any val-
ues extracted for the wildcard * are likely to be 
coordinate terms of  NOUNexample and even more 
likely to be good examples of NOUNcat. Second-
ly, they construct a graph of terms that co-occur 
within this pattern to determine which terms are 
supported by others,  and by how much.  These 
authors also use two kinds of bootstrapping: the 
first  variation,  dubbed  reckless,  uses the candi-
dates extracted from the double-anchored pattern 
(via *) as exemplars (NOUNexample) for successive 
retrieval cycles; the second variation first checks 
whether a candidate is sufficiently supported to 
be used as an exemplar in future retrieval cycles.
The approach we describe here is most similar 
to that of Kozareva  et al (2008). We too use a 
double-anchored pattern, but place the anchors in 
different  places  to  obtain  the  query  patterns 
?ADJcat NOUNcat such as *? and ?ADJcat * such 
as NOUNexample?. As a result, we obtain a finely-
discriminating  taxonomy  based  on  categories 
that are explicitly annotated with the properties 
(ADJcat)  that  they  bequeath  to  their  members. 
These categories have an obvious descriptive and 
organizational  utility,  but  of  a kind that  one is 
unlikely  to  find  in  conventional  resources  like 
WordNet and Wikipedia. Kozareva et al (2008) 
test their approach on relatively simple and ob-
jective  categories  like  states,  countries (both 
836
closed sets), singers and fish (both open, the for-
mer more so than the latter), but not on complex 
categories in which members are tied both to a 
general category, like food, and to a stereotypical 
property, like  sweet (Veale and Hao, 2007). By 
validating  membership  in  these  complex  cate-
gories  using WordNet-based heuristics,  we  can 
hang these categories and members  on specific 
WordNet senses, and thus enrich WordNet with 
this additional taxonomic structure.
3 Seeds for Taxonomic Growth 
A fine-grained taxonomy can be viewed as a set 
of triples Tijk = <Ci, Dj, Pk>, where Ci denotes a child of the parent term Pk that possesses the dis-
criminating  property  Dj;  in  effect,  each  such 
triple expresses that Ci is a specialization of the 
complex  taxonym  Dj-Pk.  Thus,  the  belief  that 
cola  is  a  carbonated-drink  is  expressed  by the 
triple <cola, carbonated, drink>. From this triple 
we  can  identify  other  categorizations  of  cola 
(such as treat and refreshment) via the web query 
?carbonated * such as cola?, or we can identify 
other similarly fizzy drinks via the query ?car-
bonated  drinks  such  as  *?.  So  this  web-based 
bootstrapping  of  fine-grained  category  hierar-
chies requires that we already possess a collec-
tion  of  fine-grained  distinctions  of  a  relatively 
high-quality.  We  now  consider  three  different 
starting points for this bootstrapping process, as 
extracted from three different resources:  Word-
Net, ConceptNet and the web at large.
3.1 WordNet 
The noun-sense taxonomy of WordNet makes a 
number  of  fine-grained  distinctions  that  prove 
useful in clustering entities into smaller and more 
natural groupings. For instance, WordNet differ-
entiates  {feline,  felid} into  the  sub-categories 
{true_cat,  cat} and  {big_cat,  cat},  the  former 
serving  to  group  domesticated  cats  with  other 
cats of a similar size, the latter serving to cluster 
cats  that  are  larger,  wilder  and  more  exotic. 
However, such fine-grained distinctions are the 
exception rather than the norm in WordNet, and 
not  one of  the  60+ words  of  the  form  Xess in 
WordNet that denote a person (such as huntress,  
waitress, Jewess, etc.) express the defining prop-
erty  female in  explicit  taxonomic  terms. 
Nonetheless, the free-text glosses associated with 
WordNet sense-entries often do state the kind of 
distinctions we would wish to find expressed as 
explicit  taxonyms.  A  shallow  parse  of  these 
glosses  thus  yields  a  sizable  number  of  fine-
grained  distinctions,  such  as  <lioness,  female,  
lion>,   <espresso,  strong,  coffee>  and  both 
<messiah, awaited, king> and <messiah, expect-
ed, deliverer>. 
3.2 ConceptNet 
Despite  its  taxonomic  organization,  WordNet 
owes much to the centralized and authority-pre-
serving  craft  of  traditional  lexicography.  Con-
ceptNet (Liu and Singh, 2004), in contrast, is a 
far less authoritative knowledge-source, one that 
owes more to the workings of the WWW than to 
conventional print dictionaries. Comprising fac-
toids culled from the template-structured contri-
butions of thousands of web users,  ConceptNet 
expresses many relationships that accurately re-
flect  a  public,  common-sense  view on a  given 
topic (from vampires to dentists) and many more 
that are simply bizarre or ill-formed. Looking to 
the relation that interests us here, the IsA rela-
tion,  ConceptNet  tells  us  that  an  espresso is  a 
strong coffee (correctly, like WordNet) but that a 
bagel is a Jewish word (confusing use with men-
tion). Likewise, we find that expressionism is an 
artistic style (correct, though WordNet deems it 
an  artistic movement) but that an  explosion is a 
suicide attack (confusing formal and telic roles). 
Since we cannot trust the content of ConceptNet 
directly, lest we bootstrap from a highly unreli-
able starting point, we use WordNet as a simple 
filter.  While  the  concise  form  of  ConceptNet 
contains over 30,000 IsA propositions, we con-
sider as our seed collection only those that define 
a noun concept (such as ?espresso?) in terms of a 
binary  compound  (e.g.,  ?strong coffee?)  where 
the head of the latter (e.g.,  ?coffee?) denotes a 
WordNet hypernym of some sense of the former. 
This  yields  triples  such  as  <Wyoming,  great,  
state>,  <wreck,  serious,  accident>  and  <wolf,  
wild, animal>.
3.3 Web-derived Stereotypes 
Veale and Hao (2007) also use the observations 
of web-users to acquire common perceptions of 
oft-mentioned ideas, but do so by harvesting sim-
ile expressions of the form ?as ADJ as a NOUN? 
directly from the web.  Their approach hinges on 
the fact that similes exploit stereotypes to draw 
out the salient properties of a target, thereby al-
lowing rich  descriptions of those stereotypes to 
be easily acquired, e.g., that snowflakes are pure 
and unique, acrobats are agile and nimble, knifes 
are  sharp and dangerous,  viruses  are  malicious 
and infectious, and so on. However, because they 
find that almost 15% of their web-harvested sim-
837
iles are ironic (e.g., ?as subtle as a rock?, ?as bul-
letproof as a sponge-cake?, etc.), they filter irony 
from these associations by hand, to yield a siz-
able  database  of  stereotypical  attributions  that 
describes over 6000 noun concepts in terms of 
over  2000  adjectival  properties.  However,  be-
cause Veale and Hao?s data directly maps stereo-
typical properties to simile vehicles, it does not 
provide  a  parent  category  for  these  vehicles. 
Thus, the seed triples derived from this data are 
only partially instantiated;  for  instance,  we ob-
tain <surgeon, skilful, ?>, <virus, malicious, ?> 
and <dog, loyal, ?>.  This does not prove to be a 
serious  impediment,  however,  as  the  missing 
field  of  each triple  is  quickly identified during 
the first cycle of bootstrapping.
3.4 Overview of Seed Resources 
Neither of these three seeds is an entirely useful 
knowledge-base in its own right. The WordNet-
based seed is clearly a representation of conve-
nience,  since  it  contains  only  those  properties 
that can be acquired from the glosses that happen 
to be amenable  to a simple  shallow-parse.  The 
ConceptNet seed is likewise a small collection of 
low-hanging fruit, made smaller still by the use 
of WordNet as a coarse but very necessary noise-
filter.  And while the simile-derived distinctions 
obtained from Veale and Hao paint a richly de-
tailed  picture  of  the  most  frequent  objects  of 
comparison, this seed offers no coverage for the 
majority of concepts that are insufficiently note-
worthy to be found in web similes. A quantita-
tive comparison of all three seeds is provided in 
Table 1 below.
WordNet ConceptNet Simile
# terms 
in total 12,227 1,133 6512
# triples 
in total 51,314 1808 16,688
# triples 
per term 4.12 1.6 2.56
# fea-
tures 2305 550 1172
Table 1:  The size of seed collections yielded from 
different sources. 
We can see that WordNet-derived seed is clearly 
the largest and apparently the most comprehen-
sive knowledge-source of the  three:  it  contains 
the most terms (concepts), the most features (dis-
criminating properties of those concepts), and the 
most triples (which situate those concepts in par-
ent  categories  that  are  further  specialized  by 
these  discriminating  features).  But  size  is  only 
weakly suggestive of quality, and as we shall see 
in  the  next  section,  even such  dramatic  differ-
ences in scale can disappear after several cycles 
of bootstrapping. In section 5 we will then con-
sider  which  of  these  seeds  yields  the  highest 
quality taxonomies after bootstrapping has been 
applied. 
4 Bootstrapping from Seeds
The seeds of the previous section each represent 
a different starting collection of triples. It is the 
goal of the bootstrapping process to grow these 
collections  of  triples,  to  capture  more  of  the 
terms ? and more of the distinctions ? that a tax-
onomy is expected to know about. The expansion 
set  of  a  triple  Tijk =  <Ci,  Dj,  Pk> is  the  set  of 
triples that can be acquired from the web using 
the  following  query  expansions  (*  is  a  search 
wildcard):
1. ?Dj * such as Ci?
2. ?Dj Pk such as *?
In the first query, a noun is sought to yield anoth-
er categorization of Ci, while in the second, other 
members of the fine-grained category Dj-Pk are 
sought to accompany Ci. In parsing the text snip-
pets  returned by these  queries,  we also exploit 
text sequences that match the following patterns:
3. ?* and Dj Pk such as *?
4. ?* and Dj * such as Ci?
These last two patterns allow us to learn new dis-
criminating  features  by  noting  how  these  dis-
criminators are combined to reinforce each other 
in  some  ad-hoc  category  formulations.  For  in-
stance, the phrase ?cold and refreshing beverages 
such  as  lemonade?  allows  us  to  acquire  the 
triples <lemonade, cold, beverage> and <lemon-
ade, refreshing, beverage>. This pattern is neces-
sary if the bootstrapping process is to expand be-
yond  the  limited  vocabulary  of  discriminating 
features  (Dj)  found in  the  original  seed collec-
tions of triples.
We denote the mapping from a triple T to the 
set of additional triples that can be acquired from 
the web using the above queries/patterns as  ex-
pand(T').  We currently implement this function 
using  the  Google  search  API.  Our  experiences 
with each query suggest  that  200 snippets is  a 
good search range for the first query, while 50 is 
usually more than adequate for the second. 
838
We can now denote the knowledge that is ac-
quired when starting from a given seed collection 
S after t cycles of bootstrapping as KtS. Thus, 
K 0S=S
K 1S=K 0S ?
{T ? T '?S ? T?expand ?T ' ?}
K t?1S =K tS ?
{T ? T '?K tS ? T?expand ?T ' ?}
Web queries, and the small snippets of text that 
they return, offer just a keyhole view of language 
as it is used in real documents.  Unsurprisingly, 
the  new triples  acquired from the  web via  ex-
pand(T') are likely to be very noisy indeed. Fol-
lowing Kozareva et al (2008), we can either in-
dulge  in  reckless  bootstrapping,  which  ignores 
the  question  of  noise  until  all  bootstrapping  is 
finished, or we can apply a noise filter after each 
incremental step.  The latter approach has the ad-
ditional advantage of keeping the search-space as 
small as possible, which is a major consideration 
when bootstrapping from sizable seeds. We use a 
simple WordNet-based filter called near-miss:  a 
new triple <Ci,  Dj,  Pk> is accepted if WordNet 
contains  a  sense  of  Ci that  is  a  descendant  of 
some sense of Pk (a hit), or a sense of Ci that is a 
descendant of the direct hypernym of some sense 
of Pk (a near-miss). This allows the bootstrapping 
process to acquire structures that are not simply a 
decorated version of the basic WordNet taxono-
my,  but  to acquire hierarchical  relations whose 
undifferentiated forms are not in WordNet (yet 
are largely compatible with WordNet). This non-
reckless bootstrapping process can be expressed 
as follows:
K t?1S =K tS ? {T ? T '?K tS ?
T? filter near?miss?expand ?T ' ??}
Figure 1 and figure 2 below illustrate the rate of 
growth  of  triple-sets  from  each  of  our  three 
seeds.
Referring again to table 1, we note that while 
the ConceptNet collection is by far the smallest 
of  the three seeds ? more  that  7 times smaller 
than the simile-derived seed, and almost 40 times 
smaller than the WordNet seed ? this difference 
is  size  shrinks  considerably over  the  course  of 
five  bootstrapping  cycles.  The  WordNet  near-
miss filter ensures that the large body of triples 
grown from each  seed  are  broadly  sound,  and 
that  we  are  not  simply  generating  comparable 
quantities of nonsense in each case.
Figure 1: Growth in the number of acquired triples, 
over 5 cycles of bootstrapping from different seeds.
Figure 2: Growth in the number of terms described by 
the acquired triples, over 5 cycles of bootstrapping 
from different seeds.
4.1 An  Example
Consider cola, for which the simile seed has one 
triple: <cola, refreshing, beverage>. After a sin-
gle cycle of bootstrapping, we find that cola can 
now be described as an effervescent beverage, a 
sweet  beverage,  a  nonalcoholic  beverage and 
more. After a second cycle, we find it described 
as a sugary food, a fizzy drink and a dark mixer. 
After a third cycle, it is found to be a  sensitive 
beverage, an  everyday beverage and a  common 
drink. After a fourth cycle, it is also found to be 
an  irritating food and an  unhealthy drink. After 
the  fifth  cycle,  it  is  found to  be  a  stimulating 
drink, a toxic food and a corrosive substance. In 
all, the single cola triple in the simile seed yields 
14 triples after 1 cycle, 43 triples after 2 cycles, 
72 after 3 cycles, 93 after 4 cycles, and 102 after 
5 cycles. During these bootstrapping cycles, the 
description  refreshing beverage additionally be-
comes  associated  with  the  terms  champagne, 
lemonade and beer. 
0 1 2 3 4 5
0
200000
400000
600000
800000
1000000
1200000
1400000
1600000
1800000 WordNet
Simile
ConceptNet
Bootstrapping Cycle
# 
Tri
ple
s
0 1 2 3 4 5
0
50000
100000
150000
200000
250000
300000
350000
WordNet
Simile
ConceptNet
Bootstrapping Cycle
# 
Te
rm
s
839
5 Empirical Evaluation
The WordNet  near-miss filter thus ensures that 
the parent field (Pk) of every triple contains a val-
ue  that  is  sensible  for  the  given  child  concept 
(Ci), but does not ensure that the discriminating 
property  (Dj)  in  each  triple  is  equally  sensible 
and apropos.  To see  whether the bootstrapping 
process  is  simply  padding  the  seed  taxonomy 
with large quantities of noise,  or whether the ac-
quired Dj values do indeed mark out the implicit 
essence of the Ci terms they describe, we need an 
evaluation framework that can quantify the onto-
logical usefulness of these Dj values. For this, we 
use  the  experimental  setup  of  Almuhareb  and 
Poesio  (2005),  who  use  information  extraction 
from the web to acquire attribute values for dif-
ferent terms/concepts, and who then compare the 
taxonomy that can be induced by clustering these 
values  with the  taxonomic  backbone  of  Word-
Net. 
Almuhareb and Poesio first created a balanced 
set  of  402  nouns  from  21  different  semantic 
classes in WordNet. They then acquired attested 
attribute values for these nouns (such as  hot for 
coffee,  red for car, etc.) using the query "(a|an|
the) * Ci  (is|was)" to find corresponding Dj val-
ues for each Ci. Unlike our work, these authors 
did  not seek to acquire hypernyms  for each Ci 
during this search, and did not try to link the ac-
quired attribute values to a particular branching 
point  (Pk) in the taxonomy (they did,  however, 
seek matching attributes for these values, such as 
Temperature for  hot, but that aspect is not rele-
vant here). They acquired 94,989 attribute values 
in all for the 402 test nouns. These values were 
then used as features of the corresponding nouns 
in  a  clustering  experiment,  using  the  CLUTO 
system of Karypis (2002). By using attribute val-
ues  as  a  basis  for  partitioning  the  set  of  402 
nouns  into  21  different  categories,  Almuhareb 
and Poesio attempted to reconstruct the original 
21  WordNet  categories  from which  the  nouns 
were drawn. The more accurate the match to the 
original WordNet clustering, the more these at-
tribute values can be seen (and used) as a repre-
sentation of conceptual structure. In their first at-
tempt, they achieved just a 56.7% clustering ac-
curacy against the original human-assigned cate-
gories of WordNet. But after using a noise-filter 
to remove almost  half of the web-harvested at-
tribute values, they achieve a higher cluster accu-
racy of 62.7%. More specifically, Poesio and Al-
muhareb achieve a cluster purity of 0.627 and a 
cluster entropy of 0.338 using 51,345 features to 
describe and cluster the 402 nouns.1
We?replicate?the?above?experiments?using?the?
same?402?nouns,?and?assess?the?clustering?accur?
acy ? (again ?using ?WordNet ? as ? a ? gold?standard)?
after?each?bootstrapping?cycle.?Recall?that?we?use?
only?the?Dj?fields?of?each?triple?as?features?for?the?
clustering ?process, ? so ? the ?comparison ?with ? the?
WordNet?gold?standard?is?still?a?fair?one.?Once?
again,?the?goal?is?to?determine?how?much?like?the?
human?crafted ? WordNet ? taxonomy ? is ? the ? tax?
onomy? that ? is ?clustered?automatically ?from?the?
discriminating?words?Dj?only.?The?clustering?ac?
curacy?for?all?three?seeds?are?shown?in?Tables?2,?
3?and?4.
Cycle  E  P # Features Coverage
1st .327 .629 907 66%
2nd .253 .712 1,482 77%
3rd .272 .717 2,114 82%
4th .312 .640 2,473 83%
5th .289 .684 2,752 83%
Table 2: Clustering accuracy using the WordNet seed 
collection (E denotes Entropy and P stands for Purity)
Cycle E P # Features Coverage
1st .115 .842 363 41%
2nd .255 .724 787 59%
3rd .286 .694 1,362 74%
4th .279 .694 1,853 79%
5th .299 .673 2,274 82%
Table 3: Clustering accuracy using the ConceptNet 
seed collection
Cycle E P # Features Coverage
1st .254 .716 837 59%
2nd .280 .712 1,338 73%
3rd .289 .693 1,944 79%
4th .313 .660 2,312 82%
5th .157 .843 2,614 82%
Table 4: Clustering accuracy using the Simile seed 
collection
The test-set of 402 nouns contains some low-fre-
quency words, such as casuarina,  cinchona,  do-
decahedron, and  concavity, and Almuhareb and 
1 We use cluster purity as a reflection of clustering accu-
racy. We express accuracy as a percentage; hence a pu-
rity of 0.627 is seen as an accuracy of 62.7%. 
840
Poesio note that one third of their data-set has a 
low-frequency of between 5-100 occurrences in 
the British National Corpus. Looking to the cov-
erage  column  of  each  table,  we  thus  see  that 
there  are  words  in  the  Poesio  and  Almuhareb 
data set for which no triples can be acquired in 5 
cycles  of  bootstrapping.  Interestingly,  though 
each seed is quite different in origin and size (see 
again Table 1), all reach similar levels of cover-
age (~82%) after  5 bootstrapping cycles.   Test 
nouns for which all three seeds fail to reach a de-
scription  include  yesteryear,  nonce (very rare), 
salient (more typically an adjective), jag, droop,  
fluting,  fete,  throb,  poundage,  stinging,  rouble,  
rupee,  riel,  drachma,  escudo,  dinar,  dirham,  
lira, dispensation,  hoard,  airstream (not typical-
ly a solid compound), riverside and curling. Fig-
ures 3 and 4 summarize the key findings in the 
above tables: while bootstrapping from all three 
seeds converges to the same level of coverage, 
the simile seed clearly produces the highest qual-
ity taxonomy. 
Figure 3: Growth in the coverage from different 
seed sources. 
Figure 4: Divergence in the clustering Purity 
achieved using different seed sources. The results of 
Poesio and Almuhareb are shown as the straight line: 
y = 0.627.
Both  the  WordNet  and  ConceptNet  seeds 
achieve comparable accuracies of 68% and 67% 
respectively  after  5  cycles  of  bootstrapping, 
which compares well with the accuracy of 62.7% 
achieved  by  Poesio  and  Almuhareb.  However, 
the simile seed clearly yields the best accuracy of 
84.3%,  which  also  exceeds  the  accuracy  of 
66.4% achieved by Poesio and Almuhareb when 
using both values  and attributes (such as  Tem-
perature, Color, etc.) for clustering, or the accu-
racy of 70.9% they achieve when using attributes 
alone. Furthermore, bootstrapping from the simi-
le seed yields higher cluster accuracy on the 402-
noun data-set than Veale and Hao (2008) them-
selves achieve with their simile data on the same 
test-set (69.85%). 
But most striking of all is the concision of the 
representations that are acquired using bootstrap-
ping. The simile seed yields a high cluster accu-
racy using a pool of just 2,614 fine discrimina-
tors,  while  Poesio  and  Almuhareb  use  51,345 
features even after their feature-set has been fil-
tered  for  noise.  Though starting  from different 
initial scales, each seed converges toward a fea-
ture-set that is roughly twenty times smaller than 
that used by Poesio and Almuhareb. 
6 Conclusions
These experiments reveal that seed knowledge of 
different authoritativeness, quality and size will 
tend to converge toward roughly the same num-
ber  of  finely  discriminating  properties  and  to-
ward much the same coverage after 5 or so cy-
cles of bootstrapping. Nonetheless, quality wins 
out,  and  the  simile-derived  seed  knowledge 
shows itself to be a clearly superior basis for rea-
soning  about  the  structure  and  organization  of 
conceptual  categories.  Bootstrapping  from  the 
simile  seed yields  a slightly smaller  set of  dis-
criminating features than bootstrapping from the 
WordNet  seed,  one that  is  many times  smaller 
than the Poesio and Almuhareb feature set. What 
matters is that they are the right features to dis-
criminate with. 
There appears to be a number of reasons for 
this  significant  difference  in  quality.  For  one, 
Veale and Hao (2007) show that similes express 
highly  stereotypical  beliefs  that  strongly  influ-
ence the affective disposition of a term/concept; 
negatively  perceived  concepts  are  commonly 
used to exemplify negative properties in similes, 
while  positively perceived  concepts  are  widely 
used to exemplify positive properties. Veale and 
Hao (2008) go on to argue that similes offer a 
very concise snapshot of those widely-held be-
liefs that are the cornerstone of everyday reason-
1 2 3 4 5
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
WordNet
Simile
ConceptNet
Bootstrapping Cycle
Co
ve
rag
e
1 2 3 4 5
0.40
0.50
0.60
0.70
0.80
0.90
1.00
WordNet
Simile
ConceptNet
Poesio & Alm.
Bootstrapping Cycle
Pu
rity
841
ing, and which should thus be the corner-stone of 
any general-purpose taxonomy.  In addition, be-
liefs expressed via the ?as Dj as Ci? form of simi-
les  appear  to  lend  themselves  to  re-expression 
via the ?Dj Pk such as  Ci? form; in each case, a 
concept Ci is held up as an exemplar of a salient 
property  Dj.  Since  the  ?such  as?  bootstrapping 
pattern seeks out  expressions of  prototypicality 
on the web, a simile-derived seed set is likely the 
best starting point for this search.
All three seeds appear to suffer the same cov-
erage limitations,  topping out  at  about  82% of 
the words in the Poesio and Almuhareb data-set. 
Indeed,  after  5  bootstrapping  cycles,  all  three 
seeds give rise to taxonomies that overlap on 328 
words from the 402-noun test-set, accounting for 
81.59% of the test-set. In effect then, bootstrap-
ping stumbles over the same core of hard words 
in each case, no matter the seed that is used. As 
such, the problem of coverage lies not in the seed 
collection, but in the queries used to perform the 
bootstrapping.  The  same  coverage  limitations 
will thus apply to other bootstrapping approaches 
to  knowledge acquisition,  such as  Kozareva  et  
al. (2008), which rely on much the same stock 
patterns.  So  while  bootstrapping may not  be  a 
general  solution  for  acquiring  all  aspects  of  a 
general-purpose taxonomy, it is clearly useful in 
acquiring large swathes  of  such a  taxonomy if 
given  a  sufficiently  high-quality  seed  to  start 
from.
References
Ahlswede, T. and Evans, M. (1988). Parsing vs. Text 
Processing in the analysis of dictionary definitions. 
In Proc. of the 26th Annual Meeting of the ACL, pp 
217-224.
Almuhareb,  A.  and  Poesio,  M.  (2005).  Concept 
Learning  and  Categorization  from  the  Web.  In 
Proc. of the annual meeting of the Cognitive  Sci-
ence Society, Italy, July. 
Budanitsky,  A.  and  Hirst,  G. (2006).   Evaluating 
WordNet-based Measures of Lexical Semantic Re-
latedness. Computational Linguistics, 32(1):13-47.
Cimiano, P. and Wenderoth, J. (2007). Automatic Ac-
quisition  of  Ranked  Qualia  Structures  from  the 
Web.  In Proc. of  the 45th Annual Meeting of  the  
ACL, pp 888-895.
Charniak, E. and Berland, M. (1999). Finding parts in 
very  large  corpora.  In  Proc.  of  the  37th Annual  
Meeting of the ACL, pp 57?64.
Etzioni,  O.,  Kok,  S.,  Soderland,  S.,  Cafarella,  M., 
Popescu, A-M., Weld, D., Downey, D., Shaked, T. 
and Yates,  A. (2004).  Web-scale information ex-
traction  in  KnowItAll  (preliminary  results).  In  
Proc. of the 13th WWW Conference, pp 100?109.
Hammond, K. J. (1986). CHEF : A Model of Case--
based Planning.  In Proc. of the 5th National Con-
ference  on  Artificial  Intelligence,  pp  267--271, 
Philadelphia, Pennsylvania.  American Association 
for Artificial Intelligence. 
Hanks, P. (2004). WordNet: What is to be done? In 
Proc. of GWC?2004, the 2nd Global WordNet con-
ference, Masaryk University, Brno.
Hearst,  M. (1992).  Automatic  acquisition  of  hy-
ponyms  from large  text  corpora.  In  Proc.  of  the 
14th Int.  Conf.  on  Computational  Linguistics,  pp 
539?545.
Kashyap,  V.  Ramakrishnan,  C.  and  Sheth,  T.  A. 
(2005). TaxaMiner: an experimentation framework 
for automated taxonomy bootstrapping.  Int. Jour-
nal of Web and Grid Services 1(2), pp 240-266.
Karypis,  G. (2002).  CLUTO:  A  clustering  toolkit. 
Technical Report 02-017, University of Minnesota. 
http://www-users.cs.umn.edu/~karypis/cluto/.
Kozareva, Z., Riloff, E. and Hovy, E. (2008). Seman-
tic  Class  Learning from the Web with Hyponym 
Pattern Linkage Graphs. In Proc. of the 46th Annu-
al Meeting of the ACL.
Lenat, D. B. and Guha, R. V. (1990). Building large 
knowledge-based  systems:  representation  and  in-
ference in the Cyc project. NY: Addison-Wesley.
Liu, H. and Singh, P. (2004), ConceptNet: A Practical 
Commonsense Reasoning Toolkit.  BT Technology 
Journal, 22(4):211-226.
Miller, G., Beckwith,R., Fellbaum, C., Gross, D. and 
Miller,  K.J.  (1990).  Introduction  to  WordNet:  an 
on-line lexical database. Int. Journal of Lexicogra-
phy, 3(4):235 ? 244.
Niles, I. and Pease, A. (2001). Toward a standard up-
per ontology. In Proc. of the 2nd International Con-
ference  on Formal  Ontology  in  Information  Sys-
tems (FOIS-2001).
Snow, R., Jurafsky, D. and Ng, A. Y. (2004). Learn-
ing syntactic patterns for automatic hypernym dis-
covery.  Advances  in Neural Information Process-
ing Systems 17.
Veale,  T.  and Hao,  Y. (2007).  Making Lexical  On-
tologies Functional and Context-Sensitive. In Proc.  
of the 45th Annual Meeting of the ACL, pp 57?64.
Veale, T. and  Hao, Y. (2008).  A Fluid Knowledge 
Representation for  Understanding and Generating 
Creative Metaphors.  In Proc. of Coling 2008, The  
22nd International  Conference  on  Computational  
Linguistics, Manchester.
842
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 326 ? 333, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Analogy as Functional Recategorization: Abstraction 
with HowNet Semantics 
Tony Veale 
Department of Computer Science, University College Dublin, 
Belfield, D4, Dublin, Ireland 
Tony.Veale@UCD.ie 
http://www.cs.ucd.ie/staff/tveale.html 
Abstract. One generally accepted hallmark of creative thinking is an ability to 
look beyond conventional labels and recategorize a concept based on its behav-
iour and functional potential. So while taxonomies are useful in any domain of 
reasoning, they typically represent the conventional label set that creative think-
ing attempts to look beyond. So if a linguistic taxonomy like WordNet [1] is to 
be useful in driving linguistic creativity, it must support some basis for recate-
gorization, to allow an agent to reorganize its category structures in a way that 
unlocks the functional potential of objects, or that recognizes similarity between 
literally dissimilar ideas. In this paper we consider how recategorization can be 
used to generate analogies using the HowNet [2] ontology, a lexical resource 
like WordNet that in addition to being bilingual (Chinese/English) also provides 
explicit semantic definitions for each of the terms that it defines. 
1   Introduction 
Analogy is a knowledge-hungry process that exploits a conceptual system?s ability to 
perform controlled generalization in one domain and re-specialization into another. 
The result is a taxonomic leap within an ontology that transfers semantic content from 
one term onto another. While all taxonomies allow vertical movement, a system must 
fully understand the effects of generalization on a given concept before any analogy 
or metaphor can be considered either deliberate or meaningful. So to properly support 
analogy, a taxonomy must provide a basis of abstracting not just to conventional cate-
gories, like Person, Animal or Tool, but to categories representing the specific causal 
behaviour of concepts such as think-agent, pain-experiencer, cutting-instrument, and 
so on. Thus, a surgeon can be meaningfully described as a repairman since both occu-
pations have the function of restoring an object to an earlier and better state; a foot-
baller can be meaningfully described as a gladiator or a warrior since each exhibits 
competitive behaviour; and a scalpel can be compared to a sabre, a sword or a cleaver 
since each has a cutting behaviour; and so on. 
Theories of metaphor and analogy are typically based either on structure-mapping 
[3,4] or on abstraction e.g., [5,6,7,8,9,10]). While the former is most associated with 
analogy, the latter has been a near-constant in the computational treatment of meta-
phor. Structure-mapping assumes that the causal behaviour of a concept is expressed 
in an explicit, graph-theoretic form so that unifying sub-graph isomorphisms can be 
 Analogy as Functional Recategorization: Abstraction with HowNet Semantics 327 
found between different representations. In contrast, abstraction theories assume that 
analogous concepts, even when far removed in ontological terms, will nonetheless 
share a common hypernym that captures their causal similarity. Thus, we should ex-
pect an analogous pairing like surgeon and butcher to have different immediate hy-
pernyms but to ultimately share an abstraction like cutting-agent (see [8,9]).  
However, the idea that a standard ontology will actually provide a hypernym like 
cutting-agent seems convenient almost to the point of incredulity. The problem is, of 
course, that as much as we want our ontologies to anticipate future analogies and 
metaphors with these pro-active categorizations, most ontologies simply do not pos-
sess terms as prescient as these. This is the question we address in this paper: if we 
assume that our ontologies lack these structures, can we nonetheless enable them to 
be added via automatic means?  We argue that we can, by generalizing not on the 
basis of a concept?s taxonomic position but on the basis of the specific relations that 
define its causal behaviour.  
Clearly then, this approach to analogy requires a resource that is rich in causal rela-
tions. We find this richness in HowNet [2, 11], a bilingual lexical ontology for Chi-
nese and English that employs an explicit propositional semantics to define each of its 
lexical concepts.  
With this goal in mind, the paper observes the following structure: in section two 
we offer a concise survey of the considerable research that has, in the past, been dedi-
cated to abstraction theories of analogy and metaphor. In section three we then com-
pare and contrast WordNet [1] and HowNet as candidate resources for the current 
abstraction approach to analogical reasoning. In section four, having established an 
argument as to why HowNet is to be preferred, we indicate how HowNet?s semantic 
definitions can be transformed in the service of analogical recategorization. The per-
formance and competence of this recategorization ability is then evaluated in section 
five. Speculation about further possible contributions of HowNet to analogical re-
search is reserved for the closing remarks of section six. 
2   Abstraction Theories of Analogy 
That analogy and metaphor operate across multiple levels of conceptual abstraction 
has been well known since classical times. Aristotle first provided a compelling taxo-
nomic account of both in his Poetics (see [5], for a translation), and computationalists 
have been fascinated by this perspective ever since. While the core idea has survived 
relatively unchanged, one must discriminate theories that apparently presume a static 
type-hierarchy to be sufficient for all abstraction purposes (e.g., [6]), from theories 
that posit the need for a dynamic type hierarchy (e.g., [7, 8]). One must also differen-
tiate theories that have actually been implemented (e.g., [6,8,9]) from those that are 
either notional or that seem to court computational intractability (e.g., [5,6]). Perhaps 
most meaningfully, one must differentiate theories and implementations that assume 
hand-crafted, purpose-built ontologies (e.g., [6]) from those that exploit an existing 
large-scale resource like WordNet (e.g., [8,9]). In the former, one has the flexibility to 
support as many functional abstractions like cutting-agent as are believed necessary, 
but at the cost of appearing to anticipate future analogies by hand-crafting them into 
the system.  
328 T. Veale 
 {DEITY, GOD}
{ARES} 
{ZEUS} 
greek 
{ATHENA} 
{GREEK_DEITY} 
{SKANDA} 
{GANESH} {VARUNA}
{HINDU_DEITY} 
hindu
? 
? {WISDOM_DEITY}
Defn: god of wisdom or prophesy Defn: goddess of wisdom and ? 
alignable
wisdom
 
Fig. 1. Analysis of the WordNet gloss for {Athena} suggests that the word-form ?wisdom? has 
analogical potential, since it is alignable with another use in {Ganesh}. This leads to the con-
struction of the dynamic sense {Wisdom_deity} which can be used to make analogical leaps 
between these concepts. 
This current work follows the latter course. We intend to automatically construct a 
new taxonomy of analogically-useful abstractions like cutting-agent, by analysing the 
semantic content of the definitions assigned to each word-sense in HowNet. Past work 
(e.g., [8]) has attempted this automatic construction of analogically-friendly taxono-
mies from WordNet, resulting in an approach that involves as much information-
extraction from free text as it does semantic inference. This is because WordNet?s 
glosses, unlike the semantic definitions of HowNet, are free-form sentences designed 
for human, rather than machine, consumption. For instance, Figure 1 above illustrates 
how features can be lifted from WordNet glosses to create new intermediate 
taxonyms, or dynamic types, from which subsequent abstraction-based analogies can 
be generated. 
The explicitly-structured semantic forms that one finds in HowNet definitions will 
clearly make this lifting of features more logical and less heuristic. In general, this 
makes HowNet an ideal knowledge-source for a computational model of metaphor 
and analogy (e.g., see [10] for a topical perspective). 
3   Comparing WordNet and HowNet 
Generalization can be considered ?controlled? if, when moving to a higher level of 
abstraction in a taxonomy, a conceptual system is able to precisely quantify that 
meaning which is lost. In this sense at least, most large-scale taxonomies do not pro-
vide a significant degree of control. Perhaps nowhere is this observation more keenly 
felt than in weak lexical ontologies like Princeton WordNet (PWN). In PWN [1], 
generalization of a concept/synset does not generally yield a functional or behavioural 
abstraction of the original concept. This is so because WordNet?s taxonomy is de-
signed not to capture common causality, function and behaviour, but to show how 
 Analogy as Functional Recategorization: Abstraction with HowNet Semantics 329 
existing lexemes relate to each other. For example, the common abstraction that 
unites {surgeon, sawbones} and {tree_surgeon} is not a concept that captures a 
shared sense of repair, improvement or care, but {person, human}. To be fair, much 
the same must be said of other taxonomies, even that of HowNet [2,11], a Chi-
nese/English semantic dictionary, and Cyc [12]. However, as we shall demonstrate, 
HowNet contains the necessary basis for such abstractions in its relational semantic 
definitions.  
PWN and HowNet have each been designed according a different theory of seman-
tic organization. PWN is differential is nature: rather than attempting to express the 
meaning of a word explicitly, PWN instead differentiates words with different mean-
ings by placing them in different synsets, and further differentiates synsets from one 
another by assigning them to different positions in its ontology. In contrast, HowNet 
is constructive in nature, exploiting sememes from a less discriminating taxonomy 
than PWN?s to compose a semantic representation of meaning for each word sense.  
Nonetheless, HowNet compensates strongly with its constructive semantics. For 
example, HowNet assigns the concept surgeon|
?
?the following definition:  
 {human|?:HostOf={Occupation|
?
?},domain={medical|
?
}, 
{doctor|
?
?:agent={~}}} 
which can be glossed thus: ?a surgeon is a human with an occupation in the medical 
domain who acts as the agent of a doctoring activity.? The {~} serves as a self-
reference here, to mark the location of the concept being defined in the given seman-
tic structure. The oblique reference offered by the tilde construct serves to make the 
definition more generic (thereby facilitating analogy), so that many different concepts 
can conceivably employ the same definition. Thus, HowNet uses the above definition 
not only for surgeon, but for medical workers in general, from orderlies to nurses to 
internists and neurologists. 
4   Extracting Functional Structure 
Our scheme for converting HowNet?s constructive definitions into a more differential 
form hinges on the use of the tilde as a self-reference in relational structures. For 
instance, consider the semantic definition that HowNet gives to repairman|???:  
{human|?:HostOf={Occupation|
?
?}, {repair|??:agent={~}}} 
Noting the position of {~} here, we can infer that a repairman is the agent of a repair-
ing activity, or in differential terms, a repair-agent. Now, since HowNet defines re-
pair|?? as a specialization of the reinstatement activity resume|??, we can further 
establish repair-agent as a specialization of resume-agent.  
resume-agent 
repair-agent  doctor-agent  amend-agent 
repairman|???  surgeon|
?
?  reviser|?
?
? 
watchmaker|
?
??         herbalist|
??
 
Fig. 2. Portion of a three-level functional hierarchy derived from HowNet 
330 T. Veale 
This double layer of abstraction establishes a new taxonomy that organizes word-
concepts according to their analogical potential, rather than their formal ontological 
properties. For instance, as shown in Figure 2, resume-agent encompasses not only 
repair-agent, but doctor-agent, since HowNet alo defines the predicate doctor|
?
? 
as a specialization of the predicate resume|?
?
. 
In general, given a semantic fragment F:role={~} in  a HowNet definition, we cre-
ate the new abstractions F-role and F?-role, where F? is the immediate hypernym of F.  
The role in question might be agent, instrument, location, patient, or any other role 
that HowNet supports. By way of example, Figure 3 illustrates a partial hierarchy 
derived from the HowNet semantics of various form-altering tools: 
AlterForm- instrument 
cut-instrument       stab-instrument split-instrument    dig-instrument 
knife|?    sword|
??
    grater|???  scissors|? 
razor|??        lance|
?
?        glasscutter|???   chainsaw|?
?
 
Fig. 3. A hierarchy of instruments derived from instances of AlterForm| ???  
5   Evaluating Analogical Competence 
We evaluate the analogical potential of the newly derived functional taxonomy using 
four criteria: topology ?  the branching structure of the new taxonomy dictates its 
ability to generate analogies; coverage ? the percentage of unique HowNet definitions 
that can be functionally re-indexed in the new taxonomy; recall ? the percentage of 
unique definitions for which at least one analogy can be found using the new taxon-
omy; and parsimony? the percentage of abstractions in the new taxonomy that can be 
used to generate analogies. 
5.1   Topological Characteristics of the New Functional Taxonomy 
The new functional taxonomy contains 1579 mid-level abstractions and 838 upper-level 
abstractions. In total, the taxonomy contains only 2219 unique abstractions, revealing 
that in 8% of cases, the upper-level abstraction of one concept serves as the upper-level 
abstraction of another. 
Analogies will be generated only if two or more unique concept definitions are co-
indexed under the same mid-level or upper-level abstraction in the new functional 
taxonomy. For example, knight|
?
? and gladiator.|??? are both co-indexed 
directly under the mid-level abstraction fight-agent. Likewise, gladiator|??? is 
indexed under HaveContest-agent via fight-agent, while footballer|??
???
 is 
indexed under HaveContest-agent via compete-agent. The upper-level of abstraction, 
represented here by HaveContest-agent, is necessary to facilitate analogy between 
semantically distant concepts.  
Nonetheless, we note that a certain degree of metaphoric licence has already been 
exercised by HowNet?s designers in assigning semantic structures, so that even se-
mantically distant concepts can still share the same mid-level abstraction. Creative 
analogies like ?Death is an assassin? can, as shown in Figure 4, be understood via a 
single generalization.  
 Analogy as Functional Recategorization: Abstraction with HowNet Semantics 331 
MakeBad-agent 
kill-agent   attack-agent 
  assassin|??  intruder|??? 
     Death|??             man-eater|??
?
 
Fig. 4. Semantic diversity among concepts with the same mid-level abstraction 
Furthermore, because HowNet contains 95,407 unique lexical concepts (excluding 
synonyms) but only 23,507 unique semantic definitions, these definitions must be 
under-specified to the extent that many are shared by non-identical concepts (e.g., 
cart|?
?
 and bicycle|
??
, are simply defined as manual vehicles).  
5.2   Analogical Coverage 
Since this new taxonomy is derived from the use of {~} in HowNet definitions, both 
the coverage and recall of analogy generation crucially depend on the widespread use 
of this reflexive construct. However, of the 23,505 unique definitions in HowNet, just 
6430 employ thus form of self-reference. The coverage of the new taxonomy is thus 
27% of HowNet definitions. 
5.3   Analogical Recall 
A majority of the abstractions in the new taxonomy, 59%, serve to co-index two or 
more HowNet definitions. Overall, analogies are generated for 6184 unique HowNet 
definitions, though these individual definitions may have many different lexical reali-
zations. The recall rate thus is 26% of HowNet?s 23,507 unique definitions, or 96% of 
the 6430 HowNet definitions that make use of {~}. The most productive abstraction is 
control_agent, which serves to co-index 210 unique definitions. 
5.4   Parsimony of Recall 
Overall, 1,315 of the 2219 nodes in the new taxonomy prove useful in co-indexing 
two or more unique definitions, while 904 nodes serve to index just a single defini-
tion. The parsimony of the new taxonomy is thus 59%, which reveals a reasonable, if 
not ideal, level of representational uniformity across HowNet?s semantic definitions. 
6   Conclusions and Future Work 
While just 27% of HowNet?s definitions are sufficiently structured to support anal-
ogy, we are encouraged that almost all of this generative potential can be achieved 
with a new functional taxonomy that is straightforward and efficient to construct. 
Furthermore, though 27% may seem slim, these analogically-friendly {~} structures 
are concentrated in the areas of the HowNet taxonomy that can most benefit from 
analogical re-description. As revealed in Table 1 below, some areas of HowNet are 
clearly more amenable to analogical reasoning than others. 
332 T. Veale 
Table 1. Analogical coverage/recall for different areas of HowNet 
 Humans Artefacts Animals Overall 
Coverage .65 .68 .42 .27 
Recall .54 .58 .16 .26 
Parsimony .50 .54 .22 .59 
But the analogical potential of HowNet resides not just in its explicit propositional 
semantics, but in its use of Chinese orthography. Consider that most Chinese entries 
in HowNet are multi-character terms, where each character is not so much a letter as a 
morpheme. . For instance, ?
?
?, meaning ?scalpel?, is a composite not just of char-
acters but of ideas, for ?
?
means ?surgery? and ? means ?knife?. This logographic 
compositionality affords a kind of semantic transparency on a scale that alphabetic 
writing systems (like that of English) simply can not match Thus, ?
?
?, which 
translates as ?philosopher?, can be seen via HowNet as a composition of ?
?
 (?phi-
losophy?) and ? (?specialist? or ?scientist?). In turn, philosophy|?
?
 is organized 
by HowNet as a specialization of knowledge|?
?
, as is logic|
??
, mathematics|
??
, lexicography|
?
?
?
 and even midwifery|
?
?
?
. By decomposing com-
pound terms in this way and generalizing the extracted modifiers, yet another three-
level taxonomy can be constructed. For instance, from these examples the partial 
taxonomy of Fig. 5 can be derived. 
Knowledge-human 
Mathematics-human       philosophy-human midwifery-human 
   mathematician|
??
?           philosopher|?
?
?     midwife|
?
? 
Fig. 5. Portion of a three-level hierarchy derived from compound Chinese terms 
The analogical potential of this ontologization becomes clear when one notices that 
it supports the classical analogy of philosopher as midwife. Clearly, then, we have 
just scratched the surface of what can usefully be derived from the lexico-semantic 
content of HowNet. Our current investigations with HowNet suggest that the full 
semantic richness of Chinese orthography may yet play a considerable role in sup-
porting creative reasoning at a linguistic level, if only because it opens a window onto 
a different cultural perspective on words and concepts. 
References 
1. Miller, G. A.: WordNet: A Lexical Database for English. Communications of the ACM, 
Vol. 38 No. 11 (1995) 
2. Dong, Z.: Knowledge Description: What, How and Who? The Proceedings of the Interna-
tional Symposium on Electronic Dictionaries, Tokyo, Japan (1988) 
 Analogy as Functional Recategorization: Abstraction with HowNet Semantics 333 
3. Falkenhainer, B.; Forbus, K.; and Gentner, D.: Structure-Mapping Engine: Algorithm and 
Examples. Artificial Intelligence, 41, pages 1-63 (1989) 
4. Veale, T., Keane, M. T.: The Competence of Sub-Optimal Structure Mapping on ?Hard? 
Analogies. The proceedings of IJCAI?97, the Int. Joint Conference on Artificial Intelli-
gence, Nagoya, Japan.  Morgan Kaufman, San Mateo California (1997) 
5. Hutton, J.: Aristotle's Poetics. Norton, New York (1982) 
6. Fass, D: An Account of Coherence, Semantic Relations, Metonymy, and Lexical Ambigu-
ity Resolution. In: Small, S. I, Cottrell, G. W., Tanenhaus, M.K. (eds.): Lexical Ambiguity 
Resolution: Perspectives from Psycholinguistics, Neuropsychology and Artificial Intelli-
gence. Morgan Kaufman, San Mateo California (1988) 
7. Way, E. C.: Knowledge Representation and Metaphor. Studies in Cognitive systems, Klu-
wer Academic Publishers (1991) 
8. Veale. T.: Dynamic Type Creation in Metaphor Interpretation and Analogical Reasoning: 
A Case-Study with WordNet. In the proceedings of ICCS2003, the 2003 International 
Conference on Conceptual Structures, Dresden, Germany (2003) 
9. Veale, T.: WordNet sits the S.A.T.: A Knowledge-Based Approach to Lexical Analogy. 
The proceedings of ECAI'2004, the 16th European Conf. on Artificial Intelligence. John 
Wiley: London (2004) 
10. Veale, T.: Analogy Generation in HowNet. In the proceedings of IJCAI?05, the 19th Inter-
national Joint Conference on Artificial Intelligence. Morgan Kaufmann: CA. 
11. Wong, S.H.S.: Fighting Arbitrariness in WordNet-like Lexical Databases ? A Natural 
Language Motivated Remedy. The proceedings of GWC 2004, the 2nd Global WordNet 
conference. Edited by Sojka, Pala, Smrz, Fellbaum, Vossen  (2004) 
12. Lenat, D., Guha, R.V.: Building Large Knowledge-Based Systems. Addison Wesley 
(1990) 
 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 160?167,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using WordNet to Automatically Deduce Relations between Words in
Noun-Noun Compounds
Fintan J. Costello,
School of Computer Science,
University College Dublin,
Dublin 6, Ireland.
fintan.costello@ucd.ie
Tony Veale,
Department of Computer Science,
University College Dublin,
Dublin 6, Ireland.
tony.veale@ucd.ie
Simon Dunne,
Department of Computer Science,
University College Dublin, Dublin 6, Ireland.
sdunne@inismor.ucd.ie
Abstract
We present an algorithm for automatically
disambiguating noun-noun compounds by
deducing the correct semantic relation be-
tween their constituent words. This algo-
rithm uses a corpus of 2,500 compounds
annotated with WordNet senses and cov-
ering 139 different semantic relations (we
make this corpus available online for re-
searchers interested in the semantics of
noun-noun compounds). The algorithm
takes as input the WordNet senses for the
nouns in a compound, finds all parent
senses (hypernyms) of those senses, and
searches the corpus for other compounds
containing any pair of those senses. The
relation with the highest proportional co-
occurrence with any sense pair is returned
as the correct relation for the compound.
This algorithm was tested using a ?leave-
one-out? procedure on the corpus of com-
pounds. The algorithm identified the cor-
rect relations for compounds with high
precision: in 92% of cases where a re-
lation was found with a proportional co-
occurrence of 1.0, it was the correct re-
lation for the compound being disam-
biguated.
Keywords: Noun-Noun Compounds, Conceputal
Combination, Word Relations, WordNet
1 Introduction
Noun-noun compounds are short phrases made up
of two or more nouns. These compounds are
common in everyday language and are especially
frequent, and important, in technical documents
(Justeson & Katz, 1995, report that such phrases
form the majority of technical content of scien-
tific and technical documents surveyed). Under-
standing these compounds requires the listener or
reader to infer the correct semantic relationship
between the words making up the compound, in-
ferring, for example, that the phrase ?flu virus?
refers to a virus that causes flu, while ?skin virus?
describes a virus that affects the skin, and marsh
virus a virus contracted in marshes. In this paper
we describe a novel algorithm for disambiguat-
ing noun-noun compounds by automatically de-
ducing the correct semantic relationship between
their constituent words.
Our approach to compound disambiguation
combines statistical and ontological information
about words and relations in compounds. On-
tological information is derived from WordNet
(Miller, 1995), a hierarchical machine readable
dictionary, which is introduced in Section 1. Sec-
tion 2 describes the construction of an annotated
corpus of 2,500 noun-noun compounds covering
139 different semantic relations, with each noun
and each relation annotated with its correct Word-
Net sense.1
Section 3 describes our algorithm for finding
the correct relation between nouns in a com-
pound, which makes use of this annotated cor-
pus. Our general approach is that the correct re-
lation between two words in a compound can be
deduced by finding other compounds containing
words from the same semantic categories as the
words in the compound to be disambiguated: if a
particular relation occurs frequently in those other
compounds, that relation is probably also the cor-
rect relation for the compound in question. Our al-
1A file containing this corpus is available for download
from http://inismor.ucd.ie/?fintanc/wordnet compounds
160
Table 1: Thematic relations proposed by Gagne?.
relation example
head causes modifier flu virus
modifier causes head college headache
head has modifier picture book
modifier has head lemon peel
head makes modifier milk cow
head made of modifier chocolate bird
head for modifier cooking toy
modifier is head dessert food
head uses modifier gas antiques
head about modifier travel magazine
head located modifier mountain cabin
head used by modifier servant language
modifier located head murder town
head derived from modifier oil money
gorithm implements this approach by taking as in-
put the correct WordNet senses for the constituent
words in a compound (both base senses and parent
or hypernyms of those senses), and searching the
corpus for other compounds containing any pair
of those base or hypernym senses. Relations are
given a score equal to their proportional occur-
rence with those sense pairs, and the relation with
the highest proportional occurrence score across
all sense-pairs is returned as the correct relation
for the compound. Section 4 describes two differ-
ent leave-one-out tests of this ?Proportional Rela-
tion Occurrence? (PRO) algorithm, in which each
compound is consecutively removed from the cor-
pus and the algorithm is used to deduce the cor-
rect sense for that compound using the set of com-
pounds left behind. These tests show that the
PRO algorithm can identify the correct relations
for compounds, and the correct senses of those re-
lations, with high precision. Section 6 compares
our algorithm for compound disambiguation with
one recently presented alternative, Rosario et al?s
(2002) rule-based system for the disambiguation
of noun-noun compounds. The paper concludes
with a discussion of future developments of the
PRO algorithm.
2 Introduction to WordNet
In both our annotated corpus of 2,500 noun-noun
compounds and our proportional relation selection
algorithm we useWordNet (Miller, 1995). The ba-
sic unit of WordNet is the sense. Each word in
WordNet is linked to a set of senses, with each
sense identifying one particular meaning of that
word. For example, the noun ?skin? has senses rep-
resenting (i) the cutis or skin of human beings, (ii)
the rind or peel of vegetables or fruit, (iii) the hide
or pelt of an animal, (iv) a skin or bag used as a
container for liquids, and so on. Each sense con-
tains an identifying number and a ?gloss? (explain-
ing what that sense means). Each sense is linked
to its parent sense, which subsumes that sense as
part of its meaning. For example, sense (i) of the
word ?skin? (the cutis or skin of human beings) has
a parent sense ?connective tissue? which contains
that sense of skin and also contains the relevant
sense of ?bone?, ?muscle?, and so on. Each par-
ent sense has its own parents, which in turn have
their own parent senses, and so on up to the (no-
tional) root node of the WordNet hierarchy. This
hierarchical structure allows computer programs
to analyse the semantics of natural language ex-
pressions, by finding the senses of the words in
a given expression and traversing the WordNet
graph to make generalisations about the meanings
of those words.
3 Corpus of Annotated Compounds
In this section we describe the construction of a
corpus of noun-noun compounds annotated with
the correct WordNet noun senses for constituent
words, the correct semantic relation between those
words, and the correct WordNet verb sense for that
relation. In addition to providing a set of com-
pounds to use as input for our compound disam-
biguation algorithm, one aim in constructing this
corpus was to examine the relations that exist in
naturally occurring noun-noun compounds. This
follows from existing research on the relations that
occur between noun-noun compounds (e.g. Gagne?
& Shoben, 1997). Gagne? and her colleagues pro-
vide a set of ?thematic relations? (derived from
relations proposed by, for example, Levi, 1978)
which, they argue, cover the majority of semantic
relations between modifier (first word) and head
(second word) in noun-noun compounds. Table
1 shows the set of thematic relations proposed in
Gagne? & Shoben (1997). A side-effect of the con-
struction of our corpus of noun-noun compounds
was an assessment of the coverage and usefulness
of this set of relations.
3.1 Procedure
The first step in constructing a corpus of anno-
tated noun-noun compounds involved selection of
a set of noun-noun compounds to classify. The
source used was the set of noun-noun compounds
161
Figure 1: Selecting WordNet senses for nouns.
defined in WordNet. Compounds from WordNet
were used for two reasons. First, each compound
had an associated gloss or definition written by
the lexicographer who entered that compound into
the corpus: this explains the relation between the
two words in that compound. Sets of compounds
from other sources would not have such associated
definitions. Second, by using compounds from
WordNet, we could guarantee that all constituent
words of those compounds would also have en-
tries in WordNet, ensuring their acceptability to
our compound disambiguation algorithm. An ini-
tial list of over 40,000 two-word noun-noun com-
pounds were extracted from WordNet version 2.0.
From this list we selected a random subset of com-
pounds and went through that set excluding all
compounds using scientific latin (e.g. ocimum
basilicum), idiomatic compounds (e.g. zero hour,
ugli fruit), compounds containing proper nouns
(e.g. Yangtze river), non-english compounds (e.g.
faux pas), and chemical terminology (e.g. carbon
dioxide).
The remaining compounds were placed in ran-
dom order, and the third author annotated each
compound with the WordNet noun senses of the
constituent words, the semantic relation between
those words, and the WordNet verb sense of that
relation (again, with senses extracted from Word-
Net version 2.0). A web page was created for
this annotation task, showing the annotator the
compound to be annotated and the WordNet gloss
(meaning) for that compound (see Figure 1). This
page also showed the annotator the list of possible
WordNet senses for the modifier noun and head
noun in the compound, allowing the annotator to
select the correct WordNet sense for each word.
After selecting correct senses for the words in the
compound, another page was presented (Figure 2)
Figure 2: Selecting relation and relation senses.
allowing the annotator to identify the correct se-
mantic relation for that compound, and then to se-
lect the correct WordNet sense for the verb in that
relation.
We began by assuming that Gagne? & Shoben?s
(1997) set of 14 relations was complete and could
account for all compounds being annotated. How-
ever, a preliminary test revealed some common
relations (e.g., eats, lives in, contains, and re-
sembles) that were not in Gagne? & Shoben?s set.
These relations were therefore added to the list of
relations we used. Various other less commonly-
occuring relations were also observed. To allow
for these other relations, a function was added to
the web page allowing the annotator to enter the
appropriate relation appearing in the form ?noun
(insert relation) modifier? and ?modifier (insert re-
lation) noun?. They would then be shown the set
of verb senses for that relation and asked to select
the correct sense.
3.2 Results
Word sense, relation, and relation sense informa-
tion was gathered for 2,500 compounds. Relation
occurrence was well distributed across these com-
pounds: there were 139 different relations used in
the corpus. Frequency of these relations ranged
widely: there were 86 relations that occured for
just one compound in the corpus, and 53 relations
that occurred more than once. For the relations
that occured more than once in the corpus, the
average number of occurrences was 46. Table 2
shows the 5 most frequent relations in the corpus:
these 5 relations account for 54% of compounds.
Note that 2 of the 5 relations in Table 2 (head con-
162
Table 2: 5 most frequent relations in the corpus.
relation frequency number of
relation senses
head used for modifier 382 3
head about modifier 360 1
head located modifier 226 3
head contains modifier 217 3
head resembles modifier 169 1
tains modifier and head resembles modifier) are
not listed in Gagne??s set of taxonomic relations.
This suggests that the taxonomy needs to be ex-
tended by the addition of further relations.
In addition to identifying the relations used in
compounds in our corpus, we also identified the
WordNet verb sense of each relation. In total 146
different relation senses occurred in the corpus.
Most relations in the corpus were associated with
just 1 relation sense. However, a significant mi-
nority of relations (29 relations, or 21% of all re-
lations) had more than one relation sense; on aver-
age, these relations had three different senses each.
Relations with more than one sense in the corpus
tended to be the more frequently occurring rela-
tions: as Table 2 shows, of the 5 most frequent
relations in the corpus, 3 were identified as hav-
ing more than one relation sense. The two rela-
tions with the largest number of different relation
senses occurring were carry (9 senses) and makes
(8 senses). Table 3 shows the 3 most frequent
senses for both relations. This diversity of rela-
tion senses suggests that Gagne??s set of thematic
relations may be too coarse-grained to capture dis-
tinctions between relations.
4 Compound Disambiguation Algorithm
The previous section described the development
of a corpus of associations between word-sense
and relation data for a large set of noun-noun
compounds. This section presents the ?Pro-
portional Relation Occurrence? (PRO) algorithm
which makes use of this information to deduce the
correct relation for a given compound.
Our approach to compound disambiguation
works by finding other compounds containing
words from the same semantic categories as the
words in the compound to be disambiguated: if a
particular relation occurs frequently in those other
compounds, that relation is probably also the cor-
rect relation for the compound in question. We
take WordNet senses to represent semantic cate-
Table 3: Senses for relations makes and carries.
relation relation sense gloss example
Makes bring forth or yield; spice tree
Makes cause to occur or exist; smoke bomb
Makes create or manufacture cider mill
a man-made product;
Carries contain or hold, have within; pocket watch
Carries move while supporting, in passenger van
a vehicle or one?s hands;
Carries transmit or serve as the radio wave
medium for transmission;
gories. Once the correct WordNet sense for a word
has been identified, that word can placed a set
of nested semantic categories: the category repre-
sented by that WordNet sense, by the parent sense
(or hypernym) of that sense, the parent of that
parent, and so on up to the (notional) root sense
of WordNet (the semantic category which sub-
sumes every other category in WordNet). Our al-
gorithm uses the set of semantic categories for the
words in a compound, and searches for other com-
pounds containing words from any pair of those
categories.
Figure 3 shows the algorithm in pseudocode.
The algorithm uses a corpus of annotated noun-
noun compounds and, to disambiguate a given
compound, takes as input the correct WordNet
sense for the modifier and head words of that com-
pound, plus all hypernyms of those senses. The al-
gorithm pairs each modifier sense with each head
sense (lines 1 & 2 in Figure 3). For each sense-
pair, the algorithm goes through the corpus of
noun-noun compounds and extracts every com-
pound whose modifier sense (or a hypernym of
that sense) is equal to the modifier sense in the
current sense-pair, and whose head sense (or a hy-
pernym of that sense) is equal to the head sense in
that pair (lines 5 to 8). The algorithm counts the
number of times each relation occurs in that set
of compounds, and assigns each relation a Propor-
tional Relation Occurrence (PRO) score for that
sense-pair (lines 10 to 12). The PRO score for a
given relation R in a sense-pair S is a tuple with
two components, as in Equation 1:
PRO(R,S) = ?
|R ? S|
|S|
,
|R ? S|
|D|
?. (1)
The first term of this tuple is the proportion of
times relationR occurs with sense-pair S (in other
words, the conditional probability of relation R
163
Preconditions:
The entry for each compound C in corpus D contains:
CmodList = sense + hypernym senses for modifier of C;
CheadList = sense + hypernym senses for head of C;
Crel = semantic relation of C;
CrelSense = verb sense for semantic relation for C;
Input:
X = compound for which a relation is required;
modList = sense + hypernym senses for modifier of X;
headList = sense + hypernym senses for head of X;
finalResultList = ();
Begin:
1 for each modifier sense M ? modList
2 for each head sense H ? headList
3 relCount = ();
4 matchCount = 0;
5 for each compound C ? corpus D
6 if ((M ? CmodList) and (H ? CheadList))
7 relCount[Crel] = relCount[Crel] + 1;
8 matchCount = matchCount + 1;
9 for each relation R ? relCount
10 condProb = relCount[R]/matchCount;
11 jointProb = relCount([R]/|D|;
12 scoreTuple = (relProp, jointProb);
13 prevScoreTuple = finalResultList[R];
14 if (scoreTuple[1] > prevScoreTuple[1])
15 finalResultList[R] = relSscoreTuple;
16 if (scoreTuple[1] = prevScoreTuple[1])
17 if (scoreTuple[2] > prevScoreTuple[2])
18 finalResultList[R] = scoreTuple;
19 sort finalResultList by relation score tuples;
20 return finalResultList;
End.
Figure 3: Compound disambiguation algorithm.
given sense-pair S); the second term is simply the
proportion of times the relation co-occurs with the
sense pair in the database of compounds D (in
other words, the joint probability of relationR and
sense-pair S). The algorithm compares the PRO
score obtained for each relationR from the current
sense-pair with the score obtained for that relation
from any other sense-pair, using the first term of
the score tuple as the main key for comparison
(lines 14 and 15), and using the second term as
a tie-breaker (lines 16 to 18). If the PRO score for
relation R in the current sense-pair is greater than
the PRO score obtained for that relation with some
other sense pair (or if no previous score for the re-
lation has been entered), the current PRO tuple is
recorded for relation R. In this way the algorithm
finds the maximum PRO score for each relation R
across all possible sense-pairs for the compound
in question. The algorithm returns a list of can-
didate relations for the compound, sorted by PRO
score (lines 19 and 20). The relations at the front
of that list (those with the highest PRO scores) are
those most likely to be the correct relation for that
compound.
Tests of this algorithm suggest that, in many
cases, candidate relations for a given compound
will be tied on the first term of their PRO score
tuple. The use of the second score-tuple term is
therefore an important part of the algorithm. For
example, suppose that two competing relations for
some compound have a proportional occurence
of 1.0 (both relations occur in every occurrence
of some sense-pair in the compound corpus). If
the first relation occurs 20 times with its selected
sense pair (i.e. there are 20 occurrences of the
sense-pair in the corpus, and the relation occurs in
each of those 20 occurrences), but the second rela-
tion only occurs occurs 2 times with its selected
sense pair (i.e. there are 2 occurrences of that
sense-pair in the corpus, and the relation occurs
in each of those 2 occurrences), the first relation
will be preferred over the second relation, because
there is more evidence for that relation being the
correct relation for the compound in question.
The algorithm in Figure 3 returns a list of can-
didate semantic relations for a given compound
(returning relations such as ?head carries modi-
fier? for the compound vegetable truck or ?mod-
ifier causes head? for the compound storm dam-
age, for example). This algorithm can also return
a list of relation senses for a given compound (re-
turning the WordNet verb sense ?carries: moves
while supporting, in a vehicle or one?s hands? for
the relation for the compound vegetable truck but
the verb sense ?carries: transmits or serves as the
medium for transmission? for the compound ra-
dio wave, for example). To return a list of rela-
tion senses rather than relations, we replace Crel
with CrelSense throughout the algorithm in Figure
3. Section 5 describes a test of both versions of the
algorithm.
5 Testing the Algorithm
To test the PRO algorithm it was implemented in a
Perl program and applied to the corpus of com-
pounds described in Section 3. We applied the
program to two tasks: computing the correct re-
lation for a given compound, and computing the
correct relation sense for that compound. We
used a ?leave-one-out? cross-validation approach,
in which we consecutively removed each com-
pound from the corpus (making it the ?query com-
pound?), recorded the correct relation or relation
sense for that compound, then passed the correct
164
  
Precision vs PRO level
0
500
1000
1500
2000
2500
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
PRO level
num
ber 
of c
omp
oun
ds
Total number ofresponsesreturned at thisPRO level
Number ofcorrectresponsesreturned at thisPRO level
 
Figure 4: Graph of precision versus PRO value for
returned relations
head and modifier senses of that query compound
(plus their hypernyms), and the corpus of remain-
ing compounds (excluding the query compound),
to the Perl program. We carried out this process
for each compound in the corpus. The result of this
procedure was a list, for each compound, of can-
didate relations or relation senses sorted by PRO
score.
We assessed the performance of the algorithm
in two ways. We first considered the rank of
the correct relation or relation sense for a given
compound in the sorted list of candidate rela-
tions/relation senses returned by the algorithm.
The algorithm always returned a large list of can-
didate relations or relation senses for each com-
pound (over 100 different candidates returned for
all compounds). In the relation selection task, the
correct relation for a compound occurred in the
first position in this list for 41% of all compounds
(1,026 out of 2,500 compounds), and occured in
one of the first 5 positions (in the top 5% of re-
turned relations or relation senses) for 72% of all
compounds (1780 compounds). In the relation-
sense selection task, the correct relation for a com-
pound occured in the first position in this list for
43% of all compounds, and occured in one of the
first 5 positions for 74% of all compounds. This
performance suggests that the algorithm is doing
well in both tasks, given the large number of pos-
sible relations and relation senses available.
Our second assessment considered the precision
and the recall of relation/relation senses returned
by the algorithm at different proportional occur-
rence levels (different levels for the first term in
PRO score tuples as described in Equation 1). For
each proportional occurrence level between 0 and
1, we assumed that the algorithm would only re-
turn a relation or relation sense when the first rela-
 
 
Precision vs PRO level
0
500
1000
1500
2000
2500
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
PRO level
num
ber 
of c
omp
oun
ds
Total number ofresponsesreturned at thisPRO level
Number ofcorrectresponsesreturned at thisPRO level
 
Figure 5: Graph of precision versus PRO value for
returned relation senses
tion in the list of candidate relations returned had
a score at or above that level. We then counted the
total number of compounds for which a response
was returned at that level, and the total number of
compounds for which a correct response was re-
turned. The precision of the algorithm at a given
PRO level was equal to the number of correct
responses returned by the algorithm at that PRO
level, divided by the total number of responses re-
turned by the algorithm at that level. The recall
of the algorithm at a given PRO level was equal
to the number of correct responses returned by the
algorithm at that level, divided by the total number
of compounds in the database (the total number of
compounds for which the algorithm could have re-
turned a correct response).
Figure 4 shows the total number of responses,
and the total number of correct responses, returned
at each PRO level for the relation selection task.
Figure 5 shows the same data for the relation-sense
selection task. As both graphs show, as PRO level
increases, the total number of responses returned
by the algorithm declines, but the total number of
correct responses does not fall significantly. For
example, in the relation selection task, at a PRO
level of 0 the algorithm return a response (selects
a relation) for all 2,500 compounds, and approx-
imately 1,000 of those responses are correct (the
algorithm?s precision at this level is 0.41). At a
PRO level of 1, the algorithm return a response
(selects a relation) for just over 900 compounds,
and approximately 850 of those responses are cor-
rect (the algorithm?s precision at this level is 0.92).
A similar pattern is seen for the relation sense re-
sponses returned by the algorithm. These graphs
show that with a PRO level around 1, the algorithm
makes a relatively small number of errors when se-
lecting the correct relation or relation sense for a
165
given compound (an error rate of less than 10%).
The PRO algorithm thus has a high degree of pre-
cision in selecting relations for compounds.
As Figures 4 and 5 show, the number of cor-
rect responses returned by the PRO algorithm did
not vary greatly across PRO levels. This means
that the recall of the algorithm remained relatively
constant across PRO levels: in the relation selec-
tion task, for example, recall ranged from 0.41 (at
a PRO level of 0) to 0.35 (at a PRO level of 1). A
similar pattern occurred in the relation-sense se-
lection task.
6 Related Work
Various approaches to noun-noun compound dis-
ambiguation in the literature have used the seman-
tic category membership of the constituent words
in a compound to determine the relation between
those words. Most of these use hand-crafted lex-
ical hierarchies designed for particular semantic
domains. We compare our algorithm for com-
pound disambiguation with one recently presented
alternative, Rosario, Hearst, and Fillmore?s (2002)
rule-based system for the disambiguation of noun-
noun compounds in the biomedical domain.
6.1 Rule-based disambiguation algorithm
Rosario et al?s (2002) general approach to noun-
noun compound disambiguation is based, as ours
is, on the semantic categories of the nouns mak-
ing up a compound. Rosario et al make use of
the MeSH (Medical Subject Headings) hierarchy,
which provides detailed coverage of the biomed-
ical domain they focus on. Their analysis in-
volves automatically extracting a corpus of noun-
noun compounds from a large set of titles and ab-
stracts from the MedLine collection of biomedical
journal articles, and identifying the MeSH seman-
tic categories under which the modifier and head
words of those compounds fall. This analysis gen-
erates a set of category pairs for each compound
(similar to our sense pairs), with each pair consist-
ing of a MeSH category for the modifier word and
a MeSH category for the head.
The aim of Rosario et al?s analysis was to pro-
duce a set of rules which would link the MeSH
category pair for a given compound to the correct
semantic relation for that compound. Given such
a set of rules, their algorithm for disabmiguat-
ing noun-noun compounds involves obtaining the
MeSH category membership for the constituent
words of the compounds to be disambiguated,
forming category pairs, and looking up those cat-
egory pairs in the list of category-pair?relation
rules. If a rule was found linking the category pair
for a given compound to a particular semantic re-
lation, that relation was returned as the correct re-
lation for the compound in question.
To produce a list of category-pair?relation
rules, Rosario et al first selected a set of cate-
gory pairs occurring in their corpus of compounds.
For each category pair, they manually examined
20% of the compounds falling under that category
pair, paraphrasing the relation between the nouns
in that compound by hand, and seeing if that re-
lation was the same across all compounds under
that category pair. If that relation was the same
across all selected compounds, that category pair
was recorded as a rule linked to the relation pro-
duced. If, on the other hand, several different re-
lations were produced for a given category pair,
analysis decended one level in the MeSH hierar-
chy, splitting that category pair into several sub-
categories. This repeated until a rule was pro-
duced assigning a relation to every compound ex-
amined. The rules produced by this process were
then tested using a randomly chosen test set of
20% of compounds falling under each category
pair, entirely distinct from the compound set used
in rule construction, and applying the rules to
those new compounds. An evaluator checked each
compound to see whether the relation returned for
that compound was an acceptable reflection of that
compound?s meaning. The results varied between
78.6% correct to 100% correct across the different
category pairs.
6.2 Comparing the algorithms
In this section we first compare Rosario et al?s
algorithm for compound disambiguation with our
own, and then compare the procedures used to as-
sess those algorithms. While both algorithms are
based on the association between category pairs
(sense pairs) and semantic relations, they differ in
that Rosario et al?s algorithm uses a static list of
manually-defined rules linking category pairs and
semantic relations, while our PRO algorithm au-
tomatically and dynamically computes links be-
tween sense pairs and relations on the basis of pro-
portional co-occurrence in a corpus of compounds.
This gives our algorithm an advantage in terms
of coverage: where Rosario et al?s algorithm can
166
only disambiguate compounds whose constituent
words match one of the category-pair?relation
rules on their list, our algorithm should be able to
apply to any compound whose constituent words
are defined in WordNet. This also gives our al-
gorithm an advantage in terms of extendability, in
that while adding a new compound to the corpus
of compounds used by Rosario et al could poten-
tially require the manual removal or re-definition
of a number of category-pair?relation rules,
adding a new compound to the annotated corpus
used by our PRO algorithm requires no such in-
tervention. Of course, the fact that Rosario et al?s
algorithm is based on a static list of rules linking
categories and relations, while our algorithm dy-
namically computes such links, gives Rosario et
al.?s algorithm a clear efficiency advantage. Im-
proving the efficiency of the PRO algorithm, per-
haps by automatically compiling a tree of associa-
tions between word senses and semantic relations
and using that tree in compound disambiguation,
is an important aim for future research.
Our second point of comparison concerns the
procedures used to assess the two algorithms. In
Rosario et al?s assessment of their rule-based al-
gorithm, an evaluator checked the relations re-
turned by the algorithm for a set of compounds,
and found that those relations were acceptable in a
large proportion of cases (up to 100%). A problem
with this procedure is that many compounds can
fall equally under a number of different acceptable
semantic relations. The compound storm damage,
for example, is best defined by the relation causes
(?damage caused by a storm?), but also falls under
the relations makes (?damage made by a storm?)
and derived from (?damage derived from a storm?):
most people would agree that these paraphrases
all acceptably describe the meaning of the com-
pound (Devereux & Costello, 2005). This means
that, while the relations returned for compounds
by Rosario et al?s algorithmmay have been judged
acceptable for those compounds by the evaluator,
they were not necessarily the most appropriate re-
lations for those compounds: the algorithm could
have returned other relations that would have been
equally acceptable. In other words, Rosario et al?s
assessment procedure is somewhat weaker than
the assessment procedure we used to test the PRO
algorithm, in which there was one correct relation
identified for each compound and the algorithm
was taken to have performed correctly only if it re-
turned that relation. One aim for future work is to
apply the assessment procedure used by Rosario et
al. to the PRO algorithm?s output, asking an eval-
uator to assess the acceptability of the relations re-
turned rather than simply counting the cases where
the best relation was returned. This would provide
a clearer basis for comparison between the algo-
rithms.
6.3 Conclusions
In this paper we?ve described an algorithm for
noun-noun compound disambiguation which au-
tomatically identifies the semantic relations and
relation senses used in such compounds. We?ve
given evidence showing that, coupled with a
corpus of noun-noun compounds annotated with
WordNet senses and semantic relations, this al-
gorithm can identify the correct semantic rela-
tions for compounds with high precision. Unlike
other approaches to automatic compound disam-
biguation which typically apply to particular spe-
cific domains, our algorithm is not domain specific
and can identify relations for a random sample
of noun-noun compounds drawn from the Word-
Net dictionary. Further, our algorithm is fully au-
tomatic: unlike other approaches, our algorithm
does not require the manual construction of rela-
tion rules to produce successful compound disam-
biguation. In future work we hope to extend this
algorithm to provide a more efficient algorithmic
implementation, and also to apply the algorithm
in areas such as the machine translation of noun-
noun compounds, where the identification of se-
mantic relations in compounds is a crucial step in
the translation process.
References
B. Devereux & F. J. Costello. 2005. Investigating the
Relations used in Conceptual Combination. Artificial In-
telligence Review, 24(3?4): 489?515.
C. L. Gagne?, & E. J. Shoben, E. 1997. Influence
of thematic relations on the comprehension of modifier-
noun combinations. Journal of Experimental Psychology:
Learning, Memory and Cognition, 23: 71?87.
J. Justeson & S. Katz. 1995. Technical Terminology: Some
linguistic properties and an algorithm for identification in
text. Natural Language Engineering, 1?1: 9?27.
J. Levi. 1978. The Syntax and Semantics of Complex Nomi-
nals. New York: Academic Press.
G. Miller. 1995. WordNet: A lexical database. Communi-
cation of the ACM, 38(11), 39?41.
B. Rosario, M. Hearst, & C. Fillmore. 2002. The De-
scent of Hierarchy, and Selection in Relational Semantics.
Proceedings of ACL-02: 247?254.
167
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 57?64,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Making Lexical Ontologies Functional and Context-Sensitive
Tony Veale
Computer Science and Informatics
University College Dublin
Ireland
tony.veale@ucd.ie
Yanfen Hao
Computer Science and Informatics
University College Dublin
Ireland
yanfen.hao@ucd.ie
Abstract
Human categorization is neither a binary nor
a context-free process. Rather, some con-
cepts are better examples of a category than
others, while the criteria for category mem-
bership may be satisfied to different degrees
by different concepts in different contexts.
In light of these empirical facts, WordNet?s
static category structure appears both exces-
sively rigid and unduly fragile for process-
ing real texts. In this paper we describe a
syntagmatic, corpus-based approach to re-
defining WordNet?s categories in a func-
tional, gradable and context-sensitive fash-
ion. We describe how the diagnostic prop-
erties for these definitions are automati-
cally acquired from the web, and how the
increased flexibility in categorization that
arises from these redefinitions offers a ro-
bust account of metaphor comprehension
in the mold of Glucksberg?s (2001) the-
ory of category-inclusion. Furthermore, we
demonstrate how this competence with figu-
rative categorization can effectively be gov-
erned by automatically-generated ontologi-
cal constraints, also acquired from the web.
1 Introduction
Linguistic variation across contexts is often symp-
tomatic of ontological differences between contexts.
These observable variations can serve as valuable
clues not just to the specific senses of words in con-
text (e.g., see Pustejovsky, Hanks and Rumshisky,
2004) but to the underlying ontological structure it-
self (see Cimiano, Hotho and Staab, 2005). The
most revealing variations are syntagmatic in nature,
which is to say, they look beyond individual word
forms to larger patterns of contiguous usage (Hanks,
2004). In most contexts, the similarity between
chocolate, say, and a narcotic like heroin will mea-
gerly reflect the simple ontological fact that both are
kinds of substances; certainly, taxonomic measures
of similarity as discussed in Budanitsky and Hirst
(2006) will capture little more than this common-
ality. However, in a context in which the addictive
properties of chocolate are very salient (e.g., an on-
line dieting forum), chocolate is more likely to be
categorized as a drug and thus be considered more
similar to heroin. Look, for instance, at the simi-
lar ways in which these words can be used: one can
be ?chocolate-crazed? or ?chocolate-addicted? and
suffer ?chocolate-induced? symptoms (e.g., each of
these uses can be found in the pages of Wikipedia).
In a context that gives rise to these expressions, it is
unsurprising that chocolate should appear altogether
more similar to a harmful narcotic.
In this paper we computationally model this idea
that language use reflects category structure. As
noted by De Leenheer and de Moor (2005), ontolo-
gies are lexical representations of concepts, so we
can expect the effects of context on language use
to closely reflect the effects of context on ontolog-
ical structure. An understanding of the linguistic ef-
fects of context, as expressed through syntagmatic
patterns of word usage, should lead therefore to the
design of more flexible lexical ontologies that natu-
rally adapt to their contexts of use. WordNet (Fell-
57
baum, 1998) is just one such lexical ontology that
can benefit greatly from the added flexibility that
context-sensitivity can bring. Though comprehen-
sive in scale and widely used, WordNet suffers from
an obvious structural rigidity in which concepts are
either entirely within a category or entirely outside
a category: no gradation of category membership
is allowed, and no contextual factors are brought to
bear on criteria for membership. Thus, a gun is al-
ways a weapon in WordNet while an axe is never so,
despite the uses (sporting or murderous) to which
each can be put.
In section two we describe a computational
framework for giving WordNet senses a functional,
context-sensitive form. These functional forms si-
multaneously represent i) an intensional definition
for each word sense; ii) a structured query capable
of retrieving instances of the corresponding category
from a context-specific corpus; and iii) a member-
ship function that assigns gradated scores to these
instances based on available syntagmatic evidence.
In section three we describe how the knowledge re-
quired to automate this functional re-definition is ac-
quired from the web and linked to WordNet. In sec-
tion four we describe how these re-definitions can
produce a robust model of metaphor, before we eval-
uate the descriptive sufficiency of this approach in
section five, comparing it to the knowledge already
available within WordNet. We conclude with some
final remarks in section six.
2 Functional Context-Sensitive Categories
We take a wholly textual view of context and as-
sume that a given context can be implicitly charac-
terized by a representative text corpus. This corpus
can be as large as a text archive or an encyclopedia
(e.g., the complete text of Wikipedia), or as small
as a single document, a sentence or even a single
noun-phrase. For instance, the micro-context ?alco-
holic apple-juice? is enough to implicate the cate-
gory Liquor, rather than Juice, as a semantic head,
while ?lovable snake? can be enough of a context to
locally categorize Snake as a kind of Pet. There is a
range of syntagmatic patterns that one can exploit to
glean category insights from a text. For instance, the
?X kills? pattern is enough to categorize X as a kind
of Killer, ?hunts X? is enough to categorize X as
a kind of Prey, while ?X-covered?, ?X-dipped? and
?X-frosted? all indicate that X is a kind of Covering.
Likewise, ?army of X? suggests that a context views
X as a kind of Soldier, while ?barrage of X? suggests
that X should be seen as a kind of Projectile.
We operationalize the collocation-type of adjec-
tive and noun via the function (attr ADJ NOUN),
which returns a number in the range 0...1; this
represents the extent to which ADJ is used to
modify NOUN in the context-defining corpus.
Dice?s coefficient (e.g., see Cimiano et al, 2005) is
used to implement this measure. A context-sensitive
category membership function can be defined, as in
that for Fundamentalist in Figure 1:
(define Fundamentalist.0 (arg
0
)
(* (max
(%isa arg
0
Person.0)
(%isa arg
0
Group.0))
(min
(max
(attr political arg
0
)
(attr religious arg
0
))
(max
(attr extreme arg
0
)
(attr violent arg
0
)
(attr radical arg
0
)))))
Figure 1. A functional re-definition of the cat-
egory Fundamentalist.
The function of Figure 1 takes, as a single ar-
gument arg
0
, a putative member of the category
Fundamentalist.0 (note how the sense tag, 0, is
used to identify a specific WordNet sense of ?fun-
damentalist?), and returns a membership score in
the range 0...1 for this term. This score reflects the
syntagmatic evidence for considering arg
0
to be
political or religious, as well as extreme or violent
or radical. The function (%isa arg
0
CAT) returns a
value of 1.0 if some sense of arg
0
is a descendent
of CAT (here Person.0 or Group.0), otherwise 0.
This safeguards ontological coherence and ensures
that only kinds of people or groups can ever be
considered as fundamentalists.
The example of Figure 1 is hand-crafted, but a
functional form can be assigned automatically to
many of the synsets in WordNet by heuristic means.
58
For instance, those of Figure 2 are automatically
derived from WordNet?s morpho-semantic links:
(define Fraternity.0 (arg
0
)
(* (%sim arg
0
Fraternity.0)
(max
(attr fraternal arg
0
)
(attr brotherly arg
0
))))
(define Orgasm.0 (arg
0
)
(* (%sim arg
0
Orgasm.0)
(max
(attr climactic arg
0
)
(attr orgasmic arg
0
))))
Figure 2. Exploiting the WordNet links be-
tween nouns and their adjectival forms.
The function (%sim arg
0
CAT) reflects the
perceived similarity between the putative member
arg
0
and a synset CAT in WordNet, using one of
the standard formulations described in Budanitsky
and Hirst (2006). Thus, any kind of group (e.g., a
glee club, a Masonic lodge, or a barbershop quartet)
described in a text as ?fraternal? or ?brotherly?
(both occupy the same WordNet synset) can be
considered a Fraternity to the corresponding degree,
tempered by its a priori similarity to a Fraternity;
likewise, any climactic event can be categorized as
an Orgasm to a more or less degree.
Alternately, the function of Figure 3 is automat-
ically obtained for the lexical concept Espresso by
shallow parsing its WordNet gloss: ?strong black
coffee brewed by forcing steam under pressure
through powdered coffee beans?.
(define Espresso.0 (arg
0
)
(* (%sim arg
0
Espresso.0)
(min
(attr strong arg
0
)
(attr black arg
0
))))
Figure 3. A functional re-definition of the cat-
egory Espresso based on its WordNet gloss.
It follows that any substance (e.g., oil or tea)
described locally as ?black? and ?strong? with a
non-zero taxonomic similarity to coffee can be
considered a kind of Espresso.
Combining the contents of WordNet 1.6 and
WordNet 2.1, 27,732 different glosses (shared by
51,035 unique word senses) can be shallow parsed to
yield a definition of the kind shown in Figure 3. Of
these, 4525 glosses yield two or more properties that
can be given functional form via attr. However, one
can question whether these features are sufficient,
and more importantly, whether they are truly diag-
nostic of the categories they are used to define. In
the next section we consider another source of diag-
nostic properties, explicit similes on the web, before,
in section 5, comparing the quality of these proper-
ties to those available from WordNet.
3 Diagnostic Properties on the Web
We employ the Google search engine as a retrieval
mechanism for acquiring the diagnostic properties
of categories from the web, since the Google API
and its support for the wildcard term * allows this
process to be fully automated. The guiding intu-
ition here is that looking for explicit similes of the
form ?X is as P as Y? is the surest way of finding
the most salient properties of a term Y; with other
syntagmatic patterns, such as adjective:noun collo-
cations, one cannot be sure that the adjective is cen-
tral to the noun.
Since we expect that explicit similes will tend to
exploit properties that occupy an exemplary point on
a scale, we first extract a list of antonymous adjec-
tives, such as ?hot? or ?cold?, from WordNet. For
every adjective ADJ on this list, we send the query
?as ADJ as *? to Google and scan the first 200 snip-
pets returned to extract different noun values for the
wildcard *. From each set of snippets we can also
ascertain the relative frequencies of different noun
values for ADJ. The complete set of nouns extracted
in this way is then used to drive a second phase of
the search, in which the query template ?as * as a
NOUN? is used to acquire similes that may have
lain beyond the 200-snippet horizon of the original
search, or that may hinge on adjectives not included
on the original list. Together, both phases collect
a wide-ranging series of core samples (of 200 hits
each) from across the web, yielding a set of 74,704
simile instances (of 42,618 unique types) relating
59
3769 different adjectives to 9286 different nouns
3.1 Property Filtering
Unfortunately, many of these similes are not suffi-
ciently well-formed to identify salient properties. In
many cases, the noun value forms part of a larger
noun phrase: it may be the modifier of a compound
noun (as in ?bread lover?), or the head of complex
noun phrase (such as ?gang of thieves? or ?wound
that refuses to heal?). In the former case, the com-
pound is used if it corresponds to a compound term
in WordNet and thus constitutes a single lexical unit;
if not, or if the latter case, the simile is rejected.
Other similes are simply too contextual or under-
specified to function well in a null context, so if one
must read the original document to make sense of
the simile, it is rejected. More surprisingly, per-
haps, a substantial number of the retrieved simi-
les are ironic, in which the literal meaning of the
simile is contrary to the meaning dictated by com-
mon sense. For instance, ?as hairy as a bowling
ball? (found once) is an ironic way of saying ?as
hairless as a bowling ball? (also found just once).
Many ironies can only be recognized using world
knowledge, such as ?as sober as a Kennedy? and ?as
tanned as an Irishman?.
Given the creativity involved in these construc-
tions, one cannot imagine a reliable automatic fil-
ter to safely identify bona-fide similes. For this
reason, the filtering task is performed by a human
judge, who annotated 30,991 of these simile in-
stances (for 12,259 unique adjective/noun pairings)
as non-ironic and meaningful in a null context; these
similes relate a set of 2635 adjectives to a set of
4061 different nouns. In addition, the judge also
annotated 4685 simile instances (of 2798 types) as
ironic; these similes relate a set of 936 adjectives
to a set of 1417 nouns. Perhaps surprisingly, ironic
pairings account for over 13% of all annotated sim-
ile instances and over 20% of all annotated types.
3.2 Linking to WordNet Senses
To create functional WordNet definitions from these
adjective:noun pairings, we first need to identify the
WordNet sense of each noun. For instance, ?as stiff
as a zombie? might refer either to a re-animated
corpse or to an alcoholic cocktail (both are senses
of ?zombie? in WordNet, and drinks can be ?stiff?
too). Disambiguation is trivial for nouns with just
a single sense in WordNet. For nouns with two or
more fine-grained senses that are all taxonomically
close, such as ?gladiator? (two senses: a boxer and a
combatant), we consider each sense to be a suitable
target. In some cases, the WordNet gloss for as par-
ticular sense will literally mention the adjective of
the simile, and so this sense is chosen. In all other
cases, we employ a strategy of mutual disambigua-
tion to relate the noun vehicle in each simile to a spe-
cific sense in WordNet. Two similes ?as A as N
1
?
and ?as A as N
2
? are mutually disambiguating if N
1
and N
2
are synonyms in WordNet, or if some sense
of N
1
is a hypernym or hyponym of some sense of
N
2
in WordNet. For instance, the adjective ?scary?
is used to describe both the noun ?rattler? and the
noun ?rattlesnake? in bona-fide (non-ironic) similes;
since these nouns share a sense, we can assume that
the intended sense of ?rattler? is that of a danger-
ous snake rather than a child?s toy. Similarly, the
adjective ?brittle? is used to describe both saltines
and crackers, suggesting that it is the bread sense of
?cracker? rather than the hacker, firework or hillbilly
senses (all in WordNet) that is intended.
These heuristics allow us to automatically disam-
biguate 10,378 bona-fide simile types (85%), yield-
ing a mapping of 2124 adjectives to 3778 different
WordNet senses. Likewise, 77% (or 2164) of the
simile types annotated as ironic are disambiguated
automatically. A remarkable stability is observed in
the alignment of noun vehicles to WordNet senses:
100% of the ironic vehicles always denote the same
sense, no matter the adjective involved, while 96%
of bona-fide vehicles always denote the same sense.
This stability suggests two conclusions: the dis-
ambiguation process is consistent and accurate; but
more intriguingly, only one coarse-grained sense of
any word is likely to be sufficiently exemplary of
some property to be useful in a simile.
4 From Similes to Category Functions
As noted in section 3, the filtered web data yields
12,259 bona-fide similes describing 4061 target
nouns in terms of 2635 different adjectival prop-
erties. Word-sense disambiguation allows 3778
synsets in WordNet to be given a functional re-
definition in terms of 2124 diagnostic properties, as
60
in the definition of Gladiator in Figure 4:
(define Gladiator.0 (arg
0
)
(* (%isa arg
0
Person.0)
(* (%sim arg
0
Gladiator.0)
(combine
(attr strong arg
0
)
(attr violent arg
0
)
(attr manly arg
0
)))))
Figure 4. Aweb-based definition of Gladiator.
Since we cannot ascertain from the web data
which properties are necessary and which are
collectively sufficient, we use the function combine
to aggregate the available evidence. This function
implements a na??ve probabilistic or, in which each
piece of syntagmatic evidence is naively assumed to
be independent, as follows:
(combine e
0
e
1
) = e
0
+ e
1
(1 ? e
0
)
(combine e
0
e
1
...e
n
) = (combine e
0
(combine e
1
...e
n
))
Thus, any combatant or competitor (such as a
sportsman) that is described as strong, violent or
manly in a corpus can be categorized as a Gladiator
in that context; the more properties that hold, and
the greater the degree to which they hold, the greater
the membership score that is assigned.
The source of the hard taxonomic constraint
(%isa arg
0
Person.0) is explained in the next sec-
tion. For now, note how the use of %sim in the
functions of Figures 2, 3 and 4 means that these
membership functions readily admit both literal and
metaphoric members. Since the line between lit-
eral and metaphoric uses of a category is often im-
possible to draw, the best one can do is to accept
metaphor as a gradable phenomenon (see Hanks,
2006). The incorporation of taxonomic similarity
via %sim ensures that literal members will tend to
receive higher membership scores, and that the most
tenuous metaphors will receive the lowest member-
ship scores (close to 0.0).
4.1 Constrained Category Inclusion
Simile and metaphor involve quite different con-
ceptual mechanisms. For instance, anything that
is particularly strong or black might meaningfully
be called ?as black as espresso? or ?as strong
as espresso?, yet few such things can meaning-
fully be called just ?espresso?. While simile is a
mechanism for highlighting inter-concept similarity,
metaphor is at heart a mechanism of category inclu-
sion (see Glucksberg, 2001). As the espresso exam-
ple demonstrates, category inclusion is more than a
matter of shared properties: humans have strong in-
tuitions about the structure of categories and the ex-
tent to which they can be stretched to include new
members. So while it is sensible to apply the cat-
egory Espresso to other substances, preferably liq-
uids, it seems nonsensical to apply the category to
animals, artifacts, places and so on.
Much as the salient properties of categories can
be acquired form the web (see section 3), so too
can the intuitions governing inclusion amongst cat-
egories. For instance, an attested web-usage of the
phrase ?Espresso-like CAT? tells us that sub-types
of CAT are allowable targets of categorization by the
category Espresso. Thus, since the query ?espresso-
like substance? returns 3 hits via Google, types of
substance (oil, etc.) can be described as Espresso if
they are contextually strong and black. In contrast,
the query ?espresso-like person? returns 0 hits, so
no instance of person can be described as Espresso,
no matter how black or how strong. While this is
clearly a heuristic approach to a complex cognitive
problem, it does allow us to tap into the tacit knowl-
edge that humans employ in categorization. More
generally, a concept X can be included in a category
C if X exhibits salient properties of C and, for some
hypernym H of X in WordNet, we can find an at-
tested use of ?C-like H? on the web.
If we can pre-fetch all possible ?C-like H?
from the web, this will allow comprehension to
proceed without having to resort to web analysis
in mid-categorization. While there are too many
possible values of H to make full pre-fetching a
practical reality, we can generalize the problem
somewhat, by selecting a range of values for H
from the middle-layer of WordNet, such as Person,
Substance, Animal, Tool, Plant, Structure, Event,
Vehicle, Idea and Place, and by pre-fetching the
query ?C-like H? for all 4061 nouns collected in
section 3, combined with this limited set of H
values. For every noun in our database then, we pre-
compile a vector of possible category inclusions.
61
For instance, ?lattice? yields the following vector:
{structure(1620), substance(8), container(1),
vehicle(1)}
where numbers in parentheses indicate the web-
frequency of the corresponding ?Lattice-like H?
query. Thus, the category Lattice can be used to
describe (and metaphorically include) other kinds
of structure (like crystals), types of substance (e.g.,
crystalline substances), containers (like honey-
combs) and even vehicles (e.g., those with many
compartments). Likewise, the noun ?snake? yields
the following vector of possibilities:
{structure(125), animal(122), person(56), ve-
hicle(17), tool(9)}
(note, the frequency for ?person? includes the
frequency for ?man? and ?woman?). The category
Snake can also be used to describe and include
structures (like tunnels), other animals (like eels),
people (e.g., the dishonest variety), vehicles (e.g.,
articulated trucks, trains) and tools (e.g., hoses). The
noun ?gladiator? yields a vector of just one element,
{person(1)}, from which the simple constraint
(%isa arg
0
Person.0) in Figure 4 is derived. In con-
trast, ?snake? is now given the definition of Figure 5:
(define Snake.0 (arg
0
)
(* (max
(%isa arg
0
Structure.0)
(%isa arg
0
Animal.0)
(%isa arg
0
Person.0)
(%isa arg
0
Vehicle.0))
(* (%sim arg
0
Snake.0)
(combine
(attr cunning arg
0
)
(attr slippery arg
0
)
(attr flexible arg
0
)
(attr slim arg
0
)
(attr sinuous arg
0
)
(attr crooked arg
0
)
(attr deadly arg
0
)
(attr poised arg
0
)))))
Figure 5. A membership function for Snake
using web-derived category-inclusion constraints.
Glucksberg (2001) notes that the same category,
used figuratively, can exhibit different qualities in
different metaphors. For instance, Snake might
describe a kind of crooked person in one metaphor,
a poised killer in another metaphor, and a kind of
flexible tool in yet another. The use of combine
in Figure 5 means that a single category definition
can give rise to each of these perspectives in the
appropriate contexts. We therefore do not need a
different category definition for each metaphoric
use of Snake.
To illustrate the high-level workings of category-
inclusion, Table 1 generalizes over the set of 3778
disambiguated nouns from section 3 to estimate the
propensity for one semantic category, like Person, to
include members of another category, like Animal,
in X-like Y constructs.
X-like Y P A Sub T Str
(P)erson .66 .05 .03 .04 .09
(A)nimal .36 .27 .04 .05 .15
(Sub)stance .14 .03 .37 .05 .32
(T)ool .08 .03 .07 .22 .34
(Str)ucture .04 .03 .03 .03 .43
Table 1. The Likelihood of a category X accommo-
dating a category Y.
Table 1 reveals that 36% of ?ANIMAL-like?
patterns on the web describe a kind of Person,
while only 5% of ?PERSON-like? patterns on the
web describe a kind of Animal. Category inclusion
appears here to be a conservative mechanism, with
like describing like in most cases; thus, types of
Person are most often used to describe other kinds
of Person (comprising 66% of ?PERSON-like?
patterns), types of substance to describe other sub-
stances, and so on. The clear exception is Animal,
with ?ANIMAL-like? phrases more often used to
describe people (36%) than other kinds of animal
(27%). The anthropomorphic uses of this category
demonstrate the importance of folk-knowledge in
figurative categorization, of the kind one is more
likely to find in real text, and on the web (as in
section 3), rather than in resources like WordNet.
62
5 Empirical Evaluation
The simile gathering process of section 3, abetted
by Google?s practice of ranking pages according to
popularity, should reveal the most frequently-used
comparative nouns, and thus, the most useful cat-
egories to capture in a general-purpose ontology
like WordNet. But the descriptive sufficiency of
these categories is not guaranteed unless the defin-
ing properties ascribed to each can be shown to
be collectively rich enough, and individually salient
enough, to predict how each category is perceived
and applied by a language user.
If similes are indeed a good basis for mining
the most salient and diagnostic properties of cate-
gories, we should expect the set of properties for
each category to accurately predict how the cate-
gory is perceived as a whole. For instance, humans
? unlike computers ? do not generally adopt a dis-
passionate view of ideas, but rather tend to asso-
ciate certain positive or negative feelings, or affec-
tive values, with particular ideas. Unsavoury activi-
ties, people and substances generally possess a nega-
tive affect, while pleasant activities and people pos-
sess a positive affect. Whissell (1989) reduces the
notion of affect to a single numeric dimension, to
produce a dictionary of affect that associates a nu-
meric value in the range 1.0 (most unpleasant) to 3.0
(most pleasant) with over 8000 words in a range of
syntactic categories (including adjectives, verbs and
nouns). So to the extent that the adjectival proper-
ties yielded by processing similes paint an accurate
picture of each category / noun-sense, we should be
able to predict the affective rating of each vehicle
via a weighted average of the affective ratings of
the adjectival properties ascribed to these nouns (i.e.,
where the affect rating of each adjective contributes
to the estimated rating of a noun in proportion to
its frequency of co-occurrence with that noun in our
simile data). More specifically, we should expect
that ratings estimated via these simile-derived prop-
erties should correlate well with the independent rat-
ings contained in Whissell?s dictionary.
To determine whether similes do offer the clearest
perspective on a category?s most salient properties,
we calculate and compare this correlation using the
following data sets:
A. Adjectives derived from annotated bona-fide
(non-ironic) similes only.
B. Adjectives derived from all annotated similes
(both ironic and non-ironic).
C. Adjectives derived from ironic similes only.
D. All adjectives used to modify a given noun in
a large corpus. We use over 2-gigabytes of
text from the online encyclopaedia Wikipedia
as our corpus.
E. The set of 63,935 unique property-of-noun
pairings extracted via shallow-parsing from
WordNet glosses in section 2, e.g., strong and
black for Espresso.
Predictions of affective rating were made from each
of these data sources and then correlated with the
ratings reported in Whissell?s dictionary of affect
using a two-tailed Pearson test (p < 0.01). As ex-
pected, property sets derived from bona-fide simi-
les only (A) yielded the best correlation (+0.514)
while properties derived from ironic similes only
(C) yielded the worst (-0.243); a middling corre-
lation coefficient of 0.347 was found for all simi-
les together, demonstrating the fact that bona-fide
similes outnumber ironic similes by a ratio of 4
to 1. A weaker correlation of 0.15 was found us-
ing the corpus-derived adjectival modifiers for each
noun (D); while this data provides quite large prop-
erty sets for each noun, these properties merely re-
flect potential rather than intrinsic properties of each
noun and so do not reveal what is most diagnostic
about a category. More surprisingly, property sets
derived from WordNet glosses (E) are also poorly
predictive, yielding a correlation with Whissell?s af-
fect ratings of just 0.278. This suggests that the
properties used to define categories in hand-crafted
resources like WordNet are not always those that ac-
tually reflect how humans think of these categories.
6 Concluding Remarks
Much of what we understand about different cate-
gories is based on tacit and defeasible knowledge of
the outside world, knowledge that cannot easily be
shoe-horned into the rigid is-a structure of an on-
tology like WordNet. This already-complex picture
63
is complicated even further by the often metaphoric
relationship between words and the categories they
denote, and by the fact that the metaphor/literal dis-
tinction is not binary but gradable. Furthermore, the
gradability of category membership is clearly influ-
enced by context: in a corpus describing the exploits
of Vikings, an axe will most likely be seen as a kind
of weapon, but in a corpus dedicated to forestry, it
will likely describe a tool. A resource like WordNet,
in which is-a links are reserved for category relation-
ships that are always true, in any context, is going to
be inherently limited when dealing with real text.
We have described an approach that can be seen as
a functional equivalent to the CPA (Corpus Pattern
Analysis) approach of Pustejovsky et al (2004), in
which our goal is not that of automated induction of
word senses in context (as it is in CPA) but the au-
tomated induction of flexible, context-sensitive cat-
egory structures. As such, our goal is primarily on-
tological rather than lexicographic, though both ap-
proaches are complementary since each views syn-
tagmatic evidence as the key to understanding the
use of lexical concepts in context. By defining cat-
egory membership in terms of syntagmatic expec-
tations, we establish a functional and gradable ba-
sis for determining whether one lexical concept (or
synset) in WordNet deserves to be seen as a de-
scendant of another in a particular corpus and con-
text. Augmented with ontological constraints de-
rived from the usage of ?X-like Y? patterns on the
web, we also show how these membership functions
can implement Glucksberg?s (2001) theory of cate-
gory inclusion.
We have focused on just one syntagmatic pattern
here ? adjectival modification of nouns ? but cate-
gorization can be inferred from a wide range of pro-
ductive patterns in text, particularly those concern-
ing verbs and their case-fillers. For instance, verb-
centred similes of the form ?to V+inf like a|an N?
and ?to be V+past like a|an N? reveal insights into
the diagnostic behaviour of entities (e.g., that preda-
tors hunt, that prey is hunted, that eagles soar and
bombs explode). Taken together, adjective-based
properties and verb-based behaviours can paint an
even more comprehensive picture of each lexical
concept, so that e.g., political agents that kill can
be categorized as assassins, loyal entities that fight
can be categorized as soldiers, and so on. An im-
portant next step, then, is to mine these behaviours
from the web and incorporate the corresponding
syntagmatic expectations into our category defini-
tions. The symbolic nature of the resulting defini-
tions means these can serve not just as mathematical
membership functions, but as ?active glosses?, capa-
ble of recruiting their own members in a particular
context while demonstrating a flexibility with cate-
gorization and a genuine competence with metaphor.
References
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based Measures of Lexical Semantic
Relatedness. Computational Linguistics, 32(1), pp 13-
47.
Christiane Fellbaum (ed.). 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge,
MA.
Cynthia Whissell. 1989. The dictionary of affect in lan-
guage. In R. Plutchnik & H. Kellerman (Eds.). Emo-
tion: Theory and research. New York, Harcourt
Brace, 113-131.
James Pustejovsky, Patrick Hanks and Anna Rumshisky.
2004. Automated Induction of Sense in Context. In
Proceedings of COLING 2004, Geneva, pp 924-931.
Patrick Hanks. 2006. Metaphoricity is a Gradable. In A.
Stefanowitsch and S. Gries (eds.). Corpora in Cog-
nitive Linguistics. Vol. 1: Metaphor and Metonymy.
Berlin: Mouton.
Patrick Hanks. 2004. The syntagmatics of metaphor and
idiom. International Journal of Lexicography, 17(3).
Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning Concept Hierarchies from Text Cor-
pora using Formal Concept Analysis. Journal of AI
Research, 24: 305-339.
Pieter De Leenheer and Aldo de Moor. 2005. Context-
driven Disambiguation in Ontology Elicitation. In
Shvaiko P. & Euzenat J. (eds.), Context and Ontolo-
gies: Theory, Practice and Applications, AAAI Tech
Report WS-05-01. AAAI Press, pp 17-24.
Sam Glucksberg. 2001. Understanding figurative lan-
guage: From metaphors to idioms. Oxford: Oxford
University Press.
64
Proceedings of ACL-08: HLT, pages 523?531,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Multilingual Harvesting of Cross-Cultural Stereotypes
Tony Veale
School of Computer Science
University College Dublin
Belfield, Dublin 4, Ireland
tony.veale@ucd.ie
Yanfen Hao
School of Computer Science
University College Dublin
Belfield, Dublin 4, Ireland
yanfen.hao@ucd.ie
Guofu Li
School of Computer Science
University College Dublin
Belfield, Dublin 4, Ireland
li.guofu.l@gmail.com
Abstract
People rarely articulate explicitly what a na-
tive speaker of a language is already assumed
to know. So to acquire the stereotypical
knowledge that underpins much of what is
said in a given culture, one must look to what
is implied by language rather than what is
overtly stated. Similes are a convenient ve-
hicle for this kind of knowledge, insofar as
they mark out the most salient aspects of the
most frequently evoked concepts. In this pa-
per we perform a multilingual exploration of
the space of common-place similes, by min-
ing a large body of Chinese similes from the
web and comparing these to the English sim-
iles harvested by Veale and Hao (2007). We
demonstrate that while the simile-frame is in-
herently leaky in both languages, a multilin-
gual analysis allows us to filter much of the
noise that otherwise hinders the knowledge
extraction process. In doing so, we can also
identify a core set of stereotypical descrip-
tions that exist in both languages and accu-
rately map these descriptions onto a multilin-
gual lexical ontology like HowNet. Finally,
we demonstrate that conceptual descriptions
that are derived from common-place similes
are extremely compact and predictive of onto-
logical structure.
1 Introduction
Direct perception of our environment is just one
of the ways we can acquire knowledge of the
world. Another, more distinctly human approach,
is through the comprehension of linguistic descrip-
tions of another person?s perceptions and beliefs.
Since computers have limited means of human-like
perception, the latter approach is also very much
suited to the automatic acquisition of world knowl-
edge by a computer (see Hearst, 1992; Charniak and
Berland, 1999; Etzioni et al, 2004; Vo?lker et al,
2005; Almuhareb and Poesio, 2005; Cimiano and
Wenderoth, 2007; Veale and Hao, 2007). Thus, by
using the web as a distributed text corpus (see Keller
et al, 2002), a multitude of facts and beliefs can
be extracted, for purposes ranging from question-
answering to ontology population.
The possible configurations of different concepts
can also be learned from how the words denoting
these concepts are distributed; thus, a computer can
learn that coffee is a beverage that can be served hot
or cold, white or black, strong or weak and sweet
or bitter (see Almuhareb and Poesio, 2005). But it
is difficult to discern from these facts the idealized
or stereotypical states of the world, e.g., that one ex-
pects coffee to be hot and beer to be cold, so that if
one spills coffee, we naturally infer the possibilities
of scalding and staining without having to be told
that the coffee was hot or black; the assumptions
of hotness and blackness are just two stereotypical
facts about coffee that we readily take for granted.
Lenat and Guha (1990) describe these assumed facts
as residing in the white space of a text, in the body
of common-sense assumptions that are rarely articu-
lated as explicit statements. These culturally-shared
common-sense beliefs cannot be harvested directly
from a single web resource or document set, but
must be gleaned indirectly, from telling phrases that
are scattered across the many texts of the web.
Veale and Hao (2007) argue that the most pivotal
523
reference points of this world-view can be detected
in common-place similes like ?as lazy as a dog?, ?as
fat as a hippo? or ?as chaste as a nun?. To the extent
that this world-view is ingrained in and influenced
by how we speak, it can differ from culture to cul-
ture and language to language. In English texts, for
example, the concept Tortoise is stereotypically as-
sociated with the properties slowness, patience and
wrinkled, but in Chinese texts, we find that the same
animal is a model of slowness, ugliness, and nutri-
tional value. Likewise, because Chinese ?wine? has
a high alcohol content, the dimension of Strength is
much more salient to a Chinese speaker than an En-
glish speaker, as reflected in how the word? is used
in statements such as??????, which means
?as strong as wine?, or literally, ?as wine equally
strong?.
In this paper, we compare the same web-based
approach to acquiring stereotypical concept descrip-
tions from text using two very different languages,
English and Chinese, to determine the extent to
which the same cross-cultural knowledge is un-
earthed for each. In other words, we treat the web as
a large parallel corpus (e.g., see Resnick and Smith,
2003), though not of parallel documents in dif-
ferent languages, but of corresponding translation-
equivalent phrases. By seeking translation equiva-
lence between different pieces of textually-derived
knowledge, this paper addresses the following ques-
tions: if a particular syntagmatic pattern is useful for
mining knowledge in English, can its translated form
be equally useful for Chinese? To what extent does
the knowledge acquired using different source lan-
guages overlap, and to what extent is this knowledge
language- (and culture-) specific? Given that the
syntagmatic patterns used in each language are not
wholly unambiguous or immune to noise, to what
extent should finding the same beliefs expressed in
two different languages increase our confidence in
the acquired knowledge? Finally, what representa-
tional synergies arise from finding these same facts
expressed in two different languages?
Given these goals, the rest of the paper as-
sumes the following structure: in section 2, we
summarize related work on syntagmatic approaches
to knowledge-acquisition; in section 3, we de-
scribe our multilingual efforts in English and Chi-
nese to acquire stereotypical or generic-level facts
from the web, by using corresponding translations
of the commonplace stereotype-establishing pattern
?as ADJ as a NOUN?; and in section 4, we describe
how these English and Chinese data-sets can be uni-
fied using the bilingual ontology HowNet (Dong and
Dong, 2006). This mapping allows us to determine
the meaning overlap in both data sets, the amount
of noise in each data set, and the degree to which
this noise is reduced when parallel translations can
be identified. In section 5 we demonstrate the
overall usefulness of stereotype-based knowledge-
representation by replicating the clustering experi-
ments of Almuhareb and Poesio (2004, 2005) and
showing that stereotype-based representations are
both compact and predictive of ontological classi-
fication. We conclude the paper with some final re-
marks in section 6.
2 Related Work
Text-based approaches to knowledge acquisition
range from the ambitiously comprehensive, in which
an entire text or resource is fully parsed and ana-
lyzed in depth, to the surgically precise, in which
highly-specific text patterns are used to eke out cor-
respondingly specific relationships from a large cor-
pus. Endeavors such as that of Harabagiu et al
(1999), in which each of the textual glosses in Word-
Net (Fellbaum, 1998) is linguistically analyzed to
yield a sense-tagged logical form, is an example of
the former approach. In contrast, foundational ef-
forts such as that of Hearst (1992) typify the latter
surgical approach, in which one fishes in a large text
for word sequences that strongly suggest a particu-
lar semantic relationship, such as hypernymy or, in
the case of Charniak and Berland (1999), the part-
whole relation. Such efforts offer high precision but
low recall, and extract just a tiny (but very useful)
subset of the semantic content of a text. The Know-
ItAll system of Etzioni et al (2004) employs the
same generic patterns as Hearst ( e.g., ?NPs such
as NP1, NP2, ...?), and more besides, to extract a
whole range of facts that can be exploited for web-
based question-answering. Cimiano and Wenderoth
(2007) also use a range of Hearst-like patterns to
find text sequences in web-text that are indicative
of the lexico-semantic properties of words; in par-
ticular, these authors use phrases like ?to * a new
524
NOUN? and ?the purpose of NOUN is to *? to
identify the agentive and telic roles of given nouns,
thereby fleshing out the noun?s qualia structure as
posited by Pustejovsky?s (1990) theory of the gener-
ative lexicon.
The basic Hearst approach has even proven use-
ful for identifying the meta-properties of concepts
in a formal ontology. Vo?lker et al (2005) show
that patterns like ?is no longer a|an NOUN? can
identify, with reasonable accuracy, those concepts
in an ontology that are not rigid, which is to say,
concepts like Teacher and Student whose instances
may at any point stop being instances of these con-
cepts. Almuhareb and Poesio (2005) use patterns
like ?a|an|the * C is|was? and ?the * of the C is|was?
to find the actual properties of concepts as they are
used in web texts; the former pattern is used to iden-
tify value features like hot, red, large, etc., while
the latter is used to identify the attribute features
that correspond to these values, such as tempera-
ture, color and size. Almuhareb and Poesio go on
to demonstrate that the values and attributes that are
found for word-concepts on the web yield a suffi-
ciently rich representation for these word-concepts
to be automatically clustered into a form resembling
that assigned by WordNet (see Fellbaum, 1998).
Veale and Hao (2007) show that the pattern ?as ADJ
as a|an NOUN? can also be used to identify the
value feature associated with a given concept, and
argue that because this pattern corresponds to that
of the simile frame in English, the adjectival fea-
tures that are retrieved are much more likely to be
highly salient of the noun-concept (the simile ve-
hicle) that is used. Whereas Almuhareb and Poe-
sio succeed in identifying the range of potential at-
tributes and values that may be possessed by a par-
ticular concept, Veale and Hao succeed in identi-
fying the generic properties of a concept as it is
conceived in its stereotypical form. As noted by
the latter authors, this results in a much smaller yet
more diagnostic feature set for each concept. How-
ever, because the simile frame is often exploited for
ironic purposes in web texts (e.g., ?as meaty as a
skeleton?), and because irony is so hard to detect,
Veale and Hao suggest that the adjective:noun pair-
ings found on the web should be hand-filtered to re-
move such examples. Given this onerous require-
ment for hand-filtering, and the unique, culturally-
loaded nature of the noise involved, we use the work
of Veale and Hao as the basis for the cross-cultural
investigation in this paper.
3 Harvesting Knowledge from Similes:
English and Chinese
Because similes are containers of culturally-
received knowledge, we can reasonably expect the
most commonly used similes to vary significantly
from language to language, especially when those
languages correspond to very different cultures.
These similes form part of the linguistic currency of
a culture which must be learned by a speaker, and
indeed, some remain opaque even to the most edu-
cated native speakers. In ?A Christmas Carol?, for
instance, Dickens (1943/1984) questions the mean-
ing of ?as dead as a doornail?, and notes: ?I might
have been inclined, myself, to regard a coffin-nail as
the deadest piece of ironmongery in the trade. But
the wisdom of our ancestors is in the simile?.
Notwithstanding the opacity of some instances of
the simile form, similes are very revealing about the
concepts one most encounters in everyday language.
In section 5 we demonstrate that concept descrip-
tions which are harvested from similes are both ex-
tremely compact and highly predictive of ontolog-
ical structure. For now, we turn to the process by
which similes can be harvested from the text of the
web. In section 3.1 we summarize the efforts of
Veale and Hao, whose database of English similes
drives part of our current investigation. In section
3.2 we describe how a comparable database of Chi-
nese similes can be harvested from the web.
3.1 Harvesting English Similes
Veale and Hao (2007) use the Google API in con-
junction with Princeton WordNet (Fellbaum, 1998)
as the basis of their harvesting system. They first
extracted a list of antonymous adjectives, such as
?hot? or ?cold?, from WordNet, the intuition being
that explicit similes will tend to exploit properties
that occupy an exemplary point on a scale. For ev-
ery adjective ADJ on this list, they then sent the
query ?as ADJ as *? to Google and scanned the
first 200 snippets returned for different noun val-
ues for the wildcard *. The complete set of nouns
extracted in this way was then used to drive a sec-
525
ond harvesting phase, in which the query ?as * as
a NOUN? was used to collect similes that employ
different adjectives or which lie beyond the 200-
snippet horizon of the original search. Based on
this wide-ranging series of core samples (of 200 hits
each) from across the web, Veale and Hao report
that both phases together yielded 74,704 simile in-
stances (of 42,618 unique types, or unique adjec-
tive:noun pairings), relating 3769 different adjec-
tives to 9286 different nouns. As often noted by
other authors, such as Vo?lker et al (2005), a pattern-
oriented approach to knowledge mining is prone to
noise, not least because the patterns used are rarely
leak-free (inasmuch as they admit word sequences
that do not exhibit the desired relationship), and be-
cause these patterns look at small text sequences in
isolation from their narrative contexts. Veale and
Hao (2007) report that when the above 42,618 simile
types are hand-annotated by a native speaker, only
12,259 were judged as non-ironic and meaningful
in a null context. In other words, just 29% of the
retrieved pairings conform to what one would con-
sider a well-formed and reusable simile that conveys
some generic aspect of cultural knowledge. Of those
deemed invalid, 2798 unique pairings were tagged
as ironic, insofar as they stated precisely the oppo-
site of what is stereotypically believed to be true.
3.2 Harvesting Chinese Similes
To harvest a comparable body of Chinese similes
from the web, we also use the Google API, in con-
junction with both WordNet and HowNet (Dong and
Dong, 2006). HowNet is a bilingual lexical ontol-
ogy that associates English and Chinese word labels
with an underlying set of approximately 100,000
lexical concepts. While each lexical concept is de-
fined using a unique numeric identifier, almost all of
HowNet?s concepts can be uniquely identified by a
pairing of English and Chinese labels. For instance,
the word ???? can mean both Tortoise and Cuck-
old in Chinese, but the combined label tortoise|??
uniquely picks out the first sense while cuckold|?
? uniquely picks out the second. Though Chi-
nese has a large number of figurative expressions,
the yoking of English to Chinese labels still serves
to identify the correct sense in almost every case.
For instance, ????? is another word for Cuck-
old in Chinese, but it can also translate as ?green
hat? and ?green scarf?. Nonetheless, green hat|?
?? uniquely identifies the literal sense of ???
?? (a green covering) while green scarf|???
and cuckold|??? both identify the same human
sense, the former being a distinctly culture-specific
metaphor for cuckolded males (in English, a dispos-
sessed lover ?wears the cuckold?s horns?; in Chi-
nese, one apparently ?wears a green scarf?).
We employ the same two-phase design as Veale
and Hao: an initial set of Chinese adjectives are
extracted from HowNet, with the stipulation that
their English translations (as given by HowNet) are
also categorized as adjectives in WordNet. We
then use the Chinese equivalent of the English sim-
ile frame ??* ??ADJ? (literally, ?as-NOUN-
equally-ADJ?) to retrieve a set of noun values that
stereotypically embody these adjectival features.
Again, a set of 200 snippets is analyzed for each
query, and only those values of the Google * wild-
card that HowNet categorizes as nouns are accepted.
In a second phase, these nouns are used to create
new queries of the form ??Noun??*? and the re-
sulting Google snippets are now scanned for adjec-
tival values of *.
In all, 25,585 unique Chinese similes (i.e., pair-
ings of an adjective to a noun) are harvested, link-
ing 3080 different Chinese adjectives to 4162 Chi-
nese nouns. When hand-annotated by a native Chi-
nese speaker, the Chinese simile frame reveals it-
self to be considerably less leaky than the corre-
sponding English frame. Over 58% of these pairings
(14,867) are tagged as well-formed and meaning-
ful similes that convey some stereotypical element
of world knowledge. The Chinese pattern ??*?
?*? is thus almost twice as reliable as the English
?as * as a *? pattern. In addition, Chinese speak-
ers exploit the simile frame much less frequently for
ironic purposes, since just 185 of the retrieved sim-
iles (or 0.7%) are tagged as ironic, compared with
ten times as many (or 7%) retrieved English similes.
In the next section we consider the extent to which
these English and Chinese similes convey the same
information.
4 Tagging and Mapping of Similes
In each case, the harvesting processes for English
and for Chinese allow us to acquire stereotypi-
526
cal associations between words, not word senses.
Nonetheless, the frequent use of synonymous terms
introduces a substantial degree of redundancy in
these associations, and this redundancy can be used
to perform sense discrimination. In the case of En-
glish similes, Veale and Hao (2007) describe how
two English similes ?as A as N1? and ?as A as
N2? will be mutually disambiguating if N1 and
N2 are synonyms in WordNet, or if some sense
of N1 is a hypernym or hyponym of some sense
of N2 in WordNet. This heuristic allows Veale
and Hao to automatically sense-tag 85%, or 10,378,
of the unique similes that are annotated as valid.
We apply a similar intuition to the disambiguation
of Chinese similes: though HowNet does not sup-
port the notion of a synset, different word-senses
that have the same meaning will be associated with
the same logical definition. Thus, the Chinese
word ???? can translate as ?celebrated?, ?fa-
mous?, ?well-known? and ?reputable?, but all four
of these possible senses, given by celebrated|??,
famous|??, well-known|?? and reputable|?
?, are associated with the same logical form in
HowNet, which defines them as a specialization of
ReputationValue|???. This allows us to safely
identify ???? with this logical form. Overall, 69%
of Chinese similes can have both their adjective and
noun assigned to specific HowNet meanings in this
way.
4.1 Translation Equivalence Among Similes
Since HowNet represents an integration of English
and Chinese lexicons, it can easily be used to con-
nect the English and Chinese data-sets. For while
the words used in any given simile are likely to
be ambiguous (in the case of one-character Chinese
words, highly so), it would seem unlikely that an
incorrect translation of a web simile would also be
found on the web. This is an intuition that we can
now use the annotated data-sets to evaluate.
For every English simile of the form <Ae as
Ne>, we use HowNet to generate a range of possible
Chinese variations <Ac0 as Nc0>, <Ac1 as Nc0>,
<Ac0 as Nc1>, <Ac1 as Nc1>, ... by using the
HowNet lexical entries Ae|Ac0, Ae|Ac1, ..., Ne|Nc0,
Ne|Nc1, ... as a translation bridge. If the variation
<Aci as Ncj> is found in the Chinese data-set, then
translation equivalence is assumed between <Ae as
Language Precision Recall F1
English 0.76 0.25 0.38
Chinese 0.82 0.27 0.41
Table 1: Automatic filtering of similes using Translation
Equivalence.
Ne> and <Aci as Ncj>; furthermore, Ae|Aci is as-
sumed to be the HowNet sense of the adjectives Ae
and Aci while Ncj is assumed to be the HowNet
sense of the nouns Ne and Ncj . Sense-tagging is
thus a useful side-effect of simile-mapping with a
bilingual lexicon.
We attempt to find Chinese translation equiva-
lences for all 42,618 of the English adjective:noun
pairings harvested by Veale and Hao; this includes
both the 12,259 pairings that were hand-annotated as
valid stereotypical facts, and the remaining 30,359
that were dismissed as noisy or ironic. Using
HowNet, we can establish equivalences from 4177
English similes to 4867 Chinese similes. In those
mapped, we find 3194 English similes and 4019
Chinese similes that were hand-annotated as valid
by their respective native-speaker judges. In other
words, translation equivalence can be used to sep-
arate well-formed stereotypical beliefs from ill-
formed or ironic beliefs with approximately 80%
precision. The precise situation is summarized in
Table 1.
As noted in section 3, just 29% of raw English
similes and 58% of raw Chinese similes that are har-
vested from web-text are judged as valid stereotyp-
ical statements by a native-speaking judge. For the
task of filtering irony and noise from raw data sets,
translation equivalence thus offers good precision
but poor recall, since most English similes appear
not to have a corresponding Chinese variant on the
web. Nonetheless, this heuristic allows us to reliably
identify a sizeable body of cross-cultural stereotypes
that hold in both languages.
4.1.1 Error Analysis
Noisy propositions may add little but empty con-
tent to a representation, but ironic propositions will
actively undermine a representation from within,
leading to inferences that are not just unlikely, but
patently false (as is generally the intention of irony).
Since Veale and Hao (2007) annotate their data-
527
set for irony, this allows us to measure the number
of egregious mistakes made when using translation
equivalence as a simile filter. Overall, we see that
1% of Chinese similes that are accepted via transla-
tion equivalence are ironic, accounting for 9% of all
errors made when filtering Chinese similes. Like-
wise, 1% of the English similes that are accepted are
ironic, accounting for 5% of all errors made when
filtering English similes.
4.2 Representational Synergies
By mapping WordNet-tagged English similes onto
HowNet-tagged Chinese similes, we effectively ob-
tain two representational viewpoints onto the same
shared data set. For instance, though HowNet
has a much shallower hierarchical organization
than WordNet, it compensates by encapsulating the
meaning of different word senses using simple log-
ical formulae of semantic primitives, or sememes,
that are derived from the meaning of common Chi-
nese characters. WordNet and HowNet thus offer
two complementary levels or granularities of gen-
eralization that can be exploited as the context de-
mands.
4.2.1 Adjective Organization
Unlike WordNet, HowNet organizes its adjec-
tival senses hierarchically, allowing one to obtain
a weaker form of a given description by climb-
ing the hierarchy, or to obtain a stronger form by
descending the hierarchy from a particular sense.
Thus, one can go up from kaleidoscopic|???
? to colored|?, or down from colored|? to
any of motley|??, dappled|??, prismatic|??
?? and even gorgeous|??. Once stereotypi-
cal descriptions have been sense-tagged relative to
HowNet, they can easily be further enhanced or
bleached to suit the context of their use. For exam-
ple, by allowing a Chinese adjective to denote any
of the senses above it or below in the HowNet hi-
erarchy, we can extend the mapping of English to
Chinese similes so as to achieve an improved recall
of .36 (though we note that this technique reduces
the precision of the translation-equivalence heuristic
to .75).
As demonstrated by Almuhareb and Poesio
(2004), the best conceptual descriptions combine
adjectival values with the attributes that they fill.
Because adjectival senses hook into HowNet?s up-
per ontology via a series of abstract taxonyms like
TasteValue|???, ReputationValue|??? and
AmountValue|???, a taxonym of the form At-
tributeValue can be identified for every adjective
sense in HowNet. For example, the English ad-
jective ?beautiful? can denote either beautiful|?,
organized by HowNet under BeautyValue|??
?, or beautiful|?, organized by HowNet un-
der gracious|? which in turn is organized under
GraceValue|???. The adjective ?beautiful? can
therefore specify either the Grace or Beauty at-
tributes of a concept. Once similes have been sense-
tagged, we can build up a picture of most salient at-
tributes of our stereotypical concepts. For instance,
?peacock? similes yield the following attributes via
HowNet: Beauty, Appearance, Color, Pride, Be-
havior, Resplendence, Bearing and Grace; likewise
?demon? similes yield the following: Morality, Be-
havior, Temperament, Ability and Competence.
4.2.2 Orthographic Form
The Chinese data-set lacks counterparts to many
similes that one would not think of as culturally-
determined, such ?as red as a ruby?, ?as cruel as
a tyrant? and ?as smelly as a skunk?. One signifi-
cant reason for this kind of omission is not cultural
difference, but obviousness: many Chinese words
are multi-character gestalts of different ideas (see
Packard, 2000), so that these ideas form an explicit
part of the orthography of a lexical concept. For in-
stance, using HowNet, we can see that skunk|??
is actually a gestalt of the concepts smelly|? and
weasel|?, so the simile ?as smelly as a skunk? is
already somewhat redundant in Chinese (somewhat
akin to the English similes ?as hot as a hotdog? or
?as hard as a hardhat?).
Such decomposition can allow us to find those
English similes that are already orthographically ex-
plicit in Chinese word-forms. We simply look for
pairs of HowNet senses of the form Noun|XYZ and
Adj|X, where X and XYZ are Chinese words and the
simile ?as Adj as a|an Noun? is found in the English
simile set. When we do so, we find that 648 English
similes, from ?as meaty as a steak? to ?as resonant
as a cello?, are already fossilized in the orthographic
realization of the corresponding Chinese concepts.
When fossilized similes are uncovered in this way,
528
the recall of translation equivalence as a noise filter
rises to .29, while its precision rises to .84 (see Table
1)
5 Empirical Evaluation: Simile-derived
Representations
Stereotypes persist in language and culture because
they are, more often than not, cognitively useful:
by emphasizing the most salient aspects of a con-
cept, a stereotype acts as a dense conceptual descrip-
tion that is easily communicated, widely shared,
and which supports rapid inference. To demonstrate
the usefulness of stereotype-based concept descrip-
tions, we replicate here the clustering experiments
of Almuhareb and Poesio (2004, 2005), who in turn
demonstrated that conceptual features that are mined
from specific textual patterns can be used to con-
struct WordNet-like ontological structures. These
authors used different text patterns for mining fea-
ture values (like hot) and attributes (like tempera-
ture), and their experiments evaluated the relative ef-
fectiveness of each as a means of ontological cluster-
ing. Since our focus in this paper is on the harvesting
of feature values, we replicate here only their exper-
iments with values.
Almuhareb and Poesio (2004) used as their ex-
perimental basis a sampling of 214 English nouns
from 13 of WordNet?s upper-level semantic cate-
gories, and proceeded to harvest adjectival features
for these noun-concepts from the web using the tex-
tual pattern ?[a | an | the] * C [is |was]?. This pattern
yielded a combined total of 51,045 value features
for these 214 nouns, such as hot, black, etc., which
were then used as the basis of a clustering algorithm
in an attempt to reconstruct the WordNet classifica-
tions for all 214 nouns. Clustering was performed
by the CLUTO-2.1 package (Karypis, 2003), which
partitioned the 214 nouns in 13 categories on the ba-
sis of their 51,045 web-derived features. Compar-
ing these clusters with the original WordNet-based
groupings, Almuhareb and Poesio report a cluster-
ing accuracy of 71.96%. In a second, larger exper-
iment, Almuhareb and Poesio (2005) sampled 402
nouns from 21 different semantic classes in Word-
Net, and harvested 94,989 feature values from the
web using the same textual pattern. They then ap-
plied the repeated bisections clustering algorithm to
Approach accuracy features
Almuhareb + Poesio 71.96% 51,045
Simile-derived stereotypes 70.2% 2,209
Table 2: Results for experiment 1 (214 nouns, 13 WN
categories).
Approach Cluster Cluster features
purity entropy
Almu. + Poesio
(no filtering) 56.7% 38.4% 94,989
Almu. + Poesio
(with filtering) 62.7% 33.8% 51345
Simile-derived
stereotypes
(no filtering) 64.3% 33% 5,547
Table 3: Results for experiment 2 (402 nouns, 21 WN
categories).
this larger data set, and report an initial cluster purity
measure of 56.7%. Suspecting that a noisy feature
set had contributed to the apparent drop in perfor-
mance, these authors then proceed to apply a variety
of noise filters to reduce the set of feature values to
51,345, which in turn leads to an improved cluster
purity measure of 62.7%.
We replicated both of Almuhareb and Poesio?s
experiments on the same experimental data-sets (of
214 and 402 nouns respectively), using instead the
English simile pattern ?as * as a NOUN? to harvest
features for these nouns from the web. Note that
in keeping with the original experiments, no hand-
tagging or filtering of these features is performed, so
that every raw match with the simile pattern is used.
Overall, we harvest just 2209 feature values for the
214 nouns of experiment 1, and 5547 features for the
402 nouns of experiment 2. A comparison of both
sets of results for experiment 1 is shown is Table 2,
while a comparison based on experiment 2 is shown
is Table 3.
While Almuhareb and Poesio achieve marginally
higher clustering on the 214 nouns of experiment 1,
they do so by using over 20 times as many features.
529
In experiment 2, we see a similar ratio of feature
quantities before filtering; after some initial filtering,
Almuhareb and Poesio reduce their feature set to just
under 10 times the size of the simile-derived feature
set.
These experiments demonstrate two key points
about stereotype-based representations. First, the
feature representations do not need to be hand-
filtered and noise-free to be effective; we see from
the above results that the raw values extracted
from the simile pattern prove slightly more effec-
tive than filtered feature sets used by Almuhareb and
Poesio. Secondly, and perhaps more importantly,
stereotype-based representations prove themselves a
much more compact means (by factor of 10 to 20
times) of achieving the same clustering goals.
6 Conclusions
Knowledge-acquisition from texts can be a process
fraught with complexity: such texts - especially
web-based texts - are frequently under-determined
and vague; highly ambiguous, both lexically and
structurally; and dense with figures of speech, hy-
perbolae and irony. None of the syntagmatic frames
surveyed in section 2, from the ?NP such as NP1,
NP2 ...? pattern of Hearst (1992) and Etzioni et al
(2004) to the ?no longer NOUN? pattern of Vo?lker
et al (2005), are leak-free and immune to noise.
Cimiano and Wenderoth (2007) mitigate this prob-
lem somewhat by performing part-of-speech anal-
ysis on all extracted text sequences, but the prob-
lem remains: the surgical, pattern-based approach
offers an efficient and targeted means of knowledge-
acquisition from corpora because it largely ignores
the context in which these patterns occur; yet one
requires this context to determine if a given text se-
quence really is a good exemplar of the semantic re-
lationship that is sought.
In this paper we have described how stereotyp-
ical associations between adjectival properties and
noun concepts can be mined from similes in web
text. When harvested in both English and Chi-
nese, these associations exhibit two kinds of re-
dundancy that can mitigate the problem of noise.
The first kind, within-language redundancy, allows
us to perform sense-tagging of the adjectives and
nouns that are used in similes, by exploiting the
fact that the same stereotypical association can oc-
cur in a variety of synonymous forms. By recog-
nizing synonymy between the elements of different
similes, we can thus identify the underlying senses
(or WordNet synsets) in these similes. The sec-
ond kind, between-language redundancy, exploits
the fact that the same associations can occur in dif-
ferent languages, allowing us to exploit translation-
equivalence to pin these associations to particular
lexical concepts in a multilingual lexical ontology
like HowNet. While between-language redundancy
is a limited phenomenon, with just 26% of Veale
and Hao?s annotated English similes having Chinese
translations on the web, this phenomenon does allow
us to identify a significant core of shared stereotyp-
ical knowledge across these two very different lan-
guages.
Overall, our analysis suggests that a comparable
number of well-formed Chinese and English similes
can be mined from the web (our exploration finds
approx. 12,000 unique examples of each). This
demonstrates that harvesting stereotypical knowl-
edge from similes is a workable strategy in both lan-
guages. Moreover, Chinese simile usage is charac-
terized by two interesting facts that are of some prac-
tical import: the simile frame ??NOUN??ADJ?
is a good deal less leaky and prone to noise than the
equivalent English frame, ?as ADJ as a NOUN?; and
Chinese speakers appear less willing to subvert the
stereotypical norms of similes for ironic purposes.
Further research is needed to determine whether
these observations generalize to other knowledge-
mining patterns.
References
A. Almuhareb and M. Poesio. 2004. Attribute-Based and
Value-Based Clustering: An Evaluation. In proceed-
ings of EMNLP 2004, pp 158?165. Barcelona, Spain.
A. Almuhareb and M. Poesio. 2005. Concept Learning
and Categorization from the Web. In proceedings of
CogSci 2005, the 27th Annual Conference of the Cog-
nitive Science Society. New Jersey: Lawrence Erl-
baum.
C. Dickens. 1843/1981. A Christmas Carol. Puffin
Books, Middlesex, UK.
C. Fellbaum. 1998. WordNet, an electronic lexical
database. MIT Press.
E. Charniak and M. Berland. 1999. Finding parts in
530
very large corpora. In proceedings of the 37th Annual
Meeting of the ACL, pp 57-64.
F. Keller, M. Lapata, and O. Ourioupina. 2002. Using
the web to overcome data sparseness. In proceedings
of EMNLP-02, pp 230-237.
F. Keller, M. Lapata, and O. Ourioupina. 1990. Building
large knowledge-based systems: representation and
inference in the Cyc project. Addison-Wesley.
G. Karypis. 2003. CLUTO: A clustering toolkit. Univer-
sity of Minnesota.
J. L. Packard. 2000. The Morphology of Chinese: A
Linguistic and Cognitive Approach. Cambridge Uni-
versity Press, UK.
J. Pustejovsky. 1991. The generative lexicon. Computa-
tional Linguistics 17(4), pp 209-441.
J. Vo?lker, D. Vrandecic and Y. Sure. 2005. Automatic
Evaluation of Ontologies (AEON). In Y. Gil, E. Motta,
V. R. Benjamins, M. A. Musen, Proceedings of the 4th
International Semantic Web Conference (ISWC2005),
volume 3729 of LNCS, pp. 716-731. Springer Verlag
Berlin-Heidelberg.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In proceedings of the 14th
intenatinal conference on Computational Linguistics,
pp 539-545.
O. Etzioni, S. Kok, S. Soderland, M. Cafarella, A-M.
Popescu, D. Weld, D. Downey, T. Shaked and A.
Yates. 2004. Web-scale information extraction in
KnowItAll (preliminary results). In proceedings of the
13th WWW Conference, pp 100-109.
P. Cimiano and J. Wenderoth. 2007. Automatic Acqui-
sition of Ranked Qualia Structures from the Web. In
proceedings of the 45th Annual Meeting of the ACL,
pp 888?895.
P. Resnik and N. A. Smith. 2003. The Web as a parallel
corpus. Computational Linguistics, 29(3),pp 349-380.
S. Harabagiu, G. Miller and D. Moldovan. 1999. Word-
Net2 - a morphologically and semantically enhanced
resource. In proceedings of SIGLEX-99, pp 1-8, Uni-
versity of Maryland.
T. Veale and Y. Hao. 2007. Making Lexical Ontologies
Functional and Context-Sensitive. In proceedings of
the 45th Annual Meeting of the ACL, pp 57-64.
Z. Dong and Q. Dong. 2006. HowNet and the Computa-
tion of Meaning. World Scientific: Singapore.
531
Systematicity and the Lexicon in Creative Metaphor
Tony Veale
Department of Computer Science,
University College Dublin, Belfield, Dublin 6, Ireland.
Tony.veale@UCD.ie
Abstract
Aptness is an umbrella term that covers a 
multitude of issues in the interpretation and 
generation of creative metaphor. In this paper 
we concentrate on one of these issues ?  the 
notion of lexical systematicity  ?   and explore 
its role in ascertaining the coherence of creative 
metaphor relative to the structure of the target 
concept being described. We argue that all else 
being equal, the most apt metaphors are those 
that resonate most with the way the target 
concept is literally and metaphorically 
organized. As such, the lexicon plays a key role 
in enforcing and recognizing aptness, insofar as 
this existing organization will already have 
been lexicalized. We perform our exploration in 
the context of WordNet, and describe how 
relational structures can be automatically 
extracted from this lexical taxonomy to 
facilitate the interpretation of creative 
metaphors.
1   Introduction
When one considers the aptness of creative 
metaphor and how one might measure it, one finds 
a whole range of issues lurking between the 
apparent unity of this umbrella term. This 
complexity is compounded by the fact that 
metaphors operate at several different levels of 
representation simultaneously: the conceptual 
level, or the level of ideas; the lexical level, or the 
level of words; and the pragmatic level, or the 
level of intentions. A metaphor may fall at any of 
these hurdles, either through a poor choice of a 
source concept, a poor choice of words in 
communicating this concept, or in a failure to 
observe the expectations of the context in which 
the metaphor is expressed.
Some degree of aptness is afforded by 
metaphors that compare semantic neighbors, 
inasmuch as the existence of a common taxonomic 
parent suggests that the source and target are in the 
same, or at least similar, domains (e.g., see Way, 
1991). For instance, metaphors that compare 
politicians to architects, or even geneticists to 
cartographers, derive some measure of aptness 
from the fact that in each case the source and target 
are sub-categories of the Profession category. 
However, since the most creative of metaphors are 
those that make the greatest semantic leaps 
between the source and target concepts, such 
category-hopping metaphors do not have the 
luxury of comparing concepts that are already 
deemed similar in taxonomic terms, as evidenced 
by a common superordinate concept, but must 
instead establish a new basis for conveying 
similarity that is not itself taxonomic. Consider for 
instance a corollary of the above metaphor in 
which ? genomes are maps? . The aptness of these 
similarity-creating metaphors is instead a measure 
of the isomorphism between the relational 
structures of the source and target, so that the 
concepts with the greatest structural overlap will 
often produce the most apt metaphors. In this 
respect, metaphoric aptness is a function of what 
Gentner terms the systematicity of a structure-
mapping. According to (Gentner, 1983) and the 
structure-mapping school of thought (e.g., see also 
Veale and Keane, 1997), the best interpretations of 
a metaphor or analogy are those that systematically 
pair-off the greatest amount of connected relational 
structure in each concept. We refer to this kind of 
structural aptness as internal systematicity, since 
any sense of aptness arises out of a coherence 
between the internal structures of the concepts 
being mapped. 
Lakoff and Johnson (1980) also place a strong 
emphasis on metaphoric systematicity, but in their 
hands the notion is construed in more external
terms. To L&J, systematicity is a measure of the 
generativity of a metaphoric schema, so that the 
same schema (such as Life is a Journey) can serve 
as the deep structure for a wide variety of different, 
but mutually systematic, surface metaphors (such 
as ? my job has hit a rocky patch?  and ? my career 
has stalled? ). In this view, systematicity is a 
measure of how much a metaphor resonates and 
coheres with existing metaphors for thinking about 
the target concept, so that when viewed 
collectively, they together suggest the operation of 
a common underlying schema. This view of 
systematicity is external to the concepts involved 
since it predicates their aptness to each other on 
the existence of other structures (metaphor 
schemas) into which they can be coherently 
connected.
In this paper we argue that the lexicon is central 
to the determination of both kinds of systematicity, 
internal and external, especially if one is an 
adherent of the generative lexicon view of word 
meaning as championed by (Pustejovsky, 1991). In 
such a lexicon we can expect to find precisely the 
kind of relational structure needed to perform 
structure mapping and thereby measure the internal 
systematicity of a metaphor like ? a passport is a 
travel diary? . In addition, we can expect to find the 
lexicalized metaphor structures that represent the 
surface manifestations of existing modes of 
thought, and it is against these structures that the 
external systematicity of an interpretation can be 
measured. 
This research is conducted in the context of 
WordNet (Miller, 1995; Fellbaum, 1998), a 
comprehensive lexical knowledge-base of English. 
The structure of WordNet makes explicit some of 
the relationships needed to construct a generative 
lexicon, most obviously the formal (taxonomic) 
and constitutive (meronymic) aspects of word 
meaning. But to truly test a model of metaphoric 
interpretation on a large-scale, it is necessary to 
augment these relationships with the telic and 
agentive components that are not encoded directly 
but merely alluded to in the textual glosses 
associated with each sense entry. In the sections to 
follow we describe a mechanism for automating 
the extraction of these relationships (in the same 
vein as (Harabagiu et al 1999), and for using them 
to generative apt interpretations for metaphors 
involving WordNet entries.
2   Qualia Extraction from Glosses
In a generative lexicon, the core elements of word 
meaning are represented by a nexus of relations 
called a qualia structure, which ties together the 
formal (i.e., hierarchical relations), constitutive 
(i.e., meronymic), telic (i.e., functional) and 
agentive (i.e., construction/creation) aspects of a 
word. For instance, a diary is formally a kind of 
?book? that constitutes a ?collection of personal 
writings? whose telic purpose is to ?record? the 
observations of the agent that ?compiles? it. When 
a word like ? diary?  is used metaphorically, this 
relational nexus provides the structure for 
determining the internal systematicity of any 
interpretation. For instance, it is apt to describe a 
passport as a kind of travel diary since both are 
kinds of book (formal) that record (telic) travel 
experiences.We describe here an approach to qualia extraction from WordNet glosses that balances coverage with quality: by attempting to extract a relatively narrow slice of the relational structure inherent in WordNet glosses, we can be confident of quite high levels of competence. Nevertheless, even 
this narrow slice yields a significant amount of qualia structure, since WordNet aleady encodes formal and constitutive relations in its taxonomic and meronymic links between synsets. We thus concentrate our efforts on the extraction of telic (i.e., goal-oriented) and agentive (activity-oriented) lexical relations. We exploit the fact that the agentive and telic aspects of lexico-conceptual structure are often expressed using nominalized verbs that implicitly encode relational structure. A small number of 
highly productive morphology rules1 can thus be used to connect ?observe? to ?observer? and ?observation? (and vice versa), ?specialize?, to ?specializer? and ?specialization?, and so on. For example, the WordNet concepts  {botanist} and {philologist} are both defined with glosses that explicitly employ the term ?specializing?, thus evoking the concept {specializer} (a hyponym of {expert}) Now, because {specializer} is compatible with the concepts {botanist} and {philologist} by virtue of being a hyponym of {person}, this in turn suggests that {botanist} and {philologist}  should be seen as hyponyms of {specializer}, making specializer_of is an appropriate telic relation for each. Thus, using a combination of derivational morphology and simple taxonomic reasoning, the relational structure specializer_of:specializationcan be associated with each concept. Since this structure is not already encoded in WordNet, it provides an additional dimension of similarity in any metaphoric mapping.  Broad clues as to the syntactic form of the gloss (such as the use of the passive voice) are also a valuable source of extraction information, especially when they can be robustly inferred from a simple combination of keyword analysis and inflectional morphology. For example, the passive voice should cause an extracted relation to be inverted, as in the case of {dupe}, whose WordNet gloss is ?a person who is swindled or tricked?. The resulting relational structure is thus:
                                                       
1
 The developers of WordNet have recently announced that 
hand-coded morpho-semantic links will be added to future 
versions of WordNet, to make explicit the relationship 
between verbs and their nominal forms, thus obviating the 
need for such rules while making the extraction task even 
more reliable.
  {dupe} ?  of_swindler:swindler ?  of_trickster:trickster
Note that the extraction process is too shallow 
to do very much with the disjunctive ? or?  present 
in the gloss of {dupe}, as this is more a process of 
information extraction than full natural-language 
parsing. Thus, it simply conjoins any relationship 
that can be reliably extracted with morphological 
cues into an overall relational structure. This 
structure is simply a bag of relations at present, 
which we choose to present here as connected via 
conjunction. Future versions of the extraction 
process may attempt to impose a more elaborate 
inter-connecting structure on the relationships that 
are extracted, but for the present, an unstructured 
bag is sufficient to support a consideration of 
metaphor in WordNet. 
Since morphology alone is not a sufficiently 
reliable guide for extraction purposes, the approach 
crucially requires the WordNet taxonomy to act as 
a vital sanity-check for any extracted relationship. In general, it is sensible to associate a relation r with a concept c if the nominalization of r denotes a concept that belongs to the same taxonomic category as c; thus, it is sensible to ascribe a specializer_of relation to {botanist} only because {specializer} and {botanist} each specify a sub-category of {person}.  However, this broad injunction finds an important exception in metonymic contexts. Consider the WordNet gloss for {diary, journal},  ?a daily record of (usually private) experiences and observations?, which yields the extracted relationships of_diarist:diarist, of_experience: experience, recorder_of:recordingand observer_of:observation. A taxonomic sanity-check reveals that {diary, journal}, as a sub-category of {communication}, is not compatible with either {recorder} or {observer}, both sub-categories of {person}. However, it is taxonomically compatible with the objects of these relations, {recording} and {observation}, which suggests that a diary is both the object of, and a metonym for, the diarist as observer and recorder. This metonymy is most evident in the familiar address ?dear diary?, in which the diary is conceived as a personified counterpart of the observer. The concept {diary, journal} therefore yields the modified relational structure:
{diary, journal} ? *observer_of:observation
? *recorder_of:recording
? of_experience:experience
The (*) here signals that the observer_of and recorder_of relations hold metonymically rather than literally. The presence of these relationships facilitate creative uses of the concept {diary} that follow the general pattern whereby artifacts are viewed from an intentional stance. For example, consider that the WordNet gloss for the concept {witness, spectator} is ?a close observer?, so that the following relational structure is extracted:  
    {witness, spectator}    ? observer_of:observation
It now becomes apt to metaphorically consider a diary to be a witness to one?s life experiences. In structure-mapping terms, this aptness is reflected in the internal systematicity of finding a key relationship,  observer_of:observation, common to each of the concepts {diary} and {witness, spectator}.
3   Internal Systematicity
Because purely taxonomic interpretations are 
created on the basis of commonalities, they tend to 
be highly symmetric, as in the case of similes such 
as ? credit unions are like banks?  and ? gamblers are 
like alcoholics? . In contrast, the most creative 
metaphors are asymmetric (Ortony, 1991), since 
they impose the highly-developed relational 
structure of the source concept onto that of  the 
less-developed target (see Lakoff and Johnson, 
1980; Gentner, 1983; Veale and Keane, 1997). 
Without this imposition of relational structure, 
metaphor can be used only to highlight existing 
similarities rather than to actually create new ones, 
and is thus robbed of its creative function. The projection of relational structure can be performed either literally or figuratively. In a literal interpretation, the relational structure of the source is simply instantiated with the target concept, so for example, a literal ?travel diary? is a diary that contains travel recordings and travel observations. In contrast, figurative interpretations first attempt to find a target domain correspondence for the 
source concept, and then project the relational structure of the source onto this counterpart (Gentner, 1983). For instance, WordNet contains a variety of concepts that are formally similar to {diary, journal} and which also mention ?travel? in their glosses, such as {travel_guidebook} and {passport}.
?travel?+  {diary, journal}   ?     {passport}  +  *observer_of:travel:observation
    ?  *recorder_of:travel:recording
?  of_experience:travel:experience
Projecting the relational structure of {diary, journal} onto {passport} causes the latter to be seen as a journal of travel observations and experiences, and indeed, many travelers retain old passports for this very purpose. Metaphors are most apt when projection highlights a latent relational structure that already exists in the target concept (Ortony, 1979). For example, the compound ?pastry surgeon? can be understood taxonomically as referring to {pastry_cook},  since like {surgeon} it is a sub-category of {person}. But to fully appreciate why {surgeon} is more apt than other hyponyms of {person}, like {astrologer} say, one must look to the shared relational structure that is highlighted by the metaphor. WordNet 1.6 defines a surgeon as a ?physician who specializes in surgery?, while a pastry cook is glossed as ?a chef who specializes in pastry?. Both {surgeon} and {pastry_cook} thus become associated with the relationship specializer_of:specialism. This common relational structure facilitates the measurement of what we have termed ?internal systematicity? (in the Gentner sense). Thus, {surgeon} is seen as an apt vehicle for {pastry_cook} as both are people that specialize in a particular field. Instantiation of the shared structure leads to the following interpretation:
?pastry? + {surgeon} ?
{pastry_cook} + specializer_of: pastry:surgery
One can reasonably argue that much more 
sophisticated interpretations are available to 
human readers of this metaphor, e.g., that pastry 
cooking and surgery are both delicate operations 
involving special training, both are performed with 
specialized instruments in very clean surroundings, 
etc. But given the inherent limitations of working 
with an existing semi-structured knowledge source 
such as WordNet, as opposed to a dedicated, hand-
crafted knowledge-base, ? pastry specialist?  must 
suffice as a generalization for these richer 
interpretations. Alternately, one might argue that it 
is ? pastry?  rather than ? surgeon?  that undergoes 
metaphoric reinterpretation, so that the phrase 
denotes a literal surgeon that operates on 
metaphoric pastries, such as movie starlets or 
supermodels. In this current work we choose to 
focus on the relational potential for the head word 
to metaphorically denote a relationally similar, if 
sometimes semantically distant, referent, while 
acknowledging that this illuminates just one part 
of the picture.
Nonetheless, interpretations like ? pastry 
specialist?  can be given more credibility if one 
delves deeper into its metaphoric ramifications to 
consider the recursive sub-metaphors that it 
implies. For instance, as stated in the analysis 
above,  ? pastry surgeon?  implies the plausibility of 
a meaningful interpretation for ? pastry surgery? . 
This choice to delve deeper, and recursively 
determine an appropriate interpretation of ? pastry 
surgery? , is left to the comprehender, who may 
instead choose to read the metaphor as a simple 
request to view pastry chefs as specialists. But this 
raises the question of how much structure must be 
shared for an interpretation to appear apt rather 
than merely inept. For example, one can equally 
well say ? pastry linguist?  or ? pastry geologist?  to 
highlight the specialist nature of pastry chefs, since 
{geologist}  and {linguist} are also associated with 
an extracted specializer_of relationship. What 
makes these alternate metaphors seem clumsy is 
the difficulty in assigning appropriate 
interpretations to the recursive metaphors that they 
imply: ? pastry geologist?  implies the metaphor 
? pastry geology? , while ? pastry linguist?  implies 
the metaphor ? pastry linguistics? .
(?)  ?pastry? + {linguist} ?{pastry_cook} + specializer_of:pastry:linguistics
There is little that can be done to put a sensible interpretation on ?pastry linguistics? in WordNet, given the taxonomic and relational structure of {pastry} and {linguistics}. In contrast, ?pastry surgery? has more potential for meaningful interpretation using WordNet structures. There exists a sense of surgery that denotes a discipline in the natural sciences, and from {pastry} a broad search will find the concept {dietetics}, another discipline of the natural sciences dedicated to food preparation. This analogue of {surgery} can be found by first considering all concepts associated with ?pastry?, then all concepts associated with ?baked goods?, then ?foodstuff? and ?food?, until an appropriately similar candidate is found.
   {dietetics}    ?   the scientific study of food preparation and intake
This is not a particularly well-known concept, so it would be difficult to argue that this forms the cornerstone of an easily understood metaphor like ?pastry surgeon?. However, the concept {dietetics}does at least concretize, in WordNet terms, the idea that one can take a precise, scientific view of food preparation, and it is the plausibility of this notion that allows us to make sense of pastry preparation as a surgical activity. There is no true substitute for situated experience of the world, but when it comes to metaphor interpretation using lexical resources like WordNet, we should be willing to use any lexical precedent we can find.As an alternate strategy, we can seek to recruit a sub-category of surgery that can be modified in some way to accommodate the concept {pastry}. One such category is {plastic_surgery}, whose gloss reveals a concern with the reformation of body tissue.
{plastic_surgery}  ? surgery concerned with therapeutic or cosmetic reformation of tissue
?   ?pastry? + {surgery} ?{plastic_surgery}  + reformation_of: pastry:tissue
This interpretation requires that an existing form of 
surgery is recruited and adapted so as to 
accommodate the concept {pastry} . In taxonomic 
terms, {plastic_surgery} is perhaps most 
appropriately adapted for this purpose, since 
{tissue} and {pastry}  are both hyponyms of 
{substance} in WordNet. Of course, the intended 
sense of ? tissue?  in the above gloss is not {tissue, 
tissue_paper} but {tissue} as a hyponym of 
{body_part} . However, creative metaphors often 
involve a degree of domain incongruence, whereby 
a given word has a different meaning in the source 
and target domains (Ortony, 1979). In fact, one 
might say that domain incongruence is essential to 
creative metaphor, since interpretation will 
necessitate the grafting of structure from radically 
distant parts of the concept ontology, and such 
grafts may fail if the features involved maintain 
their strict, source-dependent definitions.
4   External Systematicity
Metaphors appear more apt when they 
systematically evoke, or connect into, established 
modes of metaphoric thought. This is systematicity 
considered from an external vantage as described 
by (Lakoff and Johnson, 1980). For example, 
when processing the metaphor ? political 
mechanic? , several concepts can be reached from 
? political?  that prove to be taxonomically 
compatible with {mechanic}, among them 
{political_leader} , {political_scientist} and 
{machine_politician}. However, closer inspection 
of the projected structure suggests that the last, 
{machine_politician}, is the most systematic:
?political? + {mechanic} 
   ?  {machine_politician}    + machinist_of: political:machine
Because the extracted qualia structure for 
{mechanic} hinges on the relationship 
machinist_of:machine, there is a suggestive lexical 
systematicity with the concept 
{machine_politician}. Furthermore, the 
instantiated structure creates a fortuitous pairing 
political:machine, which already exists in 
WordNet as the lexicalized metaphor 
{political_machine}. This marks ? political 
mechanic?  as a systematic outgrowth of the 
established metaphor schema Political System As 
Machine (whose corollary is Political Operatives 
as Fixers). The same schema comes into play 
when interpreting the metaphor ? political 
draftsman? , whose WordNet gloss also evokes 
images of machinery.
Lexicalized metaphors like {political_machine}, 
{political_science} and {political_campaign} act 
as the recognizable landmarks in the search space 
of possible interpretations for novel metaphors. So 
if an interpretation can be generated that connects 
into an established metaphor, it has a greater 
provenance than one that stands alone. Here are 
some further examples:
{torchbearer}  ? a leader in a campaign or movement
?  ?political? + {torchbearer} ?       {political_leader} + campaigner_of:political:campaign
{missionary}  ? someone who attempts to convert   others to a [...]  program
?    ?political? + {missionary} ?   {political_commissar} + programmer_of: political:program
{sociologist}  ?  a social scientist who studies [...] human society
?    ?political? + {sociologist} ?{political_scientist} + scientist_of: political:science
These examples are fortuitous in the sense that the instantiation of qualia structure directly suggests an existing WordNet concept. In most cases, however, the external systematicity becomes visible only upon recursive consideration of the instantiated structure as a source of metaphor in itself. Consider the metaphor ?genetic cartographer?, for which {geneticist} is retrieved as a thematically similar concept:
{cartographer}    ? a person who makes maps
{geneticist}      ? a person who specializes in genetics
?    ?genetic? + {cartographer} ?{geneticist} + mapper_of: genetic:mapping
There is no denotation for ? genetic mapping?  in 
WordNet, so at first blush the above interpretation 
fails to connect into an existing lexicalized 
metaphor. However, when we recursively consider 
the combination ? genetic mapping?  as a metaphor 
in itself, we obtain the following interpretation:
    ?genetic? + {mapping} ?  {chromosome_mapping}
?? the process of locating genes   on a chromosome
This allows us to recognize ?genetic mapping? as an alternate way of denoting the concept {chromosome_mapping}, while the fact that a mapping metaphor has already been lexicalized in the genetics domain allows us to recognize the external systematicity inherent in the interpretation of ?geneticist as cartographer?. This WordNet entry serves to ground the sub-metaphor of genetic mapping in an existing concept, allowing the recursive analysis of sub-metaphors to halt at this point. A ?genetic cartographer? is thus a geneticist that performs a specialized kind of map-making called chromosome mapping, where the terrain that is mapped is biological and information-theoretic rather than geological or geographic. Though chromosome mapping is itself a metaphor, its independent existence in WordNet means that it does not need to be justified in the context of an interpretation of ?genetic cartographer?, and for the purposes of analysis can be treated as a literal stopping-point.
5   The Challenge of Aptness 
I suspect we can all agree that aptness involves a complex interaction of different issues that arise from lexical and conceptual choice. The real question is the degree to which each of these issues influences a particular interpretation, and the weighting, if any, that is to be given to each component of aptness in an algorithmic model. Take the metaphor ?political surgeon?: by considering the concepts in the semantic neighborhood of {surgeon} reachable via the thematic cue ?political?, we find the following competing interpretations:
{political_scientist}  ? a social scientist specializing in the study of government
{spin_doctor}      ?  a spokesperson for a political partyor candidate who tries to forestall negative publicity
The first of these interpretations, 
{political_scientist}, is apt for reasons of internal 
systematicity, as both it and {surgeon} have an 
extracted qualia structure that contains a 
specializer_of:specialization relationship. This 
leads to the following interpretation:
?political? + {surgeon}  
      ? {political_scientist}+ specializer_of:political:specialization
The second interpretation, {spin_doctor}, does not 
exhibit the same internal systematicity, but it does 
exhibit an external systematicity of sorts: the head 
of this compound term, ? doctor? , denotes a 
concept {doctor, physician} that is a hypernym of 
the metaphoric vehicle, {surgeon}. 
It would seem a matter of personal choice as to 
which interpretation should be privileged here, as 
different listeners may attach more weight to the 
presence of internal systematicity in 
{political_scientist} than to the suggestion of 
external systematicity in {spin_doctor}, and vice 
versa. This suggests that the problem of aptness 
determination involves a great deal of hidden 
parameters yet to be made explicit in any model of 
interpretation. As researchers interested in 
computational treatments of metaphor, our goal 
then should be to explicate what factors we can in 
algorithmic and representational terms, to provide 
the basic inventory of components needed to 
proceed with our investigation into this elusive and 
considerably vexing phenomenon. In this paper we 
have argued that the natural place to compile this 
inventory is the lexicon, since this acts as the 
bridge between word and world knowledge and 
aptness is a phenomenon that hops freely between 
both.
References
George Lakoff and Mark Johnson. 1980.Metaphors we live by. University of Chicago Press: Chicago.
George A. Miller. 1995. WordNet: A Lexical Database for English. Communications of the ACM, Vol. 38 No. 11.
Christiane Fellbaum. 1998. WordNet: An electronic lexical database. Cambridge, England: Cambridge University Press, Cambridge, MA.
Eileen C. Way. 1991. Knowledge Representation 
and Metaphor. Studies in Cognitive systems.
Kluwer Academic Publishers, Amsterdam.
James Pustejovsky. 1991. The generative lexicon. Computational Linguistics, Vol. 17 No. 4.
Dedre Gentner. 1983. Structure-mapping: A 
theoretical framework for analogy. Cognitive 
Science, 7,  pp 155-170.
Tony Veale and Mark T. Keane. 1997. The Competence of Sub-Optimal Structure Mapping on Hard Analogies. The proceedings of IJCAI?97, the International Joint Conference on Artificial Intelligence, Nagoya, Japan.  Morgan Kaufman, San Mateo CA.
Sanda Harabagiu, George A. Miller, and Dan Moldovan. 1999. WordNet 2 - A Morphologically and Semantically Enhanced Resource. The Proceedings of the ACL SIGLEX Workshop: Standardizing Lexical Resources.Maryland, USA. 
Andrew Ortony. 1979. The role of similarity in similes and metaphors. In Ortony, A. (ed.): Metaphor and Thought. Cambridge University Press : Cambridge, U.K.

Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 378?381,
Prague, June 2007. c?2007 Association for Computational Linguistics
UCD-S1: A hybrid model for detecting semantic relations between noun 
pairs in text
Cristina Butnariu
School of Computer Science and Informatics
University College Dublin
Belfield, Dublin 4, Ireland.
ioana.butnariu@UCD.ie
Tony Veale
School of Computer Science and Informatics
University College Dublin
Belfield, Dublin 4, Ireland.
tony.veale@UCD.ie
Abstract
We describe a supervised learning approach to 
categorizing  inter-noun  relations,  based  on 
Support Vector Machines, that builds a differ-
ent classifier for each of seven semantic rela-
tions.  Each  model  uses  the  same  learning 
strategy,  while  a  simple  voting  procedure 
based on five trained discriminators with vari-
ous  blends  of  features  determines  the  final 
categorization.  The features that  characterize 
each of the noun pairs are a blend of lexical-
semantic  categories extracted  from WordNet 
and  several  flavors  of  syntactic  patterns  ex-
tracted  from  various  corpora,  including 
Wikipedia and the WMTS corpus.
1 Introduction
The  SemEval  task  for  classifying  inter-noun 
semantic  relations  employs  seven  semantic 
relations  that  are  not  exhaustive:  Cause-Effect, 
Instrument-Agency,  Product-Producer  Origin-
Entity,  Theme-Tool,  Part-Whole  and  Content-
Container.  The  task  is  to  classify  the  relations 
between pairs of concepts that are part of the same 
syntactic  structure  in  a  given  sentence.  This 
approach  employs  a  context-dependent 
classification,  as  opposed to  usual  out-of-context 
approaches  in  classifying  semantic  relations 
between noun pairs (e.g., (Turney, 2005), (Nastase 
et. al., 2006)).
Our  approach  is  based  on  the  Support  Vector 
Machines  learning  paradigm  (Vapnik,  1995),  in 
which supervised machine learning is used to find 
the most  salient combination of features for each 
semantic relation. These features include semantic 
generalizations of  the noun-senses as encoded as 
WordNet (WN) hyponyms,  some manually selec-
ted  linguistic  features  (e.g.,  agentive,  gerundive, 
etc.) as well as the observed relational behaviour of 
the given nouns in three different corpora: the col-
lected  glosses  of  WordNet;  the  collected  text  of 
Wikipedia; and the WMTS corpus.
One can find similar approaches in the literature 
to the semantic classification of noun compounds. 
Turney (2005) uses automatically extracted para-
phrases to build a similarity measure between pairs 
of concepts, while  Nastase et. al. (2006) proposes 
separate models for two different word representa-
tions  when  determining  the  semantic  relation  in 
modifier-noun compounds:  a model  based on the 
lexico-semantic aspects of words and a model that 
uses contextual information from corpora. Our ap-
proach is different in that we use all the available 
features of word representations and concept inter-
actions in a single hybrid model.
2 System description
Our  system,  named  the  Semantic  Relation  Dis-
criminator (or SRD), takes as input a set of noun 
pairs that are manually classified as positive/negat-
ive for a given semantic relation and produces as 
output  a  discriminator  for  that  semantic  relation. 
We used SRD to learn different models for each of 
the  seven  semantic  relations  in  the  classification 
scheme for task 4 in the SemEval Workshop. The 
SRD system relies  on several  data-resources  and 
tools:  the  WN  noun-sense  hierarchy,  a  corpus 
made up of the WordNet glosses, the complete text 
of  Wikipedia  (downloaded June,  2005),  a  search 
engine indexing a very large corpus of text, and the 
WEKA  Data  Mining  software  package  (version 
3.5). 
SRD combines  two types  of  features  for  each 
noun pair:  semantic  features  extracted  from  WN 
noun-sense hierarchy, for which the WN synset-id 
378
information of each noun is used and syntactic fea-
tures extracted from the unlabeled and unstructured 
corpora mentioned above for which a shallow pars-
ing approach is employed. 
2.1 Feature acquisition 
SRD follows four steps in acquiring features: 
? Select  semantic  generalizations.  For  each 
noun-sense in a pair, SRD extracts all hyper-
nyms  at  depth  8 or  higher  in  the  WordNet 
noun-sense hierarchy.
? Extract  syntactic  phrases.  SRD  looks  for 
phrases in corpora that occur before or after 
each noun in a pair and which obey one of 
several  syntactic  templates.  SRD also looks 
for  joining  phrases  between  each  pair  of 
nouns that contain 5 words or less.
? Clean-up these phrases. SRD lemmatizes the 
words in each phrase and removes function 
words such as articles, possessive pronouns, 
adjective and adverbs. 
? Record  observed  patterns.  For  each  noun 
pair, SRD records the following types of syn-
tactic patterns together with their corpus fre-
quencies: joining terms that comprise at least 
one verb; phrases that are composed of one 
verb  and  one  preposition;  and  phrases  that 
are composed of a simple verb or a phrasal 
verb.
2.2 Selecting the features
Due to the large number of  features extracted in 
these  steps,  SRD employs  five  different  models 
that  use  different  combination  of  features  and 
which pool their votes to determine a single predic-
ation for each learning task. We describe below the 
feature sets used for each component. The features 
have binary values: 1 if the feature is present for a 
noun pair, and 0 otherwise. 
Each model employs WordNet hypernyms (from 
the  top  8  layers  of  the  noun  hierarchy)  of  both 
noun-senses as semantic features, while models 1 
and 2 employ the following additional features for 
each noun pair (N1, N2):
1. The  most  frequent  syntactic  patterns  that 
appear between N1 and N2 in corpora
2. The  most  frequent  syntactic  patterns  that 
appear between N2 and N1 in corpora
Model 1 and Model 2 differ only in the syntactic 
templates  used  to  validate  inter-noun  patterns. 
Model  1  fixates  on  patterns  that  contain  a  verb, 
while Model 2 accepts patterns that contain either a 
preposition or a verb, or both. This yields, on aver-
age, 5,000 binary features for Model 1 for each of 
the seven relation types, and an average of 10,000 
binary features for Model 2. 
In addition to WN-derived hypernymic-features, 
models 3 and 4 employ the following: 
1. The  most  frequent  syntactic  patterns  that 
immediately precede N1 in a corpus
2. The  most  frequent  syntactic  patterns  that 
immediately follow N1 in a corpus
3. The  most  frequent  syntactic  patterns  that 
immediately precede N2 in a corpus
4. The  most  frequent  syntactic  patterns  that 
immediately follow N2 in a corpus
In Model  3 each syntactic  pattern comprises a 
hyphenated  verb,  while  the  syntactic  patterns  in 
Model 4 each contain a preposition or a verb. SRD 
generates,  on  average,  1,500  binary  features  in 
Model 3 and 2,500 features in Model 4 for each re-
lation-type.
In addition to WN-derived hypernymic-features, 
model 5 employs the following:
1. A set of linguistic features for N1, indicat-
ing  whether  this  noun  is  a  nominalized 
verb, or whether it frequently appears in a 
specific semantic case role (e.g., agent).
2. The same set of linguistic features as de-
termined for N2.
SRD generates, on average, approximately 700 
binary features for each relation-type in Model 5. 
2.3 Building the models
The  SVM  learning  paradigm  seems  particularly 
suitable  to  our  task  for  a  number  of  reasons. 
Firstly, it  behaves robustly for all  seven learning 
tasks, ignoring the noise in the training set. This is 
important, since e.g., some training pairs for the In-
strument-Agency relation were labeled as both true 
and false. Secondly, SVM has an automated mech-
anism  for  parameter  tuning,  which  reduces  the 
overall computational effort. 
SRD employs  polynomial  SVMs because  they 
appear  to  perform  better  for  this  task  compared 
379
with simple linear SVMs or radial-basis functions. 
We used the WEKA implementation of John Plat-
t?s Sequential Minimal Optimization method (Platt, 
1998) to train the feature weights on all the avail-
able training data. Using SMO to train the polyno-
mial SVM takes approx. 2.8 CPU sec. per model.
The motivation for a multiple model scheme ap-
proach comes from empirical results. SRD yields 
higher  results  relative  to  the  five  single  models 
schemes that compose our system when evaluated 
using 10-fold cross validation on the training data. 
3 Experiments and Results
The SemEval  data-set  for  each  of  the  seven  se-
mantic relations comprises 140 annotated instances 
for training and between 70 to 90 for testing. Each 
instance  is  manually  labelled  with  the  part  of 
speech of each concept in a pair, as well as the WN 
synset-id of the intended word-sense and a sample 
sentential context. SRD?s predictions fall into eval-
uation category B, as the system uses WN synset-
id but not the query pattern used to originally pop-
ulate the data-sets with instances. SRD also skips 
those  training  instances  where  WN sense-ids  are 
not provided, so that the actual number of training 
instances used ranges from 129 to 138 manually la-
belled examples per relation-type.
SRD?s  precision,  recall,  F-score  and  accuracy 
for each relation is given by Table 1.
P R F1 Acc #t inst.
Cause-Effect 69.8 73.2 71.4 70.0 80
Instrument-Agency 72.5 76.3 74.4 74.4 78
Product-Producer 80.6 87.1 83.7 77.4 93
Origin-Entity 60.0 50.0 54.5 63.0 81
Theme-Tool 50.0 34.5 40.8 59.2 71
Part-Whole 71.4 57.7 63.8 76.4 72
Content-Container 84.8 73.7 78.9 79.7 74
Average 69.9 64.6 66.8 71.4 78.4
Table1. Results for SRD across the seven learning tasks
To  assess  the  effect  of  varying  quantities  of 
training  data,  the  model  was  tested  on  different 
fractions of the training data: dataset B1 comprises 
the first quarter of the training data, dataset B2 the 
first  half,   while  B3  dataset  comprises  the  first 
three  quarters  and  B4  comprises  the  complete 
training dataset. We report the behavior of SRD in 
predicting the unseen test data when learning from 
these datasets in table 2. The measures of table 2 
represent an average of SRD?s performance across 
all relation-types.
P R F1 Acc
Dataset B1  65.4 53.3 56.4 66.2
Dataset B2 67.8 63.8 63.5 69.6
Dataset B3 71.7 64.0 66.8 71.6
Dataset B4 69.9 64.6 66.8 71.4
Table2. Results for SRD on different training datasets
3.1 Error analysis
Three types of baseline values were proposed for 
this  task.  Baseline  1 (?majority baseline?)  is  ob-
tained by always guessing either "true" or "false", 
according to whichever is the majority category in 
the testing data-set for the given relation. Baseline 
2 (?alltrue baseline?) is achieved by always guess-
ing  ?true?.  Baseline  3  (?probmatch  baseline?)  is 
obtained  by  randomly  guessing  "true"  or  "false" 
with  a  probability  matching  the  distribution  of 
"true" or "false" in the testing dataset.
0
10
20
30
40
50
60
70
80
90
class1 class2 class3 class4 class5 class6 class7
SRD Baseline1 Baseline2 Baseline3
Figure1.  Comparison  of  SRD?s  F-scores  for  each  se-
mantic relation and the corresponding baselines.
Figure 1 plots the F-scores obtained for each se-
mantic relation. We observe that SRD has exhibits 
poor performance on two particular relations, Ori-
gin-Entity and Theme-Tool, denoted ?class4? and 
?class5? in the plot of Figure 1. SRD achieves the 
same  F-measure  score  as  the  random prediction 
baseline for Theme-Tool class, suggesting that the 
features used are simply not capable of building a 
discriminator for this semantic relation. SRD?s F-
score for Origin-Entity class is 10% higher than the 
random baseline, but still performs below the other 
two baselines. SRD?s best performance is achieved 
for Product-Producer and Part-Whole,  with an F-
score 11% higher than the highest baseline.
380
Table3. SRD F-measures using different feature sets
3.2 Improvements
One obvious problem with SRD is that we use a 
high-dimensional feature-space to train each mod-
el. Research in text categorization (e.g., Dumais et  
al., 1998) shows that feature selection algorithms 
like information gain can identify the most produc-
tive dimensions of the feature space and simultane-
ously boost classification accuracy.
To explore  this  potential  for  improvement,  we 
applied two types of feature selection filters (using 
WEKA): the InfoGainAttrEval filter that evaluates 
the utility of  a feature  by measuring information 
gain w.r.t. the class; and the  CfsSubsetEval filter, 
which evaluates the utility of a subset of features 
by considering the individual predictive ability of 
each individually and the degree of redundancy be-
tween  them  collectively.  Results  of  our  experi-
ments with SRD using different subsets of feature 
sets are displayed in Table 3. Set 1 is the complete 
set of all features. Set 2 is the subset obtained with 
the  top  n  features  as  ranked  by the  InfoGainAt-
trEval filter  (n is  determined using 10-fold cross 
validation on the training data). Set 3 is a tailored 
feature-set created for each relation-type using the 
CfsSubsetEval filter. Set 4 is the subset of all fea-
tures extracted from WN. 
We find that feature-filtering boosts the perfor-
mance of some learning tasks by up to 14 % (e.g., 
the Theme-Tool relation), but it can also decrease 
performance by the same amount (e.g., the Origin-
Entity relation). SRD achieves its best performance 
-- an overall F-measure of 71.7% -- when using a 
feature set that is tailored to each of the semantic 
relation classification tasks (e.g., Set 4 (WN only) 
for Origin-Entity, Set 1 (all) for Product-Producer 
and Container-Content, Set 4 and Set 3 (relation-
specific subsets) for everything else).
4 Conclusions
SRD  is  an  SVM-based  approach  to  classifying 
noun-pairs into categories that best reflect the se-
mantic relationship underlying each pair. Without 
feature-filtering, SRD shows modest classification 
capability, performing better than the highest base-
lines for five of the seven relational classes. Exper-
iments  with  feature  filtering  encourage  us  to  try 
and refine SRD?s feature space to focus on more 
discriminatory and semantically-revealing features 
of nouns. Feature-filtering can diminish as well as 
improve performance, and thus, should ideally be 
linked to an insightful theory of how particular fea-
tures  contribute  to  the  human-understanding  of 
noun-noun  pairs.  Filtering  techniques  provide  a 
good basis for formulating feature-based hypothe-
ses, but the most productive feature sets will come, 
we hope, from a cognitive and conceptual under-
standing of  the  processes  of  phrase  construction, 
rather than from an exhaustive and largely theory-
free exploration of different feature-sets.
Acknowledgments
We would like to thank Peter Turney for granting 
us access to the NRC copy of the WMTS.
References
Joachims,  T.  (1998)  Text  categorization  with  support 
vector  machines:  learning  with  many  relevant  fea-
tures. Proceedings of ECML-98, 10th European Con-
ference on Machine Learning.
Dumais,  S.  T.,  Platt,  J.,  Heckerman  D.,  Sahami  M., 
(1998) Inductive learning algorithms and representa-
tions  for  text  categorization,  Proceedings  of  ACM-
CIKM98
Nastase, V., Sayyad-Shirabad, J., Sokolova, M., and Sz-
pakowicz, S. (2006). Learning noun-modifier seman-
tic  relations  with  corpus-based and WordNet-based 
features. In Proceedings of the 21st National Confer-
ence on Artificial Intelligence, Boston, MA.
Platt, J. (1998), Fast Training of SVMs Using Sequen-
tial Minimal Optimization,  Support Vector Machine 
Learning, MIT Press, Cambridge.
Turney, P.D. (2005). Measuring semantic similarity by 
latent relational analysis. In Proceedings of the Nine-
teenth  International  Joint  Conference  on  Artificial  
Intelligence, Edinburgh, Scotland.
Vapnik, V. (1995). The Nature of Statistical  Learning 
Theory, Springer-Verlag, New York
Feature 
Set1
Feature 
Set2
Feature 
Set3
Feature 
Set4
Cause-Effect 71.4 72.7 75.7 61.3
Instrument-Agency 74.4 74.6 76.3 72
Product-Producer 83.7 81.3 80.5 77
Origin-Entity 54.5 44.8 38 61.5
Theme-Tool 40.8 42.8 53.8 42.5
Part-Whole 63.8 72.3 62.7 60
Content-Container 78.9 75.6 77.1 73.2
Average 66.8 66.3 66.3 64
381
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 100?105,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 9: The Interpretation of Noun Compounds
Using Paraphrasing Verbs and Prepositions
Cristina Butnariu
University College Dublin
ioana.butnariu@ucd.ie
Su Nam Kim
University of Melbourne
nkim@csse.unimelb.edu.au
Preslav Nakov
National University of Singapore
nakov@comp.nus.edu.sg
Diarmuid O? Se?aghdha
University of Cambridge
do242@cam.ac.uk
Stan Szpakowicz
University of Ottawa
Polish Academy of Sciences
szpak@site.uottawa.ca
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
We present a brief overview of the main
challenges in understanding the semantics of
noun compounds and consider some known
methods. We introduce a new task to be
part of SemEval-2010: the interpretation of
noun compounds using paraphrasing verbs
and prepositions. The task is meant to provide
a standard testbed for future research on noun
compound semantics. It should also promote
paraphrase-based approaches to the problem,
which can benefit many NLP applications.
1 Introduction
Noun compounds (NCs) ? sequences of two or more
nouns acting as a single noun,1 e.g., colon cancer
tumor suppressor protein ? are abundant in English
and pose a major challenge to the automatic anal-
ysis of written text. Baldwin and Tanaka (2004)
calculated that 3.9% and 2.6% of the tokens in
the Reuters corpus and the British National Corpus
(BNC), respectively, are part of a noun compound.
Compounding is also an extremely productive pro-
cess in English. The frequency spectrum of com-
pound types follows a Zipfian or power-law distribu-
tion (O? Se?aghdha, 2008), so in practice many com-
pound tokens encountered belong to a ?long tail?
of low-frequency types. For example, over half of
the two-noun NC types in the BNC occur just once
(Lapata and Lascarides, 2003). Even for relatively
frequent NCs that occur ten or more times in the
BNC, static English dictionaries give only 27% cov-
erage (Tanaka and Baldwin, 2003). Taken together,
1We follow the definition in (Downing, 1977).
the factors of high frequency and high productiv-
ity mean that achieving robust NC interpretation is
an important goal for broad-coverage semantic pro-
cessing. NCs provide a concise means of evoking a
relationship between two or more nouns, and natu-
ral language processing (NLP) systems that do not
try to recover these implicit relations from NCs are
effectively discarding valuable semantic informa-
tion. Broad coverage should therefore be achieved
by post-hoc interpretation rather than pre-hoc enu-
meration, since it is impossible to build a lexicon of
all NCs likely to be encountered.
The challenges presented by NCs and their se-
mantics have generated significant ongoing interest
in NC interpretation in the NLP community. Repre-
sentative publications include (Butnariu and Veale,
2008; Girju, 2007; Kim and Baldwin, 2006; Nakov,
2008b; Nastase and Szpakowicz, 2003; O? Se?aghdha
and Copestake, 2007). Applications that have been
suggested include Question Answering, Machine
Translation, Information Retrieval and Information
Extraction. For example, a question-answering sys-
tem may need to determine whether headaches in-
duced by caffeine withdrawal is a good paraphrase
for caffeine headaches when answering questions
about the causes of headaches, while an information
extraction system may need to decide whether caf-
feine withdrawal headache and caffeine headache
refer to the same concept when used in the same
document. Similarly, a machine translation system
facing the unknown NC WTO Geneva headquarters
might benefit from the ability to paraphrase it as
Geneva headquarters of the WTO or as WTO head-
quarters located in Geneva. Given a query like can-
100
cer treatment, an information retrieval system could
use suitable paraphrasing verbs like relieve and pre-
vent for page ranking and query refinement.
In this paper, we introduce a new task, which will
be part of the SemEval-2010 competition: NC inter-
pretation using paraphrasing verbs and prepositions.
The task is intended to provide a standard testbed
for future research on noun compound semantics.
We also hope that it will promote paraphrase-based
approaches to the problem, which can benefit many
NLP applications.
The remainder of the paper is organized as fol-
lows: Section 2 presents a brief overview of the
existing approaches to NC semantic interpretation
and introduces the one we will adopt for SemEval-
2010 Task 9; Section 3 provides a general descrip-
tion of the task, the data collection, and the evalua-
tion methodology; Section 4 offers a conclusion.
2 Models of Relational Semantics in NCs
2.1 Inventory-Based Semantics
The prevalent view in theoretical and computational
linguistics holds that the semantic relations that im-
plicitly link the nouns of an NC can be adequately
enumerated via a small inventory of abstract re-
lational categories. In this view, mountain hut,
field mouse and village feast all express ?location
in space?, while the relation implicit in history book
and nativity play can be characterized as ?topicality?
or ?aboutness?. A sample of some of the most influ-
ential relation inventories appears in Table 1.
Levi (1978) proposes that complex nominals ?
a general concept grouping together nominal com-
pounds (e.g., peanut butter), nominalizations (e.g.,
dream analysis) and non-predicative noun phrases
(e.g., electric shock) ? are derived through the com-
plementary processes of recoverable predicate dele-
tion and nominalization; each process is associated
with its own inventory of semantic categories. Table
1 lists the categories for the former.
Warren (1978) posits a hierarchical classifica-
tion scheme derived from a large-scale corpus study
of NCs. The top-level relations in her hierar-
chy are listed in Table 1, while the next level
subdivides CONSTITUTE into SOURCE-RESULT,
RESULT-SOURCE and COPULA; COPULA is then
further subdivided at two additional levels.
In computational linguistics, popular invento-
ries of semantic relations have been proposed by
Nastase and Szpakowicz (2003) and Girju et al
(2005), among others. The former groups 30 fine-
grained relations into five coarse-grained super-
categories, while the latter is a flat list of 21 re-
lations. Both schemes are intended to be suit-
able for broad-coverage analysis of text. For spe-
cialized applications, however, it is often useful
to use domain-specific relations. For example,
Rosario and Hearst (2001) propose 18 abstract rela-
tions for interpreting NCs in biomedical text, e.g.,
DEFECT, MATERIAL, PERSON AFFILIATED,
ATTRIBUTE OF CLINICAL STUDY.
Inventory-based analyses offer significant advan-
tages. Abstract relations such as ?location? and ?pos-
session? capture valuable generalizations about NC
semantics in a parsimonious framework. Unlike
paraphrase-based analyses (Section 2.2), they are
not tied to specific lexical items, which may them-
selves be semantically ambiguous. They also lend
themselves particularly well to automatic interpreta-
tion methods based on multi-class classification.
On the other hand, relation inventories have been
criticized on a number of fronts, most influentially
by Downing (1977). She argues that the great vari-
ety of NC relations makes listing them all impos-
sible; creative NCs like plate length (?what your
hair is when it drags in your food?) are intuitively
compositional, but cannot be assigned to any stan-
dard inventory category. A second criticism is that
restricted inventories are too impoverished a repre-
sentation scheme for NC semantics, e.g., headache
pills and sleeping pills would both be analyzed as
FOR in Levi?s classification, but express very differ-
ent (indeed, contrary) relationships. Downing writes
(p. 826): ?These interpretations are at best reducible
to underlying relationships. . . , but only with the loss
of much of the semantic material considered by sub-
jects to be relevant or essential to the definitions.?
A further drawback associated with sets of abstract
relations is that it is difficult to identify the ?correct?
inventory or to decide whether one proposed classi-
fication scheme should be favored over another.
2.2 Interpretation Using Verbal Paraphrases
An alternative approach to NC interpretation asso-
ciates each compound with an explanatory para-
101
Author(s) Relation Inventory
Levi (1978) CAUSE, HAVE, MAKE, USE, BE, IN, FOR, FROM, ABOUT
Warren (1978) POSSESSION, LOCATION, PURPOSE, ACTIVITY-ACTOR, RESEMBLANCE, CONSTITUTE
Nastase and CAUSALITY (cause, effect, detraction, purpose),
Szpakowicz PARTICIPANT (agent, beneficiary, instrument, object property,
(2003) object, part, possessor, property, product, source, whole, stative),
QUALITY (container, content, equative, material, measure, topic, type),
SPATIAL (direction, location at, location from, location),
TEMPORALITY (frequency, time at, time through)
Girju et al (2005) POSSESSION, ATTRIBUTE-HOLDER, AGENT, TEMPORAL, PART-WHOLE, IS-A, CAUSE,
MAKE/PRODUCE, INSTRUMENT, LOCATION/SPACE, PURPOSE, SOURCE, TOPIC, MANNER,
MEANS, THEME, ACCOMPANIMENT, EXPERIENCER, RECIPIENT, MEASURE, RESULT
Lauer (1995) OF, FOR, IN, AT, ON, FROM, WITH, ABOUT
Table 1: Previously proposed inventories of semantic relations for noun compound interpretation. The first two come
from linguistic theories; the rest have been proposed in computational linguistics.
phrase. Thus, cheese knife and kitchen knife can be
expanded as a knife for cutting cheese and a knife
used in a kitchen, respectively. In the paraphrase-
based paradigm, semantic relations need not come
from a small set; it is possible to have many sub-
tle distinctions afforded by the vocabulary of the
paraphrasing language (in our case, English). This
paradigm avoids the problems of coverage and rep-
resentational poverty, which Downing (1977) ob-
served in inventory-based approaches. It also re-
flects cognitive-linguistic theories of NC semantics,
in which compounds are held to express underlying
event frames and whose constituents are held to de-
note event participants (Ryder, 1994).
Lauer (1995) associates NC semantics with
prepositional paraphrases. As Lauer only consid-
ers a handful of prepositions (about, at, for,
from, in, of, on, with), his model is es-
sentially inventory-based. On the other hand, noun-
preposition co-occurrences can easily be identified
in a corpus, so an automatic interpretation can be
implemented through simple unsupervised methods.
The disadvantage of this approach is the absence of a
one-to-one mapping from prepositions to meanings;
prepositions can be ambiguous (of indicates many
different relations) or synonymous (at, in and on
all express ?location?). This concern arises with all
paraphrasing models, but it is exacerbated by the re-
stricted nature of prepositions. Furthermore, many
NCs cannot be paraphrased adequately with prepo-
sitions, e.g., woman driver, honey bee.
A richer, more flexible paraphrasing model is af-
forded by the use of verbs. In such a model, a honey
bee is a bee that produces honey, a sleeping pill
is a pill that induces sleeping and a headache pill
is a pill that relieves headaches. In some previous
computational work on NC interpretation, manually
constructed dictionaries provided typical activities
or functions associated with nouns (Finin, 1980; Is-
abelle, 1984; Johnston and Busa, 1996). It is, how-
ever, impractical to build large structured lexicons
for broad-coverage systems; these methods can only
be applied to specialized domains. On the other
hand, we expect that the ready availability of large
text corpora should facilitate the automatic mining
of rich paraphrase information.
The SemEval-2010 task we present here builds on
the work of Nakov (Nakov and Hearst, 2006; Nakov,
2007; Nakov, 2008b), where NCs are paraphrased
by combinations of verbs and prepositions. Given
the problem of synonymy, we do not provide a sin-
gle correct paraphrase for a given NC but a prob-
ability distribution over a range of candidates. For
example, highly probable paraphrases for chocolate
bar are bar made of chocolate and bar that tastes
like chocolate, while bar that eats chocolate is very
unlikely. As described in Section 3.3, a set of gold-
standard paraphrase distributions can be constructed
by collating responses from a large number of hu-
man subjects.
In this framework, the task of interpretation be-
comes one of identifying the most likely paraphrases
for an NC. Nakov (2008b) and Butnariu and Veale
(2008) have demonstrated that paraphrasing infor-
mation can be collected from corpora in an un-
supervised fashion; we expect that participants in
102
SemEval-2010 Task 9 will further develop suitable
techniques for this problem. Paraphrases of this kind
have been shown to be useful in applications such as
machine translation (Nakov, 2008a) and as an inter-
mediate step in inventory-based classification of ab-
stract relations (Kim and Baldwin, 2006; Nakov and
Hearst, 2008). Progress in paraphrasing is therefore
likely to have follow-on benefits in many areas.
3 Task Description
The description of the task we present below is pre-
liminary. We invite the interested reader to visit the
official Website of SemEval-2010 Task 9, where up-
to-date information will be published; there is also a
discussion group and a mailing list.2
3.1 Preliminary Study
In a preliminary study, we asked 25-30 human sub-
jects to paraphrase 250 noun-noun compounds us-
ing suitable paraphrasing verbs. This is the Levi-
250 dataset (Levi, 1978); see (Nakov, 2008b) for de-
tails.3 The most popular paraphrases tend to be quite
apt, while some less frequent choices are question-
able. For example, for chocolate bar we obtained
the following paraphrases (the number of subjects
who proposed each one is shown in parentheses):
contain (17); be made of (16); be made
from (10); taste like (7); be composed
of (7); consist of (5); be (3); have (2);
smell of (2); be manufactured from (2);
be formed from (2); melt into (2); serve
(1); sell (1); incorporate (1); be made with
(1); be comprised of (1); be constituted
by (1); be solidified from (1); be flavored
with (1); store (1); be flavored with (1); be
created from (1); taste of (1)
3.2 Objective
We propose a task in which participating systems
must estimate the quality of paraphrases for a test
set of NCs. A list of verb/preposition paraphrases
will be provided for each NC, and for each list a
participating system will be asked to provide aptness
2Please follow the Task #9 link at the SemEval-2010 home-
page http://semeval2.fbk.eu
3This dataset is available from http://sourceforge.
net/projects/multiword/
scores that correlate well (in terms of frequency dis-
tribution) with the human judgments collated from
our test subjects.
3.3 Datasets
Trial/Development Data. As trial/development
data, we will release the previously collected para-
phrase sets for the Levi-250 dataset (after further
review and cleaning). This dataset consists of 250
noun-noun compounds, each paraphrased by 25-30
human subjects (Nakov, 2008b).
Test Data. The test data will consist of approx-
imately 300 NCs, each accompanied by a set of
paraphrasing verbs and prepositions. Following the
methodology of Nakov (2008b), we will use the
Amazon Mechanical Turk Web service4 to recruit
human subjects. This service offers an inexpensive
way to recruit subjects for tasks that require human
intelligence, and provides an API which allows a
computer program to easily run tasks and collate
the responses from human subjects. The Mechanical
Turk is becoming a popular means to elicit and col-
lect linguistic intuitions for NLP research; see Snow
et al (2008) for an overview and a discussion of is-
sues that arise.
We intend to recruit 100 annotators for each NC,
and we will require each annotator to paraphrase
at least five NCs. Annotators will be given clear
instructions and will be asked to produce one or
more paraphrases for a given NC. To help us filter
out subjects with an insufficient grasp of English or
an insufficient interest in the task, annotators will
be asked to complete a short and simple multiple-
choice pretest on NC comprehension before pro-
ceeding to the paraphrasing step.
Post-processing. We will manually check the
trial/development data and the test data. Depending
on the quality of the paraphrases, we may decide to
drop the least frequent verbs.
License. All data will be released under the Cre-
ative Commons Attribution 3.0 Unported license5.
3.4 Evaluation
Single-NC Scores. For each NC, we will compare
human scores (our gold standard) with those pro-
posed by each participating system. We have con-
4http://www.mturk.com
5http://creativecommons.org/licenses/by/3.0/
103
sidered three scores: (1) Pearson?s correlation, (2)
cosine similarity, and (3) Spearman?s rank correla-
tion.
Pearson?s correlation coefficient is a standard
measure of the correlation strength between two dis-
tributions; it can be calculated as follows:
? = E(XY ) ? E(X)E(Y )?
E(X2) ? [E(X)]2?E(Y 2) ? [E(Y )]2
(1)
where X = (x1, . . . , xn) and Y = (y1, . . . , yn) are
vectors of numerical scores for each paraphrase pro-
vided by the humans and the competing systems, re-
spectively, n is the number of paraphrases to score,
and E(X) is the expectation of X .
Cosine correlation coefficient is another popu-
lar alternative and was used by Nakov and Hearst
(2008); it can be seen as an uncentered version of
Pearson?s correlation coefficient:
? = X.Y?X??Y ? (2)
Spearman?s rank correlation coefficient is suit-
able for comparing rankings of sets of items; it is
a special case of Pearson?s correlation, derived by
considering rank indices (1,2,. . . ) as item scores . It
is defined as follows:
? = n
?xiyi ? (?xi)(? yi)?
n?x2i ? (
?xi)2
?
n? y2i ? (
? yi)2
(3)
One problem with using Spearman?s rank coef-
ficient for the current task is the assumption that
swapping any two ranks has the same effect. The
often-skewed nature of paraphrase frequency distri-
butions means that swapping some ranks is intu-
itively less ?wrong? than swapping others. Consider,
for example, the following list of human-proposed
paraphrasing verbs for child actor, which is given in
Nakov (2007):
be (22); look like (4); portray (3); start as
(1); include (1); play (1); have (1); involve
(1); act like (1); star as (1); work as (1);
mimic (1); pass as (1); resemble (1); be
classified as (1); substitute for (1); qualify
as (1); act as (1)
Clearly, a system that swaps the positions for
be (22) and look like (4) for child actor will
have made a significant error, while swapping con-
tain (17) and be made of (16) for chocolate bar (see
Section 3.1) would be less inappropriate. However,
Spearman?s coefficient treats both alterations iden-
tically since it only looks at ranks; thus, we do not
plan to use it for official evaluation, though it may
be useful for post-hoc analysis.
Final Score. A participating system?s final score
will be the average of the scores it achieves over all
test examples.
Scoring Tool. We will provide an automatic eval-
uation tool that participants can use when train-
ing/tuning/testing their systems. We will use the
same tool for the official evaluation.
4 Conclusion
We have presented a noun compound paraphrasing
task that will run as part of SemEval-2010. The goal
of the task is to promote and explore the feasibility
of paraphrase-based methods for compound inter-
pretation. We believe paraphrasing holds some key
advantages over more traditional inventory-based
approaches, such as the ability of paraphrases to rep-
resent fine-grained and overlapping meanings, and
the utility of the resulting paraphrases for other ap-
plications such as Question Answering, Information
Extraction/Retrieval and Machine Translation.
The proposed paraphrasing task is predicated on
two important assumptions: first, that paraphrasing
via a combination of verbs and prepositions pro-
vides a powerful framework for representing and in-
terpreting the meaning of compositional nonlexical-
ized noun compounds; and second, that humans can
agree amongst themselves about what constitutes a
good paraphrase for any given NC. As researchers in
this area and as proponents of this task, we believe
that both assumptions are valid, but if the analysis
of the task were to raise doubts about either assump-
tion (e.g., by showing poor agreement amongst hu-
man annotators), then this in itself would be a mean-
ingful and successful output of the task. As such,
we anticipate that the task and its associated dataset
will inspire further research, both on the theory and
development of paraphrase-based compound inter-
pretation and on its practical applications.
104
References
Timothy Baldwin and Takaaki Tanaka. 2004. Transla-
tion by machine of compound nominals: Getting it
right. In Proceedings of the ACL 2004 Workshop on
Multiword Expressions: Integrating Processing, pages
24?31.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING 2008), pages
81?88.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810?842.
Timothy Finin. 1980. The Semantic Interpretation of
Compound Nominals. Ph.D. Dissertation, University
of Illinois, Urbana, Illinois.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Journal of Computer Speech and Language - Special
Issue on Multiword Expressions, 4(19):479?496.
Roxana Girju. 2007. Improving the interpretation of
noun phrases with cross-linguistic information. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL 2007), pages
568?575.
Pierre Isabelle. 1984. Another look at nominal com-
pounds. In Proceedings of the 10th International Con-
ference on Computational Linguistics, pages 509?516.
Michael Johnston and Frederica Busa. 1996. Qualia
structure and the compositional interpretation of com-
pounds. In Proceedings of the ACL 1996 Workshop on
Breadth and Depth of Semantic Lexicons, pages 77?
88.
Su Nam Kim and Timothy Baldwin. 2006. Interpret-
ing semantic relations in noun compounds via verb
semantics. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics
(COLING/ACL 2006) Main Conference Poster Ses-
sions, pages 491?498.
Mirella Lapata and Alex Lascarides. 2003. Detecting
novel compounds: the role of distributional evidence.
In Proceedings of the 10th conference of the European
chapter of the Association for Computational Linguis-
tics (EACL 2003), pages 235?242.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Dept. of Computing, Macquarie University,
Australia.
Judith Levi. 1978. The Syntax and Semantics of Complex
Nominals. Academic Press, New York.
Preslav Nakov andMarti A. Hearst. 2006. Using verbs to
characterize noun-noun relations. In LNCS vol. 4183:
Proceedings of the 12th international conference on
Artificial Intelligence: Methodology, Systems and Ap-
plications (AIMSA 2006), pages 233?244. Springer.
Preslav Nakov and Marti A. Hearst. 2008. Solving re-
lational similarity problems using the web as a cor-
pus. In Proceedings of the 46th Annual Meeting of the
Association of Computational Linguistics (ACL 2008),
pages 452?460.
Preslav Nakov. 2007. Using the Web as an Implicit
Training Set: Application to Noun Compound Syntax
and Semantics. Ph.D. thesis, EECS Department, Uni-
versity of California, Berkeley, UCB/EECS-2007-173.
Preslav Nakov. 2008a. Improved statistical machine
translation using monolingual paraphrases. In Pro-
ceedings of the 18th European Conference on Artificial
Intelligence (ECAI?2008), pages 338?342.
Preslav Nakov. 2008b. Noun compound interpretation
using paraphrasing verbs: Feasibility study. In LNAI
vol. 5253: Proceedings of the 13th international con-
ference on Artificial Intelligence: Methodology, Sys-
tems and Applications (AIMSA 2008), pages 103?117.
Springer.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings of
the 5th International Workshop on Computational Se-
mantics, pages 285?301.
Diarmuid O? Se?aghdha and Ann Copestake. 2007. Co-
occurrence contexts for noun compound interpreta-
tion. In Proceedings of the Workshop on A Broader
Perspective on Multiword Expressions, pages 57?64.
Diarmuid O? Se?aghdha. 2008. Learning Compound Noun
Semantics. Ph.D. thesis, University of Cambridge.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 82?90.
Mary Ellen Ryder. 1994. Ordered Chaos: The Interpre-
tation of English Noun-Noun Compounds. University
of California Press, Berkeley, CA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2008), pages 254?263.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: a feasibility
study on shallow processing. In Proceedings of the
ACL 2003 workshop on Multiword expressions, pages
17?24.
Beatrice Warren. 1978. Semantic patterns of noun-noun
compounds. In Gothenburg Studies in English 41,
Goteburg, Acta Universtatis Gothoburgensis.
105
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 278?287,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Creative Language Retrieval:
A Robust Hybrid of Information Retrieval and Linguistic Creativity
Tony Veale
School of Computer Science and Informatics,
University College Dublin,
Belfield, Dublin D4, Ireland.
Tony.Veale@UCD.ie
Abstract
Information retrieval (IR) and figurative
language processing (FLP) could scarcely
be more different in their treatment of lan-
guage and meaning. IR views language as
an open-ended set of mostly stable signs
with which texts can be indexed and re-
trieved, focusing more on a text?s potential
relevance than its potential meaning. In
contrast, FLP views language as a system
of unstable signs that can be used to talk
about the world in creative new ways.
There is another key difference: IR is prac-
tical, scalable and robust, and in daily use
by millions of casual users. FLP is neither
scalable nor robust, and not yet practical
enough to migrate beyond the lab. This pa-
per thus presents a mutually beneficial hy-
brid of IR and FLP, one that enriches IR
with new operators to enable the non-literal
retrieval of creative expressions, and which
also transplants FLP into a robust, scalable
framework in which practical applications
of linguistic creativity can be implemented.
1 Introduction
Words should not always be taken at face value.
Figurative devices like metaphor can communicate
far richer meanings than are evident from a super-
ficial ? and perhaps literally nonsensical ? reading.
Figurative Language Processing (FLP) thus uses a
variety of special mechanisms and representations,
to assign non-literal meanings not just to meta-
phors, but to similes, analogies, epithets, puns and
other creative uses of language (see Martin, 1990;
Fass, 1991; Way, 1991; Indurkhya, 1992; Fass,
1997; Barnden, 2006; Veale and Butnariu, 2010).
Computationalists have explored heterodox
solutions to the procedural and representational
challenges of metaphor, and FLP more generally,
ranging from flexible representations (e.g. the
preference semantics of Wilks (1978) and the col-
lative semantics of Fass (1991, 1997)) to processes
of cross-domain structure alignment (e.g. structure
mapping theory; see Gentner (1983) and Falken-
hainer et al 1989) and even structural inversion
(Veale, 2006). Though thematically related, each
approach to FLP is broadly distinct, giving com-
putational form to different cognitive demands of
creative language: thus, some focus on inter-
domain mappings (e.g. Gentner, 1983) while oth-
ers focus more on intra-domain inference (e.g. Ba-
rnden, 2006). However, while computationally
interesting, none has yet achieved the scalability or
robustness needed to make a significant practical
impact outside the laboratory. Moreover, such
systems tend to be developed in isolation, and are
rarely designed to cohere as part of a larger frame-
work of creative reasoning (e.g. Boden, 1994).
In contrast, Information Retrieval (IR) is both
scalable and robust, and its results translate easily
from the laboratory into practical applications (e.g.
see Salton, 1968; Van Rijsbergen, 1979). Whereas
FLP derives its utility and its fragility from its at-
tempts to identify deeper meanings beneath the
surface, the widespread applicability of IR stems
directly from its superficial treatment of language
278
and meaning. IR does not distinguish between
creative and conventional uses of language, or
between literal and non-literal meanings. IR is also
remarkably modular: its components are designed
to work together interchangeably, from stemmers
and indexers to heuristics for query expansion and
document ranking. Yet, because IR treats all lan-
guage as literal language, it relies on literal
matching between queries and the texts that they
retrieve. Documents are retrieved precisely be-
cause they contain stretches of text that literally
resemble the query. This works well in the main,
but it means that IR falls flat when the goal of re-
trieval is not to identify relevant documents but to
retrieve new and creative ways of expressing a
given idea. To retrieve creative language, and to be
potentially surprised or inspired by the results, one
needs to facilitate a non-literal relationship be-
tween queries and the texts that they match.
The complementarity of FLP and IR suggests a
productive hybrid of both paradigms. If the most
robust elements of FLP are used to provide new
non-literal query operators for IR, then IR can be
used to retrieve potentially new and creative ways
of speaking about a topic from a large text collec-
tion. In return, IR can provide a stable, robust and
extensible platform on which to use these opera-
tors to build FLP systems that exhibit linguistic
creativity. In the next section we consider the re-
lated work on which the current realization of
these ideas is founded, before presenting a specific
trio of new semantic query operators in section 3.
We describe three simple but practical applications
of this creative IR paradigm in section 4. Empirical
support for the FLP intuitions that underpin our
new operators is provided in section 5. The paper
concludes with some closing observations about
future goals and developments in section 6.
2 Related Work and Ideas
IR works on the premise that a user can turn an
information need into an effective query by antici-
pating the language that is used to talk about a
given topic in a target collection. If the collection
uses creative language in speaking about a topic,
then a query must also contain the seeds of this
creative language. Veale (2004) introduces the idea
of creative information retrieval to explore how an
IR system can itself provide a degree of creative
anticipation, acting as a mediator between the lit-
eral specification of a meaning and the retrieval of
creative articulations of this meaning. This antici-
pation ranges from simple re-articulation (e.g. a
text may implicitly evoke ?Qur?an? even if it only
contains ?Muslim bible?) to playful allusions and
epithets (e.g. the CEO of a rubber company may be
punningly described as a ?rubber baron?). A crea-
tive IR system may even anticipate out-of-
dictionary words, like chocoholic and sexoholic.
Conventional IR systems use a range of query
expansion techniques to automatically bolster a
user?s query with additional keywords or weights,
to permit the retrieval of relevant texts it might not
otherwise match (e.g. Vernimb, 1977; Voorhees,
1994). Techniques vary, from the use of stemmers
and morphological analysis to the use of thesauri
(such as WordNet; see Fellbaum, 1998; Voorhees,
1998) to pad a query with synonyms, to the use of
statistical analysis to identify more appropriate
context-sensitive associations and near-synonyms
(e.g. Xu and Croft, 1996). While some techniques
may suggest conventional metaphors that have be-
come lexicalized in a language, they are unlikely to
identify relatively novel expressions. Crucially,
expansion improves recall at the expense of overall
precision, making automatic techniques even more
dangerous when the goal is to retrieve results that
are creative and relevant. Creative IR must balance
a need for fine user control with the statistical
breadth and convenience of automatic expansion.
Fortunately, statistical corpus analysis is an ob-
vious area of overlap for IR and FLP. Distribu-
tional analyses of large corpora have been shown
to produce nuanced models of lexical similarity
(e.g. Weeds and Weir, 2005) as well as context-
sensitive thesauri for a given domain (Lin, 1998).
Hearst (1992) shows how a pattern like ?Xs and
other Ys? can be used to construct more fluid,
context-specific taxonomies than those provided
by WordNet (e.g. ?athletes and other celebrities?
suggests a context in which athletes are viewed as
stars). Mason (2004) shows how statistical analysis
can automatically detect and extract conventional
metaphors from corpora, though creative meta-
phors still remain a tantalizing challenge. Hanks
(2005) shows how the ?Xs like A, B and C? con-
struction allows us to derive flexible ad-hoc cate-
gories from corpora, while Hanks (2006) argues
for a gradable conception of metaphoricity based
on word-sense distributions in corpora.
279
Veale and Hao (2007) exploit the simile frame
?as X as Y? to harvest a great many common
similes and their underlying stereotypes from the
web (e.g. ?as hot as an oven?), while Veale and
Hao (2010) show that the pattern ?about as X as Y?
retrieves an equally large collection of creative (if
mostly ironic) comparisons. These authors demon-
strate that a large vocabulary of stereotypical ideas
(over 4000 nouns) and their salient properties (over
2000 adjectives) can be harvested from the web.
We now build on these results to develop a set
of new semantic operators, that use corpus-derived
knowledge to support finely controlled non-literal
matching and automatic query expansion.
3 Creative Text Retrieval
In language, creativity is always a matter of con-
strual. While conventional IR queries articulate a
need for information, creative IR queries articulate
a need for expressions to convey the same meaning
in a fresh or unusual way. A query and a matching
phrase can be figuratively construed to have the
same meaning if there is a non-literal mapping
between the elements of the query and the ele-
ments of the phrase. In creative IR, this non-literal
mapping is facilitated by the query?s explicit use of
semantic wildcards (e.g. see Mihalcea, 2002).
The wildcard * is a boon for power-users of the
Google search engine, precisely because it allows
users to focus on the retrieval of matching phrases
rather than relevant documents. For instance, * can
be used to find alternate ways of instantiating a
culturally-established linguistic pattern, or ?snow-
clone?: thus, the Google queries ?In * no one can
hear you scream? (from Alien), ?Reader, I * him?
(from Jane Eyre) and ?This is your brain on *?
(from a famous TV advert) find new ways in
which old patterns have been instantiated for hu-
morous effect on the Web. On a larger scale, Veale
and Hao (2007) used the * wildcard to harvest web
similes, but reported that harvesting cultural data
with wildcards is not a straightforward process.
Google and other engines are designed to maxi-
mize document relevance and to rank results ac-
cordingly. They are not designed to maximize the
diversity of results, or to find the largest set of
wildcard bindings. Nor are they designed to find
the most commonplace bindings for wildcards.
Following Guilford?s (1950) pioneering work,
diversity is widely considered a key component in
the psychology of creativity. By focusing on the
phrase level rather than the document level, and by
returning phrase sets rather than document sets,
creative IR maximizes diversity by finding as
many bindings for its wildcards as a text collection
will support. But we need more flexible and pre-
cise wildcards than *. We now consider three va-
rieties of semantic wildcards that build on insights
from corpus-linguistic approaches to FLP.
3.1 The Neighborhood Wildcard     ?X
Semantic query expansion replaces a query term X
with a set {X, X
1
, X
2
, ?, X
n
} where each X
i
 is
related to X by a prescribed lexico-semantic rela-
tionship, such as synonymy, hyponymy or
meronymy. A generic, lightweight resource like
WordNet can provide these relations, or a richer
ontology can be used if one is available (e.g. see
Navigli and Velardi, 2003). Intuitively, each query
term suggests other terms from its semantic neigh-
borhood, yet there are practical limits to this intui-
tion. X
i
 may not be an obvious or natural substitute
for X. A neighborhood can be drawn too small,
impacting recall, or too large, impacting precision.
Corpus analysis suggests an approach that is
both semantic and pragmatic. As noted in Hanks
(2005), languages provide constructions for build-
ing ad-hoc sets of items that can be considered
comparable in a given context. For instance, a co-
ordination of bare plurals suggests that two ideas
are related at a generic level, as in ?priests and
imams? or ?mosques and synagogues?. More gen-
erally, consider the pattern ?X and Y?, where X and
Y are proper-names (e.g., ?Zeus and Hera?), or X
and Y are inflected nouns or verbs with the same
inflection (e.g., the plurals ?cats and dogs? or the
verb forms ?kicking and screaming?). Millions of
matches for this pattern can be found in the Google
3-grams (Brants and Franz, 2006), allowing us to
build a map of comparable terms by linking the
root-forms of X and Y with a similarity score ob-
tained via a WordNet-based measure (e.g. see Bu-
danitsky and Hirst (2006) for a good selection).
The pragmatic neighborhood of a term X can be
defined as {X, X
1
, X
2
, ?, X
n
}, so that for each
X
i
, the Google 3-grams contain ?X+inf and
X
i
+inf? or ?X+inf and X
i
+inf?. The boundaries of
neighborhoods are thus set by usage patterns: if ?X
denotes the neighborhood of X, then ?artist
280
matches not just artist, composer and poet, but
studio,  portfolio and gallery, and many other
terms that are semantically dissimilar but prag-
matically linked to artist. Since each X
i
 ?  ?X is
ranked by similarity to X, query matches can also
be ranked by similarity.
When X is an adjective, then ?X matches any
element of {X, X
i
, X
2
, ?, X
n
}, where each X
i
pragmatically reinforces X, and X pragmatically
reinforces each X
i
. To ensure X and X
i
 really are
mutually reinforcing adjectives, we use the double-
ground simile pattern ?as X and X
i
 as? to harvest
{X
1
, ?, X
n
} for each X. Moreover, to maximize
recall, we use the Google API (rather than Google
ngrams) to harvest suitable bindings for X and X
i
from the web. For example, @witty = {charming,
clever, intelligent, entertaining, ?, edgy, fun}.
3.2 The Cultural Stereotype Wildcard   @X
Dickens claims in A Christmas Carol that ?the
wisdom of a people is in the simile?. Similes ex-
ploit familiar stereotypes to describe a less familiar
concept, so one can learn a great deal about a cul-
ture and its language from the similes that have the
most currency (Taylor, 1954). The wildcard @ X
builds on the results of Veale and Hao (2007) to
allow creative IR queries to retrieve matches on
the basis of cultural expectations. This foundation
provides a large set of adjectival features (over
2000) for a larger set of nouns (over 4000) denot-
ing stereotypes for which these features are salient.
If N is a noun, then @N matches any element
of the set {A
1
, A
2
, ?, A
n
}, where each A
i
 is an
adjective denoting a stereotypical property of N.
For example, @diamond matches any element of
{transparent, immutable, beautiful, tough,  expen-
sive, valuable, shiny, bright, lasting, desirable,
strong, ?, hard} . If A is an adjective, then @ A
matches any element of the set {N
1
, N
2
, ?, N
n
},
where each N
i
 is a noun denoting a stereotype for
which A is a culturally established property. For
example, @tall matches any element of {giraffe,
skyscraper, tree, redwood, tower, sunflower, light-
house, beanstalk,  rocket, ?, supermodel}.
Stereotypes crystallize in a language as clich?s,
so one can argue that stereotypes and clich?s are
little or no use to a creative IR system. Yet, as
demonstrated in Fishlov (1992), creative language
is replete with stereotypes, not in their clich?d
guises, but in novel and often incongruous combi-
nations. The creative value of a stereotype lies in
how it is used, as we?ll show later in section 4.
3.3 The Ad-Hoc Category Wildcard    ^X
Barsalou (1983) introduced the notion of an ad-
hoc category, a cross-cutting collection of often
disparate elements that cohere in the context of a
specific task or goal. The ad-hoc nature of these
categories is reflected in the difficulty we have in
naming them concisely: the cumbersome ?things to
take on a camping trip? is Barsalou?s most cited
example. But ad-hoc categories do not replace
natural kinds; rather, they supplement an existing
system of more-or-less rigid categories, such as the
categories found in WordNet.
The semantic wildcard ^C matches C and any
element of {C
1
, C
2
, ?, C
n
},  where each C
i
 is a
member of the category named by C. ^C can de-
note a fixed category in a resource like WordNet or
even Wikipedia; thus, ^fruit matches any member
of {apple, orange, pear, ?, lemon} and ^animal
any member of {dog, cat, mouse, ?, deer,  fox}.
Ad-hoc categories arise in creative IR when the
results of a query ? or more specifically, the bind-
ings for a query wildcard ? are funneled into a new
user-defined category. For instance, the query
?^fruit juice? matches any phrase in a text collec-
tion that denotes a named fruit juice, from ?lemon
juice? to ?pawpaw juice?. A user can now funnel
the bindings for ^fruit in this query into an ad-hoc
category juicefruit, to gather together those fruits
that are used for their juice. Elements of ^juicefruit
are ranked by the corpus frequencies discovered by
the original query; low-frequency juicefruit mem-
bers in the Google ngrams include coffee, raisin,
almond, carob and soybean. Ad-hoc categories
allow users of IR to remake a category system in
their own image, and create a new vocabulary of
categories to serve their own goals and interests, as
when ?^food pizza? is used to suggest disparate
members for the ad-hoc category pizzatopping.
The more subtle a query, the more disparate the
elements it can funnel into an ad-hoc category. We
now consider how basic semantic wildcards can be
combined to generate even more diverse results.
3.4 Compound Operators
Each wildcard maps a query term onto a set of ex-
281
pansion terms. The compositional semantics of a
wildcard combination can thus be understood in
set-theoretic terms. The most obvious and useful
combinations of ?, @ and ^ are described below:
??   Neighbor-of-a-neighbor: if ?X matches any
element of {X, X
1
, X
2
, ?, X
n
} then ??X matches
any of ?X ? ?X
1
  ? ? ? ?X
n
, where the ranking
of X
ij
 in ??X is a function of the ranking of X
i
 in
?X and the ranking of X
ij
 in ?X
i
.  Thus, ??artist
matches far more terms than ?artist, yielding more
diversity, more noise, and more creative potential.
@@  Stereotype-of-a-stereotype: if @X matches
any element of {X
1
, X
2
, ?, X
n
} then @@X
matches any of @X
1
 ? @X
2
  ? ? ?  @X
n
. For
instance, @@diamond matches any stereotype
that shares a salient property with diamond, and
@@sharp matches any salient property of any
noun for which sharp is a stereotypical property.
?@ Neighborhood-of-a-stereotype: if @X matches
any element of {X
1
, X
2
, ?, X
n
} then ? @ X
matches any of ?X
1
 ?  ?X
2
  ?  ? ? ?X
n
. Thus,
?@cunning matches any term in the pragmatic
neighborhood of a stereotype for cunning, while
?@knife matches any property that mutually rein-
forces any stereotypical property of knife
@?  Stereotypes-in-a-neighborhood: if ?X matches
any of {X, X
1
, X
2
, ?, X
n
} then @?X matches any
of @X ?  @X
1
  ?  ? ?  @X
n
. Thus, @?corpse
matches any salient property of any stereotype in
the neighborhood of corpse, while @?fast matches
any stereotype noun with a salient property that is
similar to, and reinforced by, fast.
?^ Neighborhood-of-a-category: if ^C matches
any of {C, C
1
, C
2
, ?, C
n
} then ?^C matches any
of ?C ? ?C
1
  ? ? ? ?C
n
.
^?   Categories-in-a-neighborhood: if ?X matches
any of {X, X
1
, X
2
, ?, X
n
} then ^?X matches any
of ^X ? ^X
1
  ? ? ? ^X
n
.
@^   Stereotypes-in-a-category: if ^C matches any
of {C, C
1
, C
2
, ?, C
n
} then @^C matches any of
@C ? @C
1
  ? ? ? @C
n
.
^@ Members-of-a-stereotype-category: if @ X
matches any element of {X
1
, X
2
, ?, X
n
} then
^@X matches any of ^X
1
 ?  ^X
2
  ?  ? ?  ^X
n
.
So ^@strong matches any member of a category
(such as warrior) that is stereotypically strong.
4 Applications of Creative Retrieval
The Google ngrams comprise a vast array of ex-
tracts from English web texts, of 1 to 5 words in
length (Brants and Franz, 2006). Many extracts are
well-formed phrases that give lexical form to many
different ideas. But an even greater number of
ngrams are not linguistically well-formed. The
Google ngrams can be seen as a lexicalized idea
space, embedded within a larger sea of noise.
Creative IR can be used to explore this idea space.
Each creative query is a jumping off point in a
space of lexicalized ideas that is implied by a large
corpus, with each successive match leading the
user deeper into the space. By turning matches into
queries, a user can perform a creative exploration
of the space of phrases and ideas (see Boden,
1994) while purposefully sidestepping the noise of
the Google ngrams. Consider the pleonastic query
?Catholic ?pope?. Retrieved phrases include, in
descending order of lexical similarity, ?Catholic
president?, ?Catholic politician?, ?Catholic king?,
?Catholic emperor? and ?Catholic patriarch?.
Suppose a user selects ?Catholic king?: the new
query ?Catholic ?king? now retrieves ?Catholic
queen?, ?Catholic court?, ?Catholic knight? ,
?Catholic kingdom? and ?Catholic throne?. The
subsequent query ?Catholic ?kingdom? in turn
retrieves ?Catholic dynasty? and ?Catholic army?,
among others.  In this way, creative IR allows a
user to explore the text-supported ramifications of
a metaphor like Popes are Kings (e.g., if popes are
kings, they too might have queens, command ar-
mies, found dynasties, or sit on thrones).
Creative IR gives users the tools to conduct
their own explorations of language. The more
wildcards a query contains, the more degrees of
freedom it offers to the explorer. Thus, the query
??scientist ?s ?laboratory? uncovers a plethora of
analogies for the relationship between scientists
and their labs: matches in the Google 3-grams in-
clude ?technician?s workshop?, ?artist?s studio?,
?chef?s kitchen? and ?gardener?s greenhouse?.
282
4.1 Metaphors with Aristotle
For a term X, the wildcard ?X suggests those other
terms that writers have considered to be compara-
ble to X, while ??X extrapolates beyond the cor-
pus evidence to suggest an even larger space of
potential comparisons. A meaningful metaphor can
be constructed for X by framing X with any
stereotype to which it is pragmatically comparable,
that is, any stereotype in ?X. Collectively, these
stereotypes can impart the properties @?X to X.
Suppose one wants to metaphorically ascribe
the property P to X. The set @P contains those
stereotypes for which P is culturally salient. Thus,
close metaphors for X (what MacCormac (1985)
dubs epiphors) in the context of P are suggested by
?X ? @P.  More distant metaphors (MacCormac
dubs these diaphors) are suggested by ??X ? @P.
For instance, to describe a scholar as wise, one can
use poet, yogi, philosopher or rabbi as compari-
sons. Yet even a simple metaphor will impart other
features to a topic.  If ^P
S
 denotes the ad-hoc set
of additional properties that may be inferred for X
when a stereotype S is used to convey property P,
then ^P
S
 = ?P ? @@P. The query ?^P
S 
X? now
finds corpus-attested elements of ^P
S
 that can
meaningfully be used to modify X.
These IR formulations are used by Aristotle, an
online metaphor generator, to generate targeted
metaphors that highlight a property P in a topic X.
Aristotle uses the Google ngrams to supply values
for ?X, ??X, ?P and ^P
S
. The system can be ac-
cessed at: www.educatedinsolence.com/aristotle
4.2 Expressing Attitude with Idiom Savant
Our retrieval goals in IR are often affective in na-
ture: we want to find a way of speaking about a
topic that expresses a particular sentiment and car-
ries a certain tone. However, affective categories
are amongst the most cross-cutting structures in
language. Words for disparate ideas are grouped
according to the sentiments in which they are gen-
erally held. We respect judges but dislike critics;
we respect heroes but dislike killers; we respect
sharpshooters but dislike snipers; and respect re-
bels but dislike insurgents. It seems therefore that
the particulars of sentiment are best captured by a
set of culture-specific ad-hoc categories.
We thus construct two ad-hoc categories,
^posword and ^negword, to hold the most obvi-
ously positive or negative words in Whissell?s
(1989) Dictionary of Affect. We then grow these
categories to include additional reinforcing ele-
ments from their pragmatic neighborhoods,
?^posword and ?^negword. As these categories
grow, so too do their neighborhoods, allowing a
simple semi-automated bootstrapping process to
significantly grow the categories over several it-
erations. We construct two phrasal equivalents of
these categories, ^posphrase and ^negphrase,
using the queries ?^posword - ^pastpart? (e.g.,
matching ?high-minded? and ?sharp-eyed?) and
?^negword - ^pastpart? (e.g., matching ?flat-
footed? and ?dead-eyed?) to mine affective phrases
from the Google 3-grams. The resulting ad-hoc
categories (of ~600 elements each) are manually
edited to fix any obvious mis-categorizations.
Idiom Savant is a web application that uses
^posphrase and ^negphrase to suggest flattering
and insulting epithets for a given topic. The query
?^posphrase ?X? retrieves phrases for a topic X
that put a positive spin on a related topic to which
X is sometimes compared, while ?^negphrase
?X? conversely imparts a negative spin. Thus, for
politician, the Google 4-grams provide the flatter-
ing epithets ?much-needed leader?, ?awe-inspiring
leader?, ?hands-on boss? and ?far-sighted states-
man?, as well as insults like ?power-mad leader?,
?back-stabbing boss?, ?ice-cold technocrat? and
?self-promoting hack?. Riskier diaphors can be
retrieved via ?^posphrase ??X? and ?^negphrase
? ? X ?. Idiom Savant is accessible online at:
www.educatedinsolence.com/idiom-savant/
4.3 Poetic Similes with The Jigsaw Bard
The well-formed phrases of a large corpus can be
viewed as the linguistic equivalent of objets trou-
v?s in art: readymade or ?found? objects that might
take on fresh meanings in a creative context. The
phrase ?robot fish?, for instance, denotes a more-
or-less literal object in the context of autonomous
robotic submersibles, but can also be used to con-
vey a figurative meaning as part of a creative com-
parison (e.g., ?he was as cold as a robot fish?).
Fishlov (1992) argues that poetic comparisons
are most resonant when they combine mutually-
reinforcing (if distant) ideas, to create memorable
images and evoke nuanced feelings. Building on
Fishlov?s argument, creative IR can be used to turn
283
the readymade phrases of the Google ngrams into
vehicles for creative comparison. For a topic X and
a property P, simple similes of the form ?X is as P
as S? are easily generated, where S ? @P ? ??X.
Fishlov would dub these non-poetic similes
(NPS). However, the query ??P @P? will retrieve
corpus-attested elaborations of stereotypes in @P
to suggest similes of the form ?X is as P as P
1
 S?,
where P
1
 ? ?P. These similes exhibit elements of
what Fishlov dubs poetic similes (PS). Why say
?as cold as a fish? when you can say ?as cold as a
wet fish?, ?a dead haddock?, ?a wet January?, ?a
frozen corpse?, or ?a  heartless robot?? Complex
queries can retrieve more creative combinations, so
?@P @P? (e.g. ?robot fish? or ?snow storm? for
cold), ??P @P @P? (e.g. ?creamy chocolate
mousse? for rich) and ?@P - ^pastpart @P? (e.g.
?snow-covered graveyard? and ?bullet-riddled
corpse? for cold) each retrieve ngrams that blend
two different but overlapping stereotypes.
Blended properties also make for nuanced
similes of the form ?as P and ?P as S?, where S ?
@P ? @?P. While one can be ?as rich as a fat
king?, something can be ?as rich and enticing as a
chocolate truffle?, ?a chocolate brownie?, ?a
chocolate fruitcake?, and even ?a chocolate king?.
The Jigsaw Bard is a web application that
harnesses the readymades of the Google ngrams to
formulate novel similes from existing phrases. By
mapping blended properties to ngram phrases that
combine multiple stereotypes, the Bard expands its
generative scope considerably, allowing this appli-
cation to generate hundreds of thousands of evoca-
tive comparisons. The Bard can be accessed online
at: www.educatedinsolence.com/jigsaw/
5 Empirical Evaluation
Though ^ is the most overtly categorical of our
wildcards, all three wildcards ? ?, @  and ^ ? are
categorical in nature. Each has a semantic or
pragmatic membership function that maps a term
onto an expansion set of related members. The
membership functions for specific uses of ^ are
created in an ad-hoc fashion by the users that ex-
ploit it; in contrast, the membership functions for
uses of @  and ? are derived automatically, via
pattern-matching and corpus analysis. Nonetheless,
ad-hoc categories in creative IR are often popu-
lated with the bindings produced by uses of @ and
? and combinations thereof. In a sense, ?X and
@X  and their variations are themselves ad-hoc
categories. But how well do they serve as catego-
ries? Are they large, but noisy? Or too small, with
limited coverage? We can evaluate the effective-
ness of ? and @ , and indirectly that of ^ too, by
comparing the use of ? and @ as category builders
to a hand-crafted gold standard like WordNet.
Other researchers have likewise used WordNet
as a gold standard for categorization experiments,
and we replicate here the experimental set-up of
Almuhareb and Poesio (2004, 2005), which is de-
signed to measure the effectiveness of web-
acquired conceptual descriptions. Almuhareb and
Poesio choose 214 English nouns from 13 of
WordNet?s upper-level semantic categories, and
proceed to harvest property values for these con-
cepts from the web using the Hearst-like pattern
?a|an|the * C is|was?. This pattern yields a com-
bined total of 51,045 values for all 214 nouns;
these values are primarily adjectives, such as hot
and black for coffee, but noun-modifiers of C are
also allowed, such as fruit for cake. They also har-
vest 8934 attribute nouns, such as temperature and
color, using the query ?the * of the C is|was? .
These values and attributes are then used as the
basis of a clustering algorithm to partition the 214
nouns back into their original 13 categories. Com-
paring these clusters with the original WordNet-
based groupings, Almuhareb and Poesio report a
cluster accuracy of 71.96% using just values like
hot and black (51,045 values), an accuracy of
64.02% using just attributes like temperature and
color (8,934 attributes), and an accuracy of 85.5%
using both together (a combined 59,979 features).
How concisely and accurately does @X de-
scribe a noun X for purposes of categorization? Let
^AP denote the set of 214 WordNet nouns used by
Almuhareb and Poesio. Then @^AP denotes a set
of 2,209 adjectival properties; this should be con-
trasted with the space of 51,045 adjectival values
used by Almuhareb and Poesio. Using the same
clustering algorithm over this feature set, @ X
achieves a clustering accuracy (as measured via
cluster purity) of 70.2%, compared to 71.96% for
Almuhareb and Poesio. However, when @X  is
used to harvest a further set of attribute nouns for
X, via web queries of the form ?the P  * of X ?
(where P ? @X), then @ X augmented with this
additional set of attributes (like hands for surgeon)
284
produces a larger space of 7,183 features. This in
turn yields a cluster accuracy of 90.2% which
contrasts with Almuhareb and Poesio?s 85.5% for
59,979 features. In either case, @X produces com-
parable clustering quality to Almuhareb and Poe-
sio, with just a small fraction of the features.
So how concisely and accurately does ?X de-
scribe a noun X for purposes of categorization?
While @X denotes a set of salient adjectives, ?X
denotes a set of comparable nouns. So this time,
?^AP denotes a set of 8,300 nouns in total, to act
as a feature space for the 214 nouns of Almuhareb
and Poesio. Remember, the contents of each ?X,
and of ?^AP overall, are determined entirely by
the contents of the Google 3-grams; the elements
of ?X are not ranked in any way, and all are treated
as equals. When the 8,300 features in ?^AP are
clustered into 13 categories, the resulting clusters
have a purity of 93.4% relative to WordNet. The
pragmatic neighborhood of X, ?X, appears to be an
accurate and concise proxy for the meaning of X.
What about adjectives? Almuhareb and Poe-
sio?s set of 214 words does not contain adjectives,
and besides, WordNet does not impose a category
structure on its adjectives. In any case, the role of
adjectives in the applications of section 4 is largely
an affective one: if X is a noun, then one must
have confidence that the adjectives in @X are con-
sonant with our understanding of X, and if P is a
property, that the adjectives in ?P evoke much the
same mood and sentiment as P. Our evaluation of
@X and ?P should thus be an affective one.
So how well do the properties in @X capture
our sentiments about a noun X? Well enough to
estimate the pleasantness of X from the adjectives
in @ X, perhaps? Whissell?s (1989) dictionary of
affect provides pleasantness ratings for a sizeable
number of adjectives and nouns (over 8,000 words
in total), allowing us to estimate the pleasantness
of X as a weighted average of the pleasantness of
each X
i
 in @X (the weights here are web frequen-
cies for the similes that underpin @ in section 3.2).
We thus estimate the affect of all stereotype nouns
for which Whissell also records a score. A two-
tailed Pearson test (p < 0.05) shows a positive cor-
relation of 0.5 between these estimates and the
pleasantness scores assigned by Whissell. In con-
trast, estimates based on the pleasantness of adjec-
tives found in corresponding WordNet glosses
show a positive correlation of just 0.278.
How well do the elements of ?P capture our
sentiments toward an adjective P? After all, we
hypothesize that the adjectives in ?P are highly
suggestive of P, and vice versa. Aristotle and the
Jigsaw Bard each rely on ?P to suggest adjectives
that evoke an unstated property in a metaphor or
simile, or to suggest coherent blends of properties.
When we estimate the pleasantness of each adjec-
tive P in Whissell?s dictionary via the weighted
mean of the pleasantness of adjectives in ?P (again
using web frequencies as weights), a two-tailed
Pearson test (p < 0.05) shows a correlation of 0.7
between estimates and actual scores. It seems ?P
does a rather good job of capturing the feel of P.
6 Concluding Remarks
Creative information retrieval is not a single appli-
cation, but a paradigm that allows us to conceive
of many different kinds of application for crea-
tively manipulating text. It is also a tool-kit for
implementing such an application, as shown here
in the cases of Aristotle, Idiom Savant and Jigsaw
Bard.
The  wildcards @, ? and ^ allow users to for-
mulate their own task-specific ontologies of ad-hoc
categories. In a fully automated application, they
provide developers with a simple but powerful vo-
cabulary for describing the range and relationships
of the words, phrases and ideas to be manipulated.
The @ , ? and ^ wildcards are just a start. We
expect other aspects of figurative language to be
incorporated into the framework whenever they
prove robust enough for use in an IR context. In
this respect, we aim to position Creative IR as an
open, modular platform in which diverse results in
FLP, from diverse researchers, can be meaning-
fully integrated. One can imagine wildcards for
matching potential puns, portmanteau words and
other novel forms, as well as wildcards for figura-
tive processes like metonymy, synecdoche, hyper-
bolae and even irony. Ultimately, it is hoped that
creative IR can serve as a textual bridge between
high-level creativity and the low-level creative
potentials that are implicit in a large corpus.
Acknowledgments
This work was funded in part by Science Founda-
tion Ireland (SFI), via the Centre for Next Genera-
tion Localization. (CNGL).
285
References
Almuhareb, A. and Poesio, M. (2004). Attribute-Based
and Value-Based Clustering: An Evaluation. In Proc.
of EMNLP 2004. Barcelona.
Almuhareb, A. and Poesio, M. (2005). Concept Learn-
ing and Categorization from the Web. In Proc. of the
27
th
 Annual meeting of the Cognitive Science Society.
Barnden, J. A. (2006). Artificial Intelligence, figurative
language and cognitive linguistics. In: G. Kristian-
sen, M. Achard, R. Dirven, and F. J. Ruiz de Men-
doza Ibanez (Eds.), Cognitive Linguistics: Current
Application and Future Perspectives, 431-459. Ber-
lin: Mouton de Gruyter.
Barsalou, L. W. (1983). Ad hoc categories. Memory and
Cognition, 11:211?227.
Boden, M. (1994). Creativity: A Framework for Re-
search, Behavioural & Brain Sciences 17(3):558-
568.
Brants, T. and Franz, A. (2006). Web 1T 5-gram Ver. 1.
Linguistic Data Consortium.
Budanitsky, A. and Hirst, G. (2006). Evaluating Word-
Net-based Measures of Lexical Semantic Related-
ness. Computational Linguistics, 32(1):13-47.
Falkenhainer, B., Forbus, K. and Gentner, D. (1989).
Structure-Mapping Engine: Algorithm and Exam-
ples. Artificial Intelligence, 41:1-63.
Fass, D. (1991). Met*: a method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49-90.
Fass, D. (1997). Processing Metonymy and Metaphor.
Contemporary Studies in Cognitive Science & Tech-
nology. New York: Ablex.
Fellbaum, C. (1998). WordNet: An Electronic Lexical
Database. MIT Press, Cambridge.
Fishlov, D. (1992). Poetic and Non-Poetic Simile:
Structure, Semantics, Rhetoric. Poetics Today, 14(1),
1-23.
Gentner, D. (1983), Structure-mapping: A Theoretical
Framework. Cognitive Science 7:155?170.
Guilford, J.P. (1950) Creativity, American Psychologist
5(9):444?454.
Hanks, P. (2005). Similes and Sets: The English Prepo-
sition ?like?. In: Blatn?, R. and Petkevic, V. (Eds.),
Languages and Linguistics: Festschrift for Fr. Cer-
mak. Charles University, Prague.
Hanks, P. (2006). Metaphoricity is gradable. In: Anatol
Stefanowitsch and Stefan Th. Gries (Eds.), Corpus-
Based Approaches to Metaphor and Metonymy,. 17-
35. Berlin: Mouton de Gruyter.
Hearst, M. (1992). Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14
th
 Int. Conf.
on Computational Linguistics, pp 539?545.
Indurkhya, B. (1992). Metaphor and Cognition: Studies
in Cognitive Systems. Kluwer Academic Publishers,
Dordrecht: The Netherlands.
Lin, D. (1998). Automatic retrieval and clustering of
similar words. In Proc. of the 17
th
 international con-
ference on Computational linguistics, 768-774.
MacCormac, E. R. (1985). A Cognitive Theory of Meta-
phor.  MIT Press.
Martin, J. H. (1990). A Computational Model of Meta-
phor Interpretation. New York: Academic Press.
Mason, Z. J. (2004). CorMet: A Computational, Cor-
pus-Based Conventional Metaphor Extraction Sys-
tem, Computational Linguistics, 30(1):23-44.
Mihalcea, R. (2002). The Semantic Wildcard.  In Proc.
of the LREC Workshop on Creating and Using Se-
mantics for Information Retrieval and Filtering. Ca-
nary Islands, Spain, May 2002.
Navigli, R. and Velardi, P. (2003). An Analysis of On-
tology-based Query Expansion Strategies. In Proc. of
the workshop on Adaptive Text Extraction and Min-
ing (ATEM 2003), at ECML 2003, the 14
th
 European
Conf. on Machine Learning, 42?49
Salton, G. (1968). Automatic Information Organization
and Retrieval. New York: McGraw-Hill.
Taylor, A. (1954). Proverbial Comparisons and Similes
from California. Folklore Studies 3. Berkeley: Uni-
versity of California Press.
Van Rijsbergen, C. J. (1979). Information Retrieval.
Oxford: Butterworth-Heinemann.
Veale, T. (2004). The Challenge of Creative Informa-
tion Retrieval. Computational Linguistics and Intelli-
gent Text Processing: Lecture Notes in Computer
Science, Volume 2945/2004, 457-467.
Veale, T. (2006). Re-Representation and Creative Anal-
ogy: A Lexico-Semantic Perspective. New Genera-
tion Computing 24, pp 223-240.
Veale, T. and Hao, Y. (2007). Making Lexical Ontolo-
gies Functional and Context-Sensitive. In Proc. of
the 46
th
 Annual Meeting of the Assoc. of Computa-
tional Linguistics.
Veale, T. and Hao, Y. (2010). Detecting Ironic Intent in
Creative Comparisons. In Proc. of ECAI?2010, the
19th European Conference on Artificial Intelligence.
286
Veale, T. and Butnariu, C. (2010). Harvesting and Un-
derstanding On-line Neologisms. In: Onysko, A. and
Michel, S. (Eds.), Cognitive Perspectives on Word
Formation. 393-416. Mouton De Gruyter.
Vernimb, C. (1977). Automatic Query Adjustment in
Document Retrieval. Information Processing &
Management. 13(6):339-353.
Voorhees, E. M. (1994). Query Expansion Using Lexi-
cal-Semantic Relations. In the proc. of SIGIR 94, the
17th International Conference on Research and De-
velopment in Information Retrieval. Berlin: Springer-
Verlag, 61-69.
Voorhees, E. M. (1998). Using WordNet for text re-
trieval. WordNet, An Electronic Lexical Database,
285?303. The MIT Press.
Way, E. C. (1991). Knowledge Representation and
Metaphor. Studies in Cognitive systems. Holland:
Kluwer.
Weeds, J. and Weir, D. (2005). Co-occurrence retrieval:
A flexible framework for lexical distributional simi-
larity. Computational Linguistics, 31(4):433?475.
Whissell, C. (1989). The dictionary of affect in lan-
guage. R. Plutchnik & H. Kellerman (Eds.) Emotion:
Theory and research. NY: Harcourt Brace, 113-131.
Wilks, Y. (1978). Making Preferences More Active,
Artificial Intelligence 11.
Xu, J. and Croft, B. W. (1996). Query expansion using
local and global document analysis.  In Proc. of the
19
th
 annual international ACM SIGIR conference on
Research and development in information retrieval.
287
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 14?19,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
Exploiting Readymades in Linguistic Creativity:
A System Demonstration of the Jigsaw Bard
Tony Veale
Yanfen Hao
School of Computer Science and Informatics, School of Computer Science and Informatics,
University College Dublin, University College Dublin,
Belfield, Dublin D4, Ireland. Belfield, Dublin D4, Ireland.
Tony.Veale@UCD.ie Yanfen.Hao@UCD.ie
Demonstration System can be viewed at: http://www.educatedinsolence.com/jigsaw
Abstract
Large lexical resources, such as corpora
and databases of Web ngrams, are a rich
source of pre-fabricated phrases that can be
reused in many different contexts. How-
ever, one must be careful in how these re-
sources are used, and noted writers such as
George Orwell have argued that the use of
canned phrases encourages sloppy thinking
and results in poor communication. None-
theless, while Orwell prized home-made
phrases over the readymade variety, there
is a vibrant movement in modern art which
shifts artistic creation from the production
of novel artifacts to the clever reuse of
readymades or objets trouv?s. We describe
here a system that makes creative reuse of
the linguistic readymades in the Google
ngrams. Our system, the Jigsaw Bard, thus
owes more to Marcel Duchamp than to
George Orwell. We demonstrate how tex-
tual readymades can be identified and har-
vested on a large scale, and used to drive a
modest form of linguistic creativity.
1 Introduction
In a much-quoted essay from 1946 entitled Politics
and the English Language, the writer and thinker
George Orwell outlines his prescription for halting
a perceived decline in the English language. He
argues that language and thought form a tight
feedback cycle that can be either virtuous or vi-
cious. Lazy language can thus promote lazy think-
ing, and vice versa. Orwell pours scorn on two
particular forms of lazy language: the expedient
use of overly familiar metaphors merely because
they come quickly to mind, even though they have
lost their power to evoke vivid images,; and the use
of readymade turns of phrase as substitutes for in-
dividually crafted expressions. While a good writer
bends words to his meaning, Orwell worries that a
lazy writer bends his meaning to convenient words.
Orwell is especially scornful about readymade
phrases which, when over-used, ?are tacked to-
gether like the sections of a prefabricated hen-
house.? A writer who operates by ?mechanically
repeating the familiar phrases? and ?gumming to-
gether long strips of words which have already
been set in order by someone else? has, he argues,
?gone some distance toward turning himself into a
machine.? Given his derogatory mechanistic view
of the use of readymade phrases, Orwell would not
be surprised to learn that computers are highly pro-
ficient in the large-scale use of familiar phrases,
whether acquired from large text corpora or from
the Google ngrams (see Brants and Franz, 2006).
Though argued with passion, there are serious
holes in Orwell?s logic. If one should ?never  use a
metaphor, simile or other figure of speech which
you are used to seeing in print?, how then are fa-
miliar metaphors ever to become dead metaphors
and thereby enrich the language with new terms
and new senses? And if one cannot use familiar
readymade phrases, how can one make playful ?
and creative ? allusions to the writings of others, or
14
mischievously subvert the conventional wisdom of
platitudes and clich?s? Orwell?s use of the term
readymade is entirely negative, yet the term is al-
together more respectable in the world of modern
art, thanks to its use by artists such as Marcel
Duchamp. For many artists, a readymade object is
not a substitute, but a starting point, for creativity.
Also called an objet trouv? or found object, a
readymade emerges from an artist?s encounter with
an object whose aesthetic merits are overlooked in
its banal, everyday contexts of use; when this ob-
ject is moved to an explicitly artistic context, such
as an art gallery, viewers are better able to appreci-
ate these merits. The artist?s insight is to recognize
the transformational power of this non-obvious
context switch. Perhaps the most famous (and no-
torious) readymade in the world of art is Marcel
Duchamp?s Fountain, a humble urinal that be-
comes an elegantly curved piece of sculpture when
viewed with the right mindset. Duchamp referred
to his objets trouv?s as ?assisted readymades? be-
cause they allow an artist to remake the act of
creation as one of pure insight and inspired recog-
nition rather than one of manual craftsmanship (see
Taylor, 2009). In computational terms, the
Duchampian notion of a readymade allows crea-
tivity to be modeled not as a construction problem
but as a decision problem. A computational
Duchamp need not explore an abstract conceptual
space of potential ideas, as in Boden (1994). How-
ever, a Duchampian agent must instead be exposed
to the multitude of potentially inspiring real-world
stimuli that a human artist encounters everyday.
Readymades represent a serendipitous form of
creativity that is poorly served by exploratory
models of creativity, such as that of Boden (1994),
and better served by the investment models such as
the buy-low-sell-high theory of Sternberg and Lu-
bart (1995). In this view, creators and artists find
unexpected or untapped value in unfashionable
objects or ideas that already exist, and quickly
move their gaze elsewhere once the public at large
come to recognize this value. Duchampian creators
invest in everyday objects, just as Duchamp found
artistic merit in urinals, bottles and combs. From a
linguistic perspective, these everyday objects are
commonplace words and phrases which, when
wrenched from their conventional contexts of use,
are free to take on enhanced meanings and provide
additional returns to the investor. The realm in
which a maker of linguistic readymades operates is
not the real world, and not an abstract conceptual
space, but the realm of texts: large corpora become
rich hunting grounds for investors in linguistic ob-
jets trouv?s.
This proposal is demonstrated in computa-
tional form in the following sections. We show
how a rich vocabulary of cultural stereotypes can
be acquired from the Web, and how this vocabu-
lary facilitates the implementation of a decision
procedure for recognizing potential readymades in
large corpora ? in this case, the Google database of
Web ngrams (Brants and Franz, 2006). This deci-
sion procedure provides a robust basis for a simile-
generation system called The Jigsaw Bard. The
cognitive / linguistic intuitions that underpin the
Bard?s concept of textual readymades are put to
the empirical test in section 5. While readymades
remain a contentious notion in the public?s appre-
ciation of artistic creativity ? despite Duchamp?s
Fountain being considered one of the most influ-
ential artworks of the 20
th
 century ? we shall show
that the notion of a linguistic readymade has sig-
nificant practical merit in the realms of text gen-
eration and computational creativity.
2 Linguistic Readymades
Readymades are the result of artistic appropria-
tion, in which an object with cultural resonance ?
an image, a phrase, a quote, a name, a thing ? is re-
used in a new context with a new meaning. As a
fertile source of cultural reference points, language
is an equally fertile medium for appropriation.
Thus, in the constant swirl of language and culture,
movie quotes suggest song lyrics, which in turn
suggest movie titles, which suggest book titles, or
restaurant names, or the names of racehorses, and
so on, and on. The 1996 movie The Usual Suspects
takes its name from a memorable scene in 1942?s
Casablanca, as does the Woody Allen play and
movie Play it Again Sam. The 2010 art documen-
tary Exit Through the Gift Shop, by graffiti artist
Banksy, takes its name from a banal sign some-
times seen in museums and galleries: the sign,
suggestive as it is of creeping commercialism,
makes the perfect readymade for a film that la-
ments the mediocrity of commercialized art.
Appropriations can also be combined to pro-
duce novel mashups; consider, for instance, the use
of tweets from rapper Kanye West as alternate
15
captions for cartoon images from the New Yorker
magazine (see hashtag #KanyeNew-YorkerTweets).
Hashtags can themselves be linguistic readymades.
When free-speech advocates use the hashtag
#IAMSpartacus  to show solidarity with users
whose tweets have incurred the wrath of the law,
they are appropriating an emotional line from the
1960 film Spartacus. Linguistic readymades, then,
are well-formed text fragments that are often
highly quotable because they carry some figurative
content which can be reused in different contexts.
A quote like ?round up the usual suspects? or
?I am Spartacus? requires a great deal of cultural
knowledge to appreciate. Since literal semantics
only provides a small part of their meaning, a
computer?s ability to recognize linguistic ready-
mades is only as good as the cultural knowledge at
its disposal. We thus explore here a more modest
form of readymade ? phrases that can be used as
evocative image builders in similes ? as in:
a wet haddock
snow in January
a robot fish
a bullet-ridden corpse
Each phrase can be found in the Google 1T data-
base of Web ngrams ? snippets of Web text (of one
to five words) that occur on the web with a fre-
quency of 40 or higher (Brants and Franz, 2006).
Each is likely a literal description of a real object
or event ? even ?robot fish?, which describes an
autonomous marine vehicle whose movements
mimic real fish. But each exhibits figurative po-
tential as well, providing a memorable description
of physical or emotional coldness. Whether or not
each was ever used in a figurative sense before is
not the point: once this potential is recognized,
each phrase becomes a reusable linguistic ready-
made for the construction of a vivid figurative
comparison, as in ?as cold as a robot fish?. We
now consider the building blocks from which these
comparisons can be ready-made..
3 A Vocabulary of Cultural Stereotypes
How does a computer acquire the knowledge that
fish, snow, January, bullets and corpses are cultural
signifiers of coldness? Much the same way that
humans acquire this knowledge: by attending to
the way these signifiers are used by others, espe-
cially when they are used in cultural clich?s like
proverbial similes (e.g., ?as cold as a fish?).
In fact, folk similes are an important vector in
the transmission of cultural knowledge: they point
to, and exploit, the shared cultural touchstones that
speakers and listeners alike can use to construct
and intuit meanings. Taylor (1954) catalogued
thousands of proverbial comparisons and similes
from California, identifying just as many building
blocks in the construction of new phrases and figu-
rative meanings. Only the most common similes
can be found in dictionaries, as shown by Norrick
(1986), while Moon (2008) demonstrates that
large-scale corpus analysis is needed to identify
folk similes with a breadth approaching that of
Taylor?s study. However, Veale and Hao (2007)
show that the World-Wide Web is the ultimate re-
source for harvesting similes.
Veale and Hao use the Google API to find many
instances of the pattern ?as ADJ as a|an *? on the
web, where ADJ is an adjectival property and * is
the Google wildcard. WordNet (Fellbaum, 1998) is
used to provide a set of over 2,000 different values
for ADJ, and the text snippets returned by Google
are parsed to extract the basic simile bindings.
Once the bindings are annotated to remove noise,
as well as frequent uses of irony, this Web harvest
produces over 12,000 cultural bindings between a
noun (such as fish, or robot) and its most stereo-
typical properties (such as cold, wet, stiff, logical,
heartless, etc.). Stereotypical properties are ac-
quired for approx. 4,000 common English nouns.
This is a set of building blocks on a larger scale
than even that of Taylor, allowing us to build on
Veale and Hao (2007) to identify readymades in
their hundreds of thousands in the Google ngrams.
However, to identify readymades as resonant
variations on cultural stereotypes, we need a cer-
tain fluidity in our treatment of adjectival proper-
ties. The phrase ?wet haddock?  is a readymade for
coldness because ?wet? accentuates the ?cold? that
we associate with ?haddock? (via the web simile
?as cold as a haddock?). In the words of Hofstad-
ter (1995), we need to build a SlipNet of properties
whose structure captures the propensity of proper-
ties to mutually and coherently reinforce each
other, so that phrases which subtly accentuate an
unstated property can be recognized. In the vein of
Veale and Hao (2007), we use the Google API to
harvest the elements of this SlipNet.
16
We hypothesize that the construction ?as ADJ
1
and ADJ
2
 as? shows ADJ
1 
and ADJ
2
 to be mutu-
ally reinforcing properties, since they can be seen
to work together as a single complex property in a
single comparison. Thus, using the full comple-
ment of adjectival properties used by Veale and
Hao (2007), we harvest all instances of the patterns
?as ADJ and * as? and ?as * and ADJ as? from
Google, noting the combinations that are found and
their frequencies. These frequencies provide link
weights for the Hofstadter-style SlipNet that is
then constructed. In all, over 180,000 links are
harvested, connecting over 2,500 adjectival prop-
erties to one other. We put the intuitions behind
this SlipNet to the empirical test in section five.
4 Harvesting Readymades from Corpora
In the course of an average day, a creative writer is
exposed to a constant barrage of linguistic stimuli,
any small portion of which can strike a chord as a
potential readymade. In this casual inspiration
phase, the observant writer recognizes that a cer-
tain combination of words may produce, in another
context, a meaning that is more than the sum of its
parts. Later, when an apposite phrase is needed to
strike a particular note, this combination may be
retrieved from memory (or from a trusty note-
book), if it has been recorded and suitably indexed.
Ironically, Orwell (1946) suggests that lazy
writers ?shirk? their responsibility to be ?scrupu-
lous? in their use of language by ?simply throwing
[their] mind open and letting the ready-made
phrases come crowding in?. For Orwell, words just
get in the way, and should be kept at arm?s length
until the writer has first allowed a clear meaning to
crystallize. This is dubious advice, as one expects a
creative writer to keep an open mind when consid-
ering all the possibilities that present themselves.
Yet Orwell?s proscription suggests how a computer
should go about the task of harvesting readymades
from corpora: by throwing its mind open to the
possibility that a given ngram may one day have a
second life as a creative readymade in another
context, the computer allows the phrases that
match some simple image-building criteria to come
crowding in, so they can be stored in a database.
Given a rich vocabulary of cultural stereo-
types and their properties, computers are capable
of indexing and recalling a considerably larger
body of resonant combinations than the average
human. The necessary barrage of linguistic stimuli
can be provided by the Google 1T database of Web
ngrams (Brants and Franz, 2006). Trawling these
ngrams, a modestly creative computer can recog-
nize well-formed combinations of cultural ele-
ments that might serve as a vivid vehicle of
description in a future comparison. For every
phrase P in the ngrams, where P combines stereo-
type nouns and/or adjectival modifiers, the com-
puter simply poses the following question: is there
an unstated property A such that the simile ?as A
as P? is a meaningful and memorable comparison?
The property A can be simple, as in ?as dark as a
chocolate espresso?, or complex, as in ?as dark
and sophisticated as a chocolate martini?. In either
case, the phrase P is tucked away, and indexed un-
der the property A until such time as the computer
needs to produce a vivid evocation of A.
The following patterns are used to identify
potential readymades in the Web ngrams:
(1) Noun
S1
 Noun
S2
where both nouns denote stereotypes that
share an unstated property Adj
A
. The prop-
erty Adj
A
 serves to index this combination.
Example: ?as cold as a robot fish?.
(2) Noun
S1
 Noun
S2
where both nouns denote stereotypes with
salient properties Adj
A
1 
and Adj
A
2 
respec-
tively, such that Adj
A
1 
and Adj
A
2 
are mutu-
ally reinforcing. The combination is indexed
on Adj
A
1
+Adj
A2
. Example: ?as dark and
sophisticated as a chocolate martini?.
(3)  Adj
A
  Noun
S
where Noun
S
 denotes a cultural stereotype,
and the adjective Adj
A
 
denotes a property
that mutually reinforces an unstated but sali-
ent property Adj
SA
 
of the stereotype. Exam-
ple: ?as cold as a wet haddock?. The
combination is indexed on Adj
SA
.
More complex structures for P are also possible, as
in the phrases ?a lake of tears? (a melancholy way
to accentuate the property ?wet?) and ?a statue in a
library? (for ?silent? and ?quiet?). In this current
description, we focus on 2-gram phrases only.
17
Figure 1. Screenshot of The Jigsaw Bard, retrieving
linguistic readymades for the input property ?cold?. See
http://www.educatedinsolence.com/jigsaw
Using these patterns, our application ? the Jigsaw
Bard (see Figure 1) ? pre-builds a vast collection
of figurative similes well in advance of the time it
is asked to use or suggest any of them. Each phrase
P is syntactically well-formed, and because P oc-
curs relatively frequently on the Web, it is likely to
be semantically well-formed as well. Just as
Duchamp side-stepped the need to physically
originate anything, but instead appropriated pre-
fabricated artifacts, the Bard likewise side-steps
the need for natural-language generation. Each
phrase it proposes has the ring of linguistic
authenticity; because this authenticity is rooted in
another, more literal context, the Bard also exhibits
its own Duchamp-like (if Duchamp-lite) creativity.
We now consider the scale of the Bard?s genera-
tivity, and the quality of its insights.
5 Empirical Evaluation
The vastness of the web, captured in the large-
scale sample that is the Google ngrams, means the
Jigsaw Bard finds considerable grist for its mill in
the phrases that match (1)?(3). Thus, the most
restrictive pattern, pattern (1), harvests approx.
20,000 phrases from the Google 2-grams, for al-
most a thousand simple properties (indexing an
average of 29 phrases under each property, such as
?swan song? for ?beautiful?). Pattern (2) ? which
allows a blend of stereotypes to be indexed under a
complex property ? harvests approx. 170,000
phrases from the 2-grams, for approx. 70,000 com-
plex properties (indexing an average of 12 phrases
under each, such as ?hospital bed? for ?comfort-
able and safe?). Pattern (3) ? which pairs a stereo-
type noun with an adjective that draws out a salient
property of the stereotype ? is similarly productive:
it harvests approx. 150,000 readymade 2-grams for
over 2,000 simple properties (indexing an average
of 125 phrases per property, as in ?youthful knight?
for ?heroic? and ?zealous convert? for ?devout?).
The Jigsaw Bard is best understood as a crea-
tive thesaurus: for any given property (or blend of
properties) selected by the user, the Bard presents
a range of apt similes constructed from linguistic
readymades. The numbers above show that, recall-
wise, the Bard has sufficient coverage to work
robustly as a thesaurus. Quality-wise, users must
make their own determinations as to which similes
are most suited to their descriptive purposes, yet it
is important that suggestions provided by the Bard
are sensible and well-motivated. As such, we must
be empirically satisfied about two key intuitions:
first, that salient properties are indeed acquired
from the Web for our vocabulary of stereotypes
(this point relates to the aptness of the similes sug-
gested by the Bard); and second, that the adjectives
connected by the SlipNet really do mutually rein-
force each other (this point relates to the coherence
of complex properties, and to the ability of ready-
mades to accentuate unstated properties).
Both intuitions can be tested using Whissell?s
(1989) dictionary of affect, a psycholinguistic re-
source used for sentiment analysis that assigns a
pleasantness score of between 1.0 (least pleasant)
and 3.0 (most pleasant) to over 8,000 common-
place words. We should thus be able to predict the
pleasantness of a stereotype noun (like fish) using a
weighted average of the pleasantness of its salient
properties (like cold, slippery). We should also be
able to predict the pleasantness of an adjective us-
ing a weighted average of the pleasantness of its
adjacent adjectives in the SlipNet. (In each case,
weights are provided by relevant web frequencies.)
We can use a two-tailed Pearson test (p <
0.05) to compare the predictions made in each case
to the actual pleasantness scores provided by
Whissell?s dictionary, and thereby assess the qual-
ity of the knowledge used to make the predictions.
In the first case, predictions of the pleasantness of
stereotype nouns based on the pleasantness of their
salient properties (i.e., predicting the pleasantness
of Y from the Xs in ?as X as Y?) have a positive
18
correlation of 0.5 with Whissell; conversely, ironic
properties yield a negative correlation of ?0.2. In
the second, predictions of the pleasantness of ad-
jectives based on their relations in the SlipNet (i.e.,
predicting the pleasantness of X from the Ys in ?as
X and Y as?) have a positive correlation of 0.7.
Though pleasantness is just one dimension of lexi-
cal affect, it is one that requires a broad knowledge
of a word, its usage and its denotations to accu-
rately estimate. In this respect, the Bard is well
served by a large stock of stereotypes and a coher-
ent network of informative properties.
6 Conclusions
Fishlov (1992) has argued that poetic similes rep-
resent a conscious deviation from the norms of
non-poetic comparison. His analysis shows that
poetic similes are longer and more elaborate, and
are more likely to be figurative and to flirt with
incongruity. Creative similes do not necessarily
use words that are longer, or rarer, or fancier, but
use many of the same cultural building blocks as
non-creative similes. Armed with a rich vocabulary
of building blocks, the Jigsaw Bard harvests a
great many readymade phrases from the Google
ngrams ? from the evocative ?chocolate martini? to
the seemingly incongruous ?robot fish? ? that can
be used to evoke an wide range of properties.
This generativity makes the Bard scalable and
robust. However, any creativity we may attribute
to it comes not from the phrases themselves ? they
are readymades, after all ? but from the recognition
of the subtle and often complex properties they
evoke. The Bard exploits a sweet-spot in our un-
derstanding of linguistic creativity, and so, as pre-
sented here, is merely a starting point for our
continued exploitation of linguistic readymades,
rather than an end in itself. By harvesting more
complex syntactic structures, and using more so-
phisticated techniques for analyzing the figurative
potential of these phrases, the Bard and its ilk may
gradually approach the levels of poeticity dis-
cussed by Fishlov. For now, it is sufficient that
even simple techniques serve as the basis of a ro-
bust and practical thesaurus application.
7 Hardware Requirements
The Jigsaw Bard is designed to be a lightweight
application that compiles its comprehensive data-
base of readymades in advance. It?s run-time de-
mands are low, it has no special hardware
requirements, and runs in a standard Web browser.
Acknowledgments
This work was funded in part by Science Founda-
tion Ireland (SFI), via the Centre for Next Genera-
tion Localization (CNGL).
References
Margaret Boden, 1994. Creativity: A Framework for
Research, Behavioural and Brain Sciences 17(3),
558-568.
Thorsten Brants. and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium.
Christiane Fellbaum. (ed.) 2008. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge.
David Fishlov. 1992. Poetic and Non-Poetic Simile:
Structure, Semantics, Rhetoric. Poetics Today, 14(1).
Douglas R Hofstadter. 1995. Fluid Concepts and Crea-
tive Analogies: Computer Models of the Fundamen-
tal Mechanisms of Thought. Basic Books, NY.
Rosamund Moon. 2008.  Conventionalized as-similes in
English: A problem case. International Journal of
Corpus Linguistics 13(1), 3-37.
Neal Norrick,. 1986.  Stock Similes. Journal of Literary
Semantics XV(1), 39-52.
George Orwell. 1946. Politics And The English Lan-
guage. Horizon 13(76), 252-265.
Robert J Sternberg. and T. Ivan Lubart, 1995. Defying
the crowd: Cultivating creativity in a culture of con-
formity. Free Press, New York.
Archer Taylor. 1954. Proverbial Comparisons and
Similes from California. Folklore Studies 3. Ber-
keley: University of California Press.
Michael R. Taylor. (2009). Marcel Duchamp: ?tant
donn?s (Philadelphia Museum of Art). Yale Univer-
sity Press.
Tony Veale and Yanfen Hao. 2007. Making Lexical
Ontologies Functional and Context-Sensitive. In
Proceedings of the 46
th
 Annual Meeting of the Asso-
ciation of Computational Linguistics.
Cynthia Whissell. 1989. The dictionary of affect in lan-
guage. In R. Plutchnik & H. Kellerman (eds.) Emo-
tion: Theory and research. New York: Harcourt
Brace, 113-131.
19
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 75?79,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Context-sensitive, Multi-faceted model of Lexico-Conceptual Affect  Tony Veale Web Science and Technology Division, KAIST, Daejeon, South Korea. Tony.Veale@gmail.com   Abstract Since we can ?spin? words and concepts to suit our affective needs, context is a major determinant of the perceived affect of a word or concept. We view this re-profiling as a selective emphasis or de-emphasis of the qualities that underpin our shared stere-otype of a concept or a word meaning, and construct our model of the affective lexicon accordingly. We show how a large body of affective stereotypes can be acquired from the web, and also show how these are used to create and interpret affective metaphors. 
1 Introduction The builders of affective lexica face the vexing task of distilling the many and varied pragmatic uses of a word or concept into an overall semantic measure of affect. The task is greatly complicated by the fact that in each context of use, speakers may implicitly agree to focus on just a subset of the salient features of a concept, and it is these fea-tures that determine contextual affect. Naturally, disagreements arise when speakers do not implicit-ly arrive at such a consensus, as when people disa-gree about hackers: advocates often focus on qualities that emphasize curiosity or technical vir-tuosity, while opponents focus on qualities that emphasize criminality and a disregard for the law. In each case, it is the same concept, Hacker, that is being described, yet speakers can focus on differ-ent qualities to arrive at different affective stances. Any gross measure of affect (such as e.g., that hackers are good or bad) must thus be grounded in a nuanced model of the stereotypical properties and behaviors of the underlying word-concept. As different stereotypical qualities are highlighted or 
de-emphasized in a given context ? a particular metaphor, say, might describe hackers as terrorists or hackers as artists ? we need to be able to re-calculate the perceived affect of the word-concept. This paper presents such a stereotype-grounded model of the affective lexicon. After reviewing the relevant background in section 2, we present the basis of the model in section 3. Here we describe how a large body of feature-rich stereotypes is ac-quired from the web and from local n-grams. The model is evaluated in section 4. We conclude by showing the utility of the model to that most con-textual of NLP phenomena ? affective metaphor. 2 Related Work and Ideas In its simplest form, an affect lexicon assigns an affective score ? along one or more dimensions ? to each word or sense. For instance, Whissell?s (1989) Dictionary of Affect (or DoA) assigns a trio of scores to each of its 8000+ words to describe three psycholinguistic dimensions: pleasantness, activation and imagery. In the DoA, the lowest pleasantness score of 1.0 is assigned to words like abnormal and ugly, while the highest, 3.0, is as-signed to words like wedding and winning. Though Whissell?s DoA is based on human ratings, Turney (2002) shows how affective valence can be derived from measures of word association in web texts.  Human intuitions are prized in matters of lexi-cal affect. For reliable results on a large-scale, Mo-hammad & Turney (2010) and Mohammad & Yang (2011) thus used the Mechanical Turk to elicit human ratings of the emotional content of words. Ratings were sought along the eight dimen-sions identified in Plutchik (1980) as primary emo-tions: trust , anger, anticipation, disgust, fear, joy, sadness and surprise. Automated tests were used to exclude unsuitable raters. In all, 24,000+ word-sense pairs were annotated by five different raters. 
75
 Liu et al (2003) also present a multidimension-al affective model that uses the six basic emotion categories of Ekman (1993) as its dimensions: happy, sad, angry, fearful, disgusted and surprised. These authors base estimates of affect on the con-tents of Open Mind, a common-sense knowledge-base (Singh, 2002) harvested from contributions of web volunteers. These contents are treated as sen-tential objects, and a range of NLP models is used to derive affective labels for the subset of contents (~10%) that appear to convey an emotional stance. These labels are then propagated to related con-cepts (e.g., excitement is propagated from roller-coasters to amusement parks) so that the implicit affect of many other concepts can be determined.  Strapparava and Valitutti (2004) provide a set of affective annotations for a subset of WordNet?s synsets in a resource called Wordnet-affect. The annotation labels, called a-labels, focus on the cognitive dynamics of emotion, allowing one to distinguish e.g. between words that denote an emo-tion-eliciting situation and those than denote an emotional response. Esuli and Sebastiani (2006) also build directly on WordNet as their lexical plat-form, using a semi-supervised learning algorithm to assign a trio of numbers ? positivity, negativity and neutrality ? to word senses in their newly de-rived resource, SentiWordNet. (Wordnet-affect also supports these three dimensions as a-labels, and adds a fourth, ambiguous). Esuli & Sebastiani (2007) improve on their affect scores by running a variant of the PageRank algorithm (see also Mihal-cea and Tarau, 2004) on the graph structure that tacitly connects word-senses in WordNet to each other via the words used in their textual glosses.  These lexica attempt to capture the affective profile of a word/sense when it is used in its most normative and stereotypical guise, but they do so without an explicit model of stereotypical mean-ing. Veale & Hao (2007) describe a web-based approach to acquiring such a model. They note that since the simile pattern ?as ADJ as DET NOUN? presupposes that NOUN is an exemplar of ADJness, it follows that ADJ must be a highly sa-lient property of NOUN. Veale & Hao harvested tens of thousands of instances of this pattern from the Web, to extract sets of adjectival properties for thousands of commonplace nouns. They show that if one estimates the pleasantness of a term like snake or artist as a weighted average of the pleas-antness of its properties (like sneaky or creative) in 
a resource like Whissell?s DoA, then the estimated scores show a reliable correlation with the DoA?s own scores. It thus makes computational sense to calculate the affect of a word-concept as a function of the affect of its most salient properties. Veale (2011) later built on this work to show how a prop-erty-rich stereotypical representation could be used for non-literal matching and retrieval of creative texts, such as metaphors and analogies.  Both Liu et al (2003) and Veale & Hao (2010) argue for the importance of common-sense knowledge in the determination of affect. We in-corporate ideas from both here, while choosing to build mainly on the latter, to construct a nuanced, two-level model of the affective lexicon. 3 An Affective Lexicon of Stereotypes We construct the stereotype-based lexicon in two stages. For the first layer, a large collection of ste-reotypical descriptions is harvested from the web. As in Liu et al (2003), our goal is to acquire a lightweight common-sense representation of many everyday concepts. For the second layer, we link these common-sense qualities in a support graph that captures how they mutually support each other in their co-description of a stereotypical idea. From this graph we can estimate pleasantness and un-pleasantness valence scores for each property and behavior, and for the stereotypes that exhibit them. Expanding on the approach in Veale (2011), we use two kinds of query for harvesting stereotypes from the web. The first, ?as ADJ as a NOUN?, ac-quires typical adjectival properties for noun con-cepts; the second, ?VERB+ing like a NOUN? and ?VERB+ed like a NOUN?, acquires typical verb behaviors. Rather than use a wildcard * in both positions (ADJ and NOUN, or VERB and NOUN), which gives limited results with a search engine like Google, we generate fully instantiated similes from hypotheses generated via the Google n-grams (Brants & Franz, 2006). Thus, from the 3-gram ?a drooling zombie? we generate the query ?drooling like a zombie?, and from the 3-gram ?a mindless zombie? we generate ?as mindless as a zombie?. Only those queries that retrieve one or more Web documents via the Google API indicate the most promising associations. This still gives us over 250,000 web-validated simile associations for our stereotypical model, and we filter these manu-ally, to ensure that the lexicon is both reusable and 
76
of the highest quality. We obtain rich descriptions for many stereotypical ideas, such as Baby, which is described via 163 typical properties and behav-iors like crying, drooling and guileless. After this phase, the lexicon maps each of 9,479 stereotypes to a mix of 7,898 properties and behaviors. We construct the second level of the lexicon by automatically linking these properties and behav-iors to each other in a support graph. The intuition here is that properties which reinforce each other in a single description (e.g. ?as lush and green as a jungle? or ?as hot and humid as a sauna?) are more likely to have a similar affect than properties which do not support each other. We first gather all Google 3-grams in which a pair of stereotypical properties or behaviors X and Y are linked via co-ordination, as in ?hot and humid? or ?kicking and screaming?. A bidirectional link between X and Y is added to the support graph if one or more stereo-types in the lexicon contain both X and Y. If this is not so, we also ask whether both descriptors ever reinforce each other in Web similes, by posing the web query ?as X and Y as?. If this query has non-zero hits, we still add a link between X and Y. Let N denote this support graph, and N(p) de-note the set of neighboring terms to p, that is, the set of properties and behaviors that can mutually support a property p. Since every edge in N repre-sents an affective context, we can estimate the like-lihood that p is ever used in a positive or negative context if we know the positive or negative affect of enough members of N(p). So if we label enough vertices of N with + / ? labels, we can interpolate a positive/negative affect for all vertices p in N. We thus build a reference set -R of typically negative words, and a set +R of typically positive words. Given a few seed members of -R (such as sad, evil, etc.) and a few seed members of +R (such as happy, wonderful, etc.), we find many other candidates to add to +R and -R by consider-ing neighbors of these seeds in N. After just three iterations, +R and -R contain ~2000 words each.     For a property p, we define N+(p) and N-(p) as    (1)        N+(p) = N(p) ? +R    (2)        N-(p) = N(p) ? -R We assign pos/neg valence scores to each property p  by interpolating from reference values to their neighbors in N. Unlike that of Takamura et al (2005), the approach is non-iterative and involves 
no feedback between the nodes of N, and thus, no inter-dependence between adjacent affect scores:    (3)   pos(p)   =           |N+(p)|   |N+(p) ? N-(p)|    (4)   neg(p)   =        1  -  pos(p) If a term S denotes a stereotypical idea and is de-scribed via a set of typical properties and behaviors typical(S) in the lexicon, then: 
   (5)        pos(S)   =        ?p?typical(S) pos(p)               |typical(S)| 
   (6)        neg(S)   = 1  -  pos(S) Thus, (5) and (6) calculate the mean affect of the properties and behaviors of S, as represented via typical(S). We can now use (3) and (4) to separate typical(S) into those elements that are more nega-tive than positive (putting an unpleasant spin on S in context) and those that are more positive than negative (putting a pleasant spin on S in context): (7)  posTypical(S)  = {p | p ? typical(S) ? pos(p) > 0.5} (8)  negTypical(S)  = {p | p ? typical(S) ? neg(p) > 0.5} 4 Empirical Evaluation  In the process of populating +R and -R, we identi-fy a reference set of 478 positive stereotype nouns (such as saint and hero) and 677 negative stereo-type nouns (such as tyrant and monster). We can use these reference stereotypes to test the effec-tiveness of (5) and (6), and thus, indirectly, of (3) and (4) and of the affective lexicon itself. Thus, we find that 96.7% of the stereotypes in +R are cor-rectly assigned a positivity score greater than 0.5 (pos(S) > neg(S)) by (5), while 96.2% of the stere-otypes in -R are correctly assigned a negativity score greater than 0.5 (neg(S) > pos(S)) by (6). We can also use +R and -R as a gold standard for evaluating the separation of typical(S) into dis-tinct positive and negative subsets posTypical(S) and negTypical(S) via (7) and (8). The lexicon con-tains 6,230 stereotypes with at least one property in +R?-R. On average, +R?-R contains 6.51 of the properties of each of these stereotypes, where, on average, 2.95 are in +R while 3.56 are in -R. In a perfect separation, (7) should yield a posi-tive subset that contains only those properties in 
77
typical(S)?+R, while (8) should yield a negative subset that contains only those in typical(S)?-R. Macro Averages (6230 stereotypes) Positive properties Negative properties Precision .962 .98 Recall .975 .958 F-Score .968 .968  Table 1. Average P/R/F1 scores for the affective retrieval of +/-  properties from 6,230 stereotypes. Viewing the problem as a retrieval task then, in which (7) and (8) are used to retrieve distinct posi-tive and negative property sets for a stereotype S, we report the encouraging results of Table 1 above. 
5 Re-shaping Affect in Figurative Contexts The Google n-grams are a rich source of affective metaphors of the form Target is Source, such as ?politicians are crooks?, ?Apple is a cult?, ?racism is a disease? and ?Steve Jobs is a god?. Let src(T) denote the set of stereotypes that are commonly used to describe T, where commonality is defined as the presence of the corresponding copula meta-phor in the Google n-grams. Thus, for example:  src(racism)  =   {problem, disease, poison, sin, crime, ideology, weapon, ?} src(Hitler) = {monster, criminal, tyrant, idiot, madman, vegetarian, racist, ?} Let srcTypical(T) denote the aggregation of all properties ascribable to T via metaphors in src(T): 
   (9) srcTypical (T)   =   M?src(T)typical(M) We can also use the posTypical and negTypical variants in (7) and (8) to focus only on metaphors that project positive or negative qualities onto T. In effect, (9) provides a feature representation for a topic T as viewed through the prism of meta-phor. This is useful when the source S in the meta-phor T is S is not a known stereotype in the lexicon, as happens e.g. in Apple is Scientology. We can also estimate whether a given term S is more positive than negative by taking the average pos/neg valence of src(S). Such estimates are 87% correct when evaluated using +R and -R examples. 
The properties and behaviors that are contextually relevant to the interpretation of T is S  are given by    (10)  salient (T,S)  =  |srcTypical(T) ?  typical(T)|           ?             |srcTypical(S) ?  typical(S)| In the context of T is S, the figurative perspective  M ? src(S)?src(T)?{S} is deemed apt for T if:    (11)   apt(M, T,S)  = |salient(T,S) ?  typical(M)| > 0 and the degree to which M is apt for T is given by:    (12)  aptness(M,T,S)  =     |salient(T, S) ?  typical(M)|                   |typical(M)| We can construct an interpretation for  T is S  by considering not just {S}, but the stereotypes in src(T) that are apt for T in the context of T is S, as well as the stereotypes that are commonly used to describe S ? that is, src(S) ? that are also apt for T:   (13)  interpretation(T, S)        = {M|M ? src(T)?src(S)?{S} ? apt(M, T, S)} The elements {Mi} of interpretation(T, S) can now be sorted by  aptness(Mi T, S)  to produce a ranked list of interpretations (M1, M2 ? Mn). For any in-terpretation M, the salient features of M are thus:    (14)  salient(M, T,S) = typical(M) ?  salient (T,S)   So interpretation(T, S) is an expansion of the af-fective metaphor T is S  that includes the common metaphors that are consistent with T qua S. For instance, ?Google is -Microsoft? (where - indicates a negative spin) produces {monopoly, threat, bully, giant, dinosaur, demon, ?}. For each Mi in inter-pretation(T, S), salient(Mi, T, S) is an expansion of Mi that includes all of the qualities that are apt for T qua Mi (e.g. threatening, sprawling, evil, etc.). 6 Concluding Remarks Metaphor is the perfect tool for influencing the perceived affect of words and concepts in context. The web application Metaphor Magnet provides a proof-of-concept demonstration of this re-shaping process at work, using the stereotype lexicon of ?3, the selective highlighting of (7)?(8), and the model of metaphor in (9)?(14). It can be accessed at:            http://boundinanutshell.com/metaphor-magnet 
? 
78
Acknowledgements This research was supported by the WCU (World Class University) program under the National Re-search Foundation of Korea, and funded by the Ministry of Education, Science and Technology of Korea (Project No: R31-30007).  References  Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium. Paul Ekman. 1993. Facial expression of emotion. Amer-ican Psychologist, 48:384-392.  Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-WordNet: A publicly available lexical resource for opinion mining. Proc. of LREC-2006, the 5th Confer-ence on Language Resources and Evaluation, 417-422. Andrea Esuli and Fabrizio Sebastiani. 2007. PageRank-ing WordNet Synsets: An application to opinion min-ing. Proc. of ACL-2007, the 45th Annual Meeting of the Association for Computational Linguistics. Hugo Liu, Henry Lieberman, and Ted Selker. 2003. A Model of Textual Affect Sensing Using Real-World Knowledge. Proceedings of the 8th international con-ference on Intelligent user interfaces, pp. 125-132. Rada Mihalcea and Paul Tarau. 2004.  TextRank: Bring-ing Order to Texts. Proceedings of EMNLP-04, the 2004 Conference on Empirical Methods in Natural Language Processing. Saif F. Mohammad and Peter D. Turney. 2010. Emo-tions evoked by common words and phrases: Using Mechanical Turk to create an emotional lexicon. Pro-ceedings of the NAACL-HLT 2010 workshop on Computational Approaches to Analysis and Genera-tion of Emotion in Text. Los Angeles, CA. Saif F. Mohammad and Tony Yang. 2011. Tracking sentiment in mail: how genders differ on emotional axes. Proceedings of the ACL 2011 WASSA work-shop on Computational Approaches to Subjectivity and Sentiment Analysis, Portland, Oregon. Robert Plutchik. 1980. A general psycho-evolutionary theory of emotion. Emotion: Theory, research and experience, 2(1-2):1-135. Push Singh. 2002. The public acquisition of com-monsense knowledge. Proceedings of AAAI Spring Symposium on Acquiring (and Using) Linguistic (and World) Knowledge for Information Ac-cess. Palo Alto, CA. 
Carlo Strapparava and Alessandro Valitutti. 2004. Wordnet-affect: an affective extension of Word-net.  Proceedings of LREC-2004, the 4th International Conference on Language Resources and Evaluation, Lisbon, Portugal. Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientation of words using spin model. Proceedings of the 43rd Annual Meeting of the ACL, 133?140. Turney, P. D. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. In Proceedings of ACL-2002, the 40th  Annual Meeting of the Association for Computation-al Linguistics, pp. 417?424. June 2002. Veale, T. and Hao, Y. Making Lexical Ontologies Func-tional and Context-Sensitive.  Proceedings of ACL-2007, the 45th Annual Meeting of the Association of Computational Linguistics, pp. 57?64. June 2007. Veale, T. and Hao, Y. Detecting Ironic Intent in Crea-tive Comparisons. Proceedings of ECAI?2010, the 19th European Conference on Artificial Intelligence, Lisbon. August 2010. Veale, T. Creative Language Retrieval: A Robust Hy-brid of Information Retrieval and Linguistic Creativi-ty. Proceedings of ACL?2011, the 49th Annual Meeting of the Association of Computational Lin-guistics. June 2011. Whissell, C. The dictionary of affect in language. In R. Plutchik and H. Kellerman (Eds.) Emotion: Theory and research. Harcourt Brace, pp. 113-131. 1989. 
79
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 7?12,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Specifying Viewpoint and Information Need with Affective Metaphors A System Demonstration of the Metaphor Magnet Web App/Service  Tony Veale                 Guofu Li Web Science and Technology Division,   School of Computer Science & Informatics, KAIST, Daejeon, University College Dublin, South Korea. Belfield, Dublin D4, Ireland. Tony.Veale@gmail.com        Yanfen.Hao@UCD.ie  Abstract Metaphors pervade our language because they are elastic enough to allow a speaker to express an affective viewpoint on a topic without committing to a specific meaning. This balance of expressiveness and inde-terminism means that metaphors are just as useful for eliciting information as they are for conveying information. We explore here, via a demonstration of a system for metaphor interpretation and generation called Metaphor Magnet, the practical uses of metaphor as a basis for formulating af-fective information queries. We also con-sider the kinds of deep and shallow stereotypical knowledge that are needed for such a system, and demonstrate how they can be acquired from corpora and the web. 
1 Introduction Metaphor is perhaps the most flexible and adaptive tool in the human communication toolbox. It is suited to any domain of discourse, to any register, and to the description of any concept we desire. Speakers use metaphor to communicate not just meanings, but their feelings about those meanings. The open-ended nature of metaphor interpretation means that we can use metaphor to simultaneously express and elicit opinions about a given topic. Metaphors are flexible conceits that allow us to express a position while seeking elaboration or refutation of this position from others. A metaphor is neither true or false, but a conceptual model that allow speakers to negotiate a common viewpoint. 
Computational models for the interpretation and elaboration of metaphors should allow speakers to exploit the same flexibility of expression with ma-chines as they enjoy with other humans. Such a goal clearly requires a great deal of knowledge, since metaphor is a knowledge-hungry mechanism par excellance (see Fass, 1997). However, much of the knowledge required for metaphor interpretation is already implicit in the large body of metaphors that are active in a community (see Martin, 1990; Mason, 2004). Existing metaphors are themselves a valuable source of knowledge for the production of new metaphors, so much so that a system can mine the relevant knowledge from corpora of fig-urative text (e.g. see Veale, 2011; Shutova, 2010). One area of human-machine interaction that can clearly benefit from a competence in metaphor is that of information retrieval (IR). Speakers use metaphors with ease when eliciting information from each other, as e.g. when one suggests that a certain CEO is a tyrant or a god, or that a certain company is a dinosaur while another is a cult. Those that agree might respond by elaborating the metaphor and providing substantiating evidence, while those that disagree might refute the metaphor and switch to another of their own choosing. A well-chosen metaphor can provide the talking points for an informed conversation, allowing a speaker to elicit the desired knowledge as a combi-nation of objective and subjective elements. In IR, such a capability should allow searchers to express their information needs subjectively, via affective metaphors like ?X is a cult?. The goal, of course, is not just to retrieve documents that make explicit use of the same metaphor ? a literal match-ing of non-literal texts is of limited use ?  but to 
7
retrieve texts whose own metaphors are consonant with those of the searcher, and which elaborate upon the same talking points. This requires a com-puter to understand the user?s metaphor, to appre-ciate how other metaphors might convey the same affective viewpoint, and to understand the different guises these metaphors might assume in a  text. IR extends the reach of its retrieval efforts by expanding the query it is given, in an attempt to make explicit what the user has left implicit. Meta-phors, like under-specified queries, have rich meanings that are, for the most part, implicit: they imply and suggest much more than they specify. An expansionist approach to metaphor meaning, in which an affective metaphor is interpreted by gen-erating the space of related metaphors and talking points that it implies, is thus very much suited to a more creative vision of IR, as e.g. suggested by Veale (2011). To expand a metaphorical query (like ?company-X is a cult? or ?company-Y is a dinosaur? or ?Z was a tyrant?), a system must first expand the metaphor itself, into a set of plausible construals of the metaphor (e.g. a company that is viewed as a dinosaur will likely be powerful, but also bloated, lumbering and slow). The system described in this paper, Metaphor Magnet, demonstrates this expansionist approach to metaphorical inference. Users express queries in the form of affective metaphors or similes, perhaps using explicit + or ? tags to denote a positive or negative spin on a given concept. For instance, ?Google is as ?powerful as Microsoft? does not look for documents that literally contain this simi-le, but documents that express viewpoints that are implied by this simile, that is, documents that dis-cuss the negative implications of Google?s power, where these implications are first understood in relation to Microsoft. The system does this by first considering the metaphors that are conventionally used to describe Microsoft, focusing only on those metaphors that evoke the property powerful, and which cast a negative light on Microsoft. The im-plications of these metaphors (e.g., dinosaur, bully, monopoly, etc.) are then examined in the context of Google, using the metaphors that are typically used to describe Google as a guide to what is most apt. Thus, since Google is often described as a giant in web texts, the negative properties and behaviors of a stereotypical giant ? like lumbering and sprawl-ing ? will be considered apt and highlighted. To perform this kind of analysis reliably, for a 
wide range of metaphors and an even wider range of topics,  requires a  robustly shallow approach. We exploit the fact that the Google n-grams (Brants and Franz, 2006) contains a great many copula metaphors of the form ?X is a Y? to under-stand how X is typically viewed on the web. We further exploit a large dictionary of affective stere-otypes to provide an understanding of the +/- prop-erties and behaviors of each source concept Y. Combining these resources allows the Metaphor Magnet system to understand the implications of a metaphorical query ?X as Z? in terms of the quali-ties that are typically considered salient for Z and which have been corpus-attested as apt for X. We describe the construction of our lexicon of affective stereotypes in section 2. Each stereotype is associated with a set of typical properties and behaviors (like sprawling for giant, or inspiring for guru), where the overall affect of each stereotype depends on which subset of qualities is activated in a given context (e.g., giant can be construed posi-tively or negatively, as can baby, soldier, etc.). We describe how Metaphor Magnet exploits these ste-reotypes in section 3, before providing a worked example in section 4 and screenshots in section 5. 2 An Affective Lexicon of Stereotypes We construct the lexicon in two stages. In the first stage, a large collection of stereotypical descrip-tions is harvested from the Web. As in Liu et al (2003), our goal is to acquire a lightweight com-mon-sense representation of many everyday con-cepts. In the second stage, we link these common-sense qualities in a support graph that captures how they mutually support each other in their co-description of a stereotypical idea. From this graph we can estimate positive and negative valence scores for each property and behavior, and default averages for the stereotypes that exhibit them. Similes and stereotypes share a symbiotic rela-tionship: the former exploit the latter as reference points for an evocative description, while the latter are perpetuated by their constant re-use in similes. Expanding on the approach in Veale (2011), we use two kinds of query for harvesting stereotypes from the web. The first, ?as ADJ as a NOUN?, ac-quires typical adjectival properties for noun con-cepts; the second, ?VERB+ing like a NOUN? and ?VERB+ed like a NOUN?, acquires typical verb behaviors. Rather than use a wildcard * in both 
8
positions (ADJ and NOUN, or VERB and NOUN), which yields limited results with a search engine like Google, we generate fully instantiated similes from hypotheses generated via the Google n-grams. Thus, from the 3-gram ?a drooling zombie? we generate the query ?drooling like a zombie?, and from the 3-gram ?a mindless zombie? we gen-erate ?as mindless as a zombie?. Only those similes whose queries retrieve one or more web documents via Google are considered to contain promising associations. But this still gives us over 250,000 web-validated simile associ-ations for our stereotypical model. We quickly fil-ter these candidates manually, to ensure that the contents of the lexicon are of the highest quality. As a result, we obtain rich descriptions for many stereotypical ideas, such as Baby, which is de-scribed via 163 typical properties and behaviors like crying, drooling and guileless. After this filter-ing phase, the stereotype lexicon maps 9,479 stere-otypes to a set of 7,898 properties and behaviors, to yield more than 75,000 pairings. We construct the second level of the lexicon by automatically linking these properties and behav-iors to each other in a support graph. The intuition here is that properties which reinforce each other in a single description (e.g. ?as lush and green as a jungle? or ?as hot and humid as a sauna?) are more likely to have a similar affect than properties which do not support each other. We first gather all Google 3-grams in which a pair of stereotypical properties or behaviors X and Y are linked via co-ordination, as in ?hot and humid? or ?kicking and screaming?. A bidirectional link between X and Y is added to the support graph if one or more stereo-types in the lexicon contain both X and Y. If this is not so, we consider whether both descriptors ever reinforce each other in web similes, by posing the web query ?as X and Y as?. If this query has  non-zero hits, we also add a link between X and Y. Let N denote this support graph, and N(p) de-note the set of neighboring terms to p, that is, the set of properties and behaviors that can mutually support p. Since every edge in N represents an af-fective context, we can estimate the likelihood that a property p is ever used in a positive or negative context if we know the positive or negative affect of enough members of N(p). So if we label enough vertices of N as +  or -, we can interpolate a posi-tive/negative valence score for all vertices p in N. To do this, we build a reference set -R of typi-
cally negative words, and a set +R of typically positive words. Given a few seed members of -R (such as sad, disgusting, evil, etc.) and a few seed members of +R (such as happy, wonderful, etc.), we find many other candidates to add to +R and -R by considering neighbors of these seeds in N. After three iterations in this fashion, we populate +R and -R with approx. 2000 words each. For a property p we can now define N+(p) and N-(p) as follows:    (1)        N+(p) = N(p) ? +R    (2)        N-(p) = N(p) ? -R We can now assign positive and negative valence scores to each vertex p  by interpolating from ref-erence values to their neighbors in N:    (3)   pos(p)   =           |N+(p)|   |N+(p) ? N-(p)|    (4)   neg(p)   =        1  -  pos(p) If a term S denotes a stereotypical idea and is de-scribed via a set of typical properties and behaviors typical(S) in the lexicon, then: 
   (5)        pos(S)   =        ?p?typical(S) pos(p)               |typical(S)| 
   (6)        neg(S)   = 1  -  pos(S) Thus, (5) and (6) calculate the mean affect of the properties and behaviors of S, as represented via typical(S). We can now use (3) and (4) to separate typical(S) into those elements that are more nega-tive than positive (putting a negative spin on S) and into those that are more positive than negative (putting a positive spin on S): (7)  posTypical(S)  = {p | p ? typical(S) ? pos(p) > 0.5} (8)  negTypical(S)  = {p | p ? typical(S) ? neg(p) > 0.5} 2.1 Evaluation of Stereotypical Affect In the process of populating +R and -R, we identi-fy a reference set of 478 positive stereotypes (such as saint and hero) and 677 negative stereotypes (such as tyrant and monster). When we use these reference points to test the effectiveness of (5) and (6) ? and thus, indirectly, of (3) and (4) and of the 
9
stereotype lexicon itself ? we find that 96.7% of the positive stereotypes in +R are correctly as-signed a positivity score greater than 0.5 (pos(S) > neg(S)) by (5), while 96.2% of the negative stereo-types in -R are correctly assigned a negativity score greater than 0.5  (neg(S) > pos(S)) by (6). 
3 Expansion/Interpretation of Metaphors  The Google n-grams are a rich source of affective metaphors of the form Target is Source, such as ?politicians are crooks?, ?Apple is a cult?, ?racism is a disease? and ?Steve Jobs is a god?. Let src(T) denote the set of stereotypes that are commonly used to describe T, where commonality is defined as the presence of the corresponding copula meta-phor in the Google n-grams. To find metaphors for proper-named entities like ?Bill Gates?, we also analyze n-grams of the form stereotype First [Middle] Last, such as ?tyrant Adolf Hitler?. Thus:  src(racism)  =   {problem, disease, joke, sin, poi-son, crime, ideology, weapon} src(Hitler) = {monster, criminal, tyrant, idiot, madman, vegetarian, racist, ?} We do not try to discriminate literal from non-literal assertions, nor do we even try to define liter-ality. We simply assume each putative metaphor offers a potentially useful perspective on a topic T.  Let srcTypical(T) denote the aggregation of all properties ascribable to T via metaphors in src(T): 
   (9) srcTypical (T)   =   M?src(T)typical(M) We can also use the posTypical and negTypical variants in (7) and (8) to focus only on metaphors that project positive or negative qualities onto T.  (9) is especially useful when the source S in the metaphor  T is S  is not a known stereotype in the lexicon, as happens when one describes Apple as Scientology. When the set typical(S) is empty, src-Typical(S) may not be, so srcTypical(S) can act as a proxy representation for S in these cases.   The properties and behaviors that are salient to the interpretation of   T is S   are given by:    (10)  salient (T,S)  =  |srcTypical(T) ?  typical(T)|           ?             |srcTypical(S) ?  typical(S)| In the context of T is S, the metaphorical stereotype  
M ? src(S)?src(T)?{S} is an apt vehicle for T if:    (11)   apt(M, T,S)  = |salient(T,S) ?  typical(M)| > 0 and the degree to which M is apt for T is given by:   (12)  aptness(M,T,S)  =     |salient(T, S) ?  typical(M)|                  |typical(M)| We can construct an interpretation for  T is S  by considering not just {S}, but the stereotypes in src(T) that are apt for T in the context of T is S, as well as the stereotypes that are commonly used to describe S ? that is, src(S) ? that are also apt for T:     (13)  interpretation(T, S)        = {M|M ? src(T)?src(S)?{S} ? apt(M, T, S)} In effect then, the interpretation of  T is S  is itself a set of apt metaphors for T that expand upon S. The elements {Mi} of interpretation(T, S) can now be sorted by  aptness(Mi T, S)  to produce a ranked list of interpretations (M1, M2 ? Mn). For any inter-pretation M, the salient features of M are thus:    (14)  salient(M, T,S) = typical(M) ?  salient (T,S)   If  T is S  is a creative IR query ? to find docu-ments that view T as S ? then interpretation(T, S) is an expansion of  T is S  that includes the com-mon metaphors that are consistent with T viewed as S. For any viewpoint Mi, salient(Mi, T, S) is an expansion of Mi that includes all of the qualities that T is likely to exhibit when it behaves like Mi. 4 Metaphor Magnet: A Worked Example Consider the query ?Google is Microsoft?, which expresses a need for documents in which Google exhibits qualities typically associated with Mi-crosoft. Now, both Google and Microsoft are com-plex concepts, so there are many ways in which they can be considered similar or dissimilar, either in a good or a bad light. However, the most salient aspects of Microsoft will be those that underpin our common metaphors for Microsoft, i.e., stereo-types in src(Microsoft). These metaphors will pro-vide the talking points for the interpretation.  The Google n-grams yield up the following metaphors, 57 for Microsoft and 50 for Google: src(Microsoft) = {king, master, threat, bully, giant, leader, monopoly, dinosaur ?} 
? 
10
 src(Google)   = {king, engine, threat, brand, giant, leader, celebrity, religion ?} So the following qualities are aggregated for each: srcTypical(Microsoft) = {trusted, menacing, ruling,  threatening, overbearing,  admired, commanding, ?} srcTypical(Google)  = {trusted, lurking reigning, ruling, crowned, shining, determined, admired ?} Now, the salient qualities highlighted by the meta-phor, namely salient(Google, Microsoft),  are: {celebrated, menacing, trusted, challenging, estab-lished,  threatening, admired, respected, ?} Thus, interpretation(Google, Microsoft) contains: {king, criminal, master, leader, bully,  threatening, giant, threat, monopoly, pioneer, dinosaur, ?} Suppose we focus on the metaphorical expansion ?Google is king?, since king is the most highly ranked element of the interpretation. Now,  sali-ent(king, Google, Microsoft)  contains: {celebrated, revered, admired, respected, ruling, arrogant, commanding, overbearing, reigning, ?} These properties and behaviors are already implicit in our perception of Google, insofar as they are salient aspects of the stereotypes to which Google is frequently compared. The metaphor ?Google is Microsoft? ? and its expansion ?Google is king? ? simply crystalizes these qualities, from perhaps different comparisons, into a single act of ideation. Consider the metaphor ?Google is -Microsoft?. Since -Microsoft is used to impart a negative spin (+ would impart a positive spin), negTypical is here used in place of typical in (9) and (10). Thus:   srcTypical(-Microsoft)  =    {menacing, threatening, twisted, raging, feared, sinister, lurking, domineering, overbearing, ?}   salient(Google, -Microsoft) =    {menacing, bullying, roaring, dreaded?} Now interpretation(Google, -Microsoft) becomes:     {criminal, giant, threat, bully, victim, devil, ?} In contrast, interpretation(Google, +Microsoft) is:      {king, master, leader, pioneer, partner, ?}  
More focus is achieved with the simile query ?Google is as ?powerful as Microsoft?. In explicit similes, we need to focus on just a subset of  the salient properties, using e.g. this variant of (10):  {p |  p ? salient(Google, Microsoft) ? N(powerful)             ? neg(p) > pos(p)} In this -powerful case, the interpretation becomes:    {bully, giant, devil, monopoly, dinosaur, ?}  5 The  Metaphor Magnet Web App Metaphor Magnet is designed to be a lightweight web application that provides both HTML output (for humans) and XML (for client applications).  The system allows users to enter queries such as Google is ?Microsoft, life is a +game, Steve Jobs is Tony Stark, or even Rasputin is Karl Rove (queries are case-sensitive). Each query is expanded into a set of apt metaphors via mappings in the Google n-grams, and each metaphor is expanded into a set of contextually apt qualities. In turn, each quality is then expanded into an IR query that is used to re-trieve relevant hits from Google. In effect, the sys-tem allows users to interface with a search engine like Google using metaphor and other affective language forms. The demonstration system can be accessed using a standard browser at this URL:      http://boundinanutshell.com/metaphor-magnet Metaphor Magnet can exploit the properties and behaviors of its stock of almost 10,000 stereotypes, and can infer salient qualities for many proper-named entities like Karl Rove and Steve Jobs using a combination of copula statements from the Google n-grams (e.g., ?Steve Jobs is a visionary?) and category assignments from Wikipedia. The interpretation of the simile/query ?Google is as -powerful as Microsoft? thus highlights a selec-tion of affective viewpoints on the source concept, Microsoft, and picks out an apt selection of view-points on the target Google. Metaphor Magnet dis-plays both selections as phrase clouds in which each hyperlinked phrase ? a combination of an apt stereotype and a salient quality ? is clickable, to yield linguistic evidence for the selection and cor-responding web-search results (via a Google gadg-et). The phrase cloud representing Microsoft in this simile is shown in the screenshot of Figure 1, while the phrase cloud for Google is shown in Figure 2. 
11
 Figure 1. A screenshot of a phrase cloud for the perspective cast upon the source ?Microsoft? by the simile ?Google is as ?powerful as Microsoft?.  
 Figure 2. A screenshot of a phrase cloud for the perspective cast upon the target term ?Google? by the simile ?Google is as ?powerful as Microsoft?.  Metaphor Magnet demonstrates the potential utili-ty of affective metaphors in human-computer lin-guistic interaction, and acts as a web service from which other NL applications can derive a measure of metaphorical competence. When accessed as a service, Metaphor Magnet returns either HTML or XML data, via simple get requests. For illustrative purposes, each HTML page also provides the URL for the corresponding XML-structured data set. Acknowledgements This research was partly supported by the WCU (World Class University) program under the Na-tional Research Foundation of Korea (Ministry of Education, Science and Technology of Korea, Pro-ject No: R31-30007), and partly funded by Science Foundation Ireland via the Centre for Next Genera-tion Localization (CNGL). References  Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium. 
Dan Fass. 1997. Processing Metonymy and Metaphor. Contemporary Studies in Cognitive Science & Tech-nology. New York: Ablex. Hugo Liu, Henry Lieberman and Ted Selker. 2003. A Model of Textual Affect Sensing Using Real-World Knowledge. Proc. of the 8th international conference on Intelligent user interfaces, 125-132. James H. Martin. 1990. A Computational Model of Metaphor Interpretation. NY: Academic Press. Zachary J. Mason. 2004. CorMet: A Computational, Corpus-Based Conventional Metaphor Extraction System, Computational Linguistics, 30(1):23-44. Ekaterina Shutova. 2010. Metaphor Identification Using Verb and Noun Clustering. In Proc. of the 23rd Inter-national Conference on Computational Linguistics, 1001-1010 Tony Veale. 2011. Creative Language Retrieval. Crea-tive Language Retrieval: A Robust Hybrid of Infor-mation Retrieval and Linguistic Creativity. In Proc. of ACL?2011, the 49th Annual Meeting of the Asso-ciation for Computational Linguistics. 
12


References
 Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram 
Version 1. Linguistic Data Consortium.
Dan Fass. 1997. Processing Metonymy and Metaphor.
Contemporary Studies in Cognitive Science & Technology. New York: Ablex.
Hugo Liu, Henry Lieberman and Ted Selker. 2003. A 
Model of Textual Affect Sensing Using Real-World 
Knowledge. Proc. of the 8
th
international conference 
on Intelligent user interfaces, 125-132.
James H. Martin. 1990.  A Computational Model of 
Metaphor Interpretation. NY: Academic Press.
Zachary J. Mason. 2004. CorMet: A Computational, 
Corpus-Based Conventional Metaphor Extraction 
System, Computational Linguistics, 30(1):23-44.
Ekaterina Shutova. 2010. Metaphor Identification Using 
Verb and Noun Clustering. In Proc. of the 23
rd
International Conference on Computational Linguistics, 
1001-1010
Tony Veale. 2011. Creative Language Retrieval. Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity. In  Proc. 
of ACL2011, the 49
th
Annual Meeting of the Association for Computational Linguistics.Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 660?670,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Creating Similarity:  Lateral Thinking for Vertical Similarity Judgments 
  Tony Veale Guofu Li Web Science and Technology Division, School of Computer Science and Informatics, Korean Advanced Institute of Science  University College Dublin, and Technology, Yuseong, South Korea Belfield, Dublin D2, Ireland. Tony.Veale@gmail.com li.guofu.l@gmail.com       Abstract Just as observing is more than just see-ing, comparing is far more than mere matching. It takes understanding, and even inventiveness, to discern a useful basis for judging two ideas as similar in a particular context, especially when our perspective is shaped by an act of linguis-tic creativity such as metaphor, simile or analogy. Structured resources such as WordNet offer a convenient hierarchical means for converging on a common ground for comparison, but offer little support for the divergent thinking that is needed to creatively view one concept as another. We describe such a means here, by showing how the web can be used to harvest many divergent views for many familiar ideas. These lateral views com-plement the vertical views of WordNet, and support a system for idea exploration called Thesaurus Rex. We show also how Thesaurus Rex supports a novel, genera-tive similarity measure for WordNet. 1 Seeing is Believing (and Creating) Similarity is a cognitive phenomenon that is both complex and subjective, yet for practical reasons it is often modeled as if it were simple and objec-tive. This makes sense for the many situations where we want to align our similarity judgments with those of others, and thus focus on the same conventional properties that others are also likely to focus upon. This reliance on the consensus viewpoint explains why WordNet (Fellbaum, 1998) has proven so useful as a basis for compu-tational measures of lexico-semantic similarity 
(e.g. see Pederson et al 2004, Budanitsky & Hirst, 2006; Seco et al 2006). These measures reduce the similarity of two lexical concepts to a single number, by viewing similarity as an objec-tive estimate of the overlap in their salient quali-ties. This convenient perspective is poorly suited to creative or insightful comparisons, but it is sufficient for the many mundane comparisons we often perform in daily life, such as when we or-ganize books or look for items in a supermarket. So if we do not know in which aisle to locate a given item (such as oatmeal), we may tacitly know how to locate a similar product (such as cornflakes) and orient ourselves accordingly.  Yet there are occasions when the recognition of similarities spurs the creation of similarities, when the act of comparison spurs us to invent new ways of looking at an idea. By placing pop tarts in the breakfast aisle, food manufacturers encourage us to view them as a breakfast food that is not dissimilar to oatmeal or cornflakes. When ex-PM Tony Blair published his memoirs, a mischievous activist encouraged others to move his book from Biography to Fiction in bookshops, in the hope that buyers would see it in a new light. Whenever we use a novel meta-phor to convey a non-obvious viewpoint on a topic, such as ?cigarettes are time bombs?, the comparison may spur us to insight, to see aspects of the topic that make it more similar to the vehi-cle (see Ortony, 1979; Veale & Hao, 2007).   In formal terms, assume agent A has an in-sight about concept X, and uses the metaphor X is a Y to also provoke this insight in agent B. To arrive at this insight for itself, B must intuit what X and Y have in common. But this commonality is surely more than a standard categorization of X, or else it would not count as an insight about X. To understand the metaphor, B must place X 
660
in a new category, so that X can be seen as more similar to Y. Metaphors shape the way we per-ceive the world by re-shaping the way we make similarity judgments. So if we want to imbue computers with the ability to make and to under-stand creative metaphors, we must first give them the ability to look beyond the narrow view-points of conventional resources.   Any measure that models similarity as an ob-jective function of a conventional worldview employs a convergent thought process. Using WordNet, for instance, a similarity measure can vertically converge on a common superordinate category of both inputs, and generate a single numeric result based on their distance to, and the information content of, this common generaliza-tion. So to find the most conventional ways of seeing a lexical concept, one simply ascends a narrowing concept hierarchy, using a process de Bono (1970) calls vertical thinking. To find nov-el, non-obvious and useful ways of looking at a lexical concept, one must use what Guilford (1967) calls divergent thinking and what de Bono calls lateral thinking. These processes cut across familiar category boundaries, to simultaneously place a concept in many different categories so that we can see it in many different ways.   de Bono argues that vertical thinking is selec-tive while lateral thinking is generative. Whereas vertical thinking concerns itself with the ?right? way or a single ?best? way of looking at things, lateral thinking focuses on producing alternatives to the status quo. To be as useful for creative tasks as they are for conventional tasks, we need to re-imagine our computational similarity measures as generative rather than selective, ex-pansive rather than reductive, divergent as well as convergent and lateral as well as vertical. Though WordNet is ideally structured to support vertical, convergent reasoning, its comprehensive nature means it can also be used as a solid foun-dation for building a more lateral and divergent model of similarity. Here we will use the web as a source of diverse perspectives on familiar ide-as, to complement the conventional and often narrow views codified by WordNet.   Section 2 provides a brief overview of past work in the area of similarity measurement, be-fore section 3 describes a simple bootstrapping loop for acquiring richly diverse perspectives from the web for a wide variety of familiar ideas. These perspectives are used to enhance a Word-Net-based measure of lexico-semantic similarity in section 4, by broadening the range of informa-tive viewpoints the measure can select from. 
Similarity is thus modeled as a process that is both generative and selective. This lateral-and-vertical approach is evaluated in section 5, on the Miller & Charles (1991) data-set. A web app for the lateral exploration of diverse viewpoints, named Thesaurus Rex, is also presented, before closing remarks are offered in section 6. 2 Related Work and Ideas WordNet?s taxonomic organization of noun-senses and verb-senses ? in which very general categories are successively divided into increas-ingly informative sub-categories or instance-level ideas ? allows us to gauge the overlap in information content, and thus of meaning, of two lexical concepts. We need only identify the deepest point in the taxonomy at which this con-tent starts to diverge. This point of divergence is often called the LCS, or least common subsumer, of two concepts (Pederson et al, 2004). Since sub-categories add new properties to those they inherit from their parents ? Aristotle called these properties the differentia that stop a category sys-tem from trivially collapsing into itself ? the depth of a lexical concept in a taxonomy is an intuitive proxy for its information content. Wu & Palmer (1994) use the depth of a lexical concept in the WordNet hierarchy as such a proxy, and thereby estimate the similarity of two lexical concepts as twice the depth of their LCS divided by the sum of their individual depths.  Leacock and Chodorow (1998) instead use the length of the shortest path between two con-cepts as a proxy for the conceptual distance be-tween them. To connect any two ideas in a hierarchical system, one must vertically ascend the hierarchy from one concept, change direction at a potential LCS, and then descend the hierar-chy to reach the second concept. (Aristotle was also first to suggest this approach in his Poetics). Leacock and Chodorow normalize the length of this path by dividing its size (in nodes) by twice the depth of the deepest concept in the hierarchy; the latter is an upper bound on the distance be-tween any two concepts in the hierarchy. Negat-ing the log of this normalized length yields a corresponding similarity score. While the role of an LCS is merely implied in Leacock and Cho-dorow?s use of a shortest path, the LCS is pivotal nonetheless, and like that of Wu & Palmer, the approach uses an essentially vertical reasoning process to identify a single ?best? generalization.   Depth is a convenient proxy for information content, but more nuanced proxies can yield 
661
more rounded similarity measures. Resnick (1995) draws on information theory to define the information content of a lexical concept as the negative log likelihood of its occurrence in a corpus, either explicitly (via a direct mention) or by presupposition (via a mention of any of its sub-categories or instances). Since the likelihood of a general category occurring in a corpus is higher than that of any of its sub-categories or instances, such categories are more predictable, and less informative, than rarer categories whose occurrences are less predictable and thus more informative. The negative log likelihood of the most informative LCS of two lexical concepts offers a reliable estimate of the amount of infor-mation shared by those concepts, and thus a good estimate of their similarity. Lin (1998) combines the intuitions behind Resnick?s metric and that of Wu and Palmer to estimate the similarity of two lexical concepts as an information ratio: twice the information content of their LCS divided by the sum of their individual information contents.   Jiang and Conrath (1997) consider the con-verse notion of dissimilarity, noting that two lex-ical concepts are dissimilar to the extent that each contains information that is not shared by the other. So if the information content of their most informative LCS is a good measure of what they do share, then the sum of their individual information contents, minus twice the content of their most informative LCS, is a reliable estimate of their dissimilarity.   Seco et al (2006) presents a minor innova-tion, showing how Resnick?s notion of infor-mation content can be calculated without the use of an external corpus. Rather, when using Res-nick?s metric (or that of Lin, or Jiang and Con-rath) for measuring the similarity of lexical concepts in WordNet, one can use the category structure of WordNet itself to estimate infor-mation content. Typically, the more general a concept, the more descendants it will possess. Seco et al thus estimate the information content of a lexical concept as the log of the sum of all its unique descendants (both direct and indirect), divided by the log of the total number of con-cepts in the entire hierarchy. Not only is this in-trinsic view of information content convenient to use, without recourse to an external corpus, Seco et al show that it offers a better estimate of in-formation content than its extrinsic, corpus-based alternatives, as measured relative to average hu-man similarity ratings for the 30 word-pairs in the Miller & Charles (1991) test set.  A similarity measure can draw on other 
sources of information besides WordNet?s cate-gory structures. One might eke out additional information from WordNet?s textual glosses, as in Lesk (1986), or use category structures other than those offered by WordNet. Looking beyond WordNet, entries in the online encyclopedia Wikipedia are not only connected by a dense topology of lateral links, they are also organized by a rich hierarchy of overlapping categories. Strube and Ponzetto (2006) show how Wikipedia can support a measure of similarity (and related-ness) that better approximates human judgments than many WordNet-based measures. Nonethe-less, WordNet can be a valuable component of a hybrid measure, and Agirre et al (2009) use an SVM (support vector machine) to combine in-formation from WordNet with information har-vested from the web. Their best similarity measure achieves a remarkable 0.93 correlation with human judgments on the Miller & Charles word-pair set.   Similarity is not always applied to pairs of concepts; it is sometimes analogically applied to pairs of pairs of concepts, as in proportional analogies of the form A is to B as C is to D (e.g., hacks are to writers as mercenaries are to sol-diers, or chisels are to sculptors as scalpels are to surgeons). In such analogies, one is really as-sessing the similarity of the unstated relationship between each pair of concepts: thus, mercenaries are soldiers whose allegiance is paid for, much as hacks are writers with income-driven loyalties; sculptors use chisels to carve stone, while sur-geons use scalpels to cut or carve flesh. Veale (2004) used WordNet to assess the similarity of A:B to C:D as a function of the combined simi-larity of A to C and of B to D. In contrast, Tur-ney (2005) used the web to pursue a more divergent course, to represent the tacit relation-ships of A to B and of C to D as points in a high-dimensional space. The dimensions of this space initially correspond to linking phrases on the web, before these dimensions are significantly reduced using singular value decomposition.   In the infamous SAT test, an analogy A:B::C:D has four other pairs of concepts that serve as likely distractors (e.g. singer:songwriter for hack:writer) and the goal is to choose the most appropriate C:D pair for a given A:B pair-ing. Using variants of Wu and Palmer (1994) on the 374 SAT analogies of Turney (2005), Veale (2004) reports a success rate of 38?44% using only WordNet-based similarity. In contrast, Tur-ney (2005) reports up to 55% success on the same analogies, partly because his approach aims 
662
to match implicit relations rather than explicit concepts, and in part because it uses a divergent process to gather from the web as rich a perspec-tive as it can on these latent relationships.  2.1 Clever Comparisons Create Similarity Each of these approaches to similarity is a user of information, rather than a creator, and each fails to capture how a creative comparison (such as a metaphor)  can spur a listener to view a topic from an atypical perspective. Camac & Glucks-berg (1984) provide experimental evidence for the claim that ?metaphors do not use preexisting associations to achieve their effects [?] people use metaphors to create new relations between concepts.? They also offer a salutary reminder of an often overlooked fact: every comparison ex-ploits information, but each is also a source of new information in its own right. Thus, ?this cola is acid? reveals a different perspective on cola (e.g. as a corrosive substance or an irritating food) than ?this acid is cola? highlights for acid (such as e.g., a familiar substance)    Veale & Keane (1994) model the role of simi-larity in realizing the long-term perlocutionary effect of an informative comparison. For exam-ple, to compare surgeons to butchers is to en-courage one to see all surgeons as more bloody, crude or careless. The reverse comparison, of butchers to surgeons, encourages one to see butchers as more skilled and precise. Veale & Keane present a network model of memory, called Sapper, in which activation can spread between related concepts, thus allowing one con-cept to prime the properties of a neighbor. To interpret an analogy, Sapper lays down new acti-vation-carrying bridges in memory between ana-logical counterparts, such as between surgeon & butcher, flesh & meat, and scalpel & cleaver. Comparisons can thus have lasting effects on how Sapper sees the world, changing the pattern of activation that arises when it primes a concept.   Veale (2003) adopts a similarly dynamic view of similarity in WordNet, showing how an ana-logical comparison can result in the automatic addition of new categories and relations to WordNet itself. Veale considers the problem of finding an analogical mapping between different parts of WordNet?s noun-sense hierarchy, such as between instances of Greek god and Norse god, or between the letters of different alphabets, such as of Greek and Hebrew. But no structural similarity measure for WordNet exhibits enough discernment to e.g. assign a higher similarity to 
Zeus & Odin (each is the supreme deity of its pantheon) than to a pairing of Zeus and any other Norse god, just as no structural measure will as-sign a higher similarity to Alpha & Aleph or to Beta & Beth than to any random letter pairing.   A fine-grained category hierarchy permits fine-grained similarity judgments, and though WordNet is useful, its sense hierarchies are not especially fine-grained. However, we can auto-matically make WordNet subtler and more dis-cerning, by adding new fine-grained categories to unite lexical concepts whose similarity is not reflected by any existing categories. Veale (2003) shows how a property that is found in the glosses of two lexical concepts, of the same depth, can be combined with their LCS to yield a new fine-grained parent category, so e.g. ?su-preme? + deity = Supreme-deity (for Odin, Zeus, Jupiter, etc.) and ?1st? + letter = 1st-letter (for Alpha, Aleph, etc.) Selected aspects of the textual similarity of two WordNet glosses ? the key to similarity in Lesk (1986) ? can thus be reified into an explicitly categorical WordNet form.  3 Divergent  (Re)Categorization To tap into a richer source of concept properties than WordNet?s glosses, we can use web n-grams. Consider these descriptions of a cowboy from the Google n-grams (Brants & Franz, 2006). The numbers to the right are Google fre-quency counts.  a lonesome cowboy   432  a mounted cowboy   122  a grizzled cowboy     74  a swaggering cowboy     68 To find the stable properties that can underpin a meaningful fine-grained category for cowboy, we must seek out the properties that are so often pre-supposed to be salient of all cowboys that one can use them to anchor a simile, such as "swag-gering like a cowboy? or ?as grizzled as a cow-boy?. So for each property P suggested by Google n-grams for a lexical concept C, we gen-erate a like-simile for verbal behaviors such as swaggering and an as-as-simile for adjectives such as lonesome. Each is then dispatched to Google as a phrasal query. We value quality over size, as these similes will later be used to find diverse viewpoints on the web via bootstrapping. We thus manually filter each web simile, to weed out any that are ill-formed, and those intended to be seen as ironic by their authors. This gives us a body of 12,000+ valid web similes. 
663
 Veale (2011, 2012, 2013) notes that web uses of the pattern ?as P as C? are rife with irony. In contrast, web instances of ?P S such as C? ? where S denotes a superordinate of C ? are rarely ironic. Hao & Veale (2010) exploit this fact to filter ironic comparisons from web similes, by re-expressing each ?as P as C? simile as  ?P * such as C? (using a wildcard * to match any val-ues for S) and looking for attested uses of this new form on the web. Since each hit will also yield a value for S via the wildcard *, and a fine-grained category P-S for C, we use this approach here to harvest fine-grained categories from the web from most of our similes.    Once C is seen to be an exemplary member of the category P-S, such as cola in fizzy-drink, a targeted web search is used to find other mem-bers of P-S, via the anchored query ?P S such as * and C?. For example, ?fizzy drinks such as * and cola? will retrieve web texts in which * is matched to soda or lemonade. Each new member can then be used to instantiate a further query, as in ?fizzy drinks such as * and soda?, to retrieve other members of P-S, such as champagne and root beer. This bootstrapping process runs in successive cycles, using doubly-anchored pat-terns that ? following Kozareva et al (2008) and Veale et al (2009) ? explicitly mention both the category to be populated (P-S) and a recently acquired member of this category (C).   As cautioned by Kozareva et al, it is reckless to bootstrap from members to categories to members again if each enfilade of queries is like-ly to return noisy results. A reliable filter must be applied at each stage, to ensure that any member C that is placed in a category P-S is a sensible member of the category S. Only by filtering in this way can we stop the rapid accumulation of noise. For instance, a WordNet-based filter dis-cards any categorization ?P S such as X and C? where X does not denote a WordNet entry for which S does not denote a valid hypernym. Such a filter offers no creative latitude, however, since it forces every pairing of C and P-S to precisely obey WordNet?s category hierarchy. We thus use instead the near-miss filter described in Veale et al (2009), in which X must denote a descendant of some direct hypernym of some sense of S. The filter does not (and cannot) determine whether P is salient for X. It merely assumes that if P is sa-lient for C, it is salient for X.   Five successive cycles of bootstrapping are performed, using the 12,000+ web similes as a starting point. Consider cola: after 1 cycle, we acquire 14 new categories, such as effervescent-
beverage and sweet-beverage. After 2 cycles we acquire 43 categories; after 3 cycles, 72; after 4 cycles, 93; and after 5 cycles, we acquire 102 fine-grained perspectives on cola, such as stimu-lating-drink and corrosive-substance.  
 Figure 1. Fine-grained perspectives for cola found by Thesaurus Rex on the web. See also Figures 3 and 4. These alternative viewpoints, for a broad array of concepts, are gleaned from the collective intelli-gence of the web. Some are more discerning and informative than others ? see for instance war & divorce in Figure 4 ? though as de Bono (1971) notes, lateral thinking does not privilege a nar-row set of ?correct? viewpoints, rather it gener-ates a broad array of interesting alternatives, none of which are ever ?wrong?, even if some prove more useful than others in a given context.  4 Measuring and Creating Similarity Which perspectives will be most useful and in-formative to a WordNet-based similarity metric? Simply, a perspective M-Cx  for a concept Cy can be coherently added to WordNet iff Cx de-notes a hypernym of some sense of Cy in Word-Net. For purposes of quantifying the similarity of two terms t1 and t2 ? by finding the WordNet senses of these terms that exhibit the highest sim-ilarity ? we can augment WordNet with the per-spectives on t1 and t2 that are coherent with WordNet?s hierarchy. So for t1=cola & t2=acid, corrosive-substance offers a coherent new per-spective on each, slotting in beneath the match-ing WordNet sense of substance.   A category system is a structured feature space. We estimate the similarity of C1 and C2 as the cosine of the angle between the feature vec-tors that are constructed for each. The dimen-sions of these vectors are the atomic hypernyms (direct or indirect) of C1 and C2 in WordNet; the value of a dimension H in a vector is the infor-mation content (IC) of the WordNet hypernym H:  
664
                   size(H)        ?c ? WN  size(c)) Here size(H) is the total number of lexical con-cepts in category H in WordNet, excluding any instance-level concepts, as these illustrative indi-viduals are not evenly distributed across Word-Net categories.   We also want any fine-grained perspective M-H to influence our similarity metric, provided it can be coherently tied into WordNet as a shared hypernym of the two lexical concepts being compared. The absolute information content of a category M-H  that is newly added to WordNet is given by (2): 
                                         size(M-H)    ?m-h ? WN  size(m-h)) where size(M-H) is the number of lexical con-cepts in WordNet for which M-H can be added as a new hypernym. The denominator in (2) de-notes the sum total of the size of all fine-grained categories that can be coherently added to WordNet for any term.     The IC of M-H relative to H is estimated via the geometric mean of ICabs(M-H) and IC(H) is given by (3): (3)  IC(M-H)    =   ? ICabs(M-H) . IC(H) For a shared dimension H in the feature vectors of concepts C1 and C2, if at least one fine-grained perspective M-H has been added to WordNet between H and C1 and between H and C2, then the value of dimension H for C1 and for C2 is given by (4):  (4)  weight(H)   = max(IC(H),  maxM IC(M-H)) When no shared perspective M-H can be added under H, then weight(H) = IC(H). A fine-grained perspective M-H will thus influence a similarity judgment between C1 and C2 only if M-H can be coherently added to WordNet as a hypernym of C1 and C2, and if M-H enriches our view of H. Unlike Resnick (1995), Lin (1998) and Seco et al (2006), this vector-space approach does not hinge on the information content of a single LCS, so any shared hypernym H or perspective M-H can shape a similarity judgment according to its informativeness. 
5 Empirical Evaluation  Many fascinating perspectives on familiar ideas are bootstrapped from the web using similes as a starting point. These perspectives drive an ex-ploratory web-aid to lateral thinking we call The-saurus Rex, while the cosine-distance metric constructed from WordNet and these many fine-grained categories is called, simply, Rex. When Rex provides a numeric estimate of similarity for two ideas, Thesaurus Rex provides an enhanced insight into why these ideas are similar, e.g. by explaining that cola & acid are not just substanc-es, they are corrosive substances.      We evaluate Rex by estimating how closely its judgments correlate with those of human judges on the 30-pair word set of Miller & Charles (M&C), who aggregated the judgments of multi-ple human raters into mean ratings for these pairs. We evaluate three variants of Rex on M&C: Rex-lat, which combines WordNet with all of Thesaurus Rex; Rex-wn, which uses only WordNet, with nothing at all from Thesaurus Rex; and Rex-pop, which enriches WordNet with only popular perspectives from Thesaurus Rex. A perspective is considered popular if it is dis-covered 5 or more times in the bootstrapping process, using 5 different anchors. While corro-sive-substance is a popular category for acid, it not so for cola or juice. Popularity thus approxi-mates what Ortony (1979) calls salience.   Similarity metric r Similarity metric r Wu & Palmer?94* .74 Seco et al ?06* .84 Resnick ?95* .77 Agirre et al ?09 .93 Leacock/Chod?98* .82 Han et al?09 .856 Lin ?98* .80 Rex-wn .84 Jiang/Conrath ?97* -.81 Rex-lat .89 Li et al ?03 .89 Rex-pop .93 Table 1. Product-moment correlations (Pearson?s r) with mean human ratings on all 30 word pairs of the Miller & Charles similarity data-set. * As re-evaluated by Seco et al (2006) for all 30 pairs Table 1 lists coefficients of correlation (Pear-son?s r) with mean human ratings for a range of WordNet-based metrics. Table 1 includes the hybrid WordNet+web+SVM metric of Agirre et al (2009) ? who report a correlation of .93 ? and the Mutual-Information-based PMImax metric of Han et al (2009). The latter achieves good re-sults for 27 of the 30 M&C pairs by enriching a PMI metric with an automatically-generated the-saurus. Yet while informative, this thesaurus is 
         (               ) 
 
         (                          
 
(2)  ICabs(M-H) =  -log 
(1)   IC(H)             =     - log 
665
not organized as an explanatory system of hier-archical categories as it is in Thesaurus Rex.  Rex-wn does no better than Seco et al (2006) on the M&C dataset, suggesting that Rex?s vec-tors of IC-weighted hypernyms are no more dis-cerning than a single informative LCS. However, such vectors also permit Rex to incorporate addi-tional, fine-grained perspectives from Thesaurus Rex, allowing Rex-lat in turn to achieve a com-parable correlation to that of Li et al (2003) ? .89. Yet the formulation in (2) favors unusual or idiosyncratic perspectives that are unlikely to generalize across independent judges. The mean ratings of M&C are the stuff of consensus, not individual creativity, and outside the realm of creative metaphor it often makes sense to safely align our judgments with those of others.   By limiting its use of Thesaurus Rex to the perspectives that other judges are most likely to use, Rex-pop obtains a correlation of .93 with mean human ratings on all 30 M&C pairs. This result is comparable to that reported by Agirre et al (2009), who use SVM-based supervised learning to combine the judgments of two met-rics, one based on WordNet and another on the analysis of web contexts of both input terms. However, Rex has the greater capacity for in-sight, since it augments the structured category system of WordNet with structured categories of its own. At each level of the WordNet hierarchy, Rex finds the fine-grained category that can best inform its judgments. Because Rex makes highly selective use of the diverse products of lateral thinking, this selectivity also produces concise explanations for its judgments. 5.1 Generative Uses of Similarity A similarity metric offers a numerical measure of how closely one idea can cluster with another. It can also indicate how well one object may serve as a substitute for another, as when a letter open-er is used as a knife, or tofu is used instead of meat. This need for substitution can be grist for creativity, yet most similarity metrics can only assess a suggested substitution, rather than sug-gest one for themselves. If they are to actively shape a creative decision, our similarity metrics must be made more generative.   A similarity metric can learn to be generative, by observing how people typically cluster words and ideas that are made similar by their contexts of use. The Google 3-grams contain many in-stances of the clustering pattern ?X+s and Y+s?, as in ?cowboys and pirates? or ?doctors and law-yers?, and so a comprehensive trawl yields many 
insights into the pairings of ideas that we implic-itly see as comparable. We harvest all such Google 3-grams, to build a symmetric compara-bility graph in which any two comparable terms are adjacent nodes. For any node, we can gener-ate a diverse set of comparable ideas just by reading off its adjacent nodes. Thesaurus Rex can be used to find an embracing category for many such pairs of nodes, while Rex estimates the sim-ilarity of any two adjacent nodes. A comparabil-ity graph of 28,000 nodes is produced from the Google 3-grams, with a sparse adjacency matrix of just 1,264,827 (0.16%) non-zero entries.   Is this dense enough for a task requiring gen-erative similarity? Almuhareb & Poesio (2004) describe one such task: they sample 214 words from across 13 WordNet categories, and ask if these 214 words can be partitioned into 13 clus-ters that mirror the WordNet categories from which they were drawn. They then collect tens of thousands of web contexts for these 214 words, to extract a feature representation of each. We instead use Rex to generate, as features, a diverse set of comparable terms for each word. (We also assume that each word is a feature of itself). The Rex comparability graph suggests a pool of 8,300 features for all 214 words. The clustering toolkit CLUTO is used to partition the original 214 words into 13 clusters guided only by these com-parability features. The resulting 13 clusters have an average purity of 93.4% relative to WordNet, suggesting that categorization tasks which re-quire implicit comparability judgments are well served by a generative approach to similarity.   5.2 Learning From Similarity Judgments  Rex augments the narrow worldview of WordNet with the more diverse viewpoints it gleans from the web, not by viewing them as separate knowledge sources, but by actually updating WordNet itself. The relative performance of Rex-pop > Rex-lat > Rex-wn on the M&C da-taset shows that selective use of a divergent per-spective permits WordNet to better serve its popular role as a judge of similarity. It is worth asking then whether these passing additions to WordNet should not be made permanent.   Rex estimates a similarity score for each of the 1,264,827 pairings of comparable terms it finds in the Google 3-grams. These scores are then cached to support generative similarity, and to permit fast lookup of scores for common com-parisons. This lookup table is a lightweight means of using Rex in a range of creative substi-tution or generation tasks. Though the table is 
666
sparse, ?5.1 shows that it implicitly captures key nuances of category structure. The 39,826 unique fine-grained categories added by Rex-pop (ver-sus the 44,238 categories added by Rex-lat) in the course of its 1,264,827 comparisons thus suggest credible enhancements to WordNet. Fig-ure 2 graphs the distribution of new categories and their membership sizes when Rex-pop is used on this scale. 
 Figure 2. The number of new categories (Y-axis) with a given membership size (X-axis) added to WordNet when Rex-pop/lat are used on a large, web scale. The Goldilocks categories are those that are not so small as to lack generality, and not so large as to lack information content. For example, Rex-pop suggests the addition of 15,125 new fine-grained categories to WordNet with membership sizes ranging from 5 to 25. This is a large but manageable number of categories that should be further considered for future addition to Word-Net, or indeed to any similarly curated knowledge resource.  6 Summary and Conclusions de Bono (1970) argues that the best solutions arise from using lateral and vertical thinking in unison. Lateral thinking is divergent and genera-tive, while vertical thinking is convergent and analytical. The former can thus be used to create a pool of interesting candidates for the latter to selectively consider. Thesaurus Rex uses the web to generate a rich pool of alternate perspectives on familiar ideas, and Rex selects from this pool to perform vertical reasoning with WordNet to yield precise similarity judgments. Rex also uses the most informative perspective to concisely explain each comparison, or ? when used in gen-erative mode ? to suggest a creative comparison. For instance, to highlight the potential toxicity of coffee, Thesaurus Rex suggests comparisons with alcohol, tobacco or pesticide, as all have been categorized as toxic substances on the web. A web app based on Thesaurus Rex, to support this 
kind of lateral thinking, is accessible online at this URL: http://boundinanutshell.com/therex2 Screenshots from the Thesaurus Rex application are provided in Figures 3 and 4 overleaf. Be-cause Thesaurus Rex targets the acquisition of fine-grained perspectives, ranging from the off-beat to the obvious, it acquires an order-of-magnitude more categories from the web than can be found in WordNet itself. Rex dips selec-tively into this wealth of perspectives (and Rex-pop is more selective still), though many of Rex?s needs can be anticipated by looking to how ideas are implicitly grouped into ad-hoc catego-ries (Barsalou, 1983) in constructions such as ?X+s and Y+s?. Using the Google n-grams as a source of tacit grouping constructions, we have created a comprehensive lookup table that pro-vides Rex similarity scores for the most common (if often implicit) comparisons.      Comparability is not the same as similarity, and a non-zero similarity score does not mean that two concepts would ever be considered comparable by a human. This poses a problem for the generation of sensible comparisons. However, Rex?s lookup table captures the implic-it pragmatics of comparability, making Rex usa-ble in generative tasks where a metric must both suggest and evaluate comparisons. Human simi-larity mechanisms are evaluative and generative, convergent and divergent. Our computational mechanisms should be no less so. 7 Acknowledgements This research was partly supported by the WCU (World Class University) program under the Na-tional Research Foundation of Korea (Ministry of Education, Science and Technology of Korea, Project no. R31-30007) and partly funded by Science Foundation Ireland via the Centre for Next Generation Localization (CNGL). 
667
 Figure 3.  A screenshot from the web application Thesaurus Rex, showing the fine-grained categories found by Thesaurus Rex for the lexical concept creativity on the web.    
  Figure 4.  A screenshot from the web application Thesaurus Rex, showing the shared overlapping categories found by Thesaurus Rex for the lexical concepts divorce and war.
668
References  Aristotle (translator: James Hutton). 1982. Aristotle?s Poetics. New York: Norton. Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca and Aitor Soroa. 2009. Study on Similarity and Relatedness Using Distri-butional and WordNet-based Approaches. In Pro-ceedings of NAACL '09, The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 19?27.  Abdulrahman Almuhareb and Massimo Poesio. 2004. Attribute-Based and Value-Based Clustering: An Evaluation. In Proceedings of the Conference on Empirical Methods in NLP, Barcelona. pp. 158-165.  Lawrence W. Barsalou. 1983. Ad hoc categories. Memory and Cognition, 11:211?227.  Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Ver. 1. Philadelphia: Linguistic Data Consor-tium.  Alexander Budanitsky and Graeme Hirst. 2006.  Evaluating WordNet-based Measures of Lexical Semantic Relatedness. Computational Linguistics, 32(1):13-47.  Mary K. Camac, and Sam Glucksberg. 1984. Metaphors do not use associations between concepts, they are used to create them. Journal of Psycholinguistic Research, 13, 443-455. de Bono, Edward. 1970. Lateral thinking: creativity step by step. New York: Harper & Row. de Bono, Edward. 1971. Lateral thinking for management: a handbook for creativity. New York: McGraw Hill. Christiane Fellbaum (ed.). 1998. WordNet: An Elec-tronic Lexical Database. MIT Press, Cambridge, MA.  J. Paul Guilford. 1967. The Nature of Human Intelligence. New York: McGraw Hill. Lushan Han, Tim Finin, Paul McNamee, Anupam Joshi and Yelena Yesha. 2012. Improving Word Similarity by Augmenting PMI with Estimates of Word Polysemy. IEEE Transactions on Data and Knowledge Engineering (13 Feb. 2012). Yanfen Hao and Tony Veale. 2010. An Ironic Fist in a Velvet Glove: Creative Mis-Representation in the Construction of Ironic Similes. Minds and Machines 20(4), pp. 635?650. Jay Y. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the 10th International Conference on Research in Computational Linguistics, pp. 19-33.  
 Zornitsa Kozareva, Eileen Riloff and Eduard Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. In Proc. of the 46th Annual Meeting of the ACL, pp 1048-1056.  Claudia Leacock and Martin Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In Fellbaum, C. (ed.), WordNet: An Electronic Lexical Database, 265?283.  Yuhua Li, Zuhair A. Bandar and David McLean. 2003. An Approach for Measuring Semantic Simi-larity between Words Using Multiple Information Sources. IEEE Transactions on Knowledge and Data Engineering, vol. 15, no. 4, pp. 871-882. Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th ICML, the International Conference on Machine Learning, Morgan Kaufmann, San Francisco CA, pp. 296? 304. Michael  Lesk. 1986 Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of ACM SigDoc, ACM, 24?26. George A. Miller and Walter. G. Charles. 1991. Con-textual correlates of semantic similarity. Language and Cognitive Processes 6(1):1-28. Andrew Ortony. 1979. Beyond literal similarity. Psy-chological Review, 86, pp. 161-180. Ted Pederson, Siddarth Patwardhan and Jason Michelizzi. 2004. WordNet::Similarity: measuring the relatedness of concepts. In Proceedings of HLT-NAACL?04 (Demonstration Papers) the 2004 annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 38-41. Philip Resnick. 1995. Using Information Content to Evaluate Semantic Similarity in a Taxonomy. In Proceedings of IJCAI?95, the 14th International Joint Conference on Artificial Intelligence. Nuno Seco, Tony Veale and Jer Hayes, 2004. An In-trinsic Information Content Metric for Semantic Similarity in WordNet. In Proceedings of ECAI?04, the European Conference on Artificial Intelligence.  Michael Strube and Simone Paolo Ponzetto. 2006. WikiRelate! Computing Semantic Relatedness Using Wikipedia. In Proceedings of AAAI-06, the 2006 Conference of the Association for the Advancement of AI, pp. 1419?1424. Peter Turney. 2005. Measuring semantic similarity by latent relational analysis. Proceedings of the 19th International Joint Conference on Artificial Intelli-gence, 1136-1141.  
669
Tony Veale and Mark T. Keane. 1994. Belief Model-ing, Intentionality and Perlocution in Metaphor Comprehension. In Proceedings of the 16th Annual Meeting of the Cognitive Science Society, Atlanta, Georgia. Hillsdale, NJ: Lawrence Erlbaum.  Tony Veale. 2003. The analogical thesaurus: An emerging application at the  juncture of lexical metaphor and information retrieval. In Proceedings of IAAI?03, the 15th International Conference on Innovative Applications of Artificial Intelligence, Mexico. Tony Veale. 2004. WordNet sits the SAT: A knowledge-based approach to lexical analogy. Proceedings of ECAI'04, the European Conference on Artificial Intelligence, 606-612.  Tony Veale and Yanfen Hao. 2007. Comprehending and Generating Apt Metaphors: A Web-driven, Case-based Approach to Figurative Language. In proceedings of AAAI 2007, the 22nd AAAI Con-ference on Artificial Intelligence. Vancouver, Can-ada. Tony Veale, Guofu Li and Yanfen Hao. 2009. Grow-ing Finely-Discriminating Taxonomies from Seeds of Varying Quality and Size.  In Proc. of EACL?09, the 12th Conference of the European Chapter of the Association for Computational Linguistics pp. 835-842.  Tony Veale. 2011. Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Lin-guistic Creativity. In Proceedings of ACL?2011, the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Tony Veale. 2012. Exploding the Creativity Myth: The computational foundations of linguistic crea-tivity. London: Bloomsbury Academic. Tony Veale. 2013. Humorous Similes. Humor: The International Journal of Humor Research, 21(1):3-22. Zhibiao Wu and Martha Palmer. 1994. Verb seman-tics and lexical selection. In Proceedings of ACL?94, 32nd annual meeting of the Association for Computational Linguistics, Las Cruces, New Mexi-co,. pp. 133-138.    
670
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 39?44,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 9: The Interpretation of Noun Compounds
Using Paraphrasing Verbs and Prepositions
Cristina Butnariu
University College Dublin
ioana.butnariu@ucd.ie
Su Nam Kim
University of Melbourne
nkim@csse.unimelb.edu.au
Preslav Nakov
National University of Singapore
nakov@comp.nus.edu.sg
Diarmuid
?
O S
?
eaghdha
University of Cambridge
do242@cam.ac.uk
Stan Szpakowicz
University of Ottawa
Polish Academy of Sciences
szpak@site.uottawa.ca
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
Previous research has shown that the mean-
ing of many noun-noun compounds N
1
N
2
can be approximated reasonably well by
paraphrasing clauses of the form ?N
2
that
. . . N
1
?, where ?. . . ? stands for a verb
with or without a preposition. For exam-
ple, malaria mosquito is a ?mosquito that
carries malaria?. Evaluating the quality of
such paraphrases is the theme of Task 9 at
SemEval-2010. This paper describes some
background, the task definition, the process
of data collection and the task results. We
also venture a few general conclusions be-
fore the participating teams present their
systems at the SemEval-2010 workshop.
There were 5 teams who submitted 7 sys-
tems.
1 Introduction
Noun compounds (NCs) are sequences of two or
more nouns that act as a single noun,1 e.g., stem
cell, stem cell research, stem cell research organi-
zation, etc. Lapata and Lascarides (2003) observe
that NCs pose syntactic and semantic challenges for
three basic reasons: (1) the compounding process
is extremely productive in English; (2) the seman-
tic relation between the head and the modifier is
implicit; (3) the interpretation can be influenced by
contextual and pragmatic factors. Corpus studies
have shown that while NCs are very common in
English, their frequency distribution follows a Zip-
fian or power-law distribution and the majority of
NCs encountered will be rare types (Tanaka and
Baldwin, 2003; Lapata and Lascarides, 2003; Bald-
win and Tanaka, 2004; ?O S?eaghdha, 2008). As a
consequence, Natural Language Processing (NLP)
1
We follow the definition in (Downing, 1977).
applications cannot afford either to ignore NCs or
to assume that they can be handled by relying on a
dictionary or other static resource.
Trouble with lexical resources for NCs notwith-
standing, NC semantics plays a central role in com-
plex knowledge discovery and applications, includ-
ing but not limited to Question Answering (QA),
Machine Translation (MT), and Information Re-
trieval (IR). For example, knowing the (implicit)
semantic relation between the NC components can
help rank and refine queries in QA and IR, or select
promising translation pairs in MT (Nakov, 2008a).
Thus, robust semantic interpretation of NCs should
be of much help in broad-coverage semantic pro-
cessing.
Proposed approaches to modelling NC seman-
tics have used semantic similarity (Nastase and Sz-
pakowicz, 2003; Moldovan et al, 2004; Kim and
Baldwin, 2005; Nastase and Szpakowicz, 2006;
Girju, 2007; ?O S?eaghdha and Copestake, 2007)
and paraphrasing (Vanderwende, 1994; Kim and
Baldwin, 2006; Butnariu and Veale, 2008; Nakov
and Hearst, 2008). The former body of work seeks
to measure the similarity between known and un-
seen NCs by considering various features, usually
context-related. In contrast, the latter group uses
verb semantics to interpret NCs directly, e.g., olive
oil as ?oil that is extracted from olive(s)?, drug
death as ?death that is caused by drug(s)?, flu shot
as a ?shot that prevents flu?.
The growing popularity ? and expected direct
utility ? of paraphrase-based NC semantics has
encouraged us to propose an evaluation exercise
for the 2010 edition of SemEval. This paper gives
a bird?s-eye view of the task. Section 2 presents
its objective, data, data collection, and evaluation
method. Section 3 lists the participating teams.
Section 4 shows the results and our analysis. In
Section 5, we sum up our experience so far.
39
2 Task Description
2.1 The Objective
For the purpose of the task, we focused on two-
word NCs which are modifier-head pairs of nouns,
such as apple pie or malaria mosquito. There are
several ways to ?attack? the paraphrase-based se-
mantics of such NCs.
We have proposed a rather simple problem: as-
sume that many paraphrases can be found ? perhaps
via clever Web search ? but their relevance is up in
the air. Given sufficient training data, we seek to es-
timate the quality of candidate paraphrases in a test
set. Each NC in the training set comes with a long
list of verbs in the infinitive (often with a prepo-
sition) which may paraphrase the NC adequately.
Examples of apt paraphrasing verbs: olive oil ?
be extracted from, drug death ? be caused by, flu
shot ? prevent. These lists have been constructed
from human-proposed paraphrases. For the train-
ing data, we also provide the participants with a
quality score for each paraphrase, which is a simple
count of the number of human subjects who pro-
posed that paraphrase. At test time, given a noun
compound and a list of paraphrasing verbs, a partic-
ipating system needs to produce aptness scores that
correlate well (in terms of relative ranking) with
the held out human judgments. There may be a
diverse range of paraphrases for a given compound,
some of them in fact might be inappropriate, but
it can be expected that the distribution over para-
phrases estimated from a large number of subjects
will indeed be representative of the compound?s
meaning.
2.2 The Datasets
Following Nakov (2008b), we took advantage of
the Amazon Mechanical Turk2 (MTurk) to acquire
paraphrasing verbs from human annotators. The
service offers inexpensive access to subjects for
tasks which require human intelligence. Its API
allows a computer program to run tasks easily and
collate the subjects? responses. MTurk is becoming
a popular means of eliciting and collecting linguis-
tic intuitions for NLP research; see Snow et al
(2008) for an overview and a further discussion.
Even though we recruited human subjects,
whom we required to take a qualification test,3
2
www.mturk.com
3We soon realized that we also had to offer a version of
our assignments without a qualification test (at a lower pay
rate) since very few people were willing to take a test. Overall,
data collection was time-consuming since many
annotators did not follow the instructions. We had
to monitor their progress and to send them timely
messages, pointing out mistakes. Although the
MTurk service allows task owners to accept or re-
ject individual submissions, rejection was the last
resort since it has the triply unpleasant effect of
(1) denying the worker her fee, (2) negatively af-
fecting her rating, and (3) lowering our rating as
a requester. We thus chose to try and educate our
workers ?on the fly?. Even so, we ended up with
many examples which we had to correct manu-
ally by labor-intensive post-processing. The flaws
were not different from those already described by
Nakov (2008b). Post-editing was also necessary to
lemmatize the paraphrasing verbs systematically.
Trial Data. At the end of August 2009, we
released as trial data the previously collected para-
phrase sets (Nakov, 2008b) for the Levi-250 dataset
(after further review and cleaning). This dataset
consisted of 250 noun-noun compounds form (Levi,
1978), each paraphrased by 25-30 MTurk workers
(without a qualification test).
Training Data. The training dataset was an ex-
tension of the trial dataset. It consisted of the same
250 noun-noun compounds, but the number of an-
notators per compound increased significantly. We
aimed to recruit at least 30 additional MTurk work-
ers per compound; for some compounds we man-
aged to get many more. For example, when we
added the paraphrasing verbs from the trial dataset
to the newly collected verbs, we had 131 different
workers for neighborhood bars, compared to just
50 for tear gas. On the average, we had 72.7 work-
ers per compound. Each worker was instructed
to try to produce at least three paraphrasing verbs,
so we ended up with 191.8 paraphrasing verbs per
compound, 84.6 of them being unique. See Table 1
for more details.
Test Data. The test dataset consisted of 388
noun compounds collected from two data sources:
(1) the Nastase and Szpakowicz (2003) dataset;
and (2) the Lauer (1995) dataset. The former
contains 328 noun-noun compounds (there are
also a number of adjective-noun and adverb-noun
pairs), while the latter contains 266 noun-noun
compounds. Since these datasets overlap between
themselves and with the training dataset, we had
to exclude some examples. In the end, we had 388
we found little difference in the quality of work of subjects
recruited with and without the test.
40
Training: 250 NCs Testing: 388 NCs All: 638 NCs
Total Min/Max/Avg Total Min/Max/Avg Total Min/Max/Avg
MTurk workers 28,199 50/131/72.7 17,067 57/96/68.3 45,266 50/131/71.0
Verb types 32,832 25/173/84.6 17,730 41/133/70.9 50,562 25/173/79.3
Verb tokens 74,407 92/462/191.8 46,247 129/291/185.0 120,654 92/462/189.1
Table 1: Statistics about the the training/test datasets. Shown are the total number of verbs proposed as
well as the minimum, maximum and average number of paraphrasing verb types/tokens per compound.
unique noun-noun compounds for testing, distinct
from those used for training. We aimed for 100
human workers per testing NC, but we could only
get 68.3, with a minimum of 57 and a maximum of
96; there were 185.0 paraphrasing verbs per com-
pound, 70.9 of them being unique, which is close
to what we had for the training data.
Data format. We distribute the training data as
a raw text file. Each line has the following tab-
separated format:
NC paraphrase frequency
where NC is a noun-noun compound (e.g., ap-
ple cake, flu virus), paraphrase is a human-
proposed paraphrasing verb optionally followed
by a preposition, and frequency is the number
of annotators who proposed that paraphrase. Here
is an illustrative extract from the training dataset:
flu virus cause 38
flu virus spread 13
flu virus create 6
flu virus give 5
flu virus produce 5
...
flu virus be made up of 1
flu virus be observed in 1
flu virus exacerbate 1
The test file has a similar format, except that the
frequency is not included and the paraphrases for
each noun compound appear in random order:
...
chest pain originate
chest pain start in
chest pain descend in
chest pain be in
...
License. All datasets are released under the Cre-
ative Commons Attribution 3.0 Unported license.4
4
creativecommons.org/licenses/by/3.0
2.3 Evaluation
All evaluation was performed by computing an ap-
propriate measure of similarity/correlation between
system predictions and the compiled judgements of
the human annotators. We did it on a compound-by-
compound basis and averaged over all compounds
in the test dataset. Section 4 shows results for three
measures: Spearman rank correlation, Pearson cor-
relation, and cosine similarity.
Spearman Rank Correlation (?) was adopted
as the official evaluation measure for the competi-
tion. As a rank correlation statistic, it does not use
the numerical values of the predictions or human
judgements, only their relative ordering encoded
as integer ranks. For a sample of n items ranked
by two methods x and y, the rank correlation ? is
calculated as follows:
? =
n
?
x
i
y
i
? (
?
x
i
)(
?
y
i
)
?
n
?
x
2
i
? (
?
x
i
)
2
?
n
?
y
2
i
? (
?
y
i
)
2
(1)
where x
i
, y
i
are the ranks given by x and y to the
ith item, respectively. The value of ? ranges be-
tween -1.0 (total negative correlation) and 1.0 (total
positive correlation).
Pearson Correlation (r) is a standard measure
of correlation strength between real-valued vari-
ables. The formula is the same as (1), but with
x
i
, y
i
taking real values rather than rank values;
just like ?, r?s values fall between -1.0 and 1.0.
Cosine similarity is frequently used in NLP to
compare numerical vectors:
cos =
?
n
i
x
i
y
i
?
?
n
i
x
2
i
?
n
i
y
2
i
(2)
For non-negative data, the cosine similarity takes
values between 0.0 and 1.0. Pearson?s r can be
viewed as a version of the cosine similarity which
performs centering on x and y.
Baseline: To help interpret these evaluation mea-
sures, we implemented a simple baseline. A dis-
tribution over the paraphrases was estimated by
41
System Institution Team Description
NC-INTERP International Institute of
Information Technology,
Hyderabad
Prashant
Mathur
Unsupervised model using verb-argument frequen-
cies from parsed Web snippets and WordNet
smoothing
UCAM University of Cambridge Clemens Hepp-
ner
Unsupervised model using verb-argument frequen-
cies from the British National Corpus
UCD-GOGGLE-I University College
Dublin
Guofu Li Unsupervised probabilistic model using pattern fre-
quencies estimated from the Google N-Gram corpus
UCD-GOGGLE-II Paraphrase ranking model learned from training
data
UCD-GOGGLE-III Combination of UCD-GOGGLE-I and UCD-
GOGGLE-II
UCD-PN University College
Dublin
Paul Nulty Scoring according to the probability of a paraphrase
appearing in the same set as other paraphrases pro-
vided
UVT-MEPHISTO Tilburg University Sander
Wubben
Supervised memory-based ranker using features
from Google N-Gram Corpus and WordNet
Table 2: Teams participating in SemEval-2010 Task 9
summing the frequencies for all compounds in the
training dataset, and the paraphrases for the test ex-
amples were scored according to this distribution.
Note that this baseline entirely ignores the identity
of the nouns in the compound.
3 Participants
The task attracted five teams, one of which (UCD-
GOGGLE) submitted three runs. The participants
are listed in Table 2 along with brief system de-
scriptions; for more details please see the teams?
own description papers.
4 Results and Discussion
The task results appear in Table 3. In an evaluation
by Spearman?s ? (the official ranking measure),
the winning system was UVT-MEPHISTO, which
scored 0.450. UVT also achieved the top Pear-
son?s r score. UCD-PN is the top-scoring system
according to the cosine measure. One participant
submitted part of his results after the official dead-
line, which is marked by an asterisk.
The participants used a variety of information
sources and estimation methods. UVT-MEPHISTO
is a supervised system that uses frequency informa-
tion from the Google N-Gram Corpus and features
from WordNet (Fellbaum, 1998) to rank candidate
paraphrases. On the other hand, UCD-PN uses
no external resources and no supervised training,
yet came within 0.009 of UVT-MEPHISTO in the
official evaluation. The basic idea of UCD-PN ?
that one can predict the plausibility of a paraphrase
simply by knowing which other paraphrases have
been given for that compound regardless of their
frequency ? is clearly a powerful one. Unlike the
other systems, UCD-PN used information about the
test examples (not their ranks, of course) for model
estimation; this has similarities to ?transductive?
methods for semi-supervised learning. However,
post-hoc analysis shows that UCD-PN would have
preserved its rank if it had estimated its model on
the training data only. On the other hand, if the task
had been designed differently ? by asking systems
to propose paraphrases from the set of all possi-
ble verb/preposition combinations ? then we would
not expect UCD-PN?s approach to work as well as
models that use corpus information.
The other systems are comparable to UVT-
MEPHISTO in that they use corpus frequencies
to evaluate paraphrases and apply some kind of
semantic smoothing to handle sparsity. How-
ever, UCD-GOGGLE-I, UCAM and NC-INTERP
are unsupervised systems. UCAM uses the 100-
million word BNC corpus, while the other systems
use Web-scale resources; this has presumably ex-
acerbated sparsity issues and contributed to a rela-
tively poor performance.
The hybrid approach exemplified by UCD-
GOGGLE-III combines the predictions of a sys-
tem that models paraphrase correlations and one
that learns from corpus frequencies and thus at-
tains better performance. Given that the two top-
scoring systems can also be characterized as using
these two distinct information sources, it is natu-
ral to consider combining these systems. Simply
normalizing (to unit sum) and averaging the two
sets of prediction values for each compound does
42
Rank System Supervised? Hybrid? Spearman ? Pearson r Cosine
1 UVT-MEPHISTO yes no 0.450 0.411 0.635
2 UCD-PN no no 0.441 0.361 0.669
3 UCD-GOGGLE-III yes yes 0.432 0.395 0.652
4 UCD-GOGGLE-II yes no 0.418 0.375 0.660
5 UCD-GOGGLE-I no no 0.380 0.252 0.629
6 UCAM no no 0.267 0.219 0.374
7 NC-INTERP* no no 0.186 0.070 0.466
Baseline yes no 0.425 0.344 0.524
Combining UVT and UCD-PN yes yes 0.472 0.431 0.685
Table 3: Evaluation results for SemEval-2010 Task 9 (* denotes a late submission).
indeed give better scores: Spearman ? = 0.472,
r = 0.431, Cosine = 0.685.
The baseline from Section 2.3 turns out to be
very strong. Evaluating with Spearman?s ?, only
three systems outperform it. It is less competitive
on the other evaluation measures though. This
suggests that global paraphrase frequencies may
be useful for telling sensible paraphrases from bad
ones, but will not do for quantifying the plausibility
of a paraphrase for a given noun compound.
5 Conclusion
Given that it is a newly-proposed task, this initial
experiment in paraphrasing noun compounds has
been a moderate success. The participation rate
has been sufficient for the purposes of comparing
and contrasting different approaches to the role
of paraphrases in the interpretation of noun-noun
compounds. We have seen a variety of approaches
applied to the same dataset, and we have been able
to compare the performance of pure approaches to
hybrid approaches, and of supervised approaches
to unsupervised approaches. The results reported
here are also encouraging, though clearly there is
considerable room for improvement.
This task has established a high baseline for sys-
tems to beat. We can take heart from the fact that
the best performance is apparently obtained from a
combination of corpus-derived usage features and
dictionary-derived linguistic knowledge. Although
clever but simple approaches can do quite well on
such a task, it is encouraging to note that the best
results await those who employ the most robust
and the most informed treatments of NCs and their
paraphrases. Despite a good start, this is a chal-
lenge that remains resolutely open. We expect that
the dataset created for the task will be a valuable
resource for future research.
Acknowledgements
This work is partially supported by grants from
Amazon and from the Bulgarian National Science
Foundation (D002-111/15.12.2008 ? SmartBook).
References
Timothy Baldwin and Takaaki Tanaka. 2004. Trans-
lation by Machine of Compound Nominals: Getting
it Right. In Proceedings of the ACL-04 Workshop
on Multiword Expressions: Integrating Processing,
pages 24?31, Barcelona, Spain.
Cristina Butnariu and Tony Veale. 2008. A Concept-
Centered Approach to Noun-Compound Interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (COLING-
08), pages 81?88, Manchester, UK.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810?842.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Roxana Girju. 2007. Improving the Interpretation
of Noun Phrases with Cross-linguistic Information.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL-07),
pages 568?575, Prague, Czech Republic.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet
similarity. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP-05), pages 945?956, Jeju Island, South Ko-
rea.
Su Nam Kim and Timothy Baldwin. 2006. Inter-
preting Semantic Relations in Noun Compounds via
Verb Semantics. In Proceedings of the COLING-
ACL-06 Main Conference Poster Sessions, pages
491?498, Sydney, Australia.
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional evi-
dence. In Proceedings of the 10th Conference of the
43
European Chapter of the Association for Computa-
tional Linguistics (EACL-03), pages 235?242, Bu-
dapest, Hungary.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Macquarie University.
Judith Levi. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press, New York, NY.
DanMoldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the Se-
mantic Classification of Noun Phrases. In Proceed-
ings of the HLT-NAACL-04 Workshop on Computa-
tional Lexical Semantics, pages 60?67, Boston, MA.
Preslav Nakov and Marti A. Hearst. 2008. Solving
Relational Similarity Problems Using the Web as a
Corpus. In Proceedings of the 46th Annual Meet-
ing of the Association of Computational Linguistics
(ACL-08), pages 452?460, Columbus, OH.
Preslav Nakov. 2008a. Improved Statistical Machine
Translation Using Monolingual Paraphrases. In Pro-
ceedings of the 18th European Conference on Artifi-
cial Intelligence (ECAI-08), pages 338?342, Patras,
Greece.
Preslav Nakov. 2008b. Noun Compound Interpreta-
tion Using Paraphrasing Verbs: Feasibility Study.
In Proceedings of the 13th International Confer-
ence on Artificial Intelligence: Methodology, Sys-
tems and Applications (AIMSA-08), pages 103?117,
Varna, Bulgaria.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings
of the 5th International Workshop on Computational
Semantics (IWCS-03), pages 285?301, Tilburg, The
Netherlands.
Vivi Nastase and Stan Szpakowicz. 2006. Matching
syntactic-semantic graphs for semantic relation as-
signment. In Proceedings of the 1st Workshop on
Graph Based Methods for Natural Language Pro-
cessing (TextGraphs-06), pages 81?88, New York,
NY.
Diarmuid
?
O S?eaghdha and Ann Copestake. 2007. Co-
occurrence Contexts for Noun Compound Interpre-
tation. In Proceedings of the ACL-07 Workshop
on A Broader Perspective on Multiword Expressions
(MWE-07), pages 57?64, Prague, Czech Republic.
Diarmuid
?
O S?eaghdha. 2008. Learning Compound
Noun Semantics. Ph.D. thesis, University of Cam-
bridge.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and Fast ? But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-08), pages 254?263, Honolulu, HI.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: A feasibility
study on shallow processing. In Proceedings of the
ACL-03 Workshop on Multiword Expressions (MWE-
03), pages 17?24, Sapporo, Japan.
Lucy Vanderwende. 1994. Algorithm for Automatic
Interpretation of Noun Sequences. In Proceedings
of the 15th International Conference on Compu-
tational Linguistics (COLING-94), pages 782?788,
Kyoto, Japan.
44
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 230?233,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
UCD-Goggle: A Hybrid System for Noun Compound Paraphrasing
Guofu Li
School of Computer Science
and Informatics
University College Dublin
guofu.li@ucd.ie
Alejandra Lopez-Fernandez
School of Computer Science
and Informatics
University College Dublin
alejandra.lopez
-fernandez@ucd.ie
Tony Veale
School of Computer Science
and Informatics
University College Dublin
tony.veale@ucd.ie
Abstract
This paper addresses the problem of rank-
ing a list of paraphrases associated with a
noun-noun compound as closely as possi-
ble to human raters (Butnariu et al, 2010).
UCD-Goggle tackles this task using se-
mantic knowledge learnt from the Google
n-grams together with human-preferences
for paraphrases mined from training data.
Empirical evaluation shows that UCD-
Goggle achieves 0.432 Spearman correla-
tion with human judgments.
1 Introduction
Noun compounds (NC) are sequences of nouns
acting as a single noun (Downing, 1977). Re-
search on noun compounds involves two main
tasks: NC detection and NC interpretation. The
latter has been studied in the context of many
natural language applications, including question-
answering, machine translation, information re-
trieval, and information extraction.
The use of multiple paraphrases as a semantic
intepretation of noun compounds has recently be-
come popular (Kim and Baldwin, 2006; Nakov
and Hearst, 2006; Butnariu and Veale, 2008;
Nakov, 2008). The best paraphrases are those
which most aptly characterize the relationship be-
tween the modifier noun and the head noun.
The aim of this current work is to provide a
ranking for a list of paraphrases that best approxi-
mates human rankings for the same paraphrases.
We have created a system called UCD-Goggle,
which uses semantic knowledge acquired from
Google n-grams together with human-preferences
mined from training data. Three major com-
ponents are involved in our system: B-score,
produced by a Bayesian algorithm using seman-
tic knowledge from the n-grams corpus with a
smoothing layer of additional inference; R
t
-score
captures human preferences observed in the tail
distribution of training data; and R
p
-score cap-
tures pairwise paraphrase preferences calculated
from the training data. Our best system for
SemEval-2 task 9 combines all three components
and achieves a Spearman correlation of 0.432 with
human rankings.
This paper is organized as follows: the Bayesian
B-score is introduced in section 2. In section 3
we describe two supervised approaches to mining
the preferences of human raters from training data.
Finally, section 4 presents the results of our empir-
ical evaluation of the UCD-Goggle system.
2 Semantic Approach
2.1 Collecting Data
Google have made their web n-grams, also known
as Web-1T corpus, public via the Linguistic Data
Consortium (Brants and Franz, 2006). This cor-
pus contains sequences of n terms that occur more
than 40 times on the web.
We view the paraphrase task as that of suggest-
ing the right verb phrase for two nouns (But-
nariu and Veale, 2008). Previous work has shown
the n-grams corpus to be a promising resource
for retrieving semantic evidence for this approach.
However, the corpus itself needs to be tailored to
serve our purpose. Since the n-grams corpus is a
collection of raw snippets from the web, together
with their web frequency, certain pre-processing
steps are essential before it can be used as a semi-
structured knowledge base. Following a syntac-
tic pattern approach, snippets in the n-grams that
agree with the following patterns are harvested:
1. Head VP Mod
2. Head VP DET Mod
3. Head [that|which] VP Mod
4. Head [that|which] VP DET Mod
Here, DET denotes any of the determiners (i.e.,
230
the set of {an, a, the} for English), Head and Mod
are nouns for heads and modifiers, and VP stands
for verb-based paraphrases observed in the test
data. It must be highlighted that, when we collect
snippets for the KB, any Head or Mod that falls out
of the range of the dataset are also accepted via a
process of semantic slippage (to be discussed in
Sect. 2.4). The patterns listed above enable us to
collect examples such as:
1. ?bread containing nut?
2. ?pill alleviates the headache?
3. ?novel which is about crimes?
4. ?problem that involves the students?
After a shallow parse, these snippets are formal-
ized into the triple format ?Head, Para,Mod?.
The sample snippets above are represented as:
1. ?bread, contain, nut?
2. ?pill, alleviate, headache?
3. ?novel, be about, crime?
4. ?problem, involve, student?
We use ?Head, Para,Mod? to denote the fre-
quency of ?Head, Para,Mod? in the n-grams.
2.2 Loosely Coupled Compound Analysis
Tens of millions of snippets are harvested and
cleaned up in this way, yet expecting even this
large set to provide decent coverage over the test
data is still unrealistic. We calculated the proba-
bility of an example in the test data to appear in
KB at less than 1%. To overcome the coverage is-
sue, a loosely coupled analysis and representation
of compounds is employed. Despite the fact that
both modifier and head can influence the ranking
of a paraphrase, we believe that either the modifier
or the head is the dominating factor in most cases.
This assumption has been shown to be plausible
by earlier work (Butnariu and Veale, 2008). Thus,
instead of storing complete triples in our KB, we
divide each complete triple into two partial triples
as shown below:
?Head, Para,Mod? ?
{
?Head, Para, ??
??, Para,Mod?
We can also retrieve these partial triples directly
from the n-grams corpus using partial patterns like
?Head Para? and ?Para Mod?. However, just as
shorter incomplete patterns can produce a larger
KB, they also accept much more noise. For in-
stance, single-verb paraphrases are very common
among the test data. In these cases, the partial pat-
tern approach would need to harvest snippets with
the form ?NN VV? or ?VV NN? from 2-grams,
which are too common to be reliable.
2.3 Probabilistic Framework
In the probabilistic framework, we define the B-
score as the conditional probability of a para-
phrase, Para, being suggested for a given com-
pound Comp:
B(Para;Comp) ? P (Para|Comp) (1)
Using the KB, we can estimate this conditional
probability by applying the Bayes theorem:
P (Para|Comp) =
P (Comp|Para)P (Para)
P (Comp)
(2)
The loose-coupling assumption (Sect. 2.2) allows
us to estimate P (Comp) as:
P (Comp) ? P (Mod ?Head). (3)
Meanwhile, a priori probabilities such as
P (Para) can be easily inferred from the KB.
2.4 Inferential Smoothing Layer
After applying the loose-coupling technique de-
scribed in Section 2.2, the coverage of the KB
rises to 31.78% (see Figure 1). To further in-
crease this coverage, an inference layer is added
to the system. This layer aims to stretch the con-
tents of the KB via semantic slippage to the KB, as
guided by the maximization of a fitness function.
A WordNet-based similarity matrix is employed
(Seco et al, 2004) to provide a similarity measure
between nouns (so sim(x, x) is 1). Then, a su-
perset of Head or Mod (denoted as H andM re-
spectively) can be extracted by including all nouns
with similarity greater than 0 to any of them in the
test data. Formally, for Head we have:
H = {h|sim(h,Head) ? 0, Head in dataset}.
(4)
The definition ofM is analogous to that ofH.
A system of equations is defined to produce al-
ternatives for Head and Mod and their smoothed
corpus frequencies (we show only the functions
for head here):
h
0
= Head (5)
fit(h) = sim
2
(h, h
n
)? ?h, p, ?? (6)
h
n+1
= arg max
h?H
fit(h) (7)
231
Here, fit(h) is a fitness function of the can-
didate head h, in the context of a paraphrase p.
Empirically, we use h
1
for Head and fit(h
1
) for
?Head, Para, ?? when calculating the B-score
back in the probabilistic framework (Sect. 2.3). In
theory, we can apply this smoothing step repeat-
edly until convergence is obtained.
Figure 1: Comparison on coverage.
This semantic slippage mechanism allows a
computer to infer the missing parts of the KB, by
building a bridge between the limitations of a fi-
nite KB and the knowledge demands of an appli-
cation. Figure 1 above shows how the coverage of
the system increases when using partial matching
and the smoothing technique, over the use of exact
matching with the KB.
3 Preferences for Paraphrases
3.1 Tail-based Preference
Similar to various types of data studied by social
scientists, the distribution of strings in our corpus
tends to obey Zipf?s law (Zipf, 1936). The same
Zipfian trend was also observed in the compound-
paraphrase dataset: more than 190 out of 250 com-
pounds in the training data have 60% of their para-
phrases in an undiscriminating tail, while 245 of
250 have 50% of their paraphrases in the tail. We
thus assume the existence of a long tail in the para-
phrase list for each compound.
The tail of each paraphrase list can be a valuable
heuristic for modeling human paraphrase prefer-
ences. We refer to this model as the tail-based
preference model. We assume that an occurrence
of a paraphrase is deemed to occur in the tail iff it
is mentioned by the human raters only once. Thus,
the tail preference is defined as the probability that
a paraphrase appears in the non-tail part of the list
for all compounds in the training data. Formally,
it can be expressed as:
R
t
(p) =
?
c?C
?(c, p)f(c, p)
?
c?C
f(c, p)
(8)
where C is the set of all compounds in the training
data and f(c, p) is the frequency of paraphrase p
on compound c as given by the human raters. The
?(c, p) is a filter coefficient as shown below:
?(c, p) =
{
1, f(c, p) > 1,
0, f(c, p) = 1.
(9)
The tail-based preference model is simple but
effective when used in conjunction with seman-
tic ranking via the KB acquired from n-grams.
However, an important drawback is that the tail
model assigns a static preference to paraphrase
(i.e., tail preferences are assumed to be context-
independent). More than that, this preference does
not take information from non-tail paraphrases
into consideration. Due to these downsides, we
use pairwise preferences described below.
3.2 Pairwise Preference
To fully utilize the training data, we employ an-
other preference mining approach called pairwise
preference modeling. This approach applies the
principle of pairwise comparison (David, 1988)
to determine the rank of a paraphrase inside a list.
We build a pairwise comparison matrix ? for
paraphrases using the values of Equation 10 (here
we have assumed that each of the paraphrases has
been mapped into numeric values):
?
i,j
=
{
n(p
i
,p
j
)
n(p
i
,p
j
)+n(p
j
,p
i
)
, n(p
i
, p
j
) > n(p
j
, p
i
),
0, otherwise.
(10)
where n(p
i
, p
j
) is the relative preferability of p
i
to p
j
. To illustrate the logic behind n(x, y), we
imagine a scenario with three compounds shown
in Table 1:
abor. prob. abor. vote arti. desc.
involve 12 8 3
concern 10 9 5
be about 3 9 15
Table 1: An example
1
to illustrate n(x, y)
1
In this example, abor. prob. stands for abortion problem,
abor. vote stands for abortion vote, and arti. desc. stands for
artifact description
232
The relative preferability is given by the number
of times that the frequency of p
i
from human raters
is greater than that of p
j
. Observing that 1 out of
3 times involve is ranked higher than concern, we
can calculate their relative preferability as:
n(involve, concern) = 1
n(concern, involve) = 2
Once the matrix is built, the preference score for
a paraphrase i is calculated as:
R
p
(i; c) =
?
j?P
c
?
i,j
|P
c
|
(11)
whereP
c
is the list of paraphrases for a given com-
pound c in the test data. The pairwise preference
puts a paraphrase in the context of its company, so
that the opinions of human raters can be approxi-
mated more precisely.
4 Empirical Results
We evaluated our system by tackling theSemEval-
2 task 9 test data. We created three systems with
different combinations of the three components
(B, R
t
, R
p
). Table 2 below shows the perfor-
mance of UCD-Goggle for each setting:
System Config Spearman ? Pearson r
I B + R
t
0.380 0.252
II R
p
0.418 0.375
III B + R
t
+ R
p
0.432 0.395
* Baseline 0.425 0.344
Table 2: Evaluation results on different settings of
the UCD-Goggle system.
The first setting is a hybrid system which first
calculates a ranking according to the ngrams cor-
pus and then applies a very simple preference
heuristic (Sect. 2.3 and 3.1). The second setting
simply applies the pairwise preference algorithm
to the training data to learn ranking preferences
(Sect. 3.2). Finally, the third setting integrates
both of these settings in a single approach.
The individual contribution of B-score and R
t
was tested by two-fold cross validation applied to
the training data. The training data was split into
two subsets and preferences were learnt from one
part and then applied to the other. As an unsuper-
vised algorithm, B-score produced Spearman cor-
relation of 0.31 while the R
t
-score gave 0.33. We
noticed that more than 78% of the paraphrases had
0 score by R
t
. This number not only reconfirmed
the existence of the long-tail phenomenon, but also
suggested thatR
t
-score alone could hardly capture
the preference on the non-tail part. On the other
hand, with more than 80% chance we could expect
B to produce a non-zero score for a paraphrase,
even if the paraphrase fell out of the topic. When
combined together, B and R
t
complemented each
other and improved the performance considerably.
However, this combined effort still could not beat
the pairwise preference R
p
or the baseline system,
which had no semantic knowledge involved. The
major limitation of our system is that the seman-
tic approach is totally ignorant of the training data.
In future work, we will intend to use it as a valu-
able resource in both KB construction and ranking
stage.
References
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium.
C. Butnariu and T. Veale. 2008. A concept-centered
approach to noun-compound interpretation. In Proc.
of the 22nd COLING, pages 81?88, Manchester,
UK.
C. Butnariu, S. N. Kim, P. Nakov, D.
?
O S?eaghdha,
S. Szpakowicz, and T. Veale. 2010. Semeval-2 task
9: The interpretation of noun compounds using para-
phrasing verbs and prepositions. In Workshop on
Semantic Evaluation, Uppsala, Sweden.
H. A. David. 1988. The Method of Paired Compar-
isons. Oxford University Press, New York.
P. Downing. 1977. On the creation and use of English
compound nouns. In Language 53, pages 810?842.
S. N. Kim and T. Baldwin. 2006. Interpreting seman-
tic relations in noun compounds via verb semantics.
In Proc. of the COLING/ACL, pages 491?498, Mor-
ristown, NJ, USA.
P. Nakov and M. A. Hearst. 2006. Using verbs to char-
acterize noun-noun relations. In Proc. of AIMSA,
pages 233?244.
P. Nakov. 2008. Noun compound interpretation using
paraphrasing verbs: Feasibility study. In Proc. of
the 13th AIMSA, pages 103?117, Berlin, Heidelberg.
Springer-Verlag.
N. Seco, T. Veale, and J. Hayes. 2004. An intrinsic
information content metric for semantic similarity
in WordNet. In Proc. of the 16th ECAI, Valencia,
Spain. John Wiley.
G. K. Zipf. 1936. The Psycho-Biology of Language:
An Introdution to Dynamic Philology. Routledge,
London.
233
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 138?143, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 4: Free Paraphrases of Noun Compounds
Iris Hendrickx
Radboud University Nijmegen &
Universidade de Lisboa
iris@clul.ul.pt
Preslav Nakov
QCRI, Qatar Foundation
pnakov@qf.org.qa
Stan Szpakowicz
University of Ottawa &
Polish Academy of Sciences
szpak@eecs.uottawa.ca
Zornitsa Kozareva
University of Southern California
kozareva@isi.edu
Diarmuid O? Se?aghdha
University of Cambridge
do242@cam.ac.uk
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
In this paper, we describe SemEval-2013 Task
4: the definition, the data, the evaluation and
the results. The task is to capture some of the
meaning of English noun compounds via para-
phrasing. Given a two-word noun compound,
the participating system is asked to produce
an explicitly ranked list of its free-form para-
phrases. The list is automatically compared
and evaluated against a similarly ranked list
of paraphrases proposed by human annota-
tors, recruited and managed through Ama-
zon?s Mechanical Turk. The comparison of
raw paraphrases is sensitive to syntactic and
morphological variation. The ?gold? ranking
is based on the relative popularity of para-
phrases among annotators. To make the rank-
ing more reliable, highly similar paraphrases
are grouped, so as to downplay superficial dif-
ferences in syntax and morphology. Three
systems participated in the task. They all beat
a simple baseline on one of the two evalua-
tion measures, but not on both measures. This
shows that the task is difficult.
1 Introduction
A noun compound (NC) is a sequence of nouns
which act as a single noun (Downing, 1977), as in
these examples: colon cancer, suppressor protein,
tumor suppressor protein, colon cancer tumor sup-
pressor protein, etc. This type of compounding is
highly productive in English. NCs comprise 3.9%
and 2.6% of all tokens in the Reuters corpus and the
British National Corpus (BNC), respectively (Bald-
win and Tanaka, 2004).
The frequency spectrum of compound types fol-
lows a Zipfian distribution (O? Se?aghdha, 2008), so
many NC tokens belong to a ?long tail? of low-
frequency types. More than half of the two-noun
types in the BNC occur exactly once (Kim and Bald-
win, 2006). Their high frequency and high produc-
tivity make robust NC interpretation an important
goal for broad-coverage semantic processing of En-
glish texts. Systems which ignore NCs may give up
on salient information about the semantic relation-
ships implicit in a text. Compositional interpretation
is also the only way to achieve broad NC coverage,
because it is not feasible to list in a lexicon all com-
pounds which one is likely to encounter. Even for
relatively frequent NCs occurring 10 times or more
in the BNC, static English dictionaries provide only
27% coverage (Tanaka and Baldwin, 2003).
In many natural language processing applications
it is important to understand the syntax and seman-
tics of NCs. NCs often are structurally similar,
but have very different meaning. Consider caffeine
headache and ice-cream headache: a lack of caf-
feine causes the former, an excess of ice-cream ? the
latter. Different interpretations can lead to different
inferences, query expansion, paraphrases, transla-
tions, and so on. A question answering system may
have to determine whether protein acting as a tumor
suppressor is an accurate paraphrase for tumor sup-
pressor protein. An information extraction system
might need to decide whether neck vein thrombosis
and neck thrombosis can co-refer in the same doc-
ument. A machine translation system might para-
phrase the unknown compound WTO Geneva head-
quarters as WTO headquarters located in Geneva.
138
Research on the automatic interpretation of NCs
has focused mainly on common two-word NCs. The
usual task is to classify the semantic relation under-
lying a compound with either one of a small number
of predefined relation labels or a paraphrase from an
open vocabulary. Examples of the former take on
classification include (Moldovan et al, 2004; Girju,
2007; O? Se?aghdha and Copestake, 2008; Tratz and
Hovy, 2010). Examples of the latter include (Nakov,
2008b; Nakov, 2008a; Nakov and Hearst, 2008; But-
nariu and Veale, 2008) and a previous NC paraphras-
ing task at SemEval-2010 (Butnariu et al, 2010),
upon which the task described here builds.
The assumption of a small inventory of prede-
fined relations has some advantages ? parsimony and
generalization ? but at the same time there are lim-
itations on expressivity and coverage. For exam-
ple, the NCs headache pills and fertility pills would
be assigned the same semantic relation (PURPOSE)
in most inventories, but their relational semantics
are quite different (Downing, 1977). Furthermore,
the definitions given by human subjects can involve
rich and specific meanings. For example, Down-
ing (1977) reports that a subject defined the NC
oil bowl as ?the bowl into which the oil in the en-
gine is drained during an oil change?, compared to
which a minimal interpretation bowl for oil seems
very reductive. In view of such arguments, linguists
such as Downing (1977), Ryder (1994) and Coulson
(2001) have argued for a fine-grained, essentially
open-ended space of interpretations.
The idea of working with fine-grained para-
phrases for NC semantics has recently grown in pop-
ularity among NLP researchers (Butnariu and Veale,
2008; Nakov and Hearst, 2008; Nakov, 2008a). Task
9 at SemEval-2010 (Butnariu et al, 2010) was de-
voted to this methodology. In that previous work,
the paraphrases provided by human subjects were
required to fit a restrictive template admitting only
verbs and prepositions occurring between the NC?s
constituent nouns. Annotators recruited through
Amazon Mechanical Turk were asked to provide
paraphrases for the dataset of NCs. The gold stan-
dard for each NC was the ranked list of paraphrases
given by the annotators; this reflects the idea that a
compound?s meaning can be described in different
ways, at different levels of granularity and capturing
different interpretations in the case of ambiguity.
For example, a plastic saw could be a saw made
of plastic or a saw for cutting plastic. Systems par-
ticipating in the task were given the set of attested
paraphrases for each NC, and evaluated according to
how well they could reproduce the humans? ranking.
The design of this task, SemEval-2013 Task 4,
is informed by previous work on compound anno-
tation and interpretation. It is also influenced by
similar initiatives, such as the English Lexical Sub-
stitution task at SemEval-2007 (McCarthy and Nav-
igli, 2007), and by various evaluation exercises in
the fields of paraphrasing and machine translation.
We build on SemEval-2010 Task 9, extending the
task?s flexibility in a number of ways. The restric-
tions on the form of annotators? paraphrases was re-
laxed, giving us a rich dataset of close-to-freeform
paraphrases (Section 3). Rather than ranking a set of
attested paraphrases, systems must now both gener-
ate and rank their paraphrases; the task they perform
is essentially the same as what the annotators were
asked to do. This new setup required us to innovate
in terms of evaluation measures (Section 4).
We anticipate that the dataset and task will be of
broad interest among those who study lexical se-
mantics. We believe that the overall progress in the
field will significantly benefit from a public-domain
set of free-style NC paraphrases. That is why our
primary objective is the challenging endeavour of
preparing and releasing such a dataset to the re-
search community. The common evaluation task
which we establish will also enable researchers to
compare their algorithms and their empirical results.
2 Task description
This is an English NC interpretation task, which ex-
plores the idea of interpreting the semantics of NCs
via free paraphrases. Given a noun-noun compound
such as air filter, the participating systems are asked
to produce an explicitly ranked list of free para-
phrases, as in the following example:
1 filter for air
2 filter of air
3 filter that cleans the air
4 filter which makes air healthier
5 a filter that removes impurities from the air
. . .
139
Such a list is then automatically compared and
evaluated against a similarly ranked list of para-
phrases proposed by human annotators, recruited
and managed via Amazon?s Mechanical Turk. The
comparison of raw paraphrases is sensitive to syn-
tactic and morphological variation. The ranking
of paraphrases is based on their relative popular-
ity among different annotators. To make the rank-
ing more reliable, highly similar paraphrases are
grouped so as to downplay superficial differences in
syntax and morphology.
3 Data collection
We used Amazon?s Mechanical Turk service to
collect diverse paraphrases for a range of ?gold-
standard? NCs.1 We paid the workers a small fee
($0.10) per compound, for which they were asked to
provide five paraphrases. Each paraphrase should
contain the two nouns of the compound (in sin-
gular or plural inflectional forms, but not in an-
other derivational form), an intermediate non-empty
linking phrase and optional preceding or following
terms. The paraphrasing terms could have any part
of speech, so long as the resulting paraphrase was a
well-formed noun phrase headed by the NC?s head.
We gave the workers feedback during data col-
lection if they appeared to have misunderstood the
nature of the task. Once raw paraphrases had been
collected from all workers, we collated them into a
spreadsheet, and we merged identical paraphrases
in order to calculate their overall frequencies. Ill-
formed paraphrases ? those violating the syntactic
restrictions described above ? were manually re-
moved following a consensus decision-making pro-
cedure; every paraphrase was checked by at least
two task organizers. We did not require that the
paraphrases be semantically felicitous, but we per-
formed minor edits on the remaining paraphrases if
they contained obvious typos.
The remaining well-formed paraphrases were
sorted by frequency separately for each NC. The
most frequent paraphrases for a compound are as-
signed the highest rank 0, those with the next-
highest frequency are given a rank of 1, and so on.
1Since the annotation on Mechanical Turk was going slowly,
we also recruited four other annotators to do the same work,
following exactly the same instructions.
Total Min / Max / Avg
Trial/Train (174 NCs)
paraphrases 6,069 1 / 287 / 34.9
unique paraphrases 4,255 1 / 105 / 24.5
Test (181 NCs)
paraphrases 9,706 24 / 99 / 53.6
unique paraphrases 8,216 21 / 80 / 45.4
Table 1: Statistics of the trial and test datasets: the total
number of paraphrases with and without duplicates, and
the minimum / maximum / average per noun compound.
Paraphrases with a frequency of 1 ? proposed for
a given NC by only one annotator ? always occupy
the lowest rank on the list for that compound.
We used 174+181 noun-noun compounds from
the NC dataset of O? Se?aghdha (2007). The trial
dataset, which we initially released to the partici-
pants, consisted of 4,255 human paraphrases for 174
noun-noun pairs; this dataset was also the training
dataset. The test dataset comprised paraphrases for
181 noun-noun pairs. The ?gold standard? contained
9,706 paraphrases of which 8,216 were unique for
those 181 NCs. Further statistics on the datasets are
presented in Table 1.
Compared with the data collected for the
SemEval-2010 Task 9 on the interpretation of noun
compounds, the data collected for this new task have
a far greater range of variety and richness. For ex-
ample, the following (selected) paraphrases for work
area vary from parsimonious to expansive:
? area for work
? area of work
? area where work is done
? area where work is performed
? . . .
? an area cordoned off for persons responsible for
work
? an area where construction work is carried out
? an area where work is accomplished and done
? area where work is conducted
? office area assigned as a work space
? . . .
140
4 Scoring
Noun compounding is a generative aspect of lan-
guage, but so too is the process of NC interpretation:
human speakers typically generate a range of possi-
ble interpretations for a given compound, each em-
phasizing a different aspect of the relationship be-
tween the nouns. Our evaluation framework reflects
the belief that there is rarely a single right answer
for a given noun-noun pairing. Participating systems
are thus expected to demonstrate some generativity
of their own, and are scored not just on the accu-
racy of individual interpretations, but on the overall
breadth of their output.
For evaluation, we provided a scorer imple-
mented, for good portability, as a Java class. For
each noun compound to be evaluated, the scorer
compares a list of system-suggested paraphrases
against a ?gold-standard? reference list, compiled
and rank-ordered from the paraphrases suggested
by our human annotators. The score assigned to
each system is the mean of the system?s performance
across all test compounds. Note that the scorer re-
moves all determiners from both the reference and
the test paraphrases, so a system is neither punished
for not reproducing a determiner or rewarded for
producing the same determiners.
The scorer can match words identically or non-
identically. A match of two identical words Wgold
and Wtest earns a score of 1.0. There is a partial
score of (2 |P | / (|PWgold| + |PWtest|))2 for a
match of two words PWgold and PWtest that are
not identical but share a common prefix P , |P | > 2,
e.g., wmatch(cutting, cuts) = (6/11)2 = 0.297.
Two n-grams Ngold = [GW1, . . . , GWn] and
Ntest = [TW1, . . . , TWn] can be matched if
wmatch(GWi, TWi) > 0 for all i in 1..n. The
score assigned to the match of these two n-grams is
then
?
i wmatch(GWi, TWi). For every n-gram
Ntest = [TW1, . . . , TWn] in a system-generated
paraphrase, the scorer finds a matching n-gram
Ngold = [GW1, . . . , GWn] in the reference para-
phrase Paragold which maximizes this sum.
The overall n-gram overlap score for a reference
paraphrase Paragold and a system-generated para-
phrase Paratest is the sum of the score calculated
for all n-grams in Paratest, where n ranges from 1
to the size of Paratest.
This overall score is then normalized by dividing
by the maximum value among the n-gram overlap
score for Paragold compared with itself and the n-
gram overlap score for Paratest compared with it-
self. This normalization step produces a paraphrase
match score in the range [0.0 ? 1.0]. It punishes a
paraphrase Paratest for both over-generating (con-
taining more words than are found in Paragold)
and under-generating (containing fewer words than
are found in Paragold). In other words, Paratest
should ideally reproduce everything in Paragold,
and nothing more or less.
The reference paraphrases in the ?gold standard?
are ordered by rank; the highest rank is assigned to
the paraphrases which human judges suggested most
often. The rank of a reference paraphrase matters
because a good participating system will aim to re-
produce the top-ranked ?gold-standard? paraphrases
as produced by human judges. The scorer assigns
a multiplier of R/(R + n) to reference paraphrases
at rank n; this multiplier asymptotically approaches
0 for the higher values of n of ever lower-ranked
paraphrases. We choose a default setting of R = 8,
so that a reference paraphrase at rank 0 (the highest
rank) has a multiplier of 1, while a reference para-
phrase at rank 5 has a multiplier of 8/13 = 0.615.
When a system-generated paraphrase Paratest is
matched with a reference paraphrase Paragold, their
normalized n-gram overlap score is scaled by the
rank multiplier attaching to the rank of Paragold rel-
ative to the other reference paraphrases provided by
human judges. The scorer automatically chooses the
reference paraphrase Paragold for a test paraphrase
Paratest so as to maximize this product of normal-
ized n-gram overlap score and rank multiplier.
The overall score assigned to each system for
a specific compound is calculated in two differ-
ent ways: using isomorphic matching of suggested
paraphrases to the ?gold-standard?s? reference para-
phrases (on a one-to-one basis); and using non-
isomorphic matching of system?s paraphrases to the
?gold-standard?s? reference paraphrases (in a poten-
tially many-to-one mapping).
Isomorphic matching rewards both precision and
recall. It rewards a system for accurately reproduc-
ing the paraphrases suggested by human judges, and
for reproducing as many of these as it can, and in
much the same order.
141
In isomorphic mode, system?s paraphrases are
matched 1-to-1 with reference paraphrases on a first-
come first-matched basis, so ordering can be crucial.
Non-isomorphic matching rewards only preci-
sion. It rewards a system for accurately reproducing
the top-ranked human paraphrases in the ?gold stan-
dard?. A system will achieve a higher score in a non-
isomorphic match if it reproduces the top-ranked hu-
man paraphrases as opposed to lower-ranked human
paraphrases. The ordering of system?s paraphrases
is thus not important in non-isomorphic matching.
Each system is evaluated using the scorer in both
modes, isomorphic and non-isomorphic. Systems
which aim only for precision should score highly
on non-isomorphic match mode, but poorly in iso-
morphic match mode. Systems which aim for pre-
cision and recall will face a more substantial chal-
lenge, likely reflected in their scores.
A na??ve baseline
We decided to allow preposition-only paraphrases,
which are abundant in the paraphrases suggested
by human judges in the crowdsourcing Mechanical
Turk collection process. This abundance means that
the top-ranked paraphrase for a given compound is
often a preposition-only phrase, or one of a small
number of very popular paraphrases such as used for
or used in. It is thus straightforward to build a na??ve
baseline generator which we can expect to score
reasonably on this task, at least in non-isomorphic
matching mode. For each test compound M H,
the baseline system generates the following para-
phrases, in this precise order: H of M, H in M, H
for M, H with M, H on M, H about M, H has M, H to
M, H used for M, H used in M.
This na??ve baseline is truly unsophisticated. No
attempt is made to order paraphrases by their corpus
frequencies or by their frequencies in the training
data. The same sequence of paraphrases is generated
for each and every test compound.
5 Results
Three teams participated in the challenge, and all
their systems were supervised. The MELODI sys-
tem relied on semantic vector space model built
from the UKWAC corpus (window-based, 5 words).
It used only the features of the right-hand head noun
to train a maximum entropy classifier.
Team isomorphic non-isomorphic
SFS 23.1 17.9
IIITH 23.1 25.8
MELODI-Primary 13.0 54.8
MELODI-Contrast 13.6 53.6
Naive Baseline 13.8 40.6
Table 2: Results for the participating systems; the base-
line outputs the same paraphrases for all compounds.
The IIITH system used the probabilities of the
preposition co-occurring with a relation to identify
the class of the noun compound. To collect statis-
tics, it used Google n-grams, BNC and ANC.
The SFS system extracted templates and fillers
from the training data, which it then combined with
a four-gram language model and a MaxEnt reranker.
To find similar compounds, they used Lin?s Word-
Net similarity. They further used statistics from the
English Gigaword and the Google n-grams.
Table 2 shows the performance of the partici-
pating systems, SFS, IIITH and MELODI, and the
na??ve baseline. The baseline shows that it is rela-
tively easy to achieve a moderately good score in
non-isomorphic match mode by generating a fixed
set of paraphrases which are both common and
generic: two of the three participating systems,
SFS and IIITH, under-perform the na??ve baseline
in non-isomorphic match mode, but outperform it
in isomorphic mode. The only system to surpass
this baseline in non-isomorphic match mode is the
MELODI system; yet, it under-performs against the
same baseline in isomorphic match mode. No par-
ticipating team submitted a system which would out-
perform the na??ve baseline in both modes.
6 Conclusions
The conclusions we draw from the experience of or-
ganizing the task are mixed. Participation was rea-
sonable but not large, suggesting that NC paraphras-
ing remains a niche interest ? though we believe it
deserves more attention among the broader lexical
semantics community and hope that the availabil-
ity of our freeform paraphrase dataset will attract a
wider audience in the future.
142
We also observed a varied response from our an-
notators in terms of embracing their freedom to gen-
erate complex and rich paraphrases; there are many
possible reasons for this including laziness, time
pressure and the fact that short paraphrases are often
very appropriate paraphrases. The results obtained
by our participants were also modest, demonstrating
that compound paraphrasing is both a difficult task
and a novel one that has not yet been ?solved?.
Acknowledgments
This work has partially supported by a small but ef-
fective grant from Amazon; the credit allowed us
to hire sufficiently many Turkers ? thanks! And a
thank-you to our additional annotators Dave Carter,
Chris Fournier and Colette Joubarne for their com-
plete sets of paraphrases of the noun compounds in
the test data.
References
Timothy Baldwin and Takaaki Tanaka. 2004. Transla-
tion by machine of complex nominals: Getting it right.
Proc. ACL04 Workshop on Multiword Expressions: In-
tegrating Processing, Barcelona, Spain, 24-31.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
Proc. 22nd International Conference on Computa-
tional Linguistics (COLING-08), Manchester, UK, 81-
88.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2010. SemEval-2010 Task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. Proc. 5th International ACL Workshop on Se-
mantic Evaluation, Uppsala, Sweden, 39-44.
Seana Coulson. 2001. Semantic Leaps: Frame-Shifting
and Conceptual Blending in Meaning Construction.
Cambridge University Press, Cambridge, UK.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4): 810-842.
Roxana Girju. 2007. Improving the interpretation of
noun phrases with cross-linguistic information. Proc.
45th Annual Meeting of the Association of Computa-
tional Linguistics, Prague, Czech Republic, 568-575.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting
semantic relations in noun compounds via verb seman-
tics. Proc. ACL-06 Main Conference Poster Session,
Sydney, Australia, 491-498.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. Proc.
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), Prague, Czech Republic, 48-53.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. Dan Moldovan
and Roxana Girju, eds., HLT-NAACL 2004: Workshop
on Computational Lexical Semantics, Boston, MA,
USA, 60-67.
Preslav Nakov and Marti Hearst. 2008. Solving rela-
tional similarity problems using the Web as a corpus.
Proc. 46th Annual Meeting of the Association for Com-
putational Linguistics ACL-08, Columbus, OH, USA,
452-460.
Preslav Nakov. 2008a. Improved statistical machine
translation using monolingual paraphrases. Proc. 18th
European Conference on Artificial Intelligence ECAI-
08, Patras, Greece, 338-342.
Preslav Nakov. 2008b. Noun compound interpretation
using paraphrasing verbs: Feasibility study. Proc.
13th International Conference on Artificial Intelli-
gence: Methodology, Systems, Applications AIMSA-
08, Varna, Bulgaria, Lecture Notes in Computer Sci-
ence 5253, Springer, 103-117.
Diarmuid O? Se?aghdha. 2007. Designing and Evaluating
a Semantic Annotation Scheme for Compound Nouns.
In Proceedings of the 4th Corpus Linguistics Confer-
ence, Birmingham, UK.
Diarmuid O? Se?aghdha. 2008. Learning compound
noun semantics. Ph.D. thesis, Computer Laboratory,
University of Cambridge. Published as University
of Cambridge Computer Laboratory Technical Report
735.
Diarmuid O? Se?aghdha and Ann Copestake. 2009. Using
lexical and relational similarity to classify semantic re-
lations. Proc. 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
EACL-09, Athens, Greece, 621-629.
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proc. 22nd International Conference on Computa-
tional Linguistics (COLING-08), Manchester, UK.
Mary Ellen Ryder. 1994. Ordered Chaos: The Interpre-
tation of English Noun-Noun Compounds. University
of California Press, Berkeley, CA, USA.
Takaaki Tanaka and Tim Baldwin. 2003. Noun-noun
compound machine translation: A feasibility study
on shallow processing. Proc. ACL-2003 Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment, Sapporo, Japan, 17-24.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. Proc. 48th Annual Meeting of the As-
sociation for Computational Linguistics ACL-10, Up-
psala, Sweden, 678-687.
143
Proceedings of the Second Workshop on Metaphor in NLP, pages 52?60,
Baltimore, MD, USA, 26 June 2014. c?2014 Association for Computational Linguistics
A Service-Oriented Architecture for Metaphor Processing 
   Tony Veale School of Computer Science and Informatics University College Dublin Belfield, Dublin D4, Ireland. Tony.Veale@UCD.ie 
    
    Abstract Metaphor is much more than a pyrotech-nical flourish of language or a fascinating conceptual puzzle: it is a cognitive lever that allows speakers to leverage their knowledge of one domain to describe, re-frame and understand another. Though NLP researchers tend to view metaphor as a problem to be solved, metaphor is perhaps more fittingly seen as a solution to be used, that is, as an important tool in the support of creative thinking and the generation of diverse linguistic outputs. Since it pays to think of metaphor as a foundational cognitive service, one that can be exploited in a wide array of crea-tive computational tasks, we present here a view of metaphor as a public Web ser-vice that can be freely called on demand.  1 Introduction Metaphor is a knowledge-hungry phenomenon. Fortunately, much of the knowledge needed for the processing of metaphor is already implicit in the large body of metaphors that are active in a language community (e.g. Martin, 1990; Mason, 2004). For existing metaphors are themselves a valuable source of knowledge for the production of new metaphors, so much so that a system can mine the relevant knowledge from corpora of figurative text (see Veale, 2011; Shutova, 2010). Thus, though linguistic metaphors are most natu-rally viewed as the output of a language genera-tion process, and as the input to a language un-derstanding process, it is just as meaningful to view the conceptual metaphors that underpin the-se linguistic forms as an input to the generation process and an output of the understanding pro-cess. A rich source of existing linguistic meta-phors, such as a text corpus or a database of Web 
n-grams, can thus be viewed as an implicit source of the knowledge a system needs to gen-erate and understand novel linguistic metaphors. Of course, if one finds Web data to be a useful resource for metaphor, it also makes sense to think of the algorithms and tools for manipulat-ing this knowledge as Web services, online sys-tems that hide the complexity of metaphor pro-cessing yet which can be called upon to generate and understand linguistic metaphors on demand. Such metaphors can then, in turn, be exploited in higher-level linguistic outputs such as stories and poems by yet other, inter-operable Web services.  There are compelling reasons to see metaphor as a service rather than a problem. For one, many creative language tasks ? such as poetry, joke and story generation ? require the conceptual and linguistic divergence offered by metaphor. When metaphor is offered as a reusable Web service, such systems need not implement their own met-aphor solutions, and are instead freed to focus on providing their own unique competences. For another, even as a problem, metaphor is not yet a standardized problem in NLP, and so different researchers focus on diverse aspects of metaphor using a wide range of bespoke models and ap-proaches. But when these models are provided as public services, researchers are free to draw from a rich ecology of complementary solutions. New approaches to metaphor, and to broader problems of linguistic creativity, may then emerge as re-searchers and developers mix-and-match services to meet their own specific application needs.  A Service-Oriented Architecture, or SOA, is one in which solution logic is presented in the form of discoverable, modular and composable services that hide the complexity of their data and their inner workings (Erl, 2008). This paper advocates for a SOA treatment of metaphor in the form of open and reusable Web services. To this end, a number of metaphor Web services are 
52
presented, to both offer a practical demonstration of the merits of SOA and to kick-start further development of metaphor services by the field. After discussing related work in section 2, we thus present a series of publically-accessible metaphor services, for generating creative simi-les, for performing divergent categorization, for generating new affective metaphors from old, for generating metaphor-rich poetry, and for generat-ing metaphor-inspired character arcs for stories. 
2 Related Work and Ideas Metaphor has been studied within computer sci-ence for four decades, yet it remains largely at the periphery of NLP research. The reasons for this marginalization are pragmatic ones, since metaphors can be as challenging as human crea-tivity will allow. The greatest success has thus been achieved by focusing on conventional met-aphors (e.g., Martin, 1990; Mason, 2004), or on specific domains of usage, such as figurative de-scriptions of mental states (e.g., Barnden, 2006).  From the earliest computational forays, it has been recognized that metaphor is fundamentally a problem of knowledge representation. Seman-tic representations are, by and large, designed for well-behaved mappings of words to meanings ? what Hanks (2006) calls norms ? but metaphor requires a system of soft preferences rather than hard (and brittle) constraints. Wilks (1978) thus proposed a preference semantics approach, which Fass (1991,1997) extended into a collative semantics. In contrast, Way (1990) argued that metaphor requires a dynamic concept hierarchy that can stretch to meet the norm-bending de-mands of figurative ideation, though her ap-proach lacked specific computational substance.  More recently, some success has been ob-tained with statistical approaches that side-step the problems of knowledge representation, by working instead with implied or latent represen-tations that are derived from word distributions. Turney and Littman (2005) show how a statisti-cal model of relational similarity that is con-structed from Web texts can retrieve the correct answers for proportional analogies, of the kind used in SAT/GRE tests. No hand-coded knowledge is employed, yet Turney and Littman?s system achieves an average human grade on a set of 376 real SAT analogies.   Shutova (2010) annotates verbal metaphors in corpora (such as ?to stir excitement?, where ?stir? is used metaphorically) with the corre-sponding conceptual metaphors identified by 
Lakoff and Johnson (1980). Statistical clustering techniques are then used to generalize from the annotated exemplars, allowing the system to rec-ognize and retrieve other metaphors in the same vein (e.g. ?he swallowed his anger?). These clus-ters can also be analyzed to find literal para-phrases for a given metaphor (e.g. ?to provoke excitement? or ?suppress anger?). Shutova?s ap-proach is noteworthy for operating with Lakoff and Johnson?s inventory of conceptual meta-phors without using an explicit knowledge repre-sentation of the knowledge domains involved.   Hanks (2006) argues that metaphors exploit distributional norms: to understand a metaphor, one must first recognize the norm that is exploit-ed. Common norms in language are the preferred semantic arguments of verbs, as well as idioms, clich?s and other multi-word expressions. Veale and Hao (2007a) suggest that stereotypes are conceptual norms that are found in many figura-tive expressions, and note that stereotypes and similes enjoy a symbiotic relationship that has obvious computational advantages. Similes rely on stereotypes to illustrate the qualities ascribed to a topic, while stereotypes are often promulgat-ed via proverbial similes (Taylor, 1954). Veale and Hao (2007a) show how stereotypical knowledge can be acquired by harvesting ?Hearst? patterns (Hearst, 1992) of the form ?as P as C? (e.g. ?as smooth as silk?) from the Web. They go on to show in (2007b) how this body of stereotypes can be used in a Web-based model of metaphor generation and comprehension.  Veale (2011) employs stereotypes as the basis of the Creative Information Retrieval paradigm, by introducing a variety of non-literal-matching wildcards in the vein of Mihalcea (2002). In this paradigm, @Noun matches any adjective that denotes a stereotypical property of Noun (so e.g. @knife matches sharp, pointy, etc.) while @Adj matches any noun for which Adj is stereotypical (e.g. @sharp matches sword, laser, razor, etc.). In addition, ?Adj matches any property / behav-ior that co-occurs with, and reinforces, the prop-erty denoted by Adj in similes; thus, ?hot match-es humid, sultry and spicy. Likewise, ?Noun matches any noun that denotes a pragmatic neighbor of Noun, where two words are neigh-bors if corpora attest to the fact that they are of-ten clustered together as comparable ideas, as in ?lawyers and doctors? or ?pirates and thieves?. The knowledge needed for @ is obtained by har-vesting text from the Web, while that for ? is obtained by mining Google 3-grams for instances of the form ?Xs and Ys? (Brants and Franz 2006). 
53
Creative Information Retrieval (CIR) can be used as a platform for the design of many Web services that offer linguistic creativity on de-mand. By enabling the flexible retrieval of n-gram data for non-literal queries, CIR allows a wide variety of creative tasks to be reimagined as simple IR tasks (Veale 2013). In the next section we show how CIR facilitates the generation of creative similes from linguistic readymades.  3 The Jigsaw Bard Similes and stereotypes enjoy a mutually benefi-cial relationship. Stereotypes anchor our similes in familiar concepts with obvious features, while similes, for their part, further popularize these stereotypes and entrench them in a culture. Since the core of any good simile is an evocative stere-otype that embodies just the qualities we want to communicate (see Fishelov, 1992), simile gener-ation is essentially a problem of apt stereotype retrieval. However, we can also turn this view on its head by asking: what kinds of simile might be generated from a given stereotype, or a linguistic combination or two or more lexicalized stereo-types? For instance, were we to consider the many phrases in the Google n-grams that com-bine a lexicalized stereotype with an affective modifier (such as ?cold fish?), or that combine multiple stereotypes with shared qualities (such as ?chocolate espresso? (brown) or ?robot fish? (cold and emotionless)), we might imagine re-purposing these phrases as part of a novel simile such as ?as emotionless as a robot fish? or per-haps even ?as smooth as a chocolate martini?.  The n-grams encountered and re-purposed in this way are linguistic readymades, in much the same way that the everyday objects that catch an artist?s eye for their secondary aesthetic qualities become art when re-imagined as art (see Taylor, 2009). Readymades in art are a product of seren-dipity: an artist encounters an object ? perhaps a humble tool, or the discarded detritus of modern life ? and sees in it a desired quality that can be brought to the fore in the right setting. Using a computer, however, linguistic readymades can be harvested from a resource like the Google n-grams on a near-industrial scale. Using CIR, a query can be issued for all bigrams that combine a lexicalized stereotype with a modifier that ac-centuates one of the stereotype?s core qualities. Such a query might be ??@P @P? where P de-notes a property like cold or smooth; the CIR query ??@cold @cold? thus matches ?wet fish?. Likewise, a CIR query of the form ?@P @P? 
will retrieve all Google bigrams that juxtapose two lexicalized stereotypes for the same property P; thus, ?@cold @cold? retrieves ?January rain?, ?winter snow? and ?robot fish?. More elaborate queries will retrieve more elaborate n-grams, such as ?snow-covered grave? and ?bul-let-riddled corpse? (again for the property cold). The Jigsaw Bard is a creative Web service that exploits this notion of linguistic readymades to generate novel creative similes on demand. Of course, the Bard only appears to ?invent? similes on demand (for a given input property like cold). In fact, the Bard has already scanned all of the Google n-grams to index a great many potential readymades that may, for some future request, be re-purposed as a creative simile. In keeping with the principles of SOA, the Bard does as little processing in real time as possible. Thus, when called as a Web service, it reliably retrieves, with remarkable speed, scores of fascinating similes that have already been indexed for a property. The Jigsaw Bard service can be accessed online at: www.educatedinsolence.com/jigsaw/ 4 Thesaurus Rex Metaphor is both a viewfinder and an adjustable lens: it helps us to find distant objects that share surprising similarities, and it allows us to focus on shared qualities that are not always apparent in a more conventional setting. So while meta-phor exploits our sense of similarity to generate resonant yet surprising juxtapositions, it also di-rects our sense of similarity, to highlight shared qualities that might otherwise remain unnoticed.  One cannot have an eye for metaphor without also having a well-developed sense of similarity. Lexico-semantic resources like WordNet offer NLP researchers a comprehensive and widely-used basis for measuring the similarity of two words or lexical concepts (see Fellbaum, 1998). Yet WordNet offers a somewhat monochromatic view of conceptual structure: it is a convergent structure in which every lexical concept is put in its correct place according to conventional usage. Metaphor requires a more kaleidoscopic view of conceptual structure, in which the many diverse and unconventional ways that a word, object or idea may be used  can be brought into play. The best place to find this kind of divergence is not a carefully curated resource like WordNet, but the unfiltered clamor and eclecticism of the Web. One can see the many ways in a given lexical concept is viewed on the Web using a simple search query. The ?such? construction, as used in 
54
?expensive foods such as lobster and caviar?, tells us that lobster and caviar are seen by some as expensive foods. The more often this view is found on the Web, the more credibility it can be given. Yet rather than trawl the Web for all uses of the ?such? construction, it pays to be targeted and parsimonious in our searches. For instance, suppose a system already possesses the stereo-typical association that Champagne is expensive. A targeted query of the form ?expensive * such as * and Champagne? will now retrieve Web texts that indicate other, related expensive items, and an umbrella category in which to place them all. Google, for example, provides the snippets ?expensive wines such as French Burgundy and Champagne?, ?expensive products such as Cog-nac and Champagne? and ?expensive and exotic foodstuffs such as caviar, seafood, hares, game, wine and champagne? in response to this query.  Knowing that Champagne and caviar are ex-pensive items in the same category, a system can now look for the other categories they also share, and so the query ?expensive * such as caviar and Champagne? finds that they are also considered to be expensive delicacies on the Web. By start-ing from a small seed of stereotypical knowledge (e.g. that Champagne is expensive), a system can generate a large body of targeted Web queries to elaborate and expand this knowledge. As new qualities and nuanced categories are acquired, these too can feed into the targeted acquisition process to form a virtuous bootstrapping circle. As a result, a system that starts from a seed of 12,000 or so stereotypical associations will ac-quire over 1.5 million fine-grained categoriza-tions in just five cycles of bootstrapping. Thus, for instance, a system can view Champagne as more than just a food, as the Web snippet ?luxury goods such as diamonds and champagne? can attest. These many fine-grained, overlapping and competing perspectives ? when combined in a Web service for divergent categorization we call Thesaurus Rex ? provide the kaleidoscopic swirl of possibilities that WordNet is so lacking but which creative metaphors can do so much with. Ask WordNet what the lexicalized concepts War and Peace, or Life and Death, or Divorce and War have in common, and its answer cannot fail but to disappoint. WordNet simply does not possess the fine-grained category structure to suggest what features might be shared by these very different concepts, even if, ironically, it can be used to generate a meaningful-seeming nu-merical measure of similarity in each case. In contrast, the Thesaurus Rex Web service will 
return a wealth of informative commonalities in each case. For instance, Figure 1 below presents a phrase cloud of the nuanced categories that are shared by both War and Divorce. Note how each is categorized as a stressful event, an unexpected and dramatic event, a traumatic event and an emotional event (eagle-eyed readers will note that each is also an adverse economic event).  
 
 Figure 1. Shared categories for War and Divorce. Thesaurus Rex thus provides a valuable service to any system that wishes to take a divergent view of conceptual structure, whether for pur-poses of literal similarity assessment or for non-literal metaphoric reasoning. Rex can be used as a browsing tool by Web users in search of in-sights or apt comparisons ? for instance, one can go from Leadership to Creativity via the catego-ries soft skill, valuable skill or transferable skill ? or as a flexible similarity service that supports 3rd-party metaphor processing systems. It should be noted that while Rex relies on the Web for its divergent view of the world, it does not sacrifice quality for quantity. Veale & Li (2013) show that a combination of Thesaurus Rex and WordNet produces similarity scores for the standard Miller & Charles (1991) test-set that achieve a 0.93 cor-relation with human judgments. This is as good as the best machine-learning systems (which do not explain their ratings the way that Rex can) and far superior to any WordNet-only approach. The Thesaurus Rex service can be accessed here:  http://boundinanutshell.com/therex2 5 Metaphor Magnet In many ways, a metaphor resembles a query in information retrieval (IR). Metaphors, like que-ries, allow us to simultaneously express what we believe and to elicit further information that may bolster or refute our beliefs. Metaphors, like que-
55
ries, are often short and concise, and require un-packing and expansion to be properly understood and acted upon. An expanded IR query is con-sidered successful if it leads to the retrieval of a richer set of relevant information sources. Like-wise, an expanded metaphor can be considered successful if expansion produces a rich interpre-tation that is consonant with, and consistently adds to, our beliefs about a particular topic.   Of course, there are important differences between metaphors, which elicit information from other humans, and IR queries, which elicit information from search engines. For one, IR fails to discriminate literal from non-literal lan-guage (see Veale 2004, 2011), and reduces any metaphoric query to literal keywords and key-phrases that are matched near-identically to texts (see Salton, 1968; Van Rijsbergen 1979). Yet everyday language shows that metaphor is an ideal form for expressing our information needs. A query like ?Steve Jobs was a good leader?, say, can be viewed by a creative IR system as a request to consider all the ways in which leaders are typically good, and to then consider all the metaphors that can most appropriately be used to convey these viewpoints about Steve Jobs. IR techniques such as corpus-based query ex-pansion can thus be used to understand and gen-erate metaphors on demand, if IR staples like query expansion (see Vorhees, 1998; Navigli and Velardi, 2003) are made both affect-driven and metaphor-aware. Expansion in each case can be performed using a comprehensive database of affective stereotypes that indicate e.g. the stereo-typical properties of geniuses, gurus and tyrants.  Let us return to the example of Steve Jobs qua leader. Using the CIR query ?leader is a ?leader? a range of different kinds of leader can be re-trieved. For instance, the Google n-grams oblige with the 4-grams ?leader is a visionary?, ?leader is a tyrant?, ?leader is a terrorist?, ?leader is a master?, ?leader is a shepherd?, ?leader is a dic-tator?, ?leader is an expert?, ?leader is a teacher? and ?leader is a catalyst?. But which of these views is consonant with being a good leader? If one wanted to criticize Jobs? leadership of Apple, then the stereotypes tyrant, terrorist and dictator offer clearly negative perspectives. In contrast, the stereotypes visionary, shepherd, expert and teacher are all positive, while master and cata-lyst may each evoke both good and bad qualities.  The under-specified positive metaphor ?Steve Jobs was a good leader? can thus be expanded, via the Google n-grams, to generate the specific positive metaphors ?Steve Jobs was a visionary?, 
?Steve Jobs was a shepherd?, ?Steve Jobs was an expert? and ?Steve Jobs was a teacher?. Like-wise, the under-specified negative metaphor ?Steve Jobs was a bad leader? can be expanded to yield ?Steve Jobs was a tyrant?, ?Steve Jobs was a dictator? and ?Steve Jobs was a terrorist?. The stereotypical properties of the vehicle in each case ? such as tyrant or expert ? can then be projected onto the tenor, Steve Jobs qua leader. Which properties of the vehicle are most relevant to Steve Jobs as a leader? CIR is again used to rank properties by their relevance to leadership. For instance, the CIR query ?@tyrant leader? finds Google 2-grams where a property of tyrant is used to describe a leader ? such as ?cruel lead-er? and ?demanding leader? ? and allows a sys-tem to rank the properties of tyrant according to the frequencies of these corresponding 2-grams.  Metaphor Magnet is such a system. Deployed as a Web service that generates and expands af-fective metaphors on demand, Metaphor Magnet allows clients (human users or 3rd-party software systems) to enter single terms (such as leader), compound terms with an affective spin (such as good leader or +leader), or copula statements such as ?Steve Jobs is a +leader?. For each in-put, the service marries its extensive knowledge of lexicalized stereotypes to the grand scale of the Google n-grams, to meaningfully expand up-on what it has been given and to generate the most appropriate affective elaborations and in-terpretations it can muster. In each case, Meta-phor Magnet provides a rich property-level ex-planation of its outputs. So, for instance, if Steve Jobs were to be viewed as a master, the proper-ties skilled, enlightened, free and demanding are all highlighted as being most appropriate. The Metaphor Magnet service can be accessed here: http://boundinanutshell.com/metaphor-magnet-acl 
6 Metaphorize with Metaphor Eyes Metaphor Magnet offers a property-theoretic view of metaphor: since its model of the world is entirely property-based ? in which words denote stereotypes that map to highly salient properties ? it sees metaphor interpretation as a question of which properties are mapped from the vehicle to the tenor. Metaphor Magnet lacks a proposition-level view of the world, in which stereotypes are linked to other stereotypes by arbitrary relations. Thus, though it knows that scientists are logical and objective, it does not know, and cannot use, the generalizations that scientists work in labs, 
56
wear white coats, conduct experiments, write up their results, and so on. Another service, called Metaphor Eyes, remedies this deficiency by em-ploying a propositional model of the world that reasons with subject-relation-object triples rather than subject-attribute pairs. Metaphor Eyes ac-quires its world-model from a variety of sources (see Veale & Li, 2011), but the most fascinating of these sources is a niche Web-service offered (until recently) by the Google search-engine.  Many users of Web search-engines still enter full NL questions as search queries, even though most engines do not perform syntactic analysis. The Google engine maintains a record of fre-quently-posed queries and helpfully suggests apt completions for any familiar-seeming inputs. Google also provides a completions service (now sadly defunct) through which one may automati-cally retrieve the most common completions for any given query stub. The pairing of these obser-vations ? full NL questions plus the availability of common completions ? allows a computer to acquire a propositional model of the world by polling Google for completions to question stubs of the form ?Why do Xs ??. Why-do questions are remarkably revealing about the beliefs that we take for granted when speaking to others. The query ?Why do dogs bury bones? tells us more than the fact that some dogs bury bones; it tells us that the questioner presupposes this to also be a fact held by the addressees of the query, and so it is a stereotypical generalization over all dogs. By repeating polling Google for completions of the query ?Why do Xs?, where X is any concept the system wishes to flesh out, Metaphor Eyes acquires a large body of common-sense beliefs.  Metaphor Eyes retrieves apt vehicles for a given a tenor concept T using the simple CIR query ??T?. Thus, given philosopher as a tenor, Metaphor Eyes considers scholar, moralist, theo-logian, historian, scientist, visionary, explorer, thinker, sage, pundit, poet and even warrior as possible vehicles for a copula metaphor. For any given vehicle it then attempts to accommodate its knowledge of that vehicle into its representation of the tenor, by considering which propositions associated with the vehicle can be turned into apt propositions about the tenor. Consider the choice of explorer as a vehicle, producing the copula metaphor philosophers are explorers. Knowing that explorers perform wanderings, go on quests and seek knowledge, Metaphor Eyes looks for evidence in the Google n-grams that one or more of these propositions can just as well be said of philosophers. The 3-gram ?philosopher?s quest? 
attests to the aptness of the proposition ?philoso-phers go on quests?, while the 3-gram ?philoso-pher?s knowledge? attests to ?philosophers look for knowledge?. The 2-gram ?wandering philos-opher? additionally attests to the proposition that philosophers perform wanderings of their own. Metaphor Eyes views metaphor as a represen-tational lever, allowing it to fill the holes in its weak understanding of one concept by importing relevant knowledge from a neighboring concept. As such, in offering a partial solution to meta-phor as a problem, it simultaneously views meta-phor as a an acquisition solution in its own right. The Metaphor Eyes service can be accessed here:  http://boundinanutshell.com/metaphor-eye/ 7 Stereotrope Poetry Generation The copula form ?X is a Y? is metaphor at its simplest and its purest, which perhaps explains why the form is far more prevalent in the meta-phor literature than it is in real texts. Metaphor in the wild thrives in a wide variety of syntactic forms and rhetorical guises, with the most crea-tively rhetorical found in poetry. Yet while met-aphors are the stuff of poetry, a well-written po-em is much more than a bag of fancy metaphors. Coherent poems are driven by a coherent master metaphor, a schema that governs a poet?s choice of related metaphors to elaborate this core idea. A key benefit of the SOA philosophy is that services represent modular chunks of solution logic that need not, and do not, do everything for themselves. Ideally, our Web services should be reusable modules that can be composed, mashed-up and further elaborated by other developers to yield new services. In this spirit, Stereotrope is a service that generates poems from the metaphors produced by the Metaphor Magnet Web service.  Given a topic on which to wax poetically, Ste-reotrope calls on  Metaphor Magnet to suggest a master metaphor around which its poem might be organized. Suppose our topic is love, and that Metaphor Magnet responds with, among others, the trope Love is a Fire (this copula metaphor has a frequency of 331 in the Google n-grams). Choosing this familiar trope as the core of its poem, Stereotrope now asks Metaphor Magnet to produce elaborations of this metaphor. Meta-phor Magnet generates elaborations of Love is a Fire that include Love is a Shining Flame, Love is a Dazzling Explosion and Love is a Raging Cauldron. These elaborations ? once rendered in the typical rhetorical forms of poetry ? are then packaged by Stereotrope into a complete poem.  
57
A useful rhetorical device is the Superlative. For instance, Metaphor Magnet suggests that for Love is a Fire, the properties hot, bright and burning can all be sensibly projected from Fire onto Love (as attested by the Google n-grams). The explicit statement Love is a Fire lacks a cer-tain something in a poem, yet the same meaning can be suggested with the superlative forms ?No fire is hotter? or ?No fire is brighter?. By looking to attested combinations in the Google n-grams, Stereotrope notices that ?brightly? is an adverb that frequently modifies ?burning?, and so it also suggests the superlative ?No fire burns more brightly?. Yet by also noting that hot and bright are mutually reinforcing properties, since bright ? ?hot, it sees that the line ?No fire is hotter or burns more brightly? will squeeze all three pro-jected properties of Fire into a single superlative. Stereotrope also calls upon the Metaphor Eyes Web-service to provide a proposition-level un-derstanding of the world, for its poems must do more than allude to just the properties of entities. Unfortunately, banality is tacitly a pre-condition for the inclusion of almost any generalization in a common-sense knowledge-base. For it is pre-cisely because so many of us tacitly share these beliefs that they are so worthy of inclusion in a knowledge-base and so unworthy of mention in a poem that rises above the obvious. Yet with the right rhetorical packaging, even a boring general-ization can be pushed into the realm of the pro-vocative, allowing an automated poetry system to temporarily slip the surly bonds of reality.  Consider the generalization ?celebrities ride in limousines?. Though it may fail to provoke when baldly expressed in this form, Stereotrope notes that limousines have some interesting qualities. They are typically long, for one, and though it does not believe celebrities to be typically short, it notes from the Google n-grams that the 2-gram ?short celebrities? is also frequent enough to be an interesting talking point. Combining these two observations, it generates the rhetorical question ?Why do the shortest celebrities ride in the long-est limousines??. Though Stereotrope has no real insight into the frailty of celebrity egos, vertical-ly challenged or otherwise, it is attracted to the elegant opposition of long vs. short that can be injected into this otherwise banal generalization.  As a rule, Stereotrope attempts to shoehorn a provocative opposition into any proposition that is said to be topic-relevant by Metaphor Eyes. Thus, knowing that arrows are fired from bows, that bows are curved and that arrows are straight, it generates the rhetorical question ?Why do the 
most curved bows fire the straightest arrows??. The point is to suggest a more profound meaning beneath the surface. For when Don Corleone tells us that a fish rots from the head, he is not really talking about fish, but about how power corrupts an organization from the top down. Banal facts, when expressed in the right way, allude to a fig-urative meaning greater than themselves. By packaging its meagre facts in a rhetorical guise, Stereotrope can allude to a poetic meaning that lies outside its own power to comprehend. Stereotrope generates the following poem from the master metaphor Marriage is a Prison: The legalized regime of this marriage My marriage is an emotional prison Barred visitors do marriages allow The most unitary collective scarcely organizes so much Intimidate me with the official regulation of your prison Let your sexual degradation charm me Did ever an offender go to a more oppressive prison? You confine me as securely as any locked prison cell Does any prison punish more harshly than this marriage? You punish me with your harsh security The most isolated prisons inflict the most difficult hardships Marriage, you disgust me with your undesirable security Since the Stereotrope service complements the products of Metaphor Magnet (and Metaphor Eyes), it is engaged for each individual output of Metaphor Magnet directly. Thus, once again see: http://boundinanutshell.com/metaphor-magnet-acl 8 The Flux Capacitor The landmark television series Breaking Bad showcases story-telling at its most dramatic and its most transformational. It tells the tale of put-upon family man Walter White, a scientist with a brilliant mind who is trapped in the colorless life of a high-school chemistry teacher. When Walt is diagnosed with terminal lung cancer, he throws suburban caution to the wind and embraces a life of crime, first as a drug chemist of blue crystal meth and later as the ruthless drug baron Heisen-berg. Walt?s transformation, ?from Mister Chips to Scarface? (in the words of the show?s creator Vince Gilligan) is psychologically compelling because it is so unexpected yet so strongly rooted in our common-sense notions of similarity: for a drug chemist and a chemistry teacher share many of the same skills, while a drug baron embodies many of the same moral flaws as a drug chemist.  Literary transformations are often freighted with metaphorical meaning. Just think of the transformations of people into apt animals or 
58
plants in Ovid?s Metamorphoses, or of Gregor Samsa?s sudden, shame-driven transformation into a ?gigantic vermin? in Franz Kafka?s Meta-morphosis. In Breaking Bad, where Walt?s cen-tral transformation is slow-burning rather than magically immediate, a literal transformation is explained by the same kind of similarity judg-ments that motivate many of our metaphors. A service for producing apt metaphors, rooted in meaningful similarities, can thus be re-purposed to instead propose unexpected-but-apt character arcs for psychologically-compelling stories. The Flux Capacitor is a new Web-service-in-development that re-packages the outputs of the Metaphor Eyes and Metaphor Magnet services as literal character transformations for use in com-puter-generated stories. The Flux Capacitor is thus conceived as a middleware service whose outputs are intended as inputs to other services. It does not package its outputs as metaphors, and nor does it package them as finished stories: ra-ther, embracing the SOA philosophy of modular-ity and reuse, it produces Hollywood-style pitch-es that may underpin an interesting narrative that is to be fleshed out by another service or system. Walter White?s journey from chemistry teach-er to drug baron is made believable by similarity, but it is made stimulating by dissimilarity. Like the best metaphors, a thought-provoking charac-ter transformation marries states that are both similar and incongruously dissimilar. The Flux Capacitor thus ranks the metaphors it receives from other services by their ability to surprise: a character arc from A to B is all the more surpris-ing if our stereotype of A has properties that con-flict with those in our stereotype of B. So the Flux Capacitor suggests the transformation of a scientist into a priest, or of a nun into a prosti-tute, or a king into a slave, or a fool into a phi-losopher, to capitalize on the dramatic possibili-ties of the oppositions that emerge in each case. The property-level interpretations of a character arc are given by Metaphor Magnet, while propo-sition-level insights are given by Metaphor Eyes.  The Flux Capacitor uses a variety of other techniques to ensure the meaningfulness of its proposed character arcs. For instance, it uses se-mantic knowledge to ensure that no transfor-mation will change the gender of a character, and pragmatic knowledge to ensure that no transfor-mation will reverse the age of a character. The Flux Capacitor is at present still being tested, but will soon be deployed as its own public Web service, where it may find useful work as a pitcher of new ideas to story-generation systems. 
9 Out of the Mouths of Babes and Bots The services described in this paper all operate in pull mode, where figurative products are gener-ated on demand for the 3rd-party systems or users that ask for them. Each service produces HTML for human users and XML for automated queries. We conclude this paper then by discussing an alternative model that has been overlooked here: a push mode of operation in which services broadcast their outputs, hopeful but unsolicited, to users or systems that may find some serendipi-tous value in being surprised in this way. Twitter is the ideal midwife for pushing automated met-aphors into the world. For Twitter supports twit-terbots, automated systems (or bots) that gener-ate their own tweets, largely for the consumption and edification of human Twitter users. A new twitterbot named MetaphorIsMyBusiness (han-dle: @MetaphorMagnet) employs all of the ser-vices described in previous sections to generate a novel creative metaphor every hour, on the hour. @MetaphorMagnet?s  outputs are the product of a complex reasoning process that combines a comprehensive knowledge-base of stereotypical norms with real usage data from the Google n-grams. Though encouraged by the quality of its outputs, we continue to expand its expressive range, to give the twitterbot its own unique voice and identifiable aesthetic. Outputs such as ?What is an accountant but a timid visionary? What is a visionary but a bold accountant?? lend the bot a sardonic persona that we wish to develop further. We have seen the advantages to packaging metaphor systems as Web services, but there are also real advantages to packing metaphor Web-services as twitterbots. For one, the existence of mostly random bots that make no use of world knowledge or of metaphor theory ? such as the playfully subversive @metaphorminute bot ?  provides a competitive baseline against which to evaluate the meaningfulness and value of the insights that are pushed out into the world by theory-driven / knowledge-driven twitterbots like @MetaphorMagnet. For another, the willingness of human Twitter users to follow such accounts regardless of their provenance, and to retweet the best outputs from these accounts, provides an empirical framework for estimating (and promot-ing) the figurative quality of the back-end Web services in each case. Finally, such bots may reap some social value in their own right, as sources of occasional insight, wit or profundity, or even of useful metaphors that are subsequently valued, adopted, and re-worked by human speakers. 
59
References Barnden, J. A. (2006). Artificial Intelligence, figura-tive language and cognitive linguistics. In: G. Kris-tiansen, M. Achard, R. Dirven, and F. J. Ruiz de Mendoza Ibanez (Eds.), Cognitive Linguistics: Current Application and Future Perspectives, 431-459. Berlin: Mouton de Gruyter.  Brants, T. and Franz, A. (2006). Web 1T 5-gram Ver. 1. Linguistic Data Consortium. Erl, T. (2008). SOA: Principles of Service Design. Prentice Hall. Fass, D. (1991). Met*: a method for discriminating metonymy and metaphor by computer. Computa-tional Linguistics, 17(1):49-90. Fass, D. (1997). Processing Metonymy and Metaphor. Contemporary Studies in Cognitive Science & Technology. New York: Ablex. Fellbaum, C. (ed.) (1998). WordNet: An Electronic Lexical Database. MIT Press, Cambridge. Fishelov, D. (1992). Poetic and Non-Poetic Simile: Structure, Semantics, Rhetoric. Poetics Today, 14(1), 1-23. Hanks, P. (2006). Metaphoricity is gradable. In: Ana-tol Stefanowitsch and Stefan Th. Gries (Eds.), Corpus-Based Approaches to Metaphor and Me-tonymy,. 17-35. Berlin: Mouton de Gruyter. Hearst, M. (1992). Automatic acquisition of hypo-nyms from large text corpora. In Proc. of the 14th International Conference on Computational Lin-guistics, pp 539?545. Lakoff, G. and Johnson, M. (1980). Metaphors We Live By. University of Chicago Press. Martin, J. H. (1990). A Computational Model of Met-aphor Interpretation. New York: Academic Press. Mason, Z. J. (2004). CorMet: A Computational, Cor-pus-Based Conventional Metaphor Extraction Sys-tem, Computational Linguistics, 30(1):23-44. Mihalcea, R. (2002). The Semantic Wildcard.  In Proc. of the LREC Workshop on Creating and Us-ing Semantics for Information Retrieval and Filter-ing. Canary Islands, Spain, May 2002. Miller, G. A. and Charles, W. G. (1991). Contextual correlates of semantic similarity. Language and Cognitive Processes 6(1):1-28. Navigli, R. and Velardi, P. (2003). An Analysis of Ontology-based Query Expansion Strategies. In Proc. of the workshop on Adaptive Text Extraction and Mining (ATEM 2003), at ECML 2003, the 14th European Conf. on Machine Learning, 42?49. Salton, G. (1968). Automatic Information Organiza-tion and Retrieval. New York: McGraw-Hill. 
Shutova, E. (2010). Metaphor Identification Using Verb and Noun Clustering. In the Proc. of the 23rd  International Conference on Computational Lin-guistics, 1001-1010. Taylor, A. (1954). Proverbial Comparisons and Simi-les from California. Folklore Studies 3. Berkeley: University of California Press. Taylor, M. R. (2009). Marcel Duchamp: ?tant donn?s (Philadelphia Museum of Art). Yale University Press. Turney, P.D. and Littman, M.L. (2005). Corpus-based learning of analogies and semantic relations. Ma-chine Learning 60(1-3):251-278. Van Rijsbergen, C. J. (1979). Information Retrieval. Oxford: Butterworth-Heinemann. Veale, T. (2004). The Challenge of Creative Infor-mation Retrieval. Computational Linguistics and Intelligent Text Processing: Lecture Notes in Com-puter Science, Volume 2945/2004, 457-467. Veale, T. and Hao, Y. (2007a). Making Lexical On-tologies Functional and Context-Sensitive. In Proc. of the 46th Annual Meeting of the Assoc. of Compu-tational Linguistics.  Veale, T. and Hao, Y. (2007b). Comprehending and Generating Apt Metaphors: A Web-driven, Case-based Approach to Figurative Language. In Proc. of the 22nd AAAI Conf. on A.I. Vancouver, Canada. Veale, T. (2011). Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Lin-guistic Creativity. Proceedings of ACL?2011, the 49th Annual Meeting of the Association of Com-putational Linguistics. June 2011. Veale, T. and Li, G. (2011). Creative Introspection and Knowledge Acquisition: Learning about the world thru introspective questions and exploratory metaphors. In Proc. of the 25th AAAI Conf. of the Assoc. for Advancement of A.I., San Francisco. Veale, T. and Li, G. (2013). Creating Similarity: Lat-eral Thinking for Vertical Similarity Judgments. In Proceedings of ACL 2013, the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August 2013. Veale, T. (2013). A Service-Oriented Architecture for Computational Creativity. Journal of Computing Science and Engineering, 7(3):159-167. Voorhees, E. M. (1998). Using WordNet for text re-trieval. WordNet, An Electronic Lexical Database, 285?303. The MIT Press. Way, E. C. (1991). Knowledge Representation and Metaphor. Studies in Cognitive systems. Holland: Kluwer. Wilks, Y. (1978). Making Preferences More Active, Artificial Intelligence 11. 
60
