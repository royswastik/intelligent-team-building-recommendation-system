Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 49?52,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Archivus: A multimodal system for multimedia meeting browsing and
retrieval
Marita Ailomaa, Miroslav Melichar,
Martin Rajman
Artificial Intelligence Laboratory
?Ecole Polytechnique Fe?de?rale de Lausanne
CH-1015 Lausanne, Switzerland
marita.ailomaa@epfl.ch
Agnes Lisowska,
Susan Armstrong
ISSCO/TIM/ETI
University of Geneva
CH-1211 Geneva, Switzerland
agnes.lisowska@issco.unige.ch
Abstract
This paper presents Archivus, a multi-
modal language-enabled meeting brows-
ing and retrieval system. The prototype
is in an early stage of development, and
we are currently exploring the role of nat-
ural language for interacting in this rela-
tively unfamiliar and complex domain. We
briefly describe the design and implemen-
tation status of the system, and then focus
on how this system is used to elicit useful
data for supporting hypotheses about mul-
timodal interaction in the domain of meet-
ing retrieval and for developing NLP mod-
ules for this specific domain.
1 Introduction
In the past few years, there has been an increasing
interest in research on developing systems for effi-
cient recording of and access to multimedia meet-
ing data1. This work often results in videos of
meetings, transcripts, electronic copies of docu-
ments referenced, as well as annotations of various
kinds on this data. In order to exploit this work, a
user needs to have an interface that allows them to
retrieve and browse the multimedia meeting data
easily and efficiently.
In our work we have developed a multimodal
(voice, keyboard, mouse/pen) meeting browser,
Archivus, whose purpose is to allow users to ac-
cess multimedia meeting data in a way that is most
natural to them. We believe that since this is a new
domain of interaction, users can be encouraged to
1The IM2 project http://www.im2.ch, the AMI project
www.amiproject.org, The Meeting Room Project at Carnegie
Mellon University, http://www.is.cs.cmu.edu/mie, and rich
transcription of natural and impromptu meetings at ICSI,
Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html
try out and consistently use novel input modalities
such as voice, including more complex natural lan-
guage, and that in particular in this domain, such
multimodal interaction can help the user find in-
formation more efficiently.
When developing a language interface for an in-
teractive system in a new domain, the Wizard of
Oz (WOz) methodology (Dahlba?ck et al, 1993;
Salber and Coutaz, 1993) is a very useful tool.
The user interacts with what they believe to be a
fully automated system, when in fact another per-
son, a ?wizard? is simulating the missing or incom-
plete NLP modules, typically the speech recogni-
tion, natural language understanding and dialogue
management modules. The recorded experiments
provide valuable information for implementing or
fine-tuning these parts of the system.
However, the methodology is usually applied
to unimodal (voice-only or keyboard-only) sys-
tems, where the elicitation of language data is not
a problem since this is effectively the only type of
data resulting from the experiment. In our case, we
are developing a complex multimodal system. We
found that when the Wizard of Oz methodology
is extended to multimodal systems, the number of
variables that have to be considered and controlled
for in the experiment increases substantially. For
instance, if it is the case that within a single inter-
face any task that can be performed using natural
language can also be performed with other modal-
ities, for example a mouse, the user may prefer
to use the other ? more familiar ? modality for
a sizeable portion of the experiment. In order to
gather a useful amount of natural language data,
greater care has to be taken to design the system
in a way that encourages language use. But, if
the goal of the experiment is also to study what
modalities users find more useful in some situa-
49
Figure 1: The Archivus Interface
tions compared to others, language use must be
encouraged without being forced, and finding this
balance can be very hard to achieve in practice.
2 Design and implementation
The Archivus system has been designed to sat-
isfy realistic user needs based on a user require-
ment analysis (Lisowska, 2003), where subjects
were asked to formulate queries that would enable
them to find out ?what happened at a meeting?.
The design of the user interface is based on the
metaphor of a person interacting in an archive or
library (Lisowska et al, 2004).
Furthermore, Archivus is flexibly multimodal,
meaning that users can interact unimodally choos-
ing one of the available modalities exclusively,
or multimodally, using any combination of the
modalities. In order to encourage natural lan-
guage interaction, the system gives textual and vo-
cal feedback to the user. The Archivus Interface
is shown in Figure 1. A detailed description of all
of the components can be found in Lisowska et al
(2004).
Archivus was implemented within a software
framework for designing multimodal applications
with mixed-initiative dialogue models (Cenek et
al., 2005). Systems designed within this frame-
work handle interaction with the user through
a multimodal dialogue manager. The dialogue
manager receives user input from all modalities
(speech, typing and pointing) and provides mul-
timodal responses in the form of graphical, textual
and vocal feedback.
The dialogue manager contains only linguistic
knowledge and interaction algorithms. Domain
knowledge is stored in an SQL database and is ac-
cessed by the dialogue manager based on the con-
straints expressed by the user during interaction.
The above software framework provides sup-
port for remote simulation or supervision of
some of the application functionalities. This fea-
ture makes any application developed under this
methodology well suited for WOz experiments. In
the case of Archivus, pilot experiments strongly
suggested the use of two wizards ? one supervising
the user?s input (Input Wizard) and the other con-
trolling the natural language output of the system
(Output Wizard). Both wizards see the user?s in-
put, but their actions are sequential, with the Out-
put Wizard being constrained by the actions of the
Input Wizard.
The role of the Input Wizard is to assure that
the user?s input (in any modality combination)
is correctly conveyed to the system in the form
of sets of semantic pairs. A semantic pair (SP)
is a qualified piece of information that the dia-
50
logue system is able to understand. For exam-
ple, a system could understand semantic pairs such
as date:Monday or list:next. A user?s
utterance ?What questions did this guy ask in
the meeting yesterday?? combined with point-
ing on the screen at a person called ?Raymond?
could translate to dialogact:Question,
speaker:Raymond, day:Monday.
In the current version of Archivus, user clicks
are translated into semantic pairs automatically by
the system. Where written queries are concerned,
the wizard sometimes needs to correct automat-
ically generated pairs due to the currently low
performance of our natural language understand-
ing module. Finally since the speech recognition
engine has not been implemented yet, the user?s
speech is fully processed by a wizard. The Input
Wizard also assures that the fusion of pairs coming
from different modalities is done correctly.
The role of the Output Wizard is to monitor, and
if necessary change the default prompts that are
generated by the system. Changes are made for
example to smooth the dialogue flow, i.e. to bet-
ter explain the dialogue situation to the user or to
make the response more conversational. The wiz-
ard can select a prompt from a predefined list, or
type a new one during interaction.
All wizards? actions are logged and afterwards
used to help automate the correct behavior of the
system and to increase the overall performance.
3 Collecting natural language data
In order to obtain a sufficient amount of language
data from the WOz experiments, several means
have been used to determine what encourages
users to speak to the system. These include giving
users different types of documentation before the
experiment ? lists of possible voice commands, a
user manual, and step-by-step tutorials. We found
that the best solution was to give users a tutorial
in which they worked through an example using
voice alone or in combination with other modali-
ties, explaining in each step the consequences of
the user?s actions on the system. The drawback of
this approach is that the user may be biased by the
examples and continue to interact according to the
interaction patterns that are provided, rather than
developing their own patterns. These influences
need to be considered both in the data analysis,
and in how the tutorials are written and structured.
The actual experiment consists of two parts in
which the user gets a mixed set of short-answer
and true-false questions to solve using the system.
First they are only allowed to use a subset of the
available modalities, e.g. voice and pen, and then
the full set of modalities. By giving the users dif-
ferent subsets in the first part, we can compare if
the enforcement of certain modalities has an im-
pact on how they choose to use language when all
modalities are available.
On the backend, the wizards can also to some
extent have an active role in encouraging language
use. The Input Wizard is rather constrained in
terms of what semantic pairs he can produce, be-
cause he is committed to selecting from a set of
pairs that are extracted from the meeting data.
For example if ?Monday? is not a meeting date
in the database, the input is interpreted as having
?no match?, which generates the system prompt
?I don?t understand?. Here, the Output Wizard
can intervene by replacing that prompt by one that
more precisely specifies the nature of the problem.
The Output Wizard can also decide to replace
default prompts in situations when they are too
general in a given context. For instance, when
the user is browsing different sections of a meeting
book (cover page, table of contents, transcript and
referenced documents) the default prompt gives
general advice on how to access the different parts
of the book, but can be changed to suggest a spe-
cific section instead.
4 Analysis of elicited language data
The data collected with Archivus through WOz
experiments provide useful information in several
ways. One aspect is to see the complexity of the
language used by users ? for instance whether they
use more keywords, multi-word expressions or
full-sentence queries. This is important for choos-
ing the appropriate level of language processing,
for instance for the syntactic analysis. Another as-
pect is to see the types of actions performed us-
ing language. On one hand, users can manipulate
elements in the graphical interface by expressing
commands that are semantically equivalent with
pointing, e.g. ?next page?. On the other hand,
they can freely formulate queries relating to the
information they are looking for, e.g. ?Did they
decide to put a sofa in the lounge??. Commands
are interface specific rather than domain specific.
From the graphical interface the user can easily
predict what they can say and how the system will
51
Part 1 condition Pointing Language
Experiment set 1
voice only 91% 9%
voice+keyboard 88% 12%
keyboard+pointing 66% 34%
voice+keyb.+pointing 79% 21%
Experiment set 2
voice only 68% 32%
voice+pointing 62% 38%
keyboard+pointing 39% 61%
pointing 76% 24%
Table 1: Use of each modality in part 2.
respond. Queries depend on the domain and the
data, and are more problematic for the user be-
cause they cannot immediately see what types of
queries they can ask and what the coverage of
the data is. But, using queries can be very use-
ful, because it allows the user to express them-
selves in their own terms. An important goal of the
data analysis is to determine if the language inter-
face enables the user to interact more successfully
than if they are limited to pointing only. In addi-
tion, the way in which the users use language in
these two dimensions has important implications
for the dialogue strategy and for the implementa-
tion of the language processing modules, for in-
stance the speech recognition engine. A speech
recognizer can be very accurate when trained on a
small, fixed set of commands whereas it may per-
form poorly when faced with a wide variety of lan-
guage queries.
Thus far, we have performed 3 sets of pilot
WOz experiments with 40 participants. The pri-
mary aim was to improve and finetune the system
and the WOz environment as a preparation for the
data-collection experiments that we plan to do in
the future. In these experiments we compared how
frequently users used voice and keyboard in rela-
tion to pointing as we progressively changed fea-
tures in the system and the experimental setup to
encourage language use. The results between the
first and the third set of experiments can be seen
in table 1, grouped by the subset of modalities that
the users had in the first part of the experiment.
From the table we can see that changes made
between the different iterations of the system
achieved their goal ? by the third experiment set
we were managing to elicit larger amounts of nat-
ural language data. Moreover, we noticed that the
modality conditions that are available to the user
in the first part play a role in the amount of use of
language modalities in the second part.
5 Conclusions and future work
We believe that the work presented here (both the
system and the WOz environment and experimen-
tal protocol) has now reached a stable stage that
allows for the elicitation of sufficient amounts of
natural language and interaction data. The next
step will be to run a large-scale data collection.
The results from this collection should provide
enough information to allow us to develop and in-
tegrate fairly robust natural language processing
into the system. Ideally, some of the components
used in the software framework will be made pub-
licly available at the end of the project.
References
Pavel Cenek, Miroslav Melichar, and Martin Rajman.
2005. A Framework for Rapid Multimodal Appli-
cation Design. In Va?clav Matous?ek, Pavel Mautner,
and Toma?s? Pavelka, editors, Proceedings of the 8th
International Conference on Text, Speech and Dia-
logue (TSD 2005), volume 3658 of Lecture Notes
in Computer Science, pages 393?403, Karlovy Vary,
Czech Republic, September 12-15. Springer.
Nils Dahlba?ck, Arne Jo?nsson, and Lars Ahrenberg.
1993. Wizard of Oz Studies ? Why and How. In
Dianne Murray Wayne D. Gray, William Hefley, ed-
itor, International Workshop on Intelligent User In-
terfaces 1993, pages 193?200. ACM Press.
Agnes Lisowska, Martin Rajman, and Trung H. Bui.
2004. ARCHIVUS: A System for Accessing the
Content of Recorded Multimodal Meetings. In
In Procedings of the JOINT AMI/PASCAL/IM2/M4
Workshop on Multimodal Interaction and Related
Machine Learning Algorithms, Bourlard H. & Ben-
gio S., eds. (2004), LNCS, Springer-Verlag, Berlin.,
Martigny, Switzerland, June.
Agnes Lisowska. 2003. Multimodal interface design
for the multimodal meeting domain: Preliminary in-
dications from a query analysis study. Project re-
port IM2.MDM-11, University of Geneva, Geneva,
Switzerland, November.
Daniel Salber and Joe?lle Coutaz. 1993. Applying
the wizard of oz technique to the study of multi-
modal systems. In EWHCI ?93: Selected papers
from the Third International Conference on Human-
Computer Interaction, pages 219?230, London, UK.
Springer-Verlag.
52
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1008?1015,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
User Requirements Analysis for Meeting Information Retrieval  
Based on Query Elicitation 
Vincenzo Pallotta 
Department of Computer Science 
University of Fribourg 
Switzerland 
Vincenzo.Pallotta@unifr.ch 
Violeta Seretan 
Language Technology Laboratory  
University of Geneva 
Switzerland 
seretan@lettres.unige.ch 
Marita Ailomaa 
Artificial Intelligence Laboratory  
Ecole Polytechnique F?d?rale  
de Lausanne (EPFL), Switzerland 
Marita.Ailomaa@epfl.ch 
 
 
 
Abstract 
We present a user requirements study for 
Question Answering on meeting records 
that assesses the difficulty of users ques-
tions in terms of what type of knowledge is 
required in order to provide the correct an-
swer. We grounded our work on the em-
pirical analysis of elicited user queries. We 
found that the majority of elicited queries 
(around 60%) pertain to argumentative 
processes and outcomes. Our analysis also 
suggests that standard keyword-based In-
formation Retrieval can only deal success-
fully with less than 20% of the queries, and 
that it must be complemented with other 
types of metadata and inference. 
1 Introduction 
Meeting records constitute a particularly important 
and rich source of information. Meetings are a 
frequent and sustained activity, in which multi-
party dialogues take place that are goal-oriented 
and where participants perform a series of actions, 
usually aimed at reaching a common goal: they 
exchange information, raise issues, express 
opinions, make suggestions, propose solutions, 
provide arguments (pro or con), negotiate 
alternatives, and make decisions. As outcomes of 
the meeting, agreements on future action items are 
reached, tasks are assigned, conflicts are solved, 
etc. Meeting outcomes have a direct impact on the 
efficiency of organization and team performance, 
and the stored and indexed meeting records serve 
as reference for further processing (Post et al, 
2004). They can also be used in future meetings in 
order to facilitate the decision-making process by 
accessing relevant information from previous 
meetings (Cremers et al, 2005), or in order to 
make the discussion more focused (Conklin, 2006).  
Meetings constitute a substantial and important 
source of information that improves corporate or-
ganization and performance (Corrall, 1998; Ro-
mano and Nunamaker, 2001). Novel multimedia 
techniques have been dedicated to meeting record-
ing, structuring and content analysis according to 
the metadata schema, and finally, to accessing the 
analyzed content via browsing, querying or filter-
ing (Cremers et al, 2005; Tucker and Whittaker, 
2004). 
This paper focuses on debate meetings (Cugini 
et al, 1997) because of their particular richness in 
information concerning the decision-making proc-
ess. We consider that the meeting content can be 
organized on three levels: (i) factual level (what 
happens: events, timeline, actions, dynamics); (ii) 
thematic level (what is said: topics discussed and 
details); (iii) argumentative level (which/how com-
mon goals are reached).  
The information on the first two levels is ex-
plicit information that can be usually retrieved di-
rectly by searching the meeting records with ap-
propriate IR techniques (i.e., TF-IDF). The third 
level, on the contrary, contains more abstract and 
tacit information pertaining to how the explicit in-
formation contributes to the rationale of the meet-
ing, and it is not present as such in raw meeting 
data: whether or not the meeting goal was reached, 
what issues were debated, what proposals were 
made, what alternatives were discussed, what ar-
guments were brought, what decisions were made, 
what task were assigned, etc.  
The motivating scenario is the following: A user 
1008
needs information about a past meeting, either in 
quality of a participant who wants to recollect a 
discussion (since the memories of co-participants 
are often inconsistent, cf. Banerjee et al, 2005), or 
as a non-participant who missed that meeting. 
Instead of consulting the entire meeting-related 
information, which is usually heterogeneous and 
scaterred (audio-video recordings, notes, minutes, 
e-mails, handouts, etc.), the user asks natural 
language questions to a query engine which 
retrieves relevant information from the meeting 
records. 
In this paper we assess the users' interest in 
retrieving argumentative information from 
meetings and what kind of knowledge is required 
for answering users' queries. Section 2 reviews 
previous user requirements studies for the meeting 
domain. Section 3 describes our user requirements 
study based on the analysis of elicited user queries, 
presents its main findings, and discusses the 
implications of these findings for the design of 
meeting retrieval systems. Section 4 concludes the 
paper and outlines some directions for future work. 
2  Argumentative Information in Meeting 
Information Retrieval 
Depending on the meeting browser type1, different 
levels of meeting content become accessible for 
information retrieval. Audio and video browsers 
deal with factual and thematic information, while 
artifact browsers might also touch on deliberative 
information, as long as it is present, for instance, in 
the meeting minutes. In contrast, derived-data 
browsers aim to account for the argumentative in-
formation which is not explicitly present in the 
meeting content, but can be inferred from it. If 
minutes are likely to contain only the most salient 
deliberative facts, the derived-data browsers are 
much more useful, in that they offer access to the 
full meeting record, and thus to relevant details 
about the deliberative information sought. 
2 .1  Importance of Argumentative Structure  
As shown by Rosemberg and Silince (1999), track-
ing argumentative information from meeting dis-
                                                
1 (Tucker and Whittaker, 2004) identifies 4 types of meeting 
browsers: audio browsers, video browsers, artifacts browsers 
(that exploit meeting minutes or other meeting-related docu-
ments), and browsers that work with derived data (such as 
discourse and temporal structure information). 
cussions is of central importance for building pro-
ject memories since, in addition to the "strictly fac-
tual, technical information", these memories must 
also store relevant information about deci-
sion-making processes. In a business context, the 
information derived from meetings is useful for 
future business processes, as it can explain phe-
nomena and past decisions and can support future 
actions by mining and assessment (Pallotta et al, 
2004). The argumentative structure of meeting dis-
cussions, possibly visualized in form of argumen-
tation diagrams or maps, can be helpful in meeting 
browsing. To our knowledge, there are at least 
three meeting browsers that have adopted argu-
mentative structure: ARCHIVUS (Lisowska et al, 
2004b), ViCoDe (Marchand-Maillet and Bruno, 
2005), and the Twente-AMI JFerret browser 
(Rienks and Verbree, 2006).  
2 .2  Query Elicitation Studies  
The users' interest in argumentation dimension of 
meetings has been highlighted by a series of recent 
studies that attempted to elicit the potential user 
questions about meetings (Lisowska et al, 2004a; 
Benerjee at al., 2005; Cremers et al, 2005). 
The study of Lisowska et al (2004a), part of the 
IM2 research project2, was performed in a simu-
lated environment in which users were asked to 
imagine themselves in a particular role from a se-
ries of scenarios. The participants were both IM2 
members and non-IM2 members and produced 
about 300 retrospective queries on recorded meet-
ings. Although this study has been criticized by 
Post et al (2004), Cremers et al (2005), and Ban-
erjee et al (2005) for being biased, artificial, ob-
trusive, and not conforming to strong HCI method-
ologies for survey research, it shed light on poten-
tial queries and classified them in two broad cate-
gories, that seem to correspond to our argumenta-
tive/non-argumentative distinction (Lisowska et 
al., 2004a: 994): 
? ?elements related to the interaction among par-
ticipants: acceptance/rejection, agree-
ment/disagreement; proposal, argumentation 
(for and against); assertions, statements; deci-
sions; discussions, debates; reactions; ques-
tions; solutions?; 
                                                
2 http://www.im2.ch 
1009
?  ?concepts from the meeting domains: dates, 
times; documents; meeting index: current, pre-
vious, sets; participants; presentations, talks; 
projects; tasks, responsibilities; topics?.  
Unfortunately, the study does not provide precise 
information on the relative proportions of queries 
for the classification proposed, but simply suggests 
that overall more queries belong to the second 
category, while queries requiring understanding of 
the dialogue structure still comprise a sizeable 
proportion. 
The survey conducted by Banerjee et al (2005) 
concerned instead real, non-simulated interviews 
of busy professionals about actual situations, re-
lated either to meetings in which they previously 
participated, or to meetings they missed. More than 
half of the information sought by interviewees 
concerned, in both cases, the argumentative dimen-
sion of meetings. 
For non-missed meetings, 15 out of the 26 in-
stances (i.e., 57.7%) concerned argumentative as-
pects: what the decision was regarding a topic (7); 
what task someone was assigned (4); who made a 
particular decision (2); what was the participants' 
reaction to a particular topic (1); what the future 
plan is (1). The other instances (42.3%) relate to 
the thematic dimension, i.e., specifics of the dis-
cussion on a topic (11).  
As for missed meetings, the argumentative in-
stances were equally represented (18/36): decisions 
on a topic (7); what task was assigned to inter-
viewee (4); whether a particular decision was made 
(3); what decisions were made (2); reasons for a 
decision (1); reactions to a topic (1). The thematic 
questions concern topics discussed, announce-
ments made, and background of participants.  
The study also showed that the recovery of in-
formation from meeting recordings is significantly 
faster when discourse annotations are available, 
such as the distinction between discussion, presen-
tation, and briefing. 
Another unobtrusive user requirements study 
was performed by Cremers et al (2005) in a "semi-
natural setting" related to the design of a meeting 
browser. The top 5 search interests highlighted by 
the 60 survey participants were: decisions made, 
participants/speakers, topics, agenda items, and 
arguments for decision. Of these, the ones shown 
in italics are argumentative. In fact, the authors 
acknowledge the necessity to include some "func-
tional" categories as innovative search options. 
Interestingly, from the user interface evaluation 
presented in their paper, one can indirectly infer 
how salient the argumentative information is per-
ceived by users: the icons that the authors intended 
for emotions, i.e., for a emotion-based search facil-
ity, were actually interpreted by users as referring 
to people?s opinion: What is person X's opinion?  ? 
positive, negative, neutral. 
3  User Requirements Analysis 
The existing query elicitation experiments reported 
in Section 2 highlighted a series of question types 
that users typically would like to ask about meet-
ings. It also revealed that the information sought 
can be classified into two broad categories: argu-
mentative information (about the argumentative 
process and the outcome of debate meetings), and 
non-argumentative information (factual, i.e., about 
the meeting as a physical event, or thematic, i.e., 
about what has been said in terms of topics). 
The study we present in this section is aimed at 
assessing how difficult it is to answer the questions 
that users typically ask about a meeting. Our goal 
is to provide insights into:  
? how many queries can be answered using stan-
dard IR techniques on meeting artefacts only 
(e.g., minutes, written agenda, invitations); 
? how many queries can be answered with IR on 
meeting recordings; 
? what kind of additional information and infer-
ence is needed when IR does not apply or it is 
insufficient (e.g., information about the par-
ticipants and the meeting dynamics, external 
information about the meeting?s context such 
as the relation to a project, semantic interpreta-
tion of question terms and references, compu-
tation of durations, aggregation of results, etc). 
Assessing the level of difficulty of a query based 
on the two above-mentioned categories might not 
provide insightful results, because these would be 
too general, thus less interpretable. Also, the com-
plex queries requiring mixed information would 
escape observation because assigned to a too gen-
eral class. We therefore considered it necessary to 
perform a separate analysis of each query instance, 
as this provides not only detailed, but also trace-
able information. 
1010
3 .1  Data: Collecting User Queries 
Our analysis is based on a heterogeneous collec-
tion of queries for meeting data. In general, an un-
biased queries dataset is difficult to obtain, and the 
quality of a dataset can vary if the sample is made 
of too homogenous subjects (e.g., people belong-
ing to the same group as members of the same pro-
ject). In order to cope with this problem, our strat-
egy was to use three different datasets collected in 
different settings:  
? First, we considered the I M2 dataset  collected 
by Lisowska et al (2004a), the only set of user 
queries on meetings available to date. It com-
prises 270 questions (shortly described in Sec-
tion 2) annotated with a label showing whether 
or not the query was produced by an IM2-
member. These queries are introspective and 
not related to any particular recorded meeting. 
? Second, we cross-validated this dataset with a 
large corpus of 294 natural language state-
ments about existing meetings records. This 
dataset, called the B ET observations  (Wellner 
et al, 2005), was collected by subjects who 
were asked to watch several meeting record-
ings and to report what the meeting partici-
pants appeared to consider interesting. We use 
it as a ?validation? set for the IM2 queries: an 
IM2 query is considered as ?realistic? or ?em-
pirically grounded? if there is a BET observa-
tion that represents a possible answer to the 
query. For instance, the query Why was the 
proposal made by X not accepted?  matches the 
BET observation Denis eliminated Silence of 
the Lambs as it was too violent . 
? Finally, we collected a new set of ?real? queries 
by conducting a survey of user requirements 
on meeting querying in a natural business set-
ting. The survey involved 3 top managers from 
a company and produced 35 queries. We called 
this dataset Manager Survey Set  (MS-Set). 
The queries from the IM2-set (270 queries) and the 
MS-Set (35 queries) were analyzed by two differ-
ent teams of two judges. Each team discussed each 
query, and classified it along the two main dimen-
sions we are interested in: 
? query type : the type of meeting content to 
which the query pertains; 
? query difficulty : the type of information re-
quired to provide the answer. 
3 .2  Query Type Analysis 
Each query was assigned exactly one of the follow-
ing four possible categories (the one perceived as 
the most salient): 
1. factual: the query pertains to the factual meet-
ing content; 
2. thematic: the query pertains to the thematic 
meeting content; 
3.  process : the query pertains to the argumenta-
tive meeting content, more precisely to the ar-
gumentative process; 
4. outcome: the query pertains to the argumenta-
tive meeting content, more precisely to the 
outcome of the argumentative process. 
IM 2- s et 
(s iz e:2 70) 
MS-S et  
(s iz e: 3 5) Cate go ry 
Tea m 1  Tea m 2  Tea m 1  Tea m 2  
Fac tu al 24. 8 %  20. 0 %  20. 0 %  
The m atic 18. 5 %  45. 6 %  20. 0 %  11. 4 %  
Proc es s 30. 0 %  32. 6 %  22. 9 %  28. 6 %  
Outc o me 26. 7 %  21. 8 %  37. 1 %  40. 0 %  
Proc es s + O utc o me 56. 7 %  54. 4 %  60. 0 %  68. 6 %  
Table 1. Query classification according to the 
meeting content type. 
Results from this classification task for both query 
sets are reported in Table 1. In both sets, the 
information most sought was argumentative: about 
55% of the IM2-set queries are argumentative 
(process or outcome). This invalidates the initial 
estimation of Lisowska et al (2004a:994) that the 
non-argumentative queries prevail, and confirms 
the figures obtained in (Banerjee et al, 2005), ac-
cording to which 57.7% of the queries are argu-
mentative. In our real managers survey, we ob-
tained even higher percentages for the argumenta-
tive queries (60% or 68.6%, depending on the an-
notation team). The argumentative queries are fol-
lowed by factual and thematic ones in both query 
sets, with a slight advantage for factual queries. 
The inter-annotator agreement for this first clas-
sification is reported in Table 2. The proportion of 
queries on which annotators agree in classifying 
them as argumentative is significantly high. We 
only report here the agreement results for the indi-
vidual argumentative categories (Process, Out-
come) and both (Process & Outcome). There were 
213 queries (in IM2-set) and 30 queries (in MS-
1011
set) that were consistently annotated by the two 
teams on both categories. Within this set, a high 
percentage of queries were argumentative, that is, 
they were annotated as either Process or Outcome 
(label AA in the table). 
IM 2- s et (s iz e: 27 0) MS-s e t (s iz e: 3 5)  C ate go ry rati o k app a  rati o k app a  
Proc es s 84. 8 %  82. 9 %  88. 6 %  87. 8 %  
Outc o me 90. 7 %  89. 6 %  91. 4 %  90. 9 %  
Proc es s & 
Outc o me 78. 9 %  76. 2 %  85. 7 %  84. 8 %  
AA 11 7/2 13 =  54. 9 %   
19/ 30 =  
63. 3 %   
Table 2. Inter-annotator agreement for query-type 
classification. 
Furthermore, we provided a re-assessment of the 
proportion of argumentative queries with respect to 
query origin for the IM2-set (IM2 members vs. 
non-IM2 members): non-IM2 members issued 
30.8% of agreed argumentative queries, a propor-
tion that, while smaller compared to that of IM2 
members (69.2%), is still non-negligible. This con-
trasts with the opinion expressed in (Lisowska et 
al., 2004a) that argumentative queries are almost 
exclusively produced by IM2 members.  
Among the 90 agreed IM2 queries that were 
cross-validated with the BET-observation set, 
28.9% were argumentative. We also noted that the 
ratio of BET statements that contain argumentative 
information is quite high (66.9%). 
3 .3  Query Difficulty Analysis 
In order to assess the difficulty in answering a 
query, we used the following categories that the 
annotators could assign to each query, according to 
the type of information and techniques they judged 
necessary for answering it: 
1. Role of I R : states the role of standard3  Informa-
tion Retrieval (in combination with Topic Ex-
traction4) techniques in answering the query. 
Possible values:  
a. Irrelevant (IR techniques are not appli-
cable).  Example: What decisions have 
been made?  
                                                
3  By standard IR we mean techniques based on bag-of-word 
search and TF-IDF indexing. 
4 Topic extraction techniques are based on topic shift detec-
tion (Galley et al, 2003) and keyword extraction (van der Plas 
et al, 2004). 
b. successful (IR techniques are sufficient). 
Example: Was the budget approved?  
c. insufficient (IR techniques are necessary, 
but not sufficient alone since they re-
quire additional inference and informa-
tion, such as argumentative, cross-
meeting, external corporate/project 
knowledge). Example: Who rejected the 
proposal made by X on issue Y?  
2. Artefacts : information such as agenda, min-
utes of previous meetings, e-mails, invita-
tions and other documents related and avail-
able before the meeting. Example: Who was 
invited to the meeting?  
3.  Recordings : the meeting recordings (audio, 
visual, transcription). This is almost always 
true, except for queries where Artefacts or 
Metadata are sufficient, such as What was 
the agenda?,  Who was invited to the meet-
ing? ). 
4 .  Metadata : context knowledge kept in static 
metadata (e.g., speakers, place, time). Ex-
ample: Who were the participants at the 
meeting? 
5. Dialogue Acts & Adjacency Pairs : Example: 
What was John?s response to my comment 
on the last meeting?  
6. Argumentation : metadata (annotations) 
about the argumentative structure of the 
meeting content. Example: Did  everybody 
agree on the decisions, or were there differ-
ences of opinion?  
7.  Semantics : semantic interpretation of terms 
in the query and reference resolution, in-
cluding deictics (e.g., for how long, usually, 
systematically, criticisms; this, about me, I ). 
Example: What decisions got made easily ?  
The term requiring semantic interpretation is 
underlined.  
8. Inference : inference (deriving information 
that is implicit), calculation, and aggregation 
(e.g., for ?command? queries asking for lists 
of things ? participants, issues, proposals). 
Example: What would be required from me?  
1012
9. Multiple meetings : availability of multiple 
meeting records. Example: Who usually at-
tends the project meetings?  
10. External : related knowledge, not explicitly 
present in the meeting records (e.g., infor-
mation about the corporation or the projects 
related to the meeting). Example: Did some-
body talk about me or about my work?  
Results of annotation reported on the two query 
sets are synthesized in Table 3: IR is sufficient for 
answering 14.4% of the IM2 queries, and 20% of 
the MS-set queries. In 50% and 25.7% of the cases, 
respectively, it simply cannot be applied (irrele-
vant). Finally, IR alone is not enough in 35.6% of 
the queries from the IM2-set, and in 54.3% of the 
MS-set; it has to be complemented with other 
techniques.  
IM 2- s et MS-s e t 
IR is : all  
qu eri es AA 
all  
qu eri es AA 
Suff ic ie nt 39/ 27 0 =  14. 4 %  
1/1 17 =  
0.8 %  
7/3 5 =  
20. 0 %  
1/1 9 =  
5.3 %  
Irrel ev a nt 13 5/2 70 =  50. 0 %   
55/ 11 7 =  
47. 0 %  
9/3 5 =  
25. 7 %  
3/1 9 =  
15. 8 %  
Ins uf fic i ent  96/ 27 0 =  35. 6 %  
61/ 11 7 =  
52. 1 %  
19/ 35 =  
54. 3 %  
15/ 19 =  
78. 9 %  
Table 3.  The role of IR (and topic extraction) in 
answering users? queries. 
If we consider agreed argumentative queries 
(Section 3.2), IR is effective in an extremely low 
percentage of cases (0.8% for IM2-set and 5.3% 
for MS-Set). IR is insufficient in most of the cases 
(52.1% and 78.9%) and inapplicable in the rest of 
the cases (47% and 15.8%). Only one argumenta-
tive query from each set was judged as being an-
swerable with IR alone: What were the decisions to 
be made (open questions) regarding the topic t1? 
When is the NEX T M E E TIN G planned? (e.g. to 
follow up on action items) . 
Table 4 shows the number of queries in each set 
that require argumentative information in order to 
be answered, distributed according to the query 
types. As expected, no argumentation information 
is necessary for answering factual queries, but 
some thematic queries do need it, such as What 
was decided about topic T?  (24% in the IM2-set 
and 42.9% in the M.S.-set).  
Overall, the majority of queries in both sets re-
quire argumentation information in order to be an-
swered (56.3% from IM2 queries, and 65.7% from 
MS queries). 
IM 2- s et, An no ta tio n 1  MS-s e t, A nn ot ati on 1 
Cate go ry  tot al  Req.  arg. Rati o Tot al  
Req.  
arg. Rati o 
Fac tu al 67  0  0%  7  0  0%  
The m atic  50  12  24. 0 %  7  3  42. 9 %  
Proc es s 81  73  90. 1 %  8  7  87. 5 %  
Outc o me  72  67  93. 1 %  13  13  10 0%  
All 27 0  15 2  56. 3 %  35  23  65. 7 %  
Table 4. Queries requiring argumentative informa-
tion. 
We finally looked at what kind of information is 
needed in those cases where IR is perceived as in-
sufficient or irrelevant. Table 5 lists the most fre-
quent combinations of information types required 
for the IM2-set and the MS-set. 
3 .4  Summary of Findings 
The analysis of the annotations obtained for the 
305 queries (35 from the Manager Survey set, and 
270 from the IM2-set) revealed that: 
? The information most sought by users from 
meetings is argumentative (i.e., pertains to the 
argumentative process and its outcome). It 
constitutes more than half of the total queries, 
while factual and thematic information are 
similar in proportions (Table 1); 
? There was no significant difference in this re-
spect between the IM2-set and the MS-set 
(Table 1); 
? The decision as to whether a query is argumen-
tative or not is easy to draw, as suggested by 
the high inter-annotator agreement shown in 
Table 2; 
? Standard IR and topic extraction techniques 
are perceived as insufficient in answering most 
of the queries. Only less than 20% of the 
whole query set can be answered with IR, and 
almost no argumentative question (Table 3). 
? Argumentative information is needed in an-
swering the majority of the queries (Table 4); 
? When IR alone fails, the information types that 
are needed most are (in addition to recordings): 
Argumentation, Semantics, Inference, and 
Metadata (Table 5); see Section 3.3 for their 
description. 
 
1013
 IR a lo ne fa ils IM 2-s et 
Inf orm at io n ty p es IR i ns uff ic ie nt             96 c as es   3 5.6 % IR irr el ev an t         13 5 c as es    50 %  
Artef ac ts         x     
Rec ord in gs x x x x x x x x x x x   
Me ta- da ta   x  x   x  x  x x 
Dlg ac ts & A dj . p airs              
Argu m en tat io n x x x x x x x x x  x   
Se ma ntic s x x x x x   x x x x x  
Inf ere nc e x  x x   x x x x x x  
Mu lti pl e me et in gs    x        x  
Ex tern al              
                  Cas es 15 11 9 8 7 5 4 14 9 8 8 7 5 
                  Ra tio (% )  15. 6  11. 5  9.4  8.3  7.3  5.2  4.2  10. 4  6.7  5.9  5.9  5.2  3.7  
 
IR a lo ne fa ils MS-s e t 
Inf orm at io n ty p es IR i ns uff ic ie nt     19 c as es   5 4.3 %  IR irr el ev an t   9 c as es   54. 3 %  
Artef ac ts     x x 
Rec ord in gs x x x x   
Me ta- da ta     x x 
Dlg ac ts & A dj . p airs       
Argu m en tat io n x x x x   
Se ma ntic s x  x x x  
Inf ere nc e x x  x x  
Mu lti pl e me et in gs       
Ex tern al    x   
                  Cas es 6 4 2 2 2 2 
                  Ra tio (% )  31. 6 21 10. 5 10. 5 22. 2 22. 2 
Table 5. Some of the most frequent combinations of information required for answering the queries in the 
IM2-Set and in the MS-set when IR alone fails. 
3 .5  Discussion 
Searching relevant information through the re-
corded meeting dialogues poses important prob-
lems when using standard IR indexing techniques 
(Baeza-Yates and Ribeiro-Nieto, 2000), because 
users ask different types of queries for which a 
single retrieval strategy (e.g., keywords-based) is 
insufficient. This is the case when looking at an-
swers that require some sort of entailment, such as 
inferring that a proposal has been rejected when a 
meeting participant says Are you kidding? .  
Spoken-language information retrieval (Vinci-
arelli, 2004) and automatic dialogue-act extraction 
techniques (Stolke et al, 2000; Clark and Popescu-
Belis, 2004; Ang et al, 2005) have been applied to 
meeting recordings and produced good results un-
der the assumption that the user is interested in 
retrieving either topic-based or dialog act-based 
information. But this assumption is partially in-
validated by our user query elicitation analysis, 
which showed that such information is only sought 
in a relatively small fraction of the users? queries. 
A particular problem for these approaches is that 
the topic looked for is usually not a query itself 
( Was topic T mentioned?) , but just a parameter in 
more structured questions ( What was decided 
about T? ). Moreover, the relevant participants? 
contributions (dialog acts) need to be retrieved in 
combination, not in isolation (The reactions  to the 
proposal made by X ). 
4  Conclusion and Future Work 
While most of the research community has ne-
glected the importance of argumentative queries in 
meeting information retrieval, we provided evi-
dence that this type of queries is actually very 
common. We quantified the proportion of queries 
involving the argumentative dimension of the 
meeting content by performing an in-depth analy-
sis of queries collected in two different elicitation 
surveys. The analysis of the annotations obtained 
for the 305 queries (270 from the IM2-set, 35 from 
MS-set) was aimed at providing insights into dif-
ferent matters: what type of information is typi-
cally sought by users from meetings; how difficult 
it is, and what kind of information and techniques 
are needed in order to answer user queries.  
This work represents an initial step towards a 
better understanding of user queries on the meeting 
domain. It could provide useful intuitions about 
1014
how to perform the automatic classification of an-
swer types and, more importantly, the automatic 
extraction of argumentative features and their rela-
tions with other components of the query (e.g., 
topic, named entities, events). 
In the future, we intend to better ground our first 
empirical findings by i) running the queries against 
a real IR system with indexed meeting transcripts 
and evaluate the quality of the obtained answers; 
ii) ask judges to manually rank the difficulty of 
each query, and iii) compare the two rankings. We 
would also like to see how frequent argumentative 
queries are in other domains (such as TV talk 
shows or political debates) in order to generalize 
our results. 
Acknowledgements 
We wish to thank Martin Rajman and Hatem 
Ghorbel for their constant and valuable feedback. 
This work has been partially supported by the 
Swiss National Science Foundation NCCR IM2 
and by the SNSF grant no. 200021-116235. 
References 
Jeremy Ang, Yang Liu and Elizabeth Shriberg. 2005. 
Automatic Dialog Act Segmentation and Classification in 
Multiparty Meetings. In Proceedings of IE E E IC A S S P 
2 0 0 5 , Philadelphia, PA, USA. 
Ricardo Baeza-Yates and Berthier Ribeiro-Nieto. 2000. 
Modern Information Retrieval . Addison Wesley. 
Satanjeev Banerjee, Carolyn Rose and Alexander I. Rudnicky. 
2005. The Necessity of a Meeting Recording and Playback 
System, and the Benefit of Topic-Level Annotations to 
Meeting Browsing. In Proceedings of INT E R A C T 2 0 0 5 , 
Rome, Italy. 
Alexander Clark and Andrei Popescu-Belis. 2004. Multi-level 
Dialogue Act Tags. In Proceedings of SIG D IA L ' 0 4 , pages 
163?170. Cambridge, MA, USA. 
Jeff Conklin. 2006. Dialogue Mapping: Building Shared 
Understanding of Wicked Problems . John Wiley & Sons. 
Sheila Corrall. 1998. Knowledge management. Are we in the 
knowledge management business? A R IA D N E : the Web 
version,  18. 
Anita H.M Cremers, Bart Hilhorst and Arnold P.O.S 
Vermeeren. 2005. ?What was discussed by whom, how, 
when and where?" personalized browsing of annotated 
multimedia meeting recordings. In Proceedings of HC I 
2 0 0 5 , pages 1?10, Edinburgh, UK.  
John Cugini, Laurie Damianos, Lynette Hirschman, Robyn 
Kozierok, Jeff Kurtz, Sharon Laskowski and Jean Scholtz. 
1997. Methodology for evaluation of collaborative 
systems. Technical Report Rev. 3.0, The Evaluation 
Working Group of the DARPA Intelligent Collaboration 
and Visualization Program. 
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier and 
Hongyan Jing. 2003. Discourse Segmentation of Multi-
Party Conversation. In Proceedings of AC L 2 0 0 3 , pages 
562?569, Sapporo, Japan. 
Agnes Lisowska, Andrei Popescu-Belis and Susan Armstrong. 
2004a. User Query Analysis for the Specification and 
Evaluation of a Dialogue Processing and Retrieval System. 
In Proceedings LR E C 2 0 0 4 , pages 993?996, Lisbon, 
Portugal. 
Agnes Lisowska, Martin Rajman and Trung H. Bui. 2004b. 
ARCHIVUS: A System for Accesssing the Content of 
Recorded Multimodal Meetings. In Proceedings of ML M I 
2 0 0 4 , Martigny, Switzerland. 
St?phane Marchand-Maillet and Eric Bruno. 2005. Collection 
Guiding: A new framework for handling large multimedia 
collections. In Proceeding of AV IV DiLib05 , Cortona, Italy. 
Vincenzo Pallotta, Hatem Ghorbel, Afzal Ballim, Agnes 
Lisowska and St?phane Marchand-Maillet. 2004. Towards 
meeting information systems: Meeting knowledge 
management. In Proceedings of ICE IS 2 0 0 5 , pages 464?
469, Porto, Portugal. 
Lonneke van der Plaas, Vincenzo Pallotta, Martin Rajman and 
Hatem Ghorbel. 2004. Automatic keyword extraction from 
spoken text: A comparison between two lexical resources: 
the EDR and WordNet. In Proceedings of the LR E C 2 0 0 4 , 
pages 2205?2208, Lisbon, Portugal. 
Wilfried M. Post, Anita H.M. Cremers and Olivier Blanson 
Henkemans. 2004. A Research Environment for Meeting 
Behavior. In Proceedings of the 3rd Workshop on Social 
Intelligence Design,  pages 159?165, University of Twente, 
Enschede, The Netherlands. 
Rutger Rienks and Daan Verbree. 2006. About the Usefulness 
and Learnability of Argument?Diagrams from Real 
Discussions. In Proceedings of ML MI 2 0 0 6 , Washington 
DC, USA. 
Nicholas C. Romano Jr. and Jay F. Nunamaker Jr. 2001. 
Meeting Analysis: Findings from Research and Practice. In 
Proceedings of HIC S S-3 4 , Maui, HI, IEEE Computer 
Society. 
Duska Rosemberg and John A.A. Silince. 1999. Common 
ground in computer-supported collaborative argumentation. 
In Proceedings of the CL S C L 9 9 , Stanford, CA, USA. 
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth 
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, 
Rachel Martin, Carol Van Ess-Dykema and Marie Meteer. 
2000. Dialog Act Modeling for Automatic Tagging and 
Recognition of Conversational Speech. Computational 
Linguistics,  26(3):339?373.  
Simon Tucker and Steve Whittaker. 2004. Accessing 
multimodal meeting data: systems, problems and 
possibilities. In Proceedings of ML M I 2 0 0 4 , Martigny, 
Switzerland. 
Alessandro Vinciarelli. 2004. Noisy text categorization. In 
Proceedings of ICP R 2 0 0 4 , Cambridge, UK. 
Pierre Wellner, Mike Flynn, Simon Tucker, Steve Whittaker. 
2005. A Meeting Browser Evaluation Test. In Proceedings 
of CHI 2 0 0 5 , Portand, Oregon, USA. 
1015
