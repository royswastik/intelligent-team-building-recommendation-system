A Clustering Algorithm-for Chinese Adjectives and Nouns 1 
Yang Wen ~, Chunfa Yuan ~, Changning Huang 2
~State Key Laboratory of Intelligent Technology and System 
Deptartment ofComputer Science & Technology, Tsinghua University, 
Beijing 100084, P.R.C. 
2Microsoft Research, China 
Email: ycf@~1000e c~ t~in~hua edu cn 
Key Words: 
? bidirectional hierarchical clustering, 
collocations, minimum description length, 
collocational degree, revisional distance 
Abstract 
This paper proposes a bidirctional 
hierarchical clustering algorithm for 
simultaneously clustering words of different 
parts of speech based on collocations. The 
algorithm is composed of cycles of two 
kinds of alternate clustering processes. We 
construct an objective function based on 
Minimum Description Length. To. partly 
solve the problem caused by sparse data two 
concepts of collocational degree and 
revisional distance are presented. 
1 Introduction 
Recently research on the compositional 
frames (classification and collocafional 
relationship of words) for Chinese words has 
been described in Ji et al (1996)\[1\], Ji 
(1997)\[2\]. The objective of their work is to 
obtain the clusters of words of different parts 
of speech and to derive the coUocational 
relationship between different clusters from 
the collocafional relationship between words 
of different categories. 
There are two ways to construct the 
clusters: One is to get clusters from 
thesaurus classified manually by linguists. 
But the fact is that words with the same 
meanings do not always have the same 
ability of collocating with other words. The 
method isn't fit for the NLP problems under 
our consideration. Another way is to get 
clusters automatically by computing on the 
distribution environments of words based on 
statistical method. The distribution 
environment of a word is the set of words of 
other parts of speech that can be collocated 
with it. We employ the second method in our 
work. 
Previous research usually gets the 
clusters of Words of a certain part of speech 
based on their distribution environments. But 
Supported by National Natural Science Foundation of China (69'7?3031 ) and "973" Project (G1998030507). 
124 
we accept he assumption that the clustering 
processes of words of different parts of 
speech are inherently related. For example, 
having collocations between Chinese 
adjectives and nouns and if we take on nouns 
as entities and adjectives as features of 
nouns' distribution environments, we can 
obtain clusters of nouns and vice versa. The 
key of the relationship of the two clustering 
processes is that they use the same 
collocations. Therefore we consider the 
question of clustering the nouns and 
adjectives imultaneously. Li's work shows 
that they optimize the clustering results 
based on this viewpoint (Li et al, 1997)\[3\]. 
But they don't explain how to get initial 
clusters and their scale of problem is too 
small. 
In this paper, we propose an algorithm 
named bidirectional hierarchical c ustering to 
attempt answering the question. 
2 Concepts 
2.1 Problem Description 
Our problem can be described as follows: 
given the set of adjectives A, the set of nouns 
N and the collocation instances, our system 
will construct a partition P~ over N and a 
partition Pa over A that respectively contain 
sets of nouns and sets of adjectives. And 
both partitions meet he condition that words 
in the same set (called cluster) have similar 
semantic distribution environment. 
2.2 Partitions and Clusters 
Let S be a set, S~ c S(i = 1,2,...,n). If 
Ys ={ S~ } satisfies that 
n 
(1)US, =s 
i=1 
(2) S, NSj = O, Vi, j = 1,2,...n,i ~ j 
Then Ps is a partition over S. 
In this paper, we call A i ~Pa an 
"adjective cluster" and Ni ~P~v a "noun 
cluster". And we want to obtain the 
composition of partitions < PA, PN > as the 
clustering remit. 
2.3 Distance between Clusters 
In order to measure the distance between 
clusters of the same part of speech, we use 
the following equations: 
1 \[~'\["1~/I (1) disa(Ai,Aj) 
and 
lie, U%l (2) 
where O~ is the distribution 
environment of ~ and is make up of nouns 
which can be collocated with 
distribution environment 
composed of adjectives 
collocated with N i . ~ i  
A,. ~ is the 
of N~ and is 
which can be 
andW s follow 
similar definitions. This distance is a kind of 
Euchdean distance. 
125 
2.4 Colloeational Degree 
Since redundant collocations might be 
created during clustering, the concept 
"collocational'degree" is used to measure ~e 
collocational relationship between a cluster 
and its distribution environment. The 
coUocational degree is defined as the ratio of 
the existing collocation instances between 
the cluster and its distribution envffonment 
to all possible collocations generated by 
them. Thus, 
deg~ = I(a? I a 4,? ?,,a? c}l 
1411-,I (3) 
and 
degN~ - I ( -v  I- N,,v  ,nV, sC}l (4) 
IN, till 
where C is the set of all existing instances. 
2.5 Redundant Ratio 
After we get the collocational degree of 
a cluster, redundant ratio (marked as r) is 
calculated to measure the whole performance 
of the clustering result. We define the 
redundant ratio as 1 minus the ratio of all 
existing instances to all possible colloca- 
tions generated by all clusters (including 
nouns and adjectives) and their distribution 
environments. Sor is calculated as 
r : 1- 21cl 
EIA, II*,I+EIN, II'e,I (5) 
i i 
3 A Bidirectional Hierarchical 
Clustering Algorithm 
Usually a hierarchical clustering 
algorithm \[7\] constructs a clustering "tree" 
by combining small clusters into large ones 
or (lividing large clusters into small ones. 
The bidirectional hierarchical clustering 
algorithm proposed by us is composed of 
two kinds of alternate clustering processes. 
The algorithm flow is described as 
follows: 
1)Initially, regard every noun and 
adjective ach as a cluster. Calculate 
the distances between clusters of the 
same part of speech. 
2) Suppose without loss of generality 
that we choose to cluster nouns first. 
Select two noun clusters N, & N s 
of the minimum distance and 
integrate them into a new one N~'. 
J) Calculate the collocational degree of 
the new cluster. Adjust the sequence 
numbers of the original clusters and 
the relational information of adjective 
clusters. 
4) Calculate the distances between the 
new cluster and other clusters. 
5) Repeat from step 2) to 4) until the 
satisfaction of certain condition. For 
example, the number of the clusters 
haas decreased to certain amount. 2 
6) Similarly, we can follow the same 
steps from 2) to 5) for constructing 
adjective clusters, completing one 
cycle, of clustering processes of nouns 
and adjectives. 
7) Repeat from step 2) to 6) until the 
2 In this paper, we set he proportion is 20%. 
126 
objective function 3 reaches the 
minimum value. 
One advantage of this algorithm is that: 
when two clusters of nouns have similar 
distribution environments, "they might be 
classified into one cluster. This information 
can be delivered to the clusters of adjectives 
that respectively collocate with them by the 
clustering process of nouns. Thus these 
clusters of adjectives have great possibility 
to be combined into one cluster, while the 
ordinary hierarchical clustering algorithm 
can not do it. 
4 An Objective Function Based on 
MDL 
The objective function is designed to 
control the processes of clustering words 
based on the Minimum Description Length 
(MDL) principle. According to MDL, the 
best probability model for a given set of data 
is a model that uses the shortest code length 
for encoding the model itself and the given 
data relative to it \[4\] \[5\]. We regard the 
clusters as the model for the collocations of 
adjectives and nouns. The objective function 
is defined as the sum of the code length for 
the model ("model description length") and 
that for the data ("data description length"). 
When the clustering result minimises the 
objective function, the bidirectional 
processes hould be stopped and the result is 
the best probable one. The objective function 
based on MDL trade-offs between the 
simplicity of a model and its accuracy in 
fitting to the data, which are respectively 
quantified by the model description length 
and the data description length. 
3 Described later in section 4. 
The following are the formulas to 
calculate the objective function L: 
L = L,,,od + L,~ t (6) 
Lad is the model description length 
calculated as 
L,~ 
kA1 1 *~'1 1 
= -X-H--log2 -~-- ~.~--  log 2 k~r+l (7) 
i=1 ~A tt'A i=1 ~/q" 
= log2(kAk~) + 1 
Where k A and k N respectively denote 
the number of clusters of adjectives and 
nouns. "+1" means that the algorithm needs 
one bit to indicate whether the collocational 
relationship between the two clusters exists. 
L,~ t is composed of the data description 
length of adjectives and that of nouns, 
namely 
(8) 
And the two types of data description 
length are calculated as follows 
\[?,1 
LeQt (A) - - -  - -~- - - '~ log2 1 
.__. j=. 1411Nkl 
(9) 
Vj,~j ~ ~, and ~ j~N,  
L o,(N) 1 
,__, k k. 
(10) 
~~log  2 1 
Vj, ~ j ~.W, and v i ~ A~ 
5 Our Experiment 
We take the words and collocations 
127 
6 Discussions 
6.1 Rivisional Distance 
When we combine clusters into a new 
cluster, their distribution environments will 
be combined as well. The combination of 
clusters and their distribution environments 
might very likely generate redundant 
collocations that are not listed in the 
thesaurus. With the word clustering 
processes going on, there might be more and 
more redundant collocations. They will 
obviously affect the accuracy of the 
distances between clusters. When calculating 
the distances, the redundant collocations 
must be considered. So the question is how 
to revise the distance quation. Notice that 
the collocational degree defined in the above 
measures the collocational relationship 
gathered in Ni's Thesaurus \[6\] to test our 
algorithm. From Ni's thesaurus, we obtain 
2,569 adjectives, 4,536 nouns and 37,346 
collocations between adjectives and nouns. 
Table 1 shows results of using 5 
different revisional distance formulas 
discussed in the next section. Because the 
length of this paper is limited, we only give 
some examples (10 clusters for each part of 
speech) of clusters in section 8. We can see 
that the redundant ratio obviously decreases 
by using the revisional distance, and the 
result that has the lowest redundant ratio 
corresponds of the minimum value of the 
objective function. By human evaluation, 
most clusters contain the words that have 
similar meanings and distribution 
environments. So our algorithm proves to be 
effective for word clustering based on 
collocations. 
Table 1: Results of different revisional distances 
Revisional distance k~ k u L r 
Not used 409 550 20. 067 99. 01% 
dis' = -deg lnd is  397 610 20. 082 86. 96% 
dis' = d is /deg 383 595 20. 002 78. 78% 
dis' = d is /~-~ 373 586 20. 017 80. 39% 
dis" = -d!s ln  deg 395 557 20. 007 80. 08% 
However, the redundant ratio is still very 
large. The main cause is that existing 
between a cluster and its distribution 
environment. Obviously under the same 
instances are too sparse, covering only 
0.32% of all possible collocations. So 
another advantage of our algorithm is that 
we can acquire many new reasonable 
collocations not gathered in the thesaurus, ff
we add the new collocations into initial 
thesaurus and execute the algorithm on new 
data set, the performance will have great 
potential to improve. It is further work that 
can be carried out in the future. 
distance, clusters having higher coUocational 
degree have more higher similarity between 
each other (because they have more actual 
collocations) than those having lower 
collocational degree. So the collocational 
degree can be used to revise the distance 
equations. 
There are two problems that should-be 
considered when we design the revisional 
distance quations. The first one is to convert 
128 
the collocational degrees of two clusters into 
one collocational degree as the revisional 
factor for distance equations. It is the 
average collocational degree, marked as 
deg, calculated by 
deg A - 
and 
deg I ,I141 + deg A,I ,114 
ua>,l (II) 
deg N, Iv, llN, l + deg N, Iv, IN I 
In fact it is the collocational degree of 
the new cluster into which if we assume 
combining the two original clusters. 
The second problem is that the revjsonal 
distance quations hould keep coherent of 
monotonicity with the original distance. It 
means that under the same average collo- 
cational degree, the revional distance should 
keep the same (or opposite) monotonicity 
with the original distance, and under the 
same original distance, the revional distance 
should keep the same (or opposite) 
monotonicity with the average collocational 
degree. 
In this paper, four simple revisional 
distance equations are presented based on 
consideration of the upper two problems. 
They are: 
a)d is '=-deg lnd is  
dis b) dis' - 
deg 
? dis c) dis' - 
 /deg 
d) dis' = -d i s  In deg 
Where dis'  denotes the revional 
distance and dis denotes the original 
distance. 
From the comparison of the upper 
different results (shown in Table 1), we can 
draw the conclusion that using revisonal 
distance equations can increase the 
clustering accuracy remarkably. 
6.2 Determinant of Objective Function's 
Minimum Value 
The clustering algorithm terminates 
when the objective function is minimized. 
As a result it is very important to find out the 
function's minimum value. After analyzing 
the objective function, we find that it 
normally monotonically declines with 
clustering processes going on until it gets 
minimized. At the beginning, there are a 
large number of clusters with only one 
element in each of them. So the model 
description length is quite large while the 
data description length is quite small. 
Because the clustering process is hierarchical, 
every time when the combination occurs the 
number of clusters will decrease by one with 
the model description length's decreasing as 
well. At the same time the number of a 
certain cluster's elements will increase by 
one with the data description length's 
increment as well. However, the decrement 
is larger than the increment and it is getting 
smaller while the increment is getting larger. 
In this way, the objective function declines 
until the objective function reach its 
129 
Figure 1: Values of the Objective Functions 
addition, the clustering algorithm may help 
to find new collocations that are not in the 
thesaurus. This algodthm can also be 
extended to other collocation models, such 
as verb-noun collocations. 
27 
25 
23 
21 
19 
17 
minimum value. I f  we continue to execute 
the algorithm, we will see that the value of 
the objective function rises very fast like as 
is shown in Figure 1. 
15 
0 1000 2000 3000 4000 5000 6000 7000 8000 
quantity of c lusters 
Therefore we choose a fairly simple 
way to avoid the appearance of the local 
optimum: When there are two consecutive 
increases in the objective function during 
one clustering process, stop the process and 
start another one. When two consecutive 
clustering processes are stopped due to the 
same reason, we assume that we have got the 
minimum value and stop the whole 
clusterilag process. In our future work we 
will try to fred a better way to determine the 
minimum value of the objective function. 
7 Conclusion & Future Work 
In this paper we have presented a 
bidirctional hierarchical clustering algorithm 
of simultaneously clustering Chinese 
adjectives and nouns based on their 
collocations. Our preliminary experiments 
show that it can distinguish different words 
by their distribution environments. In 
Our future work includes: 
1) Because the sparsity of collocations 
is a main factor of affecting the 
word clustering accuracy, we can 
use the clustering results to discover 
new data and enrich the thesaurus. 
2) As there are yet no adjustments o 
the hierarchical clustering results, 
we are considering using some 
iterative algorithm, such as K- 
means algofithm~ to  optimise the 
clustering results. 
8 Attachment (Examples) 
We give 10 clusters of each part of 
speech clustered by our algorithm (using 
revisional distance formula b) as follows: 
8.1 Chinese Adjective dusters (10 of 383) 
130 
A3 ~ ~ ~f~ ~ ~ ~ 
A4 ~ ~ i~ ~ ~ ~ 
A5 ~ ~O ~ -F~ ~ ~ 
A~ ~ '~ ~, ~ ,~ ~ ~ 
~ ~'~ 
A9 ~ ~ ~.  ~ 
A~0 ~,~ E~ ~ ~ ~ ~ ~ 
8. 2 Chinese noun clusters (I0 of 595) 
N3 J~,~ .~_,i,E,)~, ~/~/~ ~IE~:::~T,/~ " f -~:~ 
N6 ~. :~T?:/~ - }t~:t.IL "~i:~ ~..'\]:~ - 1~ ~:~: 
N7 ~ ~,~ ~ ~ ~ ~ 
N8 ~'1~ ~ r~ ~l,~ ~ 'l~r~ 
N9 ~.,k, ~i~:~::. ~:--~: ~:~: 
Communications of COCIPS 6(1): P25- 
P33, 1996 
\[2\] Donghong Ji, Computational Research 
on Issues of Lexical Semantics, Post- 
doctoral Research Report, Tsinghua 
University, P14-P26, 1997 
\[3\] Juanzi Li et al, Two-dimensional 
Clustering Based on Compositional 
Examples, Language Engineering, 
P164-P169, Tsinghua University Press, 
1997 
\[4\] Hang Li & Naoki Abe, Clustering 
Words with the MDL Principle, cmp- 
lg/9605014 v2, 1996 
\[5\] Wei Xu, The Study of Syntax- 
Semantics Integrated Chinese Parsing, 
Thesis for the Degree of Master in 
Computer Science, Tsinghua University, 
1997 
\[6\] Wenjie Ni et al, Modem Chinese 
Thesaurus, China People Press,. 1984 
\[7\] Zhaoqi Bian et al, Pattern gecbgnition, 
Tsinghua University Press, 1997 
References 
\[1\] Donghong Ji & Changning Huang, A 
Semantic Composition Model for 
Chinese Noun and Adjective, 
131 
Exploiting Headword Dependency and Predictive Clustering for 
Language Modeling  
 
Jianfeng Gao  
Microsoft Research, Asia  
Beijing, 100080, China  
jfgao@microsoft.com 
Hisami Suzuki  
Microsoft Research 
 Redmond WA 98052, USA  
hisamis@microsoft.com 
Yang Wen* 
Department of Computer & 
Information Sciences of 
Tsinghua University, China 
  
                                                     
* This work was done while the author was visiting Microsoft Research Asia.  
Abstract 
This paper presents several practical ways 
of incorporating linguistic structure into 
language models. A headword detector is 
first applied to detect the headword of each 
phrase in a sentence. A permuted headword 
trigram model (PHTM) is then generated 
from the annotated corpus. Finally, PHTM 
is extended to a cluster PHTM (C-PHTM) 
by defining clusters for similar words in the 
corpus. We evaluated the proposed models 
on the realistic application of Japanese 
Kana-Kanji conversion. Experiments show 
that C-PHTM achieves 15% error rate 
reduction over the word trigram model. This 
demonstrates that the use of simple methods 
such as the headword trigram and predictive 
clustering can effectively capture long 
distance word dependency, and 
substantially outperform a word trigram 
model. 
1 Introduction 
In spite of its deficiencies, trigram-based language 
modeling still dominates the statistical language 
modeling community, and is widely applied to tasks 
such as speech recognition and Asian language text 
input (Jelinek, 1990; Gao et al, 2002).  
Word trigram models are deficient because they 
can only capture local dependency relations, taking 
no advantage of richer linguistic structure. Many 
proposals have been made that try to incorporate 
linguistic structure into language models (LMs), but 
little improvement has been achieved so far in 
realistic applications because (1) capturing longer 
distance word dependency leads to higher-order 
n-gram models, where the number of parameters is 
usually too large to estimate; (2) capturing deeper 
linguistic relations in a LM requires a large amount 
of annotated training corpus and a decoder that 
assigns linguistic structure, which are not always 
available. 
This paper presents several practical ways of 
incorporating long distance word dependency and 
linguistic structure into LMs. A headword detector 
is first applied to detect the headwords in each 
phrase in a sentence. A permuted headword trigram 
model (PHTM) is then generated from the 
annotated corpus. Finally, PHTM is extended to a 
cluster model (C-PHTM), which clusters similar 
words in the corpus.  
Our models are motivated by three assumptions 
about language: (1) Headwords depend on previous 
headwords, as well as immediately preceding 
words; (2) The order of headwords in a sentence can 
freely change in some cases; and (3) Word clusters 
help us make a more accurate estimate of the 
probability of word strings. We evaluated the 
proposed models on the realistic application of 
Japanese Kana-Kanji conversion, which converts 
phonetic Kana strings into proper Japanese 
orthography. Results show that C-PHTM achieves a 
15% error rate reduction over the word trigram 
model. This demonstrates that the use of simple 
methods can effectively capture long distance word 
dependency, and substantially outperform the word 
trigram model. Although the techniques in this 
paper are described in the context of Japanese 
Kana-Kanji conversion, we believe that they can be 
extended to other languages and applications. 
This paper is organized as follows. Sections 2 
and 3 describe the techniques of using headword 
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 248-256.
                         Proceedings of the Conference on Empirical Methods in Natural
dependency and clustering for language modeling. 
Section 4 reviews related work. Section 5 
introduces the evaluation methodology, and Section 
6 presents the results of our main experiments. 
Section 7 concludes our discussion.  
2 Using Headwords 
2.1 Motivation 
Japanese linguists have traditionally distinguished 
two types of words1, content words (jiritsugo) and 
function words (fuzokugo), along with the notion of 
the bunsetsu (phrase). Each bunsetsu typically 
consists of one content word, called a headword in 
this paper, and several function words. Figure 1 
shows a Japanese example sentence and its English 
translation2.  
[??+?][??+??][??+??][??+?][??+?][??+?]
[chiryou+ni]   [sennen+shite]       [zenkai+made] 
[treatment+to][concentration+do][full-recovery+until] 
[juubun+na]      [ryouyou+ni]  [tsutome+ru] 
[enough+ADN] [rest+for]        [try+PRES] 
'(One) concentrates on the treatment and tries to rest 
enough until full recovery' 
Figure 1. A Japanese example sentence with 
bunsetsu and headword tags 
In Figure 1, we find that some headwords in the 
sentence are expected to have a stronger 
dependency relation with their preceding 
headwords than with their immediately preceding 
function words. For example, the three headwords 
??~??~??  (chiryou 'treatment' ~ sennen 
'concentrate' ~ zenkai 'full recovery') form a trigram 
with very strong semantic dependency. Therefore, 
we can hypothesize (in the trigram context) that 
headwords may be conditioned not only by the two 
immediately preceding words, but also by two 
previous headwords. This is our first assumption.  
We also note that the order of headwords in a 
sentence is flexible in some sense. From the 
                                                     
1 Or more correctly, morphemes. Strictly speaking, the 
LMs discussed in this paper are morpheme-based models 
rather than word-based, but we will not make this 
distinction in this paper.  
2 Square brackets demarcate the bunsetsu boundary, and 
+ the morpheme boundary; the underlined words are the 
headwords. ADN indicates an adnominal marker, and 
PRES indicates a present tense marker.  
example in Figure 1, we find that if ??~??~?? 
(chiryou 'treatment' ~ sennen 'concentrate' ~ zenkai 
'full recovery') is a meaningful trigram, then its 
permutations (such as ??~??~?? (zenkai 'full 
recovery' ~ chiryou 'treatment' ~ sennen 
'concentrate')) should also be meaningful, because 
headword trigrams tend to capture an order-neutral 
semantic dependency. This reflects a characteristic 
of Japanese, in which arguments and modifiers of a 
predicate can freely change their word order, a 
phenomenon known as "scrambling" in linguistic 
literature. We can then introduce our second 
assumption: headwords in a trigram are permutable. 
Note that the permutation of headwords should be 
useful more generally beyond Japanese: for 
example, in English, the book Mary bought and 
Mary bought a book can be captured by the same 
headword trigram (Mary ~ bought ~ book) if we 
allow such permutations. 
In this subsection, we have stated two 
assumptions about the structure of Japanese that can 
be exploited for language modeling. We now turn to 
discuss how to incorporate these assumptions in 
language modeling.  
2.2 Permuted headword trigram model 
(PHTM) 
A trigram model predicts the next word wi by 
estimating the conditional probability P(wi|wi-2wi-1), 
assuming that the next word depends only on two 
preceding words, wi-2 and wi-1. The PHTM is a 
simple extension of the trigram model that 
incorporates the dependencies between headwords. 
If we assume that each word token can uniquely be 
classified as a headword or a function word, the 
PHTM can be considered as a cluster-based 
language model with two clusters, headword H and 
function word F. We can then define the conditional 
probability of wi based on its history as the product 
of the two factors: the probability of the category (H 
or F), and the probability of wi given its category. 
Let hi or fi be the actual headword or function word 
in a sentence, and let Hi or Fi be the category of the 
word wi. The PHTM can then be formulated as: 
=? ? ))...(|( 11 ii wwwP   (1) 
))...(|())...(|( 1111 iiiii HwwwPwwHP ?? ???   
))...(|())...(|( 1111 iiiii FwwwPwwFP ?? ???+   
where ? is a function that maps the word history 
(w1?wi-1) onto equivalence classes. 
P(Hi|?(w1?wi-1)) and P(Fi|?(w1?wi-1)) are 
category probabilities, and P(wi|?(w1?wi-1)Fi) is 
the word probability given that the category of wi is 
function word. For these three probabilities, we 
used the standard trigram estimate (i.e., ?(w1?wi-1) 
= (wi-2wi-1)). The estimation of headword 
probability is slightly more elaborate, reflecting the 
two assumptions described in Section 2.1:  
)|(())...(|( 122111 iiiiiii HhhwPHwwwP ??? =? ??   (2) 
))|()1( 212 iiii HhhwP ???+ ?   
)|()1( 121 iiii HwwwP ???+ ? .  
This estimate is an interpolated probability of three 
probabilities: P(wi|hi-2hi-1Hi) and P(wi|hi-1hi-2Hi), 
which are the headword trigram probability with or 
without permutation, and P(wi|wi-2wi-1Hi), which is 
the probability of wi given that it is a headword, 
where hi-1 and hi-2 denote the two preceding 
headwords, and ?1, ?2 ? [0,1] are the interpolation 
weights optimized on held-out data. 
The use of ?1 in Equation (2) is motivated by the 
first assumption described in Section 2.1: 
headwords are conditioned not only on two 
immediately preceding words, but also on two 
previous headwords. In practice, we estimated the 
headword probability by interpolating the 
conditional probability based on two previous 
headwords P(wi|hi-2hi-1Hi) (and P(wi|hi-1hi-2Hi) with 
permutation), and the conditional probability based 
on two preceding words P(wi|wi-2wi-1Hi). If ?1 is 
around zero, it indicates that this assumption does 
not hold in real data. Note that we did not estimate 
the conditional probability P(wi|wi-2wi-1hi-2hi-1Hi) 
directly, because this is in the form of a 5-gram, 
where the number of parameters are too large to 
estimate. 
The use of ?2 in Equation (2) comes from the 
second assumption in Section 2.1: headword 
trigrams are permutable. This assumption can be 
formulated as a co-occurrence model for headword 
prediction: that is, the probability of a headword is 
determined by the occurrence of other headwords 
within a window. However, in our experiments, we 
instead used an interpolated probability 
?2?P(wi|hi-2hi-1Hi) + (1??2)?P(wi|hi-1hi-2Hi) for two 
reasons. First, co-occurrence models do not predict 
words from left to right, and are thus very difficult 
to interpolate with trigram models for decoding. 
Second, if we see n-gram models as one extreme 
that predicts the next word based on a strictly 
ordered word sequence, co-occurrence models go to 
the other extreme of predicting the next word based 
on a bag of previous words without taking word 
order into account at all. We prefer models that lie 
somewhere between the two extremes, and consider 
word order in a more flexible way. In PHTM of 
Equation (2), ?2 represents the impact of word order 
on headword prediction. When ?2 = 1 (i.e., the 
resulting model is a non-permuted headword 
trigram model, referred to as HTM), it indicates that 
the second assumption does not hold in real data. 
When ?2 is around 0.5, it indicates that a headword 
bag model is sufficient. 
2.3 Model parameter estimation  
Assume that all conditional probabilities in 
Equation (1) are estimated using maximum 
likelihood estimation (MLE). Then 
)|( 12 ?? iii wwwP =  
)|()|( 1212 iiiiiii HwwwPwwHP ???? , wi: headword  
??
?
?
??
?
?
?
 
)|()|( 1212 iiiiiii FwwwPwwFP ???? , wi: function word 
is a strict equality when each word token is uniquely 
classified as a headword or a function word. This 
can be trivially proven as follows. Let Ci represent 
the category of wi (Hi or Fi in our case). We have 
)|()|( 1212 iiiiiii CwwwPwwCP ???? ?   
)(
)(
)(
)(
12
12
12
2
iii
iiii
ii
iiii
CwwP
wCwwP
wwP
CwwP
??
??
??
?? ?=   
)(
)(
12
12
??
??=
ii
iiii
wwP
wCwwP  (3) 
Since each word is uniquely assigned to a category, 
P(Ci|wi)=1, and thus it follows that 
)|()()( 121212 iiiiiiiiiii wwwCPwwwPwCwwP ?????? ?=  
)|()( 12 iiiii wCPwwwP ?= ??  
)( 12 iii wwwP ??= . (4) 
Substituting Equation (4) into Equation (3), we get 
)|()|( 1212 iiiiiii CwwwPwwCP ???? ?  
)|()(
)(
12
12
12
??
??
?? == iii
ii
iii wwwPwwP
wwwP . (5) 
Now, by separating the estimates of probabilities of 
headwords and function words, Equation (1) can be 
rewritten as: 
P(wi|?(w1?wi-1))= (6) 
)|()(|(( 122121 ???? iiiiii hhwPwwHP ??
))|()1( 212 ???+ iii hhwP?
)|()1( 121 ???+ iii wwwP?  
 
wi: headword  
)|( 12 ?? iii wwwP   ??
?
?
??
?
?
?
 
wi: function word  
There are three probabilities to be estimated in 
Equation (6): word trigram probability 
P(wi|wi-2wi-1), headword trigram probability 
P(wi|hi-2hi-1) and P(wi|hi-1hi-2) (where wi is a 
headword), and category probability P(Hi|wi-2wi-1). 
In order to deal with the data sparseness problem 
of MLE, we used a backoff scheme (Katz, 1987) for 
the parameter estimation. The backoff scheme 
recursively estimates the probability of an unseen 
n-gram by utilizing (n?1)-gram estimates. To keep 
the model size manageable, we also removed all 
n-grams with frequency less than 2.  
In order to classify a word uniquely as H or F, 
we needed a mapping table where each word in the 
lexicon corresponds to a category. The table was 
generated in the following manner. We first 
assumed that the mapping from part-of-speech 
(POS) to word category is fixed. The tag set we 
used included 1,187 POS tags, of which 102 count 
as headwords in our experiments. We then used a 
POS-tagger to generate a POS-tagged corpus, from 
which we generated the mapping table3. If a word 
could be mapped to both H and F, we chose the 
more frequent category in the corpus. Using this 
mapping table, we achieved a 98.5% accuracy of 
headword detection on the test data we used. 
Through our experiments, we found that 
P(Hi|wi-2wi-1) is a poor estimator of category 
probability; in fact, the unigram estimate P(Hi) 
achieved better results in our experiments as shown 
in Section 6.1. Therefore, we also used the unigram 
estimate for word category probability in our 
                                                     
3 Since the POS-tagger does not identify phrases, our 
implementation does not identify precisely one 
headword for a phrase, but identify multiple headwords 
in the case of compounds.  
experiments. The alternative model that uses the 
unigram estimate is given below:  
 
P(wi|?(w1?wi-1))= (7) 
)|()((( 1221 ?? iiii hhwPHP ??
))|()1( 212 ???+ iii hhwP?
)|()1( 121 ???+ iii wwwP?  
 
wi: headword  
)|( 12 ?? iii wwwP   ??
?
?
??
?
?
?
wi: function word  
We will denote the models using trigram for 
category probability estimation of Equation (6) as 
T-PHTM, and the models using unigram for 
category probability estimation of Equation (7) as 
U-PHTM. 
3 Using Clusters 
3.1 Principle 
Clustering techniques attempt to make use of 
similarities between words to produce a better 
estimate of the probability of word strings 
(Goodman, 2001).  
We have mentioned in Section 2.2 that the 
headword trigram model can be thought of as a 
cluster-based model with two clusters, the 
headword and the function word. In this section, we 
describe a method of clustering automatically 
similar words and headwords. We followed the 
techniques described in Goodman (2001) and Gao 
et al (2001), and performed experiments using 
predictive clustering along with headword trigram 
models.  
3.2 Predictive clustering model 
Consider a trigram probability P(w3|w1w2), where 
w3 is the word to be predicted, called the predicted 
word, and w1 and w2 are context words used to 
predict w3, called the conditional words. Gao et al 
(2001) presents a thorough comparative study on 
various clustering models for Asian languages, 
concluding that a model that uses clusters for 
predicted words, called the predictive clustering 
model, performed the best in most cases.  
Let iw  be the cluster which word wi belongs to. 
In this study, we performed word clustering for 
words and headwords separately. As a result, we 
have the following two predictive clustering models, 
(8) for words and (9) for headwords:  
)|()|()|( 121212 iiiiiiiiii wwwwPwwwPwwwP ?????? ?=  (8) 
)|()|()|( 121212 iiiiiiiiii whhwPhhwPhhwP ?????? ?=   
wi: headword 
(9) 
Substituting Equations (8) and (9) into Equation (7), 
we get the cluster-based PHTM of Equation (10), 
referred to as C-PHTM. 
 
P(wi|?(w1?wi-1))= (10) 
)|()|()((( 121221 iiiiiiii whhwPhhwPHP ???? ???
))|()|()1( 21212 iiiiiii whhwPhhwP ???? ??+ ?  
)|()|()1( 12121 iiiiiii wwwwPwwwP ???? ??+ ?  
 
wi: headword  
)|()|( 1212 iiiiiii wwwwPwwwP ???? ?  
 
??
?
?
??
?
?
?
 
wi: function word  
3.3 Finding clusters: model estimation 
In constructing clustering models, two factors were 
considered: how to find optimal clusters, and the 
optimal number of clusters.  
The clusters were found automatically by 
attempting to minimize perplexity (Brown et al, 
1992). In particular, for predictive clustering 
models, we tried to minimize the perplexity of the 
training data of )|()|( 1 iiii wwPwwP ?? . Letting N be 
the size of the training data, we have 
?
=
? ?
N
i
iiii wwPwwP
1
1 )|()|(  
?
= ?
? ?=
N
i i
ii
i
ii
WP
wwP
wP
wwP
1 1
1
)(
)(
)(
)(  
?
=
?
?
?=
N
i i
ii
i
ii
wP
wwP
wP
wwP
1
1
1 )(
)(
)(
)(  
?
=
?
?
?=
N
i
ii
i
i wwPwP
wP
1
1
1
)|()(
)(  
Now, 
)(
)(
1?i
i
wP
wP is independent of the clustering used. 
Therefore, in order to select the best clusters, it is 
sufficient to try to maximize ?= ?Ni ii wwP1 1 )|( . 
The clustering technique we used creates a 
binary branching tree with words at the leaves. By 
cutting the tree at a certain level, it is possible to 
achieve a wide variety of different numbers of 
clusters. For instance, if the tree is cut after the sixth 
level, there will be roughly 26=64 clusters. In our 
experiments, we always tried the numbers of 
clusters that are the powers of 2. This seems to 
produce numbers of clusters that are close enough 
to optimal. In Equation (10), the optimal number of 
clusters we used was 27. 
4 Relation to Previous Work 
Our LMs are similar to a number of existing ones. 
One such model was proposed by ATR (Isotani and 
Matsunaga, 1994), which we will refer to as ATR 
model below. In ATR model, the probability of 
each word in a sentence is determined by the 
preceding content and function word pair. Isotani 
and Matsunaga (1994) reported slightly better 
results over word bigram models for Japanese 
speech recognition. Geutner (1996) interpolated the 
ATR model with word-based trigram models, and 
reported very limited improvements over word 
trigram models for German speech recognition.  
One significant difference between the ATR 
model and our own lies in the use of predictive 
clustering. Another difference is that our models 
use separate probability estimates for headwords 
and function words, as shown in Equations (6) and 
(7). In contrast, ATR models are conceptually more 
similar to skipping models (Rosenfeld, 1994; Ney et 
al., 1994; Siu and Ostendorf, 2000), where only one 
probability estimate is applied for both content and 
function words, and the word categories are used 
only for the sake of finding the content and function 
word pairs in the context. 
Another model similar to ours is Jelinek (1990), 
where the headwords of the two phrases 
immediately preceding the word as well as the last 
two words were used to compute a word 
probability. The resulting model is similar to a 
5-gram model. A sophisticated interpolation 
formula had to be used since the number of 
parameters is too large for direct estimation. Our 
models are easier to learn because they use 
trigrams. They also differ from Jelinek's model in 
that they separately estimate the probability for 
headwords and function words.  
A significant number of sophisticated techniques 
for language modeling have recently been proposed 
in order to capture more linguistic structure from a 
larger context. Unfortunately, most of them suffer 
from either high computational cost or difficulty in 
obtaining enough manually parsed corpora for 
parameter estimation, which make it difficult to 
apply them successfully to realistic applications. 
For example, maximum entropy (ME) models 
(Rosenfeld, 1994) provide a nice framework for 
incorporating arbitrary knowledge sources, but 
training and using ME models is computationally 
extremely expensive.  
Another interesting idea that exploits the use of 
linguistic structure is structured language modeling 
(SLM, Chelba and Jelinek, 2000). SLM uses a 
statistical parser trained on an annotated corpus in 
order to identify the headword of each constituent, 
which are then used as conditioning words in the 
trigram context. Though SLMs have been shown to 
significantly improve the performance of the LM 
measured in perplexity, they also pose practical 
problems. First, the performance of SLM is 
contingent on the amount and quality of 
syntactically annotated training data, but such data 
may not always be available. Second, SLMs are 
very time-intensive, both in their training and use.  
Charniak (2001) and Roark (2001) also present 
language models based on syntactic dependency 
structure, which use lexicalized PCFGs that sum 
over the derivation probabilities. They both report 
improvements in perplexity over Chelba and 
Jelinek (2000) on the Wall Street Journal section of 
the Penn Treebank data, suggesting that syntactic 
structure can be further exploited for language 
modeling. The kind of linguistic structure used in 
our models is significantly more modest than that 
provided by parser-based models, yet offers 
practical benefits for realistic applications, as 
shown in the next section.  
5 Evaluation Methodology 
The most common metric for evaluating a language 
model is perplexity. Perplexity can be roughly 
interpreted as the expected branching factor of the 
test document when presented to a language model. 
Perplexity is widely used due to its simplicity and 
efficiency. However, the ultimate quality of a 
language model must be measured by its effect on 
the specific task to which it is applied, such as 
speech recognition. Lower perplexities usually 
result in lower error rates, but there are numerous 
counterexamples to this in the literature. 
In this study, we evaluated our language models 
on the application of Japanese Kana-Kanji 
conversion, which is the standard method of 
inputting Japanese text by converting the text of 
syllabary-based Kana string into the appropriate 
combination of ideographic Kanji and Kana. This is 
a similar problem to speech recognition, except that 
it does not include acoustic ambiguity. Performance 
on this task is generally measured in terms of the 
character error rate (CER), which is the number of 
characters wrongly converted from the phonetic 
string divided by the number of characters in the 
correct transcript. The role of the language model is 
to select the word string (in a combination of Kanji 
and Kana) with the highest probability among the 
candidate strings that match the typed phonetic 
(Kana) string. Current products make about 5-10% 
errors in conversion of real data in a wide variety of 
domains.  
For our experiments, we used two newspaper 
corpora: Nikkei and Yomiuri Newspapers. Both 
corpora have been word-segmented. We built 
language models from a 36-million-word subset of 
the Nikkei Newspaper corpus. We performed 
parameter optimization on a 100,000-word subset 
of the Yomiuri Newspaper (held-out data). We 
tested our models on another 100,000-word subset 
of the Yomiuri Newspaper corpus. The lexicon we 
used contains 167,107 entries.  
In our experiments, we used the so-called 
?N-best rescoring? method. In this method, a list of 
hypotheses is generated by the baseline language 
model (a word trigram model in this study4), which 
is then rescored using a more sophisticated LM. 
Due to the limited number of hypotheses in the 
N-best list, the second pass may be constrained by 
the first pass. In this study, we used the 100-best 
list. The ?oracle? CER (i.e., the CER among the 
hypotheses with the minimum number of errors) is 
presented in Table 1. This is the upper bound on 
performance in our experiments. The performance 
of the conversion using the baseline trigram model 
is much better than the state-of-the-art performance 
currently available in the marketplace. This may be 
due to the large amount of training data we used, 
and to the similarity between the training and the 
test data. We also notice that the ?oracle? CER is 
                                                     
4  For the detailed description of the baseline trigram 
model, see Gao et al (2002).  
relatively high due to the high out-of-vocabulary 
rate, which is 1.14%. Because we have only limited 
room for improvement, the reported results of our 
experiments in this study may be underestimated. 
Baseline Trigram Oracle of 100-best 
3.73% 1.51% 
Table 1. CER results of baseline and 100-best list 
6 Results and Discussion 
6.1 Impact of headword dependency and 
predictive clustering 
We applied a series of language models proposed in 
this paper to the Japanese Kana-Kanji conversion 
task in order to test the effectiveness of our 
techniques. The results are shown in Table 2. The 
baseline result was obtained by using a 
conventional word trigram model. HTM stands for 
the headword trigram model of Equation (6) and (7) 
without permutation (i.e., ?2=1), while PHTM is the 
model with permutation. The T- and U-prefixes 
refer to the models using trigram (Equation (6)) or 
unigram (Equation (7)) estimate for word category 
probability. The C-prefix, as in C-PHTM, refers to 
PHTM with predictive clustering (Equation (10)). 
For comparison, we also include in Table 2 the 
results of using the predictive clustering model 
without taking word category into account, referred 
to as predictive clustering trigram model (PCTM). 
In PCTM, the probability for all words is estimated 
by )|()|( 1212 iiiiiii wwwwPwwwP ???? ? . 
Model ?1 ? 2 CER CER reduction 
Baseline ---- ---- 3.73% ----
T-HTM 0.2 1 3.54% 5.1% 
U-HTM  0.2 1 3.41% 8.6% 
T-PTHM 0.2 0.7 3.53% 5.4% 
U-PHTM  0.2 0.7 3.34% 10.5% 
PCTM ---- ---- 3.44% 7.8% 
C-HTM  0.3 1 3.23% 13.4% 
C-PHTM  0.3 0.7 3.17% 15.0% 
Table 2. Comparison of CER results 
In Table 2, we find that for both PHTM and HTM, 
models U-HTM and U-PHTM achieve better 
performance than models T-HTM and T-PHTM. 
Therefore, only models using unigram for category 
probability estimation are used for further 
experiments, including the models with predictive 
clustering. 
By comparing U-HTM with the baseline model, 
we can see that the headword trigram contributes 
greatly to the CER reduction: U-HTM 
outperformed the baseline model by about 8.6% in 
error rate reduction. HTM with headword 
permutation (U-PHTM) achieves further 
improvements of 10.5% CER reduction against the 
baseline. The contribution of predictive clustering is 
also very encouraging. Using predictive clustering 
alone (PCTM), we reduced the error rate by 7.8%.  
What is particularly noteworthy is that the 
combination of both techniques leads to even larger 
improvements: for both HTM and PHTM, 
predictive clustering (C-HTM and C-PHTM) brings 
consistent improvements over the models without 
clustering, achieving the CER reduction of 13.4% 
and 15.0% respectively against the baseline model, 
or 4.8% and 4.5% against the models without 
clustering.  
In sum, considering the good performance of our 
baseline system and the upper bound on 
performance improvement due to the 100-best list 
as shown in Table 1, the improvements we obtained 
are very promising. These results demonstrate that 
the simple method of using headword trigrams and 
predictive clustering can be used to effectively 
improve the performance of word trigram models. 
6.2 Comparsion with other models 
In this subsection, we present a comparison of our 
models with some of the previously proposed 
models, including the higher-order n-gram models, 
skipping models, and the ATR models.  
Higher-order n-gram models refer to those 
n-gram models in which n>3. Although most of the 
previous research showed little improvement, 
Goodman (2001) showed recently that, with a large 
amount of training data and sophisticated 
smoothing techniques, higher-order n-gram models 
could be superior to trigram models.  
The headword trigram model proposed in this 
paper can be thought of as a variation of a higher 
order n-gram model, in that the headword trigrams 
capture longer distance dependencies than trigram 
models. In order to see how far the dependency goes 
within our headword trigram models, we plotted the 
distribution of headword trigrams (y-axis) against 
the n of the word n-gram were it to be captured by 
the word n-gram (x-axis) in Figure 2. For example, 
given a word sequence w1w2w3w4w5w6, and if w1, w3 
and w6 are headwords, then the headword trigram 
P(w6|w3w1) spans the same distance as the word 
6-gram model.  
0.0E+00
5.0E+06
1.0E+07
1.5E+07
2.0E+07
2.5E+07
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
N of n-gram
Nu
mb
er 
of 
wo
rds
 Figure 2. Distribution of headword trigrams 
against the n of word n-gram 
From Figure 2, we can see that approximately 
95% of the headword trigrams can be captured by 
the higher-order n-gram model with the value of n 
smaller than 7. Based on this observation, we built 
word n-gram models with the values of n=4, 5 and 
6. For all n-gram models, we used the interpolated 
modified absolute discount smoothing method (Gao 
et al, 2001), which, in our experiments, achieved 
the best performance among the state-of-the-art 
smoothing techniques. Results showed that the 
performance of the higher-order word n-gram 
models becomes saturated quickly as n grows: the 
best performance was achieved by the word 5-gram 
model, with the CER of 3.71%. Following 
Goodman (2001), we suspect that the poor 
performance of these models is due to the data 
sparseness problem. 
Skipping models are an extension of an n-gram 
model in that they predict words based on n 
conditioning words, except that these conditioning 
words may not be adjacent to the predicted word. 
For instance, instead of computing P(wi|wi-2wi-1), a 
skipping model might compute P(wi|wi-3wi-1) or 
P(wi|wi-4wi-2). Goodman (2001) performed 
experiments of interpolating various kinds of 
higher-order n-gram skipping models, and obtained 
a very limited gain.  Our results confirm his results 
and suggest that simply extending the context 
window by brute-force can achieve little 
improvement, while the use of even the most 
modest form of structural information such as the 
identification of headwords and automatic 
clustering can help improve the performance.  
We also compared our models with the trigram 
version of the ATR models discussed in Section 4, 
in which the probability of a word is conditioned by 
the preceding content and function word pair. We 
performed experiments using the ATR models as 
described in Isotani and Matsunaga (1994). The 
results show that the CER of the ATR model alone 
is much higher than that of the baseline model, but 
when interpolated with a word trigram model, the 
CER is slightly reduced by 1.6% from 3.73% to 
3.67%. These results are consistent with those 
reported in previous work. The difference between 
the ATR model and our models indicates that the 
predictions of headwords and function words can 
better be done separately, as they play different 
semantic and syntactic roles capturing different 
dependency structure.  
6.3 Discussion 
In order to better understand the effect of the 
headword trigram, we have manually inspected the 
actual improvements given by PHTM. As expected, 
many of the improvements seem to be due to the use 
of larger context: for example, the headword 
trigram?? ~?? ~??  (shouhi 'consume' ~ 
shishutsu 'expense' ~ genshou 'decrease') 
contributed to the correct conversion of the 
phonetic string ?????  genshou into ?? 
genshou 'decrease' rather than ? ?  genshou 
'phenomenon' in the context of ?????????
? shouhi shishutsu hajimete no genshou  'consumer 
spending decreases for the first time'.  
On the other hand, the use of headword trigrams 
and predictive clustering is not without side effects. 
The overall gain in CER was 15% as we have seen 
above, but a closer inspection of the conversion 
results reveals that while C-PHTM corrected the 
conversion errors of the baseline model in 389 
sentences (8%), it also introduced new conversion 
errors in 201 sentences (4.1%). Among the newly 
introduced errors, one type of error is particularly 
worth noting: these are the errors where the 
candidate conversion preferred by the HTM is 
grammatically impossible or unlikely. For example, 
???????? (beikoku-ni shinkou-dekiru, 
USA-to invade-can 'can invade USA') was 
misconverted as ???????? (beikoku-ni 
shinkou-dekiru, USA-to new-can), even though ?
? shinkou 'invade' is far more likely to be preceded 
by the morpheme ? ni 'to', and ?? shinkou 'new' 
practically does not precede ??? dekiru 'can'. 
The HTM does not take these function words into 
account, leading to a grammatically impossible or 
implausible conversion. Finding the types of errors 
introduced by particular modeling assumptions in 
this manner and addressing them individually will 
be the next step for further improvements in the 
conversion task.   
7 Conclusion 
We proposed and evaluated a new language model, 
the permuted headword trigram model with 
clustering (C-PHTM). We have shown that the 
simple model that combines the predictive 
clustering with a headword detector can effectively 
capture structure in language. Experiments show 
that the proposed model achieves an encouraging 
15% CER reduction over a conventional word 
trigram model in a Japanese Kana-Kanji conversion 
system. We also compared C-PTHM to several 
similar models, showing that our model has many 
practical advantages, and achieves substantially 
better performance.  
One issue we did not address in this paper was 
the language model size: the models that use HTM 
are larger than the baseline model we compared the 
performance with. Though we did not pursue the 
issue of size reduction in this paper, there are many 
known techniques that effectively reduce the model 
size while minimizing the loss in performance. One 
area of future work is therefore to reduce the model 
size. Other areas include the application of the 
proposed model to a wider variety of test corpora 
and to related tasks.  
Acknowledgements 
We would like to thank Ciprian Chelba, Bill Dolan, 
Joshua Goodman, Changning Huang, Hang Li and 
Yoshiharu Sato for their comments on early 
thoughts and drafts of the paper. We would also like 
to thank Hiroaki Kanokogi, Noriko Ishibashi and 
Miyuki Seki for their help in our experiments. 
 
References 
Brown, Peter F., Vincent J. Della Pietra, Peter V. 
deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. 
Class-Based N-gram Models of Natural Language. 
Computational Linguistics, 18-4: 467-479. 
Charniak, Eugene. 2001. Immediate-head parsing for 
language models. In ACL/EACL 2001, pp.124-131.  
Chelba, Ciprian and Frederick Jelinek. 2000. Structured 
Language Modeling. Computer Speech and Language, 
Vol. 14, No. 4. pp 283-332.  
Gao, Jianfeng, Joshua T. Goodman and Jiangbo Miao. 
2001. The use of clustering techniques for language 
model ? application to Asian language. Computational 
Linguistics and Chinese Language Processing. Vol. 6, 
No. 1, pp 27-60. 
Gao, Jianfeng, Joshua Goodman, Mingjing Li and 
Kai-Fu Lee. 2002. Toward a unified approach to 
statistical language modeling for Chinese. ACM 
Transactions on Asian Language Information 
Processing, Vol. 1, No. 1, pp 3-33.  
Geutner, Petra. 1996. Introducing linguistic constraints 
into statistical language modeling. In International 
Conference on Spoken Language Processing, 
Philadelphia, USA. pp.402-405.  
Goodman, Joshua T. 2001. A bit of progress in language 
modeling. Computer Speech and Language. October, 
2001, pp 403-434. 
Goodman, Joshua T., and Jianfeng Gao. 2000. Language 
model size reduction by pruning and clustering. 
ICSLP-2000, Beijing.  
Isotani, Ryosuke and Shoichi Matsunaga. 1994. A 
stochastic language model for speech recognition 
integrating local and global constraints. ICASSP-94, 
pp. 5-8. 
Jelinek, Frederick. 1990. Self-organized language 
modeling for speech recognition. In A. Waibel and K. 
F. Lee (eds.), Readings in Speech Recognition, 
Morgan-Kaufmann, San Mateo, CA. pp. 450-506. 
Katz, S. M. 1987. Estimation of probabilities from sparse 
data for other language component of a speech 
recognizer. IEEE transactions on Acoustics, Speech 
and Signal Processing, 35(3): 400-401. 
Ney, Hermann, Ute Essen and Reinhard Kneser. 1994. 
On structuring probabilistic dependences in stochastic 
language modeling. Computer Speech and Language, 
8: 1-38. 
Roark, Brian. 2001. Probabilistic top-down parsing and 
language modeling. Computational Linguistics, 17-2: 
1-28. 
Rosenfeld, Ronald. 1994. Adaptive statistical language 
modeling: a maximum entropy approach. Ph.D. thesis, 
Carnegie Mellon University.  
Siu, Manhung and Mari Ostendorf. 2000. Variable 
n-grams and extensions for conversational speech 
language modeling. IEEE Transactions on Speech and 
Audio Processing, 8: 63-75. 
