  
An Agent-based Approach to Chinese Named Entity Recognition 
 
Shiren Ye Tat-Seng Chua Liu Jimin 
School of Computing, National University of Singapore, 
Singapore, 117543 
yesr@comp.nus.edu.sg chuats@comp.nus.edu.sg Liujm@comp.nus.edu.sg 
   
Abstract 
Chinese NE (Named Entity) recognition is 
a difficult problem because of the 
uncertainty in word segmentation and 
flexibility in language structure. This paper 
proposes the use of a rationality model in a 
multi-agent framework to tackle this 
problem. We employ a greedy strategy and 
use the NE rationality model to evaluate 
and detect all possible NEs in the text. We 
then treat the process of selecting the best 
possible NEs as a multi-agent negotiation 
problem. The resulting system is robust 
and is able to handle different types of NE 
effectively. Our test on the MET-2 test 
corpus indicates that our system is able to 
achieve high F1 values of above 92% on all 
NE types. 
1. Introduction 
Named entity (NE) recognition is a fundamental 
step to many language processing tasks. It was a 
basic task of the Message Understanding 
Conference (MUC) and has been studied 
intensively. Palma & Day (97) reported that 
person (PER), location (LOC) and organization 
(ORG) names are the most difficult sub-tasks as 
compared to other entities as defined in MUC. 
This paper thus focuses only on the recognition 
of PER, LOC and ORG entities. 
Recent research on NE recognition has been 
focused on the machine learning approach, such 
as the transformation-based learning (Aberdeen 
95), hidden Markov model (Bikel et al 97), 
decision tree (Sekin et al 98), collocation 
statistics (Lin 98), maximum entropy model 
(Borthwick 99), and EM bootstrapping 
(Cucerzan & Yarowsky 99). Other than English, 
several recent works examined the extraction of 
information from Spanish, Chinese, and 
Japanese (Isozaki 01). Most approaches for 
Chinese NE recognition used handcrafted rules, 
supplemented by word or character frequency 
statistics. These methods require a lot of 
resources to model the NEs. Chen et al (98) 
used 1-billion person name dictionary and 
employed mainly internal word statistics with no 
generalization. Yu et al (98) employed a 
common framework to model both the context 
and information residing within the entities, and 
performed rule generalization using POS 
(part-of-speech) and some semantic tags. A 
similar system is also reported in Luo & Song 
(01). 
Chinese NE recognition is much more difficult 
than that in English due to two major problems. 
The first is the word segmentation problem 
(Sproat et al 96, Palmer 97). In Chinese, there is 
no white space to delimit the words, where a 
word is defined as consisting of one or more 
characters representing a linguistic token. Word 
is a vague concept in Chinese, and Palmer (97) 
showed that even native speakers could only 
achieve about 75% agreement on ?correct? 
segmentation. As word segmentation is the basic 
initial step to almost all linguistic analysis tasks, 
many techniques developed in English NLP 
cannot be applied to Chinese. 
Second, there is no exterior feature (such as the 
capitalization) to help identify the NEs, which 
share many common characters with non-NE (or 
common words). For example, while ?  is 
normally associated with the country China, it 
could also mean the concepts in, at or hit; and? 
normally refers to the surname Zhang, but it also 
means the concepts open, sheet or spread. 
Moreover, proper names in Chinese may contain 
common words and vice versa. 
Because of the above problems, the use of 
statistical and heuristic rules commonly adopted 
in most existing systems is inadequate to tackle 
the Chinese NE recognition problem. In this 
paper, we consider a new approach of employing 
a rationality model in a multi-agent framework. 
 The main ideas of our approach are as follows. 
First, we use an NE rationality measure to 
evaluate the probability of a sequence of tokens 
being a specific NE type, and adopt a greedy 
approach to detect all possible NEs. Second, we 
treat the process of selecting the best NEs among 
a large set of possibilities as a multi-agent 
negotiation problem. We test our overall 
approach on the MET-2 test set and the system is 
able to achieve high F1 values of over 92% on all 
NE types. The results are significantly better 
than most reported systems on MET-2 test set. 
The rest of the paper describes the details of our 
rationality-based and multi-agent negotiation 
approach to detect and refine NEs. 
2. Rationality Model for NE Detection 
2.1 Named Entity and Its tokens Feature 
For clarity and without lost of generality, we 
focus our discussion mainly on PER entity. The 
problems and techniques discussed are 
applicable to LOC and ORG entities. We 
consider a simple PER name model comprising 
the surname followed by the first-name. Given 
the presence of a surname (as cue-word) in a 
token sequence, we compute the likelihood of 
this token playing the role of surname and the 
next token as the first-name. The pair could be 
recognized as PER only if both tokens are 
labeled as positive (or of the right types) as 
shown in Table 1. If either one of both of the 
tokens are evaluated negatively, then the pair 
will not be recognized as PER based on the 
model defined above.  
Sentence PER? Label Remarks 
?????? Y ?(+) ?(+) 
... invite Zhang Fei 
to speak ... 
?????? N ?(-) ?(-) 
? a piece of airline 
ticket ? 
?????? ? ?(+) ?(-) //Illegal PER 
????* ? ?(-) ?(+) //Illegal PER 
* Strictly, ??? and Mr. Zhang are not really person names. 
They are references to person names and should be detected via 
co-reference.  
Table 1: An example of NE and non-NE 
Although the example depicted in Table 1 is very 
simple, the same idea can be extended to the 
more complex NE Types for ORG and LOCs. 
The number of tokens in a NE may vary from 2 
in PER to about 20 for ORG. One constraint is 
that the sequencing of tokens and their labels 
must be consistent with the respective NE type. 
Also, there are grammatical rules governing the 
composition of different NE type. For example, 
LOC may consist of a sequence of LOCs; and 
ORG may include PER and/or LOC on its left. 
Thus by considering one pair of tokens at a time, 
and by extending the token sequence to the 
adjacent token one at a time, we can draw 
similar conclusion as that depicted in Table 1 for 
complex NE types. 
2.2 The Rationality Computation 
If we know the probability distribution of each 
type of token in a window, NE recognition is 
then the procedure of evaluating the rationality 
or certainty of a sequence of tokens with respect 
to a NE type. Motivated by the results in Table 1 
we view NE recognition as a special coloring 
problem. Initially, all the tokens in the corpus are 
considered as a sequence of White balls. Given a 
chain of tokens appears in a NE window, we 
want to use the probability distribution of these 
tokens to re-paint some of the white balls to 
different colors. A sequence of appropriately 
colored balls would induce an appropriate NE. 
For simplicity, we again focus on PER NE type 
with 2 tokens. The surname token will be 
colored red and first-name blue. We assume that 
the number of PER names in the corpus is N, 
and the rest of tokens is M. Because there are N 
surname and N first-name tokens in the corpus, 
the total number of tokens is M+2N. Hence the 
marginal probability of PER name is 
Pr(PER)=N/(2N+M) . 
 Red  Blue White 
 Format Pr. Format Pr. Format Pr. 
Red aRbR 0 aRbB 1 aRbW 0 
Blue aBbR 
N/(N
+M) aBbB 0 aBbW 
M/(N
+M)
White aWbR 
N/(N
+M) aWbB 0 aWbW 
M/(N
+M)
Note: Red ? Surname; Blue ? First-name; White - Others 
Table 2: Possibility combination of neighboring 
tokens within the corpus for PER 
Table 2 shows the possible relationships 
between the red and blue balls for the PER NE 
type by  considering the grammer that the 
surname must be followed by a first-name in a 
 formal PER. As we only permit the token pair 
for PER to be labeled as a red ball followed by a 
blue ball, the following sequences are not 
possible under our model: (a) a red (or blue) ball 
follows by itself; (b) a red ball follows by white 
ball; and (c) a white ball follows by the blue ball. 
Thus aRbR (a red follows by a red), aRbW, aBbB, 
and aWbB are illegal combinations. 
Given a pair of tokens a and b in the corpus, 
they are labeled as surname |aR| and |bR| times, as 
first-name |aB| and |bB| times, and as non-PER 
|aW| and |bW| times respectively. The expected 
value of a token sequence ab representing a PER 
when a is red and b is blue is: 
 | | | | | || | | | B R BR B R
b a b
a b a
N N
?= ? =  (1) 
The expected value of the cases when the token 
pair ab is not a PER name is the sum of expected 
values of four cases: aBbR, aBbW, aWbR, aWbW (see 
Table 2), which after simplification, is given by: 
 
| | | | | | | | | |
| | | | | | | | | | | || | | |
(| | | |) (| | | |)
(2)
B R B W W R W WR B
B W W R W RB R
B W R W
a b a b a b a b a b
a b a b a ba b
N M N M N M N M
a a b b
N M
= + + +
? ? ??= + + ++ + + +
+ ? += +
 
The ratio between the cases when ab is a PER 
versus when ab is not a PER is: 
| |
| |
C R BR B
ab a b
R B
a b
a b
?? = = ?? ??     (3) 
where | | | |;
(| | | |) (| | | |)
R BR B
a b
B W R W
a b
a a b b
? = ? =+ + ; 
and N M
N
? += . We call RbRacab and ??? ,,  
the rationality values of tokens ab, a and b of 
being a PER, red ball or blue ball respectively. 
On the other hand, the probabilities of a as a 
surname (red ball) and b as a first-name (blue 
ball) are: 
| | | |
,
| | (| | | |) | | (| | | |)
R BR B
a b
R B W B R W
a b
P P
a a a b b b
= =+ + + +  
Thus, ;
1 1
R B
R Ba b
a bR B
a b
P P
P P
? = ? =? ?  (4) 
The form of Equation (4) is similar to the 
concept of odds likelihood O(h), first introduced 
in Duda et al (79) as a generic term to denote 
the ratio of the probability and converse 
probability in the Prospector system, namely: 
 ( ) ( )( )
(- ) 1- ( )
P h P hO h
P h P h
= =  (5) 
Eq. (5) is used in a modified version of the 
Bayes theorem to solve the uncertainty 
reasoning problems. Surprisingly, our approach 
of rationality ? for NE with two tokens can be 
deduced as the product of their odds-likelihood. 
By linking the concept of odds-likelihood and 
rationality, we can compute the probability of a 
sequence of tokens being a specific NE type. 
Since the rationality values of tokens could 
vary from 0 to ?, it may incur overflow or 
underflow during the rationality evaluation. This 
is especially so for unknown tokens where their 
rationality values will be zero. To resolve this 
problem, we construct a piecewise function to 
map the rationality values from the range [0, ?] 
to [?min,?max]. Here we set the parameters 
?min=0.05 and ?max=50, and ensure that most 
rationality values will retain their original values 
after transformation. 
2.3 The Context of NEs 
In addition to identifying the structural 
information within the NEs, it is equally 
important to model the context around the NEs. 
Context is especially pivotal to language such as 
the Chinese or Korean where there is no white 
space and capital characters among the tokens. 
For PER type, the context tokens are likely to be 
person titles and action words. 
 
Figure 1: A NE detection window 
Thus after we have computed the rationality 
values of possible NEs, we enlarge the analysis 
window to cover both the NE candidate and its 
context. As shown in Figure 1, the window 
consists of three components: prefix, suffix and 
the NE candidate. If the NE is at the beginning 
or end of a paragraph, then the corresponding 
Prefix
Boundary of a possible 
Window
Suffix
?? ?   ??   ?? ?? 
NE 
 prefix or suffix is set to void. We can extend the 
rationality computation for an NE to the context 
window by incorporating both the prefix and 
suffix tokens separately. 
2.4 The Overall Procedure 
The overall procedure for estimating the 
likelihood of an NE among a sequence of tokens 
is as follows. 
a) Convert prior probability Pr(e) of each token 
e to rationality ?(e). A token e may have 
multiple Pr(e) values, each is dependent on 
the role token e plays in a possible NE, such 
as the probability of being a surname, 
first-name, prefix, suffix, general token or 
cue-word. 
b) At each cue-word position, compute the 
rationality of a possible NE by considering 
one pair of tokens at a time, and extending to 
the next token on the left or right depending 
on the NE type. The boundaries of PERs are 
extended forward; while that of ORGs and 
LOCs are extended backward. Each extension 
will produce a new NE candidate. The scope 
of the extension is also determined by the 
type of NE. The process terminates when the 
rationality value of the next token falls below 
a minimum threshold. 
c) For all possible NEs, construct the context 
window and compute its final rationality 
value within the context window. 
The process will result in multiple possible NEs, 
with most NEs overlapping with one another. 
3. Multi-Agent Framework for NE 
Confirmation 
3.1 Relationships between possible NEs 
Our greedy approach of identifying all possible 
NEs using the rationality model results in over 
segmentation of NEs. Figure 2 shows a list of 80 
possible NEs detected from a test article in the 
MET-2 test corpus. The number of correct NEs 
in this case is only 13. These possible NEs relate 
to each other in a complex way. The possible 
relationships between them are: 
a. Overlapping: This is the most common case 
when the tokens of multiple NEs overlap each 
other. Examples include ????????? 
and ???????????. They are both 
reasonable ORGs if considered separately. 
However, only one of them can be true. 
b. Repetition: Some possible NEs may repeat 
themselves with same or similar tokens. For 
example, the NE ???????????? 
is similar to ??????????????
???? in different part of the text. It means 
that these NEs have same beliefs and could 
cooperate to enhance each other?s belief. 
Figure 2: All possible NEs identified in a test article 
c. Unification: When the tokens of two NEs are 
adjacent to each other in a sentence, they may 
be unified to become a new NE by combining 
their tokens. For instance, the NEs ???? 
and ???? may be combined to form a new 
NE ????? ?. By the way, not all 
neighboring NEs can be unified because the 
unification must satisfy the syntactic and 
semantic specifications of the language. For 
example, two adjoining PERs cannot be 
unified, while it is possible for LOCs. 
d. Enumerated name list: This is a common 
language construct to present a list of names. 
An example of such construct is: ?????
(??)???, ?????????, and 
???????????????. 
If we knew the relationships between possible 
NEs, we can use this knowledge to modify the 
rationality values of possible NEs. The first 
relationship (overlapping) is of type competition 
while the other three are of type supporting. In a 
competition relationship, the rationality values of 
losing NEs are decremented, whereas in a 
supporting relationship, the rationality of the 
winning NE can be used to reinforce other NEs. 
??????????????????????????
???????? ??  
???????????????? 
??????????????????????? 
????????????????????? 
???????????????????????????
???????????? 
a team
 3.2 Agent-based Reasoning & Negotiation 
There is a need to modify the rationality values 
of possible NEs in order to identify the best 
possible NEs. One way to achieve this is to 
employ a decision tree (Sekine 98) to select the 
best possible candidates. However, it is difficult 
to use the decision tree to handle multiple 
relationships between conflicting NEs, and to 
perform incremental updates of rationality 
values in situations where the number, 
distribution and relationships in possible NEs are 
uncertain. In this work, we adopt a multi-agent 
approach to refine the rationality of possible NEs 
and vote the best potential NEs. 
Agents are software entities that perform some 
operations on behalf of their users or another 
programs with some degree of autonomy, and in 
so doing, employ some knowledge or 
representation of the user?s goals or desires (Don 
et al 96). In our system, we map every possible 
NE detected to an agent, which acts as the 
deputy of the NE and depicts all its attributes. 
Following the approach taken in the DBI system, 
we use the rationality of the NE as the belief, 
denoted by Br(A), of agent A. Agents are 
divided into Teams (Decker & Lesser 95) 
according to their contents and positions in the 
corpus. The division of agents into teams 
facilitates the negotiation of agents? beliefs.  
The negotiation between agents aims to 
eliminate underlying conflicts and uncertainty 
among them. The process of multi-agent 
negotiation is carried out as follows. 
a. We identify agents involved in an unification 
relationship. These agents will be unified if 
the constraints of unification are fulfilled. The 
new agents would inherit the evidences, 
including the rationality values, of its child 
agents. 
b. We divide the resulting agents into teams. 
Agents with overlapping tokens will be 
grouped into same teams, while independent 
agents will be assigned to different teams. 
c. We perform negotiation between agents based 
on the type of their relationship. For agents 
that are in competition relationship (i.e. those 
overlapping agents within the same team), we 
select the agent with the maximal belief (said 
ai) as the winner, and decrement the beliefs of 
the rest of Nt agents in the same team by ?(ai), 
i.e. 
  Br(aj) = Br(aj) - ?(ai), for j=1,.. Nt, and j?i 
 For agents involved in the supporting 
relations, we again select the agent with the 
maximal belief (of say ak) as the winner, but 
increment the rest of agents in the same set Sk 
by ?(ak), i.e. 
  Br(aj) = Br(aj) + ?(ak), for all j in Sk & j?k 
d. Repeat step c until the pre-defined rounds of 
negotiations have been reached. 
In order to ensure fairness in the negotiation 
process, we limit the amount of belief 
adjustment, ?(ai), during each round of 
negotiation. If the desired rounds of negotiation 
is NR, then the amount of adjustment in each 
round should be limited to ?(ai)/NR. NR should 
be set to allow all agents to have a fair chance to 
participate in the negotiation process. Here we 
set NR to 10. 
At the end of negotiation, only agents whose 
beliefs are greater than the threshold are selected. 
Figure 3 shows the resulting set of NEs derived 
from the list given in Figure 2. 
 
 
 
 
 
 
 
Fig. 3: NEs after agents-based modification 
4. The Overall Process of NE Recognition 
Since there is no white space between words in 
Chinese, the first essential step is to perform 
preliminary segmentation. Here, we adopt a 
greedy approach of generating all possible 
segmentation from the input text by performing 
the dictionary-based look-up using a common 
word dictionary. The common word dictionary is 
generated from the PKU corpus (Yu 99) (see 
Section 5.1). 
Second, we compute the rationality value of 
each token in the context of being a keyword, 
general word, or as boundary (prefix or suffix) 
of a specific NE type. 
????????????????????????
?????????? ?? ??????????
???????????????????????
???????????????????????
?????????????? ????????
?????????????????????
 Third, we identify all possible NE cue-words 
and use them as seeds of NE candidates. We 
construct all possible NEs from the cue-word 
positions through boundary extension and 
context inclusion. 
Forth, we modify the rationality values of all 
possible NEs using the agent-based negotiation 
methodology. The conflicts between possible 
NEs will disappear. 
Fifth, we select NEs with the labels of its 
corresponding seed if their rationality values are 
above a predefined limit ?. The value ? affects 
the balance between recall and precision. 
5. Experimental Results and Discussions 
5.1 The Datasets Used in Our Experiments 
We use a number of openly available datasets 
for our training and testing, including the 
PKU-corpus (Yu 99), Hownet (Dong & Dong 
00), MET2 Chinese resources (Chinchor 02), 
and two name lists (for foreign and ORG names) 
collected from the web by using a bootstrapping 
approach. The PKU is a manually tagged corpus 
containing one-month of news report from 
China?s People Daily. It uses over 30 POS tags 
including separate tags for surname and 
first-name. It contains about 37,000 sentences 
with over 106 tokens. From these resources, we 
generate the following dictionaries and statistics.  
a. We use the PKU corpus to build a common 
word dictionary by removing all words that 
are tagged as NE. The resulting dictionary 
contains 37,025 common words. 
b. From the PKU corpus, we compute each 
token?s distribution information based on its 
POS tags, and if it is an NE, its NE type and 
its role with respect to the NE. Altogether, we 
obtain the distribution information of about 
37,000 different tokens. 
c. We maintain a list of LOCs found in the 
MET-2 test corpus. We do not maintain the 
PER and ORG lists, because their 
re-occurrence probabilities are low. 
d. We supplement the distribution information 
derived in step (b) by incorporating tokens 
obtained from other resources stated above. 
The resources we derived are available for down 
loading at http://www.pris.nus.edu.sg/ie.html 
5.2 The Experiment and Results 
We test our resulting model on the MET-2 test 
corpus. Table 3 tabulates the results of our 
system in terms of recall (Rc), precision (Pr) and 
F1 measures. In order to demonstrate the 
effectiveness of our approach, we perform the 
tests under 3 different test configurations. 
a. We perform the baseline test by simply 
performing name-dictionary look-up. Notice 
that we do not use PER dictionary, and hence 
the performance under PER is left blank (*). 
b. We extract all possible NEs by using only the 
rationality-based approach where the 
threshold ? is set to 1.1. If there are conflicts 
between possible NEs, we simply select the 
NE with the maximal rationality. 
c. We employ the agent-based modification in 
conjunction with the rationality-based 
approach to select the best possible NEs. 
For comparison purpose, we also list in Table 3 
the corresponding results reported in Yu et al 
(98) and Chen et al (98) for the MUC-7 tests. 
Type NC NP NW NM NS Rc Pr F1
Base- 
line test 
(a) 
ORG
PER 
LOC 
79 3 0 295 0 
*   * * * * 
363 84 0 303 26 
21 98 35.0
*  * * 
54 86 66.0
Config 
(b) 
ORG
PER 
LOC 
309 5 28 35 47 
154 2 7 11 87 
618 0 29 103 112 
83 79 81.0
89 62 73.4
82 81 81.7
Config 
(c) 
ORG
PER 
LOC 
356 2 5 14 21 
167 1 2 4 9 
703 0 18 29 52 
95 93 93.7
96 93 94.7
94 91 92.3
Results 
of Chen 
et (98) 
ORG
PER 
LOC 
393 0 7 77 44 
159 0 0 25 56 
583 0 65 102 194 
78 83 81.3
91 74 81.6
78 69 73.2
Results 
of Yu et 
al. (98) 
ORG
PER 
LOC 
331 0 14 32 25 
160 0 7 7 74 
682 0 1 67 83 
88 89 88.5
92 66 76.7
91 89 0.0
where Pr = (NC + 0.5*NP)/(NC + NW + NP + NS); 
 Rc = (NC + 0.5*NP)/(NC + NW + NP + NM); 
 F1 = 2*Pr*Rc/(Pr+Rc). 
 and NC gives the number of NEs correctly recognized; 
   NP denotes the number of NEs partially recognized; 
   NW gives the number of NEs incorrectly recognized; 
   NM denotes the number of NEs missed; and finally 
   NS gives the number of NEs found by the system but not 
in the tagged list. 
Table 3: Results of MET2 under different configurations 
Table 3 shows that as we apply the rationality 
model (Config. b) followed by multi-agent 
framework (Config. c), the performance of the 
system improves steadily until it reaches a high 
performance of over 92% in F1 value. In fact 
 Config c results in significant improvements 
over Conig b in both precision and recall forall 
NE types. This shows that the agent-based 
modification could significantly reduce spurious 
and missing NEs. The performance of our 
overall system is significantly better than both 
reported systems as listed in Table 3. 
To demonstrate the effectiveness of our 
approach on general web-based documents, we 
perform another informal test to recognize NEs 
on the 100 randomly collected headline news 
articles from the well-known Chinese web sites 
(www.sina.com.cn, www.sohu.com, www. 
zaobao.com, www.Chinese times.com). The 
topics covered in these articles ranging from 
politic, economic, society to sports. The 
informal test shows that our approach could 
perform well on general web-based articles with 
F1 measures of over 90%. 
6. Conclusion 
Chinese NE recognition is a difficult problem 
because of the uncertainty in word segmentation. 
Many existing techniques that require 
knowledge of word segmentation, and syntactic 
and semantic tagging of text cannot be applied. 
In this paper, we propose a new approach of 
employing a rationality model in a multi-agent 
framework. We employ a greedy strategy and 
use the NE rationality measures to detect all 
possible NEs in the text. We then treat the 
process of selecting the best possible NEs as the 
multi-agent negotiation problem. The resulting 
system is robust and is able to handle different 
NE models. Our test on the MET-2 test corpus 
indicates that we could achieve high F1 values of 
above 92% on all NE types. 
We plan to further test our system on a 
large-scale test corpus. We will refine our 
techniques on a wide variety of text corpuses, 
and apply the bootstrapping technique to tackle 
the data sparseness problem. Finally, we will 
extend our research to perform relation and 
information extraction in multilingual text. 
References 
Bikel D.M., Schwartz R. & Weischedel R.M. (1999) 
An Algorithm that Learns What?s in a Name. 
Machine Learning, 34(1-3), 211-231 
Borthwick A. (1999) A Maximum Entropy Approach 
to Named Entity Recognition. Ph.D. Thesis, New 
York Univ.  
Chen H. H., Ding Y. W. Tsai S.C. & Bian, G.W. (1998) 
Description of the NTU System used for MET-2. In 
MUC-7 Proc. 
Chinchor N. A. (2002), http://www.itl.nist.gov/iaui/ 
894.02/related_projects/muc/. 
Cucerzan S. & Yarowsky D. D. (1999) Language 
Independent Named Entity Recognition Combining 
Morphological and Contextual Evidence. In Proc 
of 1999 Joint SIGDAT Conference on Empirical 
Methods in NLP & Very Large Corpora, 90-99. 
Decker K., & Lesser V. (1995) Designing a Family of 
Coordination Algorithm, In Proc Of 1st Int?l Conf. 
on Multiagent Sys, 73-80, Menlo Park, CA, AAAI 
Press. 
Don Gilbert, Manny Aparicio, et al(1996) White 
paper on intelligent agents (IBM), http://activist. 
gpl.ibm.com:81/WhitePaper/ptc2.htm. 
Dong Z.D. & Dong Q. (2000) HowNet, available at 
http://www.keenage.com/zhiwang/e_zhiwang.html. 
Duda R., Gaschnig J., & Hart P. (1979) Model design 
in the prospector consultant system for mineral 
exploration. In Expert systems in the micro 
-electronic age, Michie D. Ed., Edinburgh Univ. 
Press, Edinburgh, England. 
Isozaki H. (2001) Japanese Named Entity 
Recognition Based on a Simple Rule Generator and 
Decision Tree Learning, In ACL?01, 306-313. 
Lin D. (1998) Using collocation statistics in 
information extraction. In MUC-7 Proc. 
Luo Z.Y. & Song R. (2001) An Integrated and Fast 
Approach to Chinese Proper Name Recognition in 
Chinese Word Segmentation, In Proc. of Int?l 
Chinese Computing Conf., Singapore 323-328. 
Palmer D. D. (1997) A Trainable Rule-Based 
Algorithm for Word Segmentation, In Proc of 35th 
of ACL & 8th conf. of EACL, 321-328. 
Sproat R., Shih C., et al(1996) A Stochastic 
Finite-state Word Segmentation Algorithm for 
Chinese. Computational Linguistics, 22(3), 
377-404. 
Yu S.H., Bai S.H. & Wu P. (1998) Description of the 
Kent Ridge Digital Labs System Used For MUC-7, 
1998, In MUC-7 Proc. 
Yu S.W. (1999) The Specification and Manual of 
Chinese Word Segmentation and Part of Speech 
Tagging. http:// www.icl.pku.edu.cn/ 
Sekine S. (1998) NYU: Description of The Japanese 
NE System Used for MET-2, in MUC-7 Proc. 
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 199?207,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Summarizing Definition from Wikipedia
Shiren Ye and Tat-Seng Chua and Jie Lu
Lab of Media Search
National University of Singapore
{yesr|chuats|luj}@comp.nus.edu.sg
Abstract
Wikipedia provides a wealth of knowl-
edge, where the first sentence, infobox
(and relevant sentences), and even the en-
tire document of a wiki article could be
considered as diverse versions of sum-
maries (definitions) of the target topic.
We explore how to generate a series of
summaries with various lengths based on
them. To obtain more reliable associations
between sentences, we introduce wiki con-
cepts according to the internal links in
Wikipedia. In addition, we develop an
extended document concept lattice model
to combine wiki concepts and non-textual
features such as the outline and infobox.
The model can concatenate representative
sentences from non-overlapping salient lo-
cal topics for summary generation. We test
our model based on our annotated wiki ar-
ticles which topics come from TREC-QA
2004-2006 evaluations. The results show
that the model is effective in summariza-
tion and definition QA.
1 Introduction
Nowadays, ?ask Wikipedia? has become as pop-
ular as ?Google it? during Internet surfing, as
Wikipedia is able to provide reliable information
about the concept (entity) that the users want. As
the largest online encyclopedia, Wikipedia assem-
bles immense human knowledge from thousands of
volunteer editors, and exhibits significant contribu-
tions to NLP problems such as semantic related-
ness, word sense disambiguation and question an-
swering (QA).
For a given definition query, many search en-
gines (e.g., specified by ?define:? in Google) often
place the first sentence of the corresponding wiki1
article at the top of the returned list. The use of
1 For readability, we follow the upper/lower case rule
on web (say, ?web pages? and ?on the Web?), and utilize
one-sentence snippets provides a brief and concise
description of the query. However, users often need
more information beyond such a one-sentence de-
finition, while feeling that the corresponding wiki
article is too long. Thus, there is a strong demand
to summarize wiki articles as definitions with vari-
ous lengths to suite different user needs.
The initial motivation of this investigation is to
find better definition answer for TREC-QA task
using Wikipedia (Kor and Chua, 2007). Accord-
ing to past results on TREC-QA (Voorhees, 2004;
Voorhees and Dang, 2005), definition queries are
usually recognized as being more difficult than fac-
toid and list queries. Wikipedia could help to
improve the quality of answer finding and even
provide the answers directly. Its results are bet-
ter than other external resources such as WordNet,
Gazetteers and Google?s define operator, especially
for definition QA (Lita et al, 2004).
Different from the free text used in QA and sum-
marization, a wiki article usually contains valuable
information like infobox and wiki link. Infobox
tabulates the key properties about the target, such
as birth place/date and spouse for a person as well
as type, founder and products for a company. In-
fobox, as a form of thumbnail biography, can be
considered as a mini version of a wiki article?s sum-
mary. In addition, the relevant concepts existing in
a wiki article usually refer to other wiki pages by
wiki internal links, which will form a close set of
reference relations. The current Wikipedia recur-
sively defines over 2 million concepts (in English)
via wiki links. Most of these concepts are multi-
word terms, whereas WordNet has only 50,000 plus
multi-word terms. Any term could appear in the
definition of a concept if necessary, while the total
vocabulary existing in WordNet?s glossary defini-
tion is less than 2000. Wikipedia addresses explicit
semantics for numerous concepts. These special
knowledge representations will provide additional
information for analysis and summarization. We
thus need to extend existing summarization tech-
nologies to take advantage of the knowledge repre-
sentations in Wikipedia.
?wiki(pedia) articles? and ?on (the) Wikipedia?, the latter re-
ferring to the entire Wikipedia.
199
The goal of this investigation is to explore sum-
maries with different lengths in Wikipedia. Our
main contribution lies in developing a summariza-
tion method that can (i) explore more reliable asso-
ciations between passages (sentences) in huge fea-
ture space represented by wiki concepts; and (ii) ef-
fectively combine textual and non-textual features
such as infobox and outline in Wikipedia to gener-
ate summaries as definition.
The rest of this paper is organized as follows: In
the next section, we discuss the background of sum-
marization using both textual and structural fea-
tures. Section 3 presents the extended document
concept lattice model for summarizing wiki arti-
cles. Section 4 describes corpus construction and
experiments are described; while Section 5 con-
cludes the paper.
2 Background
Besides some heuristic rules such as sentence po-
sition and cue words, typical summarization sys-
tems measure the associations (links) between sen-
tences by term repetitions (e.g., LexRank (Erkan
and Radev, 2004)). However, sophisticated authors
usually utilize synonyms and paraphrases in vari-
ous forms rather than simple term repetitions. Fur-
nas et al (1987) reported that two people choose
the same main key word for a single well-known
object less than 20% of the time. A case study by
Ye et al (2007) showed that 61 different words ex-
isting in 8 relevant sentences could be mapped into
16 distinctive concepts by means of grouping terms
with close semantic (such as [British, Britain, UK]
and [war, fought, conflict, military]). However,
most existing summarization systems only consider
the repeated words between sentences, where latent
associations in terms of inter-word synonyms and
paraphrases are ignored. The incomplete data likely
lead to unreliable sentence ranking and selection for
summary generation.
To recover the hidden associations between sen-
tences, Ye et al (2007) compute the semantic simi-
larity using WordNet. The term pairs with semantic
similarity higher than a predefined threshold will be
grouped together. They demonstrated that collect-
ing more links between sentences will lead to bet-
ter summarization as measured by ROUGE scores,
and such systems were rated among the top systems
in DUC (document understanding conference) in
2005 and 2006. This WordNet-based approach has
several shortcomings due to the problems of data
deficiency and word sense ambiguity, etc.
Wikipedia already defined millions of multi-
word concepts in separate articles. Its definition is
much larger than that of WordNet. For instance,
more than 20 kinds of songs and movies called But-
terfly , such as Butterfly (Kumi Koda song), Butter-
fly (1999 film) and Butterfly (2004 film), are listed
in Wikipedia. When people say something about
butterfly in Wikipedia, usually, a link is assigned
to refer to a particular butterfly. Following this
link, we can acquire its explicit and exact seman-
tic (Gabrilovich and Markovitch, 2007), especially
for multi-word concepts. Phrases are more im-
portant than individual words for document re-
trieval (Liu et al, 2004). We hope that the wiki con-
cepts are appropriate text representation for sum-
marization.
Generally, wiki articles have little redundancy
in their contents as they utilize encyclopedia style.
Their authors tend to use wiki links and ?See Also?
links to refer to the involved concepts rather than
expand these concepts. In general, the guideline
for composing wiki articles is to avoid overlong
and over-complicated styles. Thus, the strategy of
?split it? into a series of articles is recommended;
so wiki articles are usually not too long and contain
limited number of sentences. These factors lead to
fewer links between sentences within a wiki article,
as compared to normal documents. However, the
principle of typical extractive summarization ap-
proaches is that the sentences whose contents are
repeatedly emphasized by the authors are most im-
portant and should be included (Silber and McCoy,
2002). Therefore, it is challenging to summarize
wiki articles due to low redundancy (and links)
between sentences. To overcome this problem,
we seek (i) more reliable links between passages,
(ii) appropriate weighting metric to emphasize the
salient concepts about the topic, and (iii) additional
guideline on utilizing non-textual features such as
outline and infobox. Thus, we develop wiki con-
cepts to replace ?bag-of-words? approach for better
link measurements between sentences, and extend
an existing summarization model on free text to in-
tegrate structural information.
By analyzing rhetorical discourse structure of
aim, background, solution, etc. or citation context,
we can obtain appropriate abstracts and the most
influential contents from scientific articles (Teufel
and Moens, 2002; Mei and Zhai, 2008). Similarly,
we believe that the structural information such as
infobox and outline is able to improve summariza-
tion as well. The outline of a wiki article using in-
ner links will render the structure of its definition.
In addition, infobox could be considered as topic
signature (Lin and Hovy, 2000) or keywords about
the topic. Since keywords and summary of a doc-
ument can be mutually boosted (Wan et al, 2007),
infobox is capable of summarization instruction.
When Ahn (2004) and Kor (2007) utilize
Wikipedia for TREC-QA definition, they treat the
Wikipedia as the Web and perform normal search
on it. High-frequency terms in the query snippets
returned from wiki index are used to extend query
and rank (re-rank) passages. These snippets usually
200
come from multiple wiki articles. Here the use-
ful information may be beyond these snippets but
existing terms are possibly irrelevant to the topic.
On the contrary, our approach concentrates on the
wiki article having the exact topic only. We as-
sume that every sentence in the article is used to de-
fine the query topic, no matter whether it contains
the term(s) of the topic or not. In order to extract
some salient sentences from the article as definition
summaries, we will build a summarization model
that describes the relations between the sentences,
where both textual and structural features are con-
sidered.
3 Our Approach
3.1 Wiki Concepts
In this subsection, we address how to find rea-
sonable and reliable links between sentences using
wiki concepts.
Consider a sentence: ?After graduating from
Boston University in 1988, she went to work at a
Calvin Klein store in Boston.? from a wiki article
?Carolyn Bessette Kennedy?2, we can find 11 dis-
tinctive terms, such as after, graduate, Boston, Uni-
versity,1988, go, work, Calvin, Klein, store, Boston,
if stop words are ignored.
However, multi-word terms such as Boston
University and Calvin Klein are linked to the
corresponding wiki articles, where their definitions
are given. Clearly, considering the anchor texts as
two wiki concepts rather than four words is more
reasonable. Their granularity are closer to semantic
content units in a summarization evaluation method
Pyramid (Nenkova et al, 2007) and nuggets in
TREC-QA . When the text is represented by
wiki concepts, whose granularity is similar to the
evaluation units, it is possibly easy to detect the
matching output using a model. Here,
? Two separate words, Calvin and Klein, are
meaningless and should be discarded; oth-
erwise, spurious links between sentences are
likely to occur.
? Boston University and Boston are processed
separately, as they are different named entities.
No link between them is appropriate3.
? Terms such as ?John F. Kennedy, Jr.? and
?John F. Kennedy? will be considered as two
diverse wiki concepts, but we do not account
on how many repeated words there are.
? Different anchor texts, such as U.S.A. and
United States of America, are recognized as
2All sample sentences in this paper come from this article
if not specified.
3Consider new pseudo sentence: ?After graduating from
Stanford in 1988, she went to work ... in Boston.? We do not
need assign link between Stanford and Boston as well.
an identical concept since they refer to the
same wiki article.
? Two concepts, such as money and cash, will
be merged into an identical concept when their
semantics are similar.
In wiki articles, the first occurrence of a wiki
concept is tagged by a wiki link, but there is no
such a link to its subsequent occurrences in the re-
maining parts of the text in most cases. To allevi-
ate this problem, a set of heuristic rules is proposed
to unify the subsequent occurrences of concepts in
normal text with previous wiki concepts in the an-
chor text. These heuristic rules include: (i) edit dis-
tance between linked wiki concept and candidates
in normal text is larger than a predefined threshold;
and (ii) partially overlapping words beginning with
capital letter, etc.
After filtering out wiki concepts, the words re-
maining in wiki articles could be grouped into two
sets: close-class terms like pronouns and preposi-
tions as well as open-class terms like nouns and
verbs. For example, in the sentence ?She died at age
33, along with her husband and sister?, the open-
class terms include die, age, 33, husband and sister.
Even though most open-class terms are defined in
Wikipedia as well, the authors of the article do not
consider it necessary to present their references us-
ing wiki links. Hence, we need to extend wiki con-
cepts by concatenating them with these open-class
terms to form an extended vector. In addition, we
ignore all close-class terms, since we cannot find
efficient method to infer reliable links across them.
As a result, texts are represented as a vector of wiki
concepts.
Once we introduce wiki concepts to replace typ-
ical ?bag-of-words? approach, the dimensions of
concept space will reach six order of magnitudes.
We cannot ignore the data spareness issue and com-
putation cost when the concept space is so huge.
Actually, for a wiki article and a set of relevant arti-
cles, the involved concepts are limited, and we need
to explore them in a small sub-space. For instance,
59 articles about Kennedy family in Wikipedia have
10,399 distinctive wiki concepts only, where 5,157
wiki concepts exist twice and more. Computing the
overlapping among them is feasible.
Furthermore, we need to merge the wiki concepts
with identical or close semantic (namely, building
links between these synonyms and paraphrases).
We measure the semantic similarity between two
concepts by using cosine distance between their
wiki articles, which are represented as the vectors
of wiki concepts as well. For computation effi-
ciency, we calculate semantic similarities between
all promising concept pairs beforehand, and then
retrieve the value in a Hash table directly. We spent
CPU time of about 12.5 days preprocessing the se-
201
mantic calculation. Details are available at our tech-
nical report (Lu et al, 2008).
Following the principle of TFIDF, we define
the weighing metric for the vector represented
by wiki concepts using the entire Wikipedia as
the observation collection. We define the CFIDF
weight of wiki concept i in article j as:
wi,j = cfi,j? idfi = ni,j?
k nk,j
? log |D||dj : ti ? dj| ,
(1)
where cfi,j is the frequency of concept i in arti-
cle j; idfi is the inverse frequency of concept i
in Wikipedia; and D is the number of articles in
Wikipedia. Here, sparse wiki concepts will have
more contribution.
In brief, we represent articles in terms of wiki
concepts using the steps below.
1. Extract the wiki concepts marked by wiki
links in context.
2. Detect the remaining open-class terms as wiki
concepts as well.
3. Merge concepts whose semantic similarity is
larger than predefined threshold (0.35 in our
experiments) into the one with largest idf .
4. Weight all concepts according to Eqn (1).
3.2 Document Concept Lattice Model
Next, we build the document concept lattice (DCL)
for articles represented by wiki concepts. For il-
lustration on how DCL is built, we consider 8 sen-
tences from DUC 2005 Cluster d324e (Ye et al,
2007) as case study. 8 sentences, represented by 16
distinctive concepts A-P, are considered as the base
nodes 1-8 as shown in Figure 1. Once we group
nodes by means of the maximal common concepts
among base nodes hierarchically, we can obtain the
derived nodes 11-41, which form a DCL. A derived
node will annotate a local topic through a set of
shared concepts, and define a sub concept space that
contains the covered base nodes under proper pro-
jection. The derived node, accompanied with its
base nodes, is apt to interpret a particular argument
(or statement) about the involved concepts. Further-
more, one base node among them, coupled with the
corresponding sentence, is capable of this interpre-
tation and could represent the other base nodes to
some degree.
In order to Extract a set of sentences to cover
key distinctive local topics (arguments) as much as
possible, we need to select a set of important non-
overlapping derived nodes. We measure the impor-
tance of node N in DCL of article j in term of rep-
resentative power (RP) as:
RP (N) =
?
ci?N
(|ci|?wi,j)/ log(|N |), (2)
         
	 
     
  
     
	
 
  



      
	
 



 
      
	
 



     
	
 
     
         