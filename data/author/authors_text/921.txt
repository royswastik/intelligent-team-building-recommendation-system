Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 11?17,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Parsing Linear Context-Free Rewriting Systems
H?kan Burden
Dept. of Linguistics
G?teborg University
cl1hburd@cling.gu.se
Peter Ljungl?f
Dept. of Computing Science
G?teborg University
peb@cs.chalmers.se
Abstract
We describe four different parsing algorithms
for Linear Context-Free Rewriting Systems
(Vijay-Shanker et al, 1987). The algorithms
are described as deduction systems, and possi-
ble optimizations are discussed.
The only parsing algorithms presented for linear context-
free rewriting systems (LCFRS; Vijay-Shanker et al,
1987) and the equivalent formalism multiple context-free
grammar (MCFG; Seki et al, 1991) are extensions of the
CKY algorithm (Younger, 1967), more designed for their
theoretical interest, and not for practical purposes. The
reason for this could be that there are not many imple-
mentations of these grammar formalisms. However, since
a very important subclass of the Grammatical Framework
(Ranta, 2004) is equivalent to LCFRS/MCFG (Ljungl?f,
2004a; Ljungl?f, 2004b), there is a need for practical
parsing algorithms.
In this paper we describe four different parsing algo-
rithms for Linear Context-Free Rewriting Systems. The
algorithms are described as deduction systems, and pos-
sible optimizations are discussed.
1 Introductory definitions
A record is a structure ? = {r1 = a1; . . . ; rn = an},
where all ri are distinct. That this can be seen as a set
of feature-value pairs. This means that we can define a
simple version of record unification ?1 unionsq ?2 as the union
?1??2, provided that there is no r such that ?1.r 6= ?2.r.
We sometimes denote a sequence X1, . . . , Xn by the
more compact ~X . To update the ith record in a list of
records, we write ~?[i := ?]. To substitute a variable
Bk for a record ?k in any data structure ?, we write
?[Bk/?k].
1.1 Decorated Context-Free Grammars
The context-free approximation described in section 4
uses a form of CFG with decorated rules of the form
f : A ? ?, where f is the name of the rule, and ? is a
sequence of terminals and categories subscripted with in-
formation needed for post-processing of the context-free
parse result. In all other respects a decorated CFG can be
seen as a straight-forward CFG.
1.2 Linear Context-Free Rewriting Systems
A linear context-free rewriting system (LCFRS; Vijay-
Shanker et al, 1987) is a linear, non-erasing multiple
context-free grammar (MCFG; Seki et al, 1991). An
MCFG rule is written1
A ? f [B1 . . . B?] := { r1 = ?1; . . . ; rn = ?n }
where A and Bi are categories, f is the name of the rule,
ri are record labels and ?i are sequences of terminals and
argument projections of the form Bi.r. The language
L(A) of a category A is a set of string records, and is
defined recursively as
L(A) = { ?[B1/?1, . . . , B?/??] |
A ? f [B1 . . . B?] := ?,
?1 ? L(B1), . . . , ?? ? L(B?) }
It is the possibility of discontinuous constituents that
makes LCFRS/MCFG more expressive than context-free
grammars. If the grammar only consists of single-label
records, it generates a context-free language.
Example A small example grammar is shown in figure 1,
and generates the language
L(S) = { s shm | s ? (a ? b)
? }
where shm is the homomorphic mapping such that
each a in s is translated to c, and each b is translated
to d. Examples of generated strings are ac, abcd and
bbaddc. However, neither abc nor abcdabcd will be
1We borrow the idea of equating argument categories and
variables from Nakanishi et al (1997) , but instead of tuples we
use the equivalent notion of records for the linearizations.
11
Figure 1: An example grammar describing the language
{ s shm | s ? (a ? b)? }
S ? f [A] := { s = A.p A.q }
A ? g[A1 A2] := { p = A1.p A2.p; q = A1.q A2.q }
A ? ac[ ] := { p = a; q = c }
A ? bd[ ] := { p = b; q = d }
generated. The language is not context-free since
it contains a combination of multiple and crossed
agreement with duplication.
If there is at most one occurrence of each possible pro-
jection Ai.r in a linearization record, the MCFG rule is
linear. If all rules are linear the grammar is linear. A rule
is erasing if there are argument projections that have no
realization in the linearization. A grammar is erasing if
it contains an erasing rule. It is possible to transform an
erasing grammar to non-erasing form (Seki et al, 1991).
Example The example grammar is both linear and non-
erasing. However, given that grammar, the rule
E ? e[A] := { r1 = A.p; r2 = A.p }
is both non-linear (since A.p occurs more than once)
and erasing (since it does not mention A.q).
1.3 Ranges
Given an input string w, a range ? is a pair of indices,
(i, j) where 0 ? i ? j ? |w| (Boullier, 2000). The en-
tire string w = w1 . . . wn spans the range (0, n). The
word wi spans the range (i ? 1, i) and the substring
wi+1, . . . , wj spans the range (i, j). A range with identi-
cal indices, (i, i), is called an empty range and spans the
empty string.
A record containing label-range pairs,
? = { r1 = ?1, . . . , rn = ?n }
is called a range record. Given a range ? = (i, j), the
ceiling of ? returns an empty range for the right index,
d?e = (j, j); and the floor of ? does the same for the
left index b?c = (i, i). Concatenation of two ranges is
non-deterministic,
(i, j) ? (j?, k) = { (i, k) | j = j? }
.
1.3.1 Range restriction
In order to retrieve the ranges of any substring s in a
sentence w = w1 . . . wn we define range restriction of s
with respect to w as ?s?w = { (i, j) | s = wi+1 . . . wj },
i.e. the set of all occurrences of s in w. If w is understood
from the context we simply write ?s?.
Range restriction of a linearization record ? is written
???, which is a set of records, where every terminal token
s is replaced by a range from ?s?. The range restriction of
two terminals next to each other fails if range concatena-
tion fails for the resulting ranges. Any unbound variables
in ? are unaffected by range restriction.
Example Given the string w = abba, range restricting
the terminal a yields
?a?w = { (0, 1), (3, 4) }
Furthermore,
?aA.r a bB.q?w =
{ (0, 1)A.r (0, 2)B.q, (3, 4)A.r (0, 2)B.q }
The other possible solutions fail since they cannot
be range concatenated.
2 Parsing as deduction
The idea with parsing as deduction (Shieber et al, 1995)
is to deduce parse items by inference rules. A parse item
is a representation of a piece of information that the pars-
ing algorithm has acquired. An inference rule is written
?1 . . . ?n
C
?
where ? is the consequence of the antecedents ?1 . . . ?n,
given that the side conditions in C hold.
2.1 Parsing decorated CFG
Decorated CFG can be parsed in a similar way as stan-
dard CFG. For our purposes it suffices to say that the al-
gorithm returns items of the form,
[f : A/? ? B1/?1 . . . Bn/?n ? ]
saying that A spans the range ?, and each daughter Bi
spans ?i.
The standard inference rule combine might look like
this for decorated CFG:
Combine
[f : A/? ? ? ? Bx ?]
[g : B/?? ? . . . ? ]
??? ? ? ? ??
[f : A/? ? ? Bx/??? ? ?]
Note that the subscript x in Bx is the decoration that will
only be used in post-processing.
12
3 The Na?ve algorithm
Seki et al (1991) give an algorithm for MCFG, which can
be seen as an extension of the CKY algorithm (Younger,
1967). The problem with that algorithm is that it has to
find items for all daughters at the same time. We modify
this basic algorithm to be able to find one daughter at the
time.
There are two kinds of items. A passive item [A; ?]
has the meaning that the category A has been found span-
ning the range record ?. An active item for the rule
A ? f [ ~B ~B?] := ? has the form
[A ? f [ ~B ? ~B?]; ?; ~?]
in which the categories to the left of the dot, ~B, have been
found with the linearizations in the list of range records
~?. ? is the result of substituting the projections in ? with
ranges for the categories found in ~B.
3.1 Inference rules
There are three inference rules, Predict, Combine and
Convert.
Predict
A ? f [ ~B] := ?
? ? ???
[A ? f [ ? ~B]; ?; ]
Prediction gives an item for every rule in the gram-
mar, where the range restriction ? is what has been
found from the beginning. The list of daughters is
empty since none of the daughters in ~B have been
found yet.
Combine
[A ? f [ ~B ? Bk ~B?]; ?; ~?]
[Bk; ?k]
?? ? ?[Bk/?k]
[A ? f [ ~B Bk ? ~B?]; ??; ~?, ?k]
An active item looking for Bk and a passive item
that has found Bk can be combined into a new active
item. In the new item we substitute Bk for ?k in
the linearization record. We also add ?k to the new
item?s list of daughters.
Convert
[A ? f [ ~B ? ]; ?; ~?]
? ? ?
[A; ?]
Every fully instantiated active item is converted into
a passive item. Since the linearization record ?
is fully instantiated, it is equivalent to the range
record ?.
Figure 2: The example grammar converted to a decorated
CFG
f : (S.s) ? (A.p) (A.q)
g : (A.p) ? (A.p)1 (A.p)2
g : (A.q) ? (A.q)1 (A.q)2
ac : (A.p) ? a
ac : (A.q) ? b
bd : (A.p) ? c
bd : (A.q) ? d
The subscripted numbers are for distinguishing the two
categories from each other, since they are equivalent.
Here A.q is a context-free category of its own, not a
record projection.
4 The Approximative algorithm
Parsing is performed in two steps in the approximative
algorithm. First we parse the sentence using a context-
free approximation. Then the resulting context-free chart
is recovered to a LCFRS chart.
The LCFRS is converted by creating a decorated
context-free rule for every row in a linearization record.
Thus, the rule
A ? f [ ~B] := { r1 = ?1; . . . ; rn = ?n }
will give n context-free rules f : A.ri ? ?i. The ex-
ample grammar from figure 1 is converted to a decorated
CFG in figure 2.
Parsing is now initiated by a context-free parsing algo-
rithm returning decorated items as in section 2.1. Since
the categories of the decorated grammar are projections
of LCFRS categories, the final items will be of the form
[f : (A.r)/? ? . . . (B.r?)x/?
? . . . ? ]
Since the decorated CFG is over-generating, the re-
turned parse chart is unsound. We therefore need to re-
trieve the items from the decorated CFG parse chart and
check them against the LCFRS to get the discontinuous
constituents and mark them for validity.
The initial parse items are of the form,
[A ? f [ ~B]; r = ?; ~?]
where ~? is extracted from a corresponding decorated item
[f : (A.r)/? ? ?], by partitioning the daughters in ?
such that ?i = { r = ? | (B.r)i/? ? ? }. In other words,
?i will consist of all r = ? such that B.r is subscripted
by i in the decorated item.
Example Given ? = (A.p)2/?? (B.q)1/??? (A.q)2/????,
we get the two range records ?1 = {q = ???} and
?2 = {p = ??; q = ????}.
13
Apart from the initial items, we use three kinds of parse
items. From the initial parse items we first build LCFRS
items, of the form
[A ? f [ ~B]; ? ? ri . . . rn; ~?]
where ri . . . rn is a list of labels, ~? is a list of | ~B| range
records, and ? is a range record for the labels r1 . . . ri?1.
In order to recover the chart we use mark items
[A ? f [ ~B ? ~B?]; ?; ~? ? ~??]
The idea is that ~? has been verified as range records span-
ning the daughters ~B. When all daughters have been ver-
ified, a mark item is converted to a passive item [A; ?].
4.1 Inference rules
There are five inference rules, Pre-Predict, Pre-Combine,
Mark-Predict, Mark-Combine and Convert.
Pre-Predict
A ? f [ ~B] := {r1 = ?1; . . . ; rn = ?n}
~?? = { }, . . . , { }
[A ? f [ ~B]; ? r1 . . . rn; ~??]
Every rule A ? f [ ~B] is predicted as an LCFRS
item. Since the context-free items contain informa-
tion about ?1 . . . ?n, we only need to use the labels
r1, . . . , rn. ~?? is a list of | ~B| empty range records.
Pre-Combine
[R; ? ? r ri . . . rn; ~?]
[R; r = ?; ~??]
~??? ? ~? unionsq ~??
[R; {?; r = ?} ? ri . . . rn; ~???]
If there is an initial parse item for the rule R with la-
bel r, we can combine it with an LCFRS item look-
ing for r, provided the daughters? range records can
be unified.
Mark-Predict
[A ? [ ~B]; ? ? ; ~?]
[A ? [ ? ~B]; ?; ? ~?]
When all record labels have been found, we can start
to check if the items have been derived in a valid way
by marking the daughters? range records for correct-
ness.
Mark-Combine
[A ? f [ ~B ? Bi ~B?]; ?; ~? ? ?i ~??]
[Bi; ?i]
[A ? f [ ~B Bi ? ~B?]; ?; ~??i ? ~??]
Record ?i is correct if there is a correct passive item
for category Bi that has found ?i.
Convert
[A ? f [ ~B ? ]; ?; ~? ? ]
[A; ?]
An item that has marked all daughters as correct is
converted to a passive item.
5 The Active algorithm
The active algorithm parses without using any context-
free approximation. Compared to the Na?ve algorithm
the dot is used to traverse the linearization record of a
rule instead of the categories in the right-hand side.
For this algorithm we use a special kind of range,
??, which denotes simultaneously all empty ranges (i, i).
Range restricting the empty string gives ??? = ??. Con-
catenation is defined as ???? = ???? = ?. Both the ceiling
and the floor of ?? are identities, d??e = b??c = ??.
There are two kinds of items. Passive items [A; ?] say
that we have found category A inside the range record ?.
An active item for the rule
A ? f [ ~B] := {?; r = ??; ?}
is of the form
[A ? f [ ~B]; ?, r = ? ? ?, ?; ~?]
where ? is a range record corresponding to the lineariza-
tion rows in ? and ? has been recognized spanning ?.
We are still looking for the rest of the row, ?, and the re-
maining linearization rows ?. ~? is a list of range records
containing information about the daughters ~B.
5.1 Inference rules
There are five inference rules, Predict, Complete, Scan,
Combine and Convert.
Predict
A ? f [ ~B] := {r = ?; ?}
~?? = { }, . . . , { }
[A ? f [ ~B]; {}, r = ?? ? ?, ?; ~??]
For every rule in the grammar, predict a correspond-
ing item that has found the empty range. ~?? is a list
of | ~B| empty range records since nothing has been
found yet.
Complete
[R; ?, r = ? ? ?, {r? = ?; ?}; ~?]
[R; {?; r = ?}, r? = ?? ? ?,?; ~?]
When an item has found an entire linearization row
we continue with the next row by starting it off with
the empty range.
14
Scan
[R; ?, r = ? ? s?, ?; ~?]
?? ? ? ? ?s?
[R; ?, r = ?? ? ?, ?; ~?]
When the next symbol to read is a terminal, its range
restriction is concatenated with the range for what
the row has found so far.
Combine
[A ? f [ ~B]; ?, r = ? ? Bi.r? ?, ?; ~?]
[Bi; ??]
?? ? ? ? ??.r?
?i ? ??
[A ? f [ ~B]; ?, r = ?? ? ?, ?; ~?[i := ??]]
If the next thing to find is a projection on Bi, and
there is a passive item where Bi is the category,
where ?? is consistent with ?i, we can move the dot
past the projection. ?i is updated with ??, since it
might contain more information about the ith daugh-
ter.
Convert
[A ? f [ ~B]; ?, r = ? ? ?, {}; ~?]
[A; {?; r = ?}]
An active item that has fully recognized all its lin-
earization rows is converted to a passive item.
6 The Incremental algorithm
An incremental algorithm reads one token at the time and
calculates all possible consequences of the token before
the next token is read2. The Active algorithm as described
above is not incremental, since we do not know in which
order the linearization rows of a rule are recognized. To
be able to parse incrementally, we have to treat the lin-
earization records as sets of feature-value pairs, instead
of a sequence.
The items for a rule A ? f [ ~B] := ? have the same
form as in the Active algorithm:
[A ? f [ ~B]; ?, r = ? ? ?, ?; ~?]
However, the order between the linearization rows does
not have to be the same as in ?. Note that in this algo-
rithm we do not use passive items. Also note that since
we always know where in the input we are, we cannot
make use of a distinguished ?-range. Another conse-
quence of knowing the current input position is that there
are fewer possible matches for the Combine rule.
2See e.g. the ACL 2004 workshop ?Incremental Parsing:
Bringing Engineering and Cognition Together?.
6.1 Inference rules
There are four inference rules, Predict, Complete, Scan
and Combine.
Predict
A ? f [ ~B] := {?; r = ?; ?}
0 ? k ? |w|
[A ? f [ ~B]; {}, r = (k, k) ? ?, {?;?}; ~??]
An item is predicted for every linearization row r
and every input position k. ~?? is a list of | ~B| empty
range records.
Complete
[R; ?, r = ? ? ?, {?; r? = ?; ?}; ~?]
d?e ? k ? |w|
[R; {?; r = ?}, r? = (k, k) ? ?, {?;?}; ~?]
Whenever a linearization row r is fully traversed, we
predict an item for every remaining linearization row
r? and every remaining input position k.
Scan
[R; ?, r = ? ? s?, ?; ~?]
?? ? ? ? ?s?
[R; ?, r = ?? ? ?, ?; ~?]
If the next symbol in the linearization row is a termi-
nal, its range restriction is concatenated to the range
for the partially recognized row.
Combine
[R; ?, r = ? ? Bi.r? ?, ?; ~?]
[Bi ? . . . ; ??, r? = ?? ? ?, . . . ; . . .]
??? ? ? ? ??
?i ? {??; r? = ??}
[R; ?, r = ??? ? ?, ?; ~?[i := {??; r? = ??}]]
If the next item is a record projection Bi.r?, and
there is an item for Bi which has found r?, then
move the dot forward. The information in ?i must
be consistent with the information found for the Bi
item, {??; r? = ??}.
7 Discussion
We have presented four different parsing algorithms for
LCFRS/MCFG. The algorithms are described as deduc-
tion systems, and in this final section we discuss some
possible optimizations, and complexity issues.
15
7.1 Different prediction strategies
The Predict rule in the above described algorithms is very
crude, predicting an item for each rule in the grammar
(for the Incremental algorithm even for each input po-
sition). A similar context-free prediction rule is called
bottom-up Earley by Sikkel and Nijholt (1997). Such
crude predictions are only intended for educational pur-
poses, since they lead to lots of uninteresting items, and
waste of computing power. For practical purposes there
are two standard context-free prediction strategies, top-
down and bottom-up (see e.g. Wir?n (1992)) and they can
be adapted to the algorithms presented in this paper.
The main idea is that an item for the rule A ? f [ ~B]
with the linearization row r = ? is only predicted if. . .
(Top-down prediction) . . . there is another item looking
for A.r.
(Bottom-up prediction) . . . there is an passive item that
has found the first symbol in ?.
For a more detailed description of these prediction strate-
gies, see Ljungl?f (2004a).
7.2 Efficiency and complexity of the algorithms
The theoretical time complexity for these algorithms is
not better than what has been presented earlier.3 The
complexity arguments are similar and the reader is re-
ferred to Seki et al (1991).
However, theoretical time complexity does not say
much about practical performance, as is already clear
from context-free parsing, where the theoretical time
complexity has remained the same ever since the first
publications (Kasami, 1965; Younger, 1967). There are
two main ways of improving the efficiency of existing
algorithms, which can be called refinement and filtering
(Sikkel and Nijholt, 1997). First, one wants to be able
to locate existing parse items efficiently, e.g. by indexing
some properties in a hash table. This is often done by
refining the parse items or inference rules, increasing the
number of items or deduction steps. Second, it is desir-
able to reduce the number of parse items, which can be
done by filtering out redundant parts of an algorithm.
The algorithms presented in this paper can all be seen
as refinements and filterings of the basic algorithm of
Seki et al (1991):
The na?ve algorithm is a refinement of the basic algo-
rithm, since single items and deduction steps are de-
composed into several different items and smaller
deduction steps.
3Nakanishi et al (1997) reduce the parsing problem to
boolean matrix multiplication, but this can be considered a
purely theoretical result.
The approximative algorithm is both a refinement and
a filtering of the na?ve algorithm; a refinement since
the inference rules Pre-Predict and Pre-Combine are
added, and a filtering since there will hopefully be
less items for Mark-Predict and Mark-Combine to
take care of.
The active algorithm is a refinement of the na?ve algo-
rithm, since the Combine rule is divided into the
rules Complete, Scan and Combine.
The incremental algorithm is finally a refinement of
the active algorithm, since Predict and Complete
can select from any possible remaining linearization
row, and not just the following.
Furthermore, the different prediction strategies (top-
down and bottom-up), become filterings of the algo-
rithms, since they reduce the number of parse items.
7.3 Implementing and testing the algorithms
The algorithms presented in this paper have been im-
plemented in the programming language Haskell, for in-
clusion in the Grammatical Framework system (Ranta,
2004). These implementations are described by Bur-
den (2005). We have also started to implement a selection
of the algorithms in the programming language Prolog.
Preliminary results suggest that the Active algorithm
with bottom-up prediction is a good candidate for parsing
grammars written in the Grammatical Framework. For
a normal sentence in the English resource grammar the
speedup is about 20 times when compared to context-free
parsing and filtering of the parse trees. In the future we
plan to test the different algorithms more extensively.
Acknowledgments
The authors are supported by the EU project TALK (Talk
and Look, Tools for Ambient Linguistic Knowledge),
IST-507802.
References
Pierre Boullier. 2000. Range concatenation grammars.
In 6th International Workshop on Parsing Technolo-
gies, pages 53?64, Trento, Italy.
H?kan Burden. 2005. Implementations of parsing al-
gorithms for linear multiple context-free grammars.
Master?s thesis, G?teborg University, Gothenburg,
Sweden.
Tadao Kasami. 1965. An efficient recognition and syntax
algorithm for context-free languages. Technical Re-
port AFCLR-65-758, Air Force Cambridge Research
Laboratory, Bedford, MA.
16
Peter Ljungl?f. 2004a. Expressivity and Complexity
of the Grammatical Framework. Ph.D. thesis, G?te-
borg University and Chalmers University of Technol-
ogy, Gothenburg, Sweden.
Peter Ljungl?f. 2004b. Grammatical Framework and
multiple context-free grammars. In 9th Conference on
Formal Grammar, Nancy, France.
Ryuichi Nakanishi, Keita Takada, and Hiroyuki Seki.
1997. An efficient recognition algorithm for multi-
ple context-free languages. In MOL5: 5th Meeting on
the Mathematics of Language, pages 119?123, Saar-
br?cken, Germany.
Aarne Ranta. 2004. Grammatical Framework, a type-
theoretical grammar formalism. Journal of Functional
Programming, 14(2):145?189.
Hiroyuki Seki, Takashi Matsumara, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free gram-
mars. Theoretical Computer Science, 88:191?229.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24(1?2):3?
36.
Klaas Sikkel and Anton Nijholt. 1997. Parsing of
context-free languages. In G. Rozenberg and A. Sa-
lomaa, editors, The Handbook of Formal Languages,
volume II, pages 61?100. Springer-Verlag, Berlin.
K. Vijay-Shanker, David Weir, and Aravind Joshi. 1987.
Characterizing structural descriptions produced by var-
ious grammatical formalisms. In 25th Meeting of the
Association for Computational Linguistics.
Mats Wir?n. 1992. Studies in Incremental Natural-
Language Analysis. Ph.D. thesis, Link?ping Univer-
sity, Link?ping, Sweden.
Daniel H Younger. 1967. Recognition of context-
free languages in time n3. Information and Control,
10(2):189?208.
17
