Proceedings of the Workshop on Statistical Machine Translation, pages 31?38,
New York City, June 2006. c?2006 Association for Computational Linguistics
Why Generative Phrase Models Underperform Surface Heuristics
John DeNero, Dan Gillick, James Zhang, Dan Klein
Department of Electrical Engineering and Computer Science
University of California, Berkeley
Berkeley, CA 94705
{denero, dgillick, jyzhang, klein}@eecs.berkeley.edu
Abstract
We investigate why weights from generative mod-
els underperform heuristic estimates in phrase-
based machine translation. We first propose a sim-
ple generative, phrase-based model and verify that
its estimates are inferior to those given by surface
statistics. The performance gap stems primarily
from the addition of a hidden segmentation vari-
able, which increases the capacity for overfitting
during maximum likelihood training with EM. In
particular, while word level models benefit greatly
from re-estimation, phrase-level models do not: the
crucial difference is that distinct word alignments
cannot all be correct, while distinct segmentations
can. Alternate segmentations rather than alternate
alignments compete, resulting in increased deter-
minization of the phrase table, decreased general-
ization, and decreased final BLEU score. We also
show that interpolation of the two methods can re-
sult in a modest increase in BLEU score.
1 Introduction
At the core of a phrase-based statistical machine
translation system is a phrase table containing
pairs of source and target language phrases, each
weighted by a conditional translation probability.
Koehn et al (2003a) showed that translation qual-
ity is very sensitive to how this table is extracted
from the training data. One particularly surprising
result is that a simple heuristic extraction algorithm
based on surface statistics of a word-aligned training
set outperformed the phrase-based generative model
proposed by Marcu and Wong (2002).
This result is surprising in light of the reverse sit-
uation for word-based statistical translation. Specif-
ically, in the task of word alignment, heuristic ap-
proaches such as the Dice coefficient consistently
underperform their re-estimated counterparts, such
as the IBM word alignment models (Brown et al,
1993). This well-known result is unsurprising: re-
estimation introduces an element of competition into
the learning process. The key virtue of competition
in word alignment is that, to a first approximation,
only one source word should generate each target
word. If a good alignment for a word token is found,
other plausible alignments are explained away and
should be discounted as incorrect for that token.
As we show in this paper, this effect does not pre-
vail for phrase-level alignments. The central differ-
ence is that phrase-based models, such as the ones
presented in section 2 or Marcu and Wong (2002),
contain an element of segmentation. That is, they do
not merely learn correspondences between phrases,
but also segmentations of the source and target sen-
tences. However, while it is reasonable to sup-
pose that if one alignment is right, others must be
wrong, the situation is more complex for segmenta-
tions. For example, if one segmentation subsumes
another, they are not necessarily incompatible: both
may be equally valid. While in some cases, such
as idiomatic vs. literal translations, two segmenta-
tions may be in true competition, we show that the
most common result is for different segmentations
to be recruited for different examples, overfitting the
training data and overly determinizing the phrase
translation estimates.
In this work, we first define a novel (but not rad-
ical) generative phrase-based model analogous to
IBM Model 3. While its exact training is intractable,
we describe a training regime which uses word-
level alignments to constrain the space of feasible
segmentations down to a manageable number. We
demonstrate that the phrase analogue of the Dice co-
efficient is superior to our generative model (a re-
sult also echoing previous work). In the primary
contribution of the paper, we present a series of ex-
periments designed to elucidate what re-estimation
learns in this context. We show that estimates are
overly determinized because segmentations are used
31
in unintuitive ways for the sake of data likelihood.
We comment on both the beneficial instances of seg-
ment competition (idioms) as well as the harmful
ones (most everything else). Finally, we demon-
strate that interpolation of the two estimates can
provide a modest increase in BLEU score over the
heuristic baseline.
2 Approach and Evaluation Methodology
The generative model defined below is evaluated
based on the BLEU score it produces in an end-
to-end machine translation system from English to
French. The top-performing diag-and extraction
heuristic (Zens et al, 2002) serves as the baseline for
evaluation.1 Each approach ? the generative model
and heuristic baseline ? produces an estimated con-
ditional distribution of English phrases given French
phrases. We will refer to the distribution derived
from the baseline heuristic as ?H . The distribution
learned via the generative model, denoted ?EM , is
described in detail below.
2.1 A Generative Phrase Model
While our model for computing ?EM is novel, it
is meant to exemplify a class of models that are
not only clear extensions to generative word align-
ment models, but also compatible with the statistical
framework assumed during phrase-based decoding.
The generative process we modeled produces a
phrase-aligned English sentence from a French sen-
tence where the former is a translation of the lat-
ter. Note that this generative process is opposite to
the translation direction of the larger system because
of the standard noisy-channel decomposition. The
learned parameters from this model will be used to
translate sentences from English to French. The gen-
erative process modeled has four steps:2
1. Begin with a French sentence f.
1This well-known heuristic extracts phrases from a sentence
pair by computing a word-level alignment for the sentence and
then enumerating all phrases compatible with that alignment.
The word alignment is computed by first intersecting the direc-
tional alignments produced by a generative IBM model (e.g.,
model 4 with minor enhancements) in each translation direc-
tion, then adding certain alignments from the union of the di-
rectional alignments based on local growth rules.
2Our notation matches the literature for phrase-based trans-
lation: e is an English word, e? is an English phrase, and e?I1 is a
sequence of I English phrases, and e is an English sentence.
2. Segment f into a sequence of I multi-word
phrases that span the sentence, f? I1 .
3. For each phrase f?i ? f? I1 , choose a correspond-
ing position j in the English sentence and es-
tablish the alignment aj = i, then generate ex-
actly one English phrase e?j from f?i.
4. The sequence e?j ordered by a describes an En-
glish sentence e.
The corresponding probabilistic model for this gen-
erative process is:
P (e|f) =
?
f?I1 ,e?
I
1,a
P (e, f? I1 , e?
I
1, a|f)
=
?
f?I1 ,e?
I
1,a
?(f? I1 |f)
?
f?i?f?I1
?(e?j |f?i)d(aj = i|f)
where P (e, f? I1 , e?
I
1, a|f) factors into a segmentation
model ?, a translation model ? and a distortion
model d. The parameters for each component of this
model are estimated differently:
? The segmentation model ?(f? I1 |f) is assumed to
be uniform over all possible segmentations for
a sentence.3
? The phrase translation model ?(e?j |f?i) is pa-
rameterized by a large table of phrase transla-
tion probabilities.
? The distortion model d(aj = i|f) is a discount-
ing function based on absolute sentence posi-
tion akin to the one used in IBM model 3.
While similar to the joint model in Marcu and Wong
(2002), our model takes a conditional form com-
patible with the statistical assumptions used by the
Pharaoh decoder. Thus, after training, the param-
eters of the phrase translation model ?EM can be
used directly for decoding.
2.2 Training
Significant approximation and pruning is required
to train a generative phrase model and table ? such
as ?EM ? with hidden segmentation and alignment
variables using the expectation maximization algo-
rithm (EM). Computing the likelihood of the data
3This segmentation model is deficient given a maximum
phrase length: many segmentations are disallowed in practice.
32
for a set of parameters (the e-step) involves summing
over exponentially many possible segmentations for
each training sentence. Unlike previous attempts to
train a similar model (Marcu and Wong, 2002), we
allow information from a word-alignment model to
inform our approximation. This approach allowed
us to directly estimate translation probabilities even
for rare phrase pairs, which were estimated heuristi-
cally in previous work.
In each iteration of EM, we re-estimate each
phrase translation probability by summing fractional
phrase counts (soft counts) from the data given the
current model parameters.
?new(e?j |f?i) =
c(f?i, e?j)
c(f?i)
=
?
(f,e)
?
f?I1 :f?i?f?
I
1
?
e?I1:e?j?e?
I
1
?
a:aj=i P (e, f?
I
1 , e?
I
1, a|f)
?
f?I1 :f?i?f?
I
1
?
e?I1
?
a P (e, f?
I
1 , e?
I
1, a|f)
This training loop necessitates approximation be-
cause summing over all possible segmentations and
alignments for each sentence is intractable, requiring
time exponential in the length of the sentences. Ad-
ditionally, the set of possible phrase pairs grows too
large to fit in memory. Using word alignments, we
can address both problems.4 In particular, we can
determine for any aligned segmentation (f? I1 , e?
I
1, a)
whether it is compatible with the word-level align-
ment for the sentence pair. We define a phrase pair
to be compatible with a word-alignment if no word
in either phrase is aligned with a word outside the
other phrase (Zens et al, 2002). Then, (f? I1 , e?
I
1, a)
is compatible with the word-alignment if each of its
aligned phrases is a compatible phrase pair.
The training process is then constrained such that,
when evaluating the above sum, only compatible
aligned segmentations are considered. That is, we
allow P (e, f? I1 , e?
I
1, a|f) > 0 only for aligned seg-
mentations (f? I1 , e?
I
1, a) such that a provides a one-
to-one mapping from f? I1 to e?
I
1 where all phrase pairs
(f?aj , e?j) are compatible with the word alignment.
This constraint has two important effects. First,
we force P (e?j |f?i) = 0 for all phrase pairs not com-
patible with the word-level alignment for some sen-
tence pair. This restriction successfully reduced the
4The word alignments used in approximating the e-step
were the same as those used to create the heuristic diag-and
baseline.
total legal phrase pair types from approximately 250
million to 17 million for 100,000 training sentences.
However, some desirable phrases were eliminated
because of errors in the word alignments.
Second, the time to compute the e-step is reduced.
While in principle it is still intractable, in practice
we can compute most sentence pairs? contributions
in under a second each. However, some spurious
word alignments can disallow all segmentations for
a sentence pair, rendering it unusable for training.
Several factors including errors in the word-level
alignments, sparse word alignments and non-literal
translations cause our constraint to rule out approx-
imately 54% of the training set. Thus, the reduced
size of the usable training set accounts for some of
the degraded performance of ?EM relative to ?H .
However, the results in figure 1 of the following sec-
tion show that ?EM trained on twice as much data
as ?H still underperforms the heuristic, indicating a
larger issue than decreased training set size.
2.3 Experimental Design
To test the relative performance of ?EM and ?H ,
we evaluated each using an end-to-end translation
system from English to French. We chose this non-
standard translation direction so that the examples
in this paper would be more accessible to a primar-
ily English-speaking audience. All training and test
data were drawn from the French/English section of
the Europarl sentence-aligned corpus. We tested on
the first 1,000 unique sentences of length 5 to 15 in
the corpus and trained on sentences of length 1 to 60
starting after the first 10,000.
The system follows the structure proposed in
the documentation for the Pharaoh decoder and
uses many publicly available components (Koehn,
2003b). The language model was generated from
the Europarl corpus using the SRI Language Model-
ing Toolkit (Stolcke, 2002). Pharaoh performed de-
coding using a set of default parameters for weight-
ing the relative influence of the language, translation
and distortion models (Koehn, 2003b). A maximum
phrase length of three was used for all experiments.
To properly compare ?EM to ?H , all aspects of
the translation pipeline were held constant except for
the parameters of the phrase translation table. In par-
ticular, we did not tune the decoding hyperparame-
ters for the different phrase tables.
33
Source 25k 50k 100kHeuristic 0.3853 0.3883 0.3897Iteration 1 0.3724 0.3775 0.3743Iteration 2 0.3735 0.3851 0.3814iteration 3 0.3705 0.384 0.3827Iteration 4 0.3695 0.285 0.3801iteration 5 0.3705 0.284 0.3774interpSource 25k 50k 100kHeuristic 0.3853 0.3883 0.3897Iteration 1 0.3724 0.3775 0.3743iteration 3 0.3705 0.384 0.3827iteration 3 0.3705 0.384 0.3827
0.360.37
0.380.39
0.40
25k 50k 100kTraining sentences
BLEU
HeuristicIteration 1iteration 3
0%20%
40%60%
80%100%
0 10 20 30 40 50 60Sentence Length
Senten
ces Sk
ipped
Figure 1: Statistical re-estimation using a generative
phrase model degrades BLEU score relative to its
heuristic initialization.
3 Results
Having generated ?H heuristically and ?EM with
EM, we now compare their performance. While the
model and training regimen for ?EM differ from the
model from Marcu and Wong (2002), we achieved
results similar to Koehn et al (2003a): ?EM slightly
underperformed ?H . Figure 1 compares the BLEU
scores using each estimate. Note that the expecta-
tion maximization algorithm for training ?EM was
initialized with the heuristic parameters ?H , so the
heuristic curve can be equivalently labeled as itera-
tion 0.
Thus, the first iteration of EM increases the ob-
served likelihood of the training sentences while si-
multaneously degrading translation performance on
the test set. As training proceeds, performance on
the test set levels off after three iterations of EM. The
system never achieves the performance of its initial-
ization parameters. The pruning of our training regi-
men accounts for part of this degradation, but not all;
augmenting ?EM by adding back in all phrase pairs
that were dropped during training does not close the
performance gap between ?EM and ?H .
3.1 Analysis
Learning ?EM degrades translation quality in large
part because EM learns overly determinized seg-
mentations and translation parameters, overfitting
the training data and failing to generalize. The pri-
mary increase in richness from generative word-
level models to generative phrase-level models is
due to the additional latent segmentation variable.
Although we impose a uniform distribution over
segmentations, it nonetheless plays a crucial role
during training. We will characterize this phe-
nomenon through aggregate statistics and transla-
tion examples shortly, but begin by demonstrating
the model?s capacity to overfit the training data.
Let us first return to the motivation behind in-
troducing and learning phrases in machine transla-
tion. For any language pair, there are contiguous
strings of words whose collocational translation is
non-compositional; that is, they translate together
differently than they would in isolation. For in-
stance, chat in French generally translates to cat in
English, but appeler un chat un chat is an idiom
which translates to call a spade a spade. Introduc-
ing phrases allows us to translate chat un chat atom-
ically to spade a spade and vice versa.
While introducing phrases and parameterizing
their translation probabilities with a surface heuris-
tic allows for this possibility, statistical re-estimation
would be required to learn that chat should never be
translated to spade in isolation. Hence, translating I
have a spade with ?H could yield an error.
But enforcing competition among segmentations
introduces a new problem: true translation ambigu-
ity can also be spuriously explained by the segmen-
tation. Consider the french fragment carte sur la
table, which could translate to map on the table or
notice on the chart. Using these two sentence pairs
as training, one would hope to capture the ambiguity
in the parameter table as:
French English ?(e|f)
carte map 0.5
carte notice 0.5
carte sur map on 0.5
carte sur notice on 0.5
sur on 1.0
... ... ...
table table 0.5
table chart 0.5
Assuming we only allow non-degenerate seg-
mentations and disallow non-monotonic alignments,
this parameter table yields a marginal likelihood
P (f|e) = 0.25 for both sentence pairs ? the intu-
itive result given two independent lexical ambigu-
34
ities. However, the following table yields a likeli-
hood of 0.28 for both sentences:5
French English ?(e|f)
carte map 1.0
carte sur notice on 1.0
carte sur la notice on the 1.0
sur on 1.0
sur la table on the table 1.0
la the 1.0
la table the table 1.0
table chart 1.0
Hence, a higher likelihood can be achieved by al-
locating some phrases to certain translations while
reserving overlapping phrases for others, thereby
failing to model the real ambiguity that exists across
the language pair. Also, notice that the phrase sur
la can take on an arbitrary distribution over any en-
glish phrases without affecting the likelihood of ei-
ther sentence pair. Not only does this counterintu-
itive parameterization give a high data likelihood,
but it is also a fixed point of the EM algorithm.
The phenomenon demonstrated above poses a
problem for generative phrase models in general.
The ambiguous process of translation can be mod-
eled either by the latent segmentation variable or the
phrase translation probabilities. In some cases, opti-
mizing the likelihood of the training corpus adjusts
for the former when we would prefer the latter. We
next investigate how this problem manifests in ?EM
and its effect on translation quality.
3.2 Learned parameters
The parameters of ?EM differ from the heuristically
extracted parameters ?H in that the conditional dis-
tributions over English translations for some French
words are sharply peaked for ?EM compared to flat-
ter distributions generated by ?H . This determinism
? predicted by the previous section?s example ? is
not atypical of EM training for other tasks.
To quantify the notion of peaked distributions
over phrase translations, we compute the entropy of
the distribution for each French phrase according to
5For example, summing over the first translation ex-
pands to 17 (?(map | carte)?(on the table | sur la table)
+?(map | carte)?(on | sur)?(the table | la table)).
it 2.76E-08 as there are 0.073952202code 2.29E-08 the 0.002670946to 1.98E-12 less helpful 6.22E-05it be 1.11E-14 please stop messing 1.12E-05
0 10 20 30 400 - .01
.01 - .5.5 - 1
1 - 1.51.5 - 2
> 2
Entrop
y
% Phrase TranslationsLearnedHeuristic
1E-04 1E-02 1E+00 1E+02',de.lall 'leetlesMost
 Comm
on Fre
nch Ph
rases
EntropyLearned Heuristic
Figure 2: Many more French phrases have very low
entropy under the learned parameterization.
the standard definition.
H(?(e?|f?)) =
?
e?
?(e?|f?) log2 ?(e?|f?)
The average entropy, weighted by frequency, for the
most common 10,000 phrases in the learned table
was 1.55, comparable to 3.76 for the heuristic table.
The difference between the tables becomes much
more striking when we consider the histogram of
entropies for phrases in figure 2. In particular, the
learned table has many more phrases with entropy
near zero. The most pronounced entropy differences
often appear for common phrases. Ten of the most
common phrases in the French corpus are shown in
figure 3.
As more probability mass is reserved for fewer
translations, many of the alternative translations un-
der ?H are assigned prohibitively small probabili-
ties. In translating 1,000 test sentences, for example,
no phrase translation with ?(e?|f?) less than 10?5 was
used by the decoder. Given this empirical threshold,
nearly 60% of entries in ?EM are unusable, com-
pared with 1% in ?H .
3.3 Effects on Translation
While this determinism of ?EM may be desirable
in some circumstances, we found that the ambi-
guity in ?H is often preferable at decoding time.
35
it 2.76E-08 as there are 0.073952202code 2.29E-08 the 0.002670946to 1.98E-12 less helpful 6.22E-05it be 1.11E-14 please stop messing 1.12E-05
01020
3040
0 - .01 .01 - .5 .5 - 1 1 - 1.5 1.5 - 2 > 2Entropy
% Phr
ase 
Transl
ations HeuristicLearned
1E-04 1E-02 1E+00 1E+02 ',.ll 'n 'quequiplusl ' unionC
ommo
n Fren
ch Phr
ases
EntropyLearned Heuristic
Figure 3: Entropy of 10 common French phrases.
Several learned distributions have very low entropy.
In particular, the pattern of translation-ambiguous
phrases receiving spuriously peaked distributions (as
described in section 3.1) introduces new tra slation
errors relative to the baseline. We now investigate
both positive and negative effects of the learning
process.
The issue that motivated training a generative
model is sometimes resolved correctly: for a word
that translates differently alone than in the context
of an idiom, the translation probabilities can more
accurately reflect this. Returning to the previous ex-
ample, the phrase table for chat has been corrected
through the learning process. The heuristic process
gives the incorrect translation spade with 61% prob-
ability, while the statistical learning approach gives
cat with 95% probability.
While such examples of improvement are en-
couraging, the trend of spurious determinism over-
whelms this benefit by introducing errors in four re-
lated ways, each of which will be explored in turn.
1. Useful phrase pairs can be assigned very low
probabilities and therefore become unusable.
2. A proper translation for a phrase can be over-
ridden by another translation with spuriously
high probability.
3. Error-prone, common, ambiguous phrases be-
come active during decoding.
4. The language model cannot distinguish be-
tween different translation options as effec-
tively due to deterministic translation model
distributions.
The first effect follows from our observation in
section 3.2 that many phrase pairs are unusable due
to vanishingly small probabilities. Some of the en-
tries that are made unusable by re-estimation are
helpful at decoding time, evidenced by the fact
that pruning the set of ?EM ?s low-scoring learned
phrases from the original heuristic table reduces
BLEU score by 0.02 for 25k training sentences (be-
low the score for ?EM ).
The second effect is more subtle. Consider the
sentence in figure 4, which to a first approxima-
tion can be translated as a series of cognates, as
demonstrated by the decoding that follows from the
heuristic parameterization ?H .6 Notice also that the
translation probabilities from heuristic extraction are
non-deterministic. On the other hand, the translation
system makes a significant lexical error on this sim-
ple sentence when parameterized by ?EM : the use
of caracte?rise in this context is incorrect. This error
arises from a sharply peaked distribution over En-
glish phrases for caracte?rise.
This example illustrates a recurring problem: er-
rors do not necessarily arise because a correct trans-
lation is not available. Notice that a preferable trans-
lation of degree as degre? is available under both pa-
rameterizations. Degre? is not used, however, be-
cause of the peaked distribution of a competing
translation candidate. In this way, very high prob-
ability translations can effectively block the use of
more appropriate translations at decoding time.
What is furthermore surprising and noteworthy in
this example is that the learned, near-deterministic
translation for caracte?rise is not a common trans-
lation for the word. Not only does the statistical
learning process yield low-entropy translation dis-
tributions, but occasionally the translation with un-
desirably high conditional probability does not have
a strong surface correlation with the source phrase.
This example is not unique; during different initial-
izations of the EM algorithm, we noticed such pat-
6While there is some agreement error and awkwardness, the
heuristic translation is comprehensible to native speakers. The
learned translation incorrectly translates degree, degrading the
translation quality.
36
the situation varies to an
la situation varie d ' une
Heuristically Extracted Phrase Table
Learned Phrase Table
enormous
immense
degree
degr?
situation varies to
la varie d '
an enormous
une immense
degree
caract?rise
the
situation
caracte?rise
English ?(e|f)
degree 0.998
characterises 0.001
characterised 0.001
caracte?rise
English ?(e|f)
characterises 0.49
characterised 0.21
permeate 0.05
features 0.05
typifies 0.05
degr e?
English ?(e|f)
degree 0.49
level 0.38
extent 0.02
amount 0.02
how 0.01
degre?
English ?(e|f)
degree 0.64
level 0.26
extent 0.10
Figure 4: Spurious determinism in the learned phrase parameters degrades translation quality.
terns even for common French phrases such as de
and ne.
The third source of errors is closely related: com-
mon phrases that translate in many ways depending
on the context can introduce errors if they have a
spuriously peaked distribution. For instance, con-
sider the lone apostrophe, which is treated as a sin-
gle token in our data set (figure 5). The shape of
the heuristic translation distribution for the phrase is
intuitively appealing, showing a relatively flat dis-
tribution among many possible translations. Such
a distribution has very high entropy. On the other
hand, the learned table translates the apostrophe to
the with probability very near 1.
Heuristic
English ?H(e|f)
our 0.10
that 0.09
is 0.06
we 0.05
next 0.05
Learned
English ?EM (e|f)
the 0.99
, 4.1 ? 10?3
is 6.5 ? 10?4
to 6.3 ? 10?4
in 5.3 ? 10?4
Figure 5: Translation probabilities for an apostro-
phe, the most common french phrase. The learned
table contains a highly peaked distribution.
Such common phrases whose translation depends
highly on the context are ripe for producing transla-
tion errors. The flatness of the distribution of ?H en-
sures that the single apostrophe will rarely be used
during decoding because no one phrase table entry
has high enough probability to promote its use. On
the other hand, using the peaked entry ?EM (the|?)
incurs virtually no cost to the score of a translation.
The final kind of errors stems from interactions
between the language and translation models. The
selection among translation choices via a language
model ? a key virtue of the noisy channel frame-
work ? is hindered by the determinism of the transla-
tion model. This effect appears to be less significant
than the previous three. We should note, however,
that adjusting the language and translation model
weights during decoding does not close the perfor-
mance gap between ?H and ?EM .
3.4 Improvements
In light of the low entropy of ?EM , we could hope to
improve translations by retaining entropy. There are
several strategies we have considered to achieve this.
Broadly, we have tried two approaches: combin-
ing ?EM and ?H via heuristic interpolation methods
and modifying the training loop to limit determin-
ism.
The simplest strategy to increase entropy is to
interpolate the heuristic and learned phrase tables.
Varying the weight of interpolation showed an im-
provement over the heuristic of up to 0.01 for 100k
sentences. A more modest improvement of 0.003 for
25k training sentences appears in table 1.
In another experiment, we interpolated the out-
put of each iteration of EM with its input, thereby
maintaining some entropy from the initialization pa-
rameters. BLEU score increased to a maximum of
0.394 using this technique with 100k training sen-
tences, outperforming the heuristic by a slim margin
of 0.005.
We might address the determinization in ?EM
without resorting to interpolation by modifying the
37
training procedure to retain entropy. By imposing a
non-uniform segmentation model that favors shorter
phrases over longer ones, we hope to prevent the
error-causing effects of EM training outlined above.
In principle, this change will encourage EM to ex-
plain training sentences with shorter sentences. In
practice, however, this approach has not led to an
improvement in BLEU.
Another approach to maintaining entropy during
the training process is to smooth the probabilities
generated by EM. In particular, we can use the fol-
lowing smoothed update equation during the train-
ing loop, which reserves a portion of probability
mass for unseen translations.
?new(e?j |f?i) =
c(f?i, e?j)
c(f?i) + kl?1
In the equation above, l is the length of the French
phrase and k is a tuning parameter. This formula-
tion not only serves to reduce very spiked probabili-
ties in ?EM , but also boosts the probability of short
phrases to encourage their use. With k = 2.5, this
smoothing approach improves BLEU by .007 using
25k training sentences, nearly equaling the heuristic
(table 1).
4 Conclusion
Re-estimating phrase translation probabilities using
a generative model holds the promise of improving
upon heuristic techniques. However, the combina-
torial properties of a phrase-based generative model
have unfortunate side effects. In cases of true ambi-
guity in the language pair to be translated, parameter
estimates that explain the ambiguity using segmen-
tation variables can in some cases yield higher data
likelihoods by determinizing phrase translation esti-
mates. However, this behavior in turn leads to errors
at decoding time.
We have also shown that some modest benefit can
be obtained from re-estimation through the blunt in-
strument of interpolation. A remaining challenge is
to design more appropriate statistical models which
tie segmentations together unless sufficient evidence
of true non-compositionality is present; perhaps
such models could properly combine the benefits of
both current approaches.
Estimate BLEU
?H 0.385
?H phrase pairs that also appear in ?EM 0.365
?EM 0.374
?EM with a non-uniform segmentation model 0.374
?EM with smoothing 0.381
?EM with gaps filled in by ?H 0.374
?EM interpolated with ?H 0.388
Table 1: BLEU results for 25k training sentences.
5 Acknowledgments
We would like to thank the anonymous reviewers for
their valuable feedback on this paper.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2), 1993.
Philipp Koehn. Europarl: A Multilingual Corpus for
Evaluation of Machine Translation. USC Information
Sciences Institute, 2002.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. Sta-
tistical phrase-based translation. HLT-NAACL, 2003.
Philipp Koehn. Pharaoh: A Beam Search Decoder for
Phrase-Based Statisical Machine Translation Models.
USC Information Sciences Institute, 2003.
Daniel Marcu and William Wong. A phrase-based, joint
probability model for statistical machine translation.
Conference on Empirical Methods in Natual Language
Processing, 2002.
Franz Josef Och and Hermann Ney. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51, 2003.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
Improved alignment models for statistical machine
translation. ACL Workshops, 1999.
Andreas Stolcke. Srilm ? an extensible language model-
ing toolkit. Proceedings of the International Confer-
ence on Statistical Language Processing, 2002.
Richard Zens, Franz Josef Och and Hermann Ney.
Phrase-Based Statistical Machine Translation. Annual
German Conference on AI, 2002.
38
