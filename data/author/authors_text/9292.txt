A Trigger Language Model-based IR System 
ZHANG Jun-lin   SUN Le   QU Wei-min   SUN Yu-fang  
 
Open System & Chinese Information Processing Center 
Institute of Software, The Chinese Academy of Sciences 
P.O.BOX 8718,Beijing 100080 
junlin01@iscas.cn
 
Abstract 
Language model based IR system proposed in 
recent 5 years has introduced the language 
model approach in the speech recognition 
area into the IR community and improves the 
performance of the IR system effectively. 
However, the assumption that all the indexed 
words are irrelative behind the method is not 
the truth. Though statistical MT approach 
alleviates the situation by taking the 
synonymy factor into account, it never helps 
to judge the different meanings of the same 
word in varied context. In this paper we 
propose the trigger language model based IR 
system to resolve the problem. Firstly we 
compute the mutual information of the words 
from training corpus and then design the 
algorithm to get the triggered words of the 
query in order to fix down the topic of query 
more clearly. We introduce the relative 
parameters into the document language model 
to form the trigger language model based IR 
system. Experiments show that the 
performance of trigger language model based 
IR system has been improved greatly. The 
precision of trigger language model increased 
12% and recall increased nearly 10.8% 
compared with Ponte language model 
method. 
 
1 Introduction 
 
Using language models for information 
retrieval has been studied extensively 
recently(Jin et al2002 Lafferty and Zhai 2001 
Srikanth and Srihari 2002  Lavrenko and Croft 
2001 Liu and Croft 2002). The basic idea is to 
compute the conditional probability P(Q|D), i.e. 
the probability of generating a query Q given the 
observation of a document D. Several different 
methods have been applied to compute this 
conditional probability. In most approaches, the 
computation is conceptually decomposed into 
two distinct steps: (1) Estimating a document 
language model; (2) Computing the query 
likelihood using the estimated document model 
based on some query model. For example, Ponte 
and Croft emphasized the first step, and used 
several heuristics to smooth the Maximum 
Likelihood of the document language model, and 
assumed that the query is generated under a 
multivariate Bernoulli model (Ponte and Croft 
1998). The BBN method (Miller et al1999) 
emphasized the second step and used a two-state 
hidden Markov model as the basis for generating 
queries, which, in effect, is to smooth the MLE 
with linear interpolation, a strategy also adopted 
in Hiemstra and Kraaij (Hiemstra and  Kraaij 
1999). In Zhai and Lafferty (Zhai and  Lafferty 
2001), it has been found that the retrieval 
performance is affected by both the estimation 
accuracy of document language models and the 
appropriate modeling of the query, and a two 
stage smoothing method was suggested to 
explicitly address these two distinct steps. 
It?s not hard to see that the unigram 
language model IR method contains the 
following assumption: Each word appearing in 
the document set and query has nothing to do 
with any other word. Obviously this assumption 
is not true in reality. Though statistical MT 
approach (Berger and  Lafferty 1999 ) alleviates 
the situation by taking the synonymy factor into 
account, it never helps to judge the different 
meanings of the same word in varied context. In 
this paper we propose the trigger language model 
based IR system to resolve the problem. Though 
the basic idea of using the triggered words to 
improve the performance of language model was 
proposed by Raymond almost 10 years ago 
(Raymond et al1993), Our method adopts a 
different approach for other objectivity in the IR 
field. Firstly we compute the mutual information 
of the words from training corpus and then 
design the algorithm to get the triggered words of 
the query in order to fix down the topic of query 
more clearly. We introduce the relative 
parameters into the document language model to 
form the trigger language model based IR system. 
Experiments show that the performance of trigger 
language model based IR system has been 
improved greatly.   
In what follows, Section 2 describes trigger 
language model based IR system in detail. 
Section 3 is our evaluation about the model. 
Finally, Section 4 summarizes the work in this 
paper. 
2 Trigger Language Model based IR 
System 
 
2.1 Inter-relationship of Indexing Words 
  
In order to find out the inter-relationship of 
words in some specific context, we consider the 
co-occurring times of different words within 
fixed sized text window of the document. When 
the co-occurring time is large enough, we think 
that relationship is meaningful. Mutual 
Information is a common tool to be applied under 
this situation. So we compute the mutual 
information as following: 
 
)()()1(
),,(
)()(
)1(
),,(
),(
baw
wwba
w
b
w
a
ww
wba
ba
wNwNL
NLwwN
N
wN
N
wN
LN
LwwN
ww
???
?=
???
?
???
?????
?
???
?
??=?
             
(1) 
  
where  denotes the size of the vocabulary, 
 is the co-occurring times of 
word  and  within   sized window 
in training set.  is the count of the word 
 appearing in the training set and  is 
the count of word  appearing in the training 
set. 
wN
, wb L ),( a wwN
aw
aw
bw
(N
wL
)aw
bw
)( bwN
We use the corpus provided by IR task of 
NTCIR2 (NTCIR 2002) as the training set to 
compute the mutual information of words. This 
corpus contains nearly 100 thousands news 
articles encoding in BIG5 charset. We think the 
mutual information which is larger than 25 is 
meaningful. Considering the stop words in 
document or query are useless to represent the 
content, we remove 200 highest frequent words 
from the document before computation. Table 1 
shows some examples with higher mutual 
information. 
 
? ? 
(test) 
? ? ? (alphabet):1895 
??(rail):1353  
? ? (delimitation):758 
? ? ? ?
(windtunnel):473   
???(meter):421   
?? (test paper):403  
? ?
(missile) 
? ? ? ?
(antiaircraft):1063  
??(develop):708 
? ?? (long-range):472 
???(anti-tank):354 
?? 
(bribe) 
? ? (tax dodging):3462 
????(jobbery):2603 
????(FBI):1041 
???(voter):730 
??(zhanjiang):478 
???(Utah):427  
? ? ?
(truculency)
????(scrutator):710 
? ? ? ? (long-range 
missile):497  
????(terrorism):457 
? ? (biochemistry):390 
??(equipoise):327 
??(plague):334   
???(Bagdad):325   
 
      Table 1. Examples of Mutual Information 
2.2 Algorithm of Triggered Words by 
Query  
Generally speaking, a word always 
represents many different meanings and its exact 
meaning adopted in specific topic can be 
determined by the co-occurring words in its 
context. Different meaning of a word often lead 
to the different vocabulary set of related word. 
In order to find out the exact meaning of the 
words contained by the query in IR system, we 
design the algorithm to compute the triggered 
vocabularies of query. It is just these triggered 
words that show the exact meaning of the words 
in query in some specific context and help fix 
down the topic of query more clearly. The basic 
idea behind the algorithm is as following: By 
computing the mutual information, we can derive 
the relative words of a query word. All these 
words mean the semantically related vocabularies 
of the query word under different contexts. We 
propose that if the intersection of the derived 
related words of different words in query is not 
null, the words in the intersection is useful to 
judge the exact meaning of the words in query. 
At the same time, the more times an intersection 
word appears in related vocabulary set of 
different query word, the higher the weight of 
this word to fix down the topic of the query is. So 
we design the following algorithm to compute 
the triggered vocabulary set of query:     
 
Algorithm 1:Triggered vocabularies by query 
Input: Vocabulary set I of query word and its 
co-occurring words after removing the stop 
words in the query. 
},......,,......,,,{ 2211 ><><><><= nnii SqSqSqSqI
Output: Triggered vocabulary set T. 
 
Setp 1. Initialize the set ?=T . 
Setp 2. for(i =2;i<=n;i++) 
{ 
for(j=1;j<= ;j++) inC
{ 
2.1get the different 
combination },......,,,{ ,,2,2,1,1, ><><><= ijijjjjjj SqSqSqL
i
 
which contains  elements from set I ; 
2.2 if any vocabulary set )1(, ikS kj <=<  
in  contains no element, then we turn to 2.4 , 
otherwise we turn to 2.3; 
jL
2.3 Compute the intersection  of all 
vocabulary set  in . 
Here
jiT ,
< mw
)1(, ikS kj <=<
,......,, 221 ><> w
jL
>i },,{ 1, <=ji wT ??? ,
where
2
log i
w =? , ( iw <==<1 ). w?  is the 
word  weight decided by the length of ; jL
jiTT ,?=
w?
q
,,{ 11 ,,......, 22 <>>< mww ???
+? )|( jij dqp
><><=
=
other
ww
dq
q
ji
)
>m
)( i
cs
q
<wm,,,,{ 2211 ??
},.... )(Qli q
?
2.4 T , adopting the higher word 
weight during the merging process; 
} 
}  
Step 3. Output the triggered vocabulary set T ;    
      
2.3 Similarity Computation of Query and 
Document 
We use the similar strategy with Ponte 
language model method (Ponte and Croft 1998) 
to compute the similarity between the query and 
the document. That is, we firstly construct the 
simple language model according to the 
statistical information of vocabulary and then 
compute the generative probability of the query. 
The difference is that the trigger language model 
method takes the context information of a word 
into account. So we compute the triggered words 
set of query  according to algorithm 1.This 
way we get the triggered vocabulary set  
}><= mq wT . 
This set contains the words triggered by query 
and it is these triggered words that determine the 
exact meaning of the vocabularies in query 
among the several optional choices. This helps 
fix down the topic of query more clearly.   
Introducing the triggered words factor into the 
document language model, we can form the 
trigger language model based information 
retrieval system. 
The similarity of query and document can 
be computed as following: 
? ?
= =
=
)(
1
)(
1
()|(
Ql
i
dl
j
d
tf
CMQP   (2) 
??
??
?
???= Tddqdqp jjijji
0
}),......()(
1
)|( ?  (3) 
 
(1) ,......,{ 21 qqqQ =  denotes query 
and is the length of the query;  )(Ql
(2) denotes the trigger language model of 
document ;  
dM
d
(3) denotes a 
document in document set and is the length 
of the document; 
},....,....,{ )(21 dlj ddddd =
(l )d
(4) 
)(
)(
dl
df
C jj =
jd
 is the weight parameter of 
words  in a document. Here  means 
the account of the words  appearing in the 
document. 
)( jdf
jd
(5)  denotes the probability of  
being triggered by the document word .When 
2 words are same, the probability equals 1. If 
they are different and the word  belongs to 
the triggered vocabulary set of query, the 
probability equals the according parameter in the 
,otherwise the probability is 0? 
)|( ji dqp iq
jd
jd
qT
(6)
cs
qtf i )(
)iq
iq
is used for data smoothing; here 
 denotes times of query word  
appearing in document set and   denotes the 
total length of documents which contains the 
word . 
(tf iq
cs
 
3 Experiment Results  
3.1 Corpus 
 
The corpus we used to evaluate the 
performance of our proposed trigger language 
model IR system is the document set offered by 
the traditional Chinese Document set of NTCIR3 
for the IR task. The corpus consists of 381681 
news articles from Hong Kong and Taiwan with 
varied topics. After the word segmentation, the 
document set contains 150700953 words. Among 
them,127519 different words are the entries of 
the vocabulary. The average length of each 
document is 394. 
The 50 queries offered by NTCIR3 IR task 
are contained in a XML file and each query 
consists of following elements: Topic 
Number(NUM),Topic Title(TITLE),Topic 
question(DESC),Topic Narrative(NARR) and 
Topic Concepts(CONC). In order to make it 
easer to compare the performance of the different 
IR methods, we adopt the Topic Question field as 
the query and regard the top 1000 retrieval 
documents as the standard result of the 
experiment.    
 
3.2 Analysis of Experiment Results 
We design 3 relative experiments to 
evaluate the trigger language model IR method: 
vector space model, Ponte language model based 
method and the trigger language model approach. 
Precision and recall are two main evaluation 
parameters. As for the trigger model IR method, 
the optimal size of the text window is 20 content 
words and the mutual information over 25 is 
regarded as the meaningful information. 
Experiment results can be seen in table 2. 
The data of column %  in table 2 shows 
the performance improvement of Ponte language 
model compared with vector space model. The 
data tells us that the precision of language model 
based method increased 10% and recall increased 
nearly 13.7%. The data of column %?  in table 
2 shows the performance improvement of trigger 
language model compared with Ponte language 
model method. From the data we can see that the 
precision of trigger language model increased 
12% and recall increased nearly 10.8%. We can 
draw the conclusion that the trigger language 
model has improved the performance greatly. 
The performance comparison can be showed 
more clearly in figure 1. 
1?
2
    
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.2 0.4 0.6 0.8 1
Recall
Pr
ec
is
io
n
tfidf
Ponte
Language
Model
Trigger
Language
Model
 
 Figure 1. Precision-Recall of 3 methods 
 
  Tfidf Lm(ponte) Trigger lm % 1?  %  2?
Relevant: 3284 3284 3284 ----
- 
----
- 
Rel.ret: 1843 2096 2322 13.7 10.8 
Precision:      
0. 00 
0. 10 
0. 20 
0. 30 
0. 40 
0. 50 
0. 60 
0. 70 
0. 80 
0. 90 
1. 00 
Avg: 
0. 6016 
0. 4607 
0. 3812 
0. 3336 
0. 2738 
0. 2495 
0. 2179 
0. 1566 
0. 0978 
0. 0389 
0. 0019 
0.2377 
0.6109 
0.4844 
0.4123 
0.3757 
0.3255 
0.2854 
0.2313 
0.1716 
0.1041 
0.0474 
0.0025 
0.2610 
0. 7537 
0. 5314 
0. 4541 
0. 4094 
0. 3648 
0. 3237 
0. 2538 
0. 2011 
0. 1153 
0. 0435 
0. 0055 
0. 2933 
+2 
+5 
+8 
+12 
+18 
+14 
+6 
+9 
+6 
+21 
+31 
+10 
+23 
+10 
+10 
+9 
+12 
+13 
+9 
+17 
+10 
-8 
+120 
+12 
 
Table 2. Experiment results
 
4  Conclusion  
Language model based IR system proposed 
in recent 5 years has introduced the language 
model approach in the speech recognition area 
into the IR community and improves the 
performance of the IR system effectively. 
However, the assumption that all the indexed 
words are irrelative behind the method is not the 
truth. Though statistical MT approach alleviates 
the situation by taking the synonymy factor into 
account, it never helps to judge the different 
meanings of the same word in varied context. In 
this paper we propose the trigger language model 
based IR system to resolve the problem. . Firstly 
we compute the mutual information of the words 
from training corpus and then design the 
algorithm to get the triggered words of the query 
in order to fix down the topic of query more 
clearly. We introduce the relative parameters into 
the document language model to form the trigger 
language model based IR system. Experiments 
show that the performance of trigger language 
model based IR system has been improved 
greatly. 
Acknowledgement 
This work is supported by Beijing New Star Plan 
of Technology & Science(NO.H020820790130) 
and the National Science Fund of China under 
contact 60203007. 
References  
Berger A. and  Lafferty J. (1999). Information 
retrieval as statistical translation. In Proceedings of 
SIGIR ?99. pp. 222-229. 
 
Jin R., Hauptmann A.G.  and Zhai C.(2002) Title 
Language Model for Information Retrieval. In 
Proceedings of the 2002 ACM SIGIR Conference 
on Research and Development in Information 
Retrieval. 
 
Hiemstra D. and  Kraaij W. (1999), Twenty-One at 
TREC-7: ad-hoc and cross-language track, In 
Proceedings of the seventh Text Retrieval 
Conference TREC-7, NIST Special Publication 
500-242, pages 227-238, 1999. 
 
Lafferty J. and Zhai. C. (2001) Document language 
models,query models and risk minimization for 
information retrieval. In Proceedings of the 24th 
ACM SIGIR Conference,pp.111-119. 
 
Lavrenko,V., and Croft,W.B.(2001) .Relevance based 
language models. In Proceedings of the 24th ACM 
SIGIR Conference.pp.120-127. 
 
Liu,X. and Croft,W.B.(2002).Passage Retrieval 
Based on Language Models. In Proceedings of the 
11th  International Conference on Information and 
Knowledge Management. Pp.375-382 
 
Miller D.,  Leek T.  and Schwartz  R. M. (1999). 
A hidden Markov model information retrieval 
system. Proceedings of SIGIR?1999, pp. 214-222. . 
 
NTCIR Workshop 
(research.nii.ac.jp/ntcir/index-en.html) 
  
Ponte J. and Croft W. B. (1998). A language 
modeling approach to information retrieval. In 
Proceedings of SIGIR? 1998, pp. 275-281. 
 
Raymond Lau, Roni Rosenfeld and Salim 
Roukos(1993) Trigger-based Language Models: A 
Maximum Entropy Apporach. Proceedings ICASSP 
'93, Minneapolis, MN, pp. II-45 - II-48. 
 
Srikanth M. and  Srihari. R(2002). Biterm 
Language Models for Document Retrieval. In 
Proceedings of the 2002 ACM SIGIR Conference 
on Research and Development in Information 
Retrieval.  
 
Zhai C. and Lafferty J.(2001). A study of smoothing 
methods for language models applied to ad hoc 
information retrieval. In Proceeding of SIGIR?01, 
2001, pp. 334-342. 
 
 
 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 585?592,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Iterative Implicit Feedback Approach to Personalized Search 
 
Yuanhua Lv 1, Le Sun 2, Junlin Zhang 2, Jian-Yun Nie 3, Wan Chen 4, and Wei Zhang 2 
1, 2 Institute of Software, Chinese Academy of Sciences, Beijing, 100080, China 
3 University of Montreal, Canada 
1 lvyuanhua@gmail.com 
2 {sunle, junlin01, zhangwei04}@iscas.cn 
3 nie@iro.umontreal.ca   4 chenwan@nus.edu.sg 
 
Abstract 
General information retrieval systems are 
designed to serve all users without con-
sidering individual needs. In this paper, 
we propose a novel approach to person-
alized search. It can, in a unified way, 
exploit and utilize implicit feedback in-
formation, such as query logs and imme-
diately viewed documents. Moreover, our 
approach can implement result re-ranking 
and query expansion simultaneously and 
collaboratively. Based on this approach, 
we develop a client-side personalized web 
search agent PAIR (Personalized Assis-
tant for Information Retrieval), which 
supports both English and Chinese. Our 
experiments on TREC and HTRDP col-
lections clearly show that the new ap-
proach is both effective and efficient. 
1 Introduction 
Analysis suggests that, while current information 
retrieval systems, e.g., web search engines, do a 
good job of retrieving results to satisfy the range 
of intents people have, they are not so well in 
discerning individuals? search goals (J. Teevan et 
al., 2005). Search engines encounter problems 
such as query ambiguity and results ordered by 
popularity rather than relevance to the user?s in-
dividual needs. 
To overcome the above problems, there have 
been many attempts to improve retrieval accuracy 
based on personalized information. Relevance 
Feedback (G. Salton and C. Buckley, 1990) is the 
main post-query method for automatically im-
proving a system?s accuracy of a user?s individual 
need. The technique relies on explicit relevance 
assessments (i.e. indications of which documents 
contain relevant information). Relevance feed-
back has been proved to be quite effective for 
improving retrieval accuracy (G. Salton and C. 
Buckley, 1990; J. J. Rocchio, 1971). However, 
searchers may be unwilling to provide relevance 
information through explicitly marking relevant 
documents (M. Beaulieu and S. Jones, 1998). 
Implicit Feedback, in which an IR system un-
obtrusively monitors search behavior, removes 
the need for the searcher to explicitly indicate 
which documents are relevant (M. Morita and Y. 
Shinoda, 1994). The technique uses implicit 
relevance indications, although not being as ac-
curate as explicit feedback, is proved can be an 
effective substitute for explicit feedback in in-
teractive information seeking environments (R. 
White et al, 2002). In this paper, we utilize the 
immediately viewed documents, which are the 
clicked results in the same query, as one type of 
implicit feedback information. Research shows 
that relative preferences derived from immedi-
ately viewed documents are reasonably accurate 
on average (T. Joachims et al, 2005). 
Another type of implicit feedback information 
that we exploit is users? query logs. Anyone who 
uses search engines has accumulated lots of click 
through data, from which we can know what 
queries were, when queries occurred, and which 
search results were selected to view. These query 
logs provide valuable information to capture us-
ers? interests and preferences. 
Both types of implicit feedback information 
above can be utilized to do result re-ranking and 
query expansion, (J. Teevan et al, 2005; Xuehua 
Shen. et al, 2005) which are the two general ap-
proaches to personalized search. (J. Pitkow et al, 
2002) However, to the best of our knowledge, 
how to exploit these two types of implicit feed-
back in a unified way, which not only brings col-
laboration between query expansion and result 
re-ranking but also makes the whole system more 
concise, has so far not been well studied in the 
previous work. In this paper, we adopt HITS al-
gorithm (J. Kleinberg, 1998), and propose a 
585
HITS-like iterative approach addressing such a 
problem. 
Our work differs from existing work in several 
aspects: (1) We propose a HITS-like iterative 
approach to personalized search, based on which, 
implicit feedback information, including imme-
diately viewed documents and query logs, can be 
utilized in a unified way. (2) We implement re-
sult re-ranking and query expansion simultane-
ously and collaboratively triggered by every 
click. (3) We develop and evaluate a client-side 
personalized web search agent PAIR, which 
supports both English and Chinese. 
The remaining of this paper is organized as 
follows. Section 2 describes our novel approach 
for personalized search. Section 3 provides the 
architecture of PAIR system and some specific 
techniques. Section 4 presents the details of the 
experiment. Section 5 discusses the previous 
work related to our approach. Section 6 draws 
some conclusions of our work. 
2 Iterative Implicit Feedback Approach 
We propose a HITS-like iterative approach for 
personalized search. HITS (Hyperlink-Induced 
Topic Search) algorithm, first described by (J. 
Kleinberg, 1998), was originally used for the 
detection of high-score hub and authority web 
pages. The Authority pages are the central web 
pages in the context of particular query topics. 
The strongest authority pages consciously do not 
link one another1 ? they can only be linked by 
some relatively anonymous hub pages. The mu-
tual reinforcement principle of HITS states that a 
web page is a good authority page if it is linked by 
many good hub pages, and that a web page is a 
good hub page if it links many good authority 
pages. A directed graph is constructed, of which 
the nodes represent web pages and the directed 
edges represent hyperlinks. After iteratively 
computing based on the reinforcement principle, 
each node gets an authority score and a hub score. 
In our approach, we exploit the relationships 
between documents and terms in a similar way to 
HITS. Unseen search results, those results which 
are retrieved from search engine yet not been 
presented to the user, are considered as ?authority 
pages?. Representative terms are considered as 
?hub pages?. Here the representative terms are the 
terms extracted from and best representing the 
implicit feedback information. Representative 
terms confer a relevance score to the unseen 
                                                          
1 For instance, There is hardly any other company?s Web 
page linked from ?http://www.microsoft.com/? 
search results ? specifically, the unseen search 
results, which contain more good representative 
terms, have a higher possibility of being relevant; 
the representative terms should be more repre-
sentative, if they occur in the unseen search re-
sults that are more likely to be relevant. Thus, 
also there is mutual reinforcement principle ex-
isting between representative terms and unseen 
search results. By the same token, we constructed 
a directed graph, of which the nodes indicate un-
seen search results and representative terms, and 
the directed edges represent the occurrence of the 
representative terms in the unseen search results. 
The following Table 1 shows how our approach 
corresponds to HITS algorithm. 
 
The Directed Graph 
Approaches
Nodes Edges 
HITS Authority Pages Hub Pages Hyperlinks
Our  
Approach
Unseen Search 
Results 
Representative 
Terms Occurrence
2
Table 1. Our approach versus HITS. 
 
Because we have already known that the rep-
resentative terms are ?hub pages?, and that the 
unseen search results are ?authority pages?, with 
respect to the former, only hub scores need to be 
computed; with respect to the latter, only author-
ity scores need to be computed. 
Finally, after iteratively computing based on 
the mutual reinforcement principle we can 
re-rank the unseen search results according to 
their authority scores, as well as select the repre-
sentative terms with highest hub scores to ex-
pand the query. Below we present how to con-
struct a directed graph to begin with. 
2.1 Constructing a Directed Graph 
We can view the unseen search results and the 
representative terms as a directed graph G = (V, E). 
A sample directed graph is shown in Figure 1: 
 
 
Figure 1. A sample directed graph. 
 
The nodes V correspond to the unseen search 
results (the rectangles in Figure 1) and the repre-
                                                          
2 The occurrence of the representative terms in the unseen 
search results. 
586
sentative terms (the circles in Figure 1); a di-
rected edge ?p?q?E? is weighed by the fre-
quency of the occurrence of a representative term 
p in an unseen search result q (e.g., the number 
put on the edge ?t1?r2? indicates that t1 occurs 
twice in r2). We say that each representative term 
only has an out-degree which is the number of the 
unseen search results it occurs in, as well as that 
each unseen search result only has an in-degree 
which is the count of the representative terms it 
contains. Based on this, we assume that the un-
seen search results and the representative terms 
respectively correspond to the authority pages 
and the hub pages ? this assumption is used 
throughout the proposed algorithm. 
2.2 A HITS-like Iterative Algorithm 
In this section, we present how to initialize the 
directed graph and how to iteratively compute the 
authority scores and the hub scores. And then 
according to these scores, we show how to re-rank 
the unseen search results and expand the initial 
query. 
Initially, each unseen search result of the query 
are considered equally authoritative, that is, 
0 0 0
1 2 | |
1 | |
Y
Yy y y= ?= =                  (1) 
Where vector Y indicates authority scores of the 
overall unseen search results, and |Y| is the size of 
such a vector. Meanwhile, each representative 
term, with the term frequency tfj in the history 
query logs that have been judged related to the 
current query, obtains its hub score according to 
the follow formulation: 
0
| |
1
X
j i
j itf tfx == ?                       (2) 
Where vector X indicates hub scores of the overall 
representative terms, and |X| is the size of the 
vector X. The nodes of the directed graph are 
initialized in this way. Next, we associate each 
edge with a weight: 
,( )ji i jw tft r? =                     (3) 
Where tfi,j indicates the term frequency of the 
representative term ti occurring in the unseen 
search result rj; ?w(ti? rj)? is the weight of edge 
that link from ti to rj. For instance, in Figure 1, 
w(t1? r2) = 2. 
After initialization, the iteratively computing of 
hub scores and authority scores starts. 
The hub score of each representative term is 
re-computed based on three factors: the authority 
scores of each unseen search result where this 
term occurs; the occurring frequency of this term 
in each unseen search result; the total occurrence 
of every representative term in each unseen search 
result. The formulation for re-computing hub 
scores is as follows: 
( 1)
:
:
( )
( )'
k ji
i
jnji
jn
k
jj
n
w
wt r
t r
t ryx t r
+
?  ?
?  ?
?= ?? ?
    (4) 
Where x`i(k+1) is the hub score of a representative 
term ti after (k+1)th iteration; yjk is the authority 
score of an unseen search result rj after kth itera-
tion; ??j: ti?rj? indicates the set of all unseen 
search results those ti occurs in; ??n: tn?rj? in-
dicates the set of all representative terms those rj 
contains. 
The authority score of each unseen search re-
sult is also re-computed relying on three factors: 
the hub scores of each representative term that 
this search result contains; the occurring fre-
quency of each representative term in this search 
result; the total occurrence of each representative 
term in every unseen search results. The formu-
lation for re-computing authority scores is as 
follows: 
( 1)
:
:
( )
( )'
k k
ij
miji
mi
ji
i
m
w
wt r
t r
t ry x t r
+
?  ?
?  ?
?= ?? ?
    (5) 
Where y`j(k+1) is the authority score of an unseen 
search result rj after (k+1)th iteration; xik  is the 
hub score of a representative term ti after kth it-
eration; ??i: ti?rj? indicates the set of all repre-
sentative terms those rj contains; ??m: ti?rm? 
indicates the set of all unseen search results those 
ti occurs in. 
After re-computation, the hub scores and the 
authority scores are normalized to 1. The formu-
lation for normalization is as follows: 
| | | |
1 1
and
' '
' '
j i
iY Xj
kkk k
y xy x
y x= =
=   =
? ?
              (6) 
The iteration, including re-computation and 
normalization, is repeated until the changes of the 
hub scores and the authority scores are smaller 
than some predefined threshold ? (e.g. 10-6). 
Specifically, after each repetition, the changes in 
authority scores and hub scores are computed 
using the following formulation: 
2 2( 1) ( 1)
| | | |
1 1
( ) ( )
k k k k
i ij j
Y x
j i
c y y x x
+ +
= =
= ? + ?? ?        (7) 
The iteration stops if c<?. Moreover, the itera-
tion will also stop if repetition has reached a 
587
predefined times k (e.g. 30). The procedure of the 
iteration is shown in Figure 2. 
As soon as the iteration stops, the top n unseen 
search results with highest authority scores are 
selected and recommended to the user; the top m 
representative terms with highest hub scores are 
selected to expand the original query. Here n is a 
predefined number (in PAIR system we set n=3, 
n is given a small number because using implicit 
feedback information is sometimes risky.) m is 
determined according to the position of the big-
gest gap, that is, if ti ? ti+1 is bigger than the gap 
of any other two neighboring ones of the top half 
representative terms, then m is given a value i. 
Furthermore, some of these representative terms 
(e.g. top 50% high score terms) will be again used 
in the next time of implementing the iterative 
algorithm together with some newly incoming 
terms extracted from the just now click. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. The HITS-like iterative algorithm. 
 
3 Implementation 
3.1 System Design 
In this section, we present our experimental sys-
tem PAIR, which is an IE Browser Helper Object 
(BHO) based on the popular Web search engine 
Google. PAIR has three main modules: Result 
Retrieval module, User Interactions module, and 
Iterative Algorithm module. The architecture is 
shown in Figure 3. 
The Result Retrieval module runs in back-
grounds and retrieves results from search engine. 
When the query has been expanded, this module 
will use the new keywords to continue retrieving. 
The User Interactions module can handle three 
types of basic user actions: (1) submitting a query; 
(2) clicking to view a search result; (3) clicking 
the ?Next Page? link. For each of these actions, 
the system responds with: (a) exploiting and ex-
tracting representative terms from implicit feed-
back information; (b) fetching the unseen search 
results via Results Retrieval module; (c) sending 
the representative terms and the unseen search 
results to Iterative Algorithm module. 
 
 
Figure 3. The architecture of PAIR. 
 
The Iterative Algorithm module implements 
the HITS-like algorithm described in section 2. 
When this module receives data from User In-
teractions module, it responds with: (a) iteratively 
computing the hub scores and authority scores; (b) 
re-ranking the unseen search results and expand-
ing the original query. 
Some specific techniques for capturing and 
exploiting implicit feedback information are de-
scribed in the following sections. 
3.2 Extract Representative Terms from 
Query Logs 
We judge whether a query log is related to the 
current query based on the similarity between the 
query log and the current query text. Here the 
query log is associated with all documents that 
the user has selected to view. The form of each 
query log is as follows 
<query text><query time> [clicked documents]* 
The ?clicked documents? consist of URL, title 
and snippet of every clicked document. The rea-
son why we utilize the query text of the current 
query but not the search results (including title, 
snippet, etc.) to compute the similarity, is out of 
consideration for efficiency. If we had used the 
search results to determine the similarity, the 
computation could only start once the search en-
gine has returned the search results. In our method, 
instead, we can exploit query logs while search 
engine is doing retrieving. Notice that although 
our system only utilizes the query logs in the last 
24 hours; in practice, we can exploit much more 
because of its low computation cost with respect 
to the retrieval process performed in parallel.
Iterate (T, R, k, ?) 
T: a collection of m terms 
R: a collection of n search results 
k: a natural number 
?: a predefined threshold 
Apply (1) to initialize Y. 
Apply (2) to initialize X. 
Apply (3) to initialize W. 
For i = 1, 2?, k 
Apply (4) to (Xi-1, Yi-1) and obtain X`i. 
Apply (5) to (Xi-1, Yi-1) and obtain Y`i. 
Apply (6) to Normalize X`i and Y`i, and respectively 
obtain Xi and Yi. 
Apply (7) and obtain c. 
If c<?, then break. 
End 
Return (X, Y). 
588
Table 2. Sample results of re-ranking. The search results in boldface are the ones that our system rec-
ommends to the user. ?-3? and ?-2? in the right side of some results indicate the how their ranks descend. 
 
We use the standard vector space retrieval 
model (G. Salton and M. J. McGill, 1983) to 
compute the similarity. If the similarity between 
any query log and the current query exceeds a 
predefined threshold, the query log will be con-
sidered to be related to current query. Our system 
will attempt to extract some (e.g. 30%) represen-
tative terms from such related query logs ac-
cording to the weights computed by applying the 
following formulation: 
( )i i iw f idftt =                      (8) 
Where tfi and idfi respectively are the term fre-
quency and inverse document frequency of ti in 
the clicked documents of a related query log. 
This formulation means that a term is more rep-
resentative if it has a higher frequency as well as 
a broader distribution in the related query log. 
3.3 Extract Representative Terms from 
Immediately Viewed Documents 
The representative terms extracted from immedi-
ately viewed documents are determined based on 
three factors: term frequency in the immediately 
viewed document, inverse document frequency in 
the entire seen search results, and a discriminant 
value. The formulation is as follows:  
( ) ( )Ni ii i
r ddw d
x xtf idfx x= ? ?             (9) 
Where tfxidr is the term frequency of term xi in the 
viewed results set dr; tfxidr is the inverse document 
frequency of xi in the entire seen results set dN. 
And the discriminant value d(xi) of xi is computed 
using the weighting schemes F2 (S. E. Robertson 
and K. Sparck Jones, 1976) as follows: 
( ) ln
( ) ( )i
r Rd
n r N Rx = ? ?
                 (10) 
Where r is the number of the immediately viewed 
documents containing term xi; n is the number of 
the seen results containing term xi; R is the num-
ber of the immediately viewed documents in the 
query; N is the number of the entire seen results.  
3.4 Sample Results 
Unlike other systems which do result re-ranking 
and query expansion respectively in different 
ways, our system implements these two functions 
simultaneously and collaboratively ?  Query 
expansion provides diversified search results 
which must rely on the use of re-ranking to be 
moved forward and recommended to the user. 
 
 
Figure 4. A screen shot for query expansion. 
 
After iteratively computing using our approach, 
the system selects some search results with top 
highest authority scores and recommends them to 
the user. In Table 2, we show that PAIR suc-
cessfully re-ranks the unseen search results of 
?jaguar? respectively using the immediately 
Google result PAIR result  
query = ?jaguar? query = ?jaguar? After the 4th result being clicked 
query = ?jaguar? 
?car? ? query logs 
1 Jaguar www.jaguar.com/ 
Jaguar 
www.jaguar.com/ 
Jaguar UK - Jaguar Cars 
www.jaguar.co.uk/ 
2 Jaguar CA - Jaguar Cars www.jaguar.com/ca/en/ 
Jaguar CA - Jaguar Cars 
www.jaguar.com/ca/en/ 
Jaguar UK - R is for? 
www.jaguar-racing.com/ 
3 Jaguar Cars www.jaguarcars.com/ 
Jaguar Cars 
www.jaguarcars.com/ 
Jaguar 
www.jaguar.com/ 
4 Apple - Mac OS X www.apple.com/macosx/ 
Apple - Mac OS X 
www.apple.com/macosx/ 
Jaguar CA - Jaguar Cars 
www.jaguar.com/ca/en/                      -2 
5 Apple - Support ? www.apple.com/support/... 
Amazon.com: Mac OS X 10.2? 
www.amazon.com/exec/obidos/... 
Jaguar Cars 
www.jaguarcars.com/                        -2 
6 Jaguar UK - Jaguar Cars www.jaguar.co.uk/ 
Mac OS X 10.2 Jaguar? 
arstechnica.com/reviews/os? 
Apple - Mac OS X 
www.apple.com/macosx/                     -2 
7 Jaguar UK - R is for? www.jaguar-racing.com/ 
Macworld: News: Macworld? 
maccentral.macworld.com/news/? 
Apple - Support ? 
www.apple.com/support/...                    -2 
8 Jaguar dspace.dial.pipex.com/? 
Apple - Support? 
www.apple.com/support/...                -3 
Jaguar 
dspace.dial.pipex.com/? 
9 Schr?dinger -> Home www.schrodinger.com/ 
Jaguar UK - Jaguar Cars 
www.jaguar.co.uk/                       -3 
Schr?dinger -> Home 
www.schrodinger.com/ 
10 Schr?dinger -> Site Map www.schrodinger.com/... 
Jaguar UK - R is for? 
www.jaguar-racing.com/                  -3 
Schr?dinger -> Site Map 
www.schrodinger.com/... 
589
viewed documents and the query logs. Simulta-
neously, some representative terms are selected 
to expand the original query. In the query of 
?jaguar? (without query logs), we click some 
results about ?Mac OS?, and then we see that a 
term ?Mac? has been selected to expand the 
original query, and some results of the new query 
?jaguar Mac? are recommended to the user under 
the help of re-ranking, as shown in Figure 4. 
4 Experiment 
4.1 Experimental Methodology 
It is a challenge to quantitatively evaluate the 
potential performance improvement of the pro-
posed approach over Google in an unbiased way 
(D. Hawking et al, 1999; Xuehua Shen et al, 
2005). Here, we adopt a similar quantitative 
evaluation as what Xuehua Shen et al (2005) do 
to evaluate our system PAIR and recruit 9 stu-
dents who have different backgrounds to partici-
pate in our experiment. We use query topics from 
TREC 2005 and 2004 Hard Track, TREC 2004 
Terabyte track for English information retrieval,3 
and use query topics from HTRDP 2005 Evalua-
tion for Chinese information retrieval.4 The rea-
son why we utilize multiple TREC tasks rather 
than using a single one is that more queries are 
more likely to cover the most interesting topics 
for each participant. 
Initially, each participant would freely choose 
some topics (typically 5 TREC topics and 5 
HTRDP topics). Each query of TREC topics will 
be submitted to three systems: UCAIR 5 (Xue-
hua Shen et al, 2005), ?PAIR No QE? (PAIR 
system of which the query expansion function is 
blocked) and PAIR. Each query of HTRDP topics 
needs only to be submitted to ?PAIR No QE? and 
PAIR. We do not evaluate UCAIR using HTRDP 
topics, since it does not support Chinese. For each 
query topic, the participants use the title of the 
topic as the initial keyword to begin with. Also 
they can form some other keywords by them-
selves if the title alone fails to describe some de-
tails of the topic. There is no limit on how many 
queries they must submit. During each query 
process, the participant may click to view some 
results, just as in normal web search. 
Then, at the end of each query, search results 
from these different systems are randomly and 
anonymously mixed together so that every par-
                                                          
3 Text REtrieval Conference. http://trec.nist.gov/ 
4 2005 HTRDP Evaluation. http://www.863data.org.cn/ 
5 The latest version released on November 11, 2005. 
http://sifaka.cs.uiuc.edu/ir/ucair/ 
ticipant would not know where a result comes 
from. The participants would judge which of 
these results are relevant. 
At last, we respectively measure precision at 
top 5, top 10, top 20 and top 30 documents of 
these system. 
4.2 Results and Analysis 
Altogether, 45 TREC topics (62 queries in all) are 
chosen for English information retrieval. 712 
documents are judged as relevant from Google 
search results. The corresponding number of 
relevant documents from UCAIR, ?PAIR No QE? 
and PAIR respectively is: 921, 891 and 1040. 
Figure 5 shows the average precision of these four 
systems at top n documents among such 45 TREC 
topics. 
 
 
Figure 5. Average precision for TREC topics. 
 
45 HTRDP topics (66 queries in all) are chosen 
for Chinese information retrieval. 809 documents 
are judged as relevant from Google search results. 
The corresponding number of relevant documents 
from ?PAIR No QE? and PAIR respectively is: 
1198 and 1416. Figure 6 shows the average pre-
cision of these three systems at top n documents 
among such 45 HTRDP topics. 
 
 
Figure 6. Average precision for HTRDP topics. 
 
PAIR and ?PAIR No QE? versus Google 
We can see clearly from Figure 5 and Figure 6 
that the precision of PAIR is improved a lot 
comparing with that of Google in all measure-
590
ments. Moreover, the improvement scale in-
creases from precision at top 10 to that of top 30. 
One explanation for this is that the more implicit 
feedback information generated, the more repre-
sentative terms can be obtained, and thus, the 
iterative algorithm can perform better, leading to 
more precise search results. ?PAIR No QE? also 
significantly outperforms Google in these meas-
urements, however, with query expansion, PAIR 
can perform even better. Thus, we say that result 
re-ranking and query expansion both play an 
important role in PAIR. 
Comparing Figure 5 with Figure 6, one can see 
that the improvement of PAIR versus Google in 
Chinese IR is even larger than that of English IR. 
One explanation for this is that: before imple-
menting the iterative algorithm, each Chinese 
search result, including title and snippet, is seg-
mented into words (or phrases). And only the 
noun, verb and adjective of these words (or 
phrases) are used in next stages, whereas, we only 
remove the stop words for English search result. 
Another explanation is that there are some Chi-
nese web pages with the same content. If one of 
such pages is clicked, then, occasionally some 
repetition pages are recommended to the user. 
However, since PAIR is based on the search re-
sults of Google and the information concerning 
the result pages that PAIR can obtained is limited, 
which leads to it difficult to avoid the replica-
tions. 
PAIR and ?PAIR No QE? versus UCAIR 
In Figure 5, we can see that the precision of 
?PAIR No QE? is better than that of UCAIR 
among top 5 and top 10 documents, and is almost 
the same as that of UCAIR among top 20 and top 
30 documents. However, PAIR is much better 
than UCAIR in all measurements. This indicates 
that result re-ranking fails to do its best without 
query expansion, since the relevant documents in 
original query are limited, and only the re-ranking 
method alone cannot solve the ?relevant docu-
ments sparseness? problem. Thus, the query ex-
pansion method, which can provide fresh and 
relevant documents, can help the re-ranking 
method to reach an even better performance. 
Efficiency of PAIR 
The iteration statistic in evaluation indicates that 
the average iteration times of our approach is 22 
before convergence on condition that we set the 
threshold ? = 10-6. The experiment shows that the 
computation time of the proposed approach is 
imperceptible for users (less than 1ms.) 
5 Related Work 
There have been many prior attempts to person-
alized search. In this paper, we focus on the re-
lated work doing personalized search based on 
implicit feedback information. 
Some of the existing studies capture users? in-
formation need by exploiting query logs. For 
example, M. Speretta and S. Gauch (2005) build 
user profiles based on activity at the search site 
and study the use of these profiles to provide 
personalized search results. F. Liu et al (2002) 
learn user's favorite categories from his query 
history. Their system maps the input query to a set 
of interesting categories based on the user profile 
and confines the search domain to these catego-
ries. Some studies improve retrieval performance 
by exploiting users? browsing history (F. Tanud-
jaja and L. Mu, 2002; M. Morita and Y. Shinoda, 
1994) or Web communities (A. Kritikopoulos 
and M. Sideri, 2003; K. Sugiyama et al, 2004) 
Some studies utilize client side interactions, for 
example, K. Bharat (2000) automatically discov-
ers related material on behalf of the user by 
serving as an intermediary between the user and 
information retrieval systems. His system ob-
serves users interacting with everyday applica-
tions and then anticipates their information needs 
using a model of the task at hand. Some latest 
studies combine several types of implicit feed-
back information. J. Teevan et al (2005) explore 
rich models of user interests, which are built 
from both search-related information, such as 
previously issued queries and previously visited 
Web pages, and other information about the user 
such as documents and email the user has read 
and created. This information is used to re-rank 
Web search results within a relevance feedback 
framework. 
Our work is partly inspired by the study of 
Xuehua Shen et al (2005), which is closely re-
lated to ours in that they also exploit immediately 
viewed documents and short-term history queries, 
implement query expansion and re-ranking, and 
develop a client-side web search agents that per-
form eager implicit feedback. However, their 
work differs from ours in three ways: First, they 
use the cosine similarity to implement query ex-
pansion, and use Rocchio formulation (J. J. 
Rocchio, 1971) to re-rank the search results. 
Thus, their query expansion and re-ranking are 
computed separately and are not so concise and 
collaborative. Secondly, their query expansion is 
based only on the past queries and is imple-
mented before the query, which leads to that 
591
their query expansion does not benefit from 
user?s click through data. Thirdly, they do not 
compute the relevance of search results and the 
relativity of expanded terms in an iterative fash-
ion. Thus, their approach does not utilize the re-
lation among search results, among expanded 
terms, and between search results and expanded 
terms. 
6 Conclusions 
In this paper, we studied how to exploit implicit 
feedback information to improve retrieval accu-
racy. Unlike most previous work, we propose a 
novel HITS-like iterative algorithm that can 
make use of query logs and immediately viewed 
documents in a unified way, which not only 
brings collaboration between query expansion 
and result re-ranking but also makes the whole 
system more concise. We further propose some 
specific techniques to capture and exploit these 
two types of implicit feedback information. Us-
ing these techniques, we develop a client-side 
web search agent PAIR. Experiments in English 
and Chinese collections show that our approach 
is both effective and efficient. 
However, there is still room to improve the 
performance of the proposed approach, such as 
exploiting other types of personalized informa-
tion, choosing some more effective strategies to 
extract representative terms, studying the effects 
of the parameters used in the approach, etc. 
Acknowledgement 
We would like to thank the anonymous review-
ers for their helpful feedback and corrections, 
and to the nine participants of our evaluation ex-
periments. Additionally, this work is supported 
by the National Science Fund of China under 
contact 60203007. 
References 
A. Kritikopoulos and M. Sideri, 2003. The Compass 
Filter: Search engine result personalization using 
Web communities. In Proceedings of ITWP, pages 
229-240. 
D. Hawking, N. Craswell, P.B. Thistlewaite, and D. 
Harman, 1999. Results and challenges in web 
search evaluation. Computer Networks, 
31(11-16):1321?1330. 
F. Liu, C. Yu, and W. Meng, 2002. Personalized web 
search by mapping user queries to categories. In 
Proceedings of CIKM, pages 558-565. 
F. Tanudjaja and L. Mu, 2002. Persona: a contextual-
ized and personalized web search. HICSS. 
G. Salton and M. J. McGill, 1983. Introduction to 
Modern Information Retrieval. McGraw-Hill. 
G. Salton and C. Buckley, 1990. Improving retrieval 
performance by relevance feedback. Journal of the 
American Society for Information Science, 
41(4):288-297. 
J. J. Rocchio, 1971. Relevance feedback in informa-
tion retrieval. In The SMART Retrieval System :  
Experiments in Automatic Document Processing, 
pages 313?323. Prentice-Hall Inc. 
J. Kleinberg, 1998. Authoritative sources in a hyper-
linked environment. ACM, 46(5):604?632. 
J. Pitkow, H. Schutze, T. Cass, R. Cooley, D. 
Turnbull, A. Edmonds, E. Adar, and T. Breuel, 
2002. Personalized search. Communications of the 
ACM, 45(9):50-55. 
J. Teevan, S. T. Dumais, and E. Horvitz, 2005. Per-
sonalizing search via automated analysis of interests 
and activities. In Proceedings of SIGIR, pages 
449-456. 
K. Bharat, 2000. SearchPad: Explicit capture of 
search context to support Web search. Computer 
Networks, 33(1-6): 493-501. 
K. Sugiyama, K. Hatano, and M. Yoshikawa, 2004. 
Adaptive Web search based on user profile con-
structed without any effort from user. In Proceed-
ings of WWW, pages 675-684. 
M. Beaulieu and S. Jones, 1998. Interactive searching 
and interface issues in the okapi best match retrieval 
system. Interacting with Computers, 10(3):237-248. 
M. Morita and Y. Shinoda, 1994. Information filtering 
based on user behavior analysis and best match text 
retrieval. In Proceedings of SIGIR, pages 272?281. 
M. Speretta and S. Gauch, 2005. Personalizing search 
based on user search history. Web Intelligence, 
pages 622-628. 
R. White, I. Ruthven, and J. M. Jose, 2002. The use of 
implicit evidence for relevance feedback in web 
retrieval. In Proceedings of ECIR, pages 93?109. 
S. E. Robertson and K. Sparck Jones, 1976. Relevance 
weighting of search terms. Journal of the 
American Society for Information Science, 
27(3):129-146. 
T. Joachims, L. Granka, B. Pang, H. Hembrooke, and 
G. Gay, 2005. Accurately Interpreting Clickthrough 
Data as Implicit Feedback, In Proceedings of 
SIGIR, pages 154-161. 
Xuehua Shen, Bin Tan, and Chengxiang Zhai, 2005. 
Implicit User Modeling for Personalized Search. In 
Proceedings of CIKM, pages 824-831. 
592
