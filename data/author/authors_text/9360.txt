Unigram Language Model for Chinese Word Segmentation 
        Aitao Chen 
Yahoo! Inc. 
701 First Avenue 
Sunnyvale, CA 94089 
aitao@yahoo-inc.com 
Yiping Zhou 
Yahoo! Inc. 
701 First Avenue 
Sunnyvale, CA 94089 
zhouy@yahoo-inc.com
Anne Zhang 
Yahoo! Inc. 
701 First Avenue 
Sunnyvale, CA 94089
annezhangya@   
yahoo.com 
Gordon Sun 
Yahoo! Inc. 
701 First Avenue 
Sunnyvale, CA 94089 
gzsun@yahoo-inc.com
Abstract
This paper describes a Chinese word 
segmentation system based on unigram 
language model for resolving segmen-
tation ambiguities. The system is aug-
mented with a set of pre-processors and 
post-processors to extract new words in 
the input texts. 
1 Introduction
The Yahoo team participated in all four closed 
tasks and all four open tasks at the second inter-
national Chinese word segmentation bakeoff. 
2 System Description 
The underlying algorithm in our word segmenta-
tion system is the unigram language model in 
which words in a sentence are assumed to occur 
independently. For an input sentence, we exam-
ine all possible ways to segment the new sen-
tence with respect to the segmentation 
dictionary, and choose the segmentation of the 
highest probability, which is estimated based on 
the unigram model. 
Our system also has a few preprocessors and 
postprocessors. The main preprocessors include 
recognizers for extracting names of people, 
places and organizations, and recognizer for 
numeric expressions. The proper name recog-
nizers are built based on the maximum entropy 
model, and the numeric expression recognizer is 
built as a finite state automaton. The conditional 
maximum entropy model in our implementation 
is based on the one described in Section 2.5 in 
(Ratnaparkhi, 1998), and features are the same 
as those described in (Xue and Shen, 2003). 
One of the post-processing steps is to com-
bine single characters in the initial segmentation 
if each character in a sequence of characters oc-
curs in a word much more frequently than as a 
word on its own. The other post-processing pro-
cedure checks the segmentation of a text frag-
ment in the input text against the segmentation 
in the training data. If the segmentation pro-
duced by our system is different from the one in 
the training data, we will use the segmentation 
in the training data as the final segmentation. 
More details on the segmentation algorithm and 
the preprocessors and postprocessors can be 
found in (Chen, 2003). 
Our system processes a sentence independ-
ently. For an input sentence, the preprocessors 
are applied to the input sentence to extract nu-
meric expressions and proper names. The ex-
tracted numeric expressions and proper names 
are added to the segmentation dictionary, if they 
are not already in the dictionary. Then the input 
sentence is segmented into words. Finally the 
post-processing procedures are applied to the 
initial segmentation to produce the final seg-
mentation. Our system processes texts encoded 
in UTF-8; and it is used in all 8 tasks. 
3 Results 
Table 1 presents the results of the 10 official 
runs we submitted in all 8 tasks. 
Run id R P F R-
oov 
R-in
as-closed 0.955 0.934 0.947 0.468 0.978 
as-open 0.958 0.938 0.948 0.506 0.978 
138
cityu-closed 0.949 0.931 0.940 0.561 0.980 
cityu-open 0.952 0.937 0.945 0.608 0.980 
pku-closed 0.953 0.946 0.950 0.636 0.972 
pku-open-a 0.964 0.966 0.965 0.841 0.971 
msr-closed-a 0.969 0.952 0.960 0.379 0.985 
msr-closed-b 0.968 0.953 0.960 0.381 0.984 
msr-open-a 0.970 0.957 0.963 0.466 0.984 
msr-open-b 0.971 0.961 0.966 0.512 0.983 
Table 1: Summary of Yahoo official results. 
The first element in the run id is the corpus 
name, as referring to the Academia Sinica cor-
pus, cityu the City University of Hong Kong 
corpus, pku the Peking University Corpus, and 
msr the Microsoft Research corpus. The second 
element in the run id is the type of task, closed
or open. The second column shows the recall, 
the third column the precision, and the fourth 
column F-score. The last two columns present 
the recall of the out-of-vocabulary words and the 
recall of the words in the training data, respec-
tively.
3.1 Closed Tasks 
For the AS closed task run as-closed, we 
manually identified about 15 thousands person 
names and about 4 thousands place names from 
the AS training corpus. We then built a person 
name recognizer and a place name recognizer 
from the AS training data. All the name recog-
nizers we built are based on the maximum en-
tropy model. We also built a rule-based numeric 
expression recognizer implemented as a finite 
state automaton. 
The segmentation dictionary consists of the 
words in the training data with occurrence fre-
quency compiled from the training data. For 
each character, the probability that a character 
occurs in a word is also computed from the 
training data only. 
Each line of texts in the testing data set is 
processed independently. From an input line, 
first the person name recognizer and place name 
recognizer are used to extract person and place 
names; the numeric expression recognizer is 
used to extract numeric expressions. The ex-
tracted new proper names and new numeric ex-
pressions are added to the segmentation 
dictionary with a constant occurrence frequency 
of 0.5 before the input text is segmented. After 
the segmentation, a sequence of single charac-
ters is combined into a single unit if each of the 
characters in the sequence occurs much more 
frequently in a word than as a word on its own. 
The threshold of a character occurring in a word 
is set to 0.80. Also the quad-grams down to uni-
grams in the segmentation are checked against 
the training data. When a text fragment is seg-
mented in a different way by our system than in 
the training data, we use the segmentation of the 
text fragment in the training data as the final 
output. 
The runs cityu-closed and pku-closed are 
produced in the same way. We first manually 
identified the person names and place names in 
the training data, and then built name recogniz-
ers from the training data. The name recognizers 
and numeric expression recognizer are used first 
to extract proper names and numeric expressions 
before segmentation. The post-processing is also 
the same. 
Two runs, named msr-closed-a and msr-
closed-b, respectively, are submitted using the 
Microsoft Research corpus for the closed task. 
Unlike in the other three corpora, the numeric 
expressions are much more versatile, and there-
fore, more difficult to write regular expressions 
to identify them. We manually identified the 
numeric expressions, person names, place 
names, and organization names in the training 
data, and then built maximum entropy model-
based recognizers for extracting numeric expres-
sions and names of people, place, and organiza-
tions. Also the organization names in this corpus 
are not segmented into words like in the other 
three corpora. The organization name recognizer 
is word-based while the other three recognizers 
are character-based. The only difference be-
tween these two runs is that the run msr-closed-
b includes an organization name recognizer 
while the other run msr-closed-a does not. 
3.2 Open Tasks 
For the AS open task, we used a user diction-
ary and a person name recognizer and a place 
name recognizer, both trained on the combined 
AS corpus and the CITYU corpus. However, the 
base dictionary and word frequency counts are 
compiled from only the AS corpus. For the open 
run, we used the annotated AS corpus we ac-
quired from Academia Sinica. Also the phrase 
segmentation table is built from the AS training 
data only. The AS open run as-open was pro-
duced with the new person and place name rec-
ognizers and with the user dictionary. The 
139
performance of the open run is almost the same 
as that of the close run. 
The training data used in the CITYU open 
task is the same as in the closed task. We built a 
person name recognizer and a place name rec-
ognizer from the combined AS and CITYU cor-
pora. In training a recognizer, we only kept the 
sentences that contain at least one person or 
place name. The run cityu-open was produced 
with new person name and place name recog-
nizers trained on the combined corpora but with-
out user dictionary. The base dictionary and 
frequency counts are from the CITYU training 
data. We prepared a user dictionary for the 
CITYU open run but forgot to turn on this fea-
ture in the configuration file. We repeated the 
CITYU open run cityu-open with user diction-
ary. The recall is 0.959; precision is 0.953; and 
F-score is 0.956. 
For the PKU open task run pku-open-a, we 
trained our segmenter from the word-segmented 
People?s Daily corpus covering the period of 
January 1 through June 30, 1998. Our base dic-
tionary with word frequency counts, character 
counts, and phrase segmentation table are built 
from this larger training corpus of about 7 mil-
lion words. The words in this corpus are anno-
tated with part-of-speech categories. Both the 
names of people and the names of places are 
uniquely tagged in this corpus. We created a 
training set for person name recognizer by com-
bining the sentences in the People?s Daily cor-
pus that contain at least one person name with 
the sentences in the MSR training corpus that 
contain at least one person name. The person 
names in the MSR corpus were manually identi-
fied. From the combined training data for person 
names, we built a person name recognizer based 
on the maximum entropy model. The place 
name recognizer was built in the same way. The 
PKU open run pku-open-a was produced using 
the segmenter trained on the 6-month People?s 
Daily corpus with the new person and place 
name recognizer trained on the People?s Daily 
corpus and the MSR corpus. A user dictionary 
of about 100 thousand entries, most being 
proper names, was used in the PKU open run. 
The training data used for the MSR open 
runs is the same MSR training corpus. Our base 
dictionary, together with word frequency counts, 
and phrase segmentation table are built from the 
MSR training data only. The numeric expression 
recognizer is the same as the one used in the 
closed task. The person name recognizer and 
place name recognizer are the same as those 
used in the PKU open task. We built an organi-
zation name recognizer from the People?s Daily 
corpus where organization names are marked. 
For example, the text fragment ?[??/ns ???
/j]nt? is marked by a pair of brackets and tagged 
with ?nt? in the annotated People?s Daily cor-
pus. We extracted all the sentences containing at 
least one organization name and built a word-
based recognizer. The feature templates are the 
same as in person name or place name recog-
nizer. We submitted two MSR open task runs, 
named msr-open-a and msr-open-b, respec-
tively. The only difference between these two 
runs is that the first run msr-open-a did not in-
clude an organization name recognizer, while 
the run msr-open-b used the organization name 
recognizer built on the annotated People?s Daily 
corpus. Both runs were produced with a user 
dictionary, the new person name recognizer and 
new place name recognizer. The increase of F-
score from 0.963 to 0.966 is due to the organiza-
tion name recognizer. While the organization 
name recognizer correctly identified many or-
ganization names, it also generated many false 
positives. So the positive impact was offset by 
the false positives. 
At about 12 hours before the due time, we 
learned that multiple submissions for the same 
task are acceptable. A colleague of ours submit-
ted one PKU open run with the run id ?b? and 
one MSR open run with the run id ?c? in the 
bakeoff official results using a different word 
segmentation system without being tuning for 
the bakeoff. These two open runs are not dis-
cussed in this paper. 
4 Discussions 
The differences between our closed task runs 
and open task runs are rather small for both the 
AS corpus and the CITYU corpus. Our CITYU 
open run would be substantially better had we 
used our user dictionary. The open task run us-
ing the PKU corpus is much better than the 
closed task run. We performed a number of ad-
ditional evaluations in both the PKU closed task 
and the PKU open task. Table 2 below presents 
the evaluation results with different features ac-
tivated in our system. The PKU training corpus 
140
was used in all the experiments presented in Ta-
ble 2.  
Run Features R P F 
1 base-dict 0.9386 0.9095 0.9238 
2 1+num-expr 0.9411 0.9161 0.9285 
3 2+person+place 0.9440 0.9249 0.9343 
4 3+single-char 0.9404 0.9420 0.9412 
5 4+consistency-
checking 
0.9529 0.9464 0.9496 
Table 2: Results with different features applied 
in PKU closed task. 
    Table 3 presents the results with different fea-
tures applied in the PKU open task. The 6-
month annotated People?s Daily corpus was 
used in all the experiments shown in Table 3. 
Run Features R P F 
1 base-dict 0.9523 0.9503 0.9513 
2 1+user-dict 0.9534 0.9565 0.9549 
3 2+num-expr 0.9547 0.9605 0.9576 
4 3+person+place 0.9562 0.9647 0.9604 
5 4+single-char 0.9487 0.9650 0.9568 
6 5+consistency-
checking 
0.9637 0.9664 0.9650 
Table 3: Results with different features applied 
in PKU open task. 
In the features column, base-dict refers to the 
base dictionary built from the training data only; 
user-dict the additional user dictionary; num-
expr the numeric expression recognizer imple-
mented as a finite state automaton; person the 
person name recognizer; place the place name 
recognizer; single-char combining a sequence of 
single characters when each one of them occurs 
in words much more frequently than as a word 
on its own; and lastly consistency-checking 
checking segmentations against the training 
texts and choosing the segmentation in the train-
ing texts if the segmentation of a text fragment 
produced by our system is different from the one 
in the training data. The tables show the results 
with more and more features included. Each run 
in the both tables includes one or two new fea-
tures over the previous run.  The last run num-
bered 5 in Table 2 is our official PKU closed run 
labeled pku-closed in Table 1; and the last run 
numbered 6 in Table 3 is our official PKU open 
run labeled pku-open-a in Table 1. 
The F-score for our closed PKU task run is 
0.950 with all available features, while using the 
larger People?s Daily corpus as training data and 
its dictionary alone, the F-score is 0.9513. So a 
larger training data contributed significantly to 
the increase in performance in our PKU open 
task run. The user dictionary, the numeric ex-
pression recognizer, the person name recognizer, 
and the place name recognizer all contributed to 
the better performance of our PKU closed run 
and open run. Selectively combining sequence 
of single characters appreciably improved the 
precision while marginally decreased the recall 
in the PKU closed run. However, in the open 
task run, combining single characters did not 
result in better performance, probably because 
the new words recovered by combining single 
characters are already in our user dictionary for 
the open run. Finally consistency checking sub-
stantially improved the performance for both the 
closed run and the open run. 
5 Conclusion 
We presented a word segmentation system that 
uses unigram language model to select the most 
probable segmentation among all possible can-
didates for an input text. The system is aug-
mented with proper name recognizers, numeric 
expression recognizers, and post-processing 
modules to extract new words. Overall the rec-
ognizers and the post-processing modules sub-
stantially improved the baseline performance. 
The larger training data set used in the PKU 
open task also significantly increased the per-
formance of our PKU open run. The additional 
user dictionary is another major contributor to 
our better performance in the open tasks over 
the closed tasks. 
References 
Aitao Chen. 2003. Chinese Word Segmentation Us-
ing Minimal Linguistic Knowledge. In: Proceed-
ings of the Second SIGHAN Workshop on 
Chinese Language Processing. 
Nianwen Xue and Libin Shen. 2003. Chinese Word 
Segmentation as LMR Tagging. In: Proceedings of 
the Second SIGHAN Workshop on Chinese Lan-
guage Processing. 
Adwait Ratnaparkhi. 1998. Maximum Entropy Mod-
els for Natural Language Ambiguity Resolution.
Dissertation in Computer and Information Sci-
ence, University of Pennsylvania. 
141
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1335?1343,
Beijing, August 2010
Resolving Surface Forms to Wikipedia Topics 
Yiping Zhou   Lan Nie   Omid Rouhani-Kalleh   Flavian Vasile   Scott Gaffney 
Yahoo! Labs at Sunnyvale 
{zhouy,lannie,omid,flavian,gaffney}@yahoo-inc.com 
 
Abstract 
Ambiguity of entity mentions and con-
cept references is a challenge to mining 
text beyond surface-level keywords. We 
describe an effective method of disambi-
guating surface forms and resolving them 
to Wikipedia entities and concepts. Our 
method employs an extensive set of fea-
tures mined from Wikipedia and other 
large data sources, and combines the fea-
tures using a machine learning approach 
with automatically generated training da-
ta. Based on a manually labeled evalua-
tion set containing over 1000 news ar-
ticles, our resolution model has 85% pre-
cision and 87.8% recall. The performance 
is significantly better than three baselines 
based on traditional context similarities 
or sense commonness measurements. Our 
method can be applied to other languages 
and scales well to new entities and con-
cepts. 
1 Introduction 
Ambiguity in natural language is prevalent and, 
as such, it can be a difficult challenge for infor-
mation retrieval systems and other text mining 
applications. For example, a search for ?Ford? in 
Yahoo! News retrieves about 40 thousand ar-
ticles containing Ford referring to a company 
(Ford Motors), an athlete (Tommy Ford), a place 
(Ford City), etc. Due to reference ambiguity, 
even if we knew the user was only interested in 
the company, they would still have to contend 
with articles referring to the other concepts as 
well. 
In this paper we focus on the problem of re-
solving references of named-entities and con-
cepts in natural language through their textual 
surface forms. Specifically, we present a method 
of resolving surface forms in general text docu-
ments to Wikipedia entries. The tasks of resolu-
tion and disambiguation are nearly identical; we 
make the distinction that resolution specifically 
applies when a known set of referent concepts 
are given a priori. Our approach differs from oth-
ers in multiple aspects including the following.  
1) We employ a rich set of disambiguation 
features leveraging mining results from large-
scale data sources. We calculate context-
sensitive features by extensively mining the cat-
egories, links and contents of the entire Wikipe-
dia corpus. Additionally we make use of context-
independent data mined from various data 
sources including Web user-behavioral data and 
Wikipedia. Our features also capture the one-to-
one relationship between a surface form and its 
referent.  
2) We use machine learning methods to train 
resolution models with a large automatically la-
beled training set. Both ranking-based and classi-
fication-based resolution approaches are ex-
plored.  
3) Our method disambiguates both entities and 
word senses. It scales well to new entities and 
concepts, and it can be easily applied to other 
languages.  
We propose an extensive set of metrics to eva-
luate not only overall resolution performance but 
also out-of-Wikipedia prediction. Our systems 
for English language are evaluated using real-
world test sets and compared with a number of 
baselines. Evaluation results show that our sys-
tems consistently and significantly outperform 
others across all test sets. 
The paper is organized as follows. We first de-
scribe related research in Section 2, followed by 
an introduction of Wikipedia in Section 3. We 
then introduce our learning method in Section 4 
and our features in Section 5. We show our expe-
rimental results in Section 6, and finally close 
with a discussion of future work. 
1335
2 Related Work 
Named entity disambiguation research can be 
divided into two categories: some works (Bagga 
and Baldwin, 1998; Mann and Yarowsky, 2003;  
Pedersen et al, 2005; Fleischman and Hovy, 
2004; Ravin and Kazi, 1999) aim to cluster am-
biguous surface forms to different groups, with 
each representing a unique entity; others (Cucer-
zan, 2007; Bunescu and Pa?ca, 2006; Han and 
Zhao, 2009; Milne and Witten, 2008a; Milne and 
Witten, 2008b) resolve a surface form to an enti-
ty or concept extracted from existing knowledge 
bases. Our work falls into the second category. 
Looking specifically at resolution, Bunescu 
and Pasca (2006) built a taxonomy SVM kernel 
to enrich a surface form?s representation with 
words from Wikipedia articles in the same cate-
gory. Cucerzan (2007) employed context vectors 
consisting of phrases and categories extracted 
from Wikipedia. The system also attempted to 
disambiguate all surface forms in a context si-
multaneously, with the constraint that their re-
solved entities should be globally consistent on 
the category level as much as possible. Milne and 
Witten (2008a, 2008b) proposed to use Wikipe-
dia?s link structure to capture the relatedness 
between Wikipedia entities so that a surface form 
is resolved to an entity based on its relatedness to 
the surface form?s surrounding entities. Besides 
relatedness, they also define a commonness fea-
ture that captures how common it is that a sur-
face form links to a particular entity in general. 
Han and Zhao (2009) defined a novel alignment 
strategy to calculate similarity between surface 
forms based on semantic relatedness in the con-
text.  
Milne and Witten?s work is most related to 
what we propose here in that we also employ 
features similar to their relatedness and com-
monness features. However, we add to this a 
much richer set of features which are extracted 
from Web-scale data sources beyond Wikipedia, 
and we develop a machine learning approach to 
automatically blend our features using complete-
ly automatically generated training data. 
3 Wikipedia 
Wikipedia has more than 200 language editions, 
and the English edition has more than 3 million 
articles as of March 2009. Newsworthy events 
are often added to Wikipedia within days of oc-
currence; Wikipedia has bi-weekly snapshots 
available for download.  
Each article in Wikipedia is uniquely identi-
fied by its title which is usually the most com-
mon surface form of an entity or concept. Each 
article includes body text, outgoing links and 
categories. Here is a sample sentence in the ar-
ticle titled ?Aristotle? in wikitext format. ?To-
gether with Plato and [[Socrates]] (Plato's 
teacher), Aristotle is one of the most important 
founding figures in [[Western philosophy]].? 
Near the end of the article, there are category 
links such as ?[[Category:Ancient Greek mathe-
maticians]]?. The double brackets annotate out-
going links to other Wikipedia articles with the 
specified titles. The category names are created 
by authors. Articles and category names have 
many-to-many relationships. 
In addition to normal articles, Wikipedia also 
has special types of articles such as redirect ar-
ticles and disambiguation articles. A redirect ar-
ticle?s title is an alternative surface form for a 
Wikipedia entry. A disambiguation article lists 
links to similarly named articles, and usually its 
title is a commonly used surface form for mul-
tiple entities and concepts.  
4 Method of Learning 
Our goal is to resolve surface forms to entities or 
concepts described in Wikipedia. To this end, we 
first need a recognizer to detect surface forms to 
be resolved. Then we need a resolver to map a 
surface form to the most probable entry in Wiki-
pedia (or to out-of-wiki) based on the context. 
Recognizer: We first create a set of Wikipedia 
(article) entries E = {e1, e2, ?} to which we want 
to resolve surface forms. Each entry?s surface 
forms are mined from multiple data sources. 
Then we use simple string match to recognize 
surface forms from text documents.  
Among all Wikipedia entries, we exclude 
those with low importance. In our experiments, 
we removed the entries that would not interest 
general Web users, such as stop words and punc-
tuations. Second, we collect surface forms for 
entries in E using Wikipedia and Web search 
query click logs based on the following assump-
tions:  
1336
? Each Wikipedia article title is a surface form 
for the entry. Redirect titles are taken as alter-
native surface forms for the target entry.  
? The anchor text of a link from one article to 
another is taken as an alternative surface form 
for the linked-to entry.  
? Web search engine queries resulting in user 
clicks on a Wikipedia article are taken as alter-
native surface forms for the entry. 
As a result, we get a number of surface forms 
for each entry ei. If we let sij denote the j-th sur-
face form for entry i, then we can represent our 
entry dictionary as EntSfDict = {<e1, (s11, s12, 
?)>, <e2, (s21, s22, ?)>, ?}.  
Resolver: We first build a labeled training set 
automatically, and then use supervised learning 
methods to learn models to resolve among Wiki-
pedia entries. In the rest of this section we de-
scribe the resolver in details. 
4.1 Automatically Labeled Data 
To learn accurate models, supervised learning 
methods require training data with both large 
quantity and high quality, which often takes lots 
of human labeling effort. However, in Wikipedia, 
links provide a supervised mapping from surface 
forms to article entries. We use these links to 
automatically generate training data. If a link's 
anchor text is a surface form in EntSfDict, we 
extract the anchor text as surface form s and the 
link's destination article as Wikipedia entry e, 
then add the pair (s, e) with a positive judgment 
to our labeled example set. Continuing, we use 
EntSfDict to find other Wikipedia entries for 
which s is a surface form and create negative 
examples for these and add them to our labeled 
example set. If e does not exist in EntSfDict (for 
example, if the link points to a Wikipedia article 
about a stop word), then a negative training ex-
ample is created for every Wikipedia entry to 
which s may resolve. We use oow (out-of-wiki) 
to denote this case. 
Instead of article level coreference resolution, 
we only match partial names with full names 
based on the observation that surface forms for 
named entities are usually capitalized word se-
quences in English language and a named entity 
is often mentioned by a long surface form fol-
lowed by mentions of short forms in the same 
article. For each pair (s, e) in the labeled example 
set, if s is a partial name of a full name s? occur-
ring earlier in the same document, we replace (s, 
e) with (s?, e) in the labeled example set.  
Using this methodology we created 2.4 million 
labeled examples from only 1% of English Wiki-
pedia articles. The abundance of data made it 
possible for us to experiment on the impact of 
training set size on model accuracy.  
4.2 Learning Algorithms 
In our experiments we explored both Gradient 
Boosted Decision Trees (GBDT) and Gradient 
Boosted Ranking (GBRank) to learn resolution 
models. They both can easily combine features 
of different scale and with missing values. Other 
supervised learning methods are to be explored 
in the future.  
GBDT: We use the stochastic variant of 
GBDTs (Friedman, 2001) to learn a binary logis-
tic regression model with the judgments as the 
target. GBDTs compute a function approxima-
tion by performing a numerical optimization in 
the function space. It is done in multiple stages, 
with each stage modeling residuals from the 
model of the last stage using a small decision 
tree. A brief summary is given in Algorithm 1. In 
the stochastic version of GBDT, one sub-samples 
the training data instead of using the entire train-
ing set to compute the loss function. 
Algorithm 1 GBDTs 
Input: training data N
iii yx 1)},{( =  , loss function 
L[y, f(x)] , the number of nodes for each tree J 
, the number of trees M . 
1: Initialize f(x)=f0 
2: For m = 1 to M 
2.1:   For i = 1 to N, compute the negative 
gradient by taking the derivative of the 
loss with respect to f(x) and substitute 
with 
iy and )(1 imi xf ? . 
2.2:    Fit a J-node regression tree to the 
components of the negative gradient. 
2.3:   Find the within-node updates m
ja  for j 
= 1 to J by performing J univariate op-
timizations of the node contributions to 
the estimated loss. 
2.4:   Do the update m
ji
m
ii
m
i arxfxf ?+= ? )()( 1 , 
where j is the node that xi belongs to, r 
is learning rate. 
3: End for 
4: Return fM 
1337
In our setting, the loss function is a negative 
binomial log-likelihood, xi is the feature vector 
for a surface-form and Wikipedia-entry pair (si, 
ei), and yi is +1 for positive judgments and -1 is 
for negative judgments. 
GBRank: From a given surface form?s judg-
ments we can infer that the correct Wikipedia 
entry is preferred over other entries. This allows 
us to derive pair-wise preference judgments from 
absolute judgments and train a model to rank all 
the Wikipedia candidate entries for each surface 
form. Let },...,1),()(|),{( '' NixlxlxxS iiii =?=  be the set of 
preference judgments, where xi and xi' are the 
feature vectors for two pairs of surface-forms and 
Wikipedia-entry, l(xi) and l(xi') are their absolute 
judgments respectively. GBRank (Zheng et al, 
2007) tries to learn a function h such that 
)()( 'ii xhxh ? for Sxx ii ?),( ' . A sketch of the algorithm 
is given in Algorithm 2. 
Algorithm 2 GBRank 
1: Initialize h=h0 
2: For k=1 to K 
2.1:   Use hk-1 as an approximation of h and 
compute 
})()(|),{( '11
' ?+??= ??+ ikikii xhxhSxxS
})()(|),{( '11
' ?+<?= ??? ikikii xhxhSxxS  
where ))()(( 'ii xlxl ?=??  
2.2:   Fit a regression function gk using 
GBDT and the incorrectly predicted 
examples 
}),(|))(,(),)(,{( '1
''
1
?
?? ??+ Sxxxhxxhx iiikiiki ??  
2.3:   Do the update 
)1/())()(()( 1 ++= ? kxgxkhxh kkk ? , where ?  is  
learning rate. 
3: End for 
4: Return hK 
We use a tuning set independent from the 
training set to select the optimal parameters for 
GBDT and GBRank. This includes the number 
of trees M, the number of nodes J, the learning 
rate r, and the sampling rate for GBDT; and for 
GBRank we select  K, ? and ?.  
The feature importance measurement given by 
GBDT and GBRank is computed by keeping 
track of the reduction in the loss function at each 
feature variable split and then computing the to-
tal reduction of loss along each explanatory fea-
ture variable. We use it to analyze feature effec-
tiveness. 
4.3 Prediction 
After applying a resolution model on the given 
test data, we obtain a score for each surface-form 
and Wikipedia-entry pair (s, e). Among all the 
pairs containing s, we find the pair with the high-
est score, denoted by (s, e~ ). 
It?s very common that a surface form refers to 
an entity or concept not defined in Wikipedia. So 
it?s important to correctly predict whether the 
given surface form cannot be mapped to any Wi-
kipedia entry in EntSfDict. 
We apply a threshold to the scores from reso-
lution models. If the score for (s, e~ ) is lower than 
the threshold, then the prediction is oow (see 
Section 4.1), otherwise e~  is predicted to be the 
entry referred by s. We select thresholds based 
on F1 (see Section 6.2) on a tuning set that is 
independent from our training set and test set. 
5 Features 
For each surface-form and Wikipedia-entry pair 
(s, e), we create a feature vector including fea-
tures capturing the context surrounding s and 
features independent of the context. They are 
context-dependent and context-independent fea-
tures respectively. Various data sources are 
mined to extract these features, including Wiki-
pedia articles, Web search query-click logs, and 
Web-user browsing logs. In addition, (s, e) is 
compared to all pairs containing s based on 
above features and the derived features are called 
differentiation features.  
5.1 Context-dependent Features 
These features measure whether the given sur-
face form s resolving to the given Wikipedia en-
try e would make the given document more co-
herent. They are based on 1) the vector represen-
tation of e, and 2) the vector representation of the 
context of s in a document d. 
Representation of e: By thoroughly mining 
Wikipedia and other large data sources we ex-
tract contextual clues for each Wikipedia entry 
e and formulate its representation in the follow-
ing ways. 
1) Background representation. The overall 
background description of e is given in the cor-
responding Wikipedia article, denoted as Ae. Na-
turally, a bag of terms and surface forms in Ae 
can represent e. So we represent e by a back-
1338
ground word vector Ebw and a background sur-
face form vector Ebs, in which each element is 
the occurrence count of a word or a surface form 
in Ae?s first paragraph. 
2) Co-occurrence representation. The terms 
and surface forms frequently co-occurring with e 
capture its contextual characteristics. We first 
identify all the Wikipedia articles linking to Ae. 
Then, for each link pointing to Ae we extract the 
surrounding words and surface forms within a 
window centered on the anchor text. The window 
size is set to 10 words in our experiment. Finally, 
we select the words and surface forms with the 
top co-occurrence frequency, and represent e by 
a co-occurring word vector Ecw and a co-
occurring surface form vector Ecs, in which each 
element is the co-occurrence frequency of a se-
lected word or surface form.  
3) Relatedness representation. We analyzed 
the relatedness between Wikipedia entries from 
different data sources using various measure-
ments, and we computed over 20 types of rela-
tedness scores in our experiments. In the follow-
ing we discuss three types as examples. The first 
type is computed based on the overlap between 
two Wikipedia entries? categories. The second 
type is mined from Wikipedia inter-article links. 
(In our experiments, two Wikipedia entries are 
considered to be related if the two articles are 
mutually linked to each other or co-cited by 
many Wikipedia articles.) The third type is 
mined from Web-user browsing data based on 
the assumption that two Wikipedia articles co-
occurring in the same browsing session are re-
lated. We used approximately one year of Yahoo! 
user data in our experiments. A number of differ-
ent metrics are used to measure the relatedness. 
For example, we apply the algorithm of Google 
distance (Milne and Witten, 2008b) on Wikipe-
dia links to calculate the Wikipedia link-based 
relatedness, and use mutual information for the 
browsing-session-based relatedness. In summary, 
we represent e by a related entry vector Er for 
each type of relatedness, in which each element 
is the relatedness score between e and a related 
entry. 
Representation of s: We represent a surface 
form?s context as a vector, then calculate a con-
text-dependent feature for a pair <s,e> by a simi-
larity function Sim from two vectors. Here are 
examples of context representation. 
1) s is represented by a word vector Sw and a 
surface form vector Ss, in which each element is 
the occurrence count of a word or a surface form 
surrounding s. We calculate each vector?s simi-
larity with the background and co-occurrence 
representation of e, and it results in Sim(Sw, Ebw) , 
Sim(Sw, Ecw) , Sim(Ss, Ebs) and Sim(Ss, Ecs) . 
2) s is represented by a Wikipedia entry vector 
Se, in which each element is a Wikipedia entry to 
which a surrounding surface form s could re-
solve. We calculate its similarity with the rela-
tedness representation of e, and it results in 
Sim(Se, Er). 
In the above description, similarity is calcu-
lated by dot product or in a summation-of-
maximum fashion. In our experiments we ex-
tracted surrounding words and surface forms for 
s from the whole document or from the text win-
dow of 55 tokens centered on s, which resulted in 
2 sets of features. We created around 50 context-
dependent features in total. 
5.2 Context-independent Features  
These features are extracted from data beyond 
the document containing s. Here are examples. 
? During the process of building the dictionary 
EntSfDict as described in Section 4, we count 
how often s maps to e and estimate the proba-
bility of s mapping to e for each data source. 
These are the commonness features. 
? The number of Wikipedia entries that s could 
map to is a feature about the ambiguity of s. 
? The string similarity between s and the title of 
Ae is used as a feature. In our experiments 
string similarity was based on word overlap. 
5.3 Differentiation Features 
Among all surface-form and Wikipedia-entry 
pairs that contain s, at most one pair gets the pos-
itive judgment. Based on this observation we 
created differentiation features to represent how 
(s, e) is compared to other pairs for s.  They are 
derived from the context-dependent and context-
independent features described above. For exam-
ple, we compute the difference between the 
string similarity for (s, e) and the maximum 
string similarity for all pairs containing s. The 
derived feature value would be zero if (s, e) has 
larger string similarity than other pairs contain-
ing s. 
1339
6 Experimental Results 
In our experiments we used the Wikipedia snap-
shot for March 6th, 2009.  Our dictionary 
EntSfDict contains 3.5 million Wikipedia entries 
and 6.5 million surface forms. 
A training set was created from randomly se-
lected Wikipedia articles using the process de-
scribed in Section 4.1. We varied the number of 
Wikipedia articles from 500 to 40,000, but the 
performance did not increase much after 5000. 
The experimental results reported in this paper 
are based on the training set generated from 5000 
articles. It contains around 1.4 million training 
examples. There are approximately 300,000 sur-
face forms, out of which 28,000 are the oow case.  
Around 400 features were created in total, and 
200 of them were selected by GBDT and 
GBRank to be used in our resolution models. 
6.1 Evaluation Datasets 
Three datasets from different data sources are 
used in evaluation. 
1) Wikipedia hold-out set. Using the same 
process for generating training data and exclud-
ing the surface forms appearing in the training 
data, we built the hold-out set from approximate-
ly 15,000 Wikipedia articles, containing around 
600,000 labeled instances. There are 400,000 
surface forms, out of which 46,000 do not re-
solve to any Wikipedia entry. 
2) MSNBC News test set. This entity disam-
biguation data set was introduced by Cucerzan 
(2007). It contains 200 news articles collected 
from ten MSNBC news categories as of January 
2, 2007. Surface forms were manually identified 
and mapped to Wikipedia entities. The data set 
contains 756 surface forms. Only 589 of them are 
contained in our dictionary EntSfDict, mainly 
because EntSfDict excludes surface forms of out-
of-Wikipedia entities and concepts. Since the 
evaluation task is focused on resolution perfor-
mance rather than recognition, we exclude the 
missing surface forms from the labeled example 
set. The final dataset contains 4,151 labeled in-
stances. There are 589 surface forms and 40 of 
them do not resolve to any Wikipedia entry. 
3) Yahoo! News set. One limitation of the 
MSNBC test set is the small size. We built a 
much larger data set by randomly sampling 
around 1,000 news articles from Yahoo! News 
over 2008 and had them manually annotated. The 
experts first identified person, location and or-
ganization names, then mapped each name to a 
Wikipedia article if the article is about the entity 
referred to by the name. We didn?t include more 
general concepts in this data set to make the ma-
nual effort easier. This data set contains around 
100,000 labeled instances. The data set includes 
15,387 surface forms and 3,532 of them cannot 
be resolved to any Wikipedia entity. We random-
ly split the data set to 2 parts of equal size. One 
part is used to tune parameters of GBDT and 
GBRank and select thresholds based on F1 value. 
The evaluation results presented in this paper is 
based on the remaining part of the Yahoo! News 
set. 
6.2 Metrics 
The possible outcomes from comparing a resolu-
tion system?s prediction with ground truth can be 
categorized into the following types. 
? True Positive (TP), the predicted e was correct-
ly referred to by s.   
? True Negative (TN), s was correctly predicted 
as resolving to oow.  
? Mismatch (MM), the predicted e was not cor-
rectly referred to by s and should have been e? 
from EntSfDict. 
? False Positive (FP), the predicted e was not 
correctly referred to by s and should have been 
oow. 
? False Negative (FN), the predicted oow is not 
correct and should have been e? from 
EntSfDict. 
Similar to the widely used metrics for classifi-
cation systems, we use following metrics to eva-
luate disambiguation performance.  
MMFPTP
TP
precision ++=
                 
MMFNTP
TP
recall ++=
 
recallprecision
recallprecision
F +
??= 21   
MMFNTNFPTP
TNTP
accuracy ++++
+=  
In the Yahoo! News test set, 23.5% of the sur-
face forms do not resolve to any Wikipedia en-
tries, and in the other two test sets the percentag-
es of oow are between 10% and 20%. This de-
monstrates it is necessary in real-world applica-
tions to explicitly measure oow prediction. We 
propose following metrics. 
FNTN
TN
oowprecision +=_
                 
FPTN
TN
oowrecall +=_
 
oowrecalloowprecision
oowrecalloowprecision
oowF
__
__2
_1 +
??=  
1340
6.3 Evaluation Results 
With our training set we trained one resolution 
model using GBDT (named as WikiRes-c) and 
another resolution model using GBRank (named 
as WikiRes-r). The models were evaluated along 
with the following systems. 
1) Baseline-r: each surface form s is randomly 
mapped to oow or a candidate entry for s in 
EntSfDict. 
2) Baseline-p: each surface form s is mapped 
to the candidate entry e for s with the highest 
commonness score. The commonness score is 
linear combination of the probability of s being 
mapped to e estimated from different data 
sources. The commonness score is among the 
features used in WikiRes-c and WikiRes-r.  
3) Baseline-m: we implemented the approach 
brought by Cucerzan (2007) based on our best 
understanding. Since we use a different version 
of Wikipedia and a different entity recognition 
approach, the evaluation result differs from the 
result presented in their paper. But we believe 
our implementation follows the algorithm de-
scribed in their paper. 
In Table 1 we present the performance for 
each system on the Yahoo! News test set and the 
MSNBC test set. The performance of WikiRes-c 
and WikiRes-r are computed after we apply the 
thresholds selected on the tuning set described in 
Section 6.1. In the upper half of Table 1, the 
three baselines use the thresholds that lead to the 
best F1 on the Yahoo! News test set. In the lower 
half of Table 1, the three baselines use the thre-
sholds that lead to the best F1 on the MSNBC 
test set. 
Among the three baselines, Baseline-r has the 
lowest performance. Baseline-m uses a few con-
text-sensitive features and Baseline-p uses a con-
text-independent feature. These two types of fea-
tures are both useful, but Baseline-p shows better 
performance, probably because the surface forms 
in our test sets are dominated by common senses. 
In our resolution models, these features are com-
bined together with many other features calcu-
lated from different large-scale data sources and 
on different granularity levels. As shown in Ta-
ble 1, both of our resolution solutions substan-
tially outperform other systems.  Furthermore, 
WikiRes-c and WikiRes-r have similar perfor-
mance. 
 
 Precision Recall F1 Accuracy p-value 
Yahoo! News Test Set 
Baseline-r 47.023 60.831 53.043 47.023 0 
Baseline-p 73.869 88.157 80.383 73.175 5.2e-78 
Baseline-m 62.240 80.517 70.208 62.240 1.3e-160
WikiRes-r 83.406 88.858 86.046 80.717 0.012 
WikiRes-c 85.038 87.831 86.412 81.463 --- 
MSNBC Test Set 
Baseline-r 60.272 64.545 62.335 60.272 8.9e-19 
Baseline-p 82.292 86.182 84.192 82.003 0.306 
Baseline-m 78.947 84.545 81.651 78.947 0.05 
WikiRes-r 88.785 86.364 87.558 84.550 0.102 
WikiRes-c 88.658 85.273 86.932 83.192 --- 
Table 1. Performance on the Yahoo! News Test 
Set and the MSNBC Test set 
 
Figure 1. Precision-recall on the Yahoo! News 
Test Set and the MSNBC Test Set 
 
We compared WikiRes-c with each competitor 
and from the statistical significance test results in 
the last column of Table 1 we see that on the Ya-
hoo! News test set WikiRes-c significantly out-
performs others. The p-values for the MSNBC 
test set are much higher than for the Yahoo! 
News test set because the MSNBC test set is 
much smaller. 
Attempting to address this point, we see that 
the F1 values of WikiRes on the MSNBC test set 
and on the Yahoo! News test set only differs by a 
couple percentage points, although, these test 
sets were created independently. This suggests 
the objectivity of our method for creating the 
Yahoo! News test set and provides a way to 
measure resolution model performance on what 
1341
would occur in a general news corpus in a statis-
tically significant manner. 
In Figure 1 we present the precision-recall 
curves on the Yahoo! News and the MSNBC test 
sets. We see that our resolution models are sub-
stantially better than the other two baselines at 
any particular precision or recall value on both 
test sets. Baseline-r is not included in the com-
parison since it does not have the tradeoff be-
tween precision and recall. We find the preci-
sion-recall curve of WikiRes-r is very similar to 
WikiRes-c at the lower precision area, but its re-
call is much lower than other systems after preci-
sion reaches around 90%. So, in Figure 1 the 
curves of WikiRes-r are truncated at the high pre-
cision area. 
In Table 2 we compare the performance of 
out-of-Wikipedia prediction. The comparison is 
done on the Yahoo! News test set only, since 
there are only 40 surface forms of oow case in 
the MSNBC test set. Each system?s threshold is 
the same as that used for the upper half of Table 
1. The results show our models have substantial-
ly higher precision and recall than Baseline-p and 
Baseline-m. From the statistical significance test 
results in the last column, we can see that Wi-
kiRes-c significantly outperforms Baseline-p and 
Baseline-m. Also, our current approaches still 
have room to improve in the area of out-of-
Wikipedia prediction.  
We also evaluated our models on a Wikipedia 
hold-out set. The model performance is greater 
than that obtained from the previous two test sets 
because the hold-out set is more similar to the 
training data source itself. Again, our models 
perform better than others. 
From the feature importance lists of our 
GBDT model and GBRank model, we find that 
the commonness features, the features based on 
Wikipedia entries? co-occurrence representation 
and the corresponding differentiation features are 
the most important. 
 
 Precision Recall F1 p-value 
Baseline-p 64.907 22.152 33.03 1.6e-20 
Baseline-m 47.207 44.78 45.961 1.3e-34 
WikiRes-r 68.166 52.994 59.630 0.084 
WikiRes-c 67.303 59.777 63.317 ---
Table 2. Performance of Out-of-Wikipedia Pre-
diction on the Yahoo! News Test Set 
7 Conclusions 
We have described a method of learning to re-
solve surface forms to Wikipedia entries. Using 
this method we can enrich the unstructured doc-
uments with structured knowledge from Wikipe-
dia, the largest knowledge base in existence. The 
enrichment makes it possible to represent a doc-
ument as a machine-readable network of senses 
instead of just a bag of words. This can supply 
critical semantic information useful for next-
generation information retrieval systems and oth-
er text mining applications. 
Our resolution models use an extensive set of 
novel features and are leveraged by a machine 
learned approach that depends only on a purely 
automated training data generation facility. Our 
methodology can be applied to any other lan-
guage that has Wikipedia and Web data available 
(after modifying the simple capitalization rules in 
Section 4.1). Our resolution models can be easily 
and quickly retrained with updated data when 
Wikipedia and the relevant Web data are 
changed. 
For future work, it will be important to inves-
tigate other approaches to better predict oow. 
Adding global constraints on resolutions of the 
same term at multiple locations in the same doc-
ument may also be important. Of course, devel-
oping new features (such as part-of-speech, 
named entity type, etc) and improving training 
data quality is always critical, especially for so-
cial content sources such as those from Twitter. 
Finally, directly demonstrating the degree of ap-
plicability to other languages is interesting when 
accounting for the fact that the quality of Wiki-
pedia is variable across languages. 
References 
Bagga, Amit and Breck Baldwin. 1998. Entity-based 
cross-document coreferencing using the Vector 
Space Model. Proceedings of the 17th interna-
tional conference on Computational linguis-
tics. 
Bunescu, Razvan and Marius Pa?ca. 2006. Using En-
cyclopedic Knowledge for Named Entity Disam-
biguation. Proceedings of the 11th Conference 
of the European Chapter of the Association of 
Computational Linguistics (EACL-2006). 
Cucerzan, Silviu. 2007. Large-Scale Named Entity 
Disambiguation Based on Wikipedia Data. Pro-
1342
ceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language 
Processing and Computational Natural Lan-
guage Learning. 
Fleischman, Ben Michael and Eduard Hovy. 2004. 
Multi-Document Person Name Resolution. Pro-
ceesing of the Association for Computational 
Linguistics. 
Friedman, J. H. 2001. Stochastic gradient boosting. 
Computational Statistics and Data Analysis, 
38:367?378. 
Han, Xianpei and Jun Zhao 2009. Named Entity Dis-
ambiguation by Leveraging Wikipedia Semantic 
Knowledge. Proceedings of the 31st annual in-
ternational ACM SIGIR conference on Re-
search and development in information re-
trieval. 
Mann, S. Gidon and David Yarowsky. 2003. Unsu-
pervised Personal Name Disambiguation. Pro-
ceedings of the seventh conference on Natural 
language learning at HLT-NAACL 2003. 
Milne, David and Ian H. Witten. 2008a. Learning to 
Link with Wikipedia. In Proceedings of the 
ACM Conference on Information and Know-
ledge Management (CIKM'2008). 
Milne, David and Ian H. Witten. 2008b. An effective, 
low-cost measure of semantic relatedness obtained 
from Wikipedia links. Proceedings of the first 
AAAI Workshop on Wikipedia and Artificial 
Intelligence. 
 Pedersen, Ted, Amruta Purandare and Anagha Kul-
karni. 2005. Name Discrimination by Clustering 
Similar Contexts. Proceedings of the Sixth In-
ternational Conference on Intelligent Text 
Processing and Computational Linguistics 
(2005). 
Ravin, Y. and Z. Kazi. 1999. Is Hillary Rodham Clin-
ton the President? In Association for Computa-
tional Linguistics Workshop on Coreference 
and its Applications. 
Yarowsky, David. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. Pro-
ceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics, 
pages 189-196.  
Zheng, Zhaohui, K. Chen, G. Sun, and H. Zha. 2007. 
A regression framework for learning ranking func-
tions using relative relevance judgments. Proceed-
ings of the 30th annual international ACM 
SIGIR conference on Research and develop-
ment in information retrieval, pages 287-294. 
 
 
1343
