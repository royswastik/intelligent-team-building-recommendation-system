Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 41?50,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
One-Class Clustering in the Text Domain
Ron Bekkerman
HP Laboratories
Palo Alto, CA 94304, USA
ron.bekkerman@hp.com
Koby Crammer
University of Pennsylvania
Philadelphia, PA 19104, USA
crammer@cis.upenn.edu
Abstract
Having seen a news title ?Alba denies wedding
reports?, how do we infer that it is primar-
ily about Jessica Alba, rather than about wed-
dings or reports? We probably realize that, in a
randomly driven sentence, the word ?Alba? is
less anticipated than ?wedding? or ?reports?,
which adds value to the word ?Alba? if used.
Such anticipation can be modeled as a ratio
between an empirical probability of the word
(in a given corpus) and its estimated proba-
bility in general English. Aggregated over all
words in a document, this ratio may be used
as a measure of the document?s topicality. As-
suming that the corpus consists of on-topic
and off-topic documents (we call them the
core and the noise), our goal is to determine
which documents belong to the core. We pro-
pose two unsupervised methods for doing this.
First, we assume that words are sampled i.i.d.,
and propose an information-theoretic frame-
work for determining the core. Second, we
relax the independence assumption and use
a simple graphical model to rank documents
according to their likelihood of belonging to
the core. We discuss theoretical guarantees of
the proposed methods and show their useful-
ness for Web Mining and Topic Detection and
Tracking (TDT).
1 Introduction
Many intelligent applications in the text domain aim
at determining whether a document (a sentence, a
snippet etc.) is on-topic or off-topic. In some appli-
cations, topics are explicitly given. In binary text
classification, for example, the topic is described
in terms of positively and negatively labeled docu-
ments. In information retrieval, the topic is imposed
by a query. In many other applications, the topic
is unspecified, however, its existence is assumed.
Examples of such applications are within text sum-
marization (extract the most topical sentences), text
clustering (group documents that are close topi-
cally), novelty detection (reason whether or not test
documents are on the same topic as training docu-
ments), spam filtering (reject incoming email mes-
sages that are too far topically from the content of a
personal email repository), etc.
Under the (standard) Bag-Of-Words (BOW) rep-
resentation of a document, words are the functional
units that bear the document?s topic. Since some
words are topical and some are not, the problem of
detecting on-topic documents has a dual formulation
of detecting topical words. This paper deals with the
following questions: (a) Which words can be con-
sidered topical? (b) How can topical words be de-
tected? (c) How can on-topic documents be detected
given a set of topical words?
The BOW formalism is usually translated into
the generative modeling terms by representing doc-
uments as multinomial word distributions. For the
on-topic/off-topic case, we assume that words in a
document are sampled from a mixture of two multi-
nomials: one over topical words and another one
over general English (i.e. the background). Obvi-
ously enough, the support of the ?topic? multinomial
is significantly smaller than the support of the back-
ground. A document?s topicality is then determined
by aggregating the topicality of its words (see below
for details). Note that by introducing the background
distribution we refrain from explicitly modeling the
class of off-topic documents?a document is sup-
posed to be off-topic if it is ?not topical enough?.
Such a formulation of topicality prescribes us-
ing the one-class modeling paradigm, as opposed
to sticking to the binary case. Besides being much
41
Figure 1: The problem of hyperspherical decision bound-
aries in one-class models for text, as projected on 2D:
(left) a too small portion of the core is captured; (right)
too much space around the core is captured.
less widely studied and therefore much more attrac-
tive from the scientific point of view, one-class mod-
els appear to be more adequate for many real-world
tasks, where negative examples are not straightfor-
wardly observable. One-class models separate the
desired class of data instances (the core) from other
data instances (the noise). Structure of noise is either
unknown, or too complex to be explicitly modeled.
One-class problems are traditionally approached
using vector-space methods, where a convex deci-
sion boundary is built around the data instances of
the desired class, separating it from the rest of the
universe. In the text domain, however, those vector-
space models are questionably applicable?unlike
effective binary vector-space models. In binary
models, decision boundaries are linear1, whereas in
(vector-space) one-class models, the boundaries are
usually hyperspherical. Intuitively, since core docu-
ments tend to lie on a lower-dimensional manifold
(Lebanon, 2005), inducing hyperspherical bound-
aries may be sub-optimal as they tend to either cap-
ture just a portion of the core, or capture too much
space around it (see illustration in Figure 1). Here
we propose alternative ways for detecting the core,
which work well in text.
One-class learning problems have been studied as
either outlier detection or identifying a small coher-
ent subset. In one-class outlier detection (Tax and
Duin, 2001; Scho?lkopf et al, 2001), the goal is to
identify a few outliers from the given set of exam-
ples, where the vast majority of the examples are
considered relevant. Alternatively, a complementary
goal is to distill a subset of relevant examples, in the
space with many outliers (Crammer and Chechik,
1As such, or after applying the kernel trick (Cristianini and
Shawe-Taylor, 2000)
2004; Gupta and Ghosh, 2005; Crammer et al,
2008). Most of the one-class approaches employ ge-
ometrical concepts to capture the notion of relevancy
(or irrelevancy) using either hyperplanes (Scho?lkopf
et al, 2001) or hyperspheres (Tax and Duin, 2001;
Crammer and Chechik, 2004; Gupta and Ghosh,
2005). In this paper we adopt the latter approach:
we formulate one-class clustering in text as an opti-
mization task of identifying the most coherent subset
(the core) of k documents drawn from a given pool
of n > k documents.2
Given a collection D of on-topic and off-topic
documents, we assume that on-topic documents
share a portion of their vocabulary that consists of
?relatively rare? words, i.e. words that are used in D
more often than they are used in general English. We
call them topical words. For example, if some doc-
uments in D share words such as ?Bayesian?, ?clas-
sifier?, ?reinforcement? and other machine learning
terms (infrequent in general English), whereas other
documents do not seem to share any subset of words
(besides stopwords), then we conclude that the ma-
chine learning documents compose the core of D,
while non-machine learning documents are noise.
We express the level of topicality of a word w
in terms of the ratio ?(w) = p(w)q(w) , where p(w) is
w?s empirical probability (in D), and q(w) is its es-
timated probability in general English. We discuss
an interesting characteristic of ?(w): if D is large
enough, then, with high probability, ?(w) values are
greater for topical words than for non-topical words.
Therefore, ?(w) can be used as a mean to measure
the topicality of w.
Obviously, the quality of this measure depends on
the quality of estimating q(w), i.e. the general En-
glish word distribution, which is usually estimated
over a large text collection. The larger the collec-
tion is, the better would be the estimation. Recently,
Google has released the Web 1T dataset3 that pro-
vides q(w) estimated on a text collection of one tril-
lion tokens. We use it in our experimentation.
We propose two methods that use the ? ratio to
2The parameter k is analogous to the number of clusters in
(multi-class) clustering, as well as to the number of outliers (Tax
and Duin, 2001) or the radius of Bregmanian ball (Crammer and
Chechik, 2004)?in other formulations of one-class clustering.
3http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T13
42
gz
r |d| nw gzy
r |d| nw
Figure 2: (left) A simple generative model; (right) Latent
Topic/Background model (Section 4).
solve the one-class clustering problem. First, we ex-
press documents? topicality in terms of aggregating
their words? ? ratios into an information-theoretic
?topicality measure?. The core is then composed
of k documents with the highest topicality measure.
We show that the proposed measure is optimal for
constructing the core cluster among documents of
equal length. However, our method is not useful
in a setup where some long documents have a top-
ical portion: such documents should be considered
on-topic, but their heavy tail of background words
overcomes the topical words? influence. We gener-
alize our method to non-equally-long documents by
first extracting words that are supposed to be topi-
cal and then projecting documents over those words.
Such projection preserves the optimality characteris-
tic and results in constructing a more accurate core
cluster in practice. We call such a method of choos-
ing both topical words and core documents One-
Class Co-Clustering (OCCC).
It turns out that our OCCC method?s performance
depends heavily on choosing the number of topical
words. We propose a heuristic for setting this num-
ber. As another alternative, we propose a method
that does not require tuning this parameter: we
use words? ? ratios to initialize an EM algorithm
that computes the likelihood of documents to be-
long to the core?we then choose k documents of
maximal likelihood. We call this model the Latent
Topic/Background (LTB) model. LTB outperforms
OCCC in most of our test cases.
Our one-class clustering models have interesting
cross-links with models applied to other Informa-
tion Retrieval tasks. For example, a model that
resembles our OCCC, is proposed by Zhou and
Croft (2007) for query performance prediction. Tao
and Zhai (2004) describe a pseudo-relevance feed-
back model that is similar to our LTB. These types
of cross-links are common for the models that are
Figure 3: (left) Words? p(w) values when sorted by their
q(w) values; (right) words? ?(w) values.
general enough and relatively simple. In this paper
we put particular emphasis on the simplicity of our
models, such that they are feasible for theoretical
analysis as well as for efficient implementation.
2 Motivation for using ? ratios
Recall that we use the ?(w) = p(w)q(w) ratios to express
the level of our ?surprise? of seeing the word w. A
high value of ?(w) means that w is used in the cor-
pus more frequently than in general English, which,
we assume, implies that w is topical. The more top-
ical words a document contains, the more ?topical?
it is?k most topical documents compose the core
Dk ? D.
An important question is whether or not the ? ra-
tios are sufficient to detecting the actually topical
words. To address this question, let us model the
corpus D using a simple graphical model (Figure 2
left). In this model, the word distribution p(w) is
represented as a mixture of two multinomial distri-
butions: pr over a set R of topical words, and pg
over all the words G ? R in D. For each word wij
in a document di, we toss a coin Zij , such that, if
Zij = 1, then wij is sampled from pr, otherwise it
is sampled from pg. Define pi , p(Zij = 1).
If |G| ? |R| ? 0, and if pi ? 0, then top-
ical words would tend to appear more often than
non-topical words. However, we cannot simply base
our conclusions on word counts, as some words are
naturally more frequent than others (in general En-
glish). Figure 3 (left) illustrates this observation: it
shows words? p(w) values sorted by their q(w) val-
ues. It is hard to fit a curve that would separate be-
tween R and G \R. We notice however, that we can
?flatten? this graph by drawing ?(w) values instead
(see Figure 3 right). Here, naturally frequent words
are penalized by the q factor, so we can assume that,
when re-normalized, ?(w) behaves as a mixture of
two discrete uniform distributions. A simple thresh-
old can then separate between R and G \ R.
43
Proposition 1 Under the uniformity assumption, it
is sufficient to have a log-linear size sample (in |G|)
in order to determine the setRwith high probability.
See Bekkerman (2008) for the proof. The proposi-
tion states that in corpora of practical size4 the set of
topical words can be almost perfectly detected, sim-
ply by taking words with the highest ? ratios. Con-
sequently, the core Dk will consist of k documents,
each of which contains more topical words than any
document from D \ Dk.
To illustrate this theoretical result, we followed
the generative process as described above, and con-
structed an artificial dataset with characteristics sim-
ilar to those of our WAD dataset (see Section 5.1).
In particular, we fixed the size of the artificial dataset
to be equal to the size of the WAD dataset (N =
330, 000). We set the ratio of topical words to 0.2
and assumed uniformity of the ? values. In this
setup, we were able to detect the set of topical words
with a 98.5% accuracy.
2.1 Max-KL Algorithm
In this section, we propose a simple information-
theoretic algorithm for identifying the core Dk, and
show that it is optimal under the uniformity assump-
tion. Given the ? ratios of words, the aggregated
topicality of the corpus D can be expressed in terms
of the KL-divergence:
KL(p||q) =
?
w?G
p(w) log p(w)q(w)
=
?
d?D,w?G
p(d,w) log p(w)q(w) .
A document d?s contribution to the aggregated topi-
cality measure will assess the topicality of d:
KLd(p||q) =
?
w?G
p(d,w) log p(w)q(w) . (1)
The core Dk will be composed of documents with
the highest topicality scores. A simple, greedy algo-
rithm for detecting Dk is then:
1. Sort documents according to their topicality
value (1), in decreasing order.
2. Select the first k documents.
4N = O(m logm), where N is the number of word tokens
in D, and m = |G| is the size of the vocabulary.
Since the algorithm chooses documents with high
values of the KL divergence we call it the Max-KL
algorithm. We now argue that it is optimal under
the uniformity assumption. Indeed, if the corpus
D is large enough, then according to Proposition 1
(with high probability) any topical word w has a
lower ? ratio than any non-topical word. Assume
that all documents are of the same length (|d| is con-
stant). The Max-KL algorithm chooses documents
that contain more topical words than any other doc-
ument in the corpus?which is exactly the definition
of the core, as presented in Section 1. We summarize
this observation in the following proposition:
Proposition 2 If the corpus D is large enough, and
all the documents are of the same length, then the
Max-KL algorithm is optimal for the one-class clus-
tering problem under the uniformity assumption.
In contrast to the (quite natural) uniformity assump-
tion, the all-the-same-length assumption is quite re-
strictive. Let us now propose an algorithm that over-
comes this issue.
3 One-Class Co-Clustering (OCCC)
As accepted in Information Retrieval, we decide that
a document is on-topic if it has a topical portion, no
matter how long its non-topical portion is. There-
fore, we decide about documents? topicality based
on topical words only?non-topical words can be
completely disregarded. This observation leads us to
proposing a one-class co-clustering (OCCC) algo-
rithm: we first detect the set R of topical words, rep-
resent documents over R, and then detect Dk based
on the new representation.5
We reexamine the document?s topicality score (1)
and omit non-topical words. The new score is then:
KLrd(p||q) =
?
w?R
p?(d,w) log p(w)q(w) , (2)
where p?(d,w) = p(d,w)/(?w?R p(d,w)) is a
joint distribution of documents and (only) topical
words. The OCCC algorithm first uses ?(w) to
5OCCC is the simplest, sequential co-clustering algorithm,
where words are clustered prior to clustering documents (see,
e.g., Slonim and Tishby (2000)). In OCCC, word clustering is
analogous to feature selection. More complex algorithms can
be considered, where this analogy is less obvious.
44
choose the most topical words, then it projects doc-
uments on these words and apply the Max-KL algo-
rithm, as summarized below:
1. Sort words according to their ? ratios, in de-
creasing order.
2. Select a subset R of the first mr words.
3. Represent documents as bags-of-words over R
(delete counts of words from G \ R).
4. Sort documents according to their topicality
score (2), in decreasing order.
5. Select a subset Dk of the first k documents.
Considerations analogous to those presented in Sec-
tion 2.1, lead us to the following result:
Proposition 3 If the corpus D is large enough, the
OCCC algorithm is optimal for one-class clustering
of documents, under the uniformity assumption.
Despite its simplicity, the OCCC algorithm shows
excellent results on real-world data (see Section 5).
OCCC?s time complexity is particularly appealing:
O(N), where N is the number of word tokens in D.
3.1 Choosing size mr of the word cluster
The choice of mr = |R| can be crucial. We propose
a useful heuristic for choosing it. We assume that
the distribution of ? ratios for w ? R is a Gaussian
with a mean ?r ? 1 and a variance ?2r , and that the
distribution of ? ratios for w ? G \ R is a Gaussian
with a mean ?nr = 1 and a variance ?2nr. We also
assume that all the words with ?(w) < 1 are non-
topical. Since Gaussians are symmetric, we further
assume that the number of non-topical words with
?(w) < 1 equals the number of non-topical words
with ?(w) ? 1. Thus, our estimate of |G\R| is twice
the number of words with ?(w) < 1, and then the
number of topical words can be estimated as mr =
|G| ? 2 ?#{words with ?(w) < 1}.
4 Latent Topic/Background (LTB) model
Instead of sharply thresholding topical and non-
topical words, we can have them all, weighted with a
probability of being topical. Also, we notice that our
original generative model (Figure 2 left) assumes
that words are i.i.d. sampled, which can be relaxed
by deciding on the document topicality first. In our
new generative model (Figure 2 right), for each doc-
ument di, Yi is a Bernoulli random variable where
Algorithm 1 EM algorithm for one-class clustering
using the LTB model.
Input:
D ? the dataset
?(wl) = p(wl)q(wl) ? ? scores for each word wl|
m
l=1
T ? number of EM iterations
Output: Posteriors p(Yi = 1|di,?T ) for each doc di|ni=1
Initialization:
for each document di initialize pi1i
for each word wl initialize p1r(wl) = ?r?(wl);
p1g(wl) = ?g?(wl) , s.t. ?r and ?g are normalization factors
Main loop:
for all t = 1, . . . , T do
E-step:
for each document di compute ?ti = p(Yi = 1|di,?t)
for each word token wij compute
?tij = p(Zij = 1|Yi = 1, wij ,?t)
M-step:
for each document di update pit+1 = 1|di|
?
j ?tij
for each word wl update
pt+1r (wl) =
?
i ?ti
?
j ?(wij = wl) ?tij?
i ?ti
?
j ?tij
pt+1g (wl) =
Nw ?
?
i ?ti
?
j ?(wij = wl) ?tij
N ??i ?ti
?
j ?tij
Yi = 1 corresponds to di being on-topic. As be-
fore, Zij decides on the topicality of a word token
wij , but now given Yi. Since not all words in a
core document are supposed to be topical, then for
each word of a core document we make a separate
decision (based on Zij) whether it is sampled from
pr(W ) or pg(W ). However, if a document does not
belong to the core (Yi = 0), each its word is sampled
from pg(W ), i.e. p(Zij = 0|Yi = 0) = 1.
Inspired by Huang and Mitchell (2006), we use
the Expectation-Maximization (EM) algorithm to
exactly estimate parameters of our model from the
dataset. We now describe the model parameters ?.
First, the probability of any document to belong to
the core is denoted by p(Yi = 1) = kn = pd (thisparameter is fixed and will not be learnt from data).
Second, for each document di, we maintain a proba-
bility of each its word to be topical given that the
document is on-topic, p(Zij = 1|Yi = 1) = pii
for i = 1, . . . , n. Third, for each word wl (for
k = 1...m), we let p(wl|Zl = 1) = pr(wl) and
p(wl|Zl = 0) = pg(wl). The overall number of pa-
45
rameters is n+ 2m+ 1, one of which (pd) is preset.
The dataset likelihood is then:
p(D) =
n?
i=1
[pd p(di|Yi = 1) + (1? pd)p(di|Yi = 0)]
=
n?
i=1
?
?pd
|di|?
j=1
[piipr(wij) + (1? pii)pg(wij)]
+(1? pd)
|di|?
j=1
pg(wij)
?
? .
At each iteration t of the EM algorithm, we first
perform the E-step, where we compute the poste-
rior distribution of hidden variables {Yi} and {Zij}
given the current parameter values ?t and the data
D. Then, at the M-step, we compute the new pa-
rameter values ?t+1 that maximize the model log-
likelihood given ?t,D and the posterior distribution.
The initialization step is crucial for the EM al-
gorithm. Our pilot experimentation showed that if
distributions pr(W ) and pg(W ) are initialized as
uniform, the EM performance is close to random.
Therefore, we decided to initialize word probabili-
ties using normalized ? scores. We do not propose
the optimal way to initialize pii parameters, however,
as we show later in Section 5, our LTB model ap-
pears to be quite robust to the choice of pii.
The EM procedure is presented in Algorithm 1.
For details, see Bekkerman (2008). After T itera-
tions, we sort the documents according to ?i in de-
creasing order and choose the first k documents to
be the core. The complexity of Algorithm 1 is lin-
ear: O(TN). To avoid overfitting, we set T to be a
small number: in our experiments we fix T = 5.
5 Experimentation
We evaluate our OCCC and LTB models on two ap-
plications: a Web Mining task (Section 5.1), and a
Topic Detection and Tracking (TDT) (Allan, 2002)
task (Section 5.2).
To define our evaluation criteria, let C be the con-
structed cluster and let Cr be its portion consisting
of documents that actually belong to the core. We
define precision as Prec = |Cr|/|C|, recall as Rec =
|Cr|/k and F-measure as (2 Prec Rec)/(Prec+Rec).
Unless stated otherwise, in our experiments we fix
|C| = k, such that precision equals recall and is then
called one-class clustering accuracy, or just accu-
racy.
We applied our one-class clustering methods in
four setups:
? OCCC with the heuristic to choose mr (from
Section 3.1).
? OCCC with optimal mr. We unfairly choose
the number mr of topical words such that the
resulting accuracy is maximal. This setup
can be considered as the upper limit of the
OCCC?s performance, which can be hypotheti-
cally achieved if a better heuristic for choosing
mr is proposed.
? LTB initialized with pii = 0.5 (for each i).
As we show in Section 5.1 below, the LTB
model demonstrates good performance with
this straightforward initialization.
? LTB initialized with pii = pd. Quite naturally,
the number of topical words in a dataset de-
pends on the number of core documents. For
example, if the core is only 10% of a dataset, it
is unrealistic to assume that 50% of all words
are topical. In this setup, we condition the ratio
of topical words on the ratio of core documents.
We compare our methods with two existing al-
gorithms: (a) One-Class SVM clustering6 (Tax and
Duin, 2001); (b) One-Class Rate Distortion (OC-
RD) (Crammer et al, 2008). The later is considered
a state-of-the-art in one-class clustering. Also, to es-
tablish the lowest baseline, we show the result of a
random assignment of documents to the core Dk.
The OC-RD algorithm is based on rate-distortion
theory and expresses the one-class problem as a
lossy coding of each instance into a few possible
instance-dependent codewords. Each document is
represented as a distribution over words, and the KL-
divergence is used as a distortion function (gener-
ally, it can be any Bregman function). The algo-
rithm also uses an ?inverse temperature? parameter
(denoted by ?) that represents the tradeoff between
compression and distortion. An annealing process
is employed, in which the algorithm is applied with
a sequence of increasing values of ?, when initial-
ized with the result obtained at the previous itera-
6We used Chih-Jen Lin?s LibSVM with the -s 2 parame-
ter. We provided the core size using the -n parameter.
46
Method WAD TW
Random assignment 38.7% 34.9? 3.1%
One-class SVM 46.3% 45.2? 3.2%
One-class rate distortion 48.8% 63.6? 3.5%
OCCC with the mr heuristic 80.2% 61.4? 4.5%
OCCC with optimal m 82.4% 68.3? 3.6%
LTB initialized with pii = 0.5 79.8% 65.3? 7.3%
LTB initialized with pii = pd 78.3% 68.0? 5.9%
Table 1: One-class clustering accuracy of our OCCC and
LTB models on the WAD and the TW detection tasks, as
compared to OC-SVM and OC-RD. For TW, the accura-
cies are macro-averaged over the 26 weekly chunks, with
the standard error of the mean presented after the ? sign.
tion. The outcome is a sequence of cores with de-
creasing sizes. The annealing process is stopped
once the largest core size is equal to k.
5.1 Web appearance disambiguation
Web appearance disambiguation (WAD) is proposed
by Bekkerman and McCallum (2005) as the problem
of reasoning whether a particular mention of a per-
son name in the Web refers to the person of interest
or to his or her unrelated namesake. The problem is
solved given a few names of people from one social
network, where the objective is to construct a cluster
of Web pages that mention names of related people,
while filtering out pages that mention their unrelated
namesakes.
WAD is a classic one-class clustering task, that
is tackled by Bekkerman and McCallum with simu-
lated one-class clustering: they use a sophisticated
agglomerative/conglomerative clustering method to
construct multiple clusters, out of which one cluster
is then selected. They also use a simple link struc-
ture (LS) analysis method that matches hyperlinks
of the Web pages in order to compose a cloud of
pages that are close to each other in the Web graph.
The authors suggest that the best performance can
be achieved by a hybrid of the two approaches.
We test our models on the WAD dataset,7 which
consists of 1085 Web pages that mention 12 people
names of AI researchers, such as Tom Mitchell and
Leslie Kaelbling. Out of the 1085 pages, 420 are
on-topic, so we apply our algorithms with k = 420.
At a preprocessing step, we binarize document vec-
tors and remove low frequent words (both in terms
7http://www.cs.umass.edu/?ronb/name_
disambiguation.html
# OCCC LTB
1 cheyer artificial
2 kachites learning
3 quickreview cs
4 adddoc intelligence
5 aaai98 machine
6 kaelbling edu
7 mviews algorithms
8 mlittman proceedings
9 hardts computational
10 meuleau reinforcement
11 dipasquo papers
12 shakshuki cmu
13 xevil aaai
14 sangkyu workshop
15 gorfu kaelbling
Table 2: Most highly ranked words by OCCC and LTB,
on the WAD dataset.
of p(w) and q(w)). The results are summarized in
the middle column of Table 1. We can see that both
OCCC and LTB dramatically outperform their com-
petitors, while showing practically indistinguishable
results compared to each other. Note that when the
size of the word cluster in OCCC is unfairly set to
its optimal value, mr = 2200, the OCCC method
is able to gain a 2% boost. However, for obvious
reasons, the optimal value of mr may not always be
obtained in practice.
Table 2 lists a few most topical words according
to the OCCC and LTB models. The OCCC algo-
rithm sorts words according to their ? scores, such
that words that often occur in the dataset but rarely in
the Web, are on the top of the list. These are mostly
last names or login names of researchers, venues etc.
The EM algorithm of LTB is the given ? scores as an
input to initialize p1r(w) and p1g(w), which are then
updated at each M-step. In the LTB columns, words
are sorted by p5r(w). High quality of the LTB list
is due to conditional dependencies in our generative
model (via the Yi nodes).
Solid lines in Figure 4 demonstrate the robustness
of our models to tuning their main parameters (mr
for OCCC, and the pii initialization for LTB). As can
be seen from the left panel, OCCC shows robust
performance: the accuracy above 80% is obtained
when the word cluster is of any size in the 1000?
3000 range. The heuristic from Section 3.1 suggests
a cluster size of 1000. The LTB is even more robust:
practically any value of pii (besides the very large
ones, pii ? 1) can be chosen.
47
0 2500 5000 7500 100000.660.7
0.740.78
0.820.86
size of word cluster
accura
cy of do
c cluster
OCCC method
 
 OCCCOCCC+link
0 0.2 0.4 0.6 0.8 10.660.7
0.740.78
0.820.86
pii parameter initialization
accura
cy of do
c cluster
LTB method
 
 LTBLTB+link
Figure 4: Web appearance disambiguation: (left)
OCCC accuracy as a function of the word cluster size;
(right) LTB accuracy over various initializations of pii pa-
rameters. The red dotted lines show the accuracy of each
method?s results combined with the Link Structure model
results. On the absolute scale, OCCC outperforms LTB,
however LTB shows more robust behavior than OCCC.
To perform a fair comparison of our results
with those obtained by Bekkerman and McCal-
lum (2005), we construct hybrids of their link struc-
ture (LS) analysis model with our OCCC and LTB,
as follows. First, we take their LS core cluster,
which consists of 360 documents. Second, we pass
over all the WAD documents in the order as they
were ranked by either OCCC or LTB, and enlarge
the LS core with 60 most highly ranked documents
that did not occur in the LS core. In either case, we
end up with a hybrid core of 420 documents.
Dotted lines in Figure 4 show accuracies of the
resulting models. As the F-measure of the hy-
brid model proposed by Bekkerman and McCal-
lum (2005) is 80.3%, we can see that it is signifi-
cantly inferior to the results of either OCCC+LS or
LTB+LS, when their parameters are set to a small
value (mr < 3000 for OCCC, pii < 0.06 for
LTB). Such a choice of parameter values can be
explained by the fact that we need only 60 docu-
ments to expand the LS core cluster to the required
size k = 420. When the values of mr and pii are
small, both OCCC and LTB are able to build very
small and very precise core clusters, which is exactly
what we need here. The OCCC+LS hybrid is par-
ticularly successful, because it uses non-canonical
words (see Table 2) to compose a clean core that al-
most does not overlap with the LS core. Remark-
ably, the OCCC+LS model obtains 86.4% accuracy
with mr = 100, which is the state-of-the-art result
on the WAD dataset.
200 400 600 800 1000
0.6
0.7
0.8
0.9
0.5 document cluster size
F?me
asure
 
 OCCCOCCLTB
Figure 5: Web appearance disambiguation: F-measure
as a function of document cluster size: a vertical line in-
dicates the point where precision equals recall (and there-
fore equals accuracy). ?OCC? refers to the OCCC model
where all the words are taken as the word cluster (i.e. no
word filtering is done).
To answer the question how much our models are
sensitive to the choice of the core size k, we com-
puted the F-measure of both OCCC and LTB as a
function of k (Figure 5). It turns out that our meth-
ods are quite robust to tuning k: choosing any value
in the 300?500 range leads to good results.
5.2 Detecting the topic of the week
Real-world data rarely consists of a clean core and
uniformly distributed noise. Usually, the noise has
some structure, namely, it may contain coherent
components. With this respect, one-class clustering
can be used to detect the largest coherent compo-
nent in a dataset, which is an integral part of many
applications. In this section, we solve the problem of
automatically detecting the Topic of the Week (TW)
in a newswire stream, i.e. detecting all articles in a
weekly news roundup that refer to the most broadly
discussed event.
We evaluate the TW detection task on the bench-
mark TDT-5 dataset8, which consists of 250 news
events spread over a time period of half a year, and
9,812 documents in English, Arabic and Chinese
(translated to English), annotated by their relation-
ship to those events.9 The largest event in TDT-5
dataset (#55106, titled ?Bombing in Riyadh, Saudi
Arabia?) has 1,144 documents, while 66 out of the
250 events have only one document each. We split
the dataset to 26 weekly chunks (to have 26 full
8http://projects.ldc.upenn.edu/TDT5/
9We take into account only labeled documents, while ignor-
ing unlabeled documents that can be found in the TDT-5 data.
48
1 2 3 4 5 6 7 8 9 10 11 12 130
0.5
1
week
acc
ura
cy
Performance of OCCC and LTB on the "topic of the week" task
14 15 16 17 18 19 20 21 22 23 24 25 260
0.5
1
week
acc
ura
cy
 
 
OCCC with the mr heuristicOCCC with the optimal mrLTB initialized with pii = 0.5LTB initialized with pii = pd
Figure 6: ?Topic of the week? detection task: Accuracies of two OCCC methods and two LTB methods.
weeks, we delete all the documents dated with the
last day in the dataset, which decreases the dataset?s
size to 9,781 documents). Each chunk contains from
138 to 1292 documents.
The one-class clustering accuracies, macro-
averaged over the 26 weekly chunks, are presented
in the right column of Table 1. As we can see, both
LTB models, as well as OCCC with the optimal mr,
outperform our baselines. Interestingly, even the op-
timal choice of mr does not lead OCCC to signif-
icantly superior results while compared with LTB.
The dataset-dependent initialization of LTB?s pii pa-
rameters (pii = pd) appears to be preferable over the
dataset-independent one (pii = 0.5).
Accuracies per week are shown in Figure 6. These
results reveal two interesting observations. First,
OCCC tends to outperform LTB only on data chunks
where the results are quite low in general (less than
60% accuracy). Specifically, on weeks 2, 4, 11,
and 16 the LTB models show extremely poor per-
formance. While investigating this phenomenon, we
discovered that in two of the four cases LTB was
able to construct very clean core clusters, however,
those clusters corresponded to the second largest
topic, while we evaluate our methods on the first
largest topic.10 Second, the (completely unsuper-
10For example, on the week-4 data, topic #55077 (?River
ferry sinks on Bangladeshi river?) was discovered by LTB as
the largest and most coherent one. However, in that dataset,
topic #55077 is represented by 20 documents, while topic
#55063 (?SARS Quarantined medics in Taiwan protest?) is
represented by 27 documents, such that topic #55077 is in fact
the second largest one.
vised) LTB model can obtain very good results on
some of the data chunks. For example, on weeks 5,
8, 19, 21, 23, 24, and 25 the LTB?s accuracy is above
90%, with a striking 100% on week-23.
6 Conclusion
We have developed the theory and proposed practi-
cal methods for one-class clustering in the text do-
main. The proposed algorithms are very simple,
very efficient and still surprisingly effective. More
sophisticated algorithms (e.g. an iterative11 version
of OCCC) are emerging.
7 Acknowledgements
We thank Erik Learned-Miller for the inspiration
on this project. We also thank Gunjan Gupta,
James Allan, and Fernando Diaz for fruitful dis-
cussions. This work was supported in part by the
Center for Intelligent Information Retrieval and in
part by the Defense Advanced Research Projects
Agency (DARPA) under contract number HR0011-
06-C-0023. Any opinions, findings and conclusions
or recommendations expressed in this material are
the authors? and do not necessarily reflect those of
the sponsor.
References
J. Allan, editor. 2002. Topic detection and tracking:
event-based information organization. Kluwer Aca-
demic Publishers.
11See, e.g., El-Yaniv and Souroujon (2001)
49
R. Bekkerman and A. McCallum. 2005. Disambiguat-
ing web appearances of people in a social network. In
Proceedings of WWW-05, the 14th International World
Wide Web Conference.
R. Bekkerman. 2008. Combinatorial Markov Random
Fields and their Applications to Information Organi-
zation. Ph.D. thesis, University of Massachusetts at
Amherst.
K. Crammer and G. Chechik. 2004. A needle in a
haystack: local one-class optimization. In Proceed-
ings of the 21st International Conference on Machine
Learning.
K. Crammer, P. Talukdar, and F. Pereira. 2008. A rate-
distortion one-class model and its applications to clus-
tering. In Proceedings of the 25st International Con-
ference on Machine Learning.
N. Cristianini and J. Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
R. El-Yaniv and O. Souroujon. 2001. Iterative double
clustering for unsupervised and semi-supervised learn-
ing. In Advances in Neural Information Processing
Systems (NIPS-14).
G. Gupta and J. Ghosh. 2005. Robust one-class cluster-
ing using hybrid global and local search. In Proceed-
ings of the 22nd International Conference on Machine
Learning, pages 273?280.
Y. Huang and T. Mitchell. 2006. Text clustering with ex-
tended user feedback. In Proceedings of the 29th an-
nual international ACM SIGIR conference, pages 413?
420.
G. Lebanon. 2005. Riemannian Geometry and Statistical
Machine Learning. Ph.D. thesis, CMU.
B. Scho?lkopf, J. C. Platt, J. C. Shawe-Taylor, A. J. Smola,
and R. C. Williamson. 2001. Estimating the support
of a high-dimensional distribution. Neural Computa-
tion, 13(7):1443?1471.
N. Slonim and N. Tishby. 2000. Document cluster-
ing using word clusters via the information bottleneck
method. In Proceedings of the 23rd annual interna-
tional ACM SIGIR conference, pages 208?215.
T. Tao and C. Zhai. 2004. A two-stage mixture model for
pseudo feedback. In Proceedings of the 27th annual
international ACM SIGIR conference, pages 486?487.
D. M. J. Tax and R. P. W. Duin. 2001. Outliers and
data descriptions. In Proceedings of the 7th Annual
Conference of the Advanced School for Computing and
Imaging, pages 234?241.
Y. Zhou and W. B. Croft. 2007. Query performance pre-
diction in web search environments. In Proceedings
of the 30th Annual International ACM SIGIR Confer-
ence.
50
